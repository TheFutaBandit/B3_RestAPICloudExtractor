Bug_ID,Bug_Summary,Bug_Description
HIVE-28030,LLAP util code refactor,
HIVE-27675,Support keystore/truststore types for hive to zookeeper integration points,"In HIVE-24253, we added support for HS2/HMS/JDBC DRiver to support other store types like BCFKS (other than JKS). This allows JDBC Clients to connect to HS2 directly. However, with service discovery enabled, the clients have to connect zookeeper to determine HS2 endpoints. This connectivity currently does not support other store types. Similarly, HS2/HMS services also do not provide ability to use different store types for the zk registration process.
{noformat}
$ beeline 
Connecting to jdbc:hive2://<snip>:2181/default;httpPath=cliservice;principal=hive/_HOST@<SNIP>;retries=5;serviceDiscoveryMode=zooKeeper;ssl=true;sslTrustStore=/var/lib/cloudera-scm-agent/agent-cert/cm-auto-global_truststore.jks;transportMode=http;trustStorePassword=RoeCFK11Pq54;trustStoreType=bcfks;zooKeeperNamespace=hiveserver2
Error: org.apache.hive.jdbc.ZooKeeperHiveClientException: Unable to read HiveServer2 configs from ZooKeeper (state=,code=0) 
{noformat}


{noformat}
Opening socket connection to server <SNIP>:2182. Will attempt to SASL-authenticate using Login Context section 'HiveZooKeeperClient'
2023-08-09 13:28:07,591 WARN  io.netty.channel.ChannelInitializer: [nioEventLoopGroup-3-1]: Failed to initialize a channel. Closing: [id: 0x0937583f]
org.apache.zookeeper.common.X509Exception$SSLContextException: Failed to create KeyManager
        at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:346) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.common.X509Util.createSSLContext(X509Util.java:278) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initSSL(ClientCnxnSocketNetty.java:454) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initChannel(ClientCnxnSocketNetty.java:444) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.ClientCnxnSocketNetty$ZKClientPipelineFactory.initChannel(ClientCnxnSocketNetty.java:429) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at io.netty.channel.ChannelInitializer.initChannel(ChannelInitializer.java:129) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.ChannelInitializer.handlerAdded(ChannelInitializer.java:112) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.AbstractChannelHandlerContext.callHandlerAdded(AbstractChannelHandlerContext.java:1114) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.DefaultChannelPipeline.callHandlerAdded0(DefaultChannelPipeline.java:609) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.DefaultChannelPipeline.access$100(DefaultChannelPipeline.java:46) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.DefaultChannelPipeline$PendingHandlerAddedTask.execute(DefaultChannelPipeline.java:1463) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.DefaultChannelPipeline.callHandlerAddedForAllHandlers(DefaultChannelPipeline.java:1115) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.DefaultChannelPipeline.invokeHandlerAddedIfNeeded(DefaultChannelPipeline.java:650) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:514) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174) [netty-common-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167) [netty-common-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470) [netty-common-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569) [netty-transport-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [netty-common-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.86.Final.jar:4.1.86.Final]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.86.Final.jar:4.1.86.Final]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_382]
Caused by: org.apache.zookeeper.common.X509Exception$KeyManagerException: java.io.IOException: Invalid keystore format
        at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:471) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:344) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        ... 23 more
Caused by: java.io.IOException: Invalid keystore format
        at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:666) ~[?:1.8.0_382]
        at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:57) ~[?:1.8.0_382]
        at sun.security.provider.KeyStoreDelegator.engineLoad(KeyStoreDelegator.java:224) ~[?:1.8.0_382]
        at sun.security.provider.JavaKeyStore$DualFormatJKS.engineLoad(JavaKeyStore.java:71) ~[?:1.8.0_382]
        at java.security.KeyStore.load(KeyStore.java:1445) ~[?:1.8.0_382]
        at org.apache.zookeeper.common.StandardTypeFileKeyStoreLoader.loadKeyStore(StandardTypeFileKeyStoreLoader.java:54) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.common.X509Util.loadKeyStore(X509Util.java:400) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.common.X509Util.createKeyManager(X509Util.java:460) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        at org.apache.zookeeper.common.X509Util.createSSLContextAndOptions(X509Util.java:344) ~[zookeeper-3.5.5.7.2.16.300-7.jar:3.5.5.7.2.16.300-7]
        ... 23 more
2023-08-09 13:28:07,591 INFO  org.apache.zookeeper.ClientCnxnSocketNetty: [nioEventLoopGroup-3-1]: future isn't success, cause:
io.netty.channel.StacklessClosedChannelException: null
        at io.netty.channel.AbstractChannel$AbstractUnsafe.ensureOpen(ChannelPromise)(Unknown Source) ~[netty-transport-4.1.86.Final.jar:4.1.86.Final]
{noformat}
"
HIVE-25659,Metastore direct sql queries with IN/(NOT IN) should be split based on max parameters allowed by SQL DB,"Function org.apache.hadoop.hive.metastore.txn.TxnUtils#buildQueryWithINClauseStrings can generate queries with huge number of parameters with very small value of DIRECT_SQL_MAX_ELEMENTS_IN_CLAUSE and DIRECT_SQL_MAX_QUERY_LENGTH while generating delete query for completed_compactions table

Example:
{code:java}
DIRECT_SQL_MAX_ELEMENTS_IN_CLAUSE = 100
DIRECT_SQL_MAX_QUERY_LENGTH = 10 (10 KB)
Number of parameters in a single query = 4759
{code}"
HIVE-25600,Compaction job creates redundant base/delta folder within base/delta folder,"{noformat}
Hive table 'aa.bb_item' is corrupt. Found sub-directory 'abfs://aabb-hive-data@gen2hiveaabb.dfs.core.windows.net/prod-data/aa.db/bb_item/part_created_on=202105/base_0004042/base_0004042' in bucket directory for partition: part_created_on=202105
 at io.prestosql.plugin.hive.BackgroundHiveSplitLoader.loadPartition(BackgroundHiveSplitLoader.java:543)
 at io.prestosql.plugin.hive.BackgroundHiveSplitLoader.loadSplits(BackgroundHiveSplitLoader.java:325)
 at io.prestosql.plugin.hive.BackgroundHiveSplitLoader$HiveSplitLoaderTask.process(BackgroundHiveSplitLoader.java:254)
 at io.prestosql.plugin.hive.util.ResumableTasks$1.run(ResumableTasks.java:38)
 at io.prestosql.$gen.Presto_347____20210615_143054_2.run(Unknown Source)
 at io.airlift.concurrent.BoundedExecutor.drainQueue(BoundedExecutor.java:80)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
 at java.base/java.lang.Thread.run(Thread.java:829);{noformat}
Why it happens:
Multiple compaction jobs for the same transactions can be triggered if the HMS gets restarted and the MR job is still in progress."
HIVE-25576,"Configurable datetime formatter for unix_timestamp, from_unixtime","HIVE-25403, HIVE-25458 switched the internal implementation of datetime formatter for unix_timestamp and from_unixtime from {{java.text.SimpleDateFormat}} to {{java.time.format.DateTimeFormatter}} in order fix some bugs and inconsistencies when the aforementioned functions are combined with other UDFs that have already migrated to use the new modern java.time package.

The two Java formatters present differences in their behavior leading to different query results. The supported patterns, between the two formatters, are also different something that makes existing queries crash at runtime (after upgrade). Adapting to the new behavior of DateTimeFormatter is a challenging and time-consuming task for end users especially due to the widespread use of the afforementioned unixtime functions.

Although DateTimeFormatter is a clear improvement over SimpleDateFormat some users still want to retain the old behavior for compatibility reasons thus introducing a property is necessary for facilitating migration.

The goal of this ticket is to introduce a new property namely {{hive.datetime.formatter}} to control the formatter used by unix_timestamp and from_unixtime. By default the new {{DateTimeFormatter}} is used while the use of {{SimpleDateFormat}} is discouraged. Eventually, {{SimpleDateFormat}} will cease to exist."
HIVE-25553,Support Map data-type natively in Arrow format,"Currently ArrowColumnarBatchSerDe converts map datatype as a list of structs data-type (where stuct is containing the key-value pair of the map). This causes issues when reading Map datatype using llap-ext-client as it reads a list of structs instead. 

HiveWarehouseConnector which uses the llap-ext-client throws exception when the schema (containing Map data type) is different from actual data (list of structs).

 

Fixing this issue requires upgrading arrow version (where map data-type is supported), modifying ArrowColumnarBatchSerDe and corresponding Serializer/Deserializer to not use list as a workaround for map and use the arrow map data-type instead. "
HIVE-25443,Arrow SerDe Cannot serialize/deserialize complex data types When there are more than 1024 values,"Complex data types like MAP, STRUCT cannot be serialized/deserialzed using Arrow SerDe when there are more than 1024 values. This happens due to ColumnVector always being initialized with a size of 1024.

Issue #1 : https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/ArrowColumnarBatchSerDe.java#L213

Issue #2 : https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/io/arrow/ArrowColumnarBatchSerDe.java#L215

Sample unit test to reproduce the case in TestArrowColumnarBatchSerDe :


{code:java}
@Test
   public void testListBooleanWithMoreThan1024Values() throws SerDeException {
     String[][] schema = {
             {""boolean_list"", ""array<boolean>""},
     };
  
     Object[][] rows = new Object[1025][1];
     for (int i = 0; i < 1025; i++) {
       rows[i][0] = new BooleanWritable(true);
     }
  
     initAndSerializeAndDeserialize(schema, toList(rows));
   }
  
{code}

"
HIVE-25403,Fix from_unixtime() to consider leap seconds ,"The Unix_timestamp() considers ""leap second"" while the from_unixtime is not; which results in to wrong result as below:

!image-2021-07-29-14-42-49-806.png!"
HIVE-25268,date_format udf returns wrong results for dates prior to 1900 if the local timezone is other than UTC,"*Hive 1.2.1*:
{code:java}
 select date_format('1400-01-14 01:00:00', 'yyyy-MM-dd HH:mm:ss z');
+--------------------------+--+
|           _c0            |
+--------------------------+--+
| 1400-01-14 01:00:00 ICT  |
+--------------------------+--+

select date_format('1800-01-14 01:00:00', 'yyyy-MM-dd HH:mm:ss z');
+--------------------------+--+
|           _c0            |
+--------------------------+--+
| 1800-01-14 01:00:00 ICT  |
+--------------------------+--+
{code}
*Hive 3.1, Hive 4.0:*
{code:java}
select date_format('1400-01-14 01:00:00', 'yyyy-MM-dd HH:mm:ss z');
+--------------------------+
|           _c0            |
+--------------------------+
| 1400-01-06 01:17:56 ICT  |
+--------------------------+

select date_format('1800-01-14 01:00:00', 'yyyy-MM-dd HH:mm:ss z');
+--------------------------+
|           _c0            |
+--------------------------+
| 1800-01-14 01:17:56 ICT  |
+--------------------------+
{code}
VM timezone is set to 'Asia/Bangkok'"
HIVE-25219,Backward incompatible timestamp serialization in Avro for certain timezones,"HIVE-12192, HIVE-20007 changed the way that timestamp computations are performed and to some extend how timestamps are serialized and deserialized in files (Parquet, Avro).

In versions that include HIVE-12192 or HIVE-20007 the serialization in Avro files is not backwards compatible. In other words writing timestamps with a version of Hive that includes HIVE-12192/HIVE-20007 and reading them with another (not including the previous issues) may lead to different results depending on the default timezone of the system.

Consider the following scenario where the default system timezone is set to US/Pacific.

At apache/master commit eedcd82bc2d61861a27205f925ba0ffab9b6bca8
{code:sql}
CREATE EXTERNAL TABLE employee(eid INT,birth timestamp) STORED AS AVRO
 LOCATION '/tmp/hiveexttbl/employee';
INSERT INTO employee VALUES (1, '1880-01-01 00:00:00');
INSERT INTO employee VALUES (2, '1884-01-01 00:00:00');
INSERT INTO employee VALUES (3, '1990-01-01 00:00:00');
SELECT * FROM employee;
{code}
|1|1880-01-01 00:00:00|
|2|1884-01-01 00:00:00|
|3|1990-01-01 00:00:00|

At apache/branch-2.3 commit 324f9faf12d4b91a9359391810cb3312c004d356
{code:sql}
CREATE EXTERNAL TABLE employee(eid INT,birth timestamp) STORED AS AVRO
 LOCATION '/tmp/hiveexttbl/employee';
SELECT * FROM employee;
{code}
|1|1879-12-31 23:52:58|
|2|1884-01-01 00:00:00|
|3|1990-01-01 00:00:00|

The timestamp for {{eid=1}} in branch-2.3 is different from the one in master."
HIVE-25104,Backward incompatible timestamp serialization in Parquet for certain timezones,"HIVE-12192, HIVE-20007 changed the way that timestamp computations are performed and to some extend how timestamps are serialized and deserialized in files (Parquet, Avro).

In versions that include HIVE-12192 or HIVE-20007 the serialization in Parquet files is not backwards compatible. In other words writing timestamps with a version of Hive that includes HIVE-12192/HIVE-20007 and reading them with another (not including the previous issues) may lead to different results depending on the default timezone of the system.

Consider the following scenario where the default system timezone is set to US/Pacific.

At apache/master commit 37f13b02dff94e310d77febd60f93d5a205254d3
{code:sql}
CREATE EXTERNAL TABLE employee(eid INT,birth timestamp) STORED AS PARQUET
 LOCATION '/tmp/hiveexttbl/employee';
INSERT INTO employee VALUES (1, '1880-01-01 00:00:00');
INSERT INTO employee VALUES (2, '1884-01-01 00:00:00');
INSERT INTO employee VALUES (3, '1990-01-01 00:00:00');
SELECT * FROM employee;
{code}
|1|1880-01-01 00:00:00|
|2|1884-01-01 00:00:00|
|3|1990-01-01 00:00:00|

At apache/branch-2.3 commit 324f9faf12d4b91a9359391810cb3312c004d356
{code:sql}
CREATE EXTERNAL TABLE employee(eid INT,birth timestamp) STORED AS PARQUET
 LOCATION '/tmp/hiveexttbl/employee';
SELECT * FROM employee;
{code}
|1|1879-12-31 23:52:58|
|2|1884-01-01 00:00:00|
|3|1990-01-01 00:00:00|

The timestamp for {{eid=1}} in branch-2.3 is different from the one in master."
HIVE-24815,"Remove ""IDXS"" Table from Metastore Schema","In Hive 3 the rarely used ""INDEXES"" was removed from the DDL

https://issues.apache.org/jira/browse/HIVE-18448

 

There are a few issues here:
 # The Standalone-Metastore schema for Hive 3+ all include the ""IDXS"" table, which has no function.
 ** [https://github.com/apache/hive/tree/master/standalone-metastore/metastore-server/src/main/sql/mysql]
 # The upgrade schemas from 2.x -> 3.x do not do any cleanup of the IDXS table
 ** If a user used the ""INDEXES"" feature in 2.x and then upgrades their metastore to 3.x+ they cannot drop any table that has an index on it due to ""IDXS_FK1"" constraint since the TBLS entry is referenced in the IDXS table
 ** Since INDEX is no longer in the DDL they cannot run any command from Hive to drop the index.
 ** Users can manually connect to the metastore and either drop the IDXS table or the foreign key constraint

 

Since indexes provide no benefits in Hive 3+ it should be fine to drop them completely in the schema upgrade scripts. At the very least the 2.x -> 3.x+ scripts should drop the fk constraint."
HIVE-24636,Memory leak due to stacking UDFClassLoader in Apache Commons LogFactory,"Much the same as [HIVE-7563|https://issues.apache.org/jira/browse/HIVE-7563], after ClassLoader is closed in JavaUtils, it should be released by Apache Commons LogFactory, or the ClassLoader can't be Garbage Collected, which leads to memory leak, exactly our PROD met."
HIVE-24570,Hive on spark tmp file should be delete when driver process finished,"Hive on spark tmp file should be delete when driver process finished, now it`s in java.io.tmpdir (default /tmp) directory until hiveserver jvm is stop"
HIVE-24423,Improve DbNotificationListener Thread,"Clean up and simplify {{DbNotificationListener}} thread class.

Most importantly, stop the thread and wait for it to finish before launching a new thread."
HIVE-24348,Beeline: Isolating dependencies and execution with java,"Currently, beeline code, binaries and executables are somewhat tightly coupled with the hive product. To be able to execute beeline from a node with just JRE installed and some jars in classpath is impossible.
* beeline.sh/hive scripts rely on HADOOP_HOME to be set which are designed to use ""hadoop"" executable to run beeline.
* Ideally, just the hive-beeline.jar and hive-jdbc-standalone jars should be enough but sadly they arent. The latter jar adds more problems than it solves because all the classfiles are shaded some dependencies cannot be resolved.
* Beeline has many other dependencies like hive-exec, hive-common. hadoop-common, supercsv, jline, commons-cli, commons-io, commons-logging etc. While it may not be possible to eliminate some of these, we should atleast have a self-contains jar that contains all these to be able to make it work.
* the underlying script used to run beeline should use JAVA as an alternate means to execute if HADOOP_HOME is not set"
HIVE-24288,Files created by CompileProcessor have incorrect permissions,Compile processor generates some temporary files as part of processing. These need to be cleaned up on exit from CLI.
HIVE-24245,Vectorized PTF with count and distinct over partition producing incorrect results.,"Vectorized PTF for count and distinct over partition is broken. It produces incorrect results.
Below is the test case.

{code}
CREATE TABLE bigd781b_new (
  id int,
  txt1 string,
  txt2 string,
  cda_date int,
  cda_job_name varchar(12));

INSERT INTO bigd781b_new VALUES 
  (1,'2010005759','7164335675012038',20200528,'load1'),
  (2,'2010005759','7164335675012038',20200528,'load2');
{code}

Running below query produces incorrect results

{code}
SELECT
    txt1,
    txt2,
    count(distinct txt1) over(partition by txt1) as n,
    count(distinct txt2) over(partition by txt2) as m
FROM bigd781b_new
{code}

as below.

{code}
+-------------+-------------------+----+----+
|    txt1     |       txt2        | n  | m  |
+-------------+-------------------+----+----+
| 2010005759  | 7164335675012038  | 2  | 2  |
| 2010005759  | 7164335675012038  | 2  | 2  |
+-------------+-------------------+----+----+
{code}

While the correct output would be

{code}
+-------------+-------------------+----+----+
|    txt1     |       txt2        | n  | m  |
+-------------+-------------------+----+----+
| 2010005759  | 7164335675012038  | 1  | 1  |
| 2010005759  | 7164335675012038  | 1  | 1  |
+-------------+-------------------+----+----+
{code}


The problem does not appear after setting below property
set hive.vectorized.execution.ptf.enabled=false;
"
HIVE-24122,"When CBO is enable, CAST(STR as Bigint)IS NOT NULL result is wrong ","{code:java}
create  database testdb;
CREATE TABLE IF NOT EXISTS testdb.z_tab 
( 
    SEARCHWORD    STRING, 
    COUNT_NUM     BIGINT, 
    WORDS         STRING 
) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
STORED AS TEXTFILE;
insert into table testdb.z_tab values('hivetest',111,'aaa'),('hivetest2',111,'bbb');

set hive.cbo.enable=true;

SELECT CAST(searchword as bigint) IS NOT NULL FROM testdb.z_tab;
SELECT CAST(searchword as bigint) IS NULL FROM testdb.z_tab;
{code}
The SQL results for both queries are the same， as follows:
{noformat}
+-------+
|  _c0  |
+-------+
| true  |
| true  |
+-------+{noformat}
SELECT CAST(searchword as bigint) IS NOT NULL FROM testdb.z_tab;  execute result is wrong

 "
HIVE-24094,"cast type mismatch and use is not null, the results are error if cbo is true","1.CREATE TABLE IF NOT EXISTS testa
( 
 SEARCHWORD STRING, 
 COUNT_NUM BIGINT, 
 WORDS STRING 
) 
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\27' 
STORED AS TEXTFILE; 

2.insert into testa values('searchword', 1, 'a');

3.set hive.cbo.enable=false;

4.SELECT 
CASE 
 WHEN CAST(searchword as bigint) IS NOT NULL THEN CAST(CAST(searchword as bigint) as String) 
 ELSE searchword 
END AS WORDS, 
searchword FROM testa;

!image-2020-08-31-10-01-26-250.png!

5.set hive.cbo.enable=true;

6.SELECT 
CASE 
 WHEN CAST(searchword as bigint) IS NOT NULL THEN CAST(CAST(searchword as bigint) as String) 
 ELSE searchword 
END AS WORDS, 
searchword FROM testa;

!image-2020-08-31-10-02-39-154.png!"
HIVE-23873,Querying Hive JDBCStorageHandler table fails with NPE when CBO is off,"Scenario is Hive table having same schema as table in Oracle, however when we query the table with data it fails with NPE, below is the trace.

{code}
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:617) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:524) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2739) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:473) ~[hive-service-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        ... 34 more
Caused by: java.lang.NullPointerException
        at org.apache.hive.storage.jdbc.JdbcSerDe.deserialize(JdbcSerDe.java:164) ~[hive-jdbc-handler-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:598) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:524) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2739) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:473) ~[hive-service-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        ... 34 more
{code}

Problem appears when column names in Oracle are in Upper case and since in Hive, table and column names are forced to store in lowercase during creation. User runs into NPE error while fetching data.

While deserializing data, input consists of column names in lower case which fails to get the value

https://github.com/apache/hive/blob/rel/release-3.1.2/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcSerDe.java#L136
{code}
rowVal = ((ObjectWritable)value).get();
{code}

Log Snio:
=============
{code}
2020-07-17T16:49:09,598 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: dao.GenericJdbcDatabaseAccessor (:()) - Query to execute is [select * from TESTHIVEJDBCSTORAGE]
2020-07-17T16:49:10,642 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: jdbc.JdbcSerDe (:()) - *** ColumnKey = ID
2020-07-17T16:49:10,642 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: jdbc.JdbcSerDe (:()) - *** Blob value = {fname=OW[class=class java.lang.String,value=Name1], id=OW[class=class java.lang.Integer,value=1]}
{code}

Simple Reproducer for this case.
=============
1. Create table in Oracle
{code}
create table TESTHIVEJDBCSTORAGE(ID INT, FNAME VARCHAR(20));
{code}

2. Insert dummy data.
{code}
Insert into TESTHIVEJDBCSTORAGE values (1, 'Name1');
{code}

3. Create JDBCStorageHandler table in Hive.
{code}
CREATE EXTERNAL TABLE default.TESTHIVEJDBCSTORAGE_HIVE_TBL (ID INT, FNAME VARCHAR(20)) 
STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' 
TBLPROPERTIES ( 
""hive.sql.database.type"" = ""ORACLE"", 
""hive.sql.jdbc.driver"" = ""oracle.jdbc.OracleDriver"", 
""hive.sql.jdbc.url"" = ""jdbc:oracle:thin:@orachehostname/XE"", 
""hive.sql.dbcp.username"" = ""chiran"", 
""hive.sql.dbcp.password"" = ""supersecurepassword"", 
""hive.sql.table"" = ""TESTHIVEJDBCSTORAGE"", 
""hive.sql.dbcp.maxActive"" = ""1"" 
);
{code}

4. Query Hive table, fails with NPE.
{code}
> select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL;
INFO  : Compiling command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc): select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:testhivejdbcstorage_hive_tbl.id, type:int, comment:null), FieldSchema(name:testhivejdbcstorage_hive_tbl.fname, type:varchar(20), comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc); Time taken: 9.914 seconds
INFO  : Executing command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc): select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL
INFO  : Completed executing command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc); Time taken: 0.019 seconds
INFO  : OK
Error: java.io.IOException: java.lang.NullPointerException (state=,code=0)
{code}

Assuming that there are no repercussions, can we convert the column names to lowercase fetched from Database/Query pointing to table in JDBCStorageHandler?
Attaching the patch for the case."
HIVE-23717,In jdbcUrl add config to create External + purge table by default	,"External + purge tables are more backward compatible with the old managed tables.

Applications can use a HS2 URL that sets the session level property for default table type to external-purge tables to be true.
 As part of this we need a notion of a ""session level only"" config parameter(s).

Adding a new jdbc config: *hiveCreateAsExternalLegacy*, a hs2 config *hive.create.as.external.legacy*, when set to true, create external purge table by default.

Adding *MANAGED* keyword. ""create managed table"" will create table based on hive.create.as.acid and hive.craete.as.insert.only values, not impacted by the new config."
HIVE-23435,Full outer join result is missing rows ,"Full Outer join result has missing rows. Appears to be a bug with the full outer join logic. Expected output is receiving when we do a left and right outer join.

Reproducible steps are mentioned below.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

SUPPORT ANALYSIS

Steps to Reproduce:

1. Create a table and insert data:

create table x (z char(5), x int, y int);

insert into x values ('one', 1, 50),
 ('two', 2, 30),
 ('three', 3, 30),
 ('four', 4, 60),
 ('five', 5, 70),
 ('six', 6, 80);

2. Try full outer with the below command. The result is incomplete, it is missing the row:

NULL NULL NULL three 3 30.0
 Full Outer Join:

select x1.`z`, x1.`x`, x1.`y`, x2.`z`,
 x2.`x`, x2.`y`
 from `x` x1 full outer join
 `x` x2 on (x1.`x` > 3) and (x2.`x` < 4) and (x1.`x` =
 x2.`x`);

Result:

----------------------------------+

x1.z x1.x x1.y x2.z x2.x x2.y
 ----------------------------------+

one 1 50 NULL NULL NULL
 NULL NULL NULL one 1 50
 two 2 30 NULL NULL NULL
 NULL NULL NULL two 2 30
 three 3 30 NULL NULL NULL
 four 4 60 NULL NULL NULL
 NULL NULL NULL four 4 60
 five 5 70 NULL NULL NULL
 NULL NULL NULL five 5 70
 six 6 80 NULL NULL NULL
 NULL NULL NULL six 6 80
 ----------------------------------+

3. Expected output is coming when we use left/right join + union:

select x1.`z`, x1.`x`, x1.`y`, x2.`z`,
 x2.`x`, x2.`y`
 from `x` x1 left outer join
 `x` x2 on (x1.`x` > 3) and (x2.`x` < 4) and (x1.`x` =
 x2.`x`)
 union
 select x1.`z`, x1.`x`, x1.`y`, x2.`z`,
 x2.`x`, x2.`y`
 from `x` x1 right outer join
 `x` x2 on (x1.`x` > 3) and (x2.`x` < 4) and (x1.`x` =
 x2.`x`);

Result:

------------------------------------+

z x y _col3 _col4 _col5
 ------------------------------------+

NULL NULL NULL five 5 70
 NULL NULL NULL four 4 60
 NULL NULL NULL one 1 50
 four 4 60 NULL NULL NULL
 one 1 50 NULL NULL NULL
 six 6 80 NULL NULL NULL
 three 3 30 NULL NULL NULL
 two 2 30 NULL NULL NULL
 NULL NULL NULL six 6 80
 NULL NULL NULL three 3 30
 NULL NULL NULL two 2 30
 five 5 70 NULL NULL NULL
 ------------------------------------+

 "
HIVE-23347,MSCK REPAIR cannot discover partitions with upper case directory names.,"For the following scenario, we expect MSCK REPAIR to discover partitions but it couldn't.
1. Have partitioned data path as follows.
hdfs://mycluster/datapath/t1/Year=2020/Month=03/Day=10
hdfs://mycluster/datapath/t1/Year=2020/Month=03/Day=11
2. create external table t1 (key int, value string) partitioned by (Year int, Month int, Day int) stored as orc location hdfs://mycluster/datapath/t1'';
3. msck repair table t1;
4. show partitions t1; --> Returns zero partitions
5. select * from t1; --> Returns empty data.

When the partition directory names are changed to lower case, this works fine.
hdfs://mycluster/datapath/t1/year=2020/month=03/day=10
hdfs://mycluster/datapath/t1/year=2020/month=03/day=11"
HIVE-23339,SBA does not check permissions for DB location specified in Create or Alter database query,"With doAs=true and StorageBasedAuthorization provider, create database with specific location succeeds even if user doesn't have access to that path.

 
{code:java}
  hadoop fs -ls -d /tmp/cannot_write
 drwx------ - hive hadoop 0 2020-04-01 22:53 /tmp/cannot_write

create a database under /tmp/cannot_write. We would expect it to fail, but is actually created successfully with ""hive"" as the owner:

rtrivedi@bdp01:~> beeline -e ""create database rtrivedi_1 location '/tmp/cannot_write/rtrivedi_1'""
 INFO : OK
 No rows affected (0.116 seconds)
hive@hpchdd2e:~> hadoop fs -ls /tmp/cannot_write
 Found 1 items
 drwx------ - hive hadoop 0 2020-04-01 23:05 /tmp/cannot_write/rtrivedi_1
{code}
 "
HIVE-23265,Duplicate rowsets are returned with Limit and Offset set,"We have a query which produces duplicate results even when there is no duplicate records in underlying tables.

Sample Query
{code:java}
select * from orderdatatest_ext order by col1 limit 1000,50
{code}

The problem appears when order by clause is used with col1 having non-unique rows. Apparently the duplicates are being produced during reducer phase of the query.
set hive.vectorized.execution.reduce.enabled=false does not cause the problem.

Data in table is as follows.
{code:java}
1,1
1,2
1,3
.
.
1,1500
{code}

Results with hive.vectorized.execution.reduce.enabled=true

{code:java}
+-------------------------+-------------------------+
| orderdatatest_ext.col1  | orderdatatest_ext.col2  |
+-------------------------+-------------------------+
| 1                       | 1001                    |
| 1                       | 1002                    |
| 1                       | 1003                    |
| 1                       | 1004                    |
| 1                       | 1005                    |
| 1                       | 1006                    |
| 1                       | 1007                    |
| 1                       | 1008                    |
| 1                       | 1009                    |
| 1                       | 1010                    |
| 1                       | 1011                    |
| 1                       | 1012                    |
| 1                       | 1013                    |
| 1                       | 1014                    |
| 1                       | 1015                    |
| 1                       | 1016                    |
| 1                       | 1017                    |
| 1                       | 1018                    |
| 1                       | 1019                    |
| 1                       | 1020                    |
| 1                       | 1021                    |
| 1                       | 1022                    |
| 1                       | 1023                    |
| 1                       | 1024                    |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
| 1                       | 1                       |
+-------------------------+-------------------------+
{code}

Results with hive.vectorized.execution.reduce.enabled=false

{code:java}
+-------------------------+-------------------------+
| orderdatatest_ext.col1  | orderdatatest_ext.col2  |
+-------------------------+-------------------------+
| 1                       | 1001                    |
| 1                       | 1002                    |
| 1                       | 1003                    |
| 1                       | 1004                    |
| 1                       | 1005                    |
| 1                       | 1006                    |
| 1                       | 1007                    |
| 1                       | 1008                    |
| 1                       | 1009                    |
| 1                       | 1010                    |
| 1                       | 1011                    |
| 1                       | 1012                    |
| 1                       | 1013                    |
| 1                       | 1014                    |
| 1                       | 1015                    |
| 1                       | 1016                    |
| 1                       | 1017                    |
| 1                       | 1018                    |
| 1                       | 1019                    |
| 1                       | 1020                    |
| 1                       | 1021                    |
| 1                       | 1022                    |
| 1                       | 1023                    |
| 1                       | 1024                    |
| 1                       | 1025                    |
| 1                       | 1026                    |
| 1                       | 1027                    |
| 1                       | 1028                    |
| 1                       | 1029                    |
| 1                       | 1030                    |
| 1                       | 1031                    |
| 1                       | 1032                    |
| 1                       | 1033                    |
| 1                       | 1034                    |
| 1                       | 1035                    |
| 1                       | 1036                    |
| 1                       | 1037                    |
| 1                       | 1038                    |
| 1                       | 1039                    |
| 1                       | 1040                    |
| 1                       | 1041                    |
| 1                       | 1042                    |
| 1                       | 1043                    |
| 1                       | 1044                    |
| 1                       | 1045                    |
| 1                       | 1046                    |
| 1                       | 1047                    |
| 1                       | 1048                    |
| 1                       | 1049                    |
| 1                       | 1050                    |
+-------------------------+-------------------------+
{code}

Table DDL
{code:java}
CREATE EXTERNAL TABLE  orderdatatest_ext (col1 int, col2 int) stored as orc
{code}

Attached sample ORC file.



Problem appears to be with VectorLimitOperator.
{code}
2020-04-20 15:35:49,693 [INFO] [TezChild] |vector.VectorSelectOperator|: Initializing operator SEL[6]
2020-04-20 15:35:49,747 [INFO] [TezChild] |vector.VectorSelectOperator|: RECORDS_OUT_INTERMEDIATE_Map_1:0, RECORDS_OUT_OPERATOR_SEL_6:1500,
2020-04-20 15:35:50,142 [INFO] [TezChild] |vector.VectorSelectOperator|: Initializing operator SEL[8]
2020-04-20 15:35:50,303 [INFO] [TezChild] |vector.VectorSelectOperator|: RECORDS_OUT_OPERATOR_SEL_8:1050, RECORDS_OUT_INTERMEDIATE_Reducer_2:0,


2020-04-20 15:35:50,142 [INFO] [TezChild] |vector.VectorLimitOperator|: Initializing operator LIM[9]
2020-04-20 15:35:50,303 [INFO] [TezChild] |vector.VectorLimitOperator|: RECORDS_OUT_INTERMEDIATE_Reducer_2:0, RECORDS_OUT_OPERATOR_LIM_9:1050,
{code}"
HIVE-23230,"""get_splits"" UDF ignores limit clause while creating splits.","Issue: Running the query {noformat}select * from <table> limit n{noformat} from spark via hive warehouse connector may return more rows than ""n"".

This happens because ""get_splits"" udf creates splits ignoring the limit constraint. These splits when submitted to multiple llap daemons will return ""n"" rows each.


How to reproduce: Needs spark-shell, hive-warehouse-connector and hive on llap with more that 1 llap daemons running.

run below commands via beeline to create and populate the table
 
{noformat}
create table test (id int);
insert into table test values (1);
insert into table test values (2);
insert into table test values (3);
insert into table test values (4);
insert into table test values (5);
insert into table test values (6);
insert into table test values (7);
delete from test where id = 7;{noformat}

now running below query via spark-shell

{noformat}
import com.hortonworks.hwc.HiveWarehouseSession 
val hive = HiveWarehouseSession.session(spark).build() 
hive.executeQuery(""select * from test limit 1"").show()
{noformat}

will return more than 1 rows."
HIVE-23178,Add Tez Total Order Partitioner,
HIVE-23058,Compaction task reattempt fails with FileAlreadyExistsException,"Issue occurs when compaction attempt is relaunched after first task attempt failure due to preemption by Scheduler or any other reason.

Since _tmp directory was created by first attempt and was left uncleaned after task attempt failure. Second attempt of the the task fails with ""FileAlreadyExistsException"" exception.

Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /warehouse/tablespace/managed/hive/default.db/compaction_test/_tmp_3670bbef-ba7a-4c10-918d-9a2ee17cbd22/base_0000186/bucket_00005 for client 10.xx.xx.xxx already exists"
HIVE-23044,Make sure Cleaner doesn't delete delta directories for running queries,
HIVE-23033,MSSQL metastore schema init script doesn't initialize NOTIFICATION_SEQUENCE,"* The inital value for this table in the schema scripts was removed in HIVE-17566: https://github.com/apache/hive/commit/32b7abac961ca3879d23b074357f211fc7c49131#diff-3d1a4bae0d5d53c8e4ea79951ebf5eceL598
* This was fixed in a number of scripts in HIVE-18781, but not for mssql: https://github.com/apache/hive/commit/59483bca262880d3e7ef1b873d3c21176e9294cb#diff-4f43efd5a45cc362cb138287d90dbf82
* This is as is since then

When using the schematool, the table gets initialized by other means.

This could be backported to all active branches for 3.x as well."
HIVE-22995,Add support for location for managed tables on database,"I have attached the initial spec to this jira.
Default location for database would be the external table base directory. Managed location can be optionally specified.

{code}
CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
[MANAGEDLOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];

ALTER (DATABASE|SCHEMA) database_name SET 
MANAGEDLOCATION
 hdfs_path;

{code}"
HIVE-22769,Incorrect query results and query failure during split generation for compressed text files,"Hive Query produces incorrect results when data is in text format and compressed and for certain data the query fails during split generation.

This behavior is seen when skip.header.line.count and skip.footer.line.count are set for table.

Case 1: Select count/aggregate query produces Incorrect row counts/displays all rows (when hive.fetch.task.conversion=none)

Steps to reproduce:

1. Create table as below
{code}
CREATE EXTERNAL TABLE `testcase1`(id int, name string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' LOCATION '/user/hive/testcase1' TBLPROPERTIES (""skip.header.line.count""=""1"", ""skip.footer.line.count""=""1"");
{code}
2. Upload attached testcase1.csv.bz2 file to /user/hive/testcase1
3. Run count(*) on table.

{code}
> select * from testcase1;
INFO  : Compiling command(queryId=hive_20200124053854_454b03c1-d4c5-4dba-a2c2-91c09f4b670f): select * from testcase1
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:testcase1.id, type:string, comment:null), FieldSchema(name:testcase1.name, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200124053854_454b03c1-d4c5-4dba-a2c2-91c09f4b670f); Time taken: 0.07 seconds
INFO  : Executing command(queryId=hive_20200124053854_454b03c1-d4c5-4dba-a2c2-91c09f4b670f): select * from testcase1
INFO  : Completed executing command(queryId=hive_20200124053854_454b03c1-d4c5-4dba-a2c2-91c09f4b670f); Time taken: 0.007 seconds
INFO  : OK
+---------------+-----------------+
| testcase1.id  | testcase1.name  |
+---------------+-----------------+
| 2             | 2019-12-31      |
+---------------+-----------------+
1 row selected (0.111 seconds)


> select count(*) from testcase1
INFO  : Compiling command(queryId=hive_20200124053645_a7d699b7-c7e1-4d92-8d99-666b0a010ba7): select count(*) from testcase1
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200124053645_a7d699b7-c7e1-4d92-8d99-666b0a010ba7); Time taken: 0.073 seconds
INFO  : Executing command(queryId=hive_20200124053645_a7d699b7-c7e1-4d92-8d99-666b0a010ba7): select count(*) from testcase1
INFO  : Query ID = hive_20200124053645_a7d699b7-c7e1-4d92-8d99-666b0a010ba7
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Subscribed to counters: [] for queryId: hive_20200124053645_a7d699b7-c7e1-4d92-8d99-666b0a010ba7
INFO  : Session is already open
INFO  : Dag name: select count(*) from testcase1 (Stage-1)
INFO  : Status: Running (Executing on YARN cluster with App id application_1579811438512_0046)
.
.
.

INFO  : Completed executing command(queryId=hive_20200124053645_a7d699b7-c7e1-4d92-8d99-666b0a010ba7); Time taken: 4.228 seconds
INFO  : OK
+------+
| _c0  |
+------+
| 3    |
+------+
1 row selected (4.335 seconds)
{code}

Case 2: Select count/aggregate query fails with java.lang.ClassCastException: java.io.PushbackInputStream cannot be cast to org.apache.hadoop.fs.Seekable

The issue is only seen when there is a space in a field (eg:- ""3,2019-12-31 01"" second column has a space)

Steps to reproduce:

1. Create table as below
{code}
CREATE EXTERNAL TABLE `testcase2`(id int, name string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' LOCATION '/user/hive/testcase2' TBLPROPERTIES (""skip.header.line.count""=""1"", ""skip.footer.line.count""=""1"");
{code}
2. Upload attached testcase2.csv.bz2 file to /user/hive/testcase2
3. Run count(*) on table.

{code}
0: > select * from testcase2;
INFO  : Compiling command(queryId=hive_20200124053159_5d8ce56a-183d-4359-a147-bd470d82e134): select * from testcase2
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:testcase2.id, type:string, comment:null), FieldSchema(name:testcase2.name, type:string, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200124053159_5d8ce56a-183d-4359-a147-bd470d82e134); Time taken: 0.075 seconds
INFO  : Executing command(queryId=hive_20200124053159_5d8ce56a-183d-4359-a147-bd470d82e134): select * from testcase2
INFO  : Completed executing command(queryId=hive_20200124053159_5d8ce56a-183d-4359-a147-bd470d82e134); Time taken: 0.01 seconds
INFO  : OK
+---------------+-----------------+
| testcase2.id  | testcase2.name  |
+---------------+-----------------+
| 2             | 2019-12-31 01   |
+---------------+-----------------+
1 row selected (0.119 seconds)


> select count(*) from testcase2;
INFO  : Compiling command(queryId=hive_20200124053542_a7d6820e-c3df-4d70-bc00-f3916441da88): select count(*) from testcase2
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200124053542_a7d6820e-c3df-4d70-bc00-f3916441da88); Time taken: 0.079 seconds
INFO  : Executing command(queryId=hive_20200124053542_a7d6820e-c3df-4d70-bc00-f3916441da88): select count(*) from testcase2
INFO  : Query ID = hive_20200124053542_a7d6820e-c3df-4d70-bc00-f3916441da88
INFO  : Total jobs = 1
INFO  : Launching Job 1 out of 1
INFO  : Starting task [Stage-1:MAPRED] in serial mode
INFO  : Subscribed to counters: [] for queryId: hive_20200124053542_a7d6820e-c3df-4d70-bc00-f3916441da88
INFO  : Session is already open
INFO  : Dag name: select count(*) from testcase2 (Stage-1)
ERROR : Status: Failed
ERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1579811438512_0046_2_00, diagnostics=[Vertex vertex_1579811438512_0046_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: testcase2 initializer failed, vertex=vertex_1579811438512_0046_2_00 [Map 1], java.lang.ClassCastException: java.io.PushbackInputStream cannot be cast to org.apache.hadoop.fs.Seekable
	at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:75)
	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.getCachedStartIndex(SkippingTextInputFormat.java:128)
	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplitInternal(SkippingTextInputFormat.java:74)
	at org.apache.hadoop.hive.ql.io.SkippingTextInputFormat.makeSplit(SkippingTextInputFormat.java:66)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:379)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:532)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:789)
	at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:243)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)
	at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:69)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
]
{code}

The above behavior appears after applying HIVE-21924"
HIVE-22758,Create database with permission error when doas set to true,"With doAs set to true, running create database on external location fails with permission denied for write access on the directory for hive user (User HMS is running as).

Steps to reproduce the issue:
1. Turn on, Hive run as end-user to true.
2. Connect to hive as some user other than admin, eg:- chiran
3. Create a database with external location
{code}
create database externaldbexample location '/user/chiran/externaldbexample'
{code}

The above statement fails as write access is not available to hive service user on HDFS as below.

{code}
> create database externaldbexample location '/user/chiran/externaldbexample';
INFO  : Compiling command(queryId=hive_20200122043626_5c95e1fd-ce00-45fd-b58d-54f5e579f87d): create database externaldbexample location '/user/chiran/externaldbexample'
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20200122043626_5c95e1fd-ce00-45fd-b58d-54f5e579f87d); Time taken: 1.377 seconds
INFO  : Executing command(queryId=hive_20200122043626_5c95e1fd-ce00-45fd-b58d-54f5e579f87d): create database externaldbexample location '/user/chiran/externaldbexample'
INFO  : Starting task [Stage-0:DDL] in serial mode
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.reflect.UndeclaredThrowableException)
INFO  : Completed executing command(queryId=hive_20200122043626_5c95e1fd-ce00-45fd-b58d-54f5e579f87d); Time taken: 0.238 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.reflect.UndeclaredThrowableException) (state=08S01,code=1)
{code}

From Hive Metastore service log, below is seen.

{code}
2020-01-22T04:36:27,870 WARN  [pool-6-thread-6]: metastore.ObjectStore (ObjectStore.java:getDatabase(1010)) - Failed to get database hive.externaldbexample, returning NoSuchObjectExcept
ion
2020-01-22T04:36:27,898 INFO  [pool-6-thread-6]: metastore.HiveMetaStore (HiveMetaStore.java:run(1339)) - Creating database path in managed directory hdfs://c470-node2.squadron.support.
hortonworks.com:8020/user/chiran/externaldbexample
2020-01-22T04:36:27,903 INFO  [pool-6-thread-6]: utils.FileUtils (FileUtils.java:mkdir(170)) - Creating directory if it doesn't exist: hdfs://namenodeaddress:8020/user/chiran/externaldbexample
2020-01-22T04:36:27,932 ERROR [pool-6-thread-6]: utils.MetaStoreUtils (MetaStoreUtils.java:logAndThrowMetaException(169)) - Got exception: org.apache.hadoop.security.AccessControlException Permission denied: user=hive, access=WRITE, inode=""/user/chiran"":chiran:chiran:drwxr-xr-x
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:255)
        at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1859)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1843)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1802)
        at org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:59)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3150)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1126)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:707)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)
{code}


The behavior looks to be a regression of HIVE-20001.
Can we add a check to see if the external path specified by a user, then create DB directory as end-user instead of hive service user?

Attaching patch, which was tested locally on Hive 3.1"
HIVE-22687,Query hangs indefinitely if LLAP daemon registers after the query is submitted,"If a query is submitted and no LLAP daemon is running, it waits for 1 minute and times out with error {{SERVICE_UNAVAILABLE}}.
While waiting, if a new LLAP Daemon starts, then the timeout is cancelled, and the tasks do not get scheduled as well. As a result, the query hangs indefinitely.
This is due to the race condition where LLAP Daemon first registers the LLAP instance at {{.../workers/worker-0000}}, and afterwards registers {{.../workers/slot-0000}}. In the gap between two, Tez AM gets notified of worker zk node and while processing it checks if slot zk node is present, if not it rejects the LLAP Daemon. Error in Tez AM is:
{code:java}
[INFO] [LlapScheduler] |impl.LlapZookeeperRegistryImpl|: Unknown slot for 8ebfdc45-0382-4757-9416-52898885af90{code}"
HIVE-22566,Drop table involved in materialized view leaves the table in inconsistent state,"If you try dropping a table which is part of the definition of a created materialized view, the table is not dropped, which is the desired state as it is part of the materialized view.

However, there was a ""drop"" call to the table, so it tried to drop it but did not succeed, leaving it in an inconsistent state.

 

Repro:

-------

1) Create tables:

 
{code:java}
CREATE TABLE emps (  empid INT,  deptno INT,  name VARCHAR(256),  salary FLOAT,  hire_date TIMESTAMP)STORED AS ORC TBLPROPERTIES ('transactional'='true'); 

CREATE TABLE depts (  deptno INT,  deptname VARCHAR(256),  locationid INT)STORED AS ORC TBLPROPERTIES ('transactional'='true');
{code}
 

2) Create the VM:

 
{code:java}
CREATE MATERIALIZED VIEW mv1 AS SELECT empid, deptname, hire_date FROM emps JOIN depts  ON (emps.deptno = depts.deptno) WHERE hire_date >= '2016-01-01';
{code}
 

3) Following is in backend database at this point:

 
{code:java}
mysql> select TBL_ID, DB_ID, SD_ID, TBL_NAME, TBL_TYPE from TBLS where DB_ID=16;
+--------+-------+-------+----------+-------------------+
| TBL_ID | DB_ID | SD_ID | TBL_NAME | TBL_TYPE          |
+--------+-------+-------+----------+-------------------+
|     81 |    16 |    81 | emps     | MANAGED_TABLE     |
|     83 |    16 |    83 | depts    | MANAGED_TABLE     |
|     84 |    16 |    84 | mv1      | MATERIALIZED_VIEW |
+--------+-------+-------+----------+-------------------+
3 rows in set (0.00 sec)
{code}
 

4) Let's drop the 'emps' table:

 
{code:java}
0: jdbc:hive2://c1122-node2.squadron.support.> drop table emps;
INFO  : Compiling command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b): drop table emps
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b); Time taken: 0.05 seconds
INFO  : Executing command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b): drop table emps
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=hive_20191202200025_c13079d0-8695-4485-8a18-14804b8b014b); Time taken: 10.281 seconds
INFO  : OK
No rows affected (16.949 seconds)
{code}
No issue displayed

 

5) List tables:

 
{code:java}
0: jdbc:hive2://c1122-node2.squadron.support.> show tables;
INFO  : Compiling command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413): show tables
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tab_name, type:string, comment:from deserializer)], properties:null)
INFO  : Completed compiling command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413); Time taken: 0.041 seconds
INFO  : Executing command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413): show tables
INFO  : Starting task [Stage-0:DDL] in serial mode
INFO  : Completed executing command(queryId=hive_20191202200125_ca12565b-1d4d-4433-a602-ecf685863413); Time taken: 0.016 seconds
INFO  : OK
+-----------+
| tab_name  |
+-----------+
| depts     |
| emps      |
+-----------+
2 rows selected (0.08 seconds)
{code}
 

6) Now, from the backend-db point of view:

 
{code:java}
mysql> select TBL_ID, DB_ID, SD_ID, TBL_NAME, TBL_TYPE from TBLS where DB_ID=16;
+--------+-------+-------+----------+-------------------+
| TBL_ID | DB_ID | SD_ID | TBL_NAME | TBL_TYPE          |
+--------+-------+-------+----------+-------------------+
|     81 |    16 |  NULL | emps     | MANAGED_TABLE     |
|     83 |    16 |    83 | depts    | MANAGED_TABLE     |
|     84 |    16 |    84 | mv1      | MATERIALIZED_VIEW |
+--------+-------+-------+----------+-------------------+
3 rows in set (0.00 sec)
{code}
The table is left with NULL in SD_ID, making it not available.

 

7) From Metastore.log

 
{code:java}
2019-12-02T20:00:25,545 INFO  [pool-6-thread-195]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(907)) - 196: source:172.25.34.150 drop_table : tbl=hive.mvs.emps
2019-12-02T20:00:25,545 INFO  [pool-6-thread-195]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(349)) - ugi=hive	ip=172.25.34.150	cmd=source:172.25.34.150 drop_table : tbl=hive.mvs.emps	
2019-12-02T20:00:25,580 INFO  [pool-6-thread-195]: metastore.ObjectStore$RetryingExecutor (ObjectStore.java:run(9966)) - Attempting to acquire the DB log notification lock: 0 out of 10 retries
javax.jdo.JDODataStoreException: Error executing SQL query ""select ""NEXT_EVENT_ID"" from ""NOTIFICATION_SEQUENCE"" for update"".
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:391) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:216) ~[datanucleus-api-jdo-4.2.4.jar:?]
	at org.apache.hadoop.hive.metastore.ObjectStore.lambda$lockForUpdate$0(ObjectStore.java:9936) ~[hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:9963) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:9938) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:10002) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at com.sun.proxy.$Proxy28.addNotificationEvent(Unknown Source) [?:?]
	at org.apache.hive.hcatalog.listener.DbNotificationListener.process(DbNotificationListener.java:968) [hive-hcatalog-server-extensions-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropTable(DbNotificationListener.java:198) [hive-hcatalog-server-extensions-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier$19.notify(MetaStoreListenerNotifier.java:99) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:273) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.MetaStoreListenerNotifier.notifyEvent(MetaStoreListenerNotifier.java:335) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:2670) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:2842) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at com.sun.proxy.$Proxy30.drop_table_with_environment_context(Unknown Source) [?:?]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:15533) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_table_with_environment_context.getResult(ThriftHiveMetastore.java:15517) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:111) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_112]
	at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_112]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) [hadoop-common-3.1.1.3.1.0.0-78.jar:?]
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:119) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) [hive-exec-3.1.0.3.1.0.0-78.jar:3.1.0.3.1.0.0-78]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.sql.BatchUpdateException: Cannot delete or update a parent row: a foreign key constraint fails (""hive"".""MV_TABLES_USED"", CONSTRAINT ""MV_TABLES_USED_FK2"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS"" (""TBL_ID""))
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2058) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1471) ~[mysql-connector-java.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:125) ~[HikariCP-2.6.1.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-2.6.1.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:319) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:211) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:633) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 37 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Cannot delete or update a parent row: a foreign key constraint fails (""hive"".""MV_TABLES_USED"", CONSTRAINT ""MV_TABLES_USED_FK2"" FOREIGN KEY (""TBL_ID"") REFERENCES ""TBLS"" (""TBL_ID""))
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_112]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_112]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_112]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_112]
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.Util.getInstance(Util.java:386) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1041) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4187) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4119) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2570) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2731) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2820) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2159) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2462) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2010) ~[mysql-connector-java.jar:?]
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1471) ~[mysql-connector-java.jar:?]
	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:125) ~[HikariCP-2.6.1.jar:?]
	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java) ~[HikariCP-2.6.1.jar:?]
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeBatch(ParamLoggingPreparedStatement.java:366) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.processConnectionStatement(SQLController.java:676) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.SQLController.getStatementForQuery(SQLController.java:319) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:211) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:633) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.store.rdbms.query.SQLQuery.executeWithArray(SQLQuery.java:807) ~[datanucleus-rdbms-4.1.19.jar:?]
	at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?]
	at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?]
	... 37 more

{code}
 

 

8) If you try to query the table:

 
{code:java}
0: jdbc:hive2://c1122-node2.squadron.support.> select * from emps;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table emps. null (state=42000,code=40000)
{code}
 

It fails as expected.

9) If you try to query the MV:
{code:java}
0: jdbc:hive2://c1122-node2.squadron.support.> select * from mv1; INFO : Compiling command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2): select * from mv1 INFO : Semantic Analysis Completed (retrial = false) INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:mv1.empid, type:int, comment:null), FieldSchema(name:mv1.deptname, type:varchar(256), comment:null), FieldSchema(name:mv1.hire_date, type:timestamp, comment:null)], properties:null) INFO : Completed compiling command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2); Time taken: 0.229 seconds INFO : Executing command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2): select * from mv1 INFO : Completed executing command(queryId=hive_20191202200818_91bf194d-8133-4670-b8d5-542ee56b6cc2); Time taken: 0.01 seconds INFO : OK +------------+---------------+----------------+ | mv1.empid | mv1.deptname | mv1.hire_date | +------------+---------------+----------------+ +------------+---------------+----------------+ No rows selected (0.276 seconds)
{code}
It does not fail, as the underlying data has not changed, and the table is still being shown as valid.

 

10) Insert data into ""depts"" table and rebuild the mv.
{code:java}
$ INSERT INTO TABLE depts VALUES (101,'IT',25);
$ INSERT INTO TABLE depts VALUES (102,'Eng',11);

0: jdbc:hive2://c1122-node2.squadron.support.> ALTER MATERIALIZED VIEW mvs.mv1 REBUILD;
Error: Error while compiling statement: FAILED: SemanticException Unable to fetch table emps. null (state=42000,code=40000)
{code}
This fails as expected.

 "
HIVE-22527,Hive on Tez : Job of merging small files will be submitted into another queue (default queue),"Hive on Tez. We enable small file merge configuration with set *hive.merge.tezfiles=true*. So , There will be another job launched for merging files after sql job. However, the merge file job is submitted into another yarn queue, not the queue of current beeline client session. It seems that the merging files job start a new tez session with new conf which is different the current session conf, leading to the merging file job goes into default queue.

 

Attachment *hive logs.png* shows that current session queue is *root.bdoc.production* ( String queueName = session.getQueueName();) incoming queue name is *null* ( String confQueueName = conf.get(TezConfiguration.TEZ_QUEUE_NAME);). In fact, we log in to the same beeline client with *set tez.queue.name=* *root.bdoc.production,* and  all  jobs should be submitted into the same queue including file merge job.

[https://github.com/apache/hive/blob/bcc7df95824831a8d2f1524e4048dfc23ab98c19/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java#L445]

[https://github.com/apache/hive/blob/bcc7df95824831a8d2f1524e4048dfc23ab98c19/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java#L446]

 

Attachment *explain with merge files.png* shows that ** the stage-4 is individual merge file job which is submitted into another yarn queue（default queue）， not the queue root.bdoc.production."
HIVE-22420,DbTxnManager.stopHeartbeat() should be thread-safe,"When a transactional query is being executed and interrupted via HS2 close operation request, both the background pool thread executing the query and the HttpHandler thread running the close operation logic will eventually call the below method:
{noformat}
Driver.releaseLocksAndCommitOrRollback(commit boolean)
{noformat}
Since this method is invoked several times in both threads, it can happen that the two threads invoke it at the same time, and due to a race condition, the txnId field of the DbTxnManager used by both threads could be set to 0 without actually successfully aborting the transaction.

The root cause is stopHeartbeat() method in DbTxnManager not being thread safe:

When Thread-1 and Thread-2 enter stopHeartbeat() with very little time difference, Thread-1 might successfully cancel the heartbeat task and set the heartbeatTask field to null, while Thread-2 is trying to observe its state. Thread-1 will return to the calling rollbackTxn() method and continue execution there, while Thread-2 wis thrown back to the same method with a NullPointerException. Thread-2 will then set txnId to 0, and Thread-1 is sending this 0 value to HMS. So, the txn will not be aborted, and the locks cannot be released later on either."
HIVE-22400,UDF minute with time returns NULL,"[impadmin@impetus-g031 ~]$ beeline
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://impetus-dsrv11.impetus.co.in:2181,ct-n0066.impetus.co.in:2181,ct-n0092.impetus.co.in:2181/default;principal=hive/_HOST@IMPETUS.CO.IN;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
19/10/24 19:03:42 [main]: INFO jdbc.HiveConnection: Connected to ct-n0092.impetus.co.in:10000
Connected to: Apache Hive (version 3.1.0.3.1.0.0-78)
Driver: Hive JDBC (version 3.1.0.3.1.0.0-78)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.0.3.1.0.0-78 by Apache Hive
0: jdbc:hive2://impetus-dsrv11.impetus.co.in:> select minute('12:58:59');
INFO : Compiling command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395): select minute('12:58:59')
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:int, comment:null)], properties:null)
INFO : Completed compiling command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395); Time taken: 0.427 seconds
INFO : Executing command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395): select minute('12:58:59')
INFO : Completed executing command(queryId=hive_20191024190401_bc517191-bd20-4f5a-b5f5-44f762c2d395); Time taken: 0.003 seconds
INFO : OK
+-------+
| _c0 |
+-------+
| NULL |
+-------+
1 row selected (0.739 seconds)
0: jdbc:hive2://impetus-dsrv11.impetus.co.in:>"
HIVE-22368,Hive JDBC Storage Handler: some mysql data type can not be cast to hive data type,"Mysql data type（date、timestamp、decimal）can not be cast to hive data type（date、timestamp、decimal）。

step to repo（take date type for example）:
{code:java}
//MySQL table:
create table testdate(id date);

//Hive table:
CREATE EXTERNAL TABLE `hive_date`(               
      id date )    
    ROW FORMAT SERDE                                   
      'org.apache.hive.storage.jdbc.JdbcSerDe'         
    STORED BY                                          
      'org.apache.hive.storage.jdbc.JdbcStorageHandler'                       
    TBLPROPERTIES (                                                            
      'hive.sql.database.type'='MYSQL',                                  
      'hive.sql.dbcp.password'='hive',             
      'hive.sql.dbcp.username'='hive',             
      'hive.sql.jdbc.driver'='com.mysql.jdbc.Driver',  
      'hive.sql.jdbc.url'='jdbc:mysql://hadoop/test',  
      'hive.sql.table'='testdate');

//Hive query:
select * from hive_date;

Error: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.sql.Date cannot be cast to org.apache.hadoop.hive.common.type.Date (state=,code=0)

//Error stack trace
Caused by: java.lang.ClassCastException: java.sql.Date cannot be cast to org.apache.hadoop.hive.common.type.Date
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.getPrimitiveJavaObject(JavaDateObjectInspector.java:41) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaDateObjectInspector.getPrimitiveJavaObject(JavaDateObjectInspector.java:27) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:422) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.serde2.SerDeUtils.toThriftPayload(SerDeUtils.java:173) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.serde2.thrift.ThriftFormatter.convert(ThriftFormatter.java:49) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.ListSinkOperator.process(ListSinkOperator.java:94) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:995) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:941) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:928) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:995) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:941) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:125) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:519) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:511) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2706) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:460) ~[hive-service-3.1.0-bc3.0.1.jar:3.1.0-bc3.0.1]
        ... 25 more
{code}"
HIVE-22357,"Schema mismatch between the Hive table definition and the ""hive.sql.query"" Parameter","The problem is that, for certain types of schema mismatch between the Hive table definition and the ""hive.sql.query"" parameter, Hive simply returns no rows rather than throwing an error when queried.

Ideally Hive would check for schema compatibility during table definition and throw an error if there's a problem - the earlier the better. But even if that cannot be made a requirement, I'd definitely expect an error rather than a silent failure (i.e. zero rows returned) at runtime.

I'm attaching investigation of this issue in ""Hive-JDBC schema matching sensitivity.txt""."
HIVE-22325,variable expansion doesn't work in beeline-site.xml,"I have a default jdbc connection string and I want to build on top on it to have customized connections like setting custom queue names.  

{code}
$ cat .beeline/beeline-site.xml
  <configuration  xmlns:xi=""http://www.w3.org/2001/XInclude"">

    <property>
      <name>beeline.hs2.jdbc.url.base</name>
      <value>jdbc:hive2://localhost/</value>
    </property>

    <property>
      <name>beeline.hs2.jdbc.url.myqueue</name>
      <value>${beeline.hs2.jdbc.url.base}?tez.queue.name=myqueue</value>
    </property>
  </configuration>
$ beeline -c myqueue
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/3.1.0.0-78/hadoop/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Error in parsing jdbc url: ${beeline.hs2.jdbc.url.base}?tez.queue.name=myqueue from beeline-site.xml
Beeline version 3.1.0.3.1.0.0-78 by Apache Hive
beeline>
{code}

Relevant code is found in https://github.com/apache/hive/blob/master/beeline/src/java/org/apache/hive/beeline/hs2connection/BeelineSiteParser.java#L94

Entry<T,T>#getValue() skips the variable expansion .  Using Configuration#get(key) would make this work."
HIVE-22208,Column name with reserved keyword is unescaped when query including join on table with mask column is re-written,"Join query  involving table with mask column and  other having reserved keyword as column name fails with SemanticException during parsing re-written query :

Original Query :
{code:java}
select a.`date`, b.nm
from sample_keyword a
join sample_mask b
on b.id = a.id;
{code}
Re-written Query :
  
{code:java}
select a.date, b.nm
from sample_keyword a
join (SELECT `id`, CAST(mask_hash(nm) AS string) AS `nm`, BLOCK__OFFSET__INSIDE__FILE, INPUT__FILE__NAME, ROW__ID FROM `default`.`sample_mask` )`b`
on b.id = a.id;
{code}
Re-written query does not have escape quotes for date column which cause SemanticException while parsing :
{code:java}
org.apache.hadoop.hive.ql.parse.ParseException: line 1:9 cannot recognize input near 'a' '.' 'date' in selection target                                            

at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.rewriteASTWithMaskAndFilter( SemanticAnalyzer.java:12084)                                                            at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal( SemanticAnalyzer.java:12298)        
at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal( CalcitePlanner.java:360)        
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze( BaseSemanticAnalyzer.java:289)        
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)        
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1869)
{code}"
HIVE-22207,"Tez: SplitGenerator throws NumberFormatException when ""dfs.block.size"" on cluster is ""128m""","When the value of ""dfs.block.size"" on the cluster is ""128m"" in place of actual bytes as Long, then Tez job fails with NumberFormatException:
{code:java}
java.lang.NumberFormatException: For input string: ""128m"" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:589) at java.lang.Long.parseLong(Long.java:631) at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1539) at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:194) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)
{code}"
HIVE-22178,Parquet FilterPredicate throws CastException after SchemaEvolution.,"Below are the repro steps.
{code:java}
create table parq_test(age int, name string) stored as parquet;
insert into parq_test values(1, 'aaaa');
alter table parq_test change age age string;
insert into parq_test values('b', 'bbbb');
select * from parq_test where age='b';{code}
Exception thrown after changing column datatype is below
{code:java}
Caused by: java.lang.IllegalArgumentException: FilterPredicate column: age's declared type (org.apache.parquet.io.api.Binary) does not match the schema found in file metadata. Column age is of type: INT32
Valid types for this column are: [class java.lang.Integer]
 at org.apache.parquet.filter2.predicate.ValidTypeMap.assertTypeValid(ValidTypeMap.java:126)
 at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:181)
 at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:151)
 at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:85)
 at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:58)
 at org.apache.parquet.filter2.predicate.Operators$Eq.accept(Operators.java:181)
 at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:63)
 at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:92)
 at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:43)
 at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:137)
 at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:64)
 at org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.getSplit(ParquetRecordReaderBase.java:111)
 at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.<init>(VectorizedParquetRecordReader.java:147)
 ... 31 more{code}"
HIVE-22170,from_unixtime and unix_timestamp should use user session time zone,"According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue."
HIVE-22134,Hive 3.1 driver includes org.glassfish.jersey.* which can interfer with an application,"An application that uses JAX-RS 1.1 can be broken by the Hive 3.1 standalone JAR.

For example, an application is running in IBM Websphere Liberty  Profile (WLP) which detects the classes packaged in the Apache Hive standalone JAR.  This results in WLP assuming that the application is providing it's implementation and should not use the default in WLP. 

Can the Apache Hive JDBC team confirm why these classes are in the JAR.
Can the Apache Hive JDBC team schedule to remove them if they are not mandatory.
Can the Apache Hive JDBC team confirm which individual JAR files can be copied instead of the uber-standalone JAR which would not include these conflicting classes.

This is the class which triggers the problem if all of the jersey stuff is deleted the issue will go away


org.glassfish.jersey.server.internal.RuntimeDelegateImpl
{{org.glassfish.jersey.*}}"
HIVE-22129,Hive 3.1 standalone JAR includes Microsoft SQL Server JDBC driver,"The Apache Hive 3.1 JDBC driver, specifically the standalone JAR has included the Microsoft SQL Server JDBC driver. A mutual customer may be trying to use their own copy of a Microsoft JDBC driver and the Apache Hive driver. Potentially the JRE will pick up the code in the Apache Hive driver.

What is it included at all and can it be removed going forward?

!ApacheHive.png!"
HIVE-22121,Turning on hive.tez.bucket.pruning produce wrong results,"*Reproducer*

{code:sql}
set hive.query.results.cache.enabled=false;
set hive.optimize.ppd.storage=true;
set hive.optimize.index.filter=true;

set hive.tez.bucket.pruning=true; 


CREATE TABLE `test_table`(                 
   `col_1` int,                                     
   `col_2` string,                                  
   `col_3` string)                                  
 CLUSTERED BY (                                     
   col_1)                                           
 INTO 4 BUCKETS;                                     

insert into test_table values(1, 'one', 'ONE'), (2, 'two', 'TWO'), (3,'three','THREE'),(4,'four','FOUR');

select * from test_table;

explain select col_1, col_2, col_3 from test_table where col_1 <> 2 order by col_2;
select col_1, col_2, col_3 from test_table where col_1 <> 2 order by col_2;

{code}

Above sql query produce zero rows."
HIVE-22114,insert query for partitioned insert only table failing when all buckets are empty,"Following insert query fails when all buckets are empty

{code:sql}
create table src_emptybucket_partitioned_1 (name string, age int, gpa decimal(3,2))
                               partitioned by(year int)
                               clustered by (age)
                               sorted by (age)
                               into 100 buckets
                               stored as orc tblproperties (""transactional""=""true"", ""transactional_properties""=""insert_only"");




create table src1(name string, age int, gpa decimal(3,2));
insert into src1 values(""name"", 56, 4);


insert into table src_emptybucket_partitioned_1
                               partition(year=2015)
                               select * from src1 limit 0;
{code}

Error:

{noformat}
ERROR : Job Commit failed with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(java.io.FileNotFoundException: No such file or directory: s3a:/<s3-location>/warehouse/tablespace/managed/hive/src_emptybucket_partitioned/year=2015)'
# org.apache.hadoop.hive.ql.metadata.HiveException: java.io.FileNotFoundException: No such file or directory: s3a://<s3-location>/warehouse/tablespace/managed/hive/src_emptybucket_partitioned/year=2015
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1403)
	at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:798)
	at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.close(TezTask.java:590)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:327)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:212)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:103)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2335)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2002)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1674)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1372)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1366)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226)
	at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:324)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:342)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: No such file or directory: s3a://<s3-location>/warehouse/tablespace/managed/hive/src_emptybucket_partitioned/year=2015
	at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2805)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2694)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2587)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerListStatus(S3AFileSystem.java:2388)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listStatus$10(S3AFileSystem.java:2367)
	at org.apache.hadoop.fs.s3a.Invoker.once(Invoker.java:109)
	at org.apache.hadoop.fs.s3a.S3AFileSystem.listStatus(S3AFileSystem.java:2367)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1880)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1922)
	at org.apache.hadoop.hive.ql.exec.Utilities.getMmDirectoryCandidates(Utilities.java:4185)
	at org.apache.hadoop.hive.ql.exec.Utilities.handleMmTableFinalPath(Utilities.java:4386)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1397)
	... 26 more

ERROR : FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.tez.TezTask
{noformat}
"
HIVE-22089,Upgrade jackson to 2.9.9,
HIVE-22056,"Beeline started with -f <script-file>, exit code is 0 even if unable to connect to HS2","When Beeline in hive 3 is started with -f <script-file> argument, even if it is not able to connect to HS2 server, it exits with success code i.e. 0. Looks like this bug got introduced with the fix for HIVE-19744
 In previous versions it used to exit with code 2, fixing by exiting with same code."
HIVE-21965,Implement parallel processing in HiveStrictManagedMigration,"This process, kicked off from Ambari can take many days for systems with 1000's of tables. The process needs to support parallel execution as it iterates through the Databases and Tables."
HIVE-21948,Implement parallel processing in Pre Upgrade Tool,"Pre Upgrade Tool scans for all databases and tables in the warehouse sequentially which can be very slow in case of lots of tables.

Example: It took the process 8-10 hours to complete on ~500k tables."
HIVE-21941,Use checkstyle ruleset in Pre Upgrade Tool project ,The project upgrade-acid/pre-upgrade does not uses the same checkstyle ruleset as hive root project
HIVE-21938,Add database and table filter options to PreUpgradeTool,"By default pre upgrade tool scans all databases and tables in the warehouse. 
Add database and table filter options to run the tool for a specific subset of databases and tables only."
HIVE-21917,COMPLETED_TXN_COMPONENTS table is never cleaned up unless Compactor runs,"The Initiator thread in the metastore repeatedly loops over entries in the COMPLETED_TXN_COMPONENTS table to determine which partitions / tables might need to be compacted. However, entries are never removed from this table except by a completed Compactor run.

In a cluster where most tables / partitions are write-once read-many, this results in stale entries in this table never being cleaned up. In a small test cluster, we have observed approximately 45k entries in this table (virtually equal to the number of partitions in the cluster) while < 100 of these tables have delta files at all. Since most of the tables will never get enough writes to trigger a compaction (and in fact have only ever been written to once), the initiator thread keeps trying to evaluate them on every loop.

On this test cluster, it takes approximately 10 minutes to loop through all the entries and results in severe performance degradation on metastore operations. With the default run timing of 5 minutes, the initiator basically never stops running.

On a production cluster with 2M partitions, this would be a non-starter.

The initiator thread should proactively remove entries from COMPLETED_TXN_COMPONENTS when it determines that a compaction is not needed, so that they are not evaluated again on the next loop.

 "
HIVE-21902,HiveServer2 UI: jetty response header needs X-Frame-Options,"there are some vulnerability are reported for hiveserver2 ui


X-Frame-Options or Content-Security-Policy: frame-ancestors HTTP Headers missing on port 10002. 
{code}
GET / HTTP/1.1 
Host: HOSTNAME:10002 
Connection: Keep-Alive 



X-XSS-Protection HTTP Header missing on port 10002. 
X-Content-Type-Options HTTP Header missing on port 10002. 
{code}
after the proposed changes

{code}
HTTP/1.1 200 OK
Date: Thu, 20 Jun 2019 05:29:59 GMT
Content-Type: text/html;charset=utf-8
X-Content-Type-Options: nosniff
X-FRAME-OPTIONS: SAMEORIGIN
X-XSS-Protection: 1; mode=block
Set-Cookie: JSESSIONID=15kscuow9cmy7qms6dzaxllqt;Path=/
Expires: Thu, 01 Jan 1970 00:00:00 GMT
Content-Length: 3824
Server: Jetty(9.3.25.v20180904)
{code}"
HIVE-21831,Stats should be reset correctly during load of a partitioned ACID table,"While running something similar to the following example, I noticed that an import of a partitioned ACID table using the ORC format fails to provide table statistics:
{code:java}
set hive.stats.autogather=true;
set hive.stats.column.autogather=true;
set hive.fetch.task.conversion=none;


set hive.support.concurrency=true;
set hive.default.fileformat.managed=ORC;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create transactional table int_src (foo int, bar int);
insert into int_src select 1,1;


create transactional table int_exp(foo int) partitioned by (bar int);
insert into int_exp select * from int_src;
select count(*) from int_exp;


create transactional table int_imp(foo int) partitioned by (bar int);


EXPORT TABLE int_exp to '/tmp/expint';
IMPORT TABLE int_imp FROM '/tmp/expint';


select count(*) FROM int_imp;
{code}
The count returned 0 (opposed to 1, but even for 100k order of records it was 0) and correct statistics were only available after running compute statistics.

 

This was unique to ACID + partitioning + ORC, but this isn't the expected behavior."
HIVE-21586,Thrift generated cpp files for metastore do not compile,"The way some structs like CreationMetadata, CompactionInfo, ColumnStatistics are defined in hive_metastore.thrift is that these structs are used before they are defined. While this works for the java code which is generated, it does not work for the generated cpp code since Thrift does not use pointer/references to the forward declared classes.

The easy fix for this would be to reorder the struct definitions in the hive_metastore.thrift so that they are always defined before they are used."
HIVE-21540,Query with join condition having date literal throws SemanticException.,"This semantic exception is thrown for the following query. 
*SemanticException '2019-03-20' encountered with 0 children*

{code}
create table date_1 (key int, dd date);
create table date_2 (key int, dd date);

select d1.key, d2.dd from(
  select key, dd as start_dd, current_date as end_dd from date_1) d1
  join date_2 as d2 on d1.key = d2.key where d2.dd between start_dd and end_dd;
{code}

When the WHERE condition below is commented out, the query completes successfully.
where d2.dd between start_dd and end_dd
------------------------------------------------"
HIVE-21538,Beeline: password source though the console reader did not pass to connection param,"Beeline: password source through the console reader do not pass to connection param, this will yield into the Authentication failure in case of LDAP authentication.
{code}
beeline -n USER -u ""jdbc:hive2://host:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2"" -p

Connecting to jdbc:hive2://host:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2;user=USER
Enter password for jdbc:hive2://host:2181/: ************
19/03/26 19:49:44 [main]: WARN jdbc.HiveConnection: Failed to connect to host:10000
19/03/26 19:49:44 [main]: ERROR jdbc.Utils: Unable to read HiveServer2 configs from ZooKeeper
Unknown HS2 problem when communicating with Thrift server.
Error: Could not open client transport for any of the Server URI's in ZooKeeper: Peer indicated failure: PLAIN auth failed: javax.security.sasl.AuthenticationException: Error validating LDAP user [Caused by javax.naming.AuthenticationException: [LDAP: error code 49 - 80090308: LdapErr: DSID-0C0903C8, comment: AcceptSecurityContext error, data 52e, v2580]] (state=08S01,code=0)
{code}"
HIVE-21499,should not remove the function from registry if create command failed with AlreadyExistsException,"As a part of HIVE-20953 we are removing the function if creation for same failed with any reason, this will yield into the following situation.
1. create function failed since function already exists
2. on #1 failure hive will clear the permanent function from the registry
3. this function will be of no use until hiveserver2 restarted."
HIVE-21455,Too verbose logging in AvroGenericRecordReader,"{{AvroGenericRecordReader}} logs the Avro schema for each datafile. It is too verbose, likely we don't need to log that on INFO level.
For example a table:
{noformat}
create table avro_tbl (c1 string, c2 int, c3 float) stored as avro;
{noformat}
and querying it with a select star - with 3 datafiles HiveServer2 logs the following:
{noformat}
2019-03-15 09:18:35,999 INFO  org.apache.hadoop.mapred.FileInputFormat: [HiveServer2-Handler-Pool: Thread-64]: Total input paths to process : 3
2019-03-15 09:18:35,999 INFO  org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader: [HiveServer2-Handler-Pool: Thread-64]: Found the avro schema in the job: {""type"":""record"",""name"":""avro_tbl"",""namespace"":""test"",""fields"":[{""name"":""c1"",""type"":[""null"",""string""],""default"":null},{""name"":""c2"",""type"":[""null"",""int""],""default"":null},{""name"":""c3"",""type"":[""null"",""float""],""default"":null}]}
2019-03-15 09:18:36,004 INFO  org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader: [HiveServer2-Handler-Pool: Thread-64]: Found the avro schema in the job: {""type"":""record"",""name"":""avro_tbl"",""namespace"":""test"",""fields"":[{""name"":""c1"",""type"":[""null"",""string""],""default"":null},{""name"":""c2"",""type"":[""null"",""int""],""default"":null},{""name"":""c3"",""type"":[""null"",""float""],""default"":null}]}
2019-03-15 09:18:36,010 INFO  org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader: [HiveServer2-Handler-Pool: Thread-64]: Found the avro schema in the job: {""type"":""record"",""name"":""avro_tbl"",""namespace"":""test"",""fields"":[{""name"":""c1"",""type"":[""null"",""string""],""default"":null},{""name"":""c2"",""type"":[""null"",""int""],""default"":null},{""name"":""c3"",""type"":[""null"",""float""],""default"":null}]}
{noformat}
This has a huge performance and storage penalty on a table with big schema and thousands of datafiles."
HIVE-21377,Using Oracle as HMS DB with DirectSQL,"When we use the Oracle as HMS DB, we saw this kind of contents in the HMS log accordingly:
{code:java}
2019-02-02 T08:23:57,102 WARN [Thread-12]: metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(3741)) - Falling back to ORM path due to direct SQL failure (this is not an error): Cannot extract boolean from column value 0 at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.extractSqlBoolean(MetaStoreDirectSql.java:1031) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsFromPartitionIds(MetaStoreDirectSql.java:728) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.access$300(MetaStoreDirectSql.java:109) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql$1.run(MetaStoreDirectSql.java:471) at org.apache.hadoop.hive.metastore.Batchable.runBatched(Batchable.java:73) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:462) at org.apache.hadoop.hive.metastore.ObjectStore$8.getSqlResult(ObjectStore.java:3392)
{code}

In Hive, we handle the Postgres, MySQL and Derby for the extractSqlBoolean.
But Oracle return the 0 or 1 for Boolean. So we need to modify the MetastoreDirectSqlUtils.java - [1]

So, could add this snip in this code?
{code:java}
  static Boolean extractSqlBoolean(Object value) throws MetaException {
    if (value == null) {
      return null;
    }
    if (value instanceof Boolean) {
      return (Boolean)value;
    }
    if (value instanceof Number) { // add
      try {
        return BooleanUtils.toBooleanObject((BigDecimal) value, 1, 0, null);
      } catch(IllegalArugmentExeception iae){
      // NOOP
      }
    if (value instanceof String) {
      try {
        return BooleanUtils.toBooleanObject((String) value, ""Y"", ""N"", null);
      } catch (IllegalArgumentException iae) {
        // NOOP
      }
    }
    throw new MetaException(""Cannot extract boolean from column value "" + value);
  }
{code}


 [1] -
https://github.com/apache/hive/blob/f51f108b761f0c88647f48f30447dae12b308f31/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/MetastoreDirectSqlUtils.java#L501-L527
 "
HIVE-21376,Incompatible change in Hive bucket computation,"HIVE-20007 seems to have inadvertently changed the bucket hash code computation via {{ObjectInspectorUtils.getBucketHashCodeOld()}} for the {{DATE}} and {{TIMESTAMP}} data type2.

{{DATE}} was previously computed using {{DateWritable}}, which uses {{daysSinceEpoch}} as the hash code. It is now computed using {{DateWritableV2}}, which uses the hash code of {{java.time.LocalDate}} (which is not days since epoch).

{{TIMESTAMP}} was previous computed using {{TimestampWritable}} and now uses {{TimestampWritableV2}}. They ostensibly use the same hash code computation, but there are two important differences:
 # {{TimestampWritable}} rounds the number of milliseconds into the seconds portion of the computation, but {{TimestampWritableV2}} does not.
 # {{TimestampWritable}} gets the epoch time from {{java.sql.Timestamp}}, which returns it relative to the JVM time zone, not UTC. {{TimestampWritableV2}} uses a {{LocalDateTime}} relative to UTC.

I was unable to get Hive 3.1 running in order to verify if this actually causes data to be read or written incorrectly (there may be code above this library method which makes things work correctly). However, if my understanding is correct, this means Hive 3.1 is both forwards and backwards incompatible with bucketed tables using either of these data types. It also indicates that Hive needs tests to verify that the hash code does not change between releases."
HIVE-21342,Analyze compute stats for column leave behind staging dir on hdfs,"staging dir cleanup does not happen for the ""analyze table .. compute statistics for columns"", this leads to stale directory on hdfs.
the problem seems to be with ColumnStatsSemanticAnalyzer which don't have hdfscleanup set for the context.
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java#L310"
HIVE-21074,Hive bucketed table query pruning does not work for IS NOT NULL condition,"The current version of bucket pruning skips all the predicates when it detects that one of the predicates is a compound type (e.g. NOT(IS_NULL) ) when evaluating AND logical operators.

This logic is faulty since as long as one of the AND operators is a bucketed column (_col_ = *literal*), the *literal* value of that _col_ should be considered in the bucket pruning optimization no matter what. For example:

SELECT * FROM tbl WHERE bucketed_col = 1 AND (some_compound_expr)

Then the the value '*1'* should be considered for pruning in the query plan. This limitation has manifested into a simpler case where a table that I am trying to optimized using bucketing technique is not effective when IS NOT NULL is used. Since IS NOT NULL is parsed into NOT(IS_NULL) (a compound expression), the pruning phase is completed skipped causing unnecessary tasks to be spawned. For instance:

SELECT * FROM tbl WHERE bucketed_col = 1 AND some_other_col IS NOT NULL

Will not trigger bucket pruning logic and perform a full table scan."
HIVE-21061,CTAS query fails with IllegalStateException for empty source,"Creating a table using CTAS from an empty source table with predicate condition evaluating to False
{code}
create table testctas1 (id int);
create table testctas2 as select * from testctas1 where 1=2;
{code}
Fails with below exception:
{code}
Caused by: java.lang.IllegalStateException: null
 at com.google.common.base.Preconditions.checkState(Preconditions.java:159)
 at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.verifyAndSetVectorPartDesc(Vectorizer.java:1312)
 at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateInputFormatAndSchemaEvolution(Vectorizer.java:1654)
 at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateAndVectorizeMapWork(Vectorizer.java:1865)
 at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:1109)
 at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:961)
 at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
 at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)
 at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)
 at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:2442)
 at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:717)
 at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:258)
 at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12443)
 at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:358)
 at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
 at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)
 at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1863)
 at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1810)
 at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1805)
 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
 at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)
 ... 36 more

{code}

 

 

 "
HIVE-21038,Fix checkstyle for standalone-metastore,Since HIVE-17506 checkstyle is not working for standalone-metastore and it's sub projects.
HIVE-21032,Refactor HiveMetaTool,"HiveMetaTool is doing everything in one class, needs to be refactored to have a nicer design."
HIVE-21029,External table replication for existing deployments running incremental replication.,"Existing deployments using hive replication do not get external tables replicated. For such deployments to enable external table replication they will have to provide a specific switch to first bootstrap external tables as part of hive incremental replication, following which the incremental replication will take care of further changes in external tables.

The switch will be provided by an additional hive configuration (for ex: hive.repl.bootstrap.external.tables) and is to be used in 
{code} WITH {code}  clause of 
{code} REPL DUMP {code} command. 

Additionally the existing hive config _hive.repl.include.external.tables_  will always have to be set to ""true"" in the above clause. 

Proposed usage for enabling external tables replication on existing replication policy.
1. Consider an ongoing repl policy <db1> in incremental phase.
Enable hive.repl.include.external.tables=true and hive.repl.bootstrap.external.tables=true in next incremental REPL DUMP.
- Dumps all events but skips events related to external tables.
- Instead, combine bootstrap dump for all external tables under “_bootstrap” directory.
- Also, includes the data locations file ""_external_tables_info”.
- LIMIT or TO clause shouldn’t be there to ensure the latest events are dumped before bootstrap dumping external tables.

2. REPL LOAD on this dump applies all the events first, copies external tables data and then bootstrap external tables (metadata).
- It is possible that the external tables (metadata) are not point-in time consistent with rest of the tables.
- But, it would be eventually consistent when the next incremental load is applied.
- This REPL LOAD is fault tolerant and can be retried if failed.

3. All future REPL DUMPs on this repl policy should set hive.repl.bootstrap.external.tables=false.
- If not set to false, then target might end up having inconsistent set of external tables as bootstrap wouldn’t clean-up any dropped external tables."
HIVE-20858,Serializer is not correctly initialized with configuration in Utilities.createEmptyBuckets(),
HIVE-20853,Expose ShuffleHandler.registerDag in the llap daemon API,"Currently DAGs are only registered when a submitWork is called for that DAG. At this point the crendentials are added to the ShuffleHandler and it can start serving.

However Tez might (and will) schedule tasks to fetch from the ShuffleHandler before anything of this happens and all this tasks will fail which may results in the query failing.

This happens in the scenario in which a LlapDaemon just comes up and tez fetchers try to open a connection before a DAG has been registered.

Adding this API will allow to register the DAG against the Daemon when the AM notices that a new Daemon is up."
HIVE-20848,After setting UpdateInputAccessTimeHook query fail with Table Not Found.,"{code}
 select from_unixtime(1540495168); 
 set hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.ATSHook,org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec;
 select from_unixtime(1540495168); 
{code}
the second select fail with following exception
{code}
ERROR ql.Driver: FAILED: Hive Internal Error: org.apache.hadoop.hive.ql.metadata.InvalidTableException(Table not found _dummy_table)
org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found _dummy_table
    at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1217)
    at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1168)
    at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1155)
    at org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec.run(UpdateInputAccessTimeHook.java:67)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1444)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1294)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1161)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1156)
    at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:197)
    at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:76)
    at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)
    at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
{code}
"
HIVE-20831,Add Session ID to Operation Logging,"{code:java|title=OperationManager.java}
LOG.info(""Adding operation: "" + operation.getHandle());
{code}

Please add additional logging to explicitly state which Hive session this operation is being added to.

https://github.com/apache/hive/blob/3963c729fabf90009cb67d277d40fe5913936358/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java#L201"
HIVE-20800,"Use ""posix"" for property tarLongFileMode for maven-assembly-plugin","Came across this error when building hive using ""mvn clean install -DskipTests""

{code}

[INFO] Building tar: /Users/wei/apache/hive/standalone-metastore/target/apache-hive-standalone-metastore-4.0.0-SNAPSHOT-src.tar.gz
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Hive Storage API 2.7.0-SNAPSHOT .................... SUCCESS [  5.656 s]
[INFO] Hive 4.0.0-SNAPSHOT ................................ SUCCESS [  0.779 s]
[INFO] Hive Classifications ............................... SUCCESS [  0.908 s]
[INFO] Hive Shims Common .................................. SUCCESS [  3.217 s]
[INFO] Hive Shims 0.23 .................................... SUCCESS [  7.102 s]
[INFO] Hive Shims Scheduler ............................... SUCCESS [  2.069 s]
[INFO] Hive Shims ......................................... SUCCESS [  1.905 s]
[INFO] Hive Common ........................................ SUCCESS [  8.185 s]
[INFO] Hive Service RPC ................................... SUCCESS [  3.603 s]
[INFO] Hive Serde ......................................... SUCCESS [  7.438 s]
[INFO] Hive Standalone Metastore .......................... FAILURE [  0.576 s]
[INFO] Hive Standalone Metastore Common Code .............. SKIPPED
[INFO] Hive Metastore ..................................... SKIPPED
[INFO] Hive Vector-Code-Gen Utilities ..................... SKIPPED
[INFO] Hive Llap Common ................................... SKIPPED
[INFO] Hive Llap Client ................................... SKIPPED
[INFO] Hive Llap Tez ...................................... SKIPPED
[INFO] Hive Spark Remote Client ........................... SKIPPED
[INFO] Hive Metastore Server .............................. SKIPPED
[INFO] Hive Query Language ................................ SKIPPED
[INFO] Hive Llap Server ................................... SKIPPED
[INFO] Hive Service ....................................... SKIPPED
[INFO] Hive Accumulo Handler .............................. SKIPPED
[INFO] Hive JDBC .......................................... SKIPPED
[INFO] Hive Beeline ....................................... SKIPPED
[INFO] Hive CLI ........................................... SKIPPED
[INFO] Hive Contrib ....................................... SKIPPED
[INFO] Hive Druid Handler ................................. SKIPPED
[INFO] Hive HBase Handler ................................. SKIPPED
[INFO] Hive JDBC Handler .................................. SKIPPED
[INFO] Hive HCatalog ...................................... SKIPPED
[INFO] Hive HCatalog Core ................................. SKIPPED
[INFO] Hive HCatalog Pig Adapter .......................... SKIPPED
[INFO] Hive HCatalog Server Extensions .................... SKIPPED
[INFO] Hive HCatalog Webhcat Java Client .................. SKIPPED
[INFO] Hive HCatalog Webhcat .............................. SKIPPED
[INFO] Hive HCatalog Streaming ............................ SKIPPED
[INFO] Hive HPL/SQL ....................................... SKIPPED
[INFO] Hive Streaming ..................................... SKIPPED
[INFO] Hive Llap External Client .......................... SKIPPED
[INFO] Hive Shims Aggregator .............................. SKIPPED
[INFO] Hive Kryo Registrator .............................. SKIPPED
[INFO] Hive TestUtils ..................................... SKIPPED
[INFO] Hive Kafka Storage Handler ......................... SKIPPED
[INFO] Hive Packaging ..................................... SKIPPED
[INFO] Hive Metastore Tools ............................... SKIPPED
[INFO] Hive Metastore Tools common libraries .............. SKIPPED
[INFO] Hive metastore benchmarks .......................... SKIPPED
[INFO] Hive Upgrade Acid .................................. SKIPPED
[INFO] Hive Pre Upgrade Acid 4.0.0-SNAPSHOT ............... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 42.026 s
[INFO] Finished at: 2018-10-24T15:34:40-07:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single (assemble) on project hive-standalone-metastore: Execution assemble of goal org.apache.maven.plugins:maven-assembly-plugin:3.1.0:single failed: group id '74715970' is too big ( > 2097151 ). Use STAR or POSIX extensions to overcome this limit -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hive-standalone-metastore

{code}"
HIVE-20792,Inserting timestamp with zones truncates the data,"For example with the table:

{code}
CREATE TABLE myTable
(
a TIMESTAMP
)
STORED AS ORC
tblproperties(""transactional""=""true"");
{code}

The following inserts store the wrong data:
{code}
INSERT INTO myTable VALUES(""2018-10-19 10:35:00 UTC""); -> 2018-10-19 00:00:00.0
INSERT INTO myTable VALUES(""2018-10-19 10:35:00 ZZZ""); -> 2018-10-19 00:00:00.0
{code}

The second one should fail since ZZZ is not a time zone.
Similarly if the column is of type DATE,
{code}
INSERT INTO myTableDate VALUES(""2018-10-19 AAAA""); -> 2018-10-19
{code}"
HIVE-20785,Wrong key name in the JDBC DatabaseMetaData.getPrimaryKeys method,"According to the documentation (1) the key should be {{KEY_SEQ, not KEQ_SEQ.}}

Pull request available: https://github.com/apache/hive/pull/440

 

(1) [https://docs.oracle.com/javase/8/docs/api/java/sql/DatabaseMetaData.html#getPrimaryKeys-java.lang.String-java.lang.String-java.lang.String-]

 "
HIVE-20771,LazyBinarySerDe fails on empty structs.,"{code:java}
CREATE TABLE cvaliente.structtest AS
SELECT named_struct();

SHOW CREATE TABLE cvaliente.structtest;

SELECT * FROM cvaliente.structtest ORDER BY rand();
{code}

The resulting schema is:
{code:sql}
CREATE TABLE `cvaliente.structtest`(
  `_c0` struct<>)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  'hdfs://nameservice1/user/cvaliente/cvaliente/structtest2'
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='true', 
  'numFiles'='1', 	  
  'numRows'='1', 
  'rawDataSize'='0', 
  'totalSize'='1', 	  
  'transient_lastDdlTime'='1539781607');
{code}
Between the MAP and REDUCE phase hive serializes to LazyBinaryStruct and when trying to read the same object back the {{SELECT}} query above fails:

{code}
2018-10-17 14:32:02,298 [FATAL] [TezChild] |tez.ReduceRecordSource|: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":0.13508293503238622},""value"":{""_col0"":{}}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:338)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:259)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:169)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:164)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating VALUE._col0
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:82)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:329)
	... 17 more
Caused by: java.lang.RuntimeException: length should be positive!
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryNonPrimitive.init(LazyBinaryNonPrimitive.java:54)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.init(LazyBinaryStruct.java:95)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)
	at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:98)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
	at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:77)
	... 18 more
{code}

this is because the LazyBinaryNonPrimitive doesn't allow for empty structs in https://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryNonPrimitive.java#L53

"
HIVE-20751,Upgrade arrow version to 0.10.0,Need to upgrade arrow version as spark is moving to arrow version 0.10.0 in it's upcoming release 2.4.0
HIVE-20734,"Beeline: When beeline-site.xml is and hive CLI redirects to beeline, it should use the system username/dummy password instead of prompting for one",
HIVE-20714,SHOW tblproperties for a single property returns the value in the name column,"show tblproperties default.tmpfoo(""bar"") returns:
{code}
+------------+-------------+
| prpt_name  | prpt_value  |
+------------+-------------+
| bar value  | NULL        |
+------------+-------------+
{code}
It should return
{code}
+------------+-------------+
| prpt_name  | prpt_value  |
+------------+-------------+
| bar  | bar value        |
+------------+-------------+
{code}"
HIVE-20679,DDL operations on hive might create large messages for DBNotification,"Certain type of ddl operations might create large messages as part of DBNoitification, this might lead to the rdbms throwing an error when storing the message since its size is to large. It will also increase the footprint of the rdbms space usage. 

We should try store compressed messages to allow handling these situations. 

Edit: For notification_log table the message column for all supported databases can store messages from 2GB to 4GB

"
HIVE-20659,Update commons-compress to 1.18 due to security issues,"Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: [https://commons.apache.org/proper/commons-compress/security-reports.html]

I propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. [https://github.com/zuinnote/hadoopoffice/wiki)] to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder."
HIVE-20646,Partition filter condition is not pushed down to metastore query if it has IS NOT NULL.,"If the partition filter condition has ""is not null"" then the filter query isn't getting pushed to the SQL query in RDMBS. 
This slows down metastore api calls for getting list of partitions with filter condition.

This condition gets added by optimizer in many cases so this is affecting many queries."
HIVE-20644,Avoid exposing sensitive infomation through a Hive Runtime exception,"The HiveException raised from the following methods is exposing the datarow the caused the run time exception.
 # ReduceRecordSource::GroupIterator::next() - around line 372
 # MapOperator::process() - around line 567
 # ExecReducer::reduce() - around line 243

In all the cases, a string representation of the row is constructed on the fly and is included in
the error message.

VectorMapOperator::process() - around line 973 raises the same exception but it's not exposing the row since the row contents are not included in the error message.

While trying to reproduce above error, I also found that the arguments to a UDF get exposed in log messages from FunctionRegistry::invoke() around line 1114. This too can cause sensitive information to be leaked through error message.

This way some sensitive information is leaked to a user through exception message. That information may not be available to the user otherwise. Hence it's a kind of security breach or violation of access control.

The contents of the row or the arguments to a function may be useful for debugging and hence it's worth to add those to logs. Hence proposal here to log a separate message with log level DEBUG or INFO containing the string representation of the row. Users can configure their logging so that DEBUG/INFO messages do not go to the client but at the same time are available in the hive server logs for debugging. The actual exception message will not contain any sensitive data like row data or argument data."
HIVE-20615,CachedStore: Background refresh thread bug fixes,Regression introduced in HIVE-18264. Fixes background thread starting and refreshing of the table cache.
HIVE-20607,TxnHandler should use PreparedStatement to execute direct SQL queries.,"TxnHandler uses direct SQL queries to operate on Txn related databases/tables in Hive metastore RDBMS.
Most of the methods are direct calls from Metastore api which should be directly append input string arguments to the SQL string.
Need to use parameterised PreparedStatement object to set these arguments."
HIVE-20599,CAST(INTERVAL_DAY_TIME AS STRING) is throwing SemanticException,"SELECT CAST(from_utc_timestamp(timestamp '2018-05-02 15:30:30', 'PST') - from_utc_timestamp(timestamp '1970-01-30 16:00:00', 'PST') AS STRING);

throws below Exception
{code:java}
Error: Error while compiling statement: FAILED: SemanticException Line 0:-1 Wrong arguments ''PST'': No matching method for class org.apache.hadoop.hive.ql.udf.UDFToString with (interval_day_time). Possible choices: _FUNC_(bigint)  _FUNC_(binary)  _FUNC_(boolean)  _FUNC_(date)  _FUNC_(decimal(38,18))  _FUNC_(double)  _FUNC_(float)  _FUNC_(int)  _FUNC_(smallint)  _FUNC_(string)  _FUNC_(timestamp)  _FUNC_(tinyint)  _FUNC_(void) (state=42000,code=40000){code}"
HIVE-20593,Load Data for partitioned ACID tables fails with bucketId out of range: -1,"Load data for ACID tables is failing to load ORC files when it is converted to IAS job.

 

The tempTblObj is inherited from target table. However, the only table property which needs to be inherited is bucketing version. Properties like transactional etc should be ignored.

 "
HIVE-20580,OrcInputFormat.isOriginal() should not rely on hive.acid.key.index,"{{org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.isOriginal()}} is checking for presence of {{hive.acid.key.index}} in the footer.  This is only created when the file is written by {{OrcRecordUpdater}}.  It should instead check for presence of Acid metadata columns so that a file can be produced by something other than {{OrcRecordUpdater}}.

Also, {{hive.acid.key.index}} counts number of different type of events which is not really useful for Acid V2 (as of Hive 3) since each file only has 1 type of event."
HIVE-20555,HiveServer2: Preauthenticated subject for http transport is not retained for entire duration of http communication in some cases,"As implemented in HIVE-8705, for http transport, we add the logged in subject's credentials in the http header via a request interceptor. The request interceptor doesn't seem to be getting used for some http traffic (e.g. knox ssl in the same rpc). It would also be better to cache the logged in subject for the duration of the whole session."
HIVE-20545,Ability to exclude potentially large parameters in HMS Notifications,Clients can add large-sized parameters in Table/Partition objects. So we need to enable adding regex patterns through HiveConf to match parameters to be filtered from table and partition objects before serialization in HMS notifications.
HIVE-20521,"HS2 doAs=true has permission issue with hadoop.tmp.dir, with MR and S3A filesystem","This is a result of changes in HIVE-18858.
 As described by [~puneetj] in HIVE-18858 -

{quote}
This seems to have broken working scenarios with Hive MR.  We now see hadoop.tmp.dir is always set to /tmp/hadoop-hive (in job.xml). This creates problems on a multi-tenant hadoop cluster since ownership of tmp folder is set to the user who executes the jobs first and other users fails to write to tmp folder.

E.g. User1 run job and /tmp/hadoop-hive is created on worker node with ownership to user1 and sibsequently user2 tries to run a job and job fails due to no write permission on /tmp/hadoop-hive/

Old behavior allowed multiple tenants to write to their respective tmp folders which was secure and contention free. User1 - /tmp/hadoop-user1, User2 - /tmp/hadoop-user2.
{quote}
 
The change in HIVE-18858 causes variable expansion to happen in HiveServer2, while it was happening in the tasks (ExecMapper, ExecReducer) before that change. THis causes 
bq. ""/tmp/hadoop-{user.name}"" 
to be expanded as /tmp/hadoop-hive instead of /tmp/hadoop-user1
"
HIVE-20499,GetTablesOperation pull all the tables meta irrespective of auth.,"GetTablesOperation pull all the tables meta irrespective of auth.
dbvisualizer and other ui based jdbc client pull tableemta similar to following operation:
{code}
ResultSet res = con.getMetaData().getTables("""", """", ""%"", new String[] { ""TABLE"", ""VIEW"" });
{code}
https://github.com/rajkrrsingh/HiveServer2JDBCSample/blob/master/src/main/java/TestConnection.java#L20"
HIVE-20476,CopyUtils used by REPL LOAD and EXPORT/IMPORT operations ignore distcp error.,"CopyUtils uses FileUtils.distCp to copy files but doesn't check the return value. It returns false if the copy fails.
Now, REPL LOAD and EXPORT/IMPORT commands internally uses CopyUtils to copy data files across clusters and here it may return success even if file copy fails and may cause data loss.

Need to throw error and retry."
HIVE-20462,"""CREATE VIEW IF NOT EXISTS"" fails if view already exists","After upgrading to Hive 3.1 from Hive 1.2, I can't run my queries anymore. It seems that ""if not exists"" is ignored in ""create view"".

*Example:*
 # create view if not exists test as select 1;
 # create view if not exists test as select 1;

The second call fails with ""Table already exists: default.test"". I changed this to ""create or replace view"" as a workaround.

Also documented [link here|https://community.hortonworks.com/questions/214772/hdp-3-hive-create-view-if-not-exists-fails.html|]"
HIVE-20433,Implicit String to Timestamp conversion is slow,"getTimestampFromString() is slow at casting dates. It throws twice before date conversion can happen.

 

cc [~gopalv] [~ashutoshc]"
HIVE-20372,WRTIE_SET typo in TxnHandler,"[https://github.com/prongs/apache-hive/blob/deabe59371e98a21f4c3a58a9d8da51e4632fca5/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java#L765]

minor typo"
HIVE-20360,QTest: ignore driver/qtest exclusions if -Dqfile param is set,"Sometimes I need to run qtests with another driver for testing purposes. In this case I have to edit testconfiguration.properties which seems a bit hacky, even if it's temporary.

In this case, no tests will run (however there's a log message):
{code:java}
mvn test -Pitests -pl itests/qtest -pl itests/util -Dtest=TestCliDriver -Dqfile=bucketizedhiveinputformat.q
{code}
 "
HIVE-20343,Hive 3: CTAS does not respect transactional_properties,"Steps to reproduce:
{code}
create table ctasexampleinsertonly stored as orc  TBLPROPERTIES (""transactional_properties""=""insert_only"") as select * from testtable limit 1;
{code}
look for transactional_properties which is 'default' not the expected ""insert_only""
{code}
 describe formatted ctasexampleinsertonly

 +-------------------------------+----------------------------------------------------+-----------------------+
|           col_name            |                     data_type                      |        comment        |
+-------------------------------+----------------------------------------------------+-----------------------+
| # col_name                    | data_type                                          | comment               |
| name                          | varchar(8)                                         |                       |
| time                          | double                                             |                       |
|                               | NULL                                               | NULL                  |
| # Detailed Table Information  | NULL                                               | NULL                  |
| Database:                     | default                                            | NULL                  |
| OwnerType:                    | USER                                               | NULL                  |
| Owner:                        | hive                                               | NULL                  |
| CreateTime:                   | Wed Aug 08 21:35:15 UTC 2018                       | NULL                  |
| LastAccessTime:               | UNKNOWN                                            | NULL                  |
| Retention:                    | 0                                                  | NULL                  |
| Location:                     | hdfs://xxxxxxxxxx:8020/warehouse/tablespace/managed/hive/ctasexampleinsertonly | NULL                  |
| Table Type:                   | MANAGED_TABLE                                      | NULL                  |
| Table Parameters:             | NULL                                               | NULL                  |
|                               | COLUMN_STATS_ACCURATE                              | {}                    |
|                               | bucketing_version                                  | 2                     |
|                               | numFiles                                           | 1                     |
|                               | numRows                                            | 1                     |
|                               | rawDataSize                                        | 0                     |
|                               | totalSize                                          | 754                   |
|                               | transactional                                      | true                  |
|                               | transactional_properties                           | default               |
|                               | transient_lastDdlTime                              | 1533764115            |
|                               | NULL                                               | NULL                  |
| # Storage Information         | NULL                                               | NULL                  |
| SerDe Library:                | org.apache.hadoop.hive.ql.io.orc.OrcSerde          | NULL                  |
| InputFormat:                  | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat    | NULL                  |
| OutputFormat:                 | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat   | NULL                  |
| Compressed:                   | No                                                 | NULL                  |
| Num Buckets:                  | -1                                                 | NULL                  |
| Bucket Columns:               | []                                                 | NULL                  |
| Sort Columns:                 | []                                                 | NULL                  |
| Storage Desc Params:          | NULL                                               | NULL                  |
|                               | serialization.format                               | 1                     |
+-------------------------------+----------------------------------------------------+-----------------------+
{code}

not sure whether its a cosmatic issue but it does creates a problem with insert 
{code}
CREATE TABLE TABLE42 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe' STORED AS RCFILE LOCATION '/tmp/test10' as select * from testtable limit 1;
{code}

it takes the default transactional_properties as insert_only instead of default and failed with the following Exception 
{code}
ERROR : Job Commit failed with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(The following files were committed but not found: [/tmp/test10/delta_0000004_0000004_0000/000000_0])'
org.apache.hadoop.hive.ql.metadata.HiveException: The following files were committed but not found: [/tmp/test10/delta_0000004_0000004_0000/000000_0]
	at org.apache.hadoop.hive.ql.exec.Utilities.handleMmTableFinalPath(Utilities.java:4329)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.jobCloseOp(FileSinkOperator.java:1393)
	at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:798)
	at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803)
	at org.apache.hadoop.hive.ql.exec.Operator.jobClose(Operator.java:803)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.close(TezTask.java:579)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:316)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2668)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2339)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2015)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1713)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1707)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224)
	at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:316)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688)
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:329)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}"
HIVE-20329,Long running repl load (incr/bootstrap) causing OOM error,The task created in the previous iterations of the load are not delinked and thus causing heap memory usage issue. need to delink the tasks to avoid OOM error.
HIVE-20316,Skip external table file listing for create table event.,"We are currently skipping external table replication. We shall also skip listing all the files in create table event generation for external tables. External tables might have very large number of files, so it would take long time to list them."
HIVE-20293,Support Replication of ACID table truncate operation,"Support truncate acid table replication.

1. Write id allocation needs to be removed"
HIVE-20290,Lazy initialize ArrowColumnarBatchSerDe so it doesn't allocate buffers during GetSplits,"When using {{GenericUDTFGetSplits}} to create {{LlapInputSplit}} for submission to {{LlapOutputFormatService}}, the physical plan generation initializes whatever SerDe is being used.

{{ArrowColumnarBatchSerDe}} initializes buffers for Arrow and {{VectorizedRowBatch}} at this point inside HS2 which are never used."
HIVE-20209,Metastore connection fails for first attempt in repl dump.,"Run the following command:
{code:java}
repl dump `*` from 60758 with ('hive.repl.dump.metadata.only'='true', 'hive.repl.dump.include.acid.tables'='true');
{code}
See this in hs2.log:
{code:java}
2018-07-10T18:07:32,308 INFO [HiveServer2-Handler-Pool: Thread-14380]: conf.HiveConf (HiveConf.java:getLogIdVar(5061)) - Using the default value passed in for log id: f1e13736-3f10-4abf-a29b-683b534dfa4c
2018-07-10T18:07:32,309 INFO [HiveServer2-Handler-Pool: Thread-14380]: session.SessionState (:()) - Updating thread name to f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380
2018-07-10T18:07:32,311 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: operation.OperationManager (:()) - Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=16eb1d07-e125-490c-8ab8-90192bfd459b]
2018-07-10T18:07:32,314 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Compiling command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b): repl dump `*` from 60758 with ('hive.repl.dump.metadata.only'='true', 'hive.repl.dump.include.acid.tables'='true')
2018-07-10T18:07:32,317 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:32,317 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:32,319 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:32,319 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
2018-07-10T18:07:32,439 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Semantic Analysis Completed (retrial = false)
2018-07-10T18:07:32,440 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:dump_dir, type:string, comment:from deserializer), FieldSchema(name:last_repl_id, type:string, comment:from deserializer)], properties:null)
2018-07-10T18:07:32,443 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: exec.ListSinkOperator (:()) - Initializing operator LIST_SINK[0]
2018-07-10T18:07:32,446 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: ql.Driver (:()) - Completed compiling command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b); Time taken: 0.132 seconds
2018-07-10T18:07:32,447 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: conf.HiveConf (HiveConf.java:getLogIdVar(5061)) - Using the default value passed in for log id: f1e13736-3f10-4abf-a29b-683b534dfa4c
2018-07-10T18:07:32,448 INFO [f1e13736-3f10-4abf-a29b-683b534dfa4c HiveServer2-Handler-Pool: Thread-14380]: session.SessionState (:()) - Resetting thread name to HiveServer2-Handler-Pool: Thread-14380
2018-07-10T18:07:32,451 INFO [HiveServer2-Background-Pool: Thread-15161]: reexec.ReExecDriver (:()) - Execution #1 of query
2018-07-10T18:07:32,452 INFO [HiveServer2-Background-Pool: Thread-15161]: lockmgr.DbTxnManager (:()) - Setting lock request transaction to txnid:30327 for queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b
2018-07-10T18:07:32,454 INFO [HiveServer2-Background-Pool: Thread-15161]: lockmgr.DbLockManager (:()) - Requesting: queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b LockRequest(component:[LockComponent(type:SHARED_READ, level:DB, dbname:default, operationType:SELECT), LockComponent(type:SHARED_READ, level:DB, dbname:hwxdemo, operationType:SELECT), LockComponent(type:SHARED_READ, level:DB, dbname:information_schema, operationType:SELECT), LockComponent(type:SHARED_READ, level:DB, dbname:sys, operationType:SELECT)], txnid:30327, user:hive, hostname:hwx-demo-2.field.hortonworks.com, agentInfo:hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b)
2018-07-10T18:07:32,497 INFO [HiveServer2-Background-Pool: Thread-15161]: lockmgr.DbLockManager (:()) - Response to queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b LockResponse(lockid:30305, state:ACQUIRED)
2018-07-10T18:07:32,501 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - Executing command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b): repl dump `*` from 60758 with ('hive.repl.dump.metadata.only'='true', 'hive.repl.dump.include.acid.tables'='true')
2018-07-10T18:07:32,503 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - Starting task [Stage-0:REPL_DUMP] in serial mode
2018-07-10T18:07:32,506 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Closed a connection to metastore, current connections: 18
2018-07-10T18:07:32,509 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:32,509 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:32,510 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:32,510 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
2018-07-10T18:07:32,536 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Closed a connection to metastore, current connections: 18
2018-07-10T18:07:32,538 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:32,539 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:32,542 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:32,542 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=hive (auth:SIMPLE) retries=24 delay=5 lifetime=0
2018-07-10T18:07:32,565 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::START: \{""dbName"":""*"",""dumpType"":""INCREMENTAL"",""estimatedNumEvents"":0,""dumpStartTime"":1531246052}
2018-07-10T18:07:32,577 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Closed a connection to metastore, current connections: 18
2018-07-10T18:07:32,578 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60759 COMMIT_TXN message : \{""txnid"":30321,""timestamp"":1531246017,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,603 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60759"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""1/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,604 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60760 COMMIT_TXN message : \{""txnid"":30322,""timestamp"":1531246021,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,628 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60760"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""2/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,629 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60761 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246022,""fromTxnId"":30323,""toTxnId"":30323,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,653 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60761"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""3/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,654 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60762 COMMIT_TXN message : \{""txnid"":30323,""timestamp"":1531246028,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,679 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60762"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""4/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,680 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60763 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246033,""fromTxnId"":30324,""toTxnId"":30324,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,702 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60763"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""5/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,702 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60764 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246038,""fromTxnId"":30325,""toTxnId"":30325,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,724 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60764"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""6/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,725 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60765 COMMIT_TXN message : \{""txnid"":30324,""timestamp"":1531246038,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,747 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60765"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""7/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,748 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60766 COMMIT_TXN message : \{""txnid"":30325,""timestamp"":1531246043,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,768 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60766"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""8/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,769 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60767 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246043,""fromTxnId"":30326,""toTxnId"":30326,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,791 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60767"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""9/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,791 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60768 COMMIT_TXN message : \{""txnid"":30326,""timestamp"":1531246049,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,816 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60768"",""eventType"":""EVENT_COMMIT_TXN"",""eventsDumpProgress"":""10/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,817 INFO [HiveServer2-Background-Pool: Thread-15161]: events.AbstractEventHandler (:()) - Processing#60769 OPEN_TXN message : \{""txnIds"":null,""timestamp"":1531246052,""fromTxnId"":30327,""toTxnId"":30327,""server"":""thrift://hwx-demo-2.field.hortonworks.com:9083"",""servicePrincipal"":""hive/_HOST@EXAMPLE.COM""}
2018-07-10T18:07:32,841 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::EVENT_DUMP: \{""dbName"":""*"",""eventId"":""60769"",""eventType"":""EVENT_OPEN_TXN"",""eventsDumpProgress"":""11/0"",""dumpTime"":1531246052}
2018-07-10T18:07:32,841 WARN [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - MetaStoreClient lost connection. Attempting to reconnect (1 of 24) after 5s. getNextNotification
org.apache.thrift.transport.TTransportException: Cannot write to null outputStream
 at org.apache.thrift.transport.TIOStreamTransport.write(TIOStreamTransport.java:142) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.protocol.TBinaryProtocol.writeI32(TBinaryProtocol.java:178) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.protocol.TBinaryProtocol.writeMessageBegin(TBinaryProtocol.java:106) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:70) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:62) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_get_next_notification(ThriftHiveMetastore.java:5547) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_next_notification(ThriftHiveMetastore.java:5539) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getNextNotification(HiveMetaStoreClient.java:2697) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[?:?]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
 at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at com.sun.proxy.$Proxy60.getNextNotification(Unknown Source) ~[?:?]
 at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[?:?]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
 at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2773) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at com.sun.proxy.$Proxy60.getNextNotification(Unknown Source) ~[?:?]
 at org.apache.hadoop.hive.metastore.messaging.EventUtils$MSClientNotificationFetcher.getNextNotificationEvents(EventUtils.java:94) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.messaging.EventUtils$NotificationEventIterator.fetchNextBatch(EventUtils.java:146) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.metastore.messaging.EventUtils$NotificationEventIterator.hasNext(EventUtils.java:176) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.incrementalDump(ReplDumpTask.java:170) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.execute(ReplDumpTask.java:121) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2668) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2339) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2015) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1713) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1707) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:316) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_112]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_112]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1688) ~[hadoop-common-3.1.0.3.0.0.0-1605.jar:?]
 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:329) ~[hive-service-3.1.0.3.0.0.0-1605.jar:3.1.0.3.0.0.0-1605]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_112]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_112]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_112]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_112]
 at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-10T18:07:33,157 INFO [org.apache.ranger.audit.queue.AuditBatchQueue1]: destination.HDFSAuditDestination (:()) - Flushing HDFS audit. Event Size:4
2018-07-10T18:07:37,842 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.RetryingMetaStoreClient (:()) - RetryingMetaStoreClient trying reconnect as hive (auth:SIMPLE)
2018-07-10T18:07:37,842 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Trying to connect to metastore with URI thrift://hwx-demo-2.field.hortonworks.com:9083
2018-07-10T18:07:37,842 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Opened a connection to metastore, current connections: 19
2018-07-10T18:07:37,843 INFO [HiveServer2-Background-Pool: Thread-15161]: metastore.HiveMetaStoreClient (:()) - Connected to metastore.
2018-07-10T18:07:37,864 INFO [HiveServer2-Background-Pool: Thread-15161]: ReplState (:()) - REPL::END: \{""dbName"":""*"",""dumpType"":""INCREMENTAL"",""actualNumEvents"":11,""dumpEndTime"":1531246057,""dumpDir"":""/user/hive/repl/c72dc152-c252-4454-9b0e-cf8c335ab07c"",""lastReplId"":""60769""}
2018-07-10T18:07:37,864 INFO [HiveServer2-Background-Pool: Thread-15161]: repl.ReplDumpTask (:()) - Done dumping events, preparing to return /user/hive/repl/c72dc152-c252-4454-9b0e-cf8c335ab07c,60769
2018-07-10T18:07:37,919 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - Completed executing command(queryId=hive_20180710180732_7dcc20db-90db-486d-a825-e6fa91dc092b); Time taken: 5.419 seconds
2018-07-10T18:07:37,919 INFO [HiveServer2-Background-Pool: Thread-15161]: ql.Driver (:()) - OK
{code}
Metastore connection failed 1st attempt, but success after reconnect. That adds 5s for every repl dump command and likely to leak connection.

Similarly, Hive.close() also causes 
{code:java}
org.apache.thrift.transport.TTransportException: Cannot write to null outputStream{code}
 "
HIVE-20192,HS2 with embedded metastore is leaking JDOPersistenceManager objects.,"Hiveserver2 instances where crashing every 3-4 days and observed HS2 in on unresponsive state. Also, observed that the FGC collection happening regularly

From JXray report it is seen that pmCache(List of JDOPersistenceManager objects) is occupying 84% of the heap and there are around 16,000 references of UDFClassLoader.
{code:java}
10,759,230K (84.7%) Object tree for GC root(s) Java Static org.apache.hadoop.hive.metastore.ObjectStore.pmf
- org.datanucleus.api.jdo.JDOPersistenceManagerFactory.pmCache ↘ 10,744,419K (84.6%), 1 reference(s)
  - j.u.Collections$SetFromMap.m ↘ 10,744,419K (84.6%), 1 reference(s)
    - {java.util.concurrent.ConcurrentHashMap}.keys ↘ 10,743,764K (84.5%), 16,872 reference(s)
      - org.datanucleus.api.jdo.JDOPersistenceManager.ec ↘ 10,738,831K (84.5%), 16,872 reference(s)
        ... 3 more references together retaining 4,933K (< 0.1%)
    - java.util.concurrent.ConcurrentHashMap self 655K (< 0.1%), 1 object(s)
      ... 2 more references together retaining 48b (< 0.1%)
- org.datanucleus.api.jdo.JDOPersistenceManagerFactory.nucleusContext ↘ 14,810K (0.1%), 1 reference(s)
... 3 more references together retaining 96b (< 0.1%){code}
When the RawStore object is re-created, it is not allowed to be updated into the ThreadWithGarbageCleanup.threadRawStoreMap which leads to the new RawStore never gets cleaned-up when the thread exit.

 "
HIVE-20123,Fix masking tests after HIVE-19617,"Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed."
HIVE-20120,Incremental repl load DAG generation is causing OOM error.,Split the incremental load into multiple iterations. In each iteration create number of tasks equal to the configured value.
HIVE-20097,Convert standalone-metastore to a submodule,"This is a subtask to stage HIVE-17751 changes into several smaller phases.

The first part is moving existing code in hive-standalone-metastore to a sub-module."
HIVE-20066,hive.load.data.owner is compared to full principal,"HIVE-19928 compares the user running HS2 to the configured owner (hive.load.data.owner) to check if we're able to move the file with LOAD DATA or need to copy.

This check compares the full username (that may contain the full kerberos principal) to hive.load.data.owner. We should compare to the short username ({{UGI.getShortUserName()}}) instead. That's used in similar context [here|https://github.com/apache/hive/blob/f519db7eafacb4b4d2d9fe2a9e10e908d8077224/common/src/java/org/apache/hadoop/hive/common/FileUtils.java#L398].

cc [~djaiswal]"
HIVE-20059,Hive streaming should try shade prefix unconditionally on exception,AbstractRecordWriter tries hive.classloader.shade.prefix on ClassNotFoundException but there are instances where OrcOutputFormat from old hive version gets loaded resulting in ClassCastException. I think we should try shadeprefix when defined and when any exception is thrown. 
HIVE-20004,Wrong scale used by ConvertDecimal64ToDecimal results in incorrect results,"ConvertDecimal64ToDecimal uses scale from output column vector which results in incorrect results.

Input: decimal(8,1) Output: decimal(9,2)

Input value: 963.8 gets converted to 96.38 which is wrong. The scale should not change this case (value should be 963.8 even after the conversion). "
HIVE-19980,GenericUDTFGetSplits fails when order by query returns 0 rows,"When order by query returns 0 rows, there will not be any files in temporary table location for GenericUDTFGetSplits

which results in the following exception
{code:java}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
  at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:217)
  at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.getSplits(GenericUDTFGetSplits.java:420)
  ... 52 more{code}"
HIVE-19970,Replication dump has a NPE when table is empty,if table directory or partition directory is missing ..dump is throwing NPE instead of file missing exception.
HIVE-19964,Apply resource plan fails if trigger expression has quotes,"{code:java}
0: jdbc:hive2://localhost:10000> CREATE TRIGGER global.big_hdfs_read WHEN HDFS_BYTES_READ > '300kb' DO KILL;
INFO : Compiling command(queryId=pjayachandran_20180621131017_72b1441b-d790-4db7-83ca-479735843890): CREATE TRIGGER global.big_hdfs_read WHEN HDFS_BYTES_READ > '300kb' DO KILL
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO : Completed compiling command(queryId=pjayachandran_20180621131017_72b1441b-d790-4db7-83ca-479735843890); Time taken: 0.015 seconds
INFO : Executing command(queryId=pjayachandran_20180621131017_72b1441b-d790-4db7-83ca-479735843890): CREATE TRIGGER global.big_hdfs_read WHEN HDFS_BYTES_READ > '300kb' DO KILL
INFO : Starting task [Stage-0:DDL] in serial mode
INFO : Completed executing command(queryId=pjayachandran_20180621131017_72b1441b-d790-4db7-83ca-479735843890); Time taken: 0.025 seconds
INFO : OK
No rows affected (0.054 seconds)
0: jdbc:hive2://localhost:10000> ALTER TRIGGER global.big_hdfs_read ADD TO UNMANAGED;
INFO : Compiling command(queryId=pjayachandran_20180621131031_dd489324-db23-412f-9409-32ba697a10e5): ALTER TRIGGER global.big_hdfs_read ADD TO UNMANAGED
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO : Completed compiling command(queryId=pjayachandran_20180621131031_dd489324-db23-412f-9409-32ba697a10e5); Time taken: 0.014 seconds
INFO : Executing command(queryId=pjayachandran_20180621131031_dd489324-db23-412f-9409-32ba697a10e5): ALTER TRIGGER global.big_hdfs_read ADD TO UNMANAGED
INFO : Starting task [Stage-0:DDL] in serial mode
INFO : Completed executing command(queryId=pjayachandran_20180621131031_dd489324-db23-412f-9409-32ba697a10e5); Time taken: 0.029 seconds
INFO : OK
No rows affected (0.054 seconds)
0: jdbc:hive2://localhost:10000> ALTER RESOURCE PLAN global ENABLE;
INFO : Compiling command(queryId=pjayachandran_20180621131036_26a5f4f3-91e3-4bec-ab42-800adb90104e): ALTER RESOURCE PLAN global ENABLE
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO : Completed compiling command(queryId=pjayachandran_20180621131036_26a5f4f3-91e3-4bec-ab42-800adb90104e); Time taken: 0.012 seconds
INFO : Executing command(queryId=pjayachandran_20180621131036_26a5f4f3-91e3-4bec-ab42-800adb90104e): ALTER RESOURCE PLAN global ENABLE
INFO : Starting task [Stage-0:DDL] in serial mode
INFO : Completed executing command(queryId=pjayachandran_20180621131036_26a5f4f3-91e3-4bec-ab42-800adb90104e); Time taken: 0.021 seconds
INFO : OK
No rows affected (0.045 seconds)
0: jdbc:hive2://localhost:10000> ALTER RESOURCE PLAN global ACTIVATE;
INFO : Compiling command(queryId=pjayachandran_20180621131037_551b2af0-321b-4638-8ac0-76771a159f4b): ALTER RESOURCE PLAN global ACTIVATE
INFO : Semantic Analysis Completed (retrial = false)
INFO : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO : Completed compiling command(queryId=pjayachandran_20180621131037_551b2af0-321b-4638-8ac0-76771a159f4b); Time taken: 0.017 seconds
INFO : Executing command(queryId=pjayachandran_20180621131037_551b2af0-321b-4638-8ac0-76771a159f4b): ALTER RESOURCE PLAN global ACTIVATE
INFO : Starting task [Stage-0:DDL] in serial mode
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Invalid expression: HDFS_BYTES_READ > 300kb
INFO : Completed executing command(queryId=pjayachandran_20180621131037_551b2af0-321b-4638-8ac0-76771a159f4b); Time taken: 0.037 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Invalid expression: HDFS_BYTES_READ > 300kb (state=08S01,code=1){code}"
HIVE-19956,Include yarn registry classes to jdbc standalone jar,HS2 Active/Passive HA requires some yarn registry classes. Include it in JDBC standalone jar. 
HIVE-19935,Hive WM session killed: Failed to update LLAP tasks count,"I'm getting this error with WM feature quite frequently. It causes AM containers to shut down and a new one created to replace it.
{noformat}
018-06-18T19:06:49,969 INFO [Thread-250] monitoring.RenderStrategy$LogToFileFunction: Map 1: 313(+270)/641
2018-06-18T19:06:49,988 INFO [NotificationEventPoll 0] metastore.HiveMetaStore: 4: get_config_value: name=metastore.batch.retrieve.max defaultValue=50
2018-06-18T19:06:49,988 INFO [NotificationEventPoll 0] HiveMetaStore.audit: ugi=hive ip=unknown-ip-addr cmd=get_config_value: name=metastore.batch.retrieve.max defaultValue=50
2018-06-18T19:06:50,204 INFO [pool-29-thread-1] tez.TriggerValidatorRunnable: Query: hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4. Trigger { name: alluxio_medium, expression: ALLUXIO_BYTES_READ >
6442450944, action: MOVE TO medium } violated. Current value: 7184667126. Applying action.
2018-06-18T19:06:50,205 INFO [pool-29-thread-1] tez.WorkloadManager: Queued move session: 49be39e5-875c-4cfe-8601-7fe84dd57e0c moving from default to medium
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WorkloadManager: Handling move session event: 49be39e5-875c-4cfe-8601-7fe84dd57e0c moving from default to medium
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WorkloadManager: Subscribed to counters: [S3A_BYTES_READ, BYTES_READ, ALLUXIO_BYTES_READ]
2018-06-18T19:06:50,205 INFO [pool-29-thread-1] tez.KillMoveTriggerActionHandler: Moved session 49be39e5-875c-4cfe-8601-7fe84dd57e0c to pool medium
2018-06-18T19:06:50,205 INFO [Workload management master] tez.GuaranteedTasksAllocator: Updating 49be39e5-875c-4cfe-8601-7fe84dd57e0c with 144 guaranteed tasks
2018-06-18T19:06:50,205 INFO [Workload management master] tez.WmEvent: Added WMEvent: EventType: MOVE EventStartTimestamp: 1529348810205 elapsedTime: 0 wmTezSessionInfo:SessionId: 49be39e5-875c-4cfe-8601-7fe
84dd57e0c Pool: medium Cluster %: 30.0
2018-06-18T19:06:50,234 INFO [StateChangeNotificationHandler] impl.ZkRegistryBase$InstanceStateChangeListener: CHILD_UPDATED for zknode /user-hive/llap/workers/worker-0000001571
2018-06-18T19:06:50,235 INFO [StateChangeNotificationHandler] tez.TezSessionPool: AM for 49be39e5-875c-4cfe-8601-7fe84dd57e0c, v.1571 has updated; updating [sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, qu
eueName=llap, user=hive, doAs=false, isOpen=true, isDefault=true, expires in 586277120ms, WM state poolName=medium, clusterFraction=0.3, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killR
eason=null] with an endpoint at 32769
2018-06-18T19:06:50,235 INFO [StateChangeNotificationHandler] tez.TezSessionState: Ignoring an outdated info update 1571: TezAmInstance [49be39e5-875c-4cfe-8601-7fe84dd57e0c, host=ip-10-8-121-231.data.bazaar
voice.com, rpcPort=33365, pluginPort=32769, token=null]
2018-06-18T19:06:50,323 ERROR [TaskCommunicator # 4] tez.GuaranteedTasksAllocator: Failed to update guaranteed tasks count for the session sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=
hive, doAs=false, isOpen=true, isDefault=true, expires in 586277032ms, WM state poolName=medium, clusterFraction=0.3, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killReason=null
com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(java.lang.NullPointerException): java.lang.NullPointerException
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.checkAndSendGuaranteedStateUpdate(LlapTaskSchedulerService.java:596)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateGuaranteedCount(LlapTaskSchedulerService.java:581)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateQuery(LlapTaskSchedulerService.java:3041)
at org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.updateQuery(LlapPluginServerImpl.java:57)
at org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos$LlapPluginProtocol$2.callBlockingMethod(LlapPluginProtocolProtos.java:835)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:242) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at com.sun.proxy.$Proxy111.updateQuery(Unknown Source) ~[?:?]
at org.apache.hadoop.hive.llap.impl.LlapPluginProtocolClientImpl.updateQuery(LlapPluginProtocolClientImpl.java:42) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl$SendUpdateQueryCallable.call(LlapPluginEndpointClientImpl.java:128) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl$SendUpdateQueryCallable.call(LlapPluginEndpointClientImpl.java:105) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) [guava-19.0.jar:?]
at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) [guava-19.0.jar:?]
at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) [guava-19.0.jar:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_171]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
Caused by: org.apache.hadoop.ipc.RemoteException: java.lang.NullPointerException
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.checkAndSendGuaranteedStateUpdate(LlapTaskSchedulerService.java:596)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateGuaranteedCount(LlapTaskSchedulerService.java:581)
at org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.updateQuery(LlapTaskSchedulerService.java:3041)
at org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginServerImpl.updateQuery(LlapPluginServerImpl.java:57)
at org.apache.hadoop.hive.llap.plugin.rpc.LlapPluginProtocolProtos$LlapPluginProtocol$2.callBlockingMethod(LlapPluginProtocolProtos.java:835)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.Client.call(Client.java:1437) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.Client.call(Client.java:1347) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) ~[hadoop-common-3.1.1-SNAPSHOT.2.6.1.0-129.jar:?]
... 11 more
2018-06-18T19:06:50,323 INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-06-18T19:06:50,323 INFO [Workload management master] tez.WorkloadManager: Update failed for sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=hive, doAs=false, isOpen=true, isDefault=true, expires in 586277032ms, WM state poolName=medium, clusterFraction=0.3, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killReason=null
2018-06-18T19:06:50,324 INFO [Workload management master] tez.WorkloadManager: Replacing sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=hive, doAs=false, isOpen=true, isDefault=true, expires in 586277031ms, WM state poolName=null, clusterFraction=null, queryId=hive_20180618190637_e65869b8-10be-4880-a8d3-84989bd055b4, killReason=Failed to update resource allocation with a new session
2018-06-18T19:06:50,325 INFO [Workload management worker 0] tez.WorkloadManager: Created new interactive session object b89aaebf-aeaa-4b76-974a-7047592a186b
2018-06-18T19:06:50,325 INFO [Workload management worker 0] tez.TezSessionState: Closing Tez Session
2018-06-18T19:06:50,325 INFO [Workload management worker 0] client.TezClient: Shutting down Tez Session, sessionName=HIVE-49be39e5-875c-4cfe-8601-7fe84dd57e0c, applicationId=application_1528322657674_0427
2018-06-18T19:06:50,353 INFO [Workload management worker 0] tez.TezSessionState: Attemting to clean up resources for 49be39e5-875c-4cfe-8601-7fe84dd57e0c: hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/49be39e5-875c-4cfe-8601-7fe84dd57e0c-resources; 0 additional files, 2 localized resources
2018-06-18T19:06:50,354 INFO [Workload management worker 0] tez.TezSessionState: User of session id b89aaebf-aeaa-4b76-974a-7047592a186b is hive
2018-06-18T19:06:50,356 INFO [Workload management worker 0] tez.DagUtils: Localizing resource because it does not exist: file:/usr/hdp/current/hive-server2-hive2/lib/hive-hcatalog-core.jar to dest: hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/hive-hcatalog-core.jar
2018-06-18T19:06:50,467 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1529348810466 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/hive-hcatalog-core.jar
2018-06-18T19:06:50,467 INFO [Workload management worker 0] tez.DagUtils: Localizing resource because it does not exist: file:/usr/hdp/3.0.0.0-1064/hive2/auxlib/alluxio-core-client-runtime.jar to dest: hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/alluxio-core-client-runtime.jar
2018-06-18T19:06:50,504 INFO [Thread-250] monitoring.RenderStrategy$LogToFileFunction: Map 1: 361(+0)/641
2018-06-18T19:06:50,785 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1529348810784 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/tmp/hive/hive/_tez_session_dir/b89aaebf-aeaa-4b76-974a-7047592a186b-resources/alluxio-core-client-runtime.jar
2018-06-18T19:06:50,785 INFO [Workload management worker 0] tez.TezSessionState: Created new resources: null
2018-06-18T19:06:50,786 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,788 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767020 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-exec-3.1.0-SNAPSHOT-7db4045ca6fe10361ffcde371b4327bc911d68f174562dd75f00abce0c42fa36.jar
2018-06-18T19:06:50,791 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,792 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767036 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-llap-tez-3.1.0-SNAPSHOT-1530a700e8bc86603cf11cd8d0a2518f21a7a7c2ac09eb3af7782017ec05c4d3.jar
2018-06-18T19:06:50,792 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,793 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767020 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-exec-3.1.0-SNAPSHOT-7db4045ca6fe10361ffcde371b4327bc911d68f174562dd75f00abce0c42fa36.jar
2018-06-18T19:06:50,794 INFO [Workload management worker 0] tez.DagUtils: Jar dir is null / directory doesn't exist. Choosing HIVE_INSTALL_DIR - /user/hive/.hiveJars
2018-06-18T19:06:50,796 INFO [Thread-250] SessionState: Status: Killed
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: Dag received [DAG_TERMINATE, DAG_KILL] in RUNNING state.
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: Received message to shutdown AM from hive (auth:SIMPLE) at 10.8.101.64
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: Vertex killed, vertexName=Map 1, vertexId=vertex_1528322657674_0427_1_00, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to DAG_TERMINATED, failedTasks:0 killedTasks:280, Vertex vertex_1528322657674_0427_1_00 [Map 1] killed/failed due to:DAG_TERMINATED]
2018-06-18T19:06:50,796 ERROR [Thread-250] SessionState: DAG did not succeed due to DAG_KILL. failedVertices:0 killedVertices:1
2018-06-18T19:06:50,797 INFO [Workload management master] tez.WorkloadManager: Processing current events
2018-06-18T19:06:50,797 INFO [Workload management worker 0] tez.DagUtils: Resource modification time: 1525405767020 for hdfs://ip-10-8-101-64.data.bazaarvoice.com:8020/user/hive/.hiveJars/hive-exec-3.1.0-SNAPSHOT-7db4045ca6fe10361ffcde371b4327bc911d68f174562dd75f00abce0c42fa36.jar
2018-06-18T19:06:50,797 INFO [Workload management master] tez.WorkloadManager: Returning sessionId=49be39e5-875c-4cfe-8601-7fe84dd57e0c, queueName=llap, user=hive, doAs=false, isOpen=false, isDefault=true, expires in 586276558ms, WM state poolName=null, clusterFraction=null, queryId=null, killReason=Failed to update resource allocation{noformat}"
HIVE-19927,Last Repl ID set by bootstrap dump is incorrect and may cause data loss if have ACID/MM tables.,"During bootstrap dump of ACID tables, let's consider the below sequence.
- Current session (REPL DUMP), Open txn (Txn1) - Event-10
- Another session (Session-2), Open txn (Txn2) - Event-11
- Session-2 -> Insert data (T1.D1) to ACID table. - Event-12
- Get lastReplId = last event ID logged. (Event-12)
- Session-2 -> Commit Txn (Txn2) - Event-13
- Dump ACID tables based on validTxnList based on Txn1. --> This step skips all the data written by txns > Txn1. So, T1.D1 will be missing.
- Commit Txn (Txn1)
- REPL LOAD from bootstrap dump will skip T1.D1.
- Incremental REPL DUMP will start from Event-13 and hence lose Txn2 which is opened after Txn1. So, data T1.D1 will be lost for ever.

Proposed to capture the lastReplId of bootstrap before opening current txn (Txn1) and store it in Driver context and use it for dump."
HIVE-19924,Tag distcp jobs run by Repl Load,"Add tags in jobconf for distcp related jobs started by replication. This will allow hive to kill these jobs in case beacon retries, or hs2 dies and beacon issues a kill command.
 * one of the tags should definitely be the query_id that starts the job : With this flow beacon before retrying the bootstrap load, will issue a kill command to hs2 with the query id of the previous issued command. hs2 will then kill an running jobs on yarn tagged with the Query_id.

 * To get around the additional failure point as mentioned above. The jobs can be tagged with an additional unique tag_id provided by Beacon in the WITH clause in repl load command to be used to tag distcp jobs ). Enhance the kill api to take the tag as input and kill jobs associated with that tag. Problem here is how do we validate the association of the tag with a hive query id to make sure this api is not used to kill jobs run by other components, however we can provide this capability to only admins and should be ok in that case."
HIVE-19902,Provide Metastore micro-benchmarks,It would be very useful to have metastore benchmarks to be able to track perf issues.
HIVE-19886,Logs may be directed to 2 files if --hiveconf hive.log.file is used,hive launch script explicitly specific log4j2 configuration file to use. The main() methods in HiveServer2 and HiveMetastore reconfigures the logger based on user input via --hiveconf hive.log.file. This may cause logs to end up in 2 different files. Initial logs goes to the file specified in hive-log4j2.properties and after logger reconfiguration the rest of the logs goes to the file specified via --hiveconf hive.log.file. 
HIVE-19884,Invalidation cache may throw NPE when there is no data in table used by materialized view,
HIVE-19881,Allow metadata-only dump for database which are not source of replication,If the dump is meta data only then allow dump even if the db is not source of replication
HIVE-19880,Repl Load to return recoverable vs non-recoverable error codes,"To enable bootstrap of large databases, application has to have the ability to keep retrying the bootstrap load till it encounters a fatal error. The ability to identify if an error is fatal or not will be decided by hive and communication of the same will happen to application via error codes.

So there should be different error codes for recoverable vs non-recoverable failures which should be propagated to application as part of running the repl load command."
HIVE-19877,Remove setting hive.execution.engine as mr in HiveStreamingConnection,HiveStreamingConnection explicitly sets execution engine to mr which was from old code. It is no longer required. 
HIVE-19876,Multiple fixes for Driver.isValidTxnListState,"1) Only locks for tables should be checked.
2) Check on whether {{txnWriteIdList}} is null or not needs to be fixed: it should be done on the configuration property value, as the object {{txnWriteIdList}} will never be null."
HIVE-19875,increase LLAP IO queue size for perf,"According to [~gopalv] queue limit has perf impact, esp. during hashtable load for mapjoin where in the past IO used to queue up more data for processing to process.
1) Overall the default limit could be adjusted higher.
2) Depending on Decimal64 availability, the weight for decimal columns could be reduced."
HIVE-19873,Cleanup operation log on query cancellation after some delay,"When a query is executed using beeline and the query is cancelled due to query timeout or kill query or triggers and when there is cursor on operation log row set, the cursor can thrown an exception as cancel will cleanup the operation log in the background. This can return a non-zero exit code in beeline. Query cancellation on success should return exit code 0.

Adding a delay to the cleanup of operation logging in operation cancel can avoid the close during read. "
HIVE-19864,Address TestTriggersWorkloadManager flakiness,TestTriggersWorkloadManager seems flaky and all test cases gets timed out at times. 
HIVE-19857,Set 3.1.0 for sys db version,
HIVE-19852,update jackson to latest,Update jackson version to latest 2.9.5
HIVE-19851,upgrade jQuery version,jQuery version seems to be very old. Update to latest stable version. 
HIVE-19829,Incremental replication load should create tasks in execution phase rather than semantic phase,Split the incremental load into multiple iterations. In each iteration create number of tasks equal to the configured value.
HIVE-19827,hiveserver2 startup should provide a way to override TEZ_CONF_DIR,"HS2 should use /etc/tez/conf, HSI should use /etc/tez_llap/conf "
HIVE-19817,Hive streaming API + dynamic partitioning + json/regex writer does not work,New streaming API for dynamic partitioning only works with delimited record writer. Json and Regex writers does not work.
HIVE-19815,Repl dump should not propagate the checkpoint and repl source properties,"For replication scenarios of A-> B -> C the repl dump on B should not include the checkpoint property when dumping out table information. 
Alter tables/partitions during incremental should not propagate this as well.
Also should not propagate the the db level parameters set by replication internally."
HIVE-19812,Disable external table replication by default via a configuration property,"use a hive config property to allow external table replication. set this property by default to prevent external table replication.

for metadata only hive repl always export metadata for external tables.

 

REPL_DUMP_EXTERNAL_TABLES(""hive.repl.dump.include.external.tables"", false,
""Indicates if repl dump should include information about external tables. It should be \n""
+ ""used in conjunction with 'hive.repl.dump.metadata.only' set to false. if 'hive.repl.dump.metadata.only' \n""
+ "" is set to true then this config parameter has no effect as external table meta data is flushed \n""
+ "" always by default."")

This should be done for only replication dump and not for export"
HIVE-19801,JDBC: Add some missing classes to jdbc standalone jar and remove hbase classes,
HIVE-19799,remove jasper dependency,jasper dependency version looks old and unwanted. There is a comment which says it is required by thrift but I don't see jasper as thrift dependency. Try removing it to see if its safe (after precommit test run). 
HIVE-19794,Disable removing order by from subquery in GenericUDTFGetSplits,"spark-llap always wraps query under a subquery, until that is removed from spark-llap
hive compiler is going to remove inner order by in GenericUDTFGetSplits. disable that optimization until then."
HIVE-19773,CBO exception while running queries with tables that are not present in materialized views,"When we obtain the valid list of write ids, some tables in the materialized views may not be present in the list because they are not present in the query, which leads to exceptions (hidden in logs) when we try to load the materialized views in the planner, as we need to verify whether they are outdated or not."
HIVE-19772,Streaming ingest V2 API can generate invalid orc file if interrupted,"Hive streaming ingest generated 0 length and 3 byte files which are invalid orc files. This will throw the following exception during compaction

{code}
Error: org.apache.orc.FileFormatException: Not a valid ORC file hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/culvert/year=2018/month=7/delta_0000025_0000025/bucket_00005 (maxFileLength= 3) at org.apache.orc.impl.ReaderImpl.extractFileTail(ReaderImpl.java:546) at org.apache.orc.impl.ReaderImpl.<init>(ReaderImpl.java:370) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.<init>(ReaderImpl.java:60) at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:90) at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:1124) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRawReader(OrcInputFormat.java:2373) at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:1000) at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:977) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:460) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:344) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
{code}"
HIVE-19731,Change staging tmp directory used by TestHCatLoaderComplexSchema,"Another one that is set to default and hence is flaky.

https://builds.apache.org/job/PreCommit-HIVE-Build/11321/testReport/org.apache.hive.hcatalog.pig/TestHCatLoaderComplexSchema/testSyntheticComplexSchema_3_/

{noformat}
org.apache.hadoop.util.Shell$ExitCodeException: chmod: cannot access ‘/tmp/hadoop/mapred/staging/hiveptest985275899/.staging/job_local985275899_0088’: No such file or directory

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:902) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1321) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1303) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:840) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:508) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:489) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:511) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:727) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.mkdirs(JobResourceUploader.java:658) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:172) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:133) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_102]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_102]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:336) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at org.apache.pig.backend.hadoop23.PigJobControl.submit(PigJobControl.java:128) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:194) [pig-0.16.0-h2.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276) [pig-0.16.0-h2.jar:?]
{noformat}"
HIVE-19726,ORC date PPD is broken,"When kryo was in version 2.22 we added a fix in HIVE-7222 and later in HIVE-10819. Now that we have updated kryo to 3.0.3 that old workaround fix was never removed. The issue was that kryo serialized Timestamp to Date type. So to recover the timestamp, during deserialization we deserialized *any* date instance to Timestamp object which is wrong (we don't know if date was serialized as date or timestamp serialized as date in first place). This breaks PPD on date time as kryo deserialization always converts Date to Timestamp breaking PPD because of type mismatch.
Now that we have newer kryo version we can remove the code added in HIVE-10819.   "
HIVE-19725,Add ability to dump non-native tables in replication metadata dump,"if hive.repl.dump.metadata.only is set to true, allow dumping non native tables also. 

Data dump for non-native tables should never be allowed."
HIVE-19708,Repl copy retrying with cm path even if the failure is due to network issue,"* During repl load
 ** for filesystem based copying of file if the copy fails due to a connection error to source Name Node, we should recreate the filesystem object.
 ** the retry logic for local file copy should be triggered using the original source file path ( and not the CM root path ) since failure can be due to network issues between DFSClient and NN.

 * When listing files in tables / partition to include them in _files, we should add retry logic when failure occurs. FileSystem object here also should be recreated since the existing one might be in inconsistent state."
HIVE-19707,Enable TestJdbcWithMiniHS2#testHttpRetryOnServerIdleTimeout,
HIVE-19706,Disable TestJdbcWithMiniHS2#testHttpRetryOnServerIdleTimeout,"https://builds.apache.org/job/PreCommit-HIVE-Build/11190/testReport/junit/org.apache.hive.jdbc/TestJdbcWithMiniHS2/testHttpRetryOnServerIdleTimeout/history/

It seems to timeout sporadically. I will ignore specifically that test."
HIVE-19699,Re-enable TestReOptimization,https://builds.apache.org/job/PreCommit-HIVE-Build/11180/testReport/junit/org.apache.hadoop.hive.ql.plan.mapping/TestReOptimization/testStatCachingMetaStore/
HIVE-19698,TestAMReporter#testMultipleAM is flaky,"https://builds.apache.org/job/PreCommit-HIVE-Build/11184/testReport/junit/org.apache.hadoop.hive.llap.daemon.impl.comparator/TestAMReporter/testMultipleAM/

It timeouts sporadically:
https://builds.apache.org/job/PreCommit-HIVE-Build/11184/testReport/junit/org.apache.hadoop.hive.llap.daemon.impl.comparator/TestAMReporter/testMultipleAM/history/
"
HIVE-19697,TestReOptimization#testStatCachingMetaStore is flaky,https://builds.apache.org/job/PreCommit-HIVE-Build/11180/testReport/junit/org.apache.hadoop.hive.ql.plan.mapping/TestReOptimization/testStatCachingMetaStore/
HIVE-19677,Disable sample6.q,"Flaky test, already found similar behavior with sample2.q and sample4.q (HIVE-19657). More info to reproduce and try to fix the issue in HIVE-19673."
HIVE-19669,Upgrade ORC to 1.5.1,
HIVE-19654,Change tmp staging mapred directory for TestBlobstoreCliDriver,Similar to HIVE-19626.
HIVE-19652,Incorrect predicate pushdown for groupby with grouping sets,
HIVE-19641,sync up hadoop version used by storage-api with hive,There is hadoop version mismatch between hive and storage-api and hence different transitive dependency versions gets pulled.
HIVE-19637,Add slow test report script to testutils,"Wrote the attached utility script to find top K slow tests from precommit test url. Would like to get that committed to testutils so that its useful for everyone.

{code:title=ascii mode}
$ python gen-report.py -b 11102 -a

Processing 1073 test xml reports from http://104.198.109.242/logs/PreCommit-HIVE-Build-11102/test-results/..

Top 25 testsuite in terms of execution time (in seconds).. [Total time: 73882.661 seconds]

##########################################################################################################
██████████████████████████████████████████████████  20806  TestCliDriver
███████████████████████                              9601  TestMiniLlapLocalCliDriver
███████████████████                                  8210  TestSparkCliDriver
██████                                               2744  TestMinimrCliDriver
█████                                                2262  TestEncryptedHDFSCliDriver
████                                                 2021  TestMiniSparkOnYarnCliDriver
████                                                 1808  TestHiveCli
███                                                  1566  TestMiniLlapCliDriver
███                                                  1345  TestReplicationScenarios
██                                                   1238  TestMiniDruidCliDriver
██                                                    940  TestNegativeCliDriver
██                                                    865  TestHBaseCliDriver
█                                                     681  TestMiniTezCliDriver
█                                                     555  TestTxnCommands2WithSplitUpdateAndVectorization
█                                                     543  TestCompactor
█                                                     528  TestTxnCommands2
                                                      378  TestStreaming
                                                      374  TestBlobstoreCliDriver
                                                      328  TestNegativeMinimrCliDriver
                                                      302  TestTxnCommandsWithSplitUpdateAndVectorization
                                                      301  TestHCatClient
                                                      299  TestTxnCommands
                                                      261  TestTxnLoadData
                                                      258  TestAcidOnTez
                                                      240  TestHBaseNegativeCliDriver

Top 25 testcases in terms of execution time (in seconds).. [Total time: 63102.607 seconds]

###############################################################################################################################################
██████████████████████████████████████████████████  680  TestMinimrCliDriver_testCliDriver[infer_bucket_sort_reducers_power_two]
█████████████████████████████████████████████       623  TestMinimrCliDriver_testCliDriver[infer_bucket_sort_map_operators]
███████████████████████████████                     429  TestMinimrCliDriver_testCliDriver[infer_bucket_sort_dyn_part]
███████████████████████████                         374  TestSparkCliDriver_testCliDriver[vectorization_short_regress]
███████████████████████████                         374  TestMiniLlapLocalCliDriver_testCliDriver[vectorization_short_regress]
████████████████████████                            330  TestMiniDruidCliDriver_testCliDriver[druidmini_dynamic_partition]
█████████████████                                   238  TestMiniLlapLocalCliDriver_testCliDriver[vector_outer_join5]
████████████████                                    227  TestMiniDruidCliDriver_testCliDriver[druidmini_test_insert]
███████████████                                     214  TestEncryptedHDFSCliDriver_testCliDriver[encryption_auto_purge_tables]
███████████████                                     211  TestMiniLlapCliDriver_testCliDriver[unionDistinct_1]
███████████████                                     210  TestMiniSparkOnYarnCliDriver_testCliDriver[vector_outer_join5]
███████████████                                     206  TestMinimrCliDriver_testCliDriver[bucket_num_reducers_acid]
██████████████                                      202  TestMinimrCliDriver_testCliDriver[infer_bucket_sort_merge]
██████████████                                      198  TestCliDriver_testCliDriver[typechangetest]
████████████                                        172  TestEncryptedHDFSCliDriver_testCliDriver[encryption_drop_table]
████████████                                        164  TestMinimrCliDriver_testCliDriver[infer_bucket_sort_num_buckets]
███████████                                         158  TestCliDriver_testCliDriver[mm_all]
███████████                                         155  TestMiniSparkOnYarnCliDriver_testCliDriver[spark_dynamic_partition_pruning]
██████████                                          145  TestMiniSparkOnYarnCliDriver_testCliDriver[spark_vectorized_dynamic_partition_pruning]
██████████                                          141  TestMiniLlapCliDriver_testCliDriver[mm_all]
██████████                                          140  TestMiniDruidCliDriver_testCliDriver[druidmini_mv]
██████████                                          137  TestSparkCliDriver_testCliDriver[auto_join_filters]
█████████                                           135  TestMiniLlapLocalCliDriver_testCliDriver[vector_outer_join3]
█████████                                           124  TestMiniSparkOnYarnCliDriver_testCliDriver[vector_outer_join3]
████████                                            121  TestCliDriver_testCliDriver[type_change_test_int_vectorized]
{code}

{code:title=text mode}
$ python gen-report.py -b 11102

Processing 1073 test xml reports from http://104.198.109.242/logs/PreCommit-HIVE-Build-11102/test-results/..

Top 25 testsuite in terms of execution time (in seconds).. [Total time: 73882.661 seconds]
TestCliDriver	20805.579
TestMiniLlapLocalCliDriver	9601.362
TestSparkCliDriver	8210.062
TestMinimrCliDriver	2743.746
TestEncryptedHDFSCliDriver	2261.866
TestMiniSparkOnYarnCliDriver	2021.468
TestHiveCli	1807.56
TestMiniLlapCliDriver	1565.858
TestReplicationScenarios	1345.344
TestMiniDruidCliDriver	1237.776
TestNegativeCliDriver	940.321
TestHBaseCliDriver	864.707
TestMiniTezCliDriver	681.457
TestTxnCommands2WithSplitUpdateAndVectorization	555.382
TestCompactor	543.173
TestTxnCommands2	527.84
TestStreaming	378.325
TestBlobstoreCliDriver	374.134
TestNegativeMinimrCliDriver	328.128
TestTxnCommandsWithSplitUpdateAndVectorization	301.718
TestHCatClient	301.324
TestTxnCommands	298.694
TestTxnLoadData	260.841
TestAcidOnTez	258.262
TestHBaseNegativeCliDriver	240.198

Top 25 testcases in terms of execution time (in seconds).. [Total time: 63102.607 seconds]
TestMinimrCliDriver_testCliDriver[infer_bucket_sort_reducers_power_two]	680.326
TestMinimrCliDriver_testCliDriver[infer_bucket_sort_map_operators]	623.404
TestMinimrCliDriver_testCliDriver[infer_bucket_sort_dyn_part]	429.358
TestSparkCliDriver_testCliDriver[vectorization_short_regress]	374.491
TestMiniLlapLocalCliDriver_testCliDriver[vectorization_short_regress]	374.164
TestMiniDruidCliDriver_testCliDriver[druidmini_dynamic_partition]	329.945
TestMiniLlapLocalCliDriver_testCliDriver[vector_outer_join5]	238.5
TestMiniDruidCliDriver_testCliDriver[druidmini_test_insert]	226.773
TestEncryptedHDFSCliDriver_testCliDriver[encryption_auto_purge_tables]	214.09
TestMiniLlapCliDriver_testCliDriver[unionDistinct_1]	210.803
TestMiniSparkOnYarnCliDriver_testCliDriver[vector_outer_join5]	209.894
TestMinimrCliDriver_testCliDriver[bucket_num_reducers_acid]	206.339
TestMinimrCliDriver_testCliDriver[infer_bucket_sort_merge]	201.754
TestCliDriver_testCliDriver[typechangetest]	198.371
TestEncryptedHDFSCliDriver_testCliDriver[encryption_drop_table]	172.267
TestMinimrCliDriver_testCliDriver[infer_bucket_sort_num_buckets]	163.617
TestCliDriver_testCliDriver[mm_all]	158.401
TestMiniSparkOnYarnCliDriver_testCliDriver[spark_dynamic_partition_pruning]	155.255
TestMiniSparkOnYarnCliDriver_testCliDriver[spark_vectorized_dynamic_partition_pruning]	145.481
TestMiniLlapCliDriver_testCliDriver[mm_all]	141.369
TestMiniDruidCliDriver_testCliDriver[druidmini_mv]	139.815
TestSparkCliDriver_testCliDriver[auto_join_filters]	137.391
TestMiniLlapLocalCliDriver_testCliDriver[vector_outer_join3]	135.227
TestMiniSparkOnYarnCliDriver_testCliDriver[vector_outer_join3]	124.469
TestCliDriver_testCliDriver[type_change_test_int_vectorized]	120.85
{code}"
HIVE-19633,Remove/Migrate Minimr tests,MR has been deprecated for a long time. Minimr tests are incredibly slow. We should remove the tests or migrate to faster options for coverage. 
HIVE-19632,Remove webapps directory from standalone jar,JDBC standalone jar contains webapps static files which just adds to the jar size and are not required by the clients. 
HIVE-19629,Enable Decimal64 reader after orc version upgrade,ORC 1.5.0 supports new fast decimal 64 reader. New VRB has to be created for making use of decimal 64 column vectors. Also LLAP IO will need a new reader to reader from long stream to decimal 64. 
HIVE-19620,Change tmp directory used by PigServer in HCat tests,
HIVE-19617,Rename test tables to avoid collisions during execution in batches,
HIVE-19616,Enable TestAutoPurgeTables test,Disabled by HIVE-19589.
HIVE-19614,GenericUDTFGetSplits does not honor ORDER BY,GenericUDTFGetSplits handles ORDER BY by writing the results to temp table. However running select * on that temp table may create >1 splits which will lose the original ordering. 
HIVE-19613,GenericUDTFGetSplits should handle fetch task with temp table rewrite,"GenericUDTFGetSplits fails for fetch task only queries. Fetch task only queries can be handled same way as >1 task queries using temp tables. 
{code:java}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Was expecting a single TezTask.
at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.createPlanFragment(GenericUDTFGetSplits.java:262)
at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.process(GenericUDTFGetSplits.java:201)
at org.apache.hadoop.hive.ql.exec.UDTFOperator.process(UDTFOperator.java:116)
at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:984)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:930)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:917)
at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
at org.apache.hadoop.hive.ql.exec.Operator.baseForward(Operator.java:984)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:930)
at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:125)
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:492)
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:484)
at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:145)
... 16 more{code}"
HIVE-19612,Add option to mask lineage in q files,"Similar to {{HIVE-19572}}, with {{-- MASK_LINEAGE}}."
HIVE-19588,Several invocation of file listing when creating VectorizedOrcAcidRowBatchReader,"Looks like we are doing file listing several times when creating one instance of VectorizedOrcAcidRowBatchReader
 AcidUtils.parseBaseOrDeltaBucketFilename() does full file listing (when there are files with bucket_* prefix) just to get a single file out of a path to figure out if it has ACID schema (as part of HIVE-18190).
 There is full file listing where we populate
 1) ColumnizedDeleteEventRegistry
 2) SortMergedDeleteEventRegistry
 3) Twice in computeOffsetAndBucket()

 

Attaching profiles which [~gopalv] took while debugging. "
HIVE-19575,TestAutoPurgeTables seems flaky,I cannot reproduce the flakiness locally. Maybe we can retry this flaky test using RetryTestRunner. 
HIVE-19573,Fix flaky TestMiniLlapLocalCliDriver#explainuser_4.q,
HIVE-19572,Add option to mask stats and data size in q files,"Some tests are flaky because of minimal data size differences, e.g., one byte. However, many times we do not actually care about these differences. One example is {{default_constraint.q}}.

Patch adds the possibility to mask 1) printing of stats selectively on q files by adding the {{-- MASK_STATS}} option, and 2) printing of data size stats selectively on q files by adding the {{-- MASK_DATA_SIZE}} option."
HIVE-19568,Active/Passive HS2 HA: Disallow direct connection to passive HS2 instance,"The recommended usage for clients when connecting to HS2 with Active/Passive HA configuration is via ZK service discovery URL. But some applications do not support ZK service discovery in which case they use direct URL to connect to HS2 instance. If direct connection is to passive HS2 instance, the connection should be dropped with proper error message. "
HIVE-19567,Fix flakiness in TestTriggers,Identified another flakiness in TestTriggersMoveWorkloadManager which can cause intermittent test failures. 
HIVE-19560,Retry test runner and retry rule for flaky tests,"Implement custom test runner that retries failed tests as a workaround for flakiness. Also a test rule for retrying failed tests (for cases where custom test runner is not possible, e.g ParametrizedTests which already is a customer TestRunner). "
HIVE-19552,Enable TestMiniDruidKafkaCliDriver#druidkafkamini_basic.q,"The failure was caused by the following sequence of steps - 
# Test queries for available hosts where a segment is located and gets the location of kafka task. 
# Kafka task hands over the data and finishes
# Now the scan query is sent to the kafka task, but the task has already completed and will fail. 

https://issues.apache.org/jira/browse/HIVE-20349 fixes this issue by retrying the broker in this case. 

One more cause of failure was the latestOffsets and minimumLag not reported when there is no task. 
This patch masks those two values also. Query results are verified to ensure that there is no lag. 
"
HIVE-19551,Enable TestBeeLineWithArgs#testQueryProgress and TestBeeLineWithArgs#testQueryProgressParallel,
HIVE-19545,Enable TestCliDriver#fouter_join_ppr.q,
HIVE-19534,Allow implementations to access member variables of AbstractRecordWriter,"The AbstractRecordWriter class in the Hive 3 Streaming API (package org.apache.hive.streaming) provides common functionality for processing incoming records (each as a byte[]) where subclasses often need only to implement the encode() and createSerde() methods and let AbstractRecordWriter handle the rest.

However for some custom RecordWriters, the records may not be available as a byte array, and thus the custom RecordWriter may need to handle the writes and the ""paperwork"" such as connection stats updates (number of records written, e.g.), basically the same code that is in the write(long, byte[]) method. To do that, the subclass will need access to the member variables of AbstractRecordWriter, which are currently private. The same likely holds for the private methods.

This Jira proposes to make the member variables and methods of AbstractRecordWriter protected (or package-protected) as prudent. "
HIVE-19512,"If parallel execution is enabled, metastore is throwing out of sequence error.","The move task does meta store operations. If the meta hive object is shared between Move tasks, then meta store throwing out of sequence error. So each move task should have a hive object of its own. "
HIVE-19509,Disable tests that are failing continuously,"As per discussion in mailing list. We will bring the failure count to zero and be less tolerant with ptest runs that are not clean.

- TestSequenceFileReadWrite#testSequenceTableWriteReadMR and TestSequenceFileReadWrite#testTextTableWriteReadMR have been disabled (HIVE-19506).
- TestNegativeCliDriver: merge_negative_5.q and mm_concatenate.q have been disabled (HIVE-19517).
- TestMiniLlapLocalCliDriver: bucket_map_join_tez1.q (diff), sysdb.q (diff), special_character_in_tabnames_1.q (failure), tez_smb_1.q (diff), union_fast_stats.q (diff), schema_evol_orc_acidvec_part.q, schema_evol_orc_vec_part_llap_io.q, tez_dynpart_hashjoin_1.q (diff), and tez_vector_dynpart_hashjoin_1.q (diff) have been disabled.
- TestCliDriver: fouter_join_ppr.q has been disabled.
- TestStats#partitionedTableDeprecatedCalls, TestStats#partitionedTableInHiveCatalog, and TestStats#partitionedTableOtherCatalog have been disabled.
- TestOldSchema#testPartitionOps has been disabled.
- TestSSL#testSSLFetchHttp has been disabled.
- TestAcidOnTez#testCtasTezUnion and TestAcidOnTez#testNonStandardConversion01 have been disabled.
- TestBeeLineWithArgs#testQueryProgress and TestBeeLineWithArgs#testQueryProgressParallel have been disabled.
- TestMiniDruidKafkaCliDriver : druidkafkamini_basic.q has been disabled.
- TestTxnExIm#testUpgrade has been disabled.

- TestDanglingQOuts.checkDanglingQOut has been disabled till we fix all the previous test errors, since I did not want to delete the dangling q.out files (they may serve as a reference)."
HIVE-19503,Create a test that checks for dropPartitions with directSql,"As a followup jira, it might be good to check that every RDBMS table is empty after a dropTable happened with DirectSQL

Also checking the JDO <-> DirectSQL cache handling"
HIVE-19500,Prevent multiple selectivity estimations for the same variable in conjuctions,"see HIVE-19097 for problem description

for filters like: {{(d_year in (2001,2002) and d_year = 2001)}} the current estimation is around {{(1/NDV)**2}} (iff column stats are available) ....

actually the source of the problem was a small typo in HIVE-17465 "
HIVE-19494,Accept shade prefix during reflective instantiation of output format,"Hive Streaming API jars are sometimes shaded with a different prefix when used in environments where another version of hive already exists (spark for example). In most cases, shading is done with rename of classes with some prefix. If an uber/assembly jar is generated with renamed prefix, Hive Streaming API will not work as Hive Streaming API will reflectively instantiate outputformat class using FQCN string provided by metastore table storage descriptor object. 
For example: 
RecordWriter will create instance of OutputFormat using string ""org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat"". When a shaded jar with renamed class references are used, this class will not be found by the classloader. 

We can optionally accept a shade prefix from user via config which will be tried (as fallback) when ClassNotFoundException is thrown."
HIVE-19488,"Enable CM root based on db parameter, identifying a db as source of replication.","* add a parameter at db level to identify if its a source of replication. user should set this.

 * Enable CM root only for databases that are a source of a replication policy, for other db's skip the CM root functionality.

 * prevent database drop if the parameter indicating its source of a replication, is set.

 * as an upgrade to this version, user should set the property on all existing database policies, in affect.

 * the parameter should be of the form . –  repl.source.for : List < policy ids >"
HIVE-19485,dump directory for non native tables should not be created,
HIVE-19472,HiveStreamingConnection swallows exception on partition creation,HiveStreamingConnection swallows exception on partition creation
HIVE-19468,Add Apache license to TestTxnConcatenate,
HIVE-19415,Support CORS for all HS2 web endpoints,"HIVE-19277 changes alone are not sufficient to support CORS. CrossOriginFilter has to be added to jetty which will serve appropriate response for OPTIONS pre-flight request. 

"
HIVE-19389,"Schematool: For Hive's Information Schema, use embedded HS2 as default","Currently, for initializing/upgrading Hive's information schema, we require a full jdbc url (for HS2). It will be good to have it connect using embedded HS2 by default."
HIVE-19388,ClassCastException during VectorMapJoinCommonOperator initialization,"I see the following exceptions when I a mapjoin operator is being initialized on Hive-on-Spark and when vectorization is turned on.

This happens when the hashTable is empty. The code in {{MapJoinTableContainerSerDe#getDefaultEmptyContainer}} method returns a HashMapWrapper while the VectorMapJoinOperator expects a {{MapJoinBytesTableContainer}} when {{hive.mapjoin.optimized.hashtable}} is set to true.

{noformat}

Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper cannot be cast to org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerDirectAccess
 at org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashTable.<init>(VectorMapJoinOptimizedHashTable.java:92) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedHashMap.<init>(VectorMapJoinOptimizedHashMap.java:127) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedStringHashMap.<init>(VectorMapJoinOptimizedStringHashMap.java:60) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedCreateHashTable.createHashTable(VectorMapJoinOptimizedCreateHashTable.java:80) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.setUpHashTable(VectorMapJoinCommonOperator.java:485) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.completeInitializationOp(VectorMapJoinCommonOperator.java:461) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Operator.completeInitialization(Operator.java:471) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:401) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:574) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:526) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:387) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:109) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT]
 ... 16 more

{noformat}"
HIVE-19385,Optional hive env variable to redirect bin/hive to use Beeline,"With beeline-site and beeline-user-site, the user can easily specify default hs2 urls to connect. We can use an optional env variable, which when set, will enable bin/hive to use beeline."
HIVE-19366,Vectorization causing TestStreaming.testStreamBucketingMatchesRegularBucketing to fail,Disabled vectorization for TestStreaming#testStreamBucketingMatchesRegularBucketing test case in HIVE-19211 as it is giving incorrect results (the issue is mostly related to wrong table directory location which returns 0 splits). 
HIVE-19360,"CBO: Add an ""optimizedSQL"" to QueryPlan object ","Calcite RelNodes can be converted back into SQL (as the new JDBC storage handler does), which allows Hive to print out the post CBO plan as a SQL query instead of having to guess the join orders from the subsequent Tez plan.

The query generated might not be always valid SQL at this point, but is a world ahead of DAG plans in readability.

Eg. tpc-ds Query4 CTEs gets expanded to

{code}
SELECT t16.$f3 customer_preferred_cust_flag
FROM
  (SELECT t0.c_customer_id $f0,
                           SUM((t2.ws_ext_list_price - t2.ws_ext_wholesale_cost - t2.ws_ext_discount_amt + t2.ws_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) $f8
   FROM
     (SELECT c_customer_sk,
             c_customer_id,
             c_first_name,
             c_last_name,
             c_preferred_cust_flag,
             c_birth_country,
             c_login,
             c_email_address
      FROM default.customer
      WHERE c_customer_sk IS NOT NULL
        AND c_customer_id IS NOT NULL) t0
   INNER JOIN (
                 (SELECT ws_sold_date_sk,
                         ws_bill_customer_sk,
                         ws_ext_discount_amt,
                         ws_ext_sales_price,
                         ws_ext_wholesale_cost,
                         ws_ext_list_price
                  FROM default.web_sales
                  WHERE ws_bill_customer_sk IS NOT NULL
                    AND ws_sold_date_sk IS NOT NULL) t2
               INNER JOIN
                 (SELECT d_date_sk,
                         CAST(2002 AS INTEGER) d_year
                  FROM default.date_dim
                  WHERE d_year = 2002
                    AND d_date_sk IS NOT NULL) t4 ON t2.ws_sold_date_sk = t4.d_date_sk) ON t0.c_customer_sk = t2.ws_bill_customer_sk
   GROUP BY t0.c_customer_id,
            t0.c_first_name,
            t0.c_last_name,
            t0.c_preferred_cust_flag,
            t0.c_birth_country,
            t0.c_login,
            t0.c_email_address) t7
INNER JOIN (
              (SELECT t9.c_customer_id $f0,
                                       t9.c_preferred_cust_flag $f3,
                                                                SUM((t11.ss_ext_list_price - t11.ss_ext_wholesale_cost - t11.ss_ext_discount_amt + t11.ss_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) $f8
               FROM
                 (SELECT c_customer_sk,
                         c_customer_id,
                         c_first_name,
                         c_last_name,
                         c_preferred_cust_flag,
                         c_birth_country,
                         c_login,
                         c_email_address
                  FROM default.customer
                  WHERE c_customer_sk IS NOT NULL
                    AND c_customer_id IS NOT NULL) t9
               INNER JOIN (
                             (SELECT ss_sold_date_sk,
                                     ss_customer_sk,
                                     ss_ext_discount_amt,
                                     ss_ext_sales_price,
                                     ss_ext_wholesale_cost,
                                     ss_ext_list_price
                              FROM default.store_sales
                              WHERE ss_customer_sk IS NOT NULL
                                AND ss_sold_date_sk IS NOT NULL) t11
                           INNER JOIN
                             (SELECT d_date_sk,
                                     CAST(2002 AS INTEGER) d_year
                              FROM default.date_dim
                              WHERE d_year = 2002
                                AND d_date_sk IS NOT NULL) t13 ON t11.ss_sold_date_sk = t13.d_date_sk) ON t9.c_customer_sk = t11.ss_customer_sk
               GROUP BY t9.c_customer_id,
                        t9.c_first_name,
                        t9.c_last_name,
                        t9.c_preferred_cust_flag,
                        t9.c_birth_country,
                        t9.c_login,
                        t9.c_email_address) t16
            INNER JOIN (
                          (SELECT t18.c_customer_id $f0,
                                                    SUM((t20.cs_ext_list_price - t20.cs_ext_wholesale_cost - t20.cs_ext_discount_amt + t20.cs_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) $f8
                           FROM
                             (SELECT c_customer_sk,
                                     c_customer_id,
                                     c_first_name,
                                     c_last_name,
                                     c_preferred_cust_flag,
                                     c_birth_country,
                                     c_login,
                                     c_email_address
                              FROM default.customer
                              WHERE c_customer_sk IS NOT NULL
                                AND c_customer_id IS NOT NULL) t18
                           INNER JOIN (
                                         (SELECT cs_sold_date_sk,
                                                 cs_bill_customer_sk,
                                                 cs_ext_discount_amt,
                                                 cs_ext_sales_price,
                                                 cs_ext_wholesale_cost,
                                                 cs_ext_list_price
                                          FROM default.catalog_sales
                                          WHERE cs_bill_customer_sk IS NOT NULL
                                            AND cs_sold_date_sk IS NOT NULL) t20
                                       INNER JOIN
                                         (SELECT d_date_sk,
                                                 CAST(2002 AS INTEGER) d_year
                                          FROM default.date_dim
                                          WHERE d_year = 2002
                                            AND d_date_sk IS NOT NULL) t22 ON t20.cs_sold_date_sk = t22.d_date_sk) ON t18.c_customer_sk = t20.cs_bill_customer_sk
                           GROUP BY t18.c_customer_id,
                                    t18.c_first_name,
                                    t18.c_last_name,
                                    t18.c_preferred_cust_flag,
                                    t18.c_birth_country,
                                    t18.c_login,
                                    t18.c_email_address) t25
                        INNER JOIN
                          (SELECT t27.c_customer_id $f0,
                                                    SUM((t29.ss_ext_list_price - t29.ss_ext_wholesale_cost - t29.ss_ext_discount_amt + t29.ss_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) $f8
                           FROM
                             (SELECT c_customer_sk,
                                     c_customer_id,
                                     c_first_name,
                                     c_last_name,
                                     c_preferred_cust_flag,
                                     c_birth_country,
                                     c_login,
                                     c_email_address
                              FROM default.customer
                              WHERE c_customer_sk IS NOT NULL
                                AND c_customer_id IS NOT NULL) t27
                           INNER JOIN (
                                         (SELECT ss_sold_date_sk,
                                                 ss_customer_sk,
                                                 ss_ext_discount_amt,
                                                 ss_ext_sales_price,
                                                 ss_ext_wholesale_cost,
                                                 ss_ext_list_price
                                          FROM default.store_sales
                                          WHERE ss_customer_sk IS NOT NULL
                                            AND ss_sold_date_sk IS NOT NULL) t29
                                       INNER JOIN
                                         (SELECT d_date_sk,
                                                 CAST(2001 AS INTEGER) d_year
                                          FROM default.date_dim
                                          WHERE d_year = 2001
                                            AND d_date_sk IS NOT NULL) t31 ON t29.ss_sold_date_sk = t31.d_date_sk) ON t27.c_customer_sk = t29.ss_customer_sk
                           GROUP BY t27.c_customer_id,
                                    t27.c_first_name,
                                    t27.c_last_name,
                                    t27.c_preferred_cust_flag,
                                    t27.c_birth_country,
                                    t27.c_login,
                                    t27.c_email_address
                           HAVING SUM((t29.ss_ext_list_price - t29.ss_ext_wholesale_cost - t29.ss_ext_discount_amt + t29.ss_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) > 0) t35 ON t25.$f0 = t35.$f0) ON t16.$f0 = t35.$f0
            INNER JOIN
              (SELECT t37.c_customer_id $f0,
                                        SUM((t39.cs_ext_list_price - t39.cs_ext_wholesale_cost - t39.cs_ext_discount_amt + t39.cs_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) $f8
               FROM
                 (SELECT c_customer_sk,
                         c_customer_id,
                         c_first_name,
                         c_last_name,
                         c_preferred_cust_flag,
                         c_birth_country,
                         c_login,
                         c_email_address
                  FROM default.customer
                  WHERE c_customer_sk IS NOT NULL
                    AND c_customer_id IS NOT NULL) t37
               INNER JOIN (
                             (SELECT cs_sold_date_sk,
                                     cs_bill_customer_sk,
                                     cs_ext_discount_amt,
                                     cs_ext_sales_price,
                                     cs_ext_wholesale_cost,
                                     cs_ext_list_price
                              FROM default.catalog_sales
                              WHERE cs_bill_customer_sk IS NOT NULL
                                AND cs_sold_date_sk IS NOT NULL) t39
                           INNER JOIN
                             (SELECT d_date_sk,
                                     CAST(2001 AS INTEGER) d_year
                              FROM default.date_dim
                              WHERE d_year = 2001
                                AND d_date_sk IS NOT NULL) t41 ON t39.cs_sold_date_sk = t41.d_date_sk) ON t37.c_customer_sk = t39.cs_bill_customer_sk
               GROUP BY t37.c_customer_id,
                        t37.c_first_name,
                        t37.c_last_name,
                        t37.c_preferred_cust_flag,
                        t37.c_birth_country,
                        t37.c_login,
                        t37.c_email_address
               HAVING SUM((t39.cs_ext_list_price - t39.cs_ext_wholesale_cost - t39.cs_ext_discount_amt + t39.cs_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) > 0) t45 ON t25.$f8 / t45.$f8 > t16.$f8 / t35.$f8
            AND t35.$f0 = t45.$f0) ON t7.$f0 = t35.$f0
INNER JOIN
  (SELECT t47.c_customer_id $f0,
                            SUM((t49.ws_ext_list_price - t49.ws_ext_wholesale_cost - t49.ws_ext_discount_amt + t49.ws_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) $f8
   FROM
     (SELECT c_customer_sk,
             c_customer_id,
             c_first_name,
             c_last_name,
             c_preferred_cust_flag,
             c_birth_country,
             c_login,
             c_email_address
      FROM default.customer
      WHERE c_customer_sk IS NOT NULL
        AND c_customer_id IS NOT NULL) t47
   INNER JOIN (
                 (SELECT ws_sold_date_sk,
                         ws_bill_customer_sk,
                         ws_ext_discount_amt,
                         ws_ext_sales_price,
                         ws_ext_wholesale_cost,
                         ws_ext_list_price
                  FROM default.web_sales
                  WHERE ws_bill_customer_sk IS NOT NULL
                    AND ws_sold_date_sk IS NOT NULL) t49
               INNER JOIN
                 (SELECT d_date_sk,
                         CAST(2001 AS INTEGER) d_year
                  FROM default.date_dim
                  WHERE d_year = 2001
                    AND d_date_sk IS NOT NULL) t51 ON t49.ws_sold_date_sk = t51.d_date_sk) ON t47.c_customer_sk = t49.ws_bill_customer_sk
   GROUP BY t47.c_customer_id,
            t47.c_first_name,
            t47.c_last_name,
            t47.c_preferred_cust_flag,
            t47.c_birth_country,
            t47.c_login,
            t47.c_email_address
   HAVING SUM((t49.ws_ext_list_price - t49.ws_ext_wholesale_cost - t49.ws_ext_discount_amt + t49.ws_ext_sales_price) / CAST(2 AS DECIMAL(10, 0))) > 0) t55 ON t25.$f8 / t45.$f8 > t7.$f8 / t55.$f8
AND t35.$f0 = t55.$f0
ORDER BY t16.$f3 IS NULL DESC, t16.$f3
LIMIT 100
{code}"
HIVE-19346,TestMiniLlapLocalCliDriver.testCliDriver[materialized_view_create_rewrite_5] failling,"{noformat}
Error Message
Client Execution succeeded but contained differences (error code = 1) after executing materialized_view_create_rewrite_5.q 
402c402
<  totalSize            1053                
---
>  totalSize            1055
{noformat}"
HIVE-19323,Create metastore SQL install and upgrade scripts for 3.1,Now that we've branched for 3.0 we need to create SQL install and upgrade scripts for 3.1
HIVE-19315,Test failure org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2#testWriteSetTracking3,"*Error Message*
{code:java}
Exception msg didn't match expected:<...conflict on default/[TAB_PART]/p=blah committed by...> but was:<...conflict on default/[tab_part]/p=blah committed by...>
{code}
*Stacktrace*

{code:java}
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] Running org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2
[ERROR] Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 22.704 s <<< FAILURE! - in org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2
[ERROR] testWriteSetTracking3(org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2)  Time elapsed: 19.501 s  <<< FAILURE!
org.junit.ComparisonFailure: Exception msg didn't match expected:<...conflict on default/[TAB_PART]/p=blah committed by...> but was:<...conflict on default/[tab_part]/p=blah committed by...>
        at org.junit.Assert.assertEquals(Assert.java:115)
        at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testWriteSetTracking3(TestDbTxnManager2.java:994)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
"
HIVE-19277,Active/Passive HA web endpoints does not allow cross origin requests,CORS is not allowed with web endpoints added for active/passive HA. Enable CORS by default for all web endpoints. 
HIVE-19249,Replication: WITH clause is not passing the configuration to Task correctly in all cases,"When running repl load like following:
{code}
REPL LOAD `repldb_kms207` FROM 'hdfs://url:8020/apps/hive/repl/f8b057a7-c3f2-43bd-8baa-f7408a9008fc' WITH ('hive.exec.parallel'='true','hive.distcp.privileged.doAs'='beacon','hive.metastore.uris'='thrift://metastore-url:9083','hive.metastore.warehouse.dir'='s3a://s3-warehouse','hive.warehouse.subdir.inherit.perms'='false','hive.repl.replica.functions.root.dir'='s3a://s3-warehouse','fs.s3a.bucket.ss-datasets.endpoint'='s3-bucket-endpoint','fs.s3a.impl.disable.cache'='true','fs.s3a.server-side-encryption-algorithm'='SSE-KMS','fs.s3a.server-side-encryption.key'='encr-key','distcp.options.pp'='','distcp.options.pg'='','distcp.options.pu'='');
{code}

the task that get created need to use the configs that are passed in the USING clause. However, in some cases the wrong config object gets used."
HIVE-19214,High throughput ingest ORC format,"Create delta files with all ORC overhead disabled (no index, no compression, no dictionary). Compactor will recreate the orc files with index, compression and dictionary encoding."
HIVE-19211,New streaming ingest API and support for dynamic partitioning,"- New streaming API under new hive sub-module
- Dynamic partitioning support
- Auto-rollover transactions
- Automatic heartbeating"
HIVE-19210,Create separate module for streaming ingest,This will retain the old hcat streaming API for old clients. The new streaming ingest API will be separate module under hive. 
HIVE-19209,Streaming ingest record writers should accept input stream,Record writers in streaming ingest currently accepts byte[]. Provide an option for clients to pass in input stream directly from which byte[] for record can be constructed. 
HIVE-19206,Automatic memory management for open streaming writers,"Problem:
 When there are 100s of record updaters open, the amount of memory required by orc writers keeps growing because of ORC's internal buffers. This can lead to potential high GC or OOM during streaming ingest.

Solution:
 The high level idea is for the streaming connection to remember all the open record updaters and flush the record updater periodically (at some interval). Records written to each record updater can be used as a metric to determine the candidate record updaters for flushing. 
 If stripe size of orc file is 64MB, the default memory management check happens only after every 5000 rows which may which may be too late when there are too many concurrent writers in a process. Example case would be 100 writers open and each of them have almost full stripe of 64MB buffered data, this would take 100*64MB ~=6GB of memory. When all of the record writers flush, the memory usage drops down to 100*~2MB which is just ~200MB memory usage."
HIVE-19193,TestActivePassiveHA fails,"This looks like a flaky test. If its not reproducible locally try improving on stability of this test, since it does fail randomly with Hive QA.
{code:java}
java.lang.AssertionError: expected:<true> but was:<false>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.hive.jdbc.TestActivePassiveHA.testManualFailover(TestActivePassiveHA.java:356)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74){code}"
HIVE-19168,Ranger changes for llap commands,"New llap commands ""llap cluster -info"" and ""llap cache -purge"" require some changes so that Ranger can log the commands for auditing. "
HIVE-18874,JDBC: HiveConnection shades log4j interfaces,"This prevents Hive JDBC from being instantiated into a regular SLF4J logger env.

{code}
java.lang.IncompatibleClassChangeError: Class org.apache.logging.slf4j.Log4jLoggerFactory does not implement the requested interface org.apache.hive.org.slf4j.ILoggerFactory
        at org.apache.hive.org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:285)
{code}"
HIVE-18767,Some alterPartitions invocations throw 'NumberFormatException: null',"Error messages:
{noformat}
[info] Cause: java.lang.NumberFormatException: null
[info] at java.lang.Long.parseLong(Long.java:552)
[info] at java.lang.Long.parseLong(Long.java:631)
[info] at org.apache.hadoop.hive.metastore.MetaStoreUtils.isFastStatsSame(MetaStoreUtils.java:315)
[info] at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartitions(HiveAlterHandler.java:605)
[info] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_partitions_with_environment_context(HiveMetaStore.java:3837)
[info] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[info] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[info] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[info] at java.lang.reflect.Method.invoke(Method.java:498)
[info] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
[info] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
[info] at com.sun.proxy.$Proxy23.alter_partitions_with_environment_context(Unknown Source)
[info] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.alter_partitions(HiveMetaStoreClient.java:1527)
{noformat}"
HIVE-18609,Results cache invalidation based on ACID table updates,Look into using the materialized view invalidation mechanisms to automatically invalidate queries in the results cache if the underlying tables used in the cached queries have been modified.
HIVE-15190,Field names are not preserved in ORC files written with ACID,"To repro:
{noformat}
drop table if exists orc_nonacid;
drop table if exists orc_acid;

create table orc_nonacid (a int) clustered by (a) into 2 buckets stored as orc;
create table orc_acid (a int) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');

insert into table orc_nonacid values(1), (2);
insert into table orc_acid values(1), (2);
{noformat}

Running {{hive --service orcfiledump <file>}} on the files created by the {{insert}} statements above, you'll see that for {{orc_nonacid}}, the files have schema {{struct<a:int>}} whereas for {{orc_acid}}, the files have schema {{struct<operation:int,originalTransaction:bigint,bucket:int,rowId:bigint,currentTransaction:bigint,row:struct<_col0:int>>}}. The last field {{row}} should have schema {{struct<a:int>}}."
