Bug_ID,Bug_Summary,Bug_Description
HIVE-26075,hive metastore connection leaking when hiveserver2 kerberos enable and  hive.server2.enable.doAs set to true,"(1)When hadoop cluster  kerberos is enable

(2) HiveServer2 config hive.server2.enable.doAs is set true

After a beeline   scripte has been executed, hivemetastore connection  is created are in ESTABLISHED state and never closed.

If we submit a lot of task to hiveserver2 ,this will result in hive metastore thrift thread(default is 1000) full ,thus new task will fail.

 

HiveServer2 use ThreadLocal<Hive>  to store multithreading  metastore connection,the application should call Hive.closeCurrent() to close connection after  task finished.

 

When HiveServer2 impersonate is enable (hive.server2.enable.doAs is set true), the ugi   will create proxy user via  UserGroupInformation.createProxyUser(
owner, UserGroupInformation.getLoginUser()),the old metastore client is never closed.

 

 

 "
HIVE-25931,hive split failed when task run on yarn with fairscheduler because yarn return memory -1,"hive split failed when task run on yarn with fairscheduler because yarn return memory -1,in general ,if resource is used full,task should be at accepted state,not return failed"
HIVE-24711,hive metastore memory leak,"hdp version:3.1.5.31-1

hive version:3.1.0.3.1.5.31-1

hadoop version:3.1.1.3.1.5.31-1

We find that the hive metastore has memory leak if we set compactor.initiator.on to true.

If we disable the configuration, the memory leak disappear.

How can we resolve this problem?

Even if we set the heap size of hive metastore to 40 GB, after 1 month the hive metastore service will be down with outofmemory."
HIVE-24650,"hiveserver2 memory usage is extremely high, GC unable to recycle","HDP's HiveServer2 is using 80GB of memory (HEAP is configured with 74GB), and when the memory is full, there will be frequent Full GC, and then the memory cannot be recycled, resulting in a service exception.Analyze memory usage.

GC config:

export HADOOP_OPTS=""$HADOOP_OPTS -Xloggc:\{{hive_log_dir}}/hiveserver2-gc-%t.log -XX:ConcGCThreads=30 -XX:ParallelGCThreads=30 -XX:+UseG1GC -XX:G1HeapRegionSize=8M -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=1000 -XX:InitiatingHeapOccupancyPercent=40 -XX:G1ReservePercent=15 -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCCause -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/hive/hs2_heapdump.hprof -Dhive.log.dir=\{{hive_log_dir}} -Dhive.log.file=hiveserver2.log

The details at

[https://blog.csdn.net/Small_codeing/article/details/112601226]"
HIVE-24636,Memory leak due to stacking UDFClassLoader in Apache Commons LogFactory,"Much the same as [HIVE-7563|https://issues.apache.org/jira/browse/HIVE-7563], after ClassLoader is closed in JavaUtils, it should be released by Apache Commons LogFactory, or the ClassLoader can't be Garbage Collected, which leads to memory leak, exactly our PROD met."
HIVE-24605,Metastore link leak in hiveserver2,"When we use hiveserver and connect to metastore server, we face metastore link leak as below:

 
{code:java}
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - db.metaStoreClient.isSameConfObj(c) is false
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - db.metaStoreClient.isCompatibleWith(c) is false
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - hive db is org.apache.hadoop.hive.ql.metadata.Hive@ec83e59
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - METASTORE_FILTER_HOOK:org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - db.metaStoreClient.isSameConfObj(c) is false
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - db.metaStoreClient.isCompatibleWith(c) is false
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook
2021-01-08T11:10:00,196 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - Closing current thread's connection to Hive Metastore.
2021-01-08T11:10:00,201 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Closed a connection to metastore, current connections: 0
2021-01-08T11:10:00,201 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - Closing current thread's connection to Hive Metastore.
2021-01-08T11:10:00,201 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - current Hive is org.apache.hadoop.hive.ql.metadata.Hive@5898c9e7
2021-01-08T11:10:00,202 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Trying to connect to metastore with URI thrift://xxx
2021-01-08T11:10:00,206 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Opened a connection to metastore, current connections: 1
2021-01-08T11:10:00,206 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Connected to metastore.
2021-01-08T11:10:00,215 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: parse.SemanticAnalyzer (:()) - Completed phase 1 of Semantic Analysis
2021-01-08T11:10:00,215 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: parse.SemanticAnalyzer (:()) - Get metadata for source tables
2021-01-08T11:10:00,215 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: parse.SemanticAnalyzer (:()) - current db in getMetaData is org.apache.hadoop.hive.ql.metadata.Hive@Hive@ec83e59
2021-01-08T11:10:00,215 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - current Hive is org.apache.hadoop.hive.ql.metadata.Hive@5898c9e7
2021-01-08T11:10:00,215 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Trying to connect to metastore with URI thrift://xxx
2021-01-08T11:10:00,218 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Opened a connection to metastore, current connections: 2
2021-01-08T11:10:00,219 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Connected to metastore.
 
.....
 
2021-01-08T11:12:16,119 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - metadata.Hive (:()) - close current: org.apache.hadoop.hive.ql.metadata.Hive@5898c9e7
2021-01-08T11:12:16,121 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: metadata.Hive (:()) - Closing current thread's connection to Hive Metastore, current is org.apache.hadoop.hive.ql.metadata.Hive@5898c9e7
2021-01-08T11:12:16,121 INFO  [qid=61_142_176_249_26122_20210108110957686_0]: hive.metastore (:()) - Closed a connection to metastore, current connections: 1
{code}
 

 

because HIVE_AUTHORIZATION_MANAGER is set to org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory

when in SessionState.setAuthorizerV2Config, sessionConf is changed, so Hive.create will  change and replace the old one in Hive.hiveDB. But the old Hive instance is already assigned in BaseSemanticAnalyzer.db, and when it's called, metaStoreClient will be assigned to a new one and create a new metastore link.  The metastoreClient in new Hive instance can be closed, but the old one assigned in BaseSemanticAnalyzer  will not close when we call Hive.close and Hive.closeCurrent, which may lead to a link leak.

So, can we use Hive.get() instead of using db.xxx in BaseSemanticAnalyzer, or it's better to re-assign to the new Hive instance in BaseSemanticAnalyzer  instead of  using the old one.

 "
HIVE-24569,LLAP daemon leaks file descriptors/log4j appenders,"With HIVE-9756 query logs in LLAP are directed to different files (file per query) using a Log4j2 routing appender. Without a purge policy in place, appenders are created dynamically by the routing appender, one for each query, and remain in memory forever. The dynamic appenders write to files so each appender holds to a file descriptor. 

Further work HIVE-14224 has mitigated the issue by introducing a custom purging policy (LlapRoutingAppenderPurgePolicy) which deletes the dynamic appenders (and closes the respective files) when the query is completed (org.apache.hadoop.hive.llap.daemon.impl.QueryTracker#handleLogOnQueryCompletion). 

However, in the presence of multiple threads appending to the logs there are race conditions. In an internal Hive cluster the number of file descriptors started going up approx one descriptor leaking per query. After some debugging it turns out that one thread (running the QueryTracker#handleLogOnQueryCompletion) signals that the query has finished and thus the purge policy should get rid of the respective appender (and close the file) while another (Task-Executor-0) attempts to append another log message for the same query. The initial appender is closed after the request from the query tracker but a new one is created to accomodate the message from the task executor and the latter is never removed thus creating a leak. 

Similar leaks have been identified and fixed for HS2 with the most similar one being that described [here|https://issues.apache.org/jira/browse/HIVE-22753?focusedCommentId=17021041&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17021041]. 

The problem relies on the timing of threads so it may not manifestate in all versions between 2.2.0 and 4.0.0. Usually the leak can be seen either via lsof (or other similar command) with the following output:

{noformat}
# 1494391 is the PID of the LLAP daemon process
ls -ltr /proc/1494391/fd
...
lrwx------ 1 hive hadoop 64 Dec 24 12:08 978 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121724_66ce273d-54a9-4dcd-a9fb-20cb5691cef7-dag_1608659125567_0008_194.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 977 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121804_ce53eeb5-c73f-4999-b7a4-b4dd04d4e4de-dag_1608659125567_0008_197.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 974 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224122002_1693bd7d-2f0e-4673-a8d1-b7cb14a02204-dag_1608659125567_0008_204.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 989 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121909_6a56218f-06c7-4906-9907-4b6dd824b100-dag_1608659125567_0008_201.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 984 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121754_78ef49a0-bc23-478f-9a16-87fa25e7a287-dag_1608659125567_0008_196.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 983 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121855_e65b9ebf-b2ec-4159-9570-1904442b7048-dag_1608659125567_0008_200.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 981 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121818_e9051ae3-1316-46af-aabb-22c53ed2fda7-dag_1608659125567_0008_198.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 980 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121744_fcf37921-4351-4368-95ee-b5be2592d89a-dag_1608659125567_0008_195.log
lrwx------ 1 hive hadoop 64 Dec 24 12:08 979 -> /hadoop/yarn/log/application_1608659125567_0006/container_e04_1608659125567_0006_01_000002/hive_20201224121837_e80c0024-f6bc-4b3c-85ed-5c0c85c55787-dag_1608659125567_0008_199.log
{noformat}

or in the heap dump with many appenders (in my case {{LlapWrappedAppender}}) holding indirectly open file descriptors:
!llap-appender-gc-roots.png! 

"
HIVE-24326,HiveServer memory leak,"After a while, the hiveserver we produce will fill up with JVMS, resulting in unresponsive hiveservers"
HIVE-24208,LLAP: query job stuck due to race conditions,"When issuing an LLAP query, sometimes the TEZ job on LLAP server never ends and it never returns the data reader."
HIVE-24099,"unix_timestamp,intersect,except throws NPE","unix_timestamp,intersect,except throws NPE when cbo is false and optimize.constant.propagation is false

reproduced problems:
 1. unix_timestap:
      set hive.cbo.enable=true;
      set hive.optimize.constant.propagation=false;
 {color:#000000}     create table test_pt(idx string, namex string) partitioned by(pt_dt string) stored as orc;{color}

{color:#000000}     explain extended select count(1) from test_pt where pt_dt = unix_timestamp();{color}

{color:#000000}!image-2020-09-01-10-22-07-549.png!{color}

{color:#000000}2.intersect{color}

{color:#000000} create table t1(id int, name string, score int);{color}

create table t2(id int, name string, score int);

insert into t1 values(1,'xiaoming', 98);

insert into t2 values(2,'xiaohong', 95);

select id from t1 intersect select id from t2;

!image-2020-09-01-10-26-14-062.png!

3.except 

select id from t1 except select id from t2;

  !image-2020-09-01-10-27-23-916.png!"
HIVE-24060,"When the CBO is false, NPE is thrown by an EXCEPT or INTERSECT execution","{code:java}
set hive.cbo.enable=false;
create table testtable(idx string, namex string) stored as orc;
insert into testtable values('123', 'aaa'), ('234', 'bbb');
explain select a.idx from (select idx,namex from testtable intersect select idx,namex from testtable) a
{code}
 The execution throws a NullPointException:
{code:java}
2020-08-24 15:12:24,261 | WARN  | HiveServer2-Handler-Pool: Thread-345 | Error executing statement:  | org.apache.hive.service.cli.thrift.ThriftCLIService.executeNewStatement(ThriftCLIService.java:1155)
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException null
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:341) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:215) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:316) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:253) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:684) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:670) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:342) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.executeNewStatement(ThriftCLIService.java:1144) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:1280) ~[hive-service-3.1.0.jar:3.1.0]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557) ~[hive-service-rpc-3.1.0.jar:3.1.0]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542) ~[hive-service-rpc-3.1.0.jar:3.1.0]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.hadoop.hive.metastore.security.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:648) ~[hive-standalone-metastore-3.1.0.jar:3.1.0]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_201]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_201]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:4367) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:4346) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:10576) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:10515) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11434) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11291) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11318) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:11304) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:12090) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12180) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11692) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:281) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:164) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:281) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:712) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:2117) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:2064) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:2059) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:137) ~[hive-exec-3.1.0.jar:3.1.0]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:204) ~[hive-service-3.1.0.jar:3.1.0]{code}"
HIVE-23873,Querying Hive JDBCStorageHandler table fails with NPE when CBO is off,"Scenario is Hive table having same schema as table in Oracle, however when we query the table with data it fails with NPE, below is the trace.

{code}
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:617) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:524) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2739) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:473) ~[hive-service-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        ... 34 more
Caused by: java.lang.NullPointerException
        at org.apache.hive.storage.jdbc.JdbcSerDe.deserialize(JdbcSerDe.java:164) ~[hive-jdbc-handler-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:598) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:524) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2739) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.getResults(ReExecDriver.java:229) ~[hive-exec-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:473) ~[hive-service-3.1.0.3.1.5.0-152.jar:3.1.0.3.1.5.0-152]
        ... 34 more
{code}

Problem appears when column names in Oracle are in Upper case and since in Hive, table and column names are forced to store in lowercase during creation. User runs into NPE error while fetching data.

While deserializing data, input consists of column names in lower case which fails to get the value

https://github.com/apache/hive/blob/rel/release-3.1.2/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcSerDe.java#L136
{code}
rowVal = ((ObjectWritable)value).get();
{code}

Log Snio:
=============
{code}
2020-07-17T16:49:09,598 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: dao.GenericJdbcDatabaseAccessor (:()) - Query to execute is [select * from TESTHIVEJDBCSTORAGE]
2020-07-17T16:49:10,642 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: jdbc.JdbcSerDe (:()) - *** ColumnKey = ID
2020-07-17T16:49:10,642 INFO  [04ed42ec-91d2-4662-aee7-37e840a06036 HiveServer2-Handler-Pool: Thread-104]: jdbc.JdbcSerDe (:()) - *** Blob value = {fname=OW[class=class java.lang.String,value=Name1], id=OW[class=class java.lang.Integer,value=1]}
{code}

Simple Reproducer for this case.
=============
1. Create table in Oracle
{code}
create table TESTHIVEJDBCSTORAGE(ID INT, FNAME VARCHAR(20));
{code}

2. Insert dummy data.
{code}
Insert into TESTHIVEJDBCSTORAGE values (1, 'Name1');
{code}

3. Create JDBCStorageHandler table in Hive.
{code}
CREATE EXTERNAL TABLE default.TESTHIVEJDBCSTORAGE_HIVE_TBL (ID INT, FNAME VARCHAR(20)) 
STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' 
TBLPROPERTIES ( 
""hive.sql.database.type"" = ""ORACLE"", 
""hive.sql.jdbc.driver"" = ""oracle.jdbc.OracleDriver"", 
""hive.sql.jdbc.url"" = ""jdbc:oracle:thin:@orachehostname/XE"", 
""hive.sql.dbcp.username"" = ""chiran"", 
""hive.sql.dbcp.password"" = ""supersecurepassword"", 
""hive.sql.table"" = ""TESTHIVEJDBCSTORAGE"", 
""hive.sql.dbcp.maxActive"" = ""1"" 
);
{code}

4. Query Hive table, fails with NPE.
{code}
> select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL;
INFO  : Compiling command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc): select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:testhivejdbcstorage_hive_tbl.id, type:int, comment:null), FieldSchema(name:testhivejdbcstorage_hive_tbl.fname, type:varchar(20), comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc); Time taken: 9.914 seconds
INFO  : Executing command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc): select * from default.TESTHIVEJDBCSTORAGE_HIVE_TBL
INFO  : Completed executing command(queryId=hive_20200717164857_cd6f5020-4a69-4a2d-9e63-9db99d0121bc); Time taken: 0.019 seconds
INFO  : OK
Error: java.io.IOException: java.lang.NullPointerException (state=,code=0)
{code}

Assuming that there are no repercussions, can we convert the column names to lowercase fetched from Database/Query pointing to table in JDBCStorageHandler?
Attaching the patch for the case."
HIVE-23792,[LLAP] Long continuous running job degrade performance of LLAP because of leaked shuffle manager threads,"*[Test Case/Reproduction]*

Run TPCH Q19 on 10 Gigs data in infinite loop and disable result caching 

*[Observation]*

On LLAP server I see a strange behaviour continuous increase in Threads.Although query will keep running but with time performance gets degrade 

*[Analysis]*

I took multiple thread-dumps at different intervals to figure out which category of threads causing this issue, and the culprit thread is *tez-shuffle manager*

.m2/org/apache/tez/tez-runtime-library/0.9.1/tez-runtime-library-0.9.1-sources.jar!/org/apache/tez/runtime/library/common/shuffle/impl/ShuffleManager.java:324

{quote}try {
 while ((runningFetchers.size() >= numFetchers || pendingHosts.isEmpty())
 && numCompletedInputs.get() < numInputs) {
 inputContext.notifyProgress();
 boolean ret = wakeLoop.await(1000, TimeUnit.MILLISECONDS);
 }
} finally {
 lock.unlock();
}{quote}

 

*[Stack Trace of culprit thread]*
{quote}threadId:Thread 16661 - state:BLOCKED
 stackTrace:
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.parkNanos(java.lang.Object, long) @bci=20, line=215 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(long, java.util.concurrent.TimeUnit) @bci=97, line=2163 (Compiled frame)
 - org.apache.tez.runtime.library.common.shuffle.impl.ShuffleManager$RunShuffleCallable.callInternal() @bci=125, line=327 (Compiled frame)
 - org.apache.tez.runtime.library.common.shuffle.impl.ShuffleManager$RunShuffleCallable.callInternal() @bci=1, line=311 (Compiled frame)
 - org.apache.tez.common.CallableWithNdc.call() @bci=8, line=36 (Compiled frame)
 - com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly() @bci=18, line=108 (Compiled frame)
 - com.google.common.util.concurrent.InterruptibleTask.run() @bci=16, line=41 (Compiled frame)
 - com.google.common.util.concurrent.TrustedListenableFutureTask.run() @bci=10, line=77 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Compiled frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame){quote}
 "
HIVE-23675,WebHcat: java level deadlock in hcat in presence of InMemoryJAAS,"ENV: Keberos/SPNEGO enabled

set hive.exec.post.hook;
org.apache.hadoop.hive.ql.hooks.ATSHook,org.apache.atlas.hive.hook.HiveHook

ATLAS Hook use InMemoryJAASConfiguration

This is a sequence of the event while issue reproduces:

WebHcat -> hcat -> Hive Driver -> post hook execution create ATSHook  -> hook start the spnego auth and stuck while finding InMemoryJAASConfiguration used by the AtlasHook (this happens in separate thread ATS Logger)

Hcat jstack
{code:java}
Found one Java-level deadlock:
 =============================
 ""ATS Logger 0"":
   waiting to lock monitor 0x00007efdc8003a38 (object 0x00000000f3fcfe28, a org.apache.atlas.plugin.classloader.AtlasPluginClassLoader),
   which is held by ""main""
 ""main"":
   waiting to lock monitor 0x00007efdc8003da8 (object 0x00000000c0050d40, a org.apache.hadoop.hive.ql.exec.UDFClassLoader),
   which is held by ""ATS Logger 0""

Java stack information for the threads listed above:
 ===================================================
 ""ATS Logger 0"":
     at org.apache.atlas.security.InMemoryJAASConfiguration.getAppConfigurationEntry(InMemoryJAASConfiguration.java:238)
     at sun.security.jgss.LoginConfigImpl.getAppConfigurationEntry(LoginConfigImpl.java:145)
     at javax.security.auth.login.LoginContext.init(LoginContext.java:251)
     at javax.security.auth.login.LoginContext.<init>(LoginContext.java:512)
     at sun.security.jgss.GSSUtil.login(GSSUtil.java:256)
     at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:158)
     at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:335)
     at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:331)
     at java.security.AccessController.doPrivileged(Native Method)
     at sun.security.jgss.krb5.Krb5InitCredential.getTgt(Krb5InitCredential.java:330)
     at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:145)
     at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122)
     at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)
     at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224)
     at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)
     at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
     at sun.security.jgss.spnego.SpNegoContext.GSS_initSecContext(SpNegoContext.java:882)
     at sun.security.jgss.spnego.SpNegoContext.initSecContext(SpNegoContext.java:317)
     at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248)
     at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)
     at sun.net.www.protocol.http.spnego.NegotiatorImpl.init(NegotiatorImpl.java:108)
     at sun.net.www.protocol.http.spnego.NegotiatorImpl.<init>(NegotiatorImpl.java:117)
     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
     at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
     at sun.net.www.protocol.http.Negotiator.getNegotiator(Negotiator.java:63)
     at sun.net.www.protocol.http.NegotiateAuthentication.isSupportedImpl(NegotiateAuthentication.java:130)
     - locked <0x00000000f48c4d90> (a java.lang.Class for sun.net.www.protocol.http.NegotiateAuthentication)
     at sun.net.www.protocol.http.NegotiateAuthentication.isSupported(NegotiateAuthentication.java:102)
     - locked <0x00000000c0050d40> (a org.apache.hadoop.hive.ql.exec.UDFClassLoader)
     at sun.net.www.protocol.http.AuthenticationHeader.parse(AuthenticationHeader.java:180)
     at sun.net.www.protocol.http.AuthenticationHeader.<init>(AuthenticationHeader.java:126)
     at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1660)
     - locked <0x00000000f47b7298> (a sun.net.www.protocol.https.DelegateHttpsURLConnection)
     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441)
     - locked <0x00000000f47b7298> (a sun.net.www.protocol.https.DelegateHttpsURLConnection)
     at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
     at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338)
     at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:191)
     at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:133)
     at org.apache.hadoop.security.authentication.client.AuthenticatedURL.openConnection(AuthenticatedURL.java:216)
     at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.openConnection(DelegationTokenAuthenticatedURL.java:322)
     at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineURLConnectionFactory.getHttpURLConnection(TimelineClientImpl.java:476)
     at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:159)
     at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:147)
     at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineJerseyRetryFilter$1.run(TimelineClientImpl.java:237)

""main"":
     at java.lang.ClassLoader.loadClass(ClassLoader.java:406)
     - waiting to lock <0x00000000c0050d40> (a org.apache.hadoop.hive.ql.exec.UDFClassLoader)
     at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
     - locked <0x00000000f430b818> (a org.apache.atlas.plugin.classloader.AtlasPluginClassLoader$MyClassLoader)
     at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
     at org.apache.atlas.plugin.classloader.AtlasPluginClassLoader.loadClass(AtlasPluginClassLoader.java:136)
     at org.apache.atlas.hook.AtlasHook.<clinit>(AtlasHook.java:76)
     at java.lang.Class.forName0(Native Method)
     at java.lang.Class.forName(Class.java:348)
     at org.apache.atlas.hive.hook.HiveHook.initialize(HiveHook.java:72)
     at org.apache.atlas.hive.hook.HiveHook.<init>(HiveHook.java:41)
     at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
     at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
     at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
     at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
     at java.lang.Class.newInstance(Class.java:442)
     at org.apache.hadoop.hive.ql.hooks.HookUtils.getHooks(HookUtils.java:61)
     at org.apache.hadoop.hive.ql.Driver.getHooks(Driver.java:1398)
     at org.apache.hadoop.hive.ql.Driver.getHooks(Driver.java:1382)
     at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1615)
     at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1303)
     at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)
     at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1160)
     at org.apache.hive.hcatalog.cli.HCatDriver.run(HCatDriver.java:43)
     at org.apache.hive.hcatalog.cli.HCatCli.processCmd(HCatCli.java:291)
     at org.apache.hive.hcatalog.cli.HCatCli.processLine(HCatCli.java:245)
     at org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:183)

{code}




"
HIVE-23632,Hive EMR S3 insert overwrite error for staging directory,Please refer error detail and settings from attachment 
HIVE-23438,Missing Rows When Left Outer Join In N-way HybridGraceHashJoin,"*Run Test in Patch File*
{code:java}
mvn test -Dtest=TestMiniTezCliDriver -Dqfile=hybridgrace_hashjoin_2.q{code}
*Manual Reproduce*

*STEP 1. Create test data(q_test_init_tez.sql)*
{code:java}
//create table src1
CREATE TABLE src1 (key STRING COMMENT 'default', value STRING COMMENT 'default') STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH ""${hiveconf:test.data.dir}/kv3.txt"" INTO TABLE src1;

//create table src2
CREATE TABLE src2(key STRING COMMENT 'default', value STRING COMMENT 'default') STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH ""${hiveconf:test.data.dir}/kv11.txt"" OVERWRITE INTO TABLE src2;

//create table srcpart
CREATE TABLE srcpart (key STRING COMMENT 'default', value STRING COMMENT 'default')
PARTITIONED BY (ds STRING, hr STRING)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH ""${hiveconf:test.data.dir}/kv1.txt""
OVERWRITE INTO TABLE srcpart PARTITION (ds=""2008-04-08"", hr=""11"");

LOAD DATA LOCAL INPATH ""${hiveconf:test.data.dir}/kv1.txt""
OVERWRITE INTO TABLE srcpart PARTITION (ds=""2008-04-08"", hr=""12"");

LOAD DATA LOCAL INPATH ""${hiveconf:test.data.dir}/kv1.txt""
OVERWRITE INTO TABLE srcpart PARTITION (ds=""2008-04-09"", hr=""11"");

LOAD DATA LOCAL INPATH ""${hiveconf:test.data.dir}/kv1.txt""
OVERWRITE INTO TABLE srcpart PARTITION (ds=""2008-04-09"", hr=""12"");{code}
*STEP 2. Run query*
{code:java}
set hive.auto.convert.join=true; 
set hive.auto.convert.join.noconditionaltask=true; 
set hive.auto.convert.join.noconditionaltask.size=10000000; 
set hive.cbo.enable=false;
set hive.mapjoin.hybridgrace.hashtable=true;

select *
from
(
select key from src1 group by key
) x
left join src2 z on x.key = z.key
join
(
select key from srcpart y group by key
) y on y.key = x.key;
{code}
*EXPECTED RESULT***

 
{code:java}
128	NULL	NULL	128
146	146	1val_1461	146
150	150	1val_1501	150
238	NULL	NULL	238
369	NULL	NULL	369
406	406	1val_4061	406
273	273	1val_2731	273
98	NULL	NULL	98
213	213	1val_2131	213
255	NULL	NULL	255
401	401	1val_4011	401
278	NULL	NULL	278
66	66	11val_6611	66
224	NULL	NULL	224
311	NULL	NULL	311
{code}
 

*ACTUAL RESULT*
{code:java}
128	NULL	NULL	128
146	146	1val_1461	146
150	150	1val_1501	150
213	213	1val_2131	213
238	NULL	NULL	238
273	273	1val_2731	273
369	NULL	NULL	369
406	406	1val_4061	406
98	NULL	NULL	98
401	401	1val_4011	401
66	66	11val_6611	66
{code}
 

*ROOT CAUSE*

src1 left join src2, src1 is big table and src2 is small table. Join result between big table row and the corresponding hashtable maybe NO_MATCH state, however, these NO_MATCH rows is needed because LEFT OUTER JOIN.

In addition, these big table rows will not spilled into matchfile related to this hashtable on disk because only SPILL state can use `spillBigTableRow`.  Then, these big table rows will be spilled into matchfile in hashtables of table `srcpart`(second small table)

Finally, when reProcessBigTable, big table rows in matchfile are only read from `firstSmallTable`, some datum are missing.

 

*WORKAROUND*

 configure firstSmallTable in completeInitializationOp and only spill big table row into firstSmallTable when spill matchfile.

 "
HIVE-23279,Unable to move source hdfs://xxx:8020/warehouse/.../.hive-staging_hive_... | 无法移动 hivestage 文件,"{color:#de350b}*error log from Cloudera Manager 6.2.1:;) | cdh 日志：*{color}

 

[HiveServer2-Background-Pool: Thread-58]: Error running hive query: 
 org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. {color:#de350b}Unable to move source{color} hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/.hive-staging_hive_2020-04-23_14-29-31_758_5380087337087305328-1/-ext-10000 to destination hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01
 at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:329) ~[hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:258) ~[hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hive.service.cli.operation.SQLOperation.access$600(SQLOperation.java:92) ~[hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:345) [hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
 at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_181]
 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875) [hadoop-common-3.0.0-cdh6.2.1.jar:?]
 at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:357) [hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
 Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: {color:#de350b}Unable to move source{color} hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/.hive-staging_hive_2020-04-23_14-29-31_758_5380087337087305328-1/-ext-10000 to destination hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01
 at org.apache.hadoop.hive.ql.metadata.Hive.getHiveException(Hive.java:3445) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.getHiveException(Hive.java:3401) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:3396) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:3693) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.loadPartitionInternal(Hive.java:1614) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1525) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1489) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:501) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2200) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1843) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1563) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1339) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1334) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:256) ~[hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 ... 11 more
 Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: {color:#de350b}Unable to move source{color} hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/.hive-staging_hive_2020-04-23_14-29-31_758_5380087337087305328-1/-ext-10000/000000_0 to destination hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/000000_0
 at org.apache.hadoop.hive.ql.metadata.Hive.handlePoolException(Hive.java:3418) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:3363) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:3693) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.loadPartitionInternal(Hive.java:1614) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1525) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1489) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:501) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2200) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1843) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1563) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1339) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1334) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:256) ~[hive-service-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 ... 11 more
 Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: {color:#de350b}Unable to move source{color} hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/.hive-staging_hive_2020-04-23_14-29-31_758_5380087337087305328-1/-ext-10000/000000_0 to destination hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/000000_0
 at org.apache.hadoop.hive.ql.metadata.Hive.getHiveException(Hive.java:3445) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.getHiveException(Hive.java:3401) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive.access$200(Hive.java:175) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:3346) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:3331) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 ... 4 more
 {color:#de350b}Caused by: java.io.IOException: rename for src path: hdfs{color}://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/.hive-staging_hive_2020-04-23_14-29-31_758_5380087337087305328-1/-ext-10000/000000_0 to dest path:hdfs://cdh231:8020/warehouse/szt.db/ads/ads_all_passengers_single_ride_spend_time_average/day=2018-09-01/000000_0 returned false
 at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:3343) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 at org.apache.hadoop.hive.ql.metadata.Hive$3.call(Hive.java:3331) ~[hive-exec-2.1.1-cdh6.2.1.jar:2.1.1-cdh6.2.1]
 ... 4 more

 

Additional notes:

补充说明：

 

My account for logging in to Cloudera Manager hue is hive, and the / warehouse directory has been authorized to hive, and I tried to set it up as follows: 

我登录 Cloudera Manager  hue 的账户是 hive ，/warehouse 目录已授权给 hive ，并且尝试设置如下：

------------------------------------------------------------------------------------hive.warehouse.subdir.inherit.perms=false

hive.exec.scratchdir=/warehouse/.hive-staging

------------------------------------------------------------------------------------

or

或者

------------------------------------------------------------------------------------hive.warehouse.subdir.inherit.perms=true

------------------------------------------------------------------------------------

None of the above configurations can solve the problem

I have been troubled by this problem for several weeks,
 I found that google and stackoverflow.com can't solve the problem

 

以上配置全都无法解决问题
 我已经被这个问题困扰几周了，我查找了 google 和 stackoverflow.com 都无法解决问题

 

My software version:

我的软件版本：
 Cloudera Manager Agent 6.2.1Cloudera Manager Management Daemon 6.2.1Hadoop 3.0.0 cdh6.2.1HDFS 3.0.0 cdh6.2.1HttpFS 3.0.0 cdh6.2.1hadoop-kms 3.0.0 cdh6.2.1MapReduce 2 3.0.0 cdh6.2.1YARN 3.0.0 cdh6.2.1Hive 2.1.1 cdh6.2.1HCatalog 2.1.1 cdh6.2.1Hue 4.3.0 cdh6.2.1Impala 3.2.0 cdh6.2.1Java 8 java version ""1.8.0_181"" Java(TM) SE Runtime Environment (build 1.8.0_181-b13) Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)Parquet 1.9.0 cdh6.2.1spark 2.4.0 cdh6.2.1

 

 "
HIVE-22833,NPE when using concatenate with AWS Glue,"When running 
{code:java}
alter table query.payment_transaction partition (insert_date='2017-10-27') concatenate;{code}
I get a NPE, with the following stack trace on the logs:
{code:java}
8-4207-837d-4fd6ea87b397): alter table query.payment_transaction partition (insert_date='2017-10-27') concatenate8-4207-837d-4fd6ea87b397): alter table query.payment_transaction partition (insert_date='2017-10-27') concatenate2020-02-05T13:55:50,840 ERROR [c8ef752c-2f03-460a-b131-ef3f6be4ddf8 main([])]: ql.Driver (SessionState.java:printError(1130)) - FAILED: SemanticException java.lang.NullPointerExceptionorg.apache.hadoop.hive.ql.parse.SemanticException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTablePartMergeFiles(DDLSemanticAnalyzer.java:1699) at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:305) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:239) at org.apache.hadoop.util.RunJar.main(RunJar.java:153)Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.metadata.Partition.getSkewedColNames(Partition.java:557) at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTablePartMergeFiles(DDLSemanticAnalyzer.java:1626) ... 19 more
{code}
The table definition looks like this:
{code:java}
CREATE TABLE `query.payment_transaction`(
  `player_id` string,
  `seamless_wallet_id` string,
  `transaction_timestamp` timestamp,
  `creation_time` bigint,
  `seamless_wallet_transaction_id` string,
  `currency` string,
  `real_delta` decimal(38,18),
  `real_delta_eur` decimal(38,18),
  `transaction_type` string,
  `payment_method_id` string,
  `execution_provider` string,
  `channel` string,
  `channel_subtype` string,
  `session_id` string)
PARTITIONED BY (
  `insert_date` string)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
LOCATION
  's3://prod.casumo.bigdata.hive/casumo_query/payment_transaction'
TBLPROPERTIES (
  'spark.sql.create.version'='2.2 or prior',
  'spark.sql.sources.schema.numPartCols'='1',
  'spark.sql.sources.schema.numParts'='1',
  'spark.sql.sources.schema.part.0'='{""type"":""struct"",""fields"":[{""name"":""player_id"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""seamless_wallet_id"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""transaction_timestamp"",""type"":""timestamp"",""nullable"":true,""metadata"":{}},{""name"":""creation_time"",""type"":""long"",""nullable"":true,""metadata"":{}},{""name"":""seamless_wallet_transaction_id"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""currency"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""real_delta"",""type"":""decimal(38,18)"",""nullable"":true,""metadata"":{}},{""name"":""real_delta_eur"",""type"":""decimal(38,18)"",""nullable"":true,""metadata"":{}},{""name"":""transaction_type"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""payment_method_id"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""execution_provider"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""channel"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""channel_subtype"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""session_id"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""insert_date"",""type"":""string"",""nullable"":true,""metadata"":{}}]}',
  'spark.sql.sources.schema.partCol.0'='insert_date')
{code}
And is worth mentioning that we are using AWS Glue as a metastore, and this table was imported from a normal mysql metastore.

Can someone guide me on the solution for this?

Thank you all!"
HIVE-22412,StatsUtils throw NPE when explain,"The demo like this:
{code:java}
drop table if exists explain_npe_map;
drop table if exists explain_npe_array;
drop table if exists explain_npe_struct;

create table explain_npe_map    ( c1 map<string, string> );
create table explain_npe_array  ( c1 array<string> );
create table explain_npe_struct ( c1 struct<name:string, age:int> );

-- error
set hive.cbo.enable=false;
explain select c1 from explain_npe_map where c1 is null;
explain select c1 from explain_npe_array where c1 is null;
explain select c1 from explain_npe_struct where c1 is null;

-- correct
set hive.cbo.enable=true;
explain select c1 from explain_npe_map where c1 is null;
explain select c1 from explain_npe_array where c1 is null;
explain select c1 from explain_npe_struct where c1 is null;{code}
 

if the conf 'hive.cbo.enable' set false , NPE will be thrown ; otherwise will not.
{code:java}
hive> drop table if exists explain_npe_map;
OK
Time taken: 0.063 seconds
hive> drop table if exists explain_npe_array;
OK
Time taken: 0.035 seconds
hive> drop table if exists explain_npe_struct;
OK
Time taken: 0.015 seconds
hive>
    > create table explain_npe_map    ( c1 map<string, string> );
OK
Time taken: 0.584 seconds
hive> create table explain_npe_array  ( c1 array<string> );
OK
Time taken: 0.216 seconds
hive> create table explain_npe_struct ( c1 struct<name:string, age:int> );
OK
Time taken: 0.17 seconds
hive>
    > set hive.cbo.enable=false;
hive> explain select c1 from explain_npe_map where c1 is null;
FAILED: NullPointerException null
hive> explain select c1 from explain_npe_array where c1 is null;
FAILED: NullPointerException null
hive> explain select c1 from explain_npe_struct where c1 is null;
FAILED: RuntimeException Error invoking signature method
hive>
    > set hive.cbo.enable=true;
hive> explain select c1 from explain_npe_map where c1 is null;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stageSTAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: explain_npe_map
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Filter Operator
            predicate: false (type: boolean)
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: c1 (type: map<string,string>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              ListSinkTime taken: 1.593 seconds, Fetched: 20 row(s)
hive> explain select c1 from explain_npe_array where c1 is null;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stageSTAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: explain_npe_array
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Filter Operator
            predicate: false (type: boolean)
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: c1 (type: array<string>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              ListSinkTime taken: 1.969 seconds, Fetched: 20 row(s)
hive> explain select c1 from explain_npe_struct where c1 is null;
OK
STAGE DEPENDENCIES:
  Stage-0 is a root stageSTAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: explain_npe_struct
          Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
          Filter Operator
            predicate: false (type: boolean)
            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
            Select Operator
              expressions: c1 (type: struct<name:string,age:int>)
              outputColumnNames: _col0
              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
              ListSinkTime taken: 2.932 seconds, Fetched: 20 row(s)
hive>
{code}
ms error like:

for map:
{code:java}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfMap(StatsUtils.java:1045)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:931)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOfVariableLengthTypes(StatsUtils.java:869)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:526)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:223)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:136)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:124)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:56)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:192)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10205)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:210)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:425)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:309)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1153)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1206)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1082)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1072)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136){code}
 

for array:
{code:java}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:1168)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOf(StatsUtils.java:1132)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:686)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:664)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:254)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:162)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:150)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:142)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:250)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12481)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11824)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:166)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1854)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1801)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1796)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:226)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:141)
{code}
for struct:

Maybe correct in branch of master,  but i think it is necessary to initialize the value of StandardConstantStructObjectInspector
{code:java}
//代码占位符
2020-06-10T16:40:56,971 ERROR [52839d08-57a7-475f-b87f-8f1410978b8a main] ql.Driver: FAILED: RuntimeException Error invoking signature method
java.lang.RuntimeException: Error invoking signature method
        at org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils$SignatureMapper.write(SignatureUtils.java:76)
        at org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils.write(SignatureUtils.java:40)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpSignature.<init>(OpSignature.java:53)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpSignature.of(OpSignature.java:57)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignature.<init>(OpTreeSignature.java:50)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignature.of(OpTreeSignature.java:63)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignatureFactory$CachedFactory.lambda$getSignature$0(OpTreeSignatureFactory.java:62)
        at java.util.Map.computeIfAbsent(Map.java:957)
        at org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignatureFactory$CachedFactory.getSignature(OpTreeSignatureFactory.java:62)
        at org.apache.hadoop.hive.ql.plan.mapper.PlanMapper.getSignatureOf(PlanMapper.java:265)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.applyRuntimeStats(StatsRulesProcFactory.java:2666)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.access$000(StatsRulesProcFactory.java:116)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$SelectStatsRule.process(StatsRulesProcFactory.java:211)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:250)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12481)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11824)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:166)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:664)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1854)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1801)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1796)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
        at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:226)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:141)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils$SignatureMapper.write(SignatureUtils.java:73)
        ... 42 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.getExprString(ExprNodeConstantDesc.java:158)
        at org.apache.hadoop.hive.ql.plan.ExprNodeDesc.getExprString(ExprNodeDesc.java:90)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.addExprToStringBuffer(PlanUtils.java:1104)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.getExprListString(PlanUtils.java:1092)
        at org.apache.hadoop.hive.ql.plan.PlanUtils.getExprListString(PlanUtils.java:1075)
        at org.apache.hadoop.hive.ql.plan.SelectDesc.getColListString(SelectDesc.java:79)
        ... 47 more
{code}
 

We can fix it by initializing value for StandardConstantMapObjectInspector, StandardConstantListObjectInspector and StandardConstantStructObjectInspector.

 "
HIVE-22091,NPE on HS2 start up due to bad data in FUNCS table,"If FUNCS table contains a stale DB_ID that has no links in DBS table, HS2 will fail to start up with NPE error:

{code:bash}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)
at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:220)
at org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:338)
at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:299)
at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:274)
at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:256)
at org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider.init(DefaultHiveAuthorizationProvider.java:29)
at org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProviderBase.setConf(HiveAuthorizationProviderBase.java:112)
... 21 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)
at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3646)
at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:231)
at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:215)
... 27 more
{code}
"
HIVE-21971,HS2 leaks classloader due to `ReflectionUtils::CONSTRUCTOR_CACHE` with temporary functions + GenericUDF,"https://issues.apache.org/jira/browse/HIVE-10329 helped in moving away from hadoop's ReflectionUtils constructor cache issue (https://issues.apache.org/jira/browse/HADOOP-10513).

However, there are corner cases where hadoop's {{ReflectionUtils}} is in use and this causes gradual build up of memory in HS2.

I have observed this in Hive 2.3. But the codepath in master for this has not changed much.

Easiest way to repro would be to add a temp function which extends {{GenericUDF}}. In {{FunctionRegistry::cloneGenericUDF,}} this would 
end up using {{org.apache.hadoop.util.ReflectionUtils.newInstance}} which in turn lands up in COSNTRUCTOR_CACHE of ReflectionUtils. 


{noformat}

CREATE TEMPORARY FUNCTION dummy AS 'com.hive.test.DummyGenericUDF' USING JAR 'file:///home/test/udf/dummy.jar';

select dummy();

	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.cloneGenericUDF(FunctionRegistry.java:1353)
	at org.apache.hadoop.hive.ql.exec.FunctionInfo.getGenericUDF(FunctionInfo.java:122)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:983)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1359)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
{noformat}

Note: Reflection based invocation of hadoop's {{ReflectionUtils::clear}} was removed in 2.x. "
HIVE-21903,HiveServer2 Query Compilation fails with StackOverflowError if the list of values in IN Clause is too big,"Steps to Reproduce:
The query including some joins and IN clause containing more than 15000 values.

Attaching the Handlers thread progress before it runs into the StackOverFlowError."
HIVE-21820,drop database command flushing other data outside db folder,"On running the below command to create new database:

*create database abc location /users/hive/warehouse/project_name/;*

here expected is, this command should create a new db folder named abc.db under project_name directory, but with hive version 1.2.1000.2.6.3.0-235 its not creating abc.db folder.

 

And on running command *drop database abc;* the entire project_name directory got deleted from HDFS. So here drop command rather than dropping just one db it deleted whole directory and just retained meta informations in hive metastore for other dbs present in project_name directory."
HIVE-21642,Hive server leaks memory on data insertion,"We are continuously loading data into Hive table backed by files in ORC format by appending data in batches. We repeatedly have seen that over a span of few days Hive server experiences {{OutOfMemoryError}} exceptions that we believe are caused by memory leaks.

Comparison of heap dumps shows that most suspicious classes that show persistent growth and not recycled with GC are
 * {{org.apache.hadoop.hive.ql.io.orc.OrcStruct$Field}}
 * {{org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField}}
 * {{String}}

Sample program used for stress test and heap dumps from 700 to 2500 GB can be uploaded on request. They are too big for Jira backing store"
HIVE-21342,Analyze compute stats for column leave behind staging dir on hdfs,"staging dir cleanup does not happen for the ""analyze table .. compute statistics for columns"", this leads to stale directory on hdfs.
the problem seems to be with ColumnStatsSemanticAnalyzer which don't have hdfscleanup set for the context.
https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java#L310"
HIVE-20928,NPE in StatsUtils for complex type,"
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getWritableSize(StatsUtils.java:1147)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfMap(StatsUtils.java:1108)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:978)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOf(StatsUtils.java:916)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExpression(StatsUtils.java:1374)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExprMap(StatsUtils.java:1197)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$GroupByStatsRule.process(StatsRulesProcFactory.java:1009)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.runStatsAnnotation(SparkCompiler.java:240)
{noformat}

Issue should be there in master as well."
HIVE-20886,Fix NPE: GenericUDFLower,"{noformat}
create table if not exists test1(uuid array<string>);
select lower(uuid) from test1;

Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)
{noformat}"
HIVE-20824,HiveServer2 is still leaking file handles when using ADD JAR ,"In Hive version 2.1.1, hiveserver2 is stll leaking jar file handle when using add jar . When using UDF jar and  creating function with this jar,  the jar file will not delete.
{noformat}
Beeline version 2.1.1 by Apache Hive
0: jdbc:hive2://localhost:7001/> add jar hdfs:///udf/UDF-1.0-SNAPSHOT.jar;No rows affected (0.45 seconds)
0: jdbc:hive2://localhost:7001/> create temporary function my_md5 as 'com.neu.hive.UDF.Md5';No rows affected (0.082 seconds)
0: jdbc:hive2://localhost:7001/> select my_md5(name) from test;+-----------------------------------+--+
| c0 |
+-----------------------------------+--+
| ad6f6c09bd4accc056568714692ff4cd |
| 836db9ecc83d8397a5d0205eb8344d4c |
| 5a7fcd4f1c785c8ef4931a5a9c698ac0 |
| 0e2bb4d743f2a009d4b84a9338c98f7c |
| 3f6697692f4506cf311c95848f3536d3 |
| 0de5fc94d0ba53fc7a44f0f136e82fbb |
| 04ef3e58a6b569c1a404efb90d50f906 |
| f505e0a965c8e721eef7e111c66e5c29 |
| c053092216d6f623615d5be978b98e67 |
| 8c2c58079bc3d10be0da79c5d9940f31 |
+-----------------------------------+--+
10 rows selected (0.12 seconds)
{noformat}
the hiveserver2 PID is 3856，then list the file handle.
{noformat}
[hadoop@10 ~]$ lsof -p 3856 | grep SNAPSHOT.jar
java 3856 hadoop DEL REG 252,16 263184 /data/emr/hive/tmp/2f3bc7b2-c385-4552-a54e-b575b4338d63_resources/UDF-1.0-SNAPSHOT.jar
java 3856 hadoop DEL REG 252,16 263178 /data/emr/hive/tmp/986015db-52bf-4f6a-99f8-977de471d528_resources/UDF-4.0-SNAPSHOT.jar
java 3856 hadoop 774r REG 252,16 90885742 263178 /data/emr/hive/tmp/986015db-52bf-4f6a-99f8-977de471d528_resources/UDF-4.0-SNAPSHOT.jar (deleted)
java 3856 hadoop 775r REG 252,16 104220075 263184 /data/emr/hive/tmp/2f3bc7b2-c385-4552-a54e-b575b4338d63_resources/UDF-1.0-SNAPSHOT.jar (deleted)
{noformat}
 

 

 

 "
HIVE-20600,Metastore connection leak,"Within the execute method of HiveServer2, there appears to be a connection leak. With fairly straightforward series of INSERT statements, the connection count in the logs continues to increase over time. Under certain loads, this can also consume all underlying threads of the Hive metastore and result in HS2 becoming unresponsive to new connections.

The log below is the result of some python code executing a single insert statement, and then looping through a series of 10 more insert statements. We can see there's one dangling connection left open after each execution leaving us with 12 open connections (11 from the execute statements + 1 from HS2 startup).

{code}
2018-09-19T17:14:32,108 INFO [main([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 1
 2018-09-19T17:14:48,175 INFO [29049f74-73c4-4f48-9cf7-b4bfe524a85b HiveServer2-Handler-Pool: Thread-31([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 2
 2018-09-19T17:15:05,543 INFO [HiveServer2-Background-Pool: Thread-36([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 1
 2018-09-19T17:15:05,548 INFO [HiveServer2-Background-Pool: Thread-36([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 2
 2018-09-19T17:15:05,932 INFO [HiveServer2-Background-Pool: Thread-36([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 1
 2018-09-19T17:15:05,935 INFO [HiveServer2-Background-Pool: Thread-36([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 2
 2018-09-19T17:15:06,123 INFO [HiveServer2-Background-Pool: Thread-36([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 1
 2018-09-19T17:15:06,126 INFO [HiveServer2-Background-Pool: Thread-36([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 2
...
 2018-09-19T17:15:20,626 INFO [29049f74-73c4-4f48-9cf7-b4bfe524a85b HiveServer2-Handler-Pool: Thread-31([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 12
 2018-09-19T17:15:21,153 INFO [HiveServer2-Background-Pool: Thread-162([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 11
 2018-09-19T17:15:21,155 INFO [HiveServer2-Background-Pool: Thread-162([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 12
 2018-09-19T17:15:21,306 INFO [HiveServer2-Background-Pool: Thread-162([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 11
 2018-09-19T17:15:21,308 INFO [HiveServer2-Background-Pool: Thread-162([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 12
 2018-09-19T17:15:21,385 INFO [HiveServer2-Background-Pool: Thread-162([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 11
 2018-09-19T17:15:21,387 INFO [HiveServer2-Background-Pool: Thread-162([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 12
 2018-09-19T17:15:21,541 INFO [HiveServer2-Handler-Pool: Thread-31([])]: hive.metastore (HiveMetaStoreClient.java:open(481)) - Opened a connection to metastore, current connections: 13
 2018-09-19T17:15:21,542 INFO [HiveServer2-Handler-Pool: Thread-31([])]: hive.metastore (HiveMetaStoreClient.java:close(564)) - Closed a connection to metastore, current connections: 12
{code}

Attached is a simple [impyla|https://github.com/cloudera/impyla] script that triggers the condition."
HIVE-20442,Hive stale lock when the hiveserver2 background thread died with NPE,"this look like a race condition where background thread is not able to release the lock it aquired.

1. hiveserver2 background thread request for lock
{code}
2018-08-20T14:13:38,813 INFO  [HiveServer2-Background-Pool: Thread-XXXXX]: lockmgr.DbLockManager (DbLockManager.java:lock(100)) - Requesting: queryId=hive_xxxxxxx LockRequest(component:[LockComponent(type:SHARED_READ, level:TABLE, dbname:testdb, tablename:test_table, operationType:SELECT)], txnid:0, user:hive, hostname:HOSTNAME, agentInfo:hive_xxxxxxx)
{code}
2. acquired the lock and start heartbeating
{code}
2018-08-20T14:36:30,233 INFO  [HiveServer2-Background-Pool: Thread-XXXXX]: lockmgr.DbTxnManager (DbTxnManager.java:startHeartbeat(517)) - Started heartbeat with delay/interval = 150000/150000             MILLISECONDS for query: agentInfo:hive_xxxxxxx
{code}

3. during time between event #1 and #2, client disconnected and deleteContext cleanup the session dir
{code}
2018-08-21T15:39:57,820 INFO  [HiveServer2-Handler-Pool: Thread-XXX]: thrift.ThriftCLIService (ThriftBinaryCLIService.java:deleteContext(136)) - Session disconnected without closing properly.
2018-08-21T15:39:57,820 INFO  [HiveServer2-Handler-Pool: Thread-XXXX]: thrift.ThriftCLIService (ThriftBinaryCLIService.java:deleteContext(140)) - Closing the session: SessionHandle [3be07faf-5544-4178-8b50-8173002b171a]
2018-08-21T15:39:57,820 INFO  [HiveServer2-Handler-Pool: Thread-XXXX]: service.CompositeService (SessionManager.java:closeSession(363)) - Session closed, SessionHandle [xxxxxxxxxxxxxxxxxxxxxxx], current sessions:2
{code}

4. background thread died with NPE while trying to get the queryid 
{code}
java.lang.NullPointerException: null
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1568) ~[hive-exec-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1414) ~[hive-exec-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1211) ~[hive-exec-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1204) ~[hive-exec-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:242) [hive-service-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) [hive-service-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:336) [hive-service-2.1.0.2.6.5.0-292.jar:2.1.0.2.6.5.0-292]
        at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_77]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_77]
{code}
did not get a chance to release the lock and heartbeater thread continue heartbeat indefinately."
HIVE-20441,NPE in GenericUDF  when hive.allow.udf.load.on.demand is set to true,"When hive.allow.udf.load.on.demand is set to true and hiveserver2 has been started, the new created function from other clients or hiveserver2 will be loaded from the metastore at the first time. 

When the udf is used in where clause, we got a NPE like:

{code:java}
Error executing statement:
org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException null
        at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:290) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:320) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:530) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAP
SHOT]
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:517) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHO
T]
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:542) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNA
PSHOT]
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNA
PSHOT]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:57) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:236) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1104) ~[hive-exec-2.
3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1359) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.
3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.ExpressionWalker.walk(ExpressionWalker.java:76) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:229) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:176) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:11613) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:11568) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:11536) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(SemanticAnalyzer.java:3303) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFilterPlan(SemanticAnalyzer.java:3283) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:9592) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:10549) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:10427) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:11125) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11138) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10807) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1295) ~[hive-exec-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:204) ~[hive-service-2.3.4-SNAPSHOT.jar:2.3.4-SNAPSHOT]
{code}
 
The code to get udf from metastore is:
{code:java}
private FunctionInfo getFunctionInfoFromMetastoreNoLock(String functionName, HiveConf conf) {
    try {
      String[] parts = FunctionUtils.getQualifiedFunctionNameParts(functionName);
      Function func = Hive.get(conf).getFunction(parts[0].toLowerCase(), parts[1]);
      if (func == null) {
        return null;
      }
      // Found UDF in metastore - now add it to the function registry.
      FunctionInfo fi = registerPermanentFunction(functionName, func.getClassName(), true,
          FunctionTask.toFunctionResource(func.getResourceUris()));
      if (fi == null) {
        LOG.error(func.getClassName() + "" is not a valid UDF class and was not registered"");
        return null;
      }
      return fi;
    } catch (Throwable e) {
      LOG.info(""Unable to look up "" + functionName + "" in metastore"", e);
    }
    return null;
  }
{code}
 
After getting the function, the function is registered to permanent function list through method 'registerPermanentFunction'.


{code:java}
public FunctionInfo registerPermanentFunction(String functionName,
      String className, boolean registerToSession, FunctionResource... resources) {
    FunctionInfo function = new FunctionInfo(functionName, className, resources);
    // register to session first for backward compatibility
    if (registerToSession) {
      String qualifiedName = FunctionUtils.qualifyFunctionName(
          functionName, SessionState.get().getCurrentDatabase().toLowerCase());
      if (registerToSessionRegistry(qualifiedName, function) != null) {
        addFunction(functionName, function);
        return function;
      }
    } else {
        addFunction(functionName, function);
    }
    return null;
  }
{code}

And the variable registerToSession is true, so  the object 'function' will be returned. But the genericUDF field of the returned function is null which cause the error. 

We should return the result of the method registerToSessionRegistry returned.

"
HIVE-20327,Compactor should gracefully handle 0 length files and invalid orc files,"Older versions of Streaming API did not handle interrupts well and could leave 0-length ORC files behind which cannot be read.

These should be just skipped.

Other cases of file where ORC Reader cannot be created
1. regular write (1 txn delta) where the client died and didn't properly close the file - this delta should be aborted and never read
2. streaming ingest write (delta_x_y, x < y).  There should always be a side file if the file was not closed properly. (though it may still indicate that length is 0)


If we check these cases and still can't create a reader, it should not silently skip the file since the system thinks it contains at least some committed data but the file is corrupted (and the side file doesn't point at a valid footer) - we should never be in this situation and we should throw so that the end user can try manual intervention (where the only option may be deleting the file)"
HIVE-20192,HS2 with embedded metastore is leaking JDOPersistenceManager objects.,"Hiveserver2 instances where crashing every 3-4 days and observed HS2 in on unresponsive state. Also, observed that the FGC collection happening regularly

From JXray report it is seen that pmCache(List of JDOPersistenceManager objects) is occupying 84% of the heap and there are around 16,000 references of UDFClassLoader.
{code:java}
10,759,230K (84.7%) Object tree for GC root(s) Java Static org.apache.hadoop.hive.metastore.ObjectStore.pmf
- org.datanucleus.api.jdo.JDOPersistenceManagerFactory.pmCache ↘ 10,744,419K (84.6%), 1 reference(s)
  - j.u.Collections$SetFromMap.m ↘ 10,744,419K (84.6%), 1 reference(s)
    - {java.util.concurrent.ConcurrentHashMap}.keys ↘ 10,743,764K (84.5%), 16,872 reference(s)
      - org.datanucleus.api.jdo.JDOPersistenceManager.ec ↘ 10,738,831K (84.5%), 16,872 reference(s)
        ... 3 more references together retaining 4,933K (< 0.1%)
    - java.util.concurrent.ConcurrentHashMap self 655K (< 0.1%), 1 object(s)
      ... 2 more references together retaining 48b (< 0.1%)
- org.datanucleus.api.jdo.JDOPersistenceManagerFactory.nucleusContext ↘ 14,810K (0.1%), 1 reference(s)
... 3 more references together retaining 96b (< 0.1%){code}
When the RawStore object is re-created, it is not allowed to be updated into the ThreadWithGarbageCleanup.threadRawStoreMap which leads to the new RawStore never gets cleaned-up when the thread exit.

 "
HIVE-20153,Count and Sum UDF consume more memory in Hive 2+,"While playing with Hive2, we noticed that queries with a lot of count() and sum() aggregations run out of memory on Hadoop side where they worked before in Hive1. 

In many queries, we have to double the Mapper Memory settings (in our particular case mapreduce.map.java.opts from -Xmx2000M to -Xmx4000M), it makes it not so easy to upgrade to Hive 2.

Taking heap dump, we see one of the main culprit is the field 'uniqueObjects' in GeneraicUDAFSum and GenericUDAFCount, which was added to support Window functions."
HIVE-20098,Statistics: NPE when getting Date column partition statistics,"The issue reproduces only for a date column for a partitioned table. It reproduces only if the date column has all the values set to null, and if the partition is not empty.

Here is a quick reproducer:

 

 
{code:java}
CREATE TABLE dummy_table (
c_date DATE,
c_bigint BIGINT
)
PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE dummy_table PARTITION (ds='2018-01-01') SELECT CAST(null AS DATE), CAST(null AS BIGINT) FROM <any non empty table>;

ANALYZE TABLE dummy_table COMPUTE STATISTICS FOR COLUMNS;

DESCRIBE FORMATTED dummy_table.c_bigint PARTITION (ds='2018-01-01');
DESCRIBE FORMATTED dummy_table.c_date PARTITION (ds='2018-01-01');
{code}
 

 

The first `DESCRIBE FORMATTED` statement succeeds, when the second fails with an `NPE`

 

It happens because the null check is missing when converting Object from the ObjectStore to the Thrift object. The null check is missing only in the date statistics conversion for the partitioned table. 

Missing: [https://github.com/apache/hive/blob/master/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java#L469]

Present: https://github.com/apache/hive/blob/master/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java#L298

 "
HIVE-19970,Replication dump has a NPE when table is empty,if table directory or partition directory is missing ..dump is throwing NPE instead of file missing exception.
HIVE-19884,Invalidation cache may throw NPE when there is no data in table used by materialized view,
HIVE-19860,HiveServer2 ObjectInspectorFactory memory leak with cachedUnionStructObjectInspector,"hiveserver2 is start seeing the memory pressure once the cachedUnionStructObjectInspector start going 

[https://github.com/apache/hive/blob/master/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorFactory.java#L345]

I did not see any eviction policy for cachedUnionStructObjectInspector, so we should implement some size or time-based eviction policy. 

  !Screen Shot 2018-06-11 at 2.01.00 PM.png!"
HIVE-19731,Change staging tmp directory used by TestHCatLoaderComplexSchema,"Another one that is set to default and hence is flaky.

https://builds.apache.org/job/PreCommit-HIVE-Build/11321/testReport/org.apache.hive.hcatalog.pig/TestHCatLoaderComplexSchema/testSyntheticComplexSchema_3_/

{noformat}
org.apache.hadoop.util.Shell$ExitCodeException: chmod: cannot access ‘/tmp/hadoop/mapred/staging/hiveptest985275899/.staging/job_local985275899_0088’: No such file or directory

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1009) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:902) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1227) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1321) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1303) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:840) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:508) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:489) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:511) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:727) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.mkdirs(JobResourceUploader.java:658) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:172) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:133) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_102]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_102]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) ~[hadoop-common-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567) ~[hadoop-mapreduce-client-core-3.1.0.jar:?]
	at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.submit(ControlledJob.java:336) [hadoop-mapreduce-client-core-3.1.0.jar:?]
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_102]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_102]
	at org.apache.pig.backend.hadoop23.PigJobControl.submit(PigJobControl.java:128) [pig-0.16.0-h2.jar:?]
	at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:194) [pig-0.16.0-h2.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:276) [pig-0.16.0-h2.jar:?]
{noformat}"
HIVE-19654,Change tmp staging mapred directory for TestBlobstoreCliDriver,Similar to HIVE-19626.
HIVE-19286,NPE in MERGE operator on MR mode,"*General Info*

Hive version : 2.3.3

{code}
commit 3f7dde31aed44b5440563d3f9d8a8887beccf0be
Author: Daniel Dai <daijy@hortonworks.com>
Date:   Wed Mar 28 16:46:29 2018 -0700

    Preparing for 2.3.3 release

{code}

Hadoop version: 2.7.2.

Engine

{code}
hive> set hive.execution.engine;
hive.execution.engine=mr
{code}

*STEP 1. Create test data*

{code}
DROP TABLE IF EXISTS customer_target;
DROP TABLE IF EXISTS customer_source;
{code}

{code}
CREATE TABLE customer_target (id STRING, first_name STRING, last_name STRING, age INT) clustered by (id) into 2 buckets stored as ORC TBLPROPERTIES ('transactional'='true');
{code}

{code}
insert into customer_target values ('001', 'John', 'Smith', 45), ('002', 'Michael', 'Watson', 27), ('003', 'Den', 'Brown', 33);
SELECT id, first_name, last_name, age  FROM customer_target;
{code}

{code}
+------+-------------+------------+------+
|  id  | first_name  | last_name  | age  |
+------+-------------+------------+------+
| 002  | Michael     | Watson     | 27   |
| 001  | John        | Smith      | 45   |
| 003  | Den         | Brown      | 33   |
+------+-------------+------------+------+
{code}



{code}
CREATE TABLE customer_source (id STRING, first_name STRING, last_name STRING, age INT);

insert into customer_source values ('001', 'Dorothi', 'Hogward', 77), ('007', 'Alex', 'Bowee', 1), ('088', 'Robert', 'Dowson', 25);
SELECT id, first_name, last_name, age  FROM customer_source;
{code}

{code}
+------+-------------+------------+------+
|  id  | first_name  | last_name  | age  |
+------+-------------+------------+------+
| 088  | Robert      | Dowson     | 25   |
| 001  | Dorothi     | Hogward    | 77   |
| 007  | Alex        | Bowee      | 1    |
+------+-------------+------------+------+
{code}

*STEP 2. Merge data*

{code}
merge into customer_target trg using customer_source src on src.id = trg.id when matched then update set first_name = src.first_name, last_name = src.last_name when not matched then insert values (src.id, src.first_name, src.last_name, src.age);
{code}

*ACTUAL RESULT*

{code}
2018-04-24T07:11:44,448 DEBUG [main] log.PerfLogger: <PERFLOG method=deserializePlan from=org.apache.hadoop.hive.ql.exec.SerializationUtilities>
2018-04-24T07:11:44,448  INFO [main] exec.SerializationUtilities: Deserializing MapredLocalWork using kryo
2018-04-24T07:11:44,463 DEBUG [main] exec.Utilities: Hive Conf not found or Session not initiated, use thread based class loader instead
2018-04-24T07:11:44,538 DEBUG [main] log.PerfLogger: </PERFLOG method=deserializePlan start=1524568304448 end=1524568304538 duration=90 from=org.apache.hadoop.hive.ql.exec.SerializationUtilities>
2018-04-24T07:11:44,545  INFO [main] mr.MapredLocalTask: 2018-04-24 07:11:44    Starting to launch local task to process map join;      maximum memory = 477626368
2018-04-24T07:11:44,545 DEBUG [main] mr.MapredLocalTask: initializeOperators: trg, children = [HASHTABLESINK[37]]
2018-04-24T07:11:44,656 DEBUG [main] exec.Utilities: Hive Conf not found or Session not initiated, use thread based class loader instead
2018-04-24T07:11:44,676  INFO [main] mr.MapredLocalTask: fetchoperator for trg created
2018-04-24T07:11:44,676  INFO [main] exec.TableScanOperator: Initializing operator TS[0]
2018-04-24T07:11:44,676 DEBUG [main] exec.TableScanOperator: Initialization Done 0 TS
2018-04-24T07:11:44,676 DEBUG [main] exec.TableScanOperator: Operator 0 TS initialized
2018-04-24T07:11:44,676 DEBUG [main] exec.TableScanOperator: Initializing children of 0 TS
2018-04-24T07:11:44,676 DEBUG [main] exec.HashTableSinkOperator: Initializing child 37 HASHTABLESINK
2018-04-24T07:11:44,676  INFO [main] exec.HashTableSinkOperator: Initializing operator HASHTABLESINK[37]
2018-04-24T07:11:44,677  INFO [main] mapjoin.MapJoinMemoryExhaustionHandler: JVM Max Heap Size: 477626368
2018-04-24T07:11:44,680 ERROR [main] mr.MapredLocalTask: Hive Runtime Error: Map local work failed
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:57) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.JoinUtil.getObjectInspectorsFromEvaluators(JoinUtil.java:91) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.initializeOp(HashTableSinkOperator.java:153) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:366) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:556) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:508) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.initializeOperators(MapredLocalTask.java:508) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:411) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeInProcess(MapredLocalTask.java:391) ~[hive-exec-2.3.3.jar:2.3.3]
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:764) ~[hive-exec-2.3.3.jar:2.3.3]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_161]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_161]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_161]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_161]
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221) ~[hadoop-common-2.7.2.jar:?]
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136) ~[hadoop-common-2.7.2.jar:?]
{code}

FYI: [~ekoifman], [~eugene.koifman]"
HIVE-19206,Automatic memory management for open streaming writers,"Problem:
 When there are 100s of record updaters open, the amount of memory required by orc writers keeps growing because of ORC's internal buffers. This can lead to potential high GC or OOM during streaming ingest.

Solution:
 The high level idea is for the streaming connection to remember all the open record updaters and flush the record updater periodically (at some interval). Records written to each record updater can be used as a metric to determine the candidate record updaters for flushing. 
 If stripe size of orc file is 64MB, the default memory management check happens only after every 5000 rows which may which may be too late when there are too many concurrent writers in a process. Example case would be 100 writers open and each of them have almost full stripe of 64MB buffered data, this would take 100*64MB ~=6GB of memory. When all of the record writers flush, the memory usage drops down to 100*~2MB which is just ~200MB memory usage."
HIVE-18956,AvroSerDe Race Condition,"{code}
  @Override
  public Writable serialize(Object o, ObjectInspector objectInspector) throws SerDeException {
    if(badSchema) {
      throw new BadSchemaException();
    }
    return getSerializer().serialize(o, objectInspector, columnNames, columnTypes, schema);
  }

  @Override
  public Object deserialize(Writable writable) throws SerDeException {
    if(badSchema) {
      throw new BadSchemaException();
    }
    return getDeserializer().deserialize(columnNames, columnTypes, writable, schema);
  }

...

  private AvroDeserializer getDeserializer() {
    if(avroDeserializer == null) {
      avroDeserializer = new AvroDeserializer();
    }

    return avroDeserializer;
  }

  private AvroSerializer getSerializer() {
    if(avroSerializer == null) {
      avroSerializer = new AvroSerializer();
    }

    return avroSerializer;
  }
{code}

{{getDeserializer}} and {{getSerializer}} methods are not thread safe, so neither are {{deserialize}} and {{serialize}} methods.  It probably didn't matter with MapReduce, but now that we have Spark/Tez, it may be an issue.

You could visualize a scenario where three threads all enter {{getSerializer}} and all see that {{avroSerializer}} is _null_ and create three instances, then they would fight to assign the new object to the {{avroSerializer}} variable."
HIVE-18931,Race condition when ensuring a partition exists often causes AlreadyExistsException for the partition,"Hiya! I'm using Apache Beam's HCatalogIO to store data in Hive. As part of HCatOutputFormatWriter#commit(), partitions are registered in FileOutputCommitterContainer#registerPartitions(). Here, it checks for the existence of the partitions, and, if so needed, creates them. For parallel processes, this fails a lot of the time, because, as far as I understand, another process creates the partition in the meantime. This causes an AlreadyExistsException to be created for the partition, which bubbles up to the API consumer.

Relevant logic: [https://github.com/apache/hive/blob/release-1.2.1/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java#L898-L920]. Logic is also present on master.

Am I missing something, and if not, is it an acceptable solution to swallow the AlreadyExistsException when adding the partitions?"
HIVE-18929,The method humanReadableInt in HiveStringUtils.java has a race condition.,"I found that the {{humanReadableInt(long number)}} method in the hive/common/src/java/org/apache/hive/common/util/HiveStringUtils.java file contains code which has a race condition as shown in Hadoop (issue tracking ID HADOOP-9252: https://issues.apache.org/jira/browse/HADOOP-9252). The fix can also be seen in the Hadoop code base.

I couldn't find a call to the method anywhere else in the code. But it might be worth to fix."
HIVE-18885,DbNotificationListener has a deadlock between Java and DB locks (2.x line),"You can see the problem from looking at the code, but it actually created severe problems for real life Hive user.

When {{alter table}} has {{cascade}} option it does the following:
{code:java}
         msdb.openTransaction()
          ...
          List<Partition> parts = msdb.getPartitions(dbname, name, -1);
          for (Partition part : parts) {
            List<FieldSchema> oldCols = part.getSd().getCols();
            part.getSd().setCols(newt.getSd().getCols());
            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());
            updatePartColumnStatsForAlterColumns(msdb, part, oldPartName, part.getValues(), oldCols, part);
            msdb.alterPartition(dbname, name, part.getValues(), part);
          }
 {code}

So it walks all partitions (and this may be huge list) and does some non-trivial operations in one single uber-transaction.

When DbNotificationListener is enabled, it adds an event for each partition, all while
holding a row lock on NOTIFICATION_SEQUENCE table. As a result, while this is happening no other write DDL can proceed. This can sometimes cause DB lock timeouts which cause HMS level operation retries which make things even worse.

In one particular case this pretty much made HMS unusable."
HIVE-18786,NPE in Hive windowing functions,"When I run a Hive query with windowing functions, if there's enough data I get an NPE.

For example something like this query might break:

select id, created_date, max(created_date) over (partition by id) latest_created_any from ...

The only workaround I've found is to remove the windowing functions entirely.

The stacktrace looks suspiciously similar to {+}HIVE-15278{+}, but I'm in hive-2.3.2 which appears to have the bugfix applied.

 

 Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) <some row data here>
       at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:297)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:317)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
       ... 14 more

 Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) <some row data here>
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:365)
       at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:287)
        ... 16 more

Caused by: java.lang.NullPointerException
         at org.apache.hadoop.hive.ql.exec.persistence.PTFRowContainer.first(PTFRowContainer.java:115)
         at org.apache.hadoop.hive.ql.exec.PTFPartition.iterator(PTFPartition.java:114)
         at org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator.getPartitionAgg(BasePartitionEvaluator.java:200)
         at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.evaluateFunctionOnPartition(WindowingTableFunction.java:155)
         at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.iterator(WindowingTableFunction.java:538)
         at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.finishPartition(PTFOperator.java:349)
         at org.apache.hadoop.hive.ql.exec.PTFOperator.process(PTFOperator.java:123)
         at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
         at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
         at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:356)"
HIVE-18611,Avoid memory allocation of aggregation buffer during stats computation ,Bloom filter aggregation buffer may result in allocation of upto ~594MB array which is unnecessary.
HIVE-18593,NPE on vectorization group by,"Vectorisation with some queries seem to be failing with null pointer exceptions. This happens only with 2.3.2 release and not the older ones.

In the case, (in BytesColumnVector.java, vector[0] is null, isRepeating is true, length[0] is 0, start[0] is 0
{code:java}
public void copySelected(
    boolean selectedInUse, int[] sel, int size, BytesColumnVector output) {

  // Output has nulls if and only if input has nulls.
  output.noNulls = noNulls;
  output.isRepeating = false;

  // Handle repeating case
  if (isRepeating) {
    output.setVal(0, vector[0], start[0], length[0]);
    output.isNull[0] = isNull[0];
    output.isRepeating = true;
    return;
  }
{code}
Exception trace below
{code:java}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:883)
at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:86)
... 17 more
Caused by: java.lang.NullPointerException
at java.lang.System.arraycopy(Native Method)
at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:173)
at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.copySelected(BytesColumnVector.java:321)
at org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnStringGroupColumn.evaluate(IfExprStringGroupColumnStringGroupColumn.java:85)
at org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.gen.VectorUDAFMaxString.aggregateInputSelection(VectorUDAFMaxString.java:135)
at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processAggregators(VectorGroupByOperator.java:218)
at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeHashAggregate.doProcessBatch(VectorGroupByOperator.java:408)
at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator$ProcessingModeBase.processBatch(VectorGroupByOperator.java:179)
at org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.process(VectorGroupByOperator.java:1021)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:137)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:123)
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)
at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:783)
... 18 more
{code}
 

*Table details* 
{code:java}
CREATE TABLE `test_table`(
 `a` string,
 `b` string,
 `c` string)
 ROW FORMAT SERDE
 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT
 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
 OUTPUTFORMAT
 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'
 TBLPROPERTIES (
 'orc.compress'='SNAPPY',
 'orc.compress.size'='262144',
 'orc.create.index'='true',
 'orc.row.index.stride'='10000',
 'orc.stripe.size'='268435456',
 'transient_lastDdlTime'='1517556432');{code}
*Query*
{code:java}
SELECT
 NVL(Max(CASE WHEN b IN ('some_literal') THEN b ELSE c END),'') AS scol
FROM test_table
GROUP BY a
LIMIT 1000;{code}"
HIVE-18459,hive-exec.jar leaks contents fb303.jar into classpath,"thrift classes are now in the hive classpath in the hive-exec.jar (HIVE-11553). This makes it hard to test with other versions of this library. This library is already a declared dependency and is not required to be included in the hive-exec.jar.

I am proposing that we not include these classes like we have done in the past releases."
HIVE-18421,Vectorized execution handles overflows in a different manner than non-vectorized execution,"In vectorized execution arithmetic operations which cause integer overflows can give wrong results. Issue is reproducible in both Orc and parquet.

Simple test case to reproduce this issue

{noformat}
set hive.vectorized.execution.enabled=true;
create table parquettable (t1 tinyint, t2 tinyint) stored as parquet;
insert into parquettable values (-104, 25), (-112, 24), (54, 9);
select t1, t2, (t1-t2) as diff from parquettable where (t1-t2) < 50 order by diff desc;
+-------+-----+-------+
|  t1   | t2  | diff  |
+-------+-----+-------+
| -104  | 25  | 127   |
| -112  | 24  | 120   |
| 54    | 9   | 45    |
+-------+-----+-------+
{noformat}

When vectorization is turned off the same query produces only one row."
HIVE-18284,NPE when inserting data with 'distribute by' clause with dynpart sort optimization,"A Null Pointer Exception occurs when inserting data with 'distribute by' clause. The following snippet query reproduces this issue:
*(non-vectorized , non-llap mode)*

{code:java}
create table table1 (col1 string, datekey int);
insert into table1 values ('ROW1', 1), ('ROW2', 2), ('ROW3', 1);
create table table2 (col1 string) partitioned by (datekey int);

set hive.vectorized.execution.enabled=false;
set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert into table table2
PARTITION(datekey)
select col1,
datekey
from table1
distribute by datekey ;
{code}

I could run the insert query without the error if I remove Distribute By  or use Cluster By clause.
It seems that the issue happens because Distribute By does not guarantee clustering or sorting properties on the distributed keys.

FileSinkOperator removes the previous fsp. FileSinkOperator will remove the previous fsp which might be re-used when we use Distribute By.
https://github.com/apache/hive/blob/branch-2.3/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java#L972

The following stack trace is logged.

{code:java}
Vertex failed, vertexName=Reducer 2, vertexId=vertex_1513111717879_0056_1_01, diagnostics=[Task failed, taskId=task_1513111717879_0056_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1513111717879_0056_1_01_000000_0:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":""ROW3"",""_col1"":1}}
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
	at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":""ROW3"",""_col1"":1}}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:365)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:250)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:317)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:762)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:356)
	... 17 more
{code}

"
HIVE-18177,Error creating temporary staging folder on HDFS when creating Hive table,"When creating a table in hive using statement:

{code:java}
create database if not exists ${DB};
use ${DB};

drop table if exists date_dim;

create table date_dim
stored as ${FILE}
as select * from ${SOURCE}.date_dim;
{code}

The statement execution failed as:

{code:java}
FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db. Error encountered near token 'TOK_TMP_FILE'
FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db. Error encountered near token 'TOK_TMP_FILE'
{code}


We got this exception stack：

{code:java}
2017-11-29T17:32:47,646  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis
2017-11-29T17:32:47,646  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] parse.CalcitePlanner: Get metadata for source tables
2017-11-29T17:32:47,646  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] metastore.HiveMetaStore: 0: get_table : db=tpcds_text_2 tbl=date_dim
2017-11-29T17:32:47,647  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] HiveMetaStore.audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=tpcds_text_2 tbl=date_dim
2017-11-29T17:32:47,748  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] parse.CalcitePlanner: Get metadata for subqueries
2017-11-29T17:32:47,748  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] parse.CalcitePlanner: Get metadata for destination tables
2017-11-29T17:32:47,748  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] metastore.HiveMetaStore: 0: get_database: tpcds_bin_partitioned_orc_2
2017-11-29T17:32:47,748  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] HiveMetaStore.audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: tpcds_bin_partitioned_orc_2
2017-11-29T17:32:48,308  INFO [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] common.FileUtils: Creating directory if it doesn't exist: hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db/.hive-staging_hive_2017-11-29_17-32-47_541_2322222506518783479-1
2017-11-29T17:32:48,330 ERROR [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] parse.CalcitePlanner: org.apache.hadoop.hive.ql.parse.SemanticException: 0:0 Error creating temporary folder on: hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db. Error encountered near token 'TOK_TMP_FILE'
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2211)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1934)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:11080)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11133)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:286)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:234)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
Caused by: java.lang.RuntimeException: Cannot create staging directory 'hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db/.hive-staging_hive_2017-11-29_17-32-47_541_2322222506518783479-1': Invalid host name: local host is: (unknown); destination host is: ""ns-offline"":8020; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:374)
	at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:632)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2208)
	... 25 more
Caused by: java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: ""ns-offline"":8020; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:744)
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:445)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1522)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:787)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy37.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1700)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1433)
	at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:528)
	at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:366)
	... 27 more
Caused by: java.net.UnknownHostException
	... 52 more

2017-11-29T17:32:48,331 ERROR [4d9462cf-43b0-4fea-b0c2-c1a9969d9763 main] ql.Driver: FAILED: SemanticException 0:0 Error creating temporary folder on: hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db. Error encountered near token 'TOK_TMP_FILE'
org.apache.hadoop.hive.ql.parse.SemanticException: 0:0 Error creating temporary folder on: hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db. Error encountered near token 'TOK_TMP_FILE'
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2211)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1934)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:11080)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11133)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:286)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:490)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:234)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
Caused by: java.lang.RuntimeException: Cannot create staging directory 'hdfs://ns-offline/user/hive2/warehouse/tpcds_bin_partitioned_orc_2.db/.hive-staging_hive_2017-11-29_17-32-47_541_2322222506518783479-1': Invalid host name: local host is: (unknown); destination host is: ""ns-offline"":8020; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:374)
	at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:632)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2208)
	... 25 more
Caused by: java.net.UnknownHostException: Invalid host name: local host is: (unknown); destination host is: ""ns-offline"":8020; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:744)
	at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:445)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1522)
	at org.apache.hadoop.ipc.Client.call(Client.java:1373)
	at org.apache.hadoop.ipc.Client.call(Client.java:1337)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy36.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:787)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:398)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)
	at com.sun.proxy.$Proxy37.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1700)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1433)
	at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:528)
	at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:366)
	... 27 more
Caused by: java.net.UnknownHostException
	... 52 more
{code}

In HADOOP core-site.xml:

{code:java}
<property>
  <name>fs.default.name</name>
  <value>hdfs://ns-offline</value>
</property>
{code}

The Exception is thrown at Hive QL:
[https://github.com/apache/hive/blob/07fe7e210cb444aec43cb5adda37f8f7cd26f243/ql/src/java/org/apache/hadoop/hive/ql/Context.java#L396]
And called HDFS mkdir operation at:
[https://github.com/apache/hive/blob/07fe7e210cb444aec43cb5adda37f8f7cd26f243/common/src/java/org/apache/hadoop/hive/common/FileUtils.java#L579]
The call above took a _+conf+_ parameter but not used.

And also, in PathInfo class where we got the FileSystem instance：
[https://github.com/apache/hive/blob/32e854ef1c25f21d53f7932723cfc76bf75a71cd/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/util/PathInfo.java#L69]
{code:java}
FileSystem fileSystem = inputPath.getFileSystem(hiveConf);
{code}
We passed  a hive configuration instead of HDFS config.
"
HIVE-18042,Correlation Optimizer lead to NPE when there's multi subquery(select distinct) union all operation after join ,"test sql:

{code:sql}
SELECT DISTINCT a.logday AS push_day, a.mtype, a.t, If(b.msgid IS NULL, 'no', 'yes') AS isnotdaoda, a.platform
    , a.uid, a.dt
FROM (SELECT DISTINCT If(tokentype = '7', msgid, If(tokentype = '6', regexp_extract(sendpushresult, 'msgId"":""([^""]+)', 1), regexp_extract(sendpushresult, 'msgId=(.+?),', 1))) AS msgid, logday, If(vid LIKE '60%', 'adr', If(vid LIKE '8%', 'ios', 'other')) AS platform, mtype, t
        , If(vid LIKE '8%', uid, gid) AS uid, concat(substr(logday, 1, 4), '-', substr(logday, 5, 2), '-', substr(logday, 7, 2)) AS dt
    FROM wirelessdata.orig_push_client
    ) a
    LEFT JOIN (SELECT DISTINCT msgid
        FROM (
            SELECT DISTINCT msgid
            FROM wirelessdata.orig_push_return
            UNION ALL
            SELECT DISTINCT msgid
            FROM wirelessdata.orig_push_return_xiaomi
            UNION ALL
            SELECT DISTINCT regexp_extract(action, '""id""：""([^""]+)', 1) AS msgid
            FROM wirelessdata.ods_client_behavior_hour4spark
        ) bb
        ) b ON lower(a.msgid) = lower(b.msgid)
{code}

the error stack
{code:java}
2017-11-10T16:01:21,123 ERROR [9b7d82f5-dfc8-43ac-8d6f-a019d8677392 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setUnionPlan(GenMapRedUtils.java:230)
	at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.joinUnionPlan(GenMapRedUtils.java:287)
	at org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.process(GenMRRedSink3.java:100)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:54)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:65)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
	at org.apache.hadoop.hive.ql.parse.MapReduceCompiler.generateTaskTree(MapReduceCompiler.java:323)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:267)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11008)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10547)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:483)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1254)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1396)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1181)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1170)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:229)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:180)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:396)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:770)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:711)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:638)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{code}"
HIVE-17791,Temp dirs under the staging directory should honour `inheritPerms`,"For [~cdrome]:

CLI creates two levels of staging directories but calls setPermissions on the top-level directory only if {{hive.warehouse.subdir.inherit.perms=true}}.

The top-level directory, {{/user/cdrome/hive/words_text_dist/dt=c/.hive-staging_hive_2016-07-15_08-44-22_082_5534649671389063929-1}} is created the first time {{Context.getExternalTmpPath}} is called.

The child directory, {{/user/cdrome/hive/words_text_dist/dt=c/.hive-staging_hive_2016-07-15_08-44-22_082_5534649671389063929-1/_tmp.-ext-10000}} is created when {{TezTask.execute}} is called at line 164:

{code:java}
DAG dag = build(jobConf, work, scratchDir, appJarLr, additionalLr, ctx);
{code}

This calls {{DagUtils.createVertex}}, which calls {{Utilities.createTmpDirs}}:

{code:java}
3770   private static void createTmpDirs(Configuration conf,
3771       List<Operator<? extends OperatorDesc>> ops) throws IOException {
3772 
3773     while (!ops.isEmpty()) {
3774       Operator<? extends OperatorDesc> op = ops.remove(0);
3775 
3776       if (op instanceof FileSinkOperator) {
3777         FileSinkDesc fdesc = ((FileSinkOperator) op).getConf();
3778         Path tempDir = fdesc.getDirName();
3779 
3780         if (tempDir != null) {
3781           Path tempPath = Utilities.toTempPath(tempDir);
3782           FileSystem fs = tempPath.getFileSystem(conf);
3783           fs.mkdirs(tempPath); // <------ HERE!
3784         }
3785       }
3786 
3787       if (op.getChildOperators() != null) {
3788         ops.addAll(op.getChildOperators());
3789       }
3790     }
3791   }
{code}

It turns out that {{inheritPerms}} is no longer part of {{master}}. I'll rebase this for {{branch-2}}, and {{branch-2.2}}. {{master}} will have to wait till the issues around {{StorageBasedAuthProvider}}, directory permissions, etc. are sorted out.

(Note to self: YHIVE-857)"
HIVE-17783,Hybrid Grace Hash Join has performance degradation for N-way join using Hive on Tez,"Most configurations are using default value. And the benchmark is to test enabling against disabling hybrid grace hash join using TPC-DS queries at 3TB data scales. Many queries related to N-way join has performance degradation over three times test. Detailed result  is attached.
"
HIVE-17737,ObjectStore.getNotificationEventsCount may cause NPE,"In ObjectStore.getNotificationEventsCount():

{code}
 public NotificationEventsCountResponse getNotificationEventsCount(NotificationEventsCountRequest rqst) {
    Long result = 0L;
    try {
      openTransaction();
      long fromEventId = rqst.getFromEventId();
      String inputDbName = rqst.getDbName();
      String queryStr = ""select count(eventId) from "" + MNotificationLog.class.getName()
                + "" where eventId > fromEventId && dbName == inputDbName"";
      query = pm.newQuery(queryStr);
      query.declareParameters(""java.lang.Long fromEventId, java.lang.String inputDbName"");
      result = (Long) query.execute(fromEventId, inputDbName); // <- Here
      commited = commitTransaction();
      return new NotificationEventsCountResponse(result.longValue());
    }
  }
{code}

It is possible that query.execute will return null in which case rsult.longValue() may throw NPE."
HIVE-17547,MoveTask for Acid tables race condition,"Consider Hive.moveAcidFiles()
it starts out with something like
{noformat}
          └── -ext-10000
            │   └── 000000_0
            │       ├── _orc_acid_version
            │       └── delta_0000019_0000019
            │           └── bucket_00000
            │   └── 000000_1
            │       ├── _orc_acid_version
            │       └── delta_0000019_0000019
            │           └── bucket_00001
{noformat}
for a write to a bucketed table.
The ""move"" handles each 000000_N separately.  The first on creates delta_0000019_0000019 under the table/partition dir, the others just add bucket_0000N there.
That means there is a small window where someone may ""ls table/part/delta_0000019_0000019"" and not see all the buckets.

Once Acid writes directly to the final location (a la MM tables) this issue resolves automatically since txn 19 is uncommitted until everything is written."
HIVE-17462,hive_1.2.1  hiveserver2  memory leak,"hiveserver2  memory leak

hive  use  third UDF  （vs-1.0.2-SNAPSHOT.jar ， alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar  ..... and so on )

lr-x------ 1 data data 64 Sep  5 18:37 964 -> /tmp/9e38cc04-5693-474b-9c7d-bfdd978bcbb4_resources/vs-1.0.2-SNAPSHOT.jar (deleted)
lr-x------ 1 data data 64 Sep  6 10:41 965 -> /tmp/188bbf2a-d8a5-48a7-81fc-b807f9ff201d_resources/alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar (deleted)
lr-x------ 1 data data 64 Sep  6 17:41 97 -> /home/data/programs/hadoop-2.7.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar
lrwx------ 1 data data 64 Sep  5 18:37 975 -> socket:[1318353317]
lr-x------ 1 data data 64 Sep  6 02:38 977 -> /tmp/64e309dc-352f-4ba4-b871-1aa78fe05945_resources/alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar (deleted)
lr-x------ 1 data data 64 Sep  6 17:41 98 -> /home/data/programs/hadoop-2.7.1/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar
lrwx------ 1 data data 64 Sep  6 08:40 983 -> socket:[1299459344]
lr-x------ 1 data data 64 Sep  5 19:37 987 -> /tmp/c3054987-c9c6-468a-8b5c-6e20b1972e0b_resources/alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar (deleted)
lr-x------ 1 data data 64 Sep  6 17:41 99 -> /home/data/programs/hadoop-2.7.1/share/hadoop/hdfs/lib/guava-11.0.2.jar
lr-x------ 1 data data 64 Sep  6 08:40 994 -> /tmp/fc5c44b3-9bd8-4a32-a39a-66cd44032fee_resources/alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar (deleted)
lr-x------ 1 data data 64 Sep  6 06:39 996 -> /tmp/3b3c2bd6-0a0e-4599-b757-4a048a968457_resources/alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar (deleted)
lr-x------ 1 data data 64 Sep  5 17:36 999 -> /tmp/6ad76494-cdda-430b-b7d0-2213731655a8_resources/alogdata-1.0.3-SNAPSHOT-jar-with-dependencies.jar (deleted)

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
20084 data      20   0 13.6g  11g 533m S 62.3  9.2   6619:16 java



/home/data/programs/jdk/jdk-current/bin/java-Djava.net.preferIPv4Stack=true-Dhadoop.log.dir=/home/data/hadoop/logs-Dhadoop.log.file=hadoop.log-Dhadoop.home.dir=/home/data/programs/hadoop-2.7.1-Dhadoop.id.str=data-Dhadoop.root.logger=INFO,DRFA-Djava.library.path=/home/data/programs/hadoop-2.7.1/lib/native-Dhadoop.policy.file=hadoop-policy.xml-Djava.net.preferIPv4Stack=true-XX:+UseConcMarkSweepGC-Xms8g-Xmx8g-Dhadoop.security.logger=INFO,NullAppenderorg.apache.hadoop.util.RunJar/home/data/programs/hive-current/lib/hive-service-1.2.1.jarorg.apache.hive.service.server.HiveServer2--hiveconfhive.log.file=hiveserver2.log"
HIVE-17379,Null Pointer Exception in WHERE clause when using aggregate function as a filter  ,"Sample Query : 

with tableAAlias as (
   select a, count(z)  as acount
   from tableA
   groupBy a 

)

select a.a, b.b 
from tableB as b JOIN 
tableAAlias a
on a.a=b.a
where a.acount > 10 

FAILED: NullPointerException null
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory$ColumnPrunerFilterProc.process(ColumnPrunerProcFactory.java:103)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
at org.apache.hadoop.hive.ql.optimizer.ColumnPruner$ColumnPrunerWalker.walk(ColumnPruner.java:176)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120)
at org.apache.hadoop.hive.ql.optimizer.ColumnPruner.transform(ColumnPruner.java:136)
at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:246)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11149)
at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:246)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:264)
at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:80)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:264)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:490)
at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1270)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1412)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1199)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1189)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:265)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:210)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:444)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:474)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:514)
at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:882)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:836)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:732)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.util.RunJar.run(RunJar.java:223)
at org.apache.hadoop.util.RunJar.main(RunJar.java:136)


The above Query Succeeds if it is modified as : 

select a.a, b.b , *a.acount*
from tableB as b JOIN 
tableAAlias a
on a.a=b.a
where a.acount > 10 


Please Note the original query worked on hive1.2 & breaks on Hive2.1.1 "
HIVE-17272,"when hive.vectorized.execution.enabled is true, query on empty partitioned table fails with NPE","{noformat}
set hive.vectorized.execution.enabled=true;
CREATE TABLE `tab`(`x` int) PARTITIONED BY ( `y` int) stored as parquet;
select * from tab t1 join tab t2 where t1.x=t2.x;
{noformat}

The query fails with the following exception.
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.createAndInitPartitionContext(VectorMapOperator.java:386) ~[hive-exec-2.3.0.jar:2.3.0]
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.internalSetChildren(VectorMapOperator.java:559) ~[hive-exec-2.3.0.jar:2.3.0]
        at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.setChildren(VectorMapOperator.java:474) ~[hive-exec-2.3.0.jar:2.3.0]
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:106) ~[hive-exec-2.3.0.jar:2.3.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_101]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_101]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_101]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_101]
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_101]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_101]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_101]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_101]
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) ~[hadoop-common-2.6.0.jar:?]
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:413) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:268) ~[hadoop-core-2.6.0-mr1-cdh5.4.2.jar:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_101]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_101]
        at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_101]
{noformat}"
HIVE-17211,SQLStdHiveAccessController.showPrivileges Method may encountered  Null pointer,"      List<HiveObjectPrivilege> msObjPrivs = mClient.list_privileges(
                        principalName, principalType,
                        this.getThriftHiveObjectRef(privObj));


 >>>     CID 166074:  Null pointer dereferences  (NULL_RETURNS)
    >>>     Calling a method on null object ""msObjPrivs"".
                    for (HiveObjectPrivilege msObjPriv : msObjPrivs) {
                         HivePrincipal resPrincipal = new HivePrincipal(
                                msObjPriv.getPrincipalName(),
                                AuthorizationUtils.getHivePrincipalType(msObjPriv
                                         .getPrincipalType()));

Reason：
The method  mClient.list_privileges(
                        principalName, principalType,
                        this.getThriftHiveObjectRef(privObj)); may return null ；"
HIVE-17188,ObjectStore runs out of memory for large batches of addPartitions().,"For large batches (e.g. hundreds) of {{addPartitions()}}, the {{ObjectStore}} runs out of memory. Flushing the {{PersistenceManager}} alleviates the problem.

Note: The problem being addressed here isn't so much with the size of the hundreds of Partition objects, but the cruft that builds with the PersistenceManager, in the JDO layer, as confirmed through memory-profiling.

(Raising this on behalf of [~cdrome] and [~thiruvel].)"
HIVE-17098,Race condition in Hbase tables,"These steps simulate our customer production env.

*STEP 1. Create test tables*

{code}
CREATE TABLE for_loading(
  key int, 
  value string,
  age int,
  salary decimal (10,2)
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
{code}

Table {{test_1}} belongs to user {{testuser1}}.

{code}
CREATE TABLE test_1(
  key int, 
  value string,
  age int,
  salary decimal (10,2)
)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key, cf1:value, cf1:age, cf1:salary', 
  'serialization.format'='1')
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='{\""BASIC_STATS\"":\""true\""}', 
  'hbase.table.name'='test_1', 
  'numFiles'='0', 
  'numRows'='0', 
  'rawDataSize'='0', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1495769316');
{code}

Table {{test_2}} belongs to user {{testuser2}}.

{code}
CREATE TABLE test_2(
  key int, 
  value string,
  age int,
  salary decimal (10,2)
)
ROW FORMAT SERDE 
  'org.apache.hadoop.hive.hbase.HBaseSerDe' 
STORED BY 
  'org.apache.hadoop.hive.hbase.HBaseStorageHandler' 
WITH SERDEPROPERTIES ( 
  'hbase.columns.mapping'=':key, cf1:value, cf1:age, cf1:salary', 
  'serialization.format'='1')
TBLPROPERTIES (
  'COLUMN_STATS_ACCURATE'='{\""BASIC_STATS\"":\""true\""}', 
  'hbase.table.name'='test_2', 
  'numFiles'='0', 
  'numRows'='0', 
  'rawDataSize'='0', 
  'totalSize'='0', 
  'transient_lastDdlTime'='1495769316');
{code}


*STEP 2. Create test data*

{code}
import java.io.IOException;
import java.math.BigDecimal;
import java.nio.charset.Charset;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Random;

import static java.lang.String.format;

public class Generator {
    private static List<String> lines = new ArrayList<>();
    private static List<String> name = Arrays.asList(""Brian"", ""John"", ""Rodger"", ""Max"", ""Freddie"", ""Albert"", ""Fedor"", ""Lev"", ""Niccolo"");
    private static List<BigDecimal> salary = new ArrayList<>();

    public static void main(String[] args) {
        generateData(Integer.parseInt(args[0]), args[1]);
    }

    public static void generateData(int rowNumber, String file) {

        double maxValue = 20000.55;
        double minValue = 1000.03;

        Random random = new Random();
        for (int i = 1; i <= rowNumber; i++) {
            lines.add(
                i + "","" +
                    name.get(random.nextInt(name.size())) + "","" +
                    (random.nextInt(62) + 18) + "","" +
                    format(""%.2f"", (minValue + (maxValue - minValue) * random.nextDouble())));
        }

        Path path = Paths.get(file);

        try {
            Files.write(path, lines, Charset.forName(""UTF-8""), StandardOpenOption.APPEND);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
{code}

{code}
javac Generator.java
java Generator 3000000 dataset.csv
hadoop fs -put dataset.csv /
{code}


*STEP 3. Upload test data*

{code}
load data local inpath '/home/myuser/dataset.csv' into table for_loading;
{code}

{code}
from for_loading
insert into table test_1
select key,value,age,salary;
{code}

{code}
from for_loading
insert into table test_2
select key,value,age,salary;
{code}

*STEP 4. Run test queries*

Run in 5 parallel terminals for table {{test_1}}

{code}
for i in {1..500}; do beeline -u ""jdbc:hive2://localhost:10000/default testuser1"" -e ""select * from test_1 limit 10;"" 1>/dev/null; done
{code}


Run in 5 parallel terminals for table {{test_2}}

{code}
for i in {1..500}; do beeline -u ""jdbc:hive2://localhost:10000/default testuser2"" -e ""select * from test_2 limit 10;"" 1>/dev/null; done
{code}

*EXPECTED RESULT:*

All queris are OK.




*ACTUAL RESULT*


{code}
org.apache.hive.service.cli.HiveSQLException: java.io.IOException: java.lang.IllegalStateException: The input format instance has not been properly ini
tialized. Ensure you call initializeTable either in your constructor or initialize method
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:484)
        at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:308)
        at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:847)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1595)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
        at com.sun.proxy.$Proxy25.fetchResults(Unknown Source)
        at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:504)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:698)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1717)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1702)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:521)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:428)
        at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146)
        at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2099)
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:479)
        ... 24 more
Caused by: java.lang.IllegalStateException: The input format instance has not been properly initialized. Ensure you call initializeTable either in your constructor or initialize method
        at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getRegionLocator(TableInputFormatBase.java:579)
        at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getStartEndKeys(TableInputFormatBase.java:225)
        at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:261)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplitsInternal(HiveHBaseTableInputFormat.java:525)
        at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:452)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:372)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:304)
        at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:459)
        ... 28 more
{code}
"
HIVE-16961,Hive on Spark leaks spark application in case user cancels query and closes session,"It's found that a Spark application is leaked when user cancels query and closes the session while Hive is waiting for remote driver to connect back. This is found for asynchronous query execution, but seemingly equally applicable for synchronous submission when session is abruptly closed. The leaked Spark application that runs Spark driver connects back to Hive successfully and run for ever (until HS2 restarts), but receives no job submission because the session is already closed. Ideally, Hive should rejects the connection from the driver so the driver will exist."
HIVE-16933,ORA-00060: deadlock detected while waiting on commit,"When running transactional workload (esp streaming ingest api) with Oracle backed Hive metastore it's possible to see Deadlock exceptions from the DB.

This due to lack of indexes on Foreign Key columns of Acid related metastore tables.
For example, TXN_COMPONENTS references TXNS.  It should have

CREATE INDEX TC_TXNID_INDEX ON TXN_COMPONENTS (TC_TXNID);

{noformat}
2017-06-20 13:42:00,687 ERROR [pool-3-thread-182]: txn.TxnHandler (TxnHandler.java:checkRetryable(1952)) - Too many repeated deadlocks in commitTxn(CommitTxnRequest(txnid:293)), giving up.
2017-06-20 13:42:00,696 ERROR [pool-3-thread-182]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invokeInternal(195)) - MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-00060: deadlock detected while waiting for resource

        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)
        at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)
        at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)
        at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)
        at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)
        at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)
        at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)
        at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)
        at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)
        at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1822)
        at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1787)
        at oracle.jdbc.driver.OracleStatementWrapper.executeUpdate(OracleStatementWrapper.java:280)
        at org.apache.commons.dbcp.DelegatingStatement.executeUpdate(DelegatingStatement.java:228)
        at org.apache.commons.dbcp.DelegatingStatement.executeUpdate(DelegatingStatement.java:228)
        at org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn(TxnHandler.java:756)
{noformat}"
HIVE-16877,NPE when issue query like alter table ... cascade onto non-partitioned table ,"After HIVE-8839 in 1.1.0 support ""alter table ... cascade"" to cascade table changes to partitions as well.  But NPE thrown when issue query like ""alter table ... cascade"" onto non-partitioned table 

Sample Query:
{code}
create table test_cascade_npe (id int);
alter table test_cascade_npe add columns (name string ) cascade;
{code}

Exception stack:
{code}
2017-06-09T22:16:05,913 ERROR [main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
    at org.apache.hadoop.hive.metastore.Warehouse.makePartName(Warehouse.java:547)
    at org.apache.hadoop.hive.metastore.Warehouse.makePartName(Warehouse.java:489)
    at org.apache.hadoop.hive.ql.metadata.Partition.getName(Partition.java:198)
    at org.apache.hadoop.hive.ql.hooks.Entity.computeName(Entity.java:339)
    at org.apache.hadoop.hive.ql.hooks.Entity.<init>(Entity.java:208)
    at org.apache.hadoop.hive.ql.hooks.WriteEntity.<init>(WriteEntity.java:104)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1496)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1473)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableModifyCols(DDLSemanticAnalyzer.java:2685)
    at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:284)
    at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:250)
    at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:474)
    at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1245)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1387)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1174)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1164)
    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:232)
    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:183)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:399)
    at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:776)
    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:714)
    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:641)

{code}
"
HIVE-16845,INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE,"*How to reproduce*
- Create a partitioned table on S3:
{noformat}
CREATE EXTERNAL TABLE s3table(user_id string COMMENT '', event_name string COMMENT '') PARTITIONED BY (reported_date string, product_id int) LOCATION 's3a://<bucket name>'; 
{noformat}
- Create a temp table:
{noformat}
create table tmp_table (id string, name string, date string, pid int) row format delimited fields terminated by '\t' lines terminated by '\n' stored as textfile;
{noformat}
- Load the following rows to the tmp table:
{noformat}
u1	value1	2017-04-10	10000
u2	value2	2017-04-10	10000
u3	value3	2017-04-10	10001
{noformat}
- Set the following parameters:
-- hive.exec.dynamic.partition.mode=nonstrict
-- mapreduce.input.fileinputformat.split.maxsize=10
-- hive.blobstore.optimizations.enabled=true
-- hive.blobstore.use.blobstore.as.scratchdir=false
-- hive.merge.mapfiles=true
- Insert the rows from the temp table into the s3 table:
{noformat}
INSERT OVERWRITE TABLE s3table
PARTITION (reported_date, product_id)
SELECT
  t.id as user_id,
  t.name as event_name,
  t.date as reported_date,
  t.pid as product_id
FROM tmp_table t;
{noformat}

A NPE will occur with the following stacktrace:
{noformat}
2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null
at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)
at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)
at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)
at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)
at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)
at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)
at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)
at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)
at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)
at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)
at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)
... 11 more 
{noformat}"
HIVE-16765,ParquetFileReader should be closed to avoid resource leak,ParquetFileReader should be closed to avoid resource leak
HIVE-16587,NPE when inserting complex types with nested null values,"
{noformat}
CREATE TABLE complex1 (c0 int, c1 array<int>, c2 map<int, string>, c3 struct<f1:int, f2:string, f3:array<int>>, c4 array<struct<f1:int, f2:string, f3:array<int>>>)

insert into complex1
 select 3, array(1, 2, null), map(1, 'one', 2, null), named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1,2,null)), array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2,3,4)))
{noformat}

Gives the following error:
{noformat}
Caused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: NullPointerException null
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:315)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:207)
	at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:291)
	at org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:531)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:517)
	at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
	at com.sun.proxy.$Proxy126.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:530)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437)
	at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException: null
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getWritableSize(StatsUtils.java:1144)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfMap(StatsUtils.java:1106)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getSizeOfComplexTypes(StatsUtils.java:978)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getAvgColLenOf(StatsUtils.java:916)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExpression(StatsUtils.java:1371)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatisticsFromExprMap(StatsUtils.java:1194)
	at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$SelectStatsRule.process(StatsRulesProcFactory.java:187)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
	at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
	at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
	at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:343)
	at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:102)
	at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11382)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:293)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:551)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1371)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1345)
	at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:205)
	... 26 more
{noformat}"
HIVE-16567,NPE when reading Parquet file when getting old timestamp configuration,"In branch-1.2, the file ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java throws an NPE on line 148:

{code}
         boolean skipConversion = Boolean.valueOf(metadata.get(HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname));
{code}

when the metadata reference is null."
HIVE-16329,TopN: use local executor info for LLAP memory checks,"{code}
      // TODO: For LLAP, assumption is off-heap cache.
      final long memoryUsedPerExecutor = (memoryMXBean.getHeapMemoryUsage().getUsed() / numExecutors);
      // this is total free memory available per executor in case of LLAP
      totalFreeMemory = conf.getMaxMemoryAvailable() - memoryUsedPerExecutor;
{code}

{code}
exec.TopNHash: isTez parameters -615768144 = 5312782848 - 71142611912 / 12
{code}

This turns off the TopNHash entirely causing something trivial like 

{code}
select c_custkey, count(1) from customer group by c_custkey limit 10;
{code}

To shuffle 30M rows instead of 10."
HIVE-16321,Possible deadlock in metastore with Acid enabled,"TxnStore.MutexAPI is a mechanism how different Metastore instances can coordinate their operations.  It uses a JDBCConnection to achieve it.

In some cases this may lead to deadlock.  TxnHandler uses a connection pool of fixed size.  Suppose you have X simultaneous calls to  TxnHandler.lock(), where X is >= size of the pool.  This take all connections form the pool, so when
{noformat}
handle = getMutexAPI().acquireLock(MUTEX_KEY.CheckLock.name());
{noformat} 
is executed in _TxnHandler.checkLock(Connection dbConn, long extLockId)_ the pool is empty and the system is deadlocked.

MutexAPI can't use the same connection as the operation it's protecting.  (TxnHandler.checkLock(Connection dbConn, long extLockId) is an example).

We could make MutexAPI use a separate connection pool (size > 'primary' conn pool).

Or we could make TxnHandler.lock(LockRequest rqst) return immediately after enqueueing the lock with the expectation that the caller will always follow up with a call to checkLock(CheckLockRequest rqst).

cc [~f1sherox]

"
HIVE-16249,"With column stats, mergejoin.q throws NPE","stack trace:
{code}
2017-03-17T16:00:26,356 ERROR [3d512d4d-72b5-48fc-92cb-0c72f7c876e5 main] parse.CalcitePlanner: CBO failed, skipping CBO.
java.lang.NullPointerException
        at org.apache.calcite.rel.metadata.RelMdUtil.estimateFilteredRows(RelMdUtil.java:719) ~[calcite-core-1.10.0.jar:1.10.0]
        at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:123) ~[calcite-core-1.10.0.jar:1.10.0]
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:201) ~[calcite-core-1.10.0.jar:1.10.0]
        at org.apache.calcite.rel.metadata.RelMdRowCount.getRowCount(RelMdRowCount.java:132) ~[calcite-core-1.10.0.jar:1.10.0]
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount_$(Unknown Source) ~[?:?]
        at GeneratedMetadataHandler_RowCount.getRowCount(Unknown Source) ~[?:?]
        at org.apache.calcite.rel.metadata.RelMetadataQuery.getRowCount(RelMetadataQuery.java:201) ~[calcite-core-1.10.0.jar:1.10.0]
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.swapInputs(LoptOptimizeJoinRule.java:1866) ~[calcite-core-1.10.0.jar:1.10.0]
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.createJoinSubtree(LoptOptimizeJoinRule.java:1739) ~[calcite-core-1.10.0.jar:1.10.0]
        at org.apache.calcite.rel.rules.LoptOptimizeJoinRule.addToTop(LoptOptimizeJoinRule.java:1216) ~[calcite-core-1.10.0.jar:1.10.0]
{code}"
HIVE-16220,Memory leak when creating a table using location and NameNode in HA,"The following simple DDL

CREATE TABLE `test`(`field` varchar(1)) LOCATION 'hdfs://benderHA/apps/hive/warehouse/test'

ends up generating a huge memory leak in the HiveServer2 service.

After two weeks without a restart, the service stops suddenly because of OutOfMemory errors.

This only happens when we're in an environment in which the NameNode is in HA,  otherwise, nothing (so weird) happens. If the location clause is not present, everything is also fine.


It seems, multiples instances of Hadoop configuration are created when we're in an HA environment:

<AFTER ONE EXECUTIONS OF CREATE TABLE WITH LOCATION>
2.618 instances of ""org.apache.hadoop.conf.Configuration"", loaded by ""sun.misc.Launcher$AppClassLoader @ 0x4d260de88"" 
occupy 350.263.816 (81,66%) bytes. These instances are referenced from one instance of ""java.util.HashMap$Node[]"", 
loaded by ""<system class loader>""

<AFTER TWO EXECUTIONS OF CREATE TABLE WITH LOCATION>
5.216 instances of ""org.apache.hadoop.conf.Configuration"", loaded by ""sun.misc.Launcher$AppClassLoader @ 0x4d260de88"" 
occupy 699.901.416 (87,32%) bytes. These instances are referenced from one instance of ""java.util.HashMap$Node[]"", 
loaded by ""<system class loader>""
"
HIVE-16180,LLAP: Native memory leak in EncodedReader,"Observed this in internal test run. There is a native memory leak in Orc EncodedReaderImpl that can cause YARN pmem monitor to kill the container running the daemon. Direct byte buffers are null'ed out which is not guaranteed to be cleaned until next Full GC. To show this issue, attaching a small test program that allocates 3x256MB direct byte buffers. First buffer is null'ed out but still native memory is used. Second buffer user Cleaner to clean up native allocation. Third buffer is also null'ed but this time invoking a System.gc() which cleans up all native memory. Output from the test program is below

{code}
Allocating 3x256MB direct memory..
Native memory used: 786432000
Native memory used after data1=null: 786432000
Native memory used after data2.clean(): 524288000
Native memory used after data3=null: 524288000
Native memory used without gc: 524288000
Native memory used after gc: 0
{code}

Longer term improvements/solutions:
1) Use DirectBufferPool from hadoop or netty's https://netty.io/4.0/api/io/netty/buffer/PooledByteBufAllocator.html as direct byte buffer allocations are expensive (System.gc() + 100ms thread sleep).
2) Use HADOOP-12760 for proper cleaner invocation in JDK8 and JDK9
"
HIVE-16150,LLAP: HiveInputFormat:getRecordReader: Fix log statements to reduce memory pressure,"Easiest: following needs to be fixed.
LOG.debug(""Found spec for "" + hsplit.getPath() + "" "" + part + "" from "" + pathToPartitionInfo);
"
HIVE-16055,HiveServer2: Prefetch a batch from the result file so that the RPC fetch request has results available in memory,"Currently:
Client Fetch -> RPC -> Server -> File Read (via FetchTask#fetch).

If the Fetch RPC can have the next batch of results available to it in memory, we can return the batch faster to the client. "
HIVE-15992,LLAP: NPE in LlapTaskCommunicator.getCompletedLogsUrl for unsuccessful attempt,"
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.getCompletedLogsUrl(LlapTaskCommunicator.java:563)
        at org.apache.tez.dag.app.TaskCommunicatorWrapper.getCompletedLogsUrl(TaskCommunicatorWrapper.java:92)
        at org.apache.tez.dag.app.TaskCommunicatorManager.getCompletedLogsUrl(TaskCommunicatorManager.java:674)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl.getCompletedLogsUrl(TaskAttemptImpl.java:1147)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl.logJobHistoryAttemptUnsuccesfulCompletion(TaskAttemptImpl.java:1089)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl$TerminateTransition.transition(TaskAttemptImpl.java:1332)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl$TerminatedBeforeRunningTransition.transition(TaskAttemptImpl.java:1430)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl$TerminatedBeforeRunningTransition.transition(TaskAttemptImpl.java:1416)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:799)
        at org.apache.tez.dag.app.dag.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:122)
        at org.apache.tez.dag.app.DAGAppMaster$TaskAttemptEventDispatcher.handle(DAGAppMaster.java:2296)
        at org.apache.tez.dag.app.DAGAppMaster$TaskAttemptEventDispatcher.handle(DAGAppMaster.java:2281)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:115)
        at java.lang.Thread.run(Thread.java:745)
{noformat}"
HIVE-15956,StackOverflowError when drop lots of partitions,"Repro steps:
1. Create partitioned table and add 10000 partitions
{code}
create table test_partition(id int) partitioned by (dt int);

alter table test_partition add partition(dt=1);
alter table test_partition add partition(dt=3);
alter table test_partition add partition(dt=4);
...
alter table test_partition add partition(dt=10000);
{code}

2. Drop 9000 partitions:
{code}
alter table test_partition drop partition(dt<9000);
{code}

Step 2 will fail with StackOverflowError:
{code}
Exception in thread ""pool-7-thread-161"" java.lang.StackOverflowError
    at org.datanucleus.query.expression.ExpressionCompiler.isOperator(ExpressionCompiler.java:819)
    at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:190)
    at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)
    at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192)
    at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)
    at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192)
    at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)
{code}

{code}
Exception in thread ""pool-7-thread-198"" java.lang.StackOverflowError
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:83)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
    at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)
{code}"
HIVE-15912,Executor kill task and Failed to get spark memory/core info,"Hive on Spark, failed with error:
Starting Spark Job = 12a8cb8c-ed0d-4049-ae06-8d32d13fe285
Failed to monitor Job[ 6] with exception 'java.lang.IllegalStateException(RPC channel is closed.)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask

Hive's log:
2017-02-14T19:03:09,147  INFO [stderr-redir-1] client.SparkClientImpl: 17/02/14 19:03:09 INFO yarn.Client: Application report for application_1486905599813_0403 (state: ACCEPTED)
2017-02-14T19:03:10,817  WARN [5bcf13e5-cb54-4cfe-a0d4-9a6556ab48b1 main] spark.SetSparkReducerParallelism: Failed to get spark memory/core info
java.util.concurrent.TimeoutException
        at io.netty.util.concurrent.AbstractFuture.get(AbstractFuture.java:49) ~[netty-all-4.0.29.Final.jar:4.0.29.Final]
        at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.getExecutorCount(RemoteHiveSparkClient.java:155) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.getExecutorCount(RemoteHiveSparkClient.java:165) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.getMemoryAndCores(SparkSessionImpl.java:77) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.process(SetSparkReducerParallelism.java:119) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:158) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.runJoinOptimizations(SparkCompiler.java:291) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.optimizeOperatorPlan(SparkCompiler.java:120) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11085) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:279) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:510) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1302) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1442) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1222) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1212) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:430) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:446) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:749) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642) ~[hive-cli-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_60]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_60]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221) ~[hadoop-common-2.7.1.jar:?]
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136) ~[hadoop-common-2.7.1.jar:?]
2017-02-14T19:03:10,817  INFO [5bcf13e5-cb54-4cfe-a0d4-9a6556ab48b1 main] spark.SetSparkReducerParallelism: Set parallelism for reduce sink RS[24] to: 1 (calculated)


yarn log:
17/02/14 19:05:36 INFO executor.Executor: Finished task 329.0 in stage 15.0 (TID 3865). 3228 bytes result sent to driver
17/02/14 19:05:36 INFO executor.Executor: Finished task 201.0 in stage 15.0 (TID 3737). 3141 bytes result sent to driver
17/02/14 19:05:36 INFO executor.Executor: Finished task 9.0 in stage 15.0 (TID 3545). 4027 bytes result sent to driver
17/02/14 19:05:36 INFO executor.Executor: Finished task 457.0 in stage 15.0 (TID 3993). 3141 bytes result sent to driver
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4221
17/02/14 19:05:36 INFO executor.Executor: Running task 687.0 in stage 15.0 (TID 4221)
17/02/14 19:05:36 INFO executor.Executor: Finished task 265.0 in stage 15.0 (TID 3801). 3141 bytes result sent to driver
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4240
17/02/14 19:05:36 INFO executor.Executor: Running task 692.0 in stage 15.0 (TID 4240)
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4242
17/02/14 19:05:36 INFO executor.Executor: Running task 763.0 in stage 15.0 (TID 4242)
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4243
17/02/14 19:05:36 INFO executor.Executor: Running task 836.0 in stage 15.0 (TID 4243)
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4244
17/02/14 19:05:36 INFO executor.Executor: Running task 863.0 in stage 15.0 (TID 4244)
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4248
17/02/14 19:05:36 INFO executor.Executor: Running task 979.0 in stage 15.0 (TID 4248)
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4280
17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_687 locally
17/02/14 19:05:36 INFO executor.Executor: Running task 990.0 in stage 15.0 (TID 4280)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 687.0 in stage 15.0 (TID 4221)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 73.0 in stage 15.0 (TID 3609)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 763.0 in stage 15.0 (TID 4242)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 979.0 in stage 15.0 (TID 4248)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 990.0 in stage 15.0 (TID 4280)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 863.0 in stage 15.0 (TID 4244)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 836.0 in stage 15.0 (TID 4243)
17/02/14 19:05:36 INFO executor.Executor: Executor is trying to kill task 692.0 in stage 15.0 (TID 4240)
17/02/14 19:05:36 INFO storage.BlockManager: Removing RDD 24
17/02/14 19:05:36 INFO executor.Executor: Finished task 73.0 in stage 15.0 (TID 3609). 3214 bytes result sent to driver
17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_863 locally
17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_692 locally
17/02/14 19:05:36 ERROR executor.Executor: Exception in task 990.0 in stage 15.0 (TID 4280)
org.apache.spark.TaskKilledException
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:264)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/02/14 19:05:36 INFO executor.Executor: Executor killed task 687.0 in stage 15.0 (TID 4221)
17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_836 locally
17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_979 locally
17/02/14 19:05:36 INFO storage.BlockManager: Found block rdd_24_763 locally
17/02/14 19:05:36 INFO executor.Executor: Executor killed task 863.0 in stage 15.0 (TID 4244)
17/02/14 19:05:36 INFO executor.Executor: Executor killed task 979.0 in stage 15.0 (TID 4248)
17/02/14 19:05:36 INFO executor.Executor: Executor killed task 836.0 in stage 15.0 (TID 4243)
17/02/14 19:05:36 INFO executor.Executor: Executor killed task 692.0 in stage 15.0 (TID 4240)
17/02/14 19:05:36 INFO executor.Executor: Executor killed task 763.0 in stage 15.0 (TID 4242)
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Driver from 192.168.1.1:35981 disconnected during shutdown
17/02/14 19:05:36 INFO executor.CoarseGrainedExecutorBackend: Driver from 192.168.1.1:35981 disconnected during shutdown
17/02/14 19:05:36 INFO memory.MemoryStore: MemoryStore cleared
17/02/14 19:05:36 INFO storage.BlockManager: BlockManager stopped
17/02/14 19:05:36 INFO util.ShutdownHookManager: Shutdown hook called"
HIVE-15908,OperationLog's LogFile writer should have autoFlush turned on,"The HS2 offers an API to fetch Operation Log results from the maintained OperationLog file. The reader used inside class OperationLog$LogFile class reads line-by-line on its input stream, for any lines available from the OS's file input perspective.

The writer inside the same class uses PrintStream to write to the file in parallel. However, the PrintStream constructor used sets PrintStream's {{autoFlush}} feature in an OFF state. This causes the BufferedWriter used by PrintStream to accumulate 8k worth of bytes in memory as the buffer before flushing the writes to disk, causing a slowness in the logs streamed back to the client. Every line must be ideally flushed entirely as-its-written, for a smoother experience.

I suggest changing the line inside {{OperationLog$LogFile}} that appears as below:

{code}
out = new PrintStream(new FileOutputStream(file));
{code}

Into:

{code}
out = new PrintStream(new FileOutputStream(file), true);
{code}

This will cause it to use the described autoFlush feature of PrintStream and make for a better reader-log-results-streaming experience: https://docs.oracle.com/javase/7/docs/api/java/io/PrintStream.html#PrintStream(java.io.OutputStream,%20boolean)"
HIVE-15855,throws NPE when using Hplsql UDF,"http://www.hplsql.org/udf

here is error log i get
Caused by: java.lang.NullPointerException
at org.apache.hive.hplsql.Exec.setVariable(Exec.java:148) ~[hive-hplsql-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hive.hplsql.Exec.setVariable(Exec.java:158) ~[hive-hplsql-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hive.hplsql.Udf.setParameters(Udf.java:92) ~[hive-hplsql-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hive.hplsql.Udf.evaluate(Udf.java:74) ~[hive-hplsql-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:187) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:80) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:68) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:438) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:430) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:147) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:2209) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:492) ~[hive-service-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]"
HIVE-15778,DROP INDEX (non-existent) throws NPE when using DbNotificationListener ,"Trying to execute a DROP INDEX operation on a non-existant index throws NPE.  
{code}
0: jdbc:hive2://nightly-unsecure-1.gce.cloude> DROP INDEX IF EXISTS vamsee1 ON sample_07;
INFO  : Compiling command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4): DROP INDEX IF EXISTS vamsee1 ON sample_07
INFO  : Semantic Analysis Completed
INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
INFO  : Completed compiling command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4); Time taken: 0.238 seconds
INFO  : Executing command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4): DROP INDEX IF EXISTS vamsee1 ON sample_07
INFO  : Starting task [Stage-0:DDL] in serial mode
ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException
INFO  : Completed executing command(queryId=hive_20170131162727_663a0909-2a82-44f9-a800-f4a35abaeaa4); Time taken: 0.061 seconds
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException (state=08S01,code=1)
{code}

HMS log:
{code}
2017-01-31 16:27:29,421 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-5-thread-3]: MetaException(message:java.lang.NullPointerException)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:5823)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rethrowException(HiveMetaStore.java:4892)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4403)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
	at com.sun.proxy.$Proxy16.drop_index_by_name(Unknown Source)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10803)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_index_by_name.getResult(ThriftHiveMetastore.java:10787)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)
	at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)
	at org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.buildDropIndexMessage(JSONMessageFactory.java:159)
	at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name_core(HiveMetaStore.java:4469)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_index_by_name(HiveMetaStore.java:4396)
	... 20 more
{code}

Looks like if an exception is raised at [HiveMetaStore#4572|https://github.com/apache/hive/blob/4becd689d59ee3f75a36119fbb950c44e16c65df/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L4572] (which gets triggered if we try to drop an index which doesn't exist) , the control directly jumps to finally block where we are trying to call drop index event on various metastore event listeners [HiveMetaStore#4619|https://github.com/apache/hive/blob/4becd689d59ee3f75a36119fbb950c44e16c65df/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L4619].

If one of the event listeners is DbNotificationListener, then it calls the code under JSONDropIndexMessage.java and this fails with NPE during instantiation at [JSONDropIndexMessage.java#46|https://github.com/apache/hive/blob/4becd689d59ee3f75a36119fbb950c44e16c65df/hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/messaging/json/JSONDropIndexMessage.java#L46] as the exception raised in HiveMetaStore.java wouldn't set the index variable."
HIVE-15766,DBNotificationlistener leaks JDOPersistenceManager,
HIVE-15669,LLAP: Improve aging in shortest job first scheduler,"Under high concurrency, some jobs can gets starved for longer time when hive.llap.task.scheduler.locality.delay is set to -1 (infinitely wait for locality).

"
HIVE-15659,StackOverflowError when ClassLoader.loadClass for Spark,"Sometimes a query needs to process a large number of input files, which could cause the following error:
{code}
17/01/15 09:31:52 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, hadoopworker1344-sjc1.prod.uber.internal): java.lang.StackOverflowError
        at java.util.concurrent.ConcurrentHashMap.putIfAbsent(ConcurrentHashMap.java:1535)
        at java.lang.ClassLoader.getClassLoadingLock(ClassLoader.java:463)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:404)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:411)
{code}

The cause, I think, is that for each input file we may need to load additional jars to the class loader of the current thread. This accumulates with the number of input files. When adding a new class loader, the old class loader will be used as the parent of the new one. 
See [Utilities#getBaseWork|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L388] for more details.

One possible solution is to detect duplicated jar paths before creating the new class loader."
HIVE-15628,Add more logs for hybrid grace hash join during the initial hash table loading,"This can be useful for debugging memory issues.
Metrics that can be possibly added:
1. Log memory usage after say every 50M data has been loaded
2. Add a counter for number of write buffers already allocated
3. Log a snapshot of partitions (memory usage for each of them)"
HIVE-15613,Include druid-handler sources in src packaging,We forgot to do this.
HIVE-15551,memory leak in directsql for mysql+bonecp specific initialization,"We observed HMS memory leak when directsql is enabled for MySQL metastore DB.  

The affected code is in the method MetaStoreDirecdtSql.executeNoResult():

((Connection)jdoConn.getNativeConnection()).createStatement().execute(queryText);

The statement object (from createStatement()) is unfortunately referenced in the Connection object.  Although close() is called on the Connection object  in finally block, the BoneCP just moves it to a freeConnection list. Hence, statement object never get chances to be closed.

The leaked statement object is not huge (~1KB as observed in memory analyzer). However long running Hive Metastore Server is very likely ended up with bad performance doing frequent garbage collection."
HIVE-15543,Don't try to get memory/cores to decide parallelism when Spark dynamic allocation is enabled,"Presently Hive tries to get numbers for memory and cores from the Spark application and use them to determine RS parallelism. However, this doesn't make sense when Spark dynamic allocation is enabled because the current numbers doesn't represent available computing resources, especially when SparkContext is initially launched.

Thus, it makes send not to do that when dynamic allocation is enabled."
HIVE-15527,Memory usage is unbound in SortByShuffler for Spark,"In SortByShuffler.java, an ArrayList is used to back the iterator for values that have the same key in shuffled result produced by spark transformation sortByKey. It's possible that memory can be exhausted because of a large key group.

{code}
            @Override
            public Tuple2<HiveKey, Iterable<BytesWritable>> next() {
              // TODO: implement this by accumulating rows with the same key into a list.
              // Note that this list needs to improved to prevent excessive memory usage, but this
              // can be done in later phase.
              while (it.hasNext()) {
                Tuple2<HiveKey, BytesWritable> pair = it.next();
                if (curKey != null && !curKey.equals(pair._1())) {
                  HiveKey key = curKey;
                  List<BytesWritable> values = curValues;
                  curKey = pair._1();
                  curValues = new ArrayList<BytesWritable>();
                  curValues.add(pair._2());
                  return new Tuple2<HiveKey, Iterable<BytesWritable>>(key, values);
                }
                curKey = pair._1();
                curValues.add(pair._2());
              }
              if (curKey == null) {
                throw new NoSuchElementException();
              }
              // if we get here, this should be the last element we have
              HiveKey key = curKey;
              curKey = null;
              return new Tuple2<HiveKey, Iterable<BytesWritable>>(key, curValues);
            }
{code}

Since the output from sortByKey is already sorted on key, it's possible to backup the value iterable using the same input iterator."
HIVE-15508,LLAP: Find better way to track memory usage per executor,"Many hive operators make runtime decisions based on memory usage. For getting the memory usage, Runtime.getUsed() or MemoryMXBean methods are used. This works fine for MR or Tez but for LLAP the entire memory is shared among multiple executors and each executors can have different memory usage. HIVE-15503 assumes the memory usage is shared across all executors. If we track memory usage per executor, also memory usage by on-heap cache better decisions can be made by the operators. "
HIVE-15503,LLAP: Fix use of Runtime.getRuntime.maxMemory in Hive operators,"{code}
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java:    maxHashTblMemory = (long) (memoryPercentage * Runtime.getRuntime().maxMemory());
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java:    // Total Free Memory = maxMemory() - Used Memory;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java:    long totalFreeMemory = Runtime.getRuntime().maxMemory() -
{code}

This will not work very well with LLAP because of the memory sharing by executors. "
HIVE-15353,Metastore throws NPE if StorageDescriptor.cols is null,"When using the HiveMetaStoreClient API directly to talk to the metastore, you get NullPointerExceptions when StorageDescriptor.cols is null in the Table/Partition object in the following calls:
 * create_table
 * alter_table
 * alter_partition

Calling add_partition with StorageDescriptor.cols set to null causes null to be stored in the metastore database and subsequent calls to alter_partition for that partition to fail with an NPE.

Null checks should be added to eliminate the NPEs in the metastore."
HIVE-15347,LLAP: Executor memory and Xmx should have some headroom for other services,"If executor memory + cache memory is configured close or equal to Xmx, the task attempts that is causing OOM can take down the LLAP daemon. Provide some leeway for other services during memory crunch. "
HIVE-15275,"""beeline -f <file>"" will throw NPE ","Execute {{""beeline -f <file>""}} and the command will throw the following NPE exception.

{noformat}
2016-11-23T13:34:54,367 WARN [Thread-1] org.apache.hadoop.util.ShutdownHookManager - ShutdownHook '' failed, java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hive.beeline.BeeLine$1.run(BeeLine.java:1247) ~[hive-beeline-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54) [hadoop-common-2.7.3.jar:?]
{noformat}"
HIVE-15221,"Improvement for MapJoin checkMemoryStatus, adding gc before throwing Exception","i see in the current master version
{code:title=MapJoinMemoryExhaustionHandler.java|borderStyle=solid}
  public void checkMemoryStatus(long tableContainerSize, long numRows)
  throws MapJoinMemoryExhaustionException {
    long usedMemory = memoryMXBean.getHeapMemoryUsage().getUsed();
    double percentage = (double) usedMemory / (double) maxHeapSize;
    String msg = Utilities.now() + ""\tProcessing rows:\t"" + numRows + ""\tHashtable size:\t""
        + tableContainerSize + ""\tMemory usage:\t"" + usedMemory + ""\tpercentage:\t"" + percentageNumberFormat.format(percentage);
    console.printInfo(msg);
    if(percentage > maxMemoryUsage) {
      throw new MapJoinMemoryExhaustionException(msg);
    }
   }
{code}

if  {{percentage > maxMemoryUsage}}, then throw MapJoinMemoryExhaustionException

in my opinion, running is better than fail. after System.gc, ' if percentage > maxMemoryUsage, then throw MapJoinMemoryExhaustionException' maybe better

And original checking way has a problem: 1) consuming much memory cause gc (e.g young gc), then check after adding row and pass. 2) consuming much memory does not cause gc, then check after adding rows but throw Exception

sometimes 2) occurs, but it contians less rows than 1)."
HIVE-15214,"LLAP: Offer a ""slow"" mode to debug race conditions in package builder ","HIVE-15125 is enabled by default, add an option to disable parallel generation of data."
HIVE-15213,"LLAP: Offer a ""slow"" mode to debug race conditions in package builder ","HIVE-15125 is enabled by default, add an option to disable parallel generation of data."
HIVE-15186,CBO: use database triggers CBO planner & NPEs,"{code}
hive> use tpcds_bin_partitioned_orc_1000;
FAILED: NullPointerException null
{code}


Quick note - org.apache.hadoop.hive.cli.CliDriver.processSelectDatabase triggers the stats NPE.

{code}
2016-11-11T21:26:28,971 ERROR [4da6dff2-ab08-4471-a32d-511a380ddbda main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.estimateRowSizeFromSchema(StatsUtils.java:538)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getNumRows(StatsUtils.java:178)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:202)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10951)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:270)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:509)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1299)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1439)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1219)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1209)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:336)
        at org.apache.hadoop.hive.cli.CliDriver.processSelectDatabase(CliDriver.java:503)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}"
HIVE-15128,"""hive.exec.stagingdir"" produces an ""AccessControlException"" when working with temp tables","The property ""hive.exec.stagingdir"" is producing an AccessControlException error, only when working with a temporary table and with an INSERT INTO TABLE statement.
The configuration used in the hive-site.xml is:
{noformat}
  <property>
      <name>hive.exec.stagingdir</name>
      <value>/user/${user.name}/.hiveStaging/.staging</value>
      <description>In Hive >= 0.14, set to ${hive.exec.scratchdir}/${user.name}/.staging</description>
  </property>
{noformat}

Trying to insert a value into a temporary table will produce the following output:
{noformat}
hive (shfs3453)> INSERT INTO TABLE test_table_tmp VALUES (11101, 'John', 'Oakland');
Query ID = shfs3453_20161103180522_5fceae9b-6fb5-4cdd-a0d9-37cc93b27a9f
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1478096256452_0018)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 16.09 s
----------------------------------------------------------------------------------------------
Loading data to table shfs3453.test_table_tmp
Failed with exception org.apache.hadoop.security.AccessControlException: User does not belong to hadoop
        at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:86)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1676)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:702)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:464)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)

FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. org.apache.hadoop.security.AccessControlException: User does not belong to hadoop
        at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setOwner(FSDirAttrOp.java:86)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwner(FSNamesystem.java:1676)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setOwner(NameNodeRpcServer.java:702)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setOwner(ClientNamenodeProtocolServerSideTranslatorPB.java:464)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)
{noformat}

But if you do an INSERT OVERWRITE TABLE, there will be a warning only. No AccessControlException error here and erverything is properly inserted.

{noformat}
hive (shfs3453)> INSERT OVERWRITE TABLE test_table_tmp VALUES (11101, 'John', 'Oakland');
Query ID = shfs3453_20161103180603_d8d5f689-cc55-44d0-b160-bba0df8d186a
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1478096256452_0018)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      2          2        0        0       0       0
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 6.74 s
----------------------------------------------------------------------------------------------
Loading data to table shfs3453.test_table_tmp
chgrp: changing ownership of 'hdfs://sandbox/tmp/hive/shfs3453/edab56b7-b1e1-4f87-bdfd-eeece41f6fc9/_tmp_space.db/874bfdbd-7bab-4e0c-85ac-ec4552ea560b': User does not belong to hadoop
OK
Time taken: 8.224 seconds
{noformat}

If the previous query is done one more time, the warning will not be displayed.

{noformat}
hive (shfs3453)> INSERT OVERWRITE TABLE test_table_tmp VALUES (11101, 'John', 'Oakland');
Query ID = shfs3453_20161103180831_40350dc3-1f66-45e2-8304-90b00933aead
Total jobs = 1
Launching Job 1 out of 1


Status: Running (Executing on YARN cluster with App id application_1478096256452_0018)

----------------------------------------------------------------------------------------------
        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED
----------------------------------------------------------------------------------------------
Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0
----------------------------------------------------------------------------------------------
VERTICES: 01/01  [==========================>>] 100%  ELAPSED TIME: 13.00 s
----------------------------------------------------------------------------------------------
Loading data to table shfs3453.test_table_tmp
OK
Time taken: 14.161 seconds
{noformat}

Note: if the warning is displayed one time, an INSERT INTO TABLE will work."
HIVE-15107,HiveLexer can throw NPE in allowQuoteId,"In HiveLexer.allowQuoteId we reference the HiveConf field, which may be null.  The configuration field is set in ParseDriver only if the hive.ql.Context variable is not null. ParseDriver exposes API such as org.apache.hadoop.hive.ql.parse.ParseDriver#parse(java.lang.String) which can result in the hive.ql.Context field to be null."
HIVE-15105,Hive shell runs out of memory on Tez,"Hive 2.0.1
Hadoop 2.7.2
Tex 0.8.4

We have a UDF in hive which take in some values and outputs a score. When running a query on a table which calls the score function on every row, looks like tez is not running the query on YARN, but trying to run it in local mode. It then runs out of memory trying to insert that data into a table.

Here's the query

{noformat}
ADD JAR score.jar;
CREATE TEMPORARY FUNCTION score AS 'hive.udf.ScoreUDF';

CREATE TABLE abc AS
SELECT
    id,
    score(col1, col2) as score
    , '2016-10-11' AS dt
FROM input_table
;
{noformat}

Here's the output of the shell

{noformat}
Query ID = hadoop_20161028232841_5a06db96-ffaa-4e75-a657-c7cb46ccb3f5
Total jobs = 1
Launching Job 1 out of 1
java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:3332)
        at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)
        at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)
        at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:622)
        at java.lang.StringBuilder.append(StringBuilder.java:202)
        at com.google.protobuf.TextFormat.escapeBytes(TextFormat.java:1283)
        at com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:394)
        at com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:327)
        at com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:286)
        at com.google.protobuf.TextFormat$Printer.print(TextFormat.java:273)
        at com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:404)
        at com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:327)
        at com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:286)
        at com.google.protobuf.TextFormat$Printer.print(TextFormat.java:273)
        at com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:404)
        at com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:327)
        at com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:286)
        at com.google.protobuf.TextFormat$Printer.print(TextFormat.java:273)
        at com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:404)
        at com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:327)
        at com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:283)
        at com.google.protobuf.TextFormat$Printer.print(TextFormat.java:273)
        at com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:404)
        at com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:327)
        at com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:283)
        at com.google.protobuf.TextFormat$Printer.print(TextFormat.java:273)
        at com.google.protobuf.TextFormat$Printer.printFieldValue(TextFormat.java:404)
        at com.google.protobuf.TextFormat$Printer.printSingleField(TextFormat.java:327)
        at com.google.protobuf.TextFormat$Printer.printField(TextFormat.java:286)
        at com.google.protobuf.TextFormat$Printer.print(TextFormat.java:273)
        at com.google.protobuf.TextFormat$Printer.access$400(TextFormat.java:248)
        at com.google.protobuf.TextFormat.shortDebugString(TextFormat.java:88)
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Java heap space
{noformat}


It looks like the job is not getting submitted to the cluster, but running locally. We can't get tez to run the query on the cluster. 
The hive shell starts with an Xmx of 4G. 

If I set hive.execution.engine = mr, then the query works, because it runs on the hadoop cluster. 
"
HIVE-14974,TestBeeLineHistory throws NPE in ShutdownHook,"{code}
2016-10-14T22:51:33,072  INFO [main] beeline.TestBeelineArgParsing: Add /home/hiveptest/104.155.175.228-hiveptest-0/apache-github-source-source/beeline/target/test-classes/DummyDriver.jar for the driver class DummyDriver
Fail to add local jar due to the exception:java.util.zip.ZipException: error in opening zip file
error in opening zip file
Fail to scan drivers due to the exception:java.util.zip.ZipException: error in opening zip file
error in opening zip file
{code}"
HIVE-14924,MSCK REPAIR table with single threaded is throwing null pointer exception,"MSCK REPAIR TABLE is throwing Null Pointer Exception while running on single threaded mode (hive.mv.files.thread=0)

Error:
2016-10-10T22:27:13,564 ERROR [e9ce04a8-2a84-426d-8e79-a2d15b8cee09 main([])]: exec.DDLTask (DDLTask.java:failed(581)) - java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkPartitionDirs(HiveMetaStoreChecker.java:423)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownPartitions(HiveMetaStoreChecker.java:315)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:291)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:236)
	at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkMetastore(HiveMetaStoreChecker.java:113)
	at org.apache.hadoop.hive.ql.exec.DDLTask.msck(DDLTask.java:1834)

In order to reproduce:

set hive.mv.files.thread=0 and run MSCK REPAIR TABLE command"
HIVE-14895,CompactorMR.CompactorOutputCommitter race condition,"commitJob() method renames each file in TMP_LOCATION separately.  so someone may read FINAL_LOCATION and see only some of the files that should be there.
Use FileSystem.getFileStatus(TMP_LOCATION) and just rename the dir."
HIVE-14847,HiveServer2: Implement some admission control mechanism for graceful degradation when resources are exhausted,"An example of where it is needed: it has been reported that when # of client connections is greater than   {{hive.server2.thrift.max.worker.threads}}, HiveServer2 stops accepting new connections and ends up having to be restarted. This should be handled more gracefully by the server and the JDBC driver, so that the end user gets aware of the problem and can take appropriate steps (either close existing connections or bump of the config value or use multiple server instances with dynamic service discovery enabled). Similarly, we should also review the behaviour of background thread pool to have a well defined behavior on the the pool getting exhausted. 

Ideally implementing some form of general admission control will be a better solution, so that we do not accept new work unless sufficient resources are available and display graceful degradation under overload."
HIVE-14814,metastoreClient is used directly in Hive cause NPE,Changes introduced by HIVE-13622 uses metastoreClient directly in Hive.java which may be null causing NPE. Instead it should use getMSC() which will initialize metastoreClient variable when null.
HIVE-14798,MSCK REPAIR TABLE throws null pointer exception,"MSCK REPAIR TABLE statement throws null pointer exception in Hive 2.1
I have tested the same against external/internal tables created both in HDFS and in Google Cloud.

The error shown in beeline/sql client 
Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask (state=08S01,code=1)

Hive Logs:

2016-09-20T17:28:00,717 ERROR [HiveServer2-Background-Pool: Thread-92]: metadata.HiveMetaStoreChecker (:()) - java.lang.NullPointerException
2016-09-20T17:28:00,717 WARN  [HiveServer2-Background-Pool: Thread-92]: exec.DDLTask (:()) - Failed to run metacheck: 
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.getAllLeafDirs(HiveMetaStoreChecker.java:444)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.getAllLeafDirs(HiveMetaStoreChecker.java:388)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.findUnknownPartitions(HiveMetaStoreChecker.java:309)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:285)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkTable(HiveMetaStoreChecker.java:230)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.checkMetastore(HiveMetaStoreChecker.java:109)
        at org.apache.hadoop.hive.ql.exec.DDLTask.msck(DDLTask.java:1814)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:403)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:197)
         at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1858)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1562)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1313)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1084)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1077)
        at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:235)
        at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:90)
        at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:299)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:312)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.putVal(ConcurrentHashMap.java:1011)
        at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:1006)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker$1.call(HiveMetaStoreChecker.java:432)
        at org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker$1.call(HiveMetaStoreChecker.java:418)
        ... 4 more

Here are the steps to recreate this issue:
use default;
DROP TABLE IF EXISTS repairtable;
CREATE TABLE repairtable(col STRING) PARTITIONED BY (p1 STRING, p2 STRING);
MSCK REPAIR TABLE default.repairtable;"
HIVE-14773,NPE aggregating column statistics for date column in partitioned table,"Hive runs into a NPE when the query has a filter on a date column and the partitioned column 
eg: 

{code}
create table date_dim (d_date date) partitioned by (d_date_sk bigint) stored as orc;
set hive.exec.dynamic.partition.mode=nonstrict;
insert into date_dim partition(d_date_sk=2416945) values('1905-04-09');
insert into date_dim partition(d_date_sk=2416946) values('1905-04-10');
insert into date_dim partition(d_date_sk=2416947) values('1905-04-11');
analyze table date_dim partition(d_date_sk) compute statistics for columns;

explain select count(*) from date_dim where d_date > date ""1900-01-02"" and d_date_sk= 2416945;
{code}

Here d_date_sk is a partition column and d_date is of type date.

{code}
2016-09-16T08:27:06,510 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.AggregateStatsCache: No aggregate stats cached for database:default, table:date_dim, column:d_date
2016-09-16T08:27:06,512 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: Direct SQL query in 1.302231ms + 0.00653ms, the query is [select ""COLUMN_NAME"", ""COLUMN_TYPE"", min(""LONG_LOW_VALUE""), max(""LONG_HIGH_VALUE""), min(""DOUBLE_LOW_VALUE""), max(""DOUBLE_HIGH_VALUE""), min(cast(""BIG_DECIMAL_LOW_VALUE"" as decimal)), max(cast(""BIG_DECIMAL_HIGH_VALUE"" as decimal)), sum(""NUM_NULLS""), max(""NUM_DISTINCTS""), max(""AVG_COL_LEN""), max(""MAX_COL_LEN""), sum(""NUM_TRUES""), sum(""NUM_FALSES""), avg((""LONG_HIGH_VALUE""-""LONG_LOW_VALUE"")/cast(""NUM_DISTINCTS"" as decimal)),avg((""DOUBLE_HIGH_VALUE""-""DOUBLE_LOW_VALUE"")/""NUM_DISTINCTS""),avg((cast(""BIG_DECIMAL_HIGH_VALUE"" as decimal)-cast(""BIG_DECIMAL_LOW_VALUE"" as decimal))/""NUM_DISTINCTS""),sum(""NUM_DISTINCTS"") from ""PART_COL_STATS"" where ""DB_NAME"" = ? and ""TABLE_NAME"" = ?  and ""COLUMN_NAME"" in (?) and ""PARTITION_NAME"" in (?) group by ""COLUMN_NAME"", ""COLUMN_TYPE""]
2016-09-16T08:27:06,526  INFO [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.MetaStoreDirectSql: useDensityFunctionForNDVEstimation = false
partsFound = 1
ColumnStatisticsObj = [ColumnStatisticsObj(colName:d_date, colType:date, statsData:<ColumnStatisticsData >)]
2016-09-16T08:27:06,526 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: Commit transaction: count = 0, isactive true at:
        org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.commit(ObjectStore.java:2827)
2016-09-16T08:27:06,531 DEBUG [90d4780f-77e4-4704-9907-4860ce11a206 main] metastore.ObjectStore: null retrieved using SQL in 43.425925ms
2016-09-16T08:27:06,545 ERROR [90d4780f-77e4-4704-9907-4860ce11a206 main] ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:451)
        at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getDateStats(ColumnStatisticsData.java:574)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:759)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:806)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:304)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:152)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:140)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:126)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:260)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:129)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:140)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10928)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:255)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:251)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:467)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:342)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1235)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1355)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1143)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1131)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:400)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:777)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:715)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:642)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}"
HIVE-14772,NPE when MSCK REPAIR,"HiveMetaStoreChecker throws NullPointerException when doing a MSCK REPAIR TABLE.

The bug is here:

{code}
...
18      package org.apache.hadoop.hive.ql.metadata;
...
58      public class HiveMetaStoreChecker {
...
408        if (!directoryFound) {
409         allDirs.put(path, null);
410        }
...
{code}

allDirs is a ConcurrentHashMap and those does not allow either key or value to be null.

I found the bug while trying to port https://github.com/klarna/HiveRunner to Hive 2.1.0

Implemented explicit test case that exposes the bug here: 
https://github.com/klarna/HiveRunner/blob/hive-2.1.0-NPE-at-msck-repair/src/test/java/com/klarna/hiverunner/MSCKRepairNPE.java

Reproduce by cloning branch https://github.com/klarna/HiveRunner/tree/hive-2.1.0-NPE-at-msck-repair
and run 
{code} mvn -Dtest=MSCKRepairNPE clean test{code}

( Does not work on windows :( )

Looks like this email thread talks about the same issue: 
http://user.hive.apache.narkive.com/ETOpbKk5/msck-repair-table-and-hive-v2-1-0
"
HIVE-14742,Hive on spark throws NPE exception for union all query ,"{noformat}
create table foo (fooId string, fooData string) partitioned by (fooPartition string) stored as parquet;
insert into foo partition (fooPartition = '1') values ('1', '1'), ('2', '2');
set hive.execution.engine=spark;
select * from ( 
select 
fooId as myId, 
fooData as myData 
from foo where fooPartition = '1' 
union all 
select 
fooId as myId, 
fooData as myData 
from foo where fooPartition = '3' 
) allData;
{noformat}

Error while compiling statement: FAILED: NullPointerException null"
HIVE-14739,Replace runnables directly added to runtime shutdown hooks to avoid deadlock,"[~deepesh] reported that a deadlock can occur when running queries through hive cli. [~cnauroth] analyzed it and reported that hive adds shutdown hooks directly to java Runtime which may execute in non-deterministic order causing deadlocks with hadoop's shutdown hooks. In one case, hadoop shutdown locked FileSystem#Cache and FileSystem.close whereas hive shutdown hook locked FileSystem.close and FileSystem#Cache order causing a deadlock. 
Hive and Hadoop has ShutdownHookManager that runs the shutdown hooks in deterministic order based on priority. We should use that to avoid deadlock throughout the code."
HIVE-14727,llap-server may case file descriptor leak in BuddyAllocator class,"llap-server,the method preallocate(int) of  BuddyAllocator may case file descriptor leak when FileChannel map allocate memory error.

the code:
        //here if failed
         ByteBuffer rwbuf = rwf.getChannel().map(MapMode.READ_WRITE, 0, arenaSize);
        // A mapping, once established, is not dependent upon the file channel that was used to
        // create it. delete file and hold onto the map
       //can not close() and delete file
        rwf.close();
        rf.delete();
"
HIVE-14694,UDF rand throws NPE when input data is NULL,"When use {{rand}} function with null, HiveServer throws NPE:
{code}
0: jdbc:hive2://10.64.35.144:21066/> desc foo1;
+-----------+------------+----------+--+
| col_name  | data_type  | comment  |
+-----------+------------+----------+--+
| c1        | bigint     |          |
+-----------+------------+----------+--+
1 row selected (0.075 seconds)
0: jdbc:hive2://10.64.35.144:21066/> select * from foo1;
+----------+--+
| foo1.c1  |
+----------+--+
| NULL     |
| 1        |
| 2        |
+----------+--+
3 rows selected (0.124 seconds)
0: jdbc:hive2://10.64.35.144:21066/> select rand(c1) from foo1;
Error: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.hive.serde2.io.DoubleWritable org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(org.apache.hadoop.io.LongWritable)  on object org.apache.hadoop.hive.ql.udf.UDFRand@37a2b47b of class org.apache.hadoop.hive.ql.udf.UDFRand with arguments {null} of size 1 (state=,code=0)
{code}

Stack trace:
{code}
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.invoke(FunctionRegistry.java:1010)
        ... 36 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.UDFRand.evaluate(UDFRand.java:57)
        ... 40 more
{code}"
HIVE-14658,UDF abs throws NPE when input arg type is string,"I know this is not the right use case, but NPE is not exptected.

{code}
0: jdbc:hive2://10.64.35.144:21066/> select abs(""foo"");
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)
{code}"
HIVE-14617,NPE in UDF MapValues() if input is null,"For query
{code}
select exploded_traits from hdrone.vehiclestore_udr_vehicle 
lateral view explode(map_values(vehicle_traits.vehicle_traits)) traits as exploded_traits 
where datestr > '2016-08-22' LIMIT 100
{code}
Job fails with error msg as follows:
{code}
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""ts"":null,""_max_added_id"":null,""identity_info"":null,""vehicle_specs"":null,""tracking_info"":null,""color_info"":null,""vehicle_traits"":null,""detail_info"":null,""_row_key"":null,""_shard"":null,""image_info"":null,""vehicle_tags"":null,""activation_info"":null,""flavor_info"":null,""sounds"":null,""legacy_info"":null,""images"":null,""datestr"":""2016-08-24""} at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:179) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""ts"":null,""_max_added_id"":null,""identity_info"":null,""vehicle_specs"":null,""tracking_info"":null,""color_info"":null,""vehicle_traits"":null,""detail_info"":null,""_row_key"":null,""_shard"":null,""image_info"":null,""vehicle_tags"":null,""activation_info"":null,""flavor_info"":null,""sounds"":null,""legacy_info"":null,""images"":null,""datestr"":""2016-08-24""} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170) ... 8 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating map_values(vehicle_traits.vehicle_traits) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:82) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.processOp(LateralViewForwardOperator.java:37) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497) ... 9 more Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapValues.evaluate(GenericUDFMapValues.java:64) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:185) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:77) ... 15 more 
{code}
It appears that null is not properly handled in GenericUDFMapValues.evaluate() method."
HIVE-14616,CBO phase fails with NPE when quering a table with array field,"A ""Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)"" message appears when issuing a query involving a table with array. Statistics where gathered issuing:

analyze table test.table_with_array compute statistics;
analyze table test.table_with_array compute statistics for columns
<all_columns_but_array_field>;

We couldn't compute statistics for all columns because we got a ""FAILED: UDFArgumentTypeException Only primitive type arguments are accepted but array<string> is passed.""

The hive log shows:

2016-08-24 02:35:29,987 INFO  [HiveServer2-Handler-Pool: Thread-145434]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(747)) - 485: get_table_statistics_req: db=test table=table_with_array
2016-08-24 02:35:29,988 INFO  [HiveServer2-Handler-Pool: Thread-145434]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(372)) - ugi=user    ip=unknown-ip-addr      cmd=get_table_statistics_req: db=test table=table_with_array
2016-08-24 02:35:30,002 ERROR [HiveServer2-Handler-Pool: Thread-145434]: ql.Driver (SessionState.java:printError(932)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:695)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:741)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:730)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:185)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:138)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:126)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:110)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:56)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)

Best wishes"
HIVE-14602,"NPE when running delete_orig_table.q,update_orig_table.q under MiniLlap","When delete_orig_table.q,update_orig_table.q is run under TestMiniLlapCliDriver, NPE is thrown

{code}
], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1471902873303_0001_31_01_000000_1:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":null},""value"":null}
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:370)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)
        at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)
        at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":null},""value"":null}
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:289)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:279)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:185)
        ... 15 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":null},""value"":null}
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:357)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:279)
        ... 17 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:36)
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:764)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:879)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)
        at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:348)
        ... 18 more
{code}"
HIVE-14524,BaseSemanticAnalyzer may leak HMS connection,"Currently {{BaseSemanticAnalyzer}} keeps a copy of thread-local {{Hive}} object to connect to HMS. However, in some cases Hive may overwrite the existing {{Hive}} object:

{{Hive#getInternal}}:
{code}
  private static Hive getInternal(HiveConf c, boolean needsRefresh, boolean isFastCheck,
      boolean doRegisterAllFns) throws HiveException {
    Hive db = hiveDB.get();
    if (db == null || !db.isCurrentUserOwner() || needsRefresh
        || (c != null && db.metaStoreClient != null && !isCompatible(db, c, isFastCheck))) {
      return create(c, false, db, doRegisterAllFns);
    }
    if (c != null) {
      db.conf = c;
    }
    return db;
  }
{code}

*This poses an potential problem*: if one first instantiates a {{BaseSemanticAnalyzer}} object with the current {{Hive}} object (let's call it A), and for some reason A is overwritten by B with the code above, then {{BaseSemanticAnalyzer}} may keep using A to contact HMS, which will leak connections.

This can be reproduced by the following steps:
1. open a session
2. execute some simple query such as {{desc formatted src}}
3. change a metastore property (I know, this is not a perfect example...), for instance: {{set hive.txn.timeout=500}}
4. run another command such as {{desc formatted src}} again

Notice that in step 4), since a metavar is changed the {{isCompatible}} will return false, and hence a new {{Hive}} object is created. As result, you'll observe in the HS2 log that an connection has been leaked.
"
HIVE-14446,Add switch to control BloomFilter in Hybrid grace hash join and make the FPP adjustable,"When row count exceeds certain limit, it doesn't make sense to generate a bloom filter, since its size will be a few hundred MB or even a few GB."
HIVE-14416,NPE trying to move results of a subselect into an RDBMS using HPL/SQL,"I was trying to use HPL/SQL to move some records from Hive to MySQL using this script:

{code}
MAP OBJECT remote TO pgtable AT mydbconn;
insert into remote values ( select * from hivetable );
{code}

When I run this script I get this NPE:
{code}
Open connection: jdbc:hive2://hdp250.example.com:10000 (285 ms)
Starting query
Query executed successfully (127 ms)
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hive.hplsql.Select.getIntoCount(Select.java:405)
	at org.apache.hive.hplsql.Select.select(Select.java:88)
	at org.apache.hive.hplsql.Exec.visitSelect_stmt(Exec.java:1002)
	at org.apache.hive.hplsql.Exec.visitSelect_stmt(Exec.java:52)
	at org.apache.hive.hplsql.HplsqlParser$Select_stmtContext.accept(HplsqlParser.java:14768)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visitChildren(AbstractParseTreeVisitor.java:70)
	at org.apache.hive.hplsql.Exec.visitStmt(Exec.java:994)
	at org.apache.hive.hplsql.Exec.visitStmt(Exec.java:52)
	at org.apache.hive.hplsql.HplsqlParser$StmtContext.accept(HplsqlParser.java:1012)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visitChildren(AbstractParseTreeVisitor.java:70)
	at org.apache.hive.hplsql.HplsqlBaseVisitor.visitBlock(HplsqlBaseVisitor.java:28)
	at org.apache.hive.hplsql.HplsqlParser$BlockContext.accept(HplsqlParser.java:446)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visitChildren(AbstractParseTreeVisitor.java:70)
	at org.apache.hive.hplsql.Exec.visitProgram(Exec.java:901)
	at org.apache.hive.hplsql.Exec.visitProgram(Exec.java:52)
	at org.apache.hive.hplsql.HplsqlParser$ProgramContext.accept(HplsqlParser.java:389)
	at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:42)
	at org.apache.hive.hplsql.Exec.run(Exec.java:760)
	at org.apache.hive.hplsql.Exec.run(Exec.java:736)
	at org.apache.hive.hplsql.Hplsql.main(Hplsql.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:233)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:148)
{code}

This is my configuration:
{code}
<configuration>
  <property>
    <name>hplsql.conn.default</name>
    <value>myhiveconn</value>
  </property>

  <property>
    <name>hplsql.conn.myhiveconn</name>
    <value>org.apache.hive.jdbc.HiveDriver;jdbc:hive2://hdp250.example.com:10000</value>
  </property>

  <property>
    <name>hplsql.conn.mydbconn</name>
    <value>com.mysql.jdbc.Driver;jdbc:mysql://hdp250.example.com:3306/hive;hive;vagrant</value>
  </property>
</configuration>
{code}"
HIVE-14402,Vectorization: Fix Mapjoin overflow deserialization ,"This is in a codepath currently disabled in master, however enabling it triggers OOB.

{code}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024
        at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setRef(BytesColumnVector.java:92)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserializeRowColumn(VectorDeserializeRow.java:415)
        at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorDeserializeRow.java:674)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultLargeMultiValue(VectorMapJoinGenerateResultOperator.java:307)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultMultiValue(VectorMapJoinGenerateResultOperator.java:226)
        at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultRepeatedAll(VectorMapJoinGenerateResultOperator.java:391)
{code}"
HIVE-14344,Intermittent failures caused by leaking delegation tokens,"We have experienced random job failures caused by leaking delegation tokens. The Tez child task will fail because it is attempting to read from the delegation tokens directory of a different (related) task.

Failure results in the following type of stack trace:

{noformat}
2016-07-21 16:57:18,061 [FATAL] [TezChild] |tez.ReduceRecordSource|: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) 
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:370)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:292)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:249)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:362)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1738)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: java.io.IOException: Exception reading file:/grid/4/tmp/yarn-local/usercache/.../appcache/application_1468602386465_489814/container_e02_1468602386465_489814_01_000001/container_tokens
	at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.first(RowContainer.java:237)
	at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.first(RowContainer.java:74)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genUniqueJoinObject(CommonJoinOperator.java:650)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:756)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinObject(CommonMergeJoinOperator.java:316)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:279)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinOneGroup(CommonMergeJoinOperator.java:272)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.process(CommonMergeJoinOperator.java:258)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:361)
	... 17 more
Caused by: java.lang.RuntimeException: java.io.IOException: Exception reading file:/grid/4/tmp/yarn-local/usercache/.../appcache/application_1468602386465_489814/container_e02_1468602386465_489814_01_000001/container_tokens
	at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:141)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:119)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:206)
	at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)
	at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.first(RowContainer.java:222)
	... 25 more
Caused by: java.io.IOException: Exception reading file:/grid/4/tmp/yarn-local/usercache/.../appcache/application_1468602386465_489814/container_e02_1468602386465_489814_01_000001/container_tokens
	at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:175)
	at org.apache.hadoop.mapreduce.security.TokenCache.mergeBinaryTokens(TokenCache.java:136)
	... 32 more
Caused by: java.io.FileNotFoundException: File file:/grid/4/tmp/yarn-local/usercache/.../appcache/application_1468602386465_489814/container_e02_1468602386465_489814_01_000001/container_tokens does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:142)
	at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:346)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:768)
	at org.apache.hadoop.security.Credentials.readTokenStorageFile(Credentials.java:170)
	... 33 more
{noformat}

The application that failed was {{application_1468602386465_489844}} while complaining about {{appcache/application_1468602386465_489814/container_e02_1468602386465_489814_01_000001/container_tokens}}.

This seems to only manifest via HiveAction through Oozie."
HIVE-14171,Parquet: Simple vectorization throws NPEs,"{code}
 create temporary table cd_parquet stored as parquet as select * from customer_demographics;

select count(1) from cd_parquet where cd_gender = 'F';
{code}

{code}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:206)
	at org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat$VectorizedParquetRecordReader.next(VectorizedParquetInputFormat.java:118)
	at org.apache.hadoop.hive.ql.io.parquet.VectorizedParquetInputFormat$VectorizedParquetRecordReader.next(VectorizedParquetInputFormat.java:51)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)
	... 17 more
{code}"
HIVE-14109,query execuction throws NPE when hive.exec.submitviachild is set to true,"If we set hive.exec.submitviachild to true and execute select count(*) from src, the following exception is thrown.

Seems queryState is not initialized when ExecDriver is called from main() in ExecDriver.

{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:262)
        at org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.progress(HadoopJobExecHelper.java:555)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:436)
        at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:756)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
{noformat}"
HIVE-14090,JDOExceptions thrown by the Metastore have their full stack trace returned to clients,"When user try to create any database or table with a name longer than 128 characters:

{code}
create database test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongNametableFAIL;
{code}

It dumps the full exception stack-trace in a non-user-friendly message. The lends to relatively negative user-experience for Beeline users who hit this exception, they are generally not interested in the full stack-trace.

The formatted stack-trace is below:

{code}
Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOFatalUserException: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data!
at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:528)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:569)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy10.createDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:923)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:962)
at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
at com.sun.proxy.$Proxy12.create_database(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8863)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8847)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:707)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:702)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:702)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745) NestedThrowablesStackTrace: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data! org.datanucleus.exceptions.NucleusUserException: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data!
at org.datanucleus.store.rdbms.mapping.datastore.CharRDBMSMapping.setString(CharRDBMSMapping.java:263)
at org.datanucleus.store.rdbms.mapping.java.SingleFieldMapping.setString(SingleFieldMapping.java:201)
at org.datanucleus.store.rdbms.fieldmanager.ParameterSetter.storeStringField(ParameterSetter.java:159)
at org.datanucleus.state.JDOStateManager.providedStringField(JDOStateManager.java:1256)
at org.apache.hadoop.hive.metastore.model.MDatabase.jdoProvideField(MDatabase.java)
at org.apache.hadoop.hive.metastore.model.MDatabase.jdoProvideFields(MDatabase.java)
at org.datanucleus.state.JDOStateManager.provideFields(JDOStateManager.java:1346)
at org.datanucleus.store.rdbms.request.InsertRequest.execute(InsertRequest.java:289)
at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertTable(RDBMSPersistenceHandler.java:167)
at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertObject(RDBMSPersistenceHandler.java:143)
at org.datanucleus.state.JDOStateManager.internalMakePersistent(JDOStateManager.java:3784)
at org.datanucleus.state.JDOStateManager.makePersistent(JDOStateManager.java:3760)
at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2219)
at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
at org.apache.hadoop.hive.metastore.ObjectStore.createDatabase(ObjectStore.java:569)
at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
at com.sun.proxy.$Proxy10.createDatabase(Unknown Source)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:923)
at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:962)
at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
at com.sun.proxy.$Proxy12.create_database(Unknown Source)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8863)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_database.getResult(ThriftHiveMetastore.java:8847)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:707)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:702)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693)
at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:702)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745) )
{code}

The ideal situation would be to just return the following message to Beeline users:

{code}
Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:javax.jdo.JDOFatalUserException: Attempt to store value ""test_longname_looooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooonglooooooongnametablefail2"" in column ""`NAME`"" that has maximum length of 128. Please correct your data!)
{code}

And have the full stack trace should up in the HiveServer2 logs."
HIVE-13998,CompactorMR for ORC ACID throws NPE,"When  job CompactorMR is running, we see NPE stack as follows,

2016-06-12 14:20:34,463 INFO [Thread-51] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1464322345445_83343_m_000003_1 to list of failed maps
2016-06-12 14:20:34,597 INFO [IPC Server handler 7 on 40704] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1464322345445_83343_m_000002_0 is : 0.0
2016-06-12 14:20:34,599 FATAL [IPC Server handler 12 on 40704] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1464322345445_83343_m_000002_0 - exited : java.lang.NullPointerException
	at java.util.Hashtable.put(Hashtable.java:514)
	at java.util.Hashtable.putAll(Hashtable.java:587)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$StringableMap.toProperties(CompactorMR.java:608)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.getWriter(CompactorMR.java:541)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:517)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:491)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

2016-06-12 14:20:34,599 INFO [IPC Server handler 12 on 40704] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1464322345445_83343_m_000002_0: Error: java.lang.NullPointerException
	at java.util.Hashtable.put(Hashtable.java:514)
	at java.util.Hashtable.putAll(Hashtable.java:587)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$StringableMap.toProperties(CompactorMR.java:608)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.getWriter(CompactorMR.java:541)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:517)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:491)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)

2016-06-12 14:20:34,599 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1464322345445_83343_m_000002_0: Error: java.lang.NullPointerException
	at java.util.Hashtable.put(Hashtable.java:514)
	at java.util.Hashtable.putAll(Hashtable.java:587)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$StringableMap.toProperties(CompactorMR.java:608)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.getWriter(CompactorMR.java:541)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:517)
	at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorMap.map(CompactorMR.java:491)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
"
HIVE-13955,Include service-rpc and llap-ext-client in packaging files,"Include info in packaging/pom.xml, packaging/src/main/assembly/src.xml, and packaging/src/main/assembly/bin.xml"
HIVE-13934,Configure Tez to make nocondiional task size memory available for the Processor,"Currently, noconditionaltasksize is not validated against the container size, the reservations made in the container by Tez for Inputs / Outputs etc.

Check this at compile time to see if enough memory is available, or set up the vertex to reserve additional memory for the Processor."
HIVE-13932,Hive SMB Map Join with small set of LIMIT failed with NPE,"1) prepare sample data:
a=1
while [[ $a -lt 100 ]]; do echo $a ; let a=$a+1; done > data

2) prepare source hive table:
CREATE TABLE `s`(`c` string);
load data local inpath 'data' into table s;

3) prepare the bucketed table:
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
CREATE TABLE `t`(`c` string) CLUSTERED BY (c) SORTED BY (c) INTO 5 BUCKETS;
insert into t select * from s;

4) reproduce this issue:
SET hive.auto.convert.sortmerge.join = true;
SET hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;
SET hive.auto.convert.sortmerge.join.noconditionaltask = true;
SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
select * from t join t t1 on t.c=t1.c limit 1;"
HIVE-13926,How to limit reduce memory only not Map and Reduce all in Tez engine,"Now there is only a property:hive.tez.container.size.
This will modify map and reduce`s memory all.
If i want turn up reduce`s memory only.How could i do?
------------------------------------
And i think if there is a possiblilty for dynamiclly allocate reduce`s memory 
depend on reduce`s input size!
detils:
assume a reduce is allocated 2GB before.And in this condition a reduce`s normal input size is 1GB ，and when reduce`s input is 2GB, if hive can turn up mem to 4GB auto?"
HIVE-13889,HiveServer2 shows stack trace when parsing invalid inputs,"HiveServer2 shows stack trace when parsing invalid syntax.

How to reproduce:
{code}
Input:
hostA$ hiveserver2
hostB$ beeline -u jdbc:hive2://localhost:10000 -n user -p pass -e ""invalid syntax;""

Output:
hostA$ NoViableAltException(26@[])                                                                                                                            [0/1248]
        at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1108)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:204)
        at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:444)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:319)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1199)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1186)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:146)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:218)
...
FAILED: ParseException line 1:0 cannot recognize input near 'invalid' 'syntax' '<EOF>'

hostB$ Error: Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'invalid' 'syntax' '<EOF>' (state=42000,code=40000)
{code}

This issue is related to the post of Hive developer mailing list:
http://mail-archives.apache.org/mod_mbox/hive-dev/201604.mbox/%3CCAOLfT9AaKZ8Nt77QnvrNcxWrQ_1htaj9C0UOsnN5HheoTzM6DQ%40mail.gmail.com%3E"
HIVE-13809,Hybrid Grace Hash Join memory usage estimation didn't take into account the bloom filter size,"Memory estimation is important during hash table loading, because we need to make the decision of whether to load the next hash partition in memory or spill it. If the assumption is there's enough memory but it turns out not the case, we will run into OOM problem.

Currently hybrid grace hash join memory usage estimation didn't take into account the bloom filter size. In large test cases (TB scale) the bloom filter grows as big as hundreds of MB, big enough to cause estimation error.

The solution is to count in the bloom filter size into memory estimation.

Another issue this patch will fix is possible NPE due to object cache reuse during hybrid grace hash join."
HIVE-13791,Fix  failure Unit Test TestHiveSessionImpl.testLeakOperationHandle,
HIVE-13786,Fix the unit test failure org.apache.hive.service.cli.session.TestHiveSessionImpl.testLeakOperationHandle,
HIVE-13777,NPE is thrown when select table after change column from string to decimal,"Run the followings to change a column from string to decimail:
{noformat}
drop table if exists shcemaevo_vectorization_true_disallowincompatible_false_fileformat_orc_string_decimal;
create table shcemaevo_vectorization_true_disallowincompatible_false_fileformat_orc_string_decimal stored as orc as select s, s_dc from schemaevo;
alter table shcemaevo_vectorization_true_disallowincompatible_false_fileformat_orc_string_decimal change column s_dc s_dc decimal(12,4);
select count(*) from shcemaevo_vectorization_true_disallowincompatible_false_fileformat_orc_string_decimal;
{noformat}

select count works fine but just select throws NPE:
{noformat}
0: jdbc:hive2://os-r6-0517-hiveserver2-1-1.op> select count(*) from shcemaevo_vectorization_true_disallowincompatible_false_fileformat_orc_string_decimal;
+---------+--+
|   c0    |
+---------+--+
| 100000  |
+---------+--+
1 row selected (13.856 seconds)
0: jdbc:hive2://os-r6-0517-hiveserver2-1-1.op> select * from shcemaevo_vectorization_true_disallowincompatible_false_fileformat_orc_string_decimal;
Error: java.io.IOException: java.lang.NullPointerException (state=,code=0)
{noformat}"
HIVE-13755,Hybrid mapjoin allocates memory the same for multi broadcast,"PROBLEM:

When hybrid mapjoin gets the memory needed, it estimates memory needed for each hashtable the same. This may cause problem when there are multiple broadcast, as it may exceeds the memory intended to allocate to it.

Example reducer task log attached.  This task has 5 broadcast input,

Reducer 3 <- Map 10 (BROADCAST_EDGE), Map 11 (BROADCAST_EDGE), Map 12 (BROADCAST_EDGE), Map 8 (SIMPLE_EDGE), Map 9 (BROADCAST_EDGE), Reducer 2 (SIMPLE_EDGE)



excerpt of it:

{code}
2016-03-15 19:23:50,811 [INFO] [pool-47-thread-1] |tez.HashTableLoader|: Memory manager allocates 0 bytes for the loading hashtable.
2016-03-15 19:23:50,811 [INFO] [pool-47-thread-1] |persistence.HashMapWrapper|: Key count from statistics is 210; setting map size to 280
2016-03-15 19:23:50,811 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Total available memory: 1968177152
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Estimated small table size: 155190
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of hash partitions to be created: 16
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Write buffer size: 524288
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of partitions created: 16
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of partitions spilled directly to disk on creation: 0
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |tez.HashTableLoader|: Using tableContainer HybridHashTableContainer
2016-03-15 19:23:50,812 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Initializing container with org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe and org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
2016-03-15 19:23:50,814 [INFO] [pool-47-thread-1] |readers.UnorderedKVReader|: Num Records read: 20
2016-03-15 19:23:50,814 [INFO] [pool-47-thread-1] |log.PerfLogger|: </PERFLOG method=LoadHashtable start=1458069830811 end=1458069830814 duration=3 from=org.apache.hadoop.hive.ql.exec.MapJoinOperator>
2016-03-15 19:23:50,814 [INFO] [pool-47-thread-1] |tez.ObjectCache|: Caching key: svc-phx-efmhadoop_20160315191303_8c53ce88-e64f-4d36-bad0-846bbf096f57__HASH_MAP_MAPJOIN_126_container
2016-03-15 19:23:50,814 [INFO] [TezChild] |exec.HashTableDummyOperator|: Initializing operator HASHTABLEDUMMY[32]
2016-03-15 19:23:50,814 [INFO] [TezChild] |exec.MapJoinOperator|: Initializing operator MAPJOIN[26]
2016-03-15 19:23:50,816 [INFO] [TezChild] |exec.CommonJoinOperator|: JOIN struct<_col3:string,_col4:decimal(5,0),_col5:char(1),_col6:char(1),_col7:date,_col8:string,_col9:string,_col12:string,_col13:string,_col14:string,_col15:string,_col16:string,_col19:decimal(13,3),_col20:string,_col22:decimal(5,0),_col23:decimal(5,0),_col24:decimal(5,0),_col25:decimal(5,0),_col26:decimal(13,2),_col27:decimal(5,0),_col28:decimal(15,2),_col29:decimal(15,2),_col31:decimal(3,0),_col33:char(1),_col41:decimal(3,1),_col42:char(1),_col43:decimal(3,1),_col44:string,_col45:char(1),_col48:char(1),_col55:char(1),_col57:char(1),_col59:char(1),_col60:string,_col64:string,_col65:string,_col67:decimal(15,2),_col76:decimal(3,0),_col81:char(1),_col98:string,_col99:string,_col105:string,_col108:string,_col122:string,_col123:decimal(5,0),_col127:string,_col128:decimal(5,0),_col129:string,_col137:char(1),_col139:string,_col145:string,_col151:string,_col152:string,_col154:string,_col158:char(1),_col164:char(1),_col204:string,_col213:string,_col214:char(1),_col215:string,_col218:char(1),_col219:date,_col220:string,_col221:decimal(5,0),_col222:decimal(5,0),_col223:string,_col224:char(1),_col225:string,_col226:decimal(3,0),_col231:string,_col232:string,_col233:string,_col234:decimal(9,5),_col236:date,_col240:date,_col256:string,_col257:string,_col268:string,_col269:string,_col270:char(1),_col271:string,_col272:char(1),_col324:string,_col344:string,_col464:string,_col478:decimal(5,0),_col479:decimal(5,0),_col519:string,_col532:string,_col534:char(1),_col540:decimal(13,3),_col541:decimal(13,3),_col561:string,_col568:char(1),_col570:string> totalsz = 95
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |log.PerfLogger|: <PERFLOG method=LoadHashtable from=org.apache.hadoop.hive.ql.exec.MapJoinOperator>
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |tez.HashTableLoader|: Memory manager allocates 0 bytes for the loading hashtable.
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |persistence.HashMapWrapper|: Key count from statistics is 5942112; setting map size to 7922816
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Total available memory: 1968177152
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Estimated small table size: 1324101915
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of hash partitions to be created: 16
2016-03-15 19:23:50,817 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Write buffer size: 8388608
2016-03-15 19:23:50,831 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of partitions created: 16
2016-03-15 19:23:50,831 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of partitions spilled directly to disk on creation: 0
2016-03-15 19:23:50,831 [INFO] [pool-47-thread-1] |tez.HashTableLoader|: Using tableContainer HybridHashTableContainer
2016-03-15 19:23:50,831 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Initializing container with org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe and org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
2016-03-15 19:23:51,543 [INFO] [pool-47-thread-1] |readers.UnorderedKVReader|: Num Records read: 852596
2016-03-15 19:23:51,563 [INFO] [pool-47-thread-1] |log.PerfLogger|: </PERFLOG method=LoadHashtable start=1458069830817 end=1458069831563 duration=746 from=org.apache.hadoop.hive.ql.exec.MapJoinOperator>
2016-03-15 19:23:51,563 [INFO] [pool-47-thread-1] |tez.ObjectCache|: Caching key: svc-phx-efmhadoop_20160315191303_8c53ce88-e64f-4d36-bad0-846bbf096f57__HASH_MAP_MAPJOIN_127_container
2016-03-15 19:23:51,563 [INFO] [TezChild] |exec.HashTableDummyOperator|: Initializing operator HASHTABLEDUMMY[31]
2016-03-15 19:23:51,564 [INFO] [TezChild] |exec.MapJoinOperator|: Initializing operator MAPJOIN[27]
2016-03-15 19:23:51,566 [INFO] [TezChild] |exec.CommonJoinOperator|: JOIN struct<_col3:string,_col4:decimal(5,0),_col5:char(1),_col6:char(1),_col7:date,_col8:string,_col9:string,_col12:string,_col13:string,_col14:string,_col15:string,_col16:string,_col19:decimal(13,3),_col20:string,_col22:decimal(5,0),_col23:decimal(5,0),_col24:decimal(5,0),_col25:decimal(5,0),_col26:decimal(13,2),_col27:decimal(5,0),_col28:decimal(15,2),_col29:decimal(15,2),_col31:decimal(3,0),_col33:char(1),_col41:decimal(3,1),_col42:char(1),_col43:decimal(3,1),_col44:string,_col45:char(1),_col48:char(1),_col55:char(1),_col57:char(1),_col59:char(1),_col60:string,_col64:string,_col65:string,_col67:decimal(15,2),_col76:decimal(3,0),_col81:char(1),_col98:string,_col99:string,_col105:string,_col108:string,_col122:string,_col123:decimal(5,0),_col127:string,_col128:decimal(5,0),_col129:string,_col137:char(1),_col139:string,_col145:string,_col151:string,_col152:string,_col154:string,_col158:char(1),_col164:char(1),_col204:string,_col213:string,_col214:char(1),_col215:string,_col218:char(1),_col219:date,_col220:string,_col221:decimal(5,0),_col222:decimal(5,0),_col223:string,_col224:char(1),_col225:string,_col226:decimal(3,0),_col231:string,_col232:string,_col233:string,_col234:decimal(9,5),_col236:date,_col240:date,_col256:string,_col257:string,_col268:string,_col269:string,_col270:char(1),_col271:string,_col272:char(1),_col324:string,_col344:string,_col464:string,_col478:decimal(5,0),_col479:decimal(5,0),_col519:string,_col532:string,_col534:char(1),_col540:decimal(13,3),_col541:decimal(13,3),_col561:string> totalsz = 93
2016-03-15 19:23:51,566 [INFO] [pool-47-thread-1] |log.PerfLogger|: <PERFLOG method=LoadHashtable from=org.apache.hadoop.hive.ql.exec.MapJoinOperator>
2016-03-15 19:23:51,567 [INFO] [pool-47-thread-1] |tez.HashTableLoader|: Memory manager allocates 0 bytes for the loading hashtable.
2016-03-15 19:23:51,567 [INFO] [pool-47-thread-1] |persistence.HashMapWrapper|: Key count from statistics is 293380; setting map size to 391174
2016-03-15 19:23:51,567 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Total available memory: 1968177152
2016-03-15 19:23:51,567 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Estimated small table size: 69929471
2016-03-15 19:23:51,567 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of hash partitions to be created: 16
2016-03-15 19:23:51,567 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Write buffer size: 4194304
2016-03-15 19:23:51,568 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of partitions created: 16
2016-03-15 19:23:51,568 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Number of partitions spilled directly to disk on creation: 0
2016-03-15 19:23:51,568 [INFO] [pool-47-thread-1] |tez.HashTableLoader|: Using tableContainer HybridHashTableContainer
2016-03-15 19:23:51,569 [INFO] [pool-47-thread-1] |persistence.HybridHashTableContainer|: Initializing container with org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe and org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
2016-03-15 19:23:51,980 [INFO] [pool-47-thread-1] |readers.UnorderedKVReader|: Num Records read: 586760


{code}"
HIVE-13754,Fix resource leak in HiveClientCache,"Found that the {{users}} reference count can go into negative values, which prevents {{tearDownIfUnused}} from closing the client connection when called.
This leads to a build up of clients which have been evicted from the cache, are no longer in use, but have not been shutdown.
GC will eventually call {{finalize}}, which forcibly closes the connection and cleans up the client, but I have seen as many as several hundred open client connections as a result.

The main resource for this is caused by RetryingMetaStoreClient, which will call {{reconnect}} on acquire, which calls {{close}}. This will decrement {{users}} to -1 on the reconnect, then acquire will increase this to 0 while using it, and back to -1 when it releases it."
HIVE-13749,Memory leak in Hive Metastore,"Looking a heap dump of 10GB, a large number of Configuration objects(> 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.

I will attach an exported snapshot from the eclipse MAT."
HIVE-13745,UDF current_date、current_timestamp、unix_timestamp NPE,NullPointerException when current_date is used in mapreduce
HIVE-13730,Avoid double spilling the same partition when memory threshold is set very low,I am seeing hybridgrace_hashjoin_1.q getting stuck on master.
HIVE-13665,HS2 memory leak When multiple queries are running with get_json_object,"The extractObjectCache in UDFJson is increased over limitation(CACHE_SIZE = 16). When multiple queries are running concurrently on HS2 local(not mr/tez) with get_json_object or get_json_tuple
{code:java|title=HS2 heap_dump}
Object at 0x515ab18f8
instance of org.apache.hadoop.hive.ql.udf.UDFJson$HashCache@0x515ab18f8 (77 bytes)
Class:
class org.apache.hadoop.hive.ql.udf.UDFJson$HashCache
Instance data members:
accessOrder (Z) : false
entrySet (L) : <null>
hashSeed (I) : 0
header (L) : java.util.LinkedHashMap$Entry@0x515a577d0 (60 bytes) 
keySet (L) : <null>
loadFactor (F) : 0.6
modCount (I) : 4741146
size (I) : 2733158                   <========== here!!
table (L) : [Ljava.util.HashMap$Entry;@0x7163d8b70 (67108880 bytes) 
threshold (I) : 5033165
values (L) : <null>
References to this object:
{code}

I think that this problem be caused by the LinkedHashMap object is not thread-safe
{code}
* <p><strong>Note that this implementation is not synchronized.</strong>
 * If multiple threads access a linked hash map concurrently, and at least
 * one of the threads modifies the map structurally, it <em>must</em> be
 * synchronized externally.  This is typically accomplished by
 * synchronizing on some object that naturally encapsulates the map.
{code}

Reproduce :
# Multiple queries are running with get_json_object and small input data(for execution on hs2 local mode)
# jvm heap dump & analyze
{code:title=test scenario}
Multiple queries are running with get_json_object and small input data(for execute on hs2 local mode)
1.hql :
SELECT get_json_object(body, '$.fileSize'), get_json_object(body, '$.ps_totalTimeSeconds'), get_json_object(body, '$.totalTimeSeconds') FROM xxx.tttt WHERE part_hour='2016040105' 
2.hql :
SELECT get_json_object(body, '$.fileSize'), get_json_object(body, '$.ps_totalTimeSeconds'), get_json_object(body, '$.totalTimeSeconds') FROM xxx.tttt WHERE part_hour='2016040106'
3.hql :
SELECT get_json_object(body, '$.fileSize'), get_json_object(body, '$.ps_totalTimeSeconds'), get_json_object(body, '$.totalTimeSeconds') FROM xxx.tttt WHERE part_hour='2016040107'
4.hql :
SELECT get_json_object(body, '$.fileSize'), get_json_object(body, '$.ps_totalTimeSeconds'), get_json_object(body, '$.totalTimeSeconds') FROM xxx.tttt WHERE part_hour='2016040108'
 
run.sh :
t_cnt=0
while true
do
echo ""query executing...""
    for i in 1 2 3 4
    do
        beeline -u jdbc:hive2://localhost:10000 -n hive --silent=true -f $i.hql > $i.log 2>&1 &
    done
wait
t_cnt=`expr $t_cnt + 1`
echo ""query count : $t_cnt""
sleep 2
done

jvm heap dump & analyze :
jmap -dump:format=b,file=hive.dmp $PID
jhat -J-mx48000m -port 8080 hive.dmp &
{code}

Finally I have attached our patch."
HIVE-13627,"When running under LLAP, for regular map joins, throw an error if memory utilization goes above what is allocated to the task","When running under LLAP, for regular map joins, throw an error if memory utilization goes above what is allocated to the task. This way, the rest of the dependent tasks can fail sooner."
HIVE-13621,compute stats in certain cases fails with NPE,"{code}
FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:693)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:739)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:728)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:183)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:136)
	at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:124){code}"
HIVE-13561,HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used,"I can repo this on branch-1.2 and branch-2.0.

It looks to be the same issues as: HIVE-11408

The patch from HIVE-11408 looks to fix the issue as well.

I've updated the patch from HIVE-11408 to be aligned with branch-1.2 and master

"
HIVE-13527,Using deprecated APIs in HBase client causes zookeeper connection leaks.,"When running queries against hbase-backed hive tables, the following log messages are seen in the HS2 log.
{code}
2016-04-11 07:25:23,657 WARN org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: You are using an HTable instance that relies on an HBase-managed Connection. This is usually due to directly creating an HTable, which is deprecated. Instead, you should create a Connection object and then request a Table instance from it. If you don't need the Table instance for your own use, you should instead use the TableInputFormatBase.initalizeTable method directly.
2016-04-11 07:25:23,658 INFO org.apache.hadoop.hbase.mapreduce.TableInputFormatBase: Creating an additional unmanaged connection because user provided one can't be used for administrative actions. We'll close it when we close out the table.
{code}

In a HS2 log file, there are 1366 zookeeper connections established but only a small fraction of them were closed. So lsof would show 1300+ open TCP connections to Zookeeper.
grep ""org.apache.zookeeper.ClientCnxn: Session establishment complete on server"" * |wc -l
1366
grep ""INFO org.apache.zookeeper.ZooKeeper: Session:"" * |grep closed |wc -l
54

According to the comments in TableInputFormatBase, the recommended means for subclasses like HiveHBaseTableInputFormat is to call initializeTable() instead of setHTable() that it currently uses.
""
Subclasses MUST ensure initializeTable(Connection, TableName) is called for an instance to function properly. Each of the entry points to this class used by the MapReduce framework, {@link #createRecordReader(InputSplit, TaskAttemptContext)} and {@link #getSplits(JobContext)}, will call {@link #initialize(JobContext)} as a convenient centralized location to handle retrieving the necessary configuration information. If your subclass overrides either of these methods, either call the parent version or call initialize yourself.
""

Currently setHTable() also creates an additional Admin connection, even though it is not needed.

So the use of deprecated APIs are to be replaced."
HIVE-13523,Fix connection leak in ORC RecordReader and refactor for unit testing,"In RecordReaderImpl, a MetadataReaderImpl object was being created (opening a file), but never closed, causing a leak. This change closes the Metadata object in RecordReaderImpl, and does substantial refactoring to make RecordReaderImpl testable:
 * Created DataReaderFactory and MetadataReaderFactory (plus default implementations) so that the create() methods can be mocked to verify that the objects are actually closed in RecordReaderImpl.close()
 * Created MetadataReaderProperties and DataReaderProperties to clean up argument lists, making code more readable
 * Created a builder() for RecordReaderImpl to make the code more readable
 * DataReader and MetadataReader now extend closeable (there was no reason for them not to in the first place) so I can use the guava Closer interface: http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/io/Closer.html
 * Use the Closer interface to guarantee that regardless of if either close() call fails, both will be attempted (preventing further potential leaks)
 * Create builders for MetadataReaderProperties, DataReaderProperties, and RecordReaderImpl to help with code readability"
HIVE-13423,Handle the overflow case for decimal datatype for sum(),"When a column col1 defined as decimal and if the sum of the column overflows, we will try to increase the decimal precision by 10. But if it's reaching 38 (the max precision), the overflow still could happen. Right now, if such case happens, the following exception will throw since hive is writing incorrect data.

Follow the following steps to repro. 
{noformat}
CREATE TABLE DECIMAL_PRECISION(dec decimal(38,18));
INSERT INTO DECIMAL_PRECISION VALUES(98765432109876543210.12345), (98765432109876543210.12345);
SELECT SUM(dec) FROM DECIMAL_PRECISION;
{noformat}

{noformat}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.readVInt(LazyBinaryUtils.java:314) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.checkObjectByteInfo(LazyBinaryUtils.java:219) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
        at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.parse(LazyBinaryStruct.java:142) ~[hive-exec-2.2.0-SNAPSHOT.jar:2.2.0-SNAPSHOT]
{noformat}"
HIVE-13405,Fix Connection Leak in OrcRawRecordMerger,"In OrcRawRecordMerger.getLastFlushLength, if the opened stream throws an IOException on .available() or on .readLong(), the function will exit without closing the stream.

This patch adds a try-with-resources to fix this."
HIVE-13343,Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash join,"Due to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews."
HIVE-13238,"""java.lang.OutOfMemoryError: Java heap space"" occurs at Hive on Tez","hive query
{code}
select
  aaa,
  sum(bbb)
from (
  select
    ...
  from access_log
  where yyyymmdd='20160224' AND ...
)x
group by aaa
{code}

stacktrace
{code}
java.lang.OutOfMemoryError: Java heap space
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:157)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
        at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:344)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
        at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
        at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
        at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.<init>(PipelinedSorter.java:172)
        at org.apache.tez.runtime.library.common.sort.impl.PipelinedSorter.<init>(PipelinedSorter.java:116)
        at org.apache.tez.runtime.library.output.OrderedPartitionedKVOutput.start(OrderedPartitionedKVOutput.java:142)
        at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:142)
        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)
        ... 14 more
{code}"
HIVE-13163,"ORC MemoryManager thread checks are fatal, should WARN ","The MemoryManager is tied to a WriterOptions on create, which can occur in a different thread from the writer calls.

This is unexpected, but safe and needs a warning not a fatal.

{code}
  /**
   * Light weight thread-safety check for multi-threaded access patterns
   */
  private void checkOwner() {
    Preconditions.checkArgument(ownerLock.isHeldByCurrentThread(),
        ""Owner thread expected %s, got %s"",
        ownerLock.getOwner(),
        Thread.currentThread());
  }
{code}

"
HIVE-13144,HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node,"When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL."
HIVE-13129,CliService leaks HMS connection,"HIVE-12790 fixes the HMS connection leaking. But seems there is one more connection from CLIService.

The init() function in CLIService will get info from DB but we never close the HMS connection for this service main thread.  

{noformat}
    // creates connection to HMS and thus *must* occur after kerberos login above
    try {
      applyAuthorizationConfigPolicy(hiveConf);
    } catch (Exception e) {
      throw new RuntimeException(""Error applying authorization policy on hive configuration: ""
          + e.getMessage(), e);
{noformat}"
HIVE-13109,NPE: Fix NDV estimator use within the StatsOptimizer,"compute_stats() UDF's size estimates only work within the Execution engine, not during optimizer pass.

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFNumericStatsEvaluator$NumericStatsAgg.estimate(GenericUDAFComputeStats.java:434)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFDoubleStatsEvaluator$DoubleStatsAgg.estimate(GenericUDAFComputeStats.java:643)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$GroupByStatsRule.checkMapSideAggregation(StatsRulesProcFactory.java:996)
{code}"
HIVE-13090,Hive metastore crashes on NPE with ZooKeeperTokenStore,"Observed that hive metastore shutdown with NPE from ZookeeperTokenStore.

{code}
INFO  [pool-5-thread-192]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(714)) - 191: Metastore shutdown complete.
 INFO  [pool-5-thread-192]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(340)) - ugi=cvdpqap	ip=/19.1.2.129	cmd=Metastore shutdown complete.	
 ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token
org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token
	at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)
	at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)
	at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)
	... 3 more
 INFO  [Thread-3]: metastore.HiveMetaStore (HiveMetaStore.java:run(5639)) - Shutting down hive metastore.
{code}"
HIVE-13065,Hive throws NPE when writing map type data to a HBase backed table,"Hive throws NPE when writing data to a HBase backed table with below conditions:

# There is a map type column
# The map type column has NULL in its values

Below are the reproduce steps:

*1) Create a HBase backed Hive table*
{code:sql}
create table hbase_test (id bigint, data map<string, string>)
stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
with serdeproperties (""hbase.columns.mapping"" = "":key,cf:map_col"")
tblproperties (""hbase.table.name"" = ""hive_test"");
{code}

*2) insert data into above table*
{code:sql}
insert overwrite table hbase_test select 1 as id, map('abcd', null) as data from src limit 1;
{code}

The mapreduce job for insert query fails. Error messages are as below:
{noformat}
2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":1,""_col1"":{""abcd"":null}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":1,""_col1"":{""abcd"":null}}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)
	... 7 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)
	... 7 more
Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)
	... 14 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)
	at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)
	at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:282)
	... 15 more
{noformat}"
HIVE-13012,NPE from simple nested ANSI Join,"Using hive 1.2.1.2.3  Connecting using JDBC, issuing the following query : 

SELECT COUNT(*) 
FROM nation n 
    INNER JOIN (customer c
                         INNER JOIN orders o ON c.c_custkey = o.o_custkey)
     ON n.n_nationkey = c.c_nationkey;

Generates the NPE and stack below. Fields are integer data type.
NOTE: Similar stack as https://issues.apache.org/jira/browse/HIVE-11433 

Stack
--------
Caused by: java.lang.NullPointerExcEeption: Remote java.lang.NullPointerException: null
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPresent(SemanticAnalyzer.java:2046)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondPopulateAlias(SemanticAnalyzer.java:2109)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondPopulateAlias(SemanticAnalyzer.java:2185)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(SemanticAnalyzer.java:2445)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(SemanticAnalyzer.java:2386)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinTree(SemanticAnalyzer.java:8192)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinTree(SemanticAnalyzer.java:8131)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9709)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9636)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10109)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:329)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10120)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:454)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:314)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1164)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1158)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:110)

"
HIVE-13008,WebHcat DDL commands in secure mode NPE when default FileSystem doesn't support delegation tokens,"{noformat}
ERROR | 11 Jan 2016 20:19:02,781 | org.apache.hive.hcatalog.templeton.CatchallExceptionMapper |
java.lang.NullPointerException
        at org.apache.hive.hcatalog.templeton.SecureProxySupport$2.run(SecureProxySupport.java:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.hcatalog.templeton.SecureProxySupport.writeProxyDelegationTokens(SecureProxySupport.java:168)
        at org.apache.hive.hcatalog.templeton.SecureProxySupport.open(SecureProxySupport.java:95)
        at org.apache.hive.hcatalog.templeton.HcatDelegator.run(HcatDelegator.java:63)
        at org.apache.hive.hcatalog.templeton.Server.ddl(Server.java:217)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)
        at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)
        at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)
        at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:302)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:108)
        at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:147)
        at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:84)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1480)
        at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1411)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1360)
        at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1350)
        at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:538)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:716)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1360)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:615)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:574)
        at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:88)
        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1331)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:477)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47)
{noformat}"
HIVE-12990,LLAP: ORC cache NPE without FileID support,"{code}
   OrcBatchKey stripeKey = hasFileId ? new OrcBatchKey(fileId, -1, 0) : null;
   ...
          if (hasFileId && metadataCache != null) {
            stripeKey.stripeIx = stripeIx;
            stripeMetadata = metadataCache.getStripeMetadata(stripeKey);
          }
...
  public void setStripeMetadata(OrcStripeMetadata m) {
    assert stripes != null;
    stripes[m.getStripeIx()] = m;
  }
{code}

{code}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.getStripeIx(OrcStripeMetadata.java:106)
        at org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.setStripeMetadata(OrcEncodedDataConsumer.java:70)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.readStripesMetadata(OrcEncodedDataReader.java:685)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:283)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:215)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:212)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:212)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:93)
        ... 5 more
{code}"
HIVE-12954,NPE with str_to_map on null strings,"Running str_to_map on a null string will return a NullPointerException.
Workaround is to use coalesce."
HIVE-12904,LLAP: deadlock in task scheduling,"{noformat}
Thread 34107: (state = BLOCKED)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.isInWaitQueue() @bci=0, line=690 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.finishableStateUpdated(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=8, line=485 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.access$1500(org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService, org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper, boolean) @bci=3, line=78 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.finishableStateUpdated(boolean) @bci=27, line=733 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.sourceStateUpdated(java.lang.String) @bci=76, line=210 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.sourceStateUpdated(java.lang.String) @bci=5, line=164 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.registerSourceStateChange(java.lang.String, java.lang.String, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateProto) @bci=34, line=228 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=47, line=255 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.sourceStateUpdated(org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=328 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.sourceStateUpdated(com.google.protobuf.RpcController, org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$SourceStateUpdatedRequestProto) @bci=5, line=105 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(com.google.protobuf.Descriptors$MethodDescriptor, com.google.protobuf.RpcController, com.google.protobuf.Message) @bci=80, line=13067 (Compiled frame)
 - org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(org.apache.hadoop.ipc.RPC$Server, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=246, line=616 (Compiled frame)
 - org.apache.hadoop.ipc.RPC$Server.call(org.apache.hadoop.ipc.RPC$RpcKind, java.lang.String, org.apache.hadoop.io.Writable, long) @bci=9, line=969 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=38, line=2151 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler$1.run() @bci=1, line=2147 (Compiled frame)
 - java.security.AccessController.doPrivileged(java.security.PrivilegedExceptionAction, java.security.AccessControlContext) @bci=0 (Compiled frame)
 - javax.security.auth.Subject.doAs(javax.security.auth.Subject, java.security.PrivilegedExceptionAction) @bci=42, line=422 (Compiled frame)
 - org.apache.hadoop.security.UserGroupInformation.doAs(java.security.PrivilegedExceptionAction) @bci=14, line=1657 (Compiled frame)
 - org.apache.hadoop.ipc.Server$Handler.run() @bci=315, line=2145 (Interpreted frame)


and 


Thread 34500: (state = BLOCKED)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo$FinishableStateTracker.unregisterForUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=0, line=195 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.unregisterFinishableStateUpdate(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=160 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.unregisterForFinishableStateUpdates(org.apache.hadoop.hive.llap.daemon.FinishableStateUpdateHandler) @bci=5, line=143 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$TaskWrapper.maybeUnregisterForFinishedStateNotifications() @bci=20, line=681 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(org.apache.tez.runtime.task.TaskRunner2Result) @bci=32, line=548 (Compiled frame)
 - org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService$InternalCompletionListener.onSuccess(java.lang.Object) @bci=5, line=535 (Compiled frame)
 - com.google.common.util.concurrent.Futures$4.run() @bci=55, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)

""IPC Server handler 0 on 15001"":
  waiting to lock Monitor@0x00007f5d322ecb08 (Object@0x00007f67032cd2c0, a org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService$TaskWrapper),
  which is held by ""ExecutionCompletionThread #0""
""ExecutionCompletionThread #0"":
  waiting to lock Monitor@0x00007f6066b9e8c8 (Object@0x00007f66b6570200, a org/apache/hadoop/hive/llap/daemon/impl/QueryInfo$FinishableStateTracker),
  which is held by ""IPC Server handler 0 on 15001""

Found a total of 1 deadlock.

{noformat}

Looks like it's caused by synchronized blocks:
{noformat}
TaskWrapper:
public synchronized void maybeUnregisterForFinishedStateNotifications
{noformat}
Eventually calls 
{noformat}
FinishableStateTracker
synchronized void unregisterForUpdates(FinishableStateUpdateHandler handler) {
{noformat}

and 
{noformat}
FST
 synchronized void sourceStateUpdated(String sourceName) {
   {noformat}
eventually calls
{noformat}
 public synchronized boolean isInWaitQueue() {
{noformat}

The latter returns the boolean, so it definitely doesn't need synchronized, however I don't know if there are other similar issues and what is necessary inside sync blocks, perhaps there's a better fix.

Overall I'd say synch methods on objects that call any other non-trivial objects should not be used. Perhaps for now it would be good to replace all sync methods by sync blocks that cover entire method, as well as remove the unnecessary ones like the isWait... one. Then the scope of the blocks can be adjusted based on logic in future.
"
HIVE-12864,StackOverflowError parsing queries with very large predicates,"We have seen that queries with very large predicates might fail with the following stacktrace:

{noformat}
016-01-12 05:47:36,516|beaver.machine|INFO|552|5072|Thread-22|Exception in thread ""main"" java.lang.StackOverflowError

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:145)

2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,517|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,519|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,520|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,522|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,523|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,525|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)

2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,526|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:36,634|beaver.machine|INFO|552|5072|Thread-22|at org.antlr.runtime.tree.CommonTree.setUnknownTokenBoundaries(CommonTree.java:146)
2016-01-12 05:47:37,582|main|INFO|552|4568|MainThread|TEST ""test_WideQuery"" FAILED in 10.95 seconds
{noformat}

The problem could be solved by reimplementing some of the parsing methods so they are iterative instead of recursive."
HIVE-12847,ORC file footer cache should be memory sensitive,"The size based footer cache can not control memory usage properly.
Having seen a HiveServer2 hang (full GC all the time) due to ORC file footer cache taking up too much heap memory.
A simple query like ""select * from orc_table limit 1"" can make HiveServer2 hang.
The input table has about 1000 ORC files and each ORC file owns about 2500 stripes.
{noformat}
 num     #instances         #bytes  class name
----------------------------------------------
   1:     214653601    25758432120  org.apache.hadoop.hive.ql.io.orc.OrcProto$ColumnStatistics
   3:     122233301     8800797672  org.apache.hadoop.hive.ql.io.orc.OrcProto$StringStatistics
   5:      89439001     6439608072  org.apache.hadoop.hive.ql.io.orc.OrcProto$IntegerStatistics
   7:       2981300      262354400  org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeInformation
   9:       2981300      143102400  org.apache.hadoop.hive.ql.io.orc.OrcProto$StripeStatistics
  12:       2983691       71608584  org.apache.hadoop.hive.ql.io.orc.ReaderImpl$StripeInformationImpl
  15:         80929        7121752  org.apache.hadoop.hive.ql.io.orc.OrcProto$Type
  17:        103282        5783792  org.apache.hadoop.mapreduce.lib.input.FileSplit
  20:         51641        3305024  org.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit
  21:         51641        3305024  org.apache.hadoop.hive.ql.io.orc.OrcSplit
  31:             1         413152  [Lorg.apache.hadoop.hive.ql.exec.FetchOperator$FetchInputFormatSplit;  
 100:          1122          26928  org.apache.hadoop.hive.ql.io.orc.Metadata  
{noformat}"
HIVE-12845,"When you try to view file from HWI , you get error with ""HTTP ERROR 500"" and stacktrace for JSP file. It says that ""Unable to compile class for JSP file""","Here are the details for stacktrace :-

Problem accessing /hwi/view_file.jsp. Reason:

    Unable to compile class for JSP: 

An error occurred at line: 61 in the jsp file: /view_file.jsp
The type java.nio.CharBuffer cannot be resolved. It is indirectly referenced from required .class files
58: 			  char [] c = new char [bsize] ;
59: 			  int cread=-1;
60: 			  
61: 			  if( ( cread=br.read(c)) != -1 ){
62: 			   out.println( c ); 
63: 			  }
64: 			  br.close();	  


Stacktrace:
Caused by:

org.apache.jasper.JasperException: Unable to compile class for JSP: 

An error occurred at line: 61 in the jsp file: /view_file.jsp
The type java.nio.CharBuffer cannot be resolved. It is indirectly referenced from required .class files
58: 			  char [] c = new char [bsize] ;
59: 			  int cread=-1;
60: 			  
61: 			  if( ( cread=br.read(c)) != -1 ){
62: 			   out.println( c ); 
63: 			  }
64: 			  br.close();	  


Stacktrace:
	at org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:85)
	at org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:330)
	at org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:435)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:298)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:277)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:265)
	at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:564)
	at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:299)
	at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:315)
	at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:265)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:767)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582

I have used below configuration :-

<property>
    <name>hive.hwi.listen.host</name>
    <value>localhost</value>
    <description>This is the host address the Hive Web Interface will listen on</description>
  </property>
  <property>
    <name>hive.hwi.listen.port</name>
    <value>9999</value>
    <description>This is the port the Hive Web Interface will listen on</description>
  </property>
  <property>
    <name>hive.hwi.war.file</name>
    <value>/lib/hive-hwi-0.11.0.war</value>
    <description>This sets the path to the HWI war file, relative to ${HIVE_HOME}. </description>
  </property>

Note :- I have used hwi-0.11.0.war , as 'war' file for HWI is not available in tar file(1.2.1 version) , also i can't complie code in local to generate it."
HIVE-12837,Better memory estimation/allocation for hybrid grace hash join during hash table loading,"This is to avoid an edge case when the memory available is very little (less than a single write buffer size), and we start loading the hash table. Since the write buffer is lazily allocated, we will easily run out of memory before even checking if we should spill any hash partition.

e.g.
Total memory available: 210 MB
Size of ref array of BytesBytesMultiHashMap for each hash partition: ~16 MB
Size of write buffer: 8 MB (lazy allocation)
Number of hash partitions: 16
Number of hash partitions created in memory: 13
Number of hash partitions created on disk: 3
Available memory left after HybridHashTableContainer initialization: 210-16*13=2MB

Now let's say a row is to be loaded into a hash partition in memory, it will try to allocate an 8MB write buffer for it, but we only have 2MB, thus OOM.

Solution is to perform the check for possible spilling earlier so we can spill partitions if memory is about to be full, to avoid OOM."
HIVE-12815,column stats NPE for a query w/o a table,"I was running something like create table as select 1;

First it logs why it cannot get stats:
{noformat}
2016-01-08T21:46:31,876 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: stats.StatsUtils (StatsUtils.java:getTableColumnStats(756)) - Failed to retrieve table statistics:
org.apache.hadoop.hive.ql.metadata.HiveException: NoSuchObjectException(message:Specified database/table does not exist : _dummy_database._dummy_table)
        at org.apache.hadoop.hive.ql.metadata.Hive.getTableColumnStatistics(Hive.java:3195) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:752) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:198) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
{noformat}

and returns null, then it fails with NPE:
{noformat}
2016-01-08T21:46:31,885 ERROR [0883a32c-c789-4695-aec2-ed73bb1cc9ce 0883a32c-c789-4695-aec2-ed73bb1cc9ce main]: ql.Driver (SessionState.java:printError(1010)) - FAILED: NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getDataSizeFromColumnStats(StatsUtils.java:1450)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:199)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:144)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:132)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
{noformat}

Only ""NullPointerException null"" is logged to CLI... :("
HIVE-12790,Metastore connection leaks in HiveServer2,"HiveServer2 keeps opening new connections to HMS each time it launches a task. These connections do not appear to be closed when the task completes thus causing a HMS connection leak. ""lsof"" for the HS2 process shows connections to port 9083.

{code}
2015-12-03 04:20:56,352 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 11 out of 41
2015-12-03 04:20:56,354 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083
2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14824
2015-12-03 04:20:56,360 INFO  [Thread-405728()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
....
2015-12-03 04:21:06,355 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 12 out of 41
2015-12-03 04:21:06,357 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083
2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14825
2015-12-03 04:21:06,362 INFO  [Thread-405756()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
...
2015-12-03 04:21:08,357 INFO  [HiveServer2-Background-Pool: Thread-424756()]: ql.Driver (SessionState.java:printInfo(558)) - Launching Job 13 out of 41
2015-12-03 04:21:08,360 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(311)) - Trying to connect to metastore with URI thrift://<anonymizedURL>:9083
2015-12-03 04:21:08,364 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(351)) - Opened a connection to metastore, current connections: 14826
2015-12-03 04:21:08,365 INFO  [Thread-405782()]: hive.metastore (HiveMetaStoreClient.java:open(400)) - Connected to metastore.
... 
{code}

The TaskRunner thread starts a new SessionState each time, which creates a new connection to the HMS (via Hive.get(conf).getMSC()) that is never closed.

Even SessionState.close(), currently not being called by the TaskRunner thread, does not close this connection.

Attaching a anonymized log snippet where the number of HMS connections reaches north of 25000+ connections."
HIVE-12787,Trace improvement - Inconsistent logging upon shutdown-start of the Hive metastore process,"The log at: https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L793 logged at the start of the shutdown of the Hive metastore process can be improved to match the finish of the shutdown log at: https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java#L793
by rephrasing from: ""Shutting down the object store..."" to: ""Metastore shutdown started..."". This will match the shutdown completion log: ""Metastore shutdown complete.""."
HIVE-12740,NPE with HS2 when using null input format,"When we have a query that returns empty rows and when using tez with hs2, we hit NPE:

{code}
java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)
	... 4 more
15/12/17 18:59:06 INFO log.PerfLogger: </PERFLOG method=getSplits start=1450378746335 end=1450378746433 duration=98 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>
15/12/17 18:59:06 ERROR exec.Task: Failed to execute tez graph.
org.apache.tez.dag.api.TezUncheckedException: Failed to generate InputSplits
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:124)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:617)
	at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1103)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:386)
	at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:156)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1816)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1561)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1338)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1154)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1147)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:181)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:502)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:447)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.writeOldSplits(MRInputHelpers.java:559)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplits(MRInputHelpers.java:619)
	at org.apache.tez.mapreduce.hadoop.MRInputHelpers.configureMRInputWithLegacySplitGeneration(MRInputHelpers.java:109)
	... 23 more
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:192)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:490)
	... 27 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.Utilities.isVectorMode(Utilities.java:3241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.wrapForLlap(HiveInputFormat.java:208)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getInputFormatFromCache(HiveInputFormat.java:267)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:103)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CheckNonCombinablePathCallable.call(CombineHiveInputFormat.java:80)
	... 4 more
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
15/12/17 18:59:06 ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
15/12/17 18:59:06 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1450378746093 end=1450378746434 duration=341 from=org.apache.hadoop.hive.ql.Driver>
15/12/17 18:59:06 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/12/17 18:59:06 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1450378746434 end=1450378746434 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/12/17 18:59:06 ERROR operation.Operation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:367)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:183)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:73)
	at org.apache.hive.service.cli.operation.SQLOperation$2$1.run(SQLOperation.java:234)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hive.service.cli.operation.SQLOperation$2.run(SQLOperation.java:247)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}"
HIVE-12691,Compute stats on hbase tables causes Zookeeper connection leaks.,"hive.stats.autogather defaults to true in newer hive releases which causes stats to be collected on hbase-backed hive tables.

Using HTable APIs causes a new zookeeper connections to be created. So if HTable.close() is not called, the underlying ZK connection remains open as in HIVE-12250."
HIVE-12684,NPE in stats annotation when all values in decimal column are NULLs,"When all column values are null for a decimal column and when column stats exists. AnnotateWithStatistics optimization can throw NPE. Following is the exception trace

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:712)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.convertColStats(StatsUtils.java:764)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:750)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:197)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:143)
        at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:131)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:114)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.walk(LevelOrderWalker.java:143)
        at org.apache.hadoop.hive.ql.lib.LevelOrderWalker.startWalking(LevelOrderWalker.java:122)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:228)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10156)
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:225)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237)

{code}"
HIVE-12662,StackOverflowError in HiveSortJoinReduceRule when limit=0,"L96 of HiveSortJoinReduceRule, you will see 

{noformat}
    // Finally, if we do not reduce the input size, we bail out
    if (RexLiteral.intValue(sortLimit.fetch)
            >= RelMetadataQuery.getRowCount(reducedInput)) {
      return false;
    }
{noformat}

It is using “ RelMetadataQuery.getRowCount” which is always at least 1. This is the problem that we resolved in CALCITE-987.

To confirm this, I just run the q file :

{noformat}
set hive.mapred.mode=nonstrict;
set hive.optimize.limitjointranspose=true;
set hive.optimize.limitjointranspose.reductionpercentage=1f;
set hive.optimize.limitjointranspose.reductiontuples=0;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 0;
{noformat}

  And I got

{noformat}
2015-12-11T10:21:04,435 ERROR [c1efb099-f900-46dc-9f74-97af0944a99d main[]]: parse.CalcitePlanner (CalcitePlanner.java:genOPTree(301)) - CBO failed, skipping CBO.
java.lang.RuntimeException: java.lang.StackOverflowError
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.rethrowCalciteException(CalcitePlanner.java:749) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.getOptimizedAST(CalcitePlanner.java:645) ~[hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:264) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10076) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:223) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) [hive-exec-2.1.0-SNAPSHOT.jar:2.1.0-SNAPSHOT]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:456) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1138) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1187) [hive-exec-2.1.0-SNAPSHOT.jar:?]
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1063) [hive-exec-2.1.0-SNAPSHOT.jar:?]
{noformat}

via [~pxiong]"
HIVE-12660,HS2 memory leak with .hiverc file use,"The Operation objects created to process .hiverc file in HS2 are not closed.
In HiveSessionImpl, GlobalHivercFileProcessor calls executeStatementInternal but ignores the OperationHandle it returns.
"
HIVE-12610,"Hybrid Grace Hash Join should fail task faster if processing first batch fails, instead of continuing processing the rest","During processing the spilled partitions, if there's any fatal error, such as Kryo exception, then we should exit early, instead of moving on to process the rest of spilled partitions."
HIVE-12585,fix TxnHandler connection leak,checkLock(CheckLockRequest rqst) is leaking connection
HIVE-12577,NPE in LlapTaskCommunicator when unregistering containers,"{code}
2015-12-02 13:29:00,160 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator$EntityTracker.unregisterContainer(LlapTaskCommunicator.java:586)
        at org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.registerContainerEnd(LlapTaskCommunicator.java:188)
        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:389)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
        at java.lang.Thread.run(Thread.java:745)
2015-12-02 13:29:00,167 [ERROR] [Dispatcher thread {Central}] |common.AsyncDispatcher|: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.tez.dag.app.TaskCommunicatorManager.unregisterRunningContainer(TaskCommunicatorManager.java:386)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.unregisterFromTAListener(AMContainerImpl.java:1121)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtLaunchingTransition.transition(AMContainerImpl.java:699)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtIdleTransition.transition(AMContainerImpl.java:805)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:892)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl$StopRequestAtRunningTransition.transition(AMContainerImpl.java:887)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:415)
        at org.apache.tez.dag.app.rm.container.AMContainerImpl.handle(AMContainerImpl.java:72)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:60)
        at org.apache.tez.dag.app.rm.container.AMContainerMap.handle(AMContainerMap.java:36)
        at org.apache.tez.common.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
        at org.apache.tez.common.AsyncDispatcher$1.run(AsyncDispatcher.java:114)
        at java.lang.Thread.run(Thread.java:745)
{code}"
HIVE-12532,LLAP Cache: Uncompressed data cache has NPE,"{code}
2015-11-26 08:28:45,232 [TezTaskRunner_attempt_1448429572030_0255_2_02_000019_2(attempt_1448429572030_0255_2_02_000019_2)] WARN org.apache.tez.runtime.LogicalIOProcessorRuntimeTask: Ignoring exception when closing input a(cleanup). Exception class=java.io.IOException, message=java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.rethrowErrorIfAny(LlapInputFormat.java:283)
	at org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat$LlapRecordReader.close(LlapInputFormat.java:275)
	at org.apache.hadoop.hive.ql.io.HiveRecordReader.doClose(HiveRecordReader.java:50)
	at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.close(HiveContextAwareRecordReader.java:104)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.close(TezGroupedSplitsInputFormat.java:177)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.close(MRReaderMapred.java:96)
	at org.apache.tez.mapreduce.input.MRInput.close(MRInput.java:559)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.cleanup(LogicalIOProcessorRuntimeTask.java:872)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:104)
	at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:35)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.preReadUncompressedStream(EncodedReaderImpl.java:795)
	at org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:320)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.performDataRead(OrcEncodedDataReader.java:413)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:194)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader$4.run(OrcEncodedDataReader.java:191)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:191)
	at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:74)
	... 5 more
{code}

Not clear if current.next can set it to null before the continue; 

{code}
      assert partOffset <= current.getOffset();
      if (partOffset == current.getOffset() && current instanceof CacheChunk) {
        // We assume cache chunks would always match the way we read, so check and skip it.
        assert current.getOffset() == partOffset && current.getEnd() == partEnd;
        lastUncompressed = (CacheChunk)current;
        current = current.next;
        continue;
      }
{code}"
HIVE-12445,Tracking of completed dags is a slow memory leak,"LLAP daemons track completed DAGs, but never clean up these structures. This is primarily to disallow out of order executions. Evaluate whether that can be avoided - otherwise this structure needs to be cleaned up with a delay."
HIVE-12420,WebHCat server throws NPE when you run command with -d user.name.,"When you run with '-d user.name', it failed with:
{noformat}
[hrt_qa@os-r6-bccslu-hive-1-r-5 ~]$ curl -s -d user.name=hrt_qa -d execute=""drop table if exists templetontest_tab2;"" http://os-r6-bccslu-hive-1-r-3.novalocal:20111/templeton/v1/ddl
<html>
<head>
<meta http-equiv=""Content-Type"" content=""text/html;charset=ISO-8859-1""/>
<title>Error 500 Server Error</title>
</head>
<body>
<h2>HTTP ERROR: 500</h2>
<p>Problem accessing /templeton/v1/ddl. Reason:
<pre>    Server Error</pre></p>
<hr /><i><small>Powered by Jetty://</small></i>




















</body>
</html>
{noformat}

server log shows:
{noformat}
WARN  | 16 Nov 2015 19:48:22,738 | org.eclipse.jetty.servlet.ServletHandler | /templeton/v1/ddl
java.lang.NullPointerException
	at org.apache.http.client.utils.URLEncodedUtils.parse(URLEncodedUtils.java:235) ~[hive-jdbc-1.2.1.2.3.5.0-13-standalone.jar:1.2.1.2.3.5.0-13]
	at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.getUserName(PseudoAuthenticationHandler.java:143) ~[hadoop-auth-2.6.0.jar:?]
	at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:179) ~[hadoop-auth-2.6.0.jar:?]
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507) ~[hadoop-auth-2.6.0.jar:?]
	at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:88) ~[hadoop-hdfs-2.7.1.2.3.5.0-13.jar:?]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1331) ~[jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:477) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.Server.handle(Server.java:349) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
WARN  | 16 Nov 2015 19:48:22,738 | org.eclipse.jetty.servlet.ServletHandler | /templeton/v1/ddl
java.lang.NullPointerException
	at org.apache.http.client.utils.URLEncodedUtils.parse(URLEncodedUtils.java:235) ~[hive-jdbc-1.2.1.2.3.5.0-13-standalone.jar:1.2.1.2.3.5.0-13]
	at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.getUserName(PseudoAuthenticationHandler.java:143) ~[hadoop-auth-2.6.0.jar:?]
	at org.apache.hadoop.security.authentication.server.PseudoAuthenticationHandler.authenticate(PseudoAuthenticationHandler.java:179) ~[hadoop-auth-2.6.0.jar:?]
	at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:507) ~[hadoop-auth-2.6.0.jar:?]
	at org.apache.hadoop.hdfs.web.AuthFilter.doFilter(AuthFilter.java:88) ~[hadoop-hdfs-2.7.1.2.3.5.0-13.jar:?]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1331) ~[jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:477) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:47) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.Server.handle(Server.java:349) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:599) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:534) [jetty-all-7.6.0.v20120127.jar:7.6.0.v20120127]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
{noformat}

If you use user.name as a part of restful api call, it works."
HIVE-12418,HiveHBaseTableInputFormat.getRecordReader() causes Zookeeper connection leak.,"  @Override
  public RecordReader<ImmutableBytesWritable, ResultWritable> getRecordReader(
...
...
 setHTable(HiveHBaseInputFormatUtil.getTable(jobConf));
...

The HiveHBaseInputFormatUtil.getTable() creates new ZooKeeper connections(when HTable instance is created) which are never closed."
HIVE-12349,NPE in ORC SARG for IS NULL queries on Timestamp and Date columns,"IS NULL queries can trigger an NPE for timestamp and date columns. All column values per row group or stripe should be NULL to trigger this case. Following is the exception stack trace
{code}
Caused by: java.lang.NullPointerException 
at org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl$TimestampStatisticsImpl.getMinimum(ColumnStatisticsImpl.java:795) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.getMin(RecordReaderImpl.java:2343) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.evaluatePredicate(RecordReaderImpl.java:2366) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:2564) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:2627) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:3060) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:3102) 
at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:288) 
at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:534) 
at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183) 
at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226) 
at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437) 
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1141) 
at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1039) 
at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:246) 
... 26 more
{code}"
HIVE-12257,Enhance ORC FileDump utility to handle flush_length files and recovery,ORC file dump utility currently does not handle delta directories that contain *_flush_length files. These files contains offsets to footer in the corresponding delta file.
HIVE-12250,Zookeeper connection leaks in Hive's HBaseHandler.,"HiveServer2 performance regresses severely due to what appears to be a leak in the ZooKeeper connections. lsof output on the HS2 process shows about 8000 TCP connections to the ZK ensemble nodes.

grep TCP lsof-hive-node11 | grep node11 | grep -E ""node03|node04|node05"" | wc -l
    7866 
grep TCP lsof-hive-node11 | grep node11 | grep -E ""node03"" | wc -l
    2615
grep TCP lsof-hive-node11 | grep node11 | grep -E ""node04"" | wc -l
    2622
grep TCP lsof-hive-node11 | grep node11 | grep -E ""node05"" | wc -l
    2629


node11 - HMS node
node03, node04 and node05 are the hosts for zookeeper ensemble."
HIVE-12243,NPE in HiveMetaStoreClient,"While writing a test for SPARK-11265, I can trigger an NPE if the principal is set to ""bob"" and the metastore uri = ""localhost:-1"", due to a null thrift transport.

presumably one of the arguments is invalid"
HIVE-12208,Vectorized JOIN NPE on dynamically partitioned hash-join + map-join,"TPC-DS Q82 with reducer vectorized join optimizations

{code}
  Reducer 5 <- Map 1 (CUSTOM_SIMPLE_EDGE), Map 2 (CUSTOM_SIMPLE_EDGE), Map 3 (BROADCAST_EDGE), Map 4 (CUSTOM_SIMPLE_EDGE)
{code}

{code}
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.vectorized.execution.reduce.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;

select  i_item_id
       ,i_item_desc
       ,i_current_price
 from item, inventory, date_dim, store_sales
 where i_current_price between 30 and 30+30
 and inv_item_sk = i_item_sk
 and d_date_sk=inv_date_sk
 and d_date between '2002-05-30' and '2002-07-30'
 and i_manufact_id in (437,129,727,663)
 and inv_quantity_on_hand between 100 and 500
 and ss_item_sk = i_item_sk
 group by i_item_id,i_item_desc,i_current_price
 order by i_item_id
 limit 100
{code}

possibly a trivial plan setup issue, since the NPE is pretty much immediate.

{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:852)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.forwardBigTableBatch(VectorMapJoinGenerateResultOperator.java:603)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:362)
	... 19 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.commonSetup(VectorMapJoinInnerGenerateResultOperator.java:112)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.process(VectorMapJoinInnerLongOperator.java:96)
	... 22 more
{code}"
HIVE-12202,NPE thrown when reading legacy ACID delta files,"When reading legacy ACID deltas of the form {{delta_$startTxnId_$endTxnId}} a {{NullPointerException}} is thrown on:

{code:title=org.apache.hadoop.hive.ql.io.AcidUtils.deserializeDeltas#371}
if(dmd.getStmtIds().isEmpty()) {
{code}

The older ACID data format (pre-Hive 1.3.0) which does not include the statement ID, and code written for that format should still be supported. Therefore the above condition should also include a null check or alternatively {{AcidInputFormat.DeltaMetaData}} should never return null, and return an empty list in this specific scenario."
HIVE-12196,NPE when converting bad timestamp value,"When I convert a timestamp value that is slightly wrong, the result is a NPE. Other queries correctly reject the timestamp:

{code}
hive> select from_utc_timestamp('2015-04-11-12:24:34.535', 'UTC');
FAILED: NullPointerException null
hive> select TIMESTAMP '2015-04-11-12:24:34.535';
FAILED: SemanticException Unable to convert time literal '2015-04-11-12:24:34.535' to time value.
{code}"
HIVE-12074,"Conditionally turn off hybrid grace hash join based on est. data size, etc","Currently, as long as the below flag is set to true, we always do grace hash join for map join. This may not be necessary, esp. for cases where the data size is quite small, and number of distinct values is also small.

hive.mapjoin.hybridgrace.hashtable"
HIVE-12030,Hive throws NPE with ACID enabled tables,"
Code based on master: commit 507442319985198466b4f6c2ba18c6b068d8435e Date: Thu Oct 1 

Exception
=========
{noformat}
Caused by: java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
        at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:253)
        at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:193)
        ... 25 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.io.AcidUtils.deserializeDeltas(AcidUtils.java:371)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1272)
        at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1190)
        at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:250)
        ... 26 more
{noformat}

Steps to reproduce the issue:
=============================

{noformat}
--hiveconf hive.support.concurrency=true --hiveconf hive.enforce.bucketing=true --hiveconf hive.exec.dynamic.partition.mode=nonstrict --hiveconf hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager --hiveconf hive.compactor.initiator.on=true --hiveconf hive.compactor.worker.threads=1

DROP TABLE `lineitem_acid_bucket`;

CREATE TABLE `lineitem_acid_bucket`(
  `l_orderkey` bigint,
  `l_partkey` bigint,
  `l_suppkey` bigint,
  `l_linenumber` bigint,
  `l_quantity` double,
  `l_extendedprice` double,
  `l_discount` double,
  `l_tax` double,
  `l_returnflag` string,
  `l_linestatus` string,
  `l_shipdate` string,
  `l_commitdate` string,
  `l_receiptdate` string,
  `l_shipinstruct` string,
  `l_shipmode` string,
  `l_comment` string)
CLUSTERED BY (l_orderkey)
INTO 10 BUCKETS STORED AS ORC TBLPROPERTIES(""transactional""=""true""); 

INSERT INTO lineitem_acid_bucket SELECT * FROM tpch_flat_orc_1000.lineitem WHERE l_orderkey > 0 AND l_orderkey < 10000000;

INSERT INTO lineitem_acid_bucket SELECT * FROM tpch_flat_orc_1000.lineitem WHERE l_orderkey > 10000001 AND l_orderkey < 20000000;

update lineitem_acid_bucket set l_quantity=1 where l_orderkey=5963520;

ALTER TABLE lineitem_acid_bucket COMPACT 'minor';

update lineitem_acid_bucket set l_quantity=1 where l_orderkey=5963520;

exception thrown here

{noformat}"
HIVE-12022,NPE in SARG with timestamp cast,"Running the following query throws NPE during SARG creation
{code:title=Query}
select count(*) from alltypes_orc where ts1 between cast('1969-12-31' as timestamp) and cast('1970-12-31' as timestamp);
{code}

{code:title=Exception}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$PredicateLeafImpl.toString(SearchArgumentImpl.java:148)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.toString(SearchArgumentImpl.java:997)
	at java.lang.String.valueOf(String.java:2854)
	at java.lang.StringBuilder.append(StringBuilder.java:128)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.setSearchArgument(OrcInputFormat.java:319)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1245)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1151)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:251)
	... 26 more

{code}"
HIVE-11987,CompactionTxnHandler.createValidCompactTxnList() can use much less memory,"This method only needs HWM and list of txn IDs in 'a' state and smallest 'o' txn id.

It's currently implemented to get the list from TxnHandler.getOpenTxnsInfo(),
which returns (txn id, state, host, user) for each txn and includes Aborted txns.

This can easily be 120 bytes or more per txn overhead (over 1 Java long) which not an issue in general but when the system is misconfigured, the number of opened/aborted txns can get into the millions.  This creates unnecessary memory pressure on metastore.

Should consider fixing this.
This should be easy to fix since the result of getOpenTxnsInfo() doesn't go over the wire.

Also, ValidCompactorTxnList doesn't actually need to store the 'o' txn ids, just the 'a' ones."
HIVE-11940,"""INSERT OVERWRITE"" query is very slow because it creates one ""distcp"" per file to copy data from staging directory to target directory","When hive.exec.stagingdir is set to "".hive-staging"", which will be placed under the target directory when running ""INSERT OVERWRITE"" query, Hive will grab all files under the staging directory and copy them ONE BY ONE to target directory.

When hive exec.stagingdir is set to ""/tmp/hive"", Hive will simply do a RENAME operation which will be instant.

This happens with files that are not encrypted. "
HIVE-11935,Race condition in  HiveMetaStoreClient: isCompatibleWith and close,"We saw intermittent failure of the following stack:
{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.isCompatibleWith(HiveMetaStoreClient.java:287)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
        at com.sun.proxy.$Proxy9.isCompatibleWith(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:206)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.createHiveDB(BaseSemanticAnalyzer.java:205)
        at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.<init>(DDLSemanticAnalyzer.java:223)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:259)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:409)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1116)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:110)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:181)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:388)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:375)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)
        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)
        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)
        at com.sun.proxy.$Proxy20.executeStatementAsync(Unknown Source)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:274)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:486)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)
        at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:171)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)
        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)
        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)
        at org.eclipse.jetty.server.Server.handle(Server.java:349)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:857)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}
HiveMetaStoreClient.isCompatibleWith does a null check of currentMetaVars in the beginning, but it is possible HiveMetaStoreClient.close is invoked before it gets used, thus we will see the above stack. Access of currentMetaVars should be synchronized."
HIVE-11906,IllegalStateException: Attempting to flush a RecordUpdater on....bucket_00000 with a single transaction.,"{noformat}
java.lang.IllegalStateException: Attempting to flush a RecordUpdater on hdfs://127.0.0.1:9000/user/hive/warehouse/store_sales/dt=2015/delta_0003405_0003405/bucket_00000 with a single transaction.
	at org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.flush(OrcRecordUpdater.java:341)
	at org.apache.hive.hcatalog.streaming.AbstractRecordWriter.flush(AbstractRecordWriter.java:124)
	at org.apache.hive.hcatalog.streaming.DelimitedInputWriter.flush(DelimitedInputWriter.java:49)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.commitImpl(HiveEndPoint.java:723)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$TransactionBatchImpl.commit(HiveEndPoint.java:701)
	at org.apache.hive.acid.RueLaLaTest.test(RueLaLaTest.java:89)
{noformat}

{noformat}
package org.apache.hive.acid;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.hive.common.JavaUtils;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.ql.Driver;
import org.apache.hadoop.hive.ql.session.SessionState;
import org.apache.hive.hcatalog.streaming.DelimitedInputWriter;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingConnection;
import org.apache.hive.hcatalog.streaming.TransactionBatch;
import org.junit.Test;

import java.net.URL;
import java.util.ArrayList;
import java.util.List;

/**
 */
public class RueLaLaTest {
  static final private Log LOG = LogFactory.getLog(RueLaLaTest.class);
  @Test
  public void test() throws Exception {
    HiveConf.setHiveSiteLocation(new URL(""file:///Users/ekoifman/dev/hwxhive/packaging/target/apache-hive-0.14.0-bin/apache-hive-0.14.0-bin/conf/hive-site.xml""));
    HiveConf hiveConf = new HiveConf(this.getClass());
    final String workerName = ""test_0"";
    SessionState.start(new SessionState(hiveConf));
    Driver d = new Driver(hiveConf);
    d.setMaxRows(200002);//make sure Driver returns all results
    runStatementOnDriver(d, ""drop table if exists store_sales"");
    runStatementOnDriver(d, ""create table store_sales\n"" +
      ""(\n"" +
      ""    ss_sold_date_sk           int,\n"" +
      ""    ss_sold_time_sk           int,\n"" +
      ""    ss_item_sk                int,\n"" +
      ""    ss_customer_sk            int,\n"" +
      ""    ss_cdemo_sk               int,\n"" +
      ""    ss_hdemo_sk               int,\n"" +
      ""    ss_addr_sk                int,\n"" +
      ""    ss_store_sk               int,\n"" +
      ""    ss_promo_sk               int,\n"" +
      ""    ss_ticket_number          int,\n"" +
      ""    ss_quantity               int,\n"" +
      ""    ss_wholesale_cost         decimal(7,2),\n"" +
      ""    ss_list_price             decimal(7,2),\n"" +
      ""    ss_sales_price            decimal(7,2),\n"" +
      ""    ss_ext_discount_amt       decimal(7,2),\n"" +
      ""    ss_ext_sales_price        decimal(7,2),\n"" +
      ""    ss_ext_wholesale_cost     decimal(7,2),\n"" +
      ""    ss_ext_list_price         decimal(7,2),\n"" +
      ""    ss_ext_tax                decimal(7,2),\n"" +
      ""    ss_coupon_amt             decimal(7,2),\n"" +
      ""    ss_net_paid               decimal(7,2),\n"" +
      ""    ss_net_paid_inc_tax       decimal(7,2),\n"" +
      ""    ss_net_profit             decimal(7,2)\n"" +
      "")\n"" +
      "" partitioned by (dt string)\n"" +
      ""clustered by (ss_store_sk, ss_promo_sk)\n"" +
      ""INTO 2 BUCKETS stored as orc TBLPROPERTIES ('orc.compress'='NONE', 'transactional'='true')"");

    runStatementOnDriver(d, ""alter table store_sales add partition(dt='2015')"");
    LOG.info(workerName + "" starting..."");
    List<String> partitionVals = new ArrayList<String>();
    partitionVals.add(""2015"");
    HiveEndPoint endPt = new HiveEndPoint(HiveConf.getVar(hiveConf, HiveConf.ConfVars.METASTOREURIS, ""thrift://localhost:9933""), ""default"", ""store_sales"", partitionVals);
    DelimitedInputWriter writer = new DelimitedInputWriter(new String[] {""ss_sold_date_sk"",""ss_sold_time_sk"", ""ss_item_sk"", 
      ""ss_customer_sk"", ""ss_cdemo_sk"", ""ss_hdemo_sk"", ""ss_addr_sk"", ""ss_store_sk"", ""ss_promo_sk"", ""ss_ticket_number"", ""ss_quantity"", 
      ""ss_wholesale_cost"", ""ss_list_price"", ""ss_sales_price"", ""ss_ext_discount_amt"", ""ss_ext_sales_price"", ""ss_ext_wholesale_cost"", 
      ""ss_ext_list_price"", ""ss_ext_tax"", ""ss_coupon_amt"", ""ss_net_paid"", ""ss_net_paid_inc_tax"", ""ss_net_profit""},"","", endPt);
    StreamingConnection connection = endPt.newConnection(false, null);//should this really be null?

    TransactionBatch txnBatch =  connection.fetchTransactionBatch(1, writer);
    LOG.info(workerName + "" started txn batch"");
    txnBatch.beginNextTransaction();
    LOG.info(workerName + "" started commit txn "" + JavaUtils.txnIdToString(txnBatch.getCurrentTxnId()));

    StringBuilder row = new StringBuilder();
    for(int i = 0; i < 1; i++) {
      for(int ints = 0; ints < 11; ints++) {
        row.append(ints).append(',');
      }
      for(int decs = 0; decs < 12; decs++) {
        row.append(i + 0.1).append(',');
      }
      row.setLength(row.length() - 1);
      txnBatch.write(row.toString().getBytes());
    }
    txnBatch.commit();
    txnBatch.close();
    connection.close();
  }
  private List<String> runStatementOnDriver(Driver d, String stmt) throws Exception {
    return AcidSystemTest.runStatementOnDriver(d, stmt);
  }
}
{noformat}

key part being that TransactionBatch has size 1.  > 1 works OK."
HIVE-11849,NPE in HiveHBaseTableShapshotInputFormat in query with just count(*),"Adding the following example as a qfile test in hbase-handler fails. Looks like this may have been introduced by HIVE-5277.

{noformat}
SET hive.hbase.snapshot.name=src_hbase_snapshot;
SET hive.hbase.snapshot.restoredir=/tmp;

select count(*) from src_hbase;
{noformat}"
HIVE-11836,ORC SARG creation throws NPE for null constants with void type,"Queries like
{code}
select * from table where col = null
{code}

will throw the following exception
{code}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.boxLiteral(SearchArgumentImpl.java:446)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.getLiteral(SearchArgumentImpl.java:476)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.createLeaf(SearchArgumentImpl.java:524)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.createLeaf(SearchArgumentImpl.java:584)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.parse(SearchArgumentImpl.java:629)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.addChildren(SearchArgumentImpl.java:598)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.parse(SearchArgumentImpl.java:621)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.addChildren(SearchArgumentImpl.java:598)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.parse(SearchArgumentImpl.java:621)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl$ExpressionBuilder.expression(SearchArgumentImpl.java:916)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.<init>(SearchArgumentImpl.java:953)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory.create(SearchArgumentFactory.java:36)
	at org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory.createFromConf(SearchArgumentFactory.java:50)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.setSearchArgument(OrcInputFormat.java:312)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1224)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1113)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:249)
{code}

This issue does not happen when CBO is enabled."
HIVE-11824,Insert to local directory causes staging directory to be copied,"While running tez_insert_overwrite_local_directory_1.q test under tez and llap, the results were flaky. The reason being insert into local directory copies .staging directory to destination directory. This causes dfs -cat dest-dir/* to fail, failing the tests. Non local insert directory works fine as MoveTask performs move operation with hidden files removal filter before moving whereas local directory insert uses DFS.copyToLocalFile without hidden files filter."
HIVE-11768,java.io.DeleteOnExitHook leaks memory on long running Hive Server2 Instances,"  More than 490,000 paths was added to java.io.DeleteOnExitHook on one of our long running HiveServer2 instances,taken up more than 100MB on heap.
  Most of the paths contains a suffix of "".pipeout"".
"
HIVE-11749,Deadlock of fetching InputFormat table when multiple root stage,"But not always, to deadlock when it run the query. Environment are as follows:
* Hadoop 2.6.0
* Hive 0.13
* JDK 1.7.0_79
It will attach the stack trace."
HIVE-11714,Turn off hybrid grace hash join for cross product join,"Current partitioning calculation is solely based on hash value of the key. For cross product join where keys are empty, all the rows will be put into partition 0. This falls back to the regular mapjoin behavior where we only have one hashtable."
HIVE-11596,"nvl(x, y) throws NPE if type x and type y doesn't match, rather than throwing the meaningful error","{noformat}
create table test(key string);
select nvl(key, true) from test;
{noformat}

The query above will throw NPE rather than the meaningful error ""The first and seconds arguments of function NLV should have the same type"".
"
HIVE-11587,Fix memory estimates for mapjoin hashtable,"Due to the legacy in in-memory mapjoin and conservative planning, the memory estimation code for mapjoin hashtable is currently not very good. It allocates the probe erring on the side of more memory, not taking data into account because unlike the probe, it's free to resize, so it's better for perf to allocate big probe and hope for the best with regard to future data size. It is not true for hybrid case.
There's code to cap the initial allocation based on memory available (memUsage argument), but due to some code rot, the memory estimates from planning are not even passed to hashtable anymore (there used to be two config settings, hashjoin size fraction by itself, or hashjoin size fraction for group by case), so it never caps the memory anymore below 1 Gb. 
Initial capacity is estimated from input key count, and in hybrid join cache can exceed Java memory due to number of segments.

There needs to be a review and fix of all this code.
Suggested improvements:
1) Make sure ""initialCapacity"" argument from Hybrid case is correct given the number of segments. See how it's calculated from keys for regular case; it needs to be adjusted accordingly for hybrid case if not done already.
1.5) Note that, knowing the number of rows, the maximum capacity one will ever need for probe size (in longs) is row count (assuming key per row, i.e. maximum possible number of keys) divided by load factor, plus some very small number to round up. That is for flat case. For hybrid case it may be more complex due to skew, but that is still a good upper bound for the total probe capacity of all segments.
2) Rename memUsage to maxProbeSize, or something, make sure it's passed correctly based on estimates that take into account both probe and data size, esp. in hybrid case.
3) Make sure that memory estimation for hybrid case also doesn't come up with numbers that are too small, like 1-byte hashtable. I am not very familiar with that code but it has happened in the past.

Other issues we have seen:
4) Cap single write buffer size to 8-16Mb. The whole point of WBs is that you should not allocate large array in advance. Even if some estimate passes 500Mb or 40Mb or whatever, it doesn't make sense to allocate that.
5) For hybrid, don't pre-allocate WBs - only allocate on write.
6) Change everywhere rounding up to power of two is used to rounding down, at least for hybrid case (?)

I wanted to put all of these items in single JIRA so we could keep track of fixing all of them.
I think there are JIRAs for some of these already, feel free to link them to this one."
HIVE-11567,Some trace logs seeped through with new log4j2 changes,Observed hive.log file size difference when running with new log4j2 changes (HIVE-11304). Looks like the default threshold was DEBUG in log4j1.x (as log4j.threshold was misspelt). In log4j2 the default threshold was set to ALL which emitted some trace logs.
HIVE-11566,Hybrid grace hash join should only allocate write buffer for a hash partition when first write happens,"Currently it's allocating one write buffer for a number of hash partitions up front, which can cause GC pause.

It's better to do the write buffer allocation on demand."
HIVE-11499,Datanucleus leaks classloaders when used using embedded metastore with HiveServer2 with UDFs,"When UDFs are used, we create a new classloader to add the UDF jar. Similar to what hadoop's reflection utils does(HIVE-11408), datanucleus caches the classloaders (https://github.com/datanucleus/datanucleus-core/blob/3.2/src/java/org/datanucleus/NucleusContext.java#L161). JDOPersistanceManager factory (1 per JVM) holds on to a NucleusContext reference (https://github.com/datanucleus/datanucleus-api-jdo/blob/3.2/src/java/org/datanucleus/api/jdo/JDOPersistenceManagerFactory.java#L115). Until we call  NucleusContext#close, the classloader cache is not cleared. In case of UDFs this can lead to permgen leak, as shown in the attached screenshot, where NucleusContext holds on to several URLClassloader objects."
HIVE-11470,NPE in DynamicPartFileRecordWriterContainer on null part-keys.,"When partitioning data using {{HCatStorer}}, one sees the following NPE, if the dyn-part-key is of null-value:

{noformat}
2015-07-30 23:59:59,627 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.lang.NullPointerException
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)
at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.NullPointerException
at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)
at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)
at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)
at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)
at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)
at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)
... 11 more
{noformat}

The reason is that the {{DynamicPartitionFileRecordWriterContainer}} makes an unfortunate assumption when fetching a local file-writer instance:

{code:title=DynamicPartitionFileRecordWriterContainer.java}
  @Override
  protected LocalFileWriter getLocalFileWriter(HCatRecord value) 
    throws IOException, HCatException {
    
    OutputJobInfo localJobInfo = null;
    // Calculate which writer to use from the remaining values - this needs to
    // be done before we delete cols.
    List<String> dynamicPartValues = new ArrayList<String>();
    for (Integer colToAppend : dynamicPartCols) {
      dynamicPartValues.add(value.get(colToAppend).toString()); // <-- YIKES!
    }
    ...
  }
{code}

Must check for null, and substitute with {{""\_\_HIVE_DEFAULT_PARTITION\_\_""}}, or equivalent."
HIVE-11433,NPE for a multiple inner join query,"NullPointException is thrown for query that has multiple (greater than 3) inner joins. Stacktrace for 1.1.0
{code}
NullPointerException null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.ParseUtils.getIndex(ParseUtils.java:149)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(ParseUtils.java:166)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(ParseUtils.java:185)
        at org.apache.hadoop.hive.ql.parse.ParseUtils.checkJoinFilterRefersOneAlias(ParseUtils.java:185)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.mergeJoins(SemanticAnalyzer.java:8257)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.mergeJoinTree(SemanticAnalyzer.java:8422)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9805)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9714)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10150)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10161)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10078)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1110)
        at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1104)
        at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)
        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:386)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:373)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:486)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}.
However, the problem can also be reproduced in latest master branch. Further investigation shows that the following code (in ParseUtils.java) is problematic:
{code}
  static int getIndex(String[] list, String elem) {
    for(int i=0; i < list.length; i++) {
      if (list[i].toLowerCase().equals(elem)) {
        return i;
      }
    }
    return -1;
  }
{code}
The code assumes that every element in the list is not null, which isn't true because of the following code in SemanticAnalyzer.java (method genJoinTree()):
{code}
    if ((right.getToken().getType() == HiveParser.TOK_TABREF)
        || (right.getToken().getType() == HiveParser.TOK_SUBQUERY)
        || (right.getToken().getType() == HiveParser.TOK_PTBLFUNCTION)) {
      String tableName = getUnescapedUnqualifiedTableName((ASTNode) right.getChild(0))
          .toLowerCase();
      String alias = extractJoinAlias(right, tableName);
      String[] rightAliases = new String[1];
      rightAliases[0] = alias;
      joinTree.setRightAliases(rightAliases);
      String[] children = joinTree.getBaseSrc();
      if (children == null) {
        children = new String[2];
      }
      children[1] = alias;
      joinTree.setBaseSrc(children);
      joinTree.setId(qb.getId());
      joinTree.getAliasToOpInfo().put(
          getModifiedAlias(qb, alias), aliasToOpInfo.get(alias));
      // remember rhs table for semijoin
      if (joinTree.getNoSemiJoin() == false) {
        joinTree.addRHSSemijoin(alias);
      }
    } else {
{code}.
Specifically, this code can result a null element as base source:
{code}
      if (children == null) {
        children = new String[2];
      }
      children[1] = alias;
{code}
This appears to be a regression from earlier release (0.14.1). However, it's unclear which commit caused this."
HIVE-11431,Vectorization: select * Left Semi Join projections NPE,"The ""select *"" is meant to only apply to the left most table, not the right most - the unprojected ""d"" from tmp1 triggers this NPE.

{code}
select * from tmp2 left semi join tmp1 where c1 = id and c0 = q;
{code}

{code}
Caused by: java.lang.NullPointerException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.hadoop.io.Text.set(Text.java:225)
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow$StringExtractorByValue.extract(VectorExtractRow.java:472)
        at org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.extractRow(VectorExtractRow.java:732)
        at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.process(VectorFileSinkOperator.java:96)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
        at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:136)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
        at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:117)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
{code}"
HIVE-11408,HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used due to constructor caching in Hadoop ReflectionUtils,"I'm able to reproduce with 0.14. I'm yet to see if HIVE-10453 fixes the issue (since it's on top of a larger patch: HIVE-2573 that was added in 1.2). 

Basically, add jar creates a new classloader for loading the classes from the new jar and adds the new classloader to the SessionState object of user's session, making the older one its parent. Creating a temporary function uses the new classloader to load the class used for the function. On closing a session, although there is code to close the classloader for the session, I'm not seeing the new classloader getting GCed and from the heapdump I can see it holds on to the temporary function's class that should have gone away after the session close. 

Steps to reproduce:
1.
{code}
jdbc:hive2://localhost:10000/> add jar hdfs:///tmp/audf.jar;
{code}


2. 
Use a profiler (I'm using yourkit) to verify that a new URLClassLoader was added.


3. 
{code}
jdbc:hive2://localhost:10000/> CREATE TEMPORARY FUNCTION funcA AS 'org.gumashta.udf.AUDF'; 
{code}


4. 
Close the jdbc session.

5. 
Take the memory snapshot and verify that the new URLClassLoader is indeed there and is holding onto the class it loaded (org.gumashta.udf.AUDF) for the session which we already closed.



"
HIVE-11380,NPE when FileSinkOperator is not initialized,"When FileSinkOperator's initializeOp is not called (which may happen when an operator before FileSinkOperator initializeOp failed), FileSinkOperator will throw NPE at close time. The stacktrace:
{noformat}
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException

at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:523)

at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:952)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:598)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:610)

at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.close(ExecMapper.java:199)

at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)

at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)

at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)

at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)

at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

at java.util.concurrent.FutureTask.run(FutureTask.java:262)

at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)

Caused by: java.lang.NullPointerException

at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:519)

... 18 more
{noformat}
This Exception is misleading and often distracts users from finding real issues. "
HIVE-11371,Null pointer exception for nested table query when using ORC versus text,"Following query will fail if the file format is ORC 

select tj1rnum, tj2rnum, tjoin3.rnum as rnumt3 from   (select tjoin1.rnum tj1rnum, tjoin2.rnum tj2rnum, tjoin2.c1 tj2c1  from tjoin1 left outer join tjoin2 on tjoin1.c1 = tjoin2.c1 ) tj  left outer join tjoin3 on tj2c1 = tjoin3.c1 


aused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow$LongCopyRow.copy(VectorCopyRow.java:60)
	at org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.copyByReference(VectorCopyRow.java:260)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.generateHashMapResultMultiValue(VectorMapJoinGenerateResultOperator.java:238)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.finishOuter(VectorMapJoinOuterGenerateResultOperator.java:495)
	at org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.process(VectorMapJoinOuterLongOperator.java:430)
	... 22 more
]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:0, Vertex vertex_1437788144883_0004_2_02 [Map 1] killed/failed due to:null]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0
SQLState:  08S01
ErrorCode: 2

getDatabaseProductName	Apache Hive
getDatabaseProductVersion	1.2.1.2.3.0.0-2557
getDriverName	Hive JDBC
getDriverVersion	1.2.1.2.3.0.0-2557
getDriverMajorVersion	1
getDriverMinorVersion	2



create table  if not exists TJOIN1 (RNUM int , C1 int, C2 int)
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc 					;


create table  if not exists TJOIN2 (RNUM int , C1 int, C2 char(2))
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc ;


create table  if not exists TJOIN3 (RNUM int , C1 int, C2 char(2))
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc ;


create table  if not exists TJOIN4 (RNUM int , C1 int, C2 char(2))
-- ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' 
 STORED AS orc ;


"
HIVE-11355,Hive on tez: memory manager for sort buffers (input/output) and operators,"We need to better manage the sort buffer allocations to ensure better performance. Also, we need to provide configurations to certain operators to stay within memory limits."
HIVE-11299,Hive query planning allocating lots of memory and hitting OOMs ,Plan generation for queries with lots of disjunct filters spends lots of time compiling. 
HIVE-11221,"In Tez mode, alter table concatenate orc files can intermittently fail with NPE","We are not waiting for input ready events which can trigger occasional NPE if input is not actually ready.

Stacktrace:
{code}
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:186)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileTezProcessor.run(MergeFileTezProcessor.java:42)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:265)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:478)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:471)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:648)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:146)
	at org.apache.tez.mapreduce.lib.MRReaderMapred.<init>(MRReaderMapred.java:73)
	at org.apache.tez.mapreduce.input.MRInput.initializeInternal(MRInput.java:483)
	at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:108)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.getMRInput(MergeFileRecordProcessor.java:220)
	at org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.init(MergeFileRecordProcessor.java:72)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)
	... 13 more
{code}"
HIVE-11220,HS2 Lineage leakage with 16x concurrency tests,"Test scenario is the HS2 + LLAP, 16x concurrency of TPCDS queries which take less than 4 seconds.

session.LineageState accumulates optimizer lineage info and HS2 OOMs due to the amount of data being held in the SessionState, since the sessions are continously being used without pause.

The issue seems to be triggered due to the volume of fast queries or the life time of a single JDBC connection."
HIVE-11216,UDF GenericUDFMapKeys throws NPE when a null map value is passed in,"We can reproduce the problem as below:
{noformat}
hive> show create table map_txt;
OK
CREATE  TABLE `map_txt`(
  `id` int,
  `content` map<int,string>)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
...
Time taken: 0.233 seconds, Fetched: 18 row(s)
hive> select * from map_txt;
OK
1       NULL
Time taken: 0.679 seconds, Fetched: 1 row(s)
hive> select id, map_keys(content) from map_txt;
....
Error during job, obtaining debugging information...
Examining task ID: task_1435534231122_0025_m_000000 (and more) from job job_1435534231122_0025

Task with the most failures(4):
-----
Task ID:
  task_1435534231122_0025_m_000000

URL:
  http://host-10-17-80-40.coe.cloudera.com:8088/taskdetails.jsp?jobid=job_1435534231122_0025&tipid=task_1435534231122_0025_m_000000
-----
Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""id"":1,""content"":null}
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:198)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""id"":1,""content"":null}
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:559)
        at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:180)
        ... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating map_keys(content)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
        at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
        at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
        ... 9 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.evaluate(GenericUDFMapKeys.java:64)
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:166)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:79)
        ... 13 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL
hive>
{noformat}

The error is as below (in mappers):
{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFMapKeys.evaluate(GenericUDFMapKeys.java:64)
        at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator._evaluate(ExprNodeGenericFuncEvaluator.java:166)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)
        at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)
        at org.apache.hadoop.hive.ql.exec.KeyWrapperFactory$ListKeyWrapper.getNewKey(KeyWrapperFactory.java:113)
        at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:778)
        ... 17 more
{noformat}

Looking at the source code:
{code}
  public Object evaluate(DeferredObject[] arguments) throws HiveException {
    retArray.clear();
    Object mapObj = arguments[0].get();
    retArray.addAll(mapOI.getMap(mapObj).keySet());
    return retArray;
  }
{code}
It is obvious that we will have a NPE when a NULL map value is passed in"
HIVE-11215,Vectorized grace hash-join throws FileUtil warnings,"TPC-DS query13 warnings about a null-file deletion.

{code}
2015-07-09 03:14:18,880 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows restored from KeyValueContainer: 31184
2015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Deserializing spilled hash partition...
2015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows in hashmap: 31184
2015-07-09 03:14:18,897 INFO [TezChild] exec.MapJoinOperator: spilled: true abort: false. Clearing spilled partitions.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.
{code}"
HIVE-11200,LLAP: Cache BuddyAllocator throws NPE,"Built off da1e0cf21aeff0a9501c5e220a6f66ba61f6da94 merge point

{code}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.allocateWithSplit(BuddyAllocator.java:331)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.allocateWithExpand(BuddyAllocator.java:399)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator$Arena.access$300(BuddyAllocator.java:228)
        at org.apache.hadoop.hive.llap.cache.BuddyAllocator.allocateMultiple(BuddyAllocator.java:156)
        at org.apache.hadoop.hive.ql.io.orc.InStream.readEncodedStream(InStream.java:761)
        at org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.readEncodedColumns(EncodedReaderImpl.java:462)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:342)
        at org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.callInternal(OrcEncodedDataReader.java:59)
        at org.apache.hadoop.hive.common.CallableWithNdc.call(CallableWithNdc.java:37)
        ... 4 more
2015-07-08 01:17:42,798 [TezTaskRunner_attempt_1435700346116_1212_4_05_000080_0(attempt_1435700346116_1212_4_05_000080_0)] ERROR org.apache.hadoop.hive.ql.exec.tez.TezProcessor: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.IOException: java.io.IOException: java.lang.NullPointerException
{code}"
HIVE-11062,Remove Exception stacktrace from Log.info when ACL is not supported.,"When logging set to info, Extended ACL Enabled and the file system does not support ACL, there are a lot of Exception stack trace in the log file. Although it is benign, it can easily make users frustrated. We should set the level to show the Exception in debug. 
Current, the Exception in the log looks like:
{noformat}
2015-06-19 05:09:59,376 INFO org.apache.hadoop.hive.shims.HadoopShimsSecure: Skipping ACL inheritance: File system for path s3a://yibing/hive does not support ACLs but dfs.namenode.acls.enabled is set to true: java.lang.UnsupportedOperationException: S3AFileSystem doesn't support getAclStatus
java.lang.UnsupportedOperationException: S3AFileSystem doesn't support getAclStatus
	at org.apache.hadoop.fs.FileSystem.getAclStatus(FileSystem.java:2429)
	at org.apache.hadoop.hive.shims.Hadoop23Shims.getFullFileStatus(Hadoop23Shims.java:729)
	at org.apache.hadoop.hive.ql.metadata.Hive.inheritFromTable(Hive.java:2786)
	at org.apache.hadoop.hive.ql.metadata.Hive.replaceFiles(Hive.java:2694)
	at org.apache.hadoop.hive.ql.metadata.Table.replaceFiles(Table.java:640)
	at org.apache.hadoop.hive.ql.metadata.Hive.loadTable(Hive.java:1587)
	at org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:297)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1638)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1397)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1181)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1047)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1042)
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:145)
	at org.apache.hive.service.cli.operation.SQLOperation.access$100(SQLOperation.java:70)
	at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:209)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
"
HIVE-11036,Race condition in DataNucleus makes Metastore to hang,Under moderate to high concurrent query workload Metastore gets deadlocked in DataNucleus
HIVE-10963,Hive throws NPE rather than meaningful error message when window is missing,"{{select sum(salary) over w1 from emp;}} throws NPE rather than meaningful error message like ""missing window"".

And also give the right window name rather than the classname in the error message after NPE issue is fixed.
{noformat}
org.apache.hadoop.hive.ql.parse.SemanticException: Window Spec org.apache.hadoop.hive.ql.parse.WindowingSpec$WindowSpec@7954e1de refers to an unknown source
{noformat}"
HIVE-10961,LLAP: ShuffleHandler + Submit work init race condition,"When flexing in a new node, it accepts DAG requests before the shuffle handler is setup, causing fatals

{code}
DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:2
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1433459966952_0729_1_00, diagnostics=[Task failed, taskId=task_1t
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.get(ShuffleHandler.java:353)
        at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.submitWork(ContainerRunnerImpl.java:192)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.submitWork(LlapDaemon.java:301)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.submitWork(LlapDaemonProtocolServerImpl.java:75)
        at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(LlapDaemonProtocolProtos.java:12094)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:972)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2085)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1654)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2081)
], TaskAttempt 1 failed, info=[org.apache.hadoop.ipc.RemoteException(java.lang.IllegalStateException): ShuffleHandler must be started before invoking get
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.get(ShuffleHandler.java:353)
        at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.submitWork(ContainerRunnerImpl.java:192)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.submitWork(LlapDaemon.java:301)
        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.submitWork(LlapDaemonProtocolServerImpl.java:75)
        at org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos$LlapDaemonProtocol$2.callBlockingMethod(LlapDaemonProtocolProtos.java:12094)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:972)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2085)
{code}"
HIVE-10925,Non-static threadlocals in metastore code can potentially cause memory leak,"There are many places where non-static threadlocals are used. I can't seem to find a good logic for using them. However, they can potentially result in leaking objects if for example they are created in a long running thread every time the thread handles a new session."
HIVE-10895,"ObjectStore does not close Query objects in some calls, causing a potential leak in some metastore db resources","During testing, we've noticed Oracle db running out of cursors. Might be related to this."
HIVE-10793,Hybrid Hybrid Grace Hash Join : Don't allocate all hash table memory upfront,"HybridHashTableContainer will allocate memory based on estimate, which means if the actual is less than the estimate the allocated memory won't be used.

Number of partitions is calculated based on estimated data size
{code}
numPartitions = calcNumPartitions(memoryThreshold, estimatedTableSize, minNumParts, minWbSize,
          nwayConf);
{code}

Then based on number of partitions writeBufferSize is set

{code}
writeBufferSize = (int)(estimatedTableSize / numPartitions);
{code}

Each hash partition will allocate 1 WriteBuffer, with no further allocation if the estimate data size is correct.

Suggested solution is to reduce writeBufferSize by a factor such that only X% of the memory is preallocated.

"
HIVE-10781,HadoopJobExecHelper Leaks RunningJobs,"On one of our busy hadoop cluster, hiveServer2 holds more than 4000 org.apache.hadoop.mapred.JobClient$NetworkedJob instances,while only has less than 3 backgroud handler thread at the same time.
All these instances are hold in one LinkedList from org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper's  runningJobs property,which is static."
HIVE-10759,LLAP: Add aging to wait queue tasks,Wait queue priority does not increase the priority (aging) of pre-empted tasks making them starve in concurrent tests.
HIVE-10595,Dropping a table can cause NPEs in the compactor,"Reproduction:
# start metastore with compactor off
# insert enough entries in a table to trigger a compaction
# drop the table
# stop metastore
# restart metastore with compactor on

Result:  NPE in the compactor threads.  I suspect this would also happen if the inserts and drops were done in between a run of the compactor, but I haven't proven it."
HIVE-10538,Fix NPE in FileSinkOperator from hashcode mismatch,"A Null Pointer Exception occurs when in FileSinkOperator when using bucketed tables and distribute by with multiFileSpray enabled. The following snippet query reproduces this issue:

{code}
set hive.enforce.bucketing = true;
set hive.exec.reducers.max = 20;

create table bucket_a(key int, value_a string) clustered by (key) into 256 buckets;
create table bucket_b(key int, value_b string) clustered by (key) into 256 buckets;
create table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets;

-- Insert data into bucket_a and bucket_b

insert overwrite table bucket_ab
select a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key;
{code}

The following stack trace is logged.

{code}
2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{},""value"":{""_col0"":""113"",""_col1"":""val_113""}}
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)
	... 8 more
{code}"
HIVE-10500,Repeated deadlocks in underlying RDBMS cause transaction or lock failure,"In some cases in a busy system, deadlocks in the metastore RDBMS can cause failures in Hive locks and transactions when using DbTxnManager"
HIVE-10495,Hive index creation code throws NPE if index table is null,"The stack trace would be:

Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_index(HiveMetaStore.java:2870)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:60)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)
        at java.lang.reflect.Method.invoke(Method.java:611)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)
        at $Proxy9.add_index(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createIndex(HiveMetaStoreClient.java:962)"
HIVE-10483,insert overwrite partition deadlocks on itself with DbTxnManager,"insert overwrite table ta partition(part=xxxx) select xxx from tb join ta where part=xxxx

It seems like the Shared conflicts with the Exclusive lock for Insert Overwrite even though both are part of the same txn.
More precisely insert overwrite requires X lock on partition and the read side needs an S lock on the query.

A simpler case is
insert overwrite table ta partition(part=xxxx) select * from ta"
HIVE-10456,Grace Hash Join should not load spilled partitions on abort,Grace Hash Join loads the spilled partitions to complete the join in closeOp(). This should not happen when closeOp with abort is invoked. Instead it should clean up all the spilled data.
HIVE-10446,Hybrid Hybrid Grace Hash Join : java.lang.IllegalArgumentException in Kryo while spilling big table,"TPC-DS Q85 fails with Kryo exception when spilling big table data.

Query 
{code}
select  substr(r_reason_desc,1,20) as r
       ,avg(wr_return_ship_cost) wq
       ,avg(wr_refunded_cash) ref
       ,avg(wr_fee) fee
 from web_returns, customer_demographics cd1,
      customer_demographics cd2, customer_address, date_dim, reason 
 where 
   cd1.cd_demo_sk = web_returns.wr_refunded_cdemo_sk 
   and cd2.cd_demo_sk = web_returns.wr_returning_cdemo_sk
   and customer_address.ca_address_sk = web_returns.wr_refunded_addr_sk
   and reason.r_reason_sk = web_returns.wr_reason_sk
   and cd1.cd_marital_status = cd2.cd_marital_status
   and cd1.cd_education_status = cd2.cd_education_status
group by r_reason_desc
order by r, wq, ref, fee
limit 100
{code}

Plan 
{code}
OK
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE), Map 5 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE), Map 7 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
      DagName: mmokhtar_20150422165209_d8eb5634-c19f-4576-9525-cad248c7ca37:5
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: web_returns
                  filterExpr: (((wr_refunded_addr_sk is not null and wr_reason_sk is not null) and wr_refunded_cdemo_sk is not null) and wr_returning_cdemo_sk is not null) (type: boolean)
                  Statistics: Num rows: 2062802370 Data size: 185695406284 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (((wr_refunded_addr_sk is not null and wr_reason_sk is not null) and wr_refunded_cdemo_sk is not null) and wr_returning_cdemo_sk is not null) (type: boolean)
                    Statistics: Num rows: 1875154723 Data size: 51267313780 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: wr_refunded_cdemo_sk (type: int), wr_refunded_addr_sk (type: int), wr_returning_cdemo_sk (type: int), wr_reason_sk (type: int), wr_fee (type: float), wr_return_ship_cost (type: float), wr_refunded_cash (type: float)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                      Statistics: Num rows: 1875154723 Data size: 51267313780 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0, _col2, _col3, _col4, _col5, _col6
                        input vertices:
                          1 Map 4
                        Statistics: Num rows: 1875154688 Data size: 45003712512 Basic stats: COMPLETE Column stats: COMPLETE
                        HybridGraceHashJoin: true
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          keys:
                            0 _col3 (type: int)
                            1 _col0 (type: int)
                          outputColumnNames: _col0, _col2, _col4, _col5, _col6, _col9
                          input vertices:
                            1 Map 5
                          Statistics: Num rows: 1875154688 Data size: 219393098496 Basic stats: COMPLETE Column stats: COMPLETE
                          HybridGraceHashJoin: true
                          Map Join Operator
                            condition map:
                                 Inner Join 0 to 1
                            keys:
                              0 _col0 (type: int)
                              1 _col0 (type: int)
                            outputColumnNames: _col2, _col4, _col5, _col6, _col9, _col11, _col12
                            input vertices:
                              1 Map 6
                            Statistics: Num rows: 1875154688 Data size: 547545168896 Basic stats: COMPLETE Column stats: COMPLETE
                            HybridGraceHashJoin: true
                            Map Join Operator
                              condition map:
                                   Inner Join 0 to 1
                              keys:
                                0 _col2 (type: int), _col11 (type: string), _col12 (type: string)
                                1 _col0 (type: int), _col1 (type: string), _col2 (type: string)
                              outputColumnNames: _col4, _col5, _col6, _col9
                              input vertices:
                                1 Map 7
                              Statistics: Num rows: 402058172 Data size: 43824340748 Basic stats: COMPLETE Column stats: COMPLETE
                              HybridGraceHashJoin: true
                              Select Operator
                                expressions: _col9 (type: string), _col5 (type: float), _col6 (type: float), _col4 (type: float)
                                outputColumnNames: _col0, _col1, _col2, _col3
                                Statistics: Num rows: 402058172 Data size: 43824340748 Basic stats: COMPLETE Column stats: COMPLETE
                                Group By Operator
                                  aggregations: avg(_col1), avg(_col2), avg(_col3)
                                  keys: _col0 (type: string)
                                  mode: hash
                                  outputColumnNames: _col0, _col1, _col2, _col3
                                  Statistics: Num rows: 10975 Data size: 1064575 Basic stats: COMPLETE Column stats: COMPLETE
                                  Reduce Output Operator
                                    key expressions: _col0 (type: string)
                                    sort order: +
                                    Map-reduce partition columns: _col0 (type: string)
                                    Statistics: Num rows: 10975 Data size: 1064575 Basic stats: COMPLETE Column stats: COMPLETE
                                    value expressions: _col1 (type: struct<count:bigint,sum:double,input:float>), _col2 (type: struct<count:bigint,sum:double,input:float>), _col3 (type: struct<count:bigint,sum:double,input:float>)
            Execution mode: vectorized
        Map 4
            Map Operator Tree:
                TableScan
                  alias: customer_address
                  filterExpr: ca_address_sk is not null (type: boolean)
                  Statistics: Num rows: 40000000 Data size: 40595195284 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ca_address_sk is not null (type: boolean)
                    Statistics: Num rows: 40000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ca_address_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 40000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 40000000 Data size: 160000000 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 5
            Map Operator Tree:
                TableScan
                  alias: reason
                  filterExpr: r_reason_sk is not null (type: boolean)
                  Statistics: Num rows: 72 Data size: 14400 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: r_reason_sk is not null (type: boolean)
                    Statistics: Num rows: 72 Data size: 7272 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: r_reason_sk (type: int), r_reason_desc (type: string)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 72 Data size: 7272 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 72 Data size: 7272 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 6
            Map Operator Tree:
                TableScan
                  alias: cd1
                  filterExpr: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                  Statistics: Num rows: 1920800 Data size: 718379200 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                    Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cd_demo_sk (type: int), cd_marital_status (type: string), cd_education_status (type: string)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string), _col2 (type: string)
            Execution mode: vectorized
        Map 7
            Map Operator Tree:
                TableScan
                  alias: cd1
                  filterExpr: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                  Statistics: Num rows: 1920800 Data size: 718379200 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((cd_demo_sk is not null and cd_marital_status is not null) and cd_education_status is not null) (type: boolean)
                    Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: cd_demo_sk (type: int), cd_marital_status (type: string), cd_education_status (type: string)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int), _col1 (type: string), _col2 (type: string)
                        sort order: +++
                        Map-reduce partition columns: _col0 (type: int), _col1 (type: string), _col2 (type: string)
                        Statistics: Num rows: 1920800 Data size: 351506400 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2
            Reduce Operator Tree:
              Group By Operator
                aggregations: avg(VALUE._col0), avg(VALUE._col1), avg(VALUE._col2)
                keys: KEY._col0 (type: string)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 25 Data size: 3025 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: substr(_col0, 1, 20) (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                  Reduce Output Operator
                    key expressions: _col0 (type: string), _col1 (type: double), _col2 (type: double), _col3 (type: double)
                    sort order: ++++
                    Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                    TopN Hash Memory Usage: 0.04
        Reducer 3
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: double), KEY.reducesinkkey2 (type: double), KEY.reducesinkkey3 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 25 Data size: 5200 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink
{code}

Exception 
{code}
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:337)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:179)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:171)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:171)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:167)
	at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:91)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.pushRecord(MapRecordSource.java:68)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.run(MapRecordProcessor.java:290)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:148)
	... 14 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:52)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordSource.processRow(MapRecordSource.java:83)
	... 17 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception: output cannot be null.
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:411)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.process(VectorMapJoinOperator.java:287)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.process(VectorSelectOperator.java:138)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.process(VectorFilterOperator.java:114)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:97)
	at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:162)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:45)
	... 18 more
Caused by: java.lang.IllegalArgumentException: output cannot be null.
	at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:601)
	at org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.add(ObjectContainer.java:101)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.spillBigTableRow(MapJoinOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.spillBigTableRow(VectorMapJoinOperator.java:307)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:390)
	... 27 more
]], Vertex failed as one or more tasks failed. failedTasks:1, Vertex vertex_1426707664723_3652_3_04 [Map 1] killed/failed due to:null]Vertex killed, vertexName=Reducer 3, vertexId=vertex_1426707664723_3652_3_06, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0, Vertex vertex_1426707664723_3652_3_06 [Reducer 3] killed/failed due to:null]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1426707664
{code}"
HIVE-10428,NPE in RegexSerDe using HCat,"When HCatalog calls to table with ""org.apache.hadoop.hive.serde2.RegexSerDe"", when doing Hcatalog call to get read the table, it throws exception:

{noformat}
15/04/21 14:07:31 INFO security.TokenCache: Got dt for hdfs://hdpsecahdfs; Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:hdpsecahdfs, Ident: (HDFS_DELEGATION_TOKEN token 1478 for haha)
15/04/21 14:07:31 INFO mapred.FileInputFormat: Total input paths to process : 1
Splits len : 1
SplitInfo : [hdpseca03.seca.hwxsup.com, hdpseca04.seca.hwxsup.com, hdpseca05.seca.hwxsup.com]
15/04/21 14:07:31 INFO mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.RegexSerDe with properties {name=casetest.regex_table, numFiles=1, columns.types=string,string, serialization.format=1, columns=id,name, rawDataSize=0, numRows=0, output.format.string=%1$s %2$s, serialization.lib=org.apache.hadoop.hive.serde2.RegexSerDe, COLUMN_STATS_ACCURATE=true, totalSize=25, serialization.null.format=\N, input.regex=([^ ]*) ([^ ]*), transient_lastDdlTime=1429590172}
15/04/21 14:07:31 WARN serde2.RegexSerDe: output.format.string has been deprecated
Exception in thread ""main"" java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)
	at com.google.common.base.Splitter.split(Splitter.java:371)
	at org.apache.hadoop.hive.serde2.RegexSerDe.initialize(RegexSerDe.java:155)
	at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:49)
	at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:518)
	at org.apache.hive.hcatalog.mapreduce.InternalUtil.initializeDeserializer(InternalUtil.java:156)
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.createDeserializer(HCatRecordReader.java:127)
	at org.apache.hive.hcatalog.mapreduce.HCatRecordReader.initialize(HCatRecordReader.java:92)
	at HCatalogSQLMR.main(HCatalogSQLMR.java:81)
{noformat}"
HIVE-10411,LLAP: NPE caused by HIVE-10397,Fix NPE caused by HIVE-10397
HIVE-10410,Apparent race condition in HiveServer2 causing intermittent query failures,"On our secure Hadoop cluster, queries submitted to HiveServer2 through JDBC occasionally trigger odd Thrift exceptions with messages such as ""Read a negative frame size (-2147418110)!"" or ""out of sequence response"" in HiveServer2's connections to the metastore. For certain metastore calls (for example, showDatabases), these Thrift exceptions are converted to MetaExceptions in HiveMetaStoreClient, which prevents RetryingMetaStoreClient from retrying these calls and thus causes the failure to bubble out to the JDBC client.

Note that as far as we can tell, this issue appears to only affect queries that are submitted with the runAsync flag on TExecuteStatementReq set to true (which, in practice, seems to mean all JDBC queries), and it appears to only manifest when HiveServer2 is using the new HTTP transport mechanism. When both these conditions hold, we are able to fairly reliably reproduce the issue by spawning about 100 simple, concurrent hive queries (we have been using ""show databases""), two or three of which typically fail. However, when either of these conditions do not hold, we are no longer able to reproduce the issue.

Some example stack traces from the HiveServer2 logs:

{noformat}
2015-04-16 13:54:55,486 ERROR hive.log: Got exception: org.apache.thrift.transport.TTransportException Read a negative frame size (-2147418110)!
org.apache.thrift.transport.TTransportException: Read a negative frame size (-2147418110)!
        at org.apache.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:435)
        at org.apache.thrift.transport.TSaslTransport.read(TSaslTransport.java:414)
        at org.apache.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
        at org.apache.hadoop.hive.thrift.TFilterTransport.readAll(TFilterTransport.java:62)
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_databases(ThriftHiveMetastore.java:600)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_databases(ThriftHiveMetastore.java:587)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:837)
        at org.apache.sentry.binding.metastore.SentryHiveMetaStoreClient.getDatabases(SentryHiveMetaStoreClient.java:60)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)
        at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getDatabasesByPattern(Hive.java:1139)
        at org.apache.hadoop.hive.ql.exec.DDLTask.showDatabases(DDLTask.java:2445)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:364)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1554)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1321)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1139)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:962)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:957)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:145)
        at org.apache.hive.service.cli.operation.SQLOperation.access$000(SQLOperation.java:69)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:213)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

{noformat}

The above exception being converted into a MetaException and re-thrown:

{noformat}
2015-04-16 13:54:55,486 ERROR hive.ql.exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception: org.apache.thrift.transport.TTransportException Read a negative frame size (-2147418110)!)
        at org.apache.hadoop.hive.ql.metadata.Hive.getDatabasesByPattern(Hive.java:1141)
        at org.apache.hadoop.hive.ql.exec.DDLTask.showDatabases(DDLTask.java:2445)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:364)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1554)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1321)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1139)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:962)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:957)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:145)
        at org.apache.hive.service.cli.operation.SQLOperation.access$000(SQLOperation.java:69)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:213)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: MetaException(message:Got exception: org.apache.thrift.transport.TTransportException Read a negative frame size (-2147418110)!)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.logAndThrowMetaException(MetaStoreUtils.java:1116)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:839)
        at org.apache.sentry.binding.metastore.SentryHiveMetaStoreClient.getDatabases(SentryHiveMetaStoreClient.java:60)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)
        at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getDatabasesByPattern(Hive.java:1139)
        ... 22 more

{noformat}

The above MetaException causing the query as a whole to fail:

{noformat}
2015-04-16 13:54:55,486 ERROR org.apache.hive.service.cli.operation.Operation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: org.apache.thrift.transport.TTransportException Read a negative frame size (-2147418110)!)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:148)
        at org.apache.hive.service.cli.operation.SQLOperation.access$000(SQLOperation.java:69)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:213)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

{noformat}

An ""out of sequence response"" that occurred shortly after the above exception and may have been triggered by it:

{noformat}
2015-04-16 13:54:55,498 ERROR hive.log: Got exception: org.apache.thrift.TApplicationException get_databases failed: out of sequence response
org.apache.thrift.TApplicationException: get_databases failed: out of sequence response
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:76)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_databases(ThriftHiveMetastore.java:600)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_databases(ThriftHiveMetastore.java:587)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getDatabases(HiveMetaStoreClient.java:837)
        at org.apache.sentry.binding.metastore.SentryHiveMetaStoreClient.getDatabases(SentryHiveMetaStoreClient.java:60)
        at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:90)
        at com.sun.proxy.$Proxy6.getDatabases(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.getDatabasesByPattern(Hive.java:1139)
        at org.apache.hadoop.hive.ql.exec.DDLTask.showDatabases(DDLTask.java:2445)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:364)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1554)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1321)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1139)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:962)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:957)
        at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:145)
        at org.apache.hive.service.cli.operation.SQLOperation.access$000(SQLOperation.java:69)
        at org.apache.hive.service.cli.operation.SQLOperation$1$1.run(SQLOperation.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)
        at org.apache.hive.service.cli.operation.SQLOperation$1.run(SQLOperation.java:213)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

{noformat}
"
HIVE-10403,Add n-way join support for Hybrid Grace Hash Join,Currently Hybrid Grace Hash Join only supports 2-way join (one big table and one small table). This task will enable n-way join (one big table and multiple small tables).
HIVE-10368,VectorExpressionWriter doesn't match vectorColumn during row spilling in HybridGraceHashJoin,"This problem was exposed by HIVE-10284, when testing vectorized_context.q

Below is the query and backtrace:
{code}
select store.s_city, ss_net_profit
from store_sales
JOIN store ON store_sales.ss_store_sk = store.s_store_sk
JOIN household_demographics ON store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
limit 100
{code}
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector
	at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:175)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.getRowObject(VectorMapJoinOperator.java:347)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.spillBigTableRow(VectorMapJoinOperator.java:306)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.process(MapJoinOperator.java:390)
	... 24 more"
HIVE-10365,First job fails with StackOverflowError [Spark Branch],"When running some queries on Yarn with standalone Hadoop, the first query fails with StackOverflowError:

{noformat}
java.lang.StackOverflowError
	at java.util.concurrent.ConcurrentHashMap.hash(ConcurrentHashMap.java:333)
	at java.util.concurrent.ConcurrentHashMap.putIfAbsent(ConcurrentHashMap.java:1145)
	at java.lang.ClassLoader.getClassLoadingLock(ClassLoader.java:464)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:405)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:412)
{noformat}"
HIVE-10273,Union with partition tables which have no data fails with NPE,"As shown in the test case in the patch below, when we have partitioned tables which have no data, we fail with an NPE with the following stack trace:

{code}
NullPointerException null
java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
  at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:357)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:321)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:307)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:194)
  at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:139)
  at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:847)
  at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:468)
  at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:223)
  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10170)
  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
  at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
{code}"
HIVE-10233,Hive on tez: memory manager for grace hash join,We need a memory manager in llap/tez to manage the usage of memory across threads. 
HIVE-10232,Map join in tez needs to account for memory limits due to other map join operators possible in the same work,There seems to be a regression with respect to MR in terms of allowing multiple map joins in the same task by not accounting for the memory consumed in each of the joins.
HIVE-10225,CLI JLine does not flush history on quit/Ctrl-C,"Hive CLI is not saving history, if hive cli is terminated using a Ctrl-C or ""quit;"".

HIVE-9310 fixed it for the case where one exits with Ctrl-D (EOF), but not for the above ways of exiting.
"
HIVE-10211,Hive2 JDBC connection errors can leak database credential information,"In jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java, the string returned when there is an exception includes the entire JDBC connection string. This can leak credential information if there is a problem like a network error.

In our application (Looker) we have to capture every area where an exception can occur and filter out the password. It would be better if the driver took care of this by replacing the password with something like [FILTERED]. 

Here is an example string:
Java::JavaSql::SQLException: Could not open connection to jdbc:hive2://localhost:21050/;user=test;password=secret: java.net.ConnectException: Connection refused"
HIVE-10174,LLAP: ORC MemoryManager is singleton synchronized,"ORC MemoryManager::addedRow() checks are bad for LLAP multi-threaded performance.

!orc-memorymanager-1.png!
!orc-memorymanager-2.png!"
HIVE-10123,Hybrid grace Hash join : Use estimate key count from stats to initialize BytesBytesMultiHashMap,"Hybrid grace Hash join is not using estimated number of rows from the statistics to initialize BytesBytesMultiHashMap. 

Add some logging to BytesBytesMultiHashMap to track get probes and use msec for expandAndRehash as us overflow."
HIVE-10072,Add vectorization support for Hybrid Grace Hash Join,This task is to enable vectorization support for Hybrid Grace Hash Join feature.
HIVE-10018,Activating SQLStandardAuth results in NPE [hbase-metastore branch],Setting the config to run SQLStandardAuth and then doing even simple SQL statements results in an NPE.
HIVE-10010,Alter table results in NPE [hbase-metastore branch],"Doing an alter table results in:

{code}
2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException
    at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)
    at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)
    at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)
    at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)
    at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)
    at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)
    at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)
{code}"
HIVE-10006,RSC has memory leak while execute multi queries.[Spark Branch],"While execute query with RSC, MapWork/ReduceWork number is increased all the time, and lead to OOM at the end."
HIVE-9989,hive on tez group by NPE,"set hive.execution.engine=tez   run sql throw NPE   when I modify my sql (see my test.sql file line 14) to  cast(NULL AS BIGINT).The 
sql run ok."
HIVE-9985,Vectorization: NPE for added columns in ORC non-partitioned tables,"
If you add STRING columns to a non-partitioned table (ORC format) and try to read the added STRING column using vectorization, you will get a NullPointerException."
HIVE-9976,Possible race condition in DynamicPartitionPruner for <200ms tasks,"Race condition in the DynamicPartitionPruner between DynamicPartitionPruner::processVertex() and DynamicPartitionpruner::addEvent() for tasks which respond with both the result and success in a single heartbeat sequence.

{code}
2015-03-16 07:05:01,589 ERROR [InputInitializer [Map 1] #0] tez.DynamicPartitionPruner: Expecting: 1, received: 0
2015-03-16 07:05:01,590 ERROR [Dispatcher thread: Central] impl.VertexImpl: Vertex Input: store_sales initializer failed, vertex=vertex_1424502260528_1113_4_04 [Map 1]
org.apache.tez.dag.app.dag.impl.AMUserCodeException: org.apache.hadoop.hive.ql.metadata.HiveException: Incorrect event count in dynamic parition pruning
{code}

!llap_vertex_200ms.png!

All 4 upstream vertices of Map 1 need to finish within ~200ms to trigger this, which seems to be consistently happening with LLAP."
HIVE-9893,HiveServer2 java.lang.OutOfMemoryError: Java heap space,"Everything runs but dies after a few days with the Java heap space memory error - and with no activity on cluster. It failed most resently after 5 days.

I tried to fix it after noticing the SLF4J library duplication, wondering if conflicting libraries would be causing the error, but still fails.
Nagios reports HiveServer2 ""Flapping""
HiveServer2.log output:
22488-2015-03-07 22:19:08,359 INFO  [HiveServer2-Handler-Pool: Thread-13139]: thrift.ThriftCLIService (ThriftCLIService.java:OpenSession(232)) - Client protocol version: HIVE_CLI_SERVICE_PROTOCOL_V6
22489-2015-03-07 22:19:19,056 ERROR [HiveServer2-Handler-Pool: Thread-13139]: thrift.ProcessFunction (ProcessFunction.java:process(41)) - Internal error processing OpenSession
22490:java.lang.OutOfMemoryError: Java heap space
22491-2015-03-07 22:19:22,515 INFO  [Thread-6]: server.HiveServer2 (HiveServer2.java:stop(299)) - Shutting down HiveServer2
22492-2015-03-07 22:19:22,516 INFO  [Thread-6]: thrift.ThriftCLIService (ThriftCLIService.java:stop(137)) - Thrift server has stopped
22493-2015-03-07 22:19:22,516 INFO  [Thread-6]: service.AbstractService (AbstractService.java:stop(125)) - Service:ThriftBinaryCLIService is stopped.
22494-2015-03-07 22:19:27,078 INFO  [Thread-6]: service.AbstractService (AbstractService.java:stop(125)) - Service:OperationManager is stopped.
22495-2015-03-07 22:19:27,078 INFO  [Thread-6]: service.AbstractService (AbstractService.java:stop(125)) - Service:SessionManager is stopped.
22496:2015-03-07 22:19:36,096 WARN  [Thread-0]: util.ShutdownHookManager (ShutdownHookManager.java:run(56)) - ShutdownHook 'ClientFinalizer' failed, java.lang.OutOfMemoryError: Java heap space
22497:java.lang.OutOfMemoryError: Java heap space
"
HIVE-9886,Hive on tez: NPE when converting join to SMB in sub-query,"{code}
set hive.auto.convert.sortmerge.join = true;

create table t1(
id string,
od string);

create table t2(
id string,
od string);

select vt1.id from
(select rt1.id from
(select t1.id, row_number() over (partition by id order by od desc) as row_no from t1) rt1
where rt1.row_no=1) vt1
join
(select rt2.id from
(select t2.id, row_number() over (partition by id order by od desc) as row_no from t2) rt2
where rt2.row_no=1) vt2
where vt1.id=vt2.id;
{code}

throws NPE:

{code}
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:162)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.getValueObjectInspectors(AbstractMapJoinOperator.java:96)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:167)
	at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:72)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:89)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:66)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeOp(Operator.java:410)
	at org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:89)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:469)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:425)
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:385)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:116)
	... 14 more
{code}
"
HIVE-9854,OutofMemory while read ORCFile table,"
Log:
Diagnostic Messages for this Task:
Error: java.io.IOException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)
	at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:294)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.<init>(HadoopShimsSecure.java:241)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileInputFormatShim.getRecordReader(HadoopShimsSecure.java:365)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:591)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:166)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:407)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:160)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:155)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.initNextRecordReader(HadoopShimsSecure.java:280)
	... 11 more
Caused by: java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.grow(DynamicByteArray.java:64)
	at org.apache.hadoop.hive.ql.io.orc.DynamicByteArray.readAll(DynamicByteArray.java:142)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringDictionaryTreeReader.startStripe(RecordReaderImpl.java:1547)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StringTreeReader.startStripe(RecordReaderImpl.java:1337)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl$StructTreeReader.startStripe(RecordReaderImpl.java:1825)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:2537)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:2950)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:2992)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:284)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:480)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.createReaderFromFile(OrcInputFormat.java:214)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.<init>(OrcInputFormat.java:146)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:997)
	at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.<init>(CombineHiveRecordReader.java:65)
	... 16 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 105   Cumulative CPU: 656.39 sec   HDFS Read: 4040094761 HDFS Write: 139 FAIL
Total MapReduce CPU Time Spent: 10 minutes 56 seconds 390 msec"
HIVE-9839,HiveServer2 leaks OperationHandle on async queries which fail at compile phase,"Using beeline to connect to HiveServer2.And type the following:
drop table if exists table_not_exists;
select * from table_not_exists;

There will be an OperationHandle object staying in HiveServer2's memory for ever even after quit from beeline .

"
HIVE-9791,insert into table throws NPE,"to reproduce NPE run the following
{code}
create table a as select 'A' letter;
OK

insert into table a select 'B' letter;
FAILED: NullPointerException null

-- works fine if add ""from <table>"" to select statement
insert into table a select 'B' letter from dual;
OK
{code}"
HIVE-9786,CBO (Calcite Return Path): HiveJoinAddNotNullRule causes StackOverflowError [CBO branch],
HIVE-9622,Getting NPE when trying to restart HS2 when metastore is configured to use org.apache.hadoop.hive.thrift.DBTokenStore,"# Configure the cluster to use kerberos for HS2 and Metastore.
## http://www.cloudera.com/content/cloudera/en/documentation/cdh4/v4-3-0/CDH4-Security-Guide/cdh4sg_topic_9_1.html
## http://www.cloudera.com/content/cloudera/en/documentation/cdh4/v4-6-0/CDH4-Security-Guide/cdh4sg_topic_9_2.html
# Set hive metastore delegation token to org.apache.hadoop.hive.thrift.DBTokenStore in hive-site.xml
{code}
<property>
     <name>hive.cluster.delegation.token.store.class</name>
     <value>org.apache.hadoop.hive.thrift.DBTokenStore</value>
</property>
{code}
# Then trying to restart hive service, HS2 fails to start the NPE below: 
{code}
9:43:10.711 AM	ERROR	org.apache.hive.service.cli.thrift.ThriftCLIService	
Error: 
org.apache.thrift.transport.TTransportException: Failed to start token manager
	at org.apache.hive.service.auth.HiveAuthFactory.<init>(HiveAuthFactory.java:107)
	at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:51)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to initialize master key
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads(TokenStoreDelegationTokenSecretManager.java:223)
	at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server.startDelegationTokenSecretManager(HadoopThriftAuthBridge20S.java:438)
	at org.apache.hive.service.auth.HiveAuthFactory.<init>(HiveAuthFactory.java:105)
	... 2 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads(TokenStoreDelegationTokenSecretManager.java:221)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnRawStore(DBTokenStore.java:145)
	at org.apache.hadoop.hive.thrift.DBTokenStore.addMasterKey(DBTokenStore.java:41)
	at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.logUpdateMasterKey(TokenStoreDelegationTokenSecretManager.java:203)
	at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:339)
	... 9 more
9:43:10.719 AM	INFO	org.apache.hive.service.server.HiveServer2	
SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down HiveServer2 at a1909.halxg.cloudera.com/10.20.202.109
************************************************************/
{code}

The problem appears that we didn't pass a {{RawStore}} object in the following:

https://github.com/apache/hive/blob/trunk/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java#L111"
HIVE-9513,NULL POINTER EXCEPTION,"NPE duting parsing  of :

{noformat}
select * from (
     select * from ( select 1 as id , ""foo"" as str_1 from staging.dual ) f
  union	all
     select * from ( select 2 as id , ""bar"" as str_2 from staging.dual ) g
) e ;
{noformat}"
HIVE-9505,"explain extended insert into table foo values(4,5,6) leaks temp tables","do 
{noformat}
show tables;
explain extended insert into table foo values(4,5,6);
show tables;
{noformat}

you'll notice that 2nd 'show tables' will includes something like 'values__tmp__table__19' which is the temp table shown in 'ABSTRACT SYNTAX TREE' portion of the plan

quit and restart Hive CLI and this table disappears as is expected for temp tables."
HIVE-9404,NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct(),"{noformat}
Caused by: java.lang.NullPointerException

	at org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct(TxnHandler.java:1015)

	at org.apache.hadoop.hive.metastore.txn.TxnHandler.checkRetryable(TxnHandler.java:906)

	at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:238)

	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5321)

{noformat}"
HIVE-9377,UDF in_file() in WHERE predicate causes NPE.,"Consider the following query:

{code:sql}
SELECT foo, bar from mythdb.foobar where in_file( bar, '/tmp/bar_list.txt' );
{code}

Using {{in_file()}} in a WHERE predicate causes the following NPE:
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getWritableConstantValue(ObjectInspectorUtils.java:1041)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.getRequiredFiles(GenericUDFInFile.java:93)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.isDeterministicUdf(ConstantPropagateProcFactory.java:303)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.foldExpr(ConstantPropagateProcFactory.java:226)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.access$000(ConstantPropagateProcFactory.java:92)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory$ConstantPropagateFilterProc.process(ConstantPropagateProcFactory.java:623)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate$ConstantPropagateWalker.walk(ConstantPropagate.java:147)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
	at org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform(ConstantPropagate.java:117)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:177)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10032)
	at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:189)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1156)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:701)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:674)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{noformat}
I have a tentative fix I need advice on."
HIVE-9361,Intermittent NPE in SessionHiveMetaStoreClient.alterTempTable,"it's happening at 
{noformat}
    MetaStoreUtils.updateUnpartitionedTableStatsFast(newtCopy,
        wh.getFileStatusesForSD(newtCopy.getSd()), false, true);
{noformat}

other methods in this class call getWh() to get Warehouse so this likely explains why it's intermittent."
HIVE-9342,add num-executors / executor-cores / executor-memory option support for hive on spark in Yarn mode [Spark Branch],"When I run hive on spark with Yarn mode, I want to control some yarn option, such as --num-executors, --executor-cores, --executor-memory.
We can append these options into argv in SparkClientImpl."
HIVE-9331,get rid of pre-optimized-hashtable memory optimizations,"These were added in 13 because optimized hashtable couldn't make it in; they reduced memory usage by some amount (10-25%), and informed the design of the optimized hashtable, but now extra settings and code branches are just confusing and may have their own bugs. Might as well remove them."
HIVE-9310,CLI JLine does not flush history back to ~/.hivehistory,"Hive CLI does not seem to be saving history anymore.

In JLine with the PersistentHistory class, to keep history across sessions, you need to do {{reader.getHistory().flush()}}."
HIVE-9265,Hive with encryption throws NPE to fs path without schema,"Hive throws out NPE exception to fs path without schema (missing hdfs\:) when the encryption is enabled. see exception stacktrace:
{code}
2014-12-31 13:22:01,920 ERROR org.apache.hadoop.hive.ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStagingDirectoryPathname(SemanticAnalyzer.java:1640)
{code}

NO PRECOMMIT TESTS"
HIVE-9234,HiveServer2 leaks FileSystem objects in FileSystem.CACHE,"Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting {{fs.hdfs.impl.disable.cache}} and {{fs.file.impl.disable.cache}} to {{true}}), but creating new FileSystem objects is expensive."
HIVE-9185,NPE in ColumnPruner pruneReduceSinkOperator,"When nm is null reverseLookup(outputCol) is called which also returns null for the outputColumn lookup.

Query:
{code}
SELECT session_aggregate.app_frequency AS number_sessions, COUNT(session_aggregate.s) AS users
FROM (
    SELECT session_info.s, MAX(session_info.session_number) AS app_frequency
    FROM (
        SELECT kt_session(calc_session.s, calc_session.evt_lst,2) AS (s, session_number, session_length)
        FROM (
            SELECT session_set.s, collect_set(session_set.timestamps) evt_lst
            FROM (
                SELECT s, evt.utc_timestamp AS timestamps
                FROM event evt
                WHERE month = 201412
            ) session_set
        GROUP BY session_set.s
        ) calc_session
    ORDER BY s,session_number DESC
    )session_info
GROUP BY session_info.s
)session_aggregate
GROUP BY session_aggregate.app_frequency
{code}


Trace log on explain of query:
{code}
14/12/21 01:10:41 INFO parse.ParseDriver: Parsing command: explain SELECT session_aggregate.app_frequency AS number_sessions, COUNT(session_aggregate.s) AS users
FROM (
    SELECT session_info.s, MAX(session_info.session_number) AS app_frequency
    FROM (
        SELECT kt_session(calc_session.s, calc_session.evt_lst,2) AS (s, session_number, session_length)
        FROM (
            SELECT session_set.s, collect_set(session_set.timestamps) evt_lst
            FROM (
                SELECT s, evt.utc_timestamp AS timestamps
                FROM event evt
                WHERE month = 201412
            ) session_set
        GROUP BY session_set.s
        ) calc_session
    ORDER BY s,session_number DESC
    )session_info
GROUP BY session_info.s
)session_aggregate
GROUP BY session_aggregate.app_frequency
14/12/21 01:10:41 INFO parse.ParseDriver: Parse Completed
14/12/21 01:10:41 INFO log.PerfLogger: </PERFLOG method=parse start=1419124241784 end=1419124241810 duration=26 from=org.apache.hadoop.hive.ql.Driver>
14/12/21 01:10:41 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
14/12/21 01:10:41 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: collect_set
14/12/21 01:10:41 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: kt_session
14/12/21 01:10:41 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: MAX
14/12/21 01:10:41 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: COUNT
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Completed phase 1 of Semantic Analysis
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for source tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for subqueries
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for source tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for subqueries
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for source tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for subqueries
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for source tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for subqueries
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for source tables
14/12/21 01:10:41 WARN conf.Configuration: org.apache.hadoop.hive.conf.LoopingByteArrayInputStream@52b84700:an attempt to override final parameter: mapred.reduce.tasks.speculative.execution;  Ignoring.
14/12/21 01:10:41 DEBUG hive.log: DDL: struct event { i64 utc_timestamp, string type, string s, string n, i32 v, i32 l, string st1, string st2, string st3, string json_data, string ts}
14/12/21 01:10:41 DEBUG hive.log: DDL: struct event { i64 utc_timestamp, string type, string s, string n, i32 v, i32 l, string st1, string st2, string st3, string json_data, string ts}
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for subqueries
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for destination tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for destination tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for destination tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for destination tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Get metadata for destination tables
14/12/21 01:10:41 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
14/12/21 01:10:42 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: collect_set
14/12/21 01:10:42 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: MAX
14/12/21 01:10:42 DEBUG exec.FunctionRegistry: Looking up GenericUDAF: COUNT
14/12/21 01:10:42 DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[_col0, _col1] columnTypes=[int, bigint] separator=[[B@45045faa] nullstring=\N lastColumnTakesRest=false
14/12/21 01:10:42 DEBUG lazy.LazySimpleSerDe: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[_col0, _col1] columnTypes=[int, bigint] separator=[[B@9fa27e9] nullstring=\N lastColumnTakesRest=false
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for FS(22)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(21)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for GBY(20)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for RS(19)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for GBY(18)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(17)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(16)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for GBY(15)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for RS(14)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for GBY(13)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(12)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for EX(11)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for RS(10)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for UDTF(9)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(8)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(7)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for GBY(6)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for RS(5)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for GBY(4)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(3)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for SEL(2)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for FIL(1)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Pushdown Predicates of FIL For Alias : evt
14/12/21 01:10:42 INFO ppd.OpProcFactory: 	(month = 201412)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Processing for TS(0)
14/12/21 01:10:42 INFO ppd.OpProcFactory: Pushdown Predicates of TS For Alias : evt
14/12/21 01:10:42 INFO ppd.OpProcFactory: 	(month = 201412)
14/12/21 01:10:42 TRACE ppr.PartitionPruner: Started pruning partiton
14/12/21 01:10:42 TRACE ppr.PartitionPruner: dbname = default
14/12/21 01:10:42 TRACE ppr.PartitionPruner: tabname = event
14/12/21 01:10:42 TRACE ppr.PartitionPruner: prune Expression = GenericUDFOPEqual(Column[month], Const int 201412)
14/12/21 01:10:42 DEBUG ppr.PartitionPruner: tabname = event is partitioned
14/12/21 01:10:42 DEBUG ppr.PartitionPruner: Filter w/ compacting: (month = 201412); filter w/o compacting: (month = 201412)
14/12/21 01:10:42 INFO log.PerfLogger: <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
14/12/21 01:10:42 INFO log.PerfLogger: </PERFLOG method=partition-retrieving start=1419124242223 end=1419124242320 duration=97 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
14/12/21 01:10:42 DEBUG hive.log: DDL: struct event { i64 utc_timestamp, string type, string s, string n, i32 v, i32 l, string st1, string st2, string st3, string json_data, string ts}
14/12/21 01:10:42 INFO optimizer.ColumnPrunerProcFactory: RS 19 oldColExprMap: {KEY._col0=Column[_col0], VALUE._col0=Column[_col1]}
14/12/21 01:10:42 INFO optimizer.ColumnPrunerProcFactory: RS 19 newColExprMap: {KEY._col0=Column[_col0], VALUE._col0=Column[_col1]}
14/12/21 01:10:42 INFO optimizer.ColumnPrunerProcFactory: RS 14 oldColExprMap: {KEY._col0=Column[_col0], VALUE._col0=Column[_col1]}
14/12/21 01:10:42 INFO optimizer.ColumnPrunerProcFactory: RS 14 newColExprMap: {KEY._col0=Column[_col0], VALUE._col0=Column[_col1]}
14/12/21 01:10:42 INFO optimizer.ColumnPrunerProcFactory: RS 10 oldColExprMap: {_col2=Column[sessionlength], _col1=Column[sessionid], _col0=Column[s]}
FAILED: NullPointerException null
14/12/21 01:10:42 ERROR ql.Driver: FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.pruneReduceSinkOperator(ColumnPrunerProcFactory.java:715)
	at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.access$200(ColumnPrunerProcFactory.java:78)
	at org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory$ColumnPrunerReduceSinkProc.process(ColumnPrunerProcFactory.java:427)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)
	at org.apache.hadoop.hive.ql.optimizer.ColumnPruner$ColumnPrunerWalker.walk(ColumnPruner.java:166)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
	at org.apache.hadoop.hive.ql.optimizer.ColumnPruner.transform(ColumnPruner.java:129)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:146)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9268)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:64)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:327)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:422)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:322)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:975)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1040)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
{code}"
HIVE-9123,Query with join fails with NPE when using join auto conversion,"I have two simple tables:

desc kgorlo_comm;
| col_name  | data_type  | comment  |
| id        | bigint     |          |
| dest_id   | bigint     |          |

desc kgorlo_log; 
| col_name  | data_type  | comment  |
| id        | bigint     |          |
| dest_id   | bigint     |          |
| tstamp    | bigint     |          |

With data:

select * from kgorlo_comm; 
| kgorlo_comm.id  | kgorlo_comm.dest_id  |
| 1               | 2                    |
| 2               | 1                    |
| 1               | 3                    |
| 2               | 3                    |
| 3               | 5                    |
| 4               | 5                    |

select * from kgorlo_log; 
| kgorlo_log.id  | kgorlo_log.dest_id  | kgorlo_log.tstamp  |
| 1              | 2                   | 0                  |
| 1              | 3                   | 0                  |
| 1              | 5                   | 0                  |
| 3              | 1                   | 0                  |

Following query fails in second stage of execution:

bq. select v.id, v.dest_id from kgorlo_log v join (select id, dest_id, count(*) as wiad from kgorlo_comm group by id, dest_id)com1 on com1.id=v.id and com1.dest_id=v.dest_id;

with following exception:

{quote}
  2014-12-16 17:09:17,629 ERROR [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.MapJoinOperator: Unxpected exception: null
  java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.getRefKey(MapJoinOperator.java:198)
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.computeMapJoinKey(MapJoinOperator.java:186)
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:216)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:540)
  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:295)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:181)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:224)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
  at java.util.concurrent.FutureTask.run(FutureTask.java:262)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
  2014-12-16 17:09:17,659 FATAL [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""_col0"":1,""_col1"":2}
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:550)
  at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
  at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
  at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
  at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:295)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:181)
  at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:224)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
  at java.util.concurrent.FutureTask.run(FutureTask.java:262)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
  Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unxpected exception: null
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:254)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:540)
  ... 13 more
  Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.getRefKey(MapJoinOperator.java:198)
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.computeMapJoinKey(MapJoinOperator.java:186)
  at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:216)
  ... 17 more
{quote}

When I set hive.auto.convert.join=false everything works.

Here are explains with this variable turned off and on:

https://gist.github.com/kgs/20db747c8d81d94ac20e
https://gist.github.com/kgs/63bc1fc148354b98a63e"
HIVE-9111,Potential NPE in OrcStruct for list and map types,Currently getters in OrcStruct class for list and map object inspectors does not have null checks which may throw NPE when UDFs like size() is used on list or map column.
HIVE-9073,NPE when using custom windowing UDAFs,"From the hive-user email group:

{noformat}
While executing a simple select query using a custom windowing UDAF I created I am constantly running into this error.
 
Error: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:409)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
        ... 9 more
Caused by: java.lang.RuntimeException: Reduce operator initialization failed
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:173)
        ... 14 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:647)
        at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getWindowFunctionInfo(FunctionRegistry.java:1875)
        at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.streamingPossible(WindowingTableFunction.java:150)
        at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.setCanAcceptInputAsStream(WindowingTableFunction.java:221)
        at org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.initializeStreaming(WindowingTableFunction.java:266)
        at org.apache.hadoop.hive.ql.exec.PTFOperator$PTFInvocation.initializeStreaming(PTFOperator.java:292)
        at org.apache.hadoop.hive.ql.exec.PTFOperator.initializeOp(PTFOperator.java:86)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
        at org.apache.hadoop.hive.ql.exec.ExtractOperator.initializeOp(ExtractOperator.java:40)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
        at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:166)
        ... 14 more
 
Just wanted to check if any of you have faced this earlier. Also when I try to run the Custom UDAF on another server it works fine. The only difference I can see it that the hive version I am using on my local machine is 0.13.1 where it is working and on the other machine it is 0.13.0 where I see the above mentioned error. I am not sure if this was a bug which was fixed in the later release but I just wanted to confirm the same.
{noformat}"
HIVE-9047,NULL POINTER EXCEPTION,"NPE during parsing .

{noformat}
insert into table foobar
               select cast ( t.begindate as date ) as begindt , cast ( t.enddate as date ) as enddt ,
                   datediff ( cast ( t.enddate as date ) , cast ( t.begindate as date ) ) as diffday
                   t.iduser from foobaractions t ;
{noformat}
"
HIVE-8943,Fix memory limit check for combine nested mapjoins [Spark Branch],"Its the opposite problem of what we thought in HIVE-8701.

SparkMapJoinOptimizer does combine nested mapjoins into one work due to removal of RS for big-table.  So we need to enhance the check to calculate if all the MapJoins in that work (spark-stage) will fit into the memory, otherwise it might overwhelm memory for that particular spark executor."
HIVE-8879,Upgrade derby version to address race candition,"DERBY-4160 describes the race condition that i sometimes notice.. particularly on windows. Below is the stack trace

Error Message
{code}

java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient

{code}
Stacktrace
{code}
java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1449)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	... 29 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	... 34 more
Caused by: javax.jdo.JDODataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
NestedThrowables:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	... 39 more
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	... 57 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 80 more
Standard Output

2014-11-14 12:56:43,043 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:118)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 65 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:118)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
2014-11-14 12:56:43,047 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,048 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,089 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,089 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,128 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,128 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,162 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MTableColumnStatistics and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:119)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 65 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:119)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
2014-11-14 12:56:43,165 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,165 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,203 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,203 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,240 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,242 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,278 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:120)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 65 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:120)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
2014-11-14 12:56:43,281 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing
2014-11-14 12:56:43,295 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 67 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
2014-11-14 12:56:43,298 (main) [WARN - org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6570)] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
2014-11-14 12:56:43,311 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 67 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
2014-11-14 12:56:43,328 (main) [WARN - org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:589)] Retrying creating default database after error: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
javax.jdo.JDODataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
NestedThrowablesStackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 80 more
2014-11-14 12:56:43,382 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:118)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 65 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:118)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
2014-11-14 12:56:43,384 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,384 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,423 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,424 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,459 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,459 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,500 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MTableColumnStatistics and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:119)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 65 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:119)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
2014-11-14 12:56:43,501 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,502 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,541 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,542 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,589 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,589 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
2014-11-14 12:56:43,633 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:120)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 65 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.<init>(MetaStoreDirectSql.java:120)
	at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270)
	at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233)
	at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:56)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 73 more
2014-11-14 12:56:43,637 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing
2014-11-14 12:56:43,647 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 67 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
2014-11-14 12:56:43,649 (main) [WARN - org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6570)] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.0
2014-11-14 12:56:43,662 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidates
Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception ""Add classes to Catalog """", Schema ""APP""""
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	... 67 more
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
Nested Throwables StackTrace:
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source)
	at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947)
	at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1744)
	at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672)
	at org.datanucleus.store.query.Query.execute(Query.java:1654)
	at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221)
	at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at $Proxy12.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	at org.apache.flume.sink.hive.TestHiveWriter.<init>(TestHiveWriter.java:96)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 75 more
{code}"
HIVE-8865,Needs to set hashTableMemoryUsage for MapJoinDesc [Spark Branch],"If this part is not done, hashTableMemoryUsage is always 0.0, which will cause MapJoinMemoryExhaustionException."
HIVE-8811,Dynamic partition pruning can result in NPE during query compilation,Bug in tarjan's algo results in incorrect strongly connected components. I've seen this manifest itself as an NPE in TezCompiler.
HIVE-8798,Some Oracle deadlocks not being caught in TxnHandler,Oracle seems to give different error codes and different error messages at different times for deadlocks.  There are still some error codes/messages we are missing in TxnHandler.
HIVE-8794,Hive on Tez leaks AMs when killed before first dag is run,The shutdown hook that guards against this kind of leakage is only set up when the TezJobMonitor class is loaded. If you kill the shell before that - that might be too late.
HIVE-8778,ORC split elimination can cause NPE when column statistics is null,Row group elimination has protection for NULL statistics values in RecordReaderImpl.evaluatePredicate() which then calls evaluatePredicateRange(). But split elimination directly calls evaluatePredicateRange() without NULL protection. This can lead to NullPointerException when a column is NULL in entire stripe. 
HIVE-8747,Estimate number of rows for table with 0 rows overflows resulting in an in-efficient plan ,"ship_mode table has 0 rows.

Query 
{code}
select count(*) 
from
          web_sales
         ,date_dim
 	  ,ship_mode
     where
 web_sales.ws_sold_date_sk = date_dim.d_date_sk
 	and web_sales.ws_ship_mode_sk = ship_mode.sm_ship_mode_sk
        and d_year = 2002
 	and sm_carrier in ('DIAMOND','AIRBORNE')
{code}

Explain 
{code}
STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 <- Map 4 (BROADCAST_EDGE)
        Map 4 <- Map 3 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
      DagName: mmokhtar_20141105180404_59e6fb65-529f-4eaa-9446-7f34d12bffac:30
      Vertices:
        Map 1
            Map Operator Tree:
                TableScan
                  alias: ship_mode
                  filterExpr: ((sm_carrier) IN ('DIAMOND', 'AIRBORNE') and sm_ship_mode_sk is not null) (type: boolean)
                  Statistics: Num rows: 0 Data size: 45 Basic stats: PARTIAL Column stats: COMPLETE
                  Filter Operator
                    predicate: ((sm_carrier) IN ('DIAMOND', 'AIRBORNE') and sm_ship_mode_sk is not null) (type: boolean)
                    Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: sm_ship_mode_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775807 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0
                          1
                        keys:
                          0 _col1 (type: int)
                          1 _col0 (type: int)
                        input vertices:
                          0 Map 4
                        Statistics: Num rows: 9223372036854775807 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                        Select Operator
                          Statistics: Num rows: 9223372036854775807 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                          Group By Operator
                            aggregations: count()
                            mode: hash
                            outputColumnNames: _col0
                            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                            Reduce Output Operator
                              sort order:
                              Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                              value expressions: _col0 (type: bigint)
            Execution mode: vectorized
        Map 3
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((d_year = 2002) and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 652 Data size: 5216 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 652 Data size: 2608 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 652 Data size: 2608 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Map 4
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: (ws_sold_date_sk is not null and ws_ship_mode_sk is not null) (type: boolean)
                  Statistics: Num rows: 143966864 Data size: 19577477788 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (ws_sold_date_sk is not null and ws_ship_mode_sk is not null) (type: boolean)
                    Statistics: Num rows: 143948856 Data size: 1151518824 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_sold_date_sk (type: int), ws_ship_mode_sk (type: int)
                      outputColumnNames: _col0, _col1
                      Statistics: Num rows: 143948856 Data size: 1151518824 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0 {_col1}
                          1
                        keys:
                          0 _col0 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col1
                        input vertices:
                          1 Map 3
                        Statistics: Num rows: 1284818 Data size: 5139272 Basic stats: COMPLETE Column stats: COMPLETE
                        Reduce Output Operator
                          key expressions: _col1 (type: int)
                          sort order: +
                          Map-reduce partition columns: _col1 (type: int)
                          Statistics: Num rows: 1284818 Data size: 5139272 Basic stats: COMPLETE Column stats: COMPLETE
            Execution mode: vectorized
        Reducer 2
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                Select Operator
                  expressions: _col0 (type: bigint)
                  outputColumnNames: _col0
                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
{code}"
HIVE-8738,Potential file descriptor leak at mr.HashTableLoader.load() method,"The following code is faulty:
{code}
        ObjectInputStream in = new ObjectInputStream(new BufferedInputStream(
            new FileInputStream(path.toUri().getPath()), 4096));
        try{
          mapJoinTables[pos] = mapJoinTableSerdes[pos].load(in);
        } finally {
          in.close();
        }
{code}

If an exception is thrown from any of the stream constructors, then the wrapped stream will not get closed."
HIVE-8711,"DB deadlocks not handled in TxnHandler for Postgres, Oracle, and SQLServer","TxnHandler.detectDeadlock has code to catch deadlocks in MySQL and Derby.  But it does not detect a deadlock for Postgres, Oracle, or SQLServer"
HIVE-8671,Overflow in estimate row count and data size with fetch column stats,"Overflow in row counts and data size for several TPC-DS queries.
Interestingly the operators which have overflow end up running with a small parallelism.

For instance Reducer 2 has an overflow but it only runs with parallelism of 2.
{code}
       Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string), KEY._col3 (type: string), KEY._col4 (type: float)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775341 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col3 (type: string), _col3 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col3 (type: string)
                  Statistics: Num rows: 9223372036854775807 Data size: 9223372036854775341 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float), _col5 (type: double)
            Execution mode: vectorized
{code}

{code}
VERTEX       TOTAL_TASKS    DURATION_SECONDS     CPU_TIME_MILLIS INPUT_RECORDS   OUTPUT_RECORDS 
Map 1                 62               26.41           1,779,510   211,978,502       60,628,390
Map 5                  1                4.28               6,950       138,098          138,098
Map 6                  1                2.44               3,910            31               31
Reducer 2              2               22.69              61,320    60,628,390           69,182
Reducer 3              1                2.63               3,910        69,182              100
Reducer 4              1                1.01               1,180           100              100
{code}

Query
{code}
explain  
select  i_item_desc 
      ,i_category 
      ,i_class 
      ,i_current_price
      ,i_item_id
      ,sum(ws_ext_sales_price) as itemrevenue 
      ,sum(ws_ext_sales_price)*100/sum(sum(ws_ext_sales_price)) over
          (partition by i_class) as revenueratio
from	
	web_sales
    	,item 
    	,date_dim
where 
	web_sales.ws_item_sk = item.i_item_sk 
  	and item.i_category in ('Jewelry', 'Sports', 'Books')
  	and web_sales.ws_sold_date_sk = date_dim.d_date_sk
	and date_dim.d_date between '2001-01-12' and '2001-02-11'
group by 
	i_item_id
        ,i_item_desc 
        ,i_category
        ,i_class
        ,i_current_price
order by 
	i_category
        ,i_class
        ,i_item_id
        ,i_item_desc
        ,revenueratio
limit 100
{code}

Explain 
{code}
STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Map 1 <- Map 5 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE)
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
        Reducer 4 <- Reducer 3 (SIMPLE_EDGE)
      DagName: mmokhtar_20141019164343_854cb757-01bd-40cb-843e-9ada7c5e6f38:1
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: web_sales
                  filterExpr: ws_item_sk is not null (type: boolean)
                  Statistics: Num rows: 21594638446 Data size: 2850189889652 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ws_item_sk is not null (type: boolean)
                    Statistics: Num rows: 21594638446 Data size: 172746300152 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: ws_item_sk (type: int), ws_ext_sales_price (type: float), ws_sold_date_sk (type: int)
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 21594638446 Data size: 172746300152 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0 {_col0} {_col1}
                          1 
                        keys:
                          0 _col2 (type: int)
                          1 _col0 (type: int)
                        outputColumnNames: _col0, _col1
                        input vertices:
                          1 Map 6
                        Statistics: Num rows: 24145061366 Data size: 193160490928 Basic stats: COMPLETE Column stats: COMPLETE
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          condition expressions:
                            0 {_col1}
                            1 {_col1} {_col2} {_col3} {_col4} {_col5}
                          keys:
                            0 _col0 (type: int)
                            1 _col0 (type: int)
                          outputColumnNames: _col1, _col6, _col7, _col8, _col9, _col10
                          input vertices:
                            1 Map 5
                          Statistics: Num rows: 25381041158 Data size: 11929089344260 Basic stats: COMPLETE Column stats: COMPLETE
                          Select Operator
                            expressions: _col6 (type: string), _col7 (type: string), _col10 (type: string), _col9 (type: string), _col8 (type: float), _col1 (type: float)
                            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                            Statistics: Num rows: 25381041158 Data size: 11929089344260 Basic stats: COMPLETE Column stats: COMPLETE
                            Group By Operator
                              aggregations: sum(_col5)
                              keys: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float)
                              mode: hash
                              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                              Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                              Reduce Output Operator
                                key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float)
                                sort order: +++++
                                Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float)
                                Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                                value expressions: _col5 (type: double)
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: item
                  filterExpr: ((i_category) IN ('Jewelry', 'Sports', 'Books') and i_item_sk is not null) (type: boolean)
                  Statistics: Num rows: 462000 Data size: 663862160 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((i_category) IN ('Jewelry', 'Sports', 'Books') and i_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 231000 Data size: 109491664 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: i_item_sk (type: int), i_item_id (type: string), i_item_desc (type: string), i_current_price (type: float), i_class (type: string), i_category (type: string)
                      outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                      Statistics: Num rows: 231000 Data size: 109491664 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 231000 Data size: 109491664 Basic stats: COMPLETE Column stats: COMPLETE
                        value expressions: _col1 (type: string), _col2 (type: string), _col3 (type: float), _col4 (type: string), _col5 (type: string)
            Execution mode: vectorized
        Map 6 
            Map Operator Tree:
                TableScan
                  alias: date_dim
                  filterExpr: (d_date BETWEEN '2001-01-12' AND '2001-02-11' and d_date_sk is not null) (type: boolean)
                  Statistics: Num rows: 73049 Data size: 81741831 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: (d_date BETWEEN '2001-01-12' AND '2001-02-11' and d_date_sk is not null) (type: boolean)
                    Statistics: Num rows: 36524 Data size: 3579352 Basic stats: COMPLETE Column stats: COMPLETE
                    Select Operator
                      expressions: d_date_sk (type: int)
                      outputColumnNames: _col0
                      Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Statistics: Num rows: 36524 Data size: 146096 Basic stats: COMPLETE Column stats: COMPLETE
                      Select Operator
                        expressions: _col0 (type: int)
                        outputColumnNames: _col0
                        Statistics: Num rows: 36524 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                        Group By Operator
                          keys: _col0 (type: int)
                          mode: hash
                          outputColumnNames: _col0
                          Statistics: Num rows: 36524 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                          Dynamic Partitioning Event Operator
                            Target Input: web_sales
                            Partition key expr: ws_sold_date_sk
                            Statistics: Num rows: 36524 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
                            Target column: ws_sold_date_sk
                            Target Vertex: Map 1
            Execution mode: vectorized
        Reducer 2 
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0)
                keys: KEY._col0 (type: string), KEY._col1 (type: string), KEY._col2 (type: string), KEY._col3 (type: string), KEY._col4 (type: float)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
                Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                Reduce Output Operator
                  key expressions: _col3 (type: string), _col3 (type: string)
                  sort order: ++
                  Map-reduce partition columns: _col3 (type: string)
                  Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float), _col5 (type: double)
            Execution mode: vectorized
        Reducer 3 
            Reduce Operator Tree:
              Extract
                Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                PTF Operator
                  Statistics: Num rows: 119291 Data size: 1908656 Basic stats: COMPLETE Column stats: COMPLETE
                  Select Operator
                    expressions: _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: float), _col0 (type: string), _col5 (type: double), ((_col5 * 100.0) / _wcol0) (type: double)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                    Reduce Output Operator
                      key expressions: _col1 (type: string), _col2 (type: string), _col4 (type: string), _col0 (type: string), _col6 (type: double)
                      sort order: +++++
                      Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                      TopN Hash Memory Usage: 0.04
                      value expressions: _col3 (type: float), _col5 (type: double)
        Reducer 4 
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey3 (type: string), KEY.reducesinkkey0 (type: string), KEY.reducesinkkey1 (type: string), VALUE._col0 (type: float), KEY.reducesinkkey2 (type: string), VALUE._col1 (type: double), KEY.reducesinkkey4 (type: double)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 119291 Data size: 954328 Basic stats: COMPLETE Column stats: COMPLETE
                Limit
                  Number of rows: 100
                  Statistics: Num rows: 100 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 100 Data size: 800 Basic stats: COMPLETE Column stats: COMPLETE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized

  Stage: Stage-0
    Fetch Operator
      limit: 100
      Processor Tree:
        ListSink

{code}"
HIVE-8628,NPE in case of shuffle join in tez,"test throws NullPointerException:
{noformat}
Vertex failed, vertexName=Reducer 2, vertexId=vertex_1413774081318_0803_5_03, diagnostics=[Task failed, taskId=task_1413774081318_0803_5_03_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
], TaskAttempt 1 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
], TaskAttempt 2 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
], TaskAttempt 3 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:187)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:142)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)
	at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: Hive Runtime Error while closing operators: null
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:218)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:178)
	... 13 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.joinFinalLeftData(CommonMergeJoinOperator.java:368)
	at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.closeOp(CommonMergeJoinOperator.java:310)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:582)
	at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.close(ReduceRecordProcessor.java:200)
	... 14 more
]], Vertex failed as one or more tasks failed. failedTasks:1]
Vertex killed, vertexName=Reducer 3, vertexId=vertex_1413774081318_0803_5_04, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0]
DAG failed due to vertex failure. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
{noformat}

{code:sql}
set hive.auto.convert.join=false;
select c.c_first_name, c.c_last_name, cd.cd_gender, hd.hd_buy_potential from customer c left outer join customer_demographics cd on cd.cd_demo_sk = c.c_current_cdemo_sk left outer join household_demographics hd on hd.hd_demo_sk = c.c_current_hdemo_sk where c.c_customer_sk < 1000;
{code}

Plan (auto.convert.join=false, vectorization on, cbo on,execution.engine=tez)
{noformat}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 4 (SIMPLE_EDGE)
        Reducer 3 <- Map 5 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)
      DagName: hrt_qa_20141020210707_53fdc731-2d96-455e-8c20-ce7fec75f01f:6
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: c
                  filterExpr: (c_customer_sk < 1000) (type: boolean)
                  Statistics: Num rows: 100000 Data size: 4679516 Basic stats: COMPLETE Column stats: NONE
                  Filter Operator
                    predicate: (c_customer_sk < 1000) (type: boolean)
                    Statistics: Num rows: 33333 Data size: 1559823 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: c_current_cdemo_sk (type: int), c_current_hdemo_sk (type: int), c_first_name (type: string), c_last_name (type: string)
                      outputColumnNames: _col1, _col2, _col3, _col4
                      Statistics: Num rows: 33333 Data size: 1559823 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col1 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col1 (type: int)
                        Statistics: Num rows: 33333 Data size: 1559823 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col2 (type: int), _col3 (type: string), _col4 (type: string)
            Execution mode: vectorized
        Map 4 
            Map Operator Tree:
                TableScan
                  alias: cd
                  Statistics: Num rows: 1920800 Data size: 5893494 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: cd_demo_sk (type: int), cd_gender (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 1920800 Data size: 5893494 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 1920800 Data size: 5893494 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col1 (type: string)
            Execution mode: vectorized
        Map 5 
            Map Operator Tree:
                TableScan
                  alias: hd
                  Statistics: Num rows: 7200 Data size: 840 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: hd_demo_sk (type: int), hd_buy_potential (type: string)
                    outputColumnNames: _col0, _col1
                    Statistics: Num rows: 7200 Data size: 840 Basic stats: COMPLETE Column stats: NONE
                    Reduce Output Operator
                      key expressions: _col0 (type: int)
                      sort order: +
                      Map-reduce partition columns: _col0 (type: int)
                      Statistics: Num rows: 7200 Data size: 840 Basic stats: COMPLETE Column stats: NONE
                      value expressions: _col1 (type: string)
            Execution mode: vectorized
        Reducer 2 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Right Outer Join0 to 1
                condition expressions:
                  0 {VALUE._col0}
                  1 {VALUE._col1} {VALUE._col2} {VALUE._col3}
                outputColumnNames: _col1, _col4, _col5, _col6
                Statistics: Num rows: 2112880 Data size: 6482843 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col4 (type: int)
                  sort order: +
                  Map-reduce partition columns: _col4 (type: int)
                  Statistics: Num rows: 2112880 Data size: 6482843 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: string), _col5 (type: string), _col6 (type: string)
        Reducer 3 
            Reduce Operator Tree:
              Merge Join Operator
                condition map:
                     Left Outer Join0 to 1
                condition expressions:
                  0 {VALUE._col1} {VALUE._col4} {VALUE._col5}
                  1 {VALUE._col0}
                outputColumnNames: _col1, _col5, _col6, _col8
                Statistics: Num rows: 2324168 Data size: 7131127 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col5 (type: string), _col6 (type: string), _col1 (type: string), _col8 (type: string)
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Statistics: Num rows: 2324168 Data size: 7131127 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 2324168 Data size: 7131127 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink
{noformat}"
HIVE-8606,[hs2] Do not unnecessarily call setPermission on staging directories,"HS2 has made setPermission mandatory within its CLIService#setupStagingDir method as a result of HIVE-6602.

This causes HS2 to fail to start if the owner of the staging directory is not the same user as it, even though the directory is already 777. This is because only owners and superusers of a directory can change its permission, not group or others.

Failure appears as:

{code}
Caused by: org.apache.hive.service.ServiceException: Error setting stage directories 
at org.apache.hive.service.cli.CLIService.start(CLIService.java:132) 
at org.apache.hive.service.CompositeService.start(CompositeService.java:70) 
... 8 more 
Caused by: org.apache.hadoop.security.AccessControlException: Permission denied 
{code}

We should only call the setPermission if it is unsatisfactory."
HIVE-8549,NPE in PK-FK inference when one side of join is complex tree,"HIVE-8168 added PK-FK inference from column stats. But when one side of join is complex tree which propagates FK, relationship inference fails with NPE.

{code}
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.getSelectivity(StatsRulesProcFactory.java:1293)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.inferPKFKRelationship(StatsRulesProcFactory.java:1250)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$JoinStatsRule.process(StatsRulesProcFactory.java:1067)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)
        at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:78)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsAnnotation(TezCompiler.java:248)
        at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:120)
        at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:99)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10039)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
        at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303)
        at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1129)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:994)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)
        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)

{code}"
HIVE-8400,Fix building and packaging hwi war file,"hive 0.11 used to have hwi-0.11.war file but it seems that hive 13 is not configured to build a war file, instead it builds a jar file for hwi.

A fix for this would be to change jar to war in hwi/pom.xml.
Diff is below : 
diff --git a/hwi/pom.xml b/hwi/pom.xml
index 35c124b..88d83fb 100644
--- a/hwi/pom.xml
+++ b/hwi/pom.xml
@@ -24,7 +24,7 @@
   </parent>
 
   <artifactId>hive-hwi</artifactId>
-  <packaging>jar</packaging>
+  <packaging>war</packaging>
   <name>Hive HWI</name>

Please let me know if jar was intended or is it actually a bug so that I can submit a patch for the fix."
HIVE-8378,NPE in TezTask due to null counters,The counters variable iterated over on line 177 can be null apparently.
HIVE-8372,Potential NPE in Tez MergeFileRecordProcessor,MergeFileRecordProcessor retrieves map work from cache. This map work can be instance of merge file work. When the merge file work already exists in the cache casting the map work to merge file work is missing which will result in NullPointerException.
HIVE-8332,Reading an ACID table with vectorization on results in NPE,"On a transactional table, insert some data, then with vectorization turned on do a select.  The result is:
{code}
Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$1.getObjectInspector(OrcInputFormat.java:1137) at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowReader.<init>(VectorizedOrcAcidRowReader.java:61) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1041) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:246)
	... 25 more
{code}"
HIVE-8325,NPE in map join execution (submit via child),"Exec context doesn't get created in this code path.

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeInProcess(MapredLocalTask.java:310)
	at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
Execution failed with exit status: 1
Obtaining error information

Task failed!
{noformat}"
HIVE-8203,ACID operations result in NPE when run through HS2,"When accessing Hive via HS2, any operation requiring the DbTxnManager results in an NPE."
HIVE-8156,Vectorized reducers need to avoid memory build-up during a single key,"When encountering a skewed key with a large number of values, the vectorized reducer will not release memory within the loop."
HIVE-8104,Insert statements against ACID tables NPE when vectorization is on,Doing an insert against a table that is using ACID format with the transaction manager set to DbTxnManager and vectorization turned on results in an NPE.  
HIVE-8090,Potential null pointer reference in WriterImpl#StreamFactory#createStream(),"{code}
      switch (kind) {
...
      default:
        modifiers = null;
        break;
      }

      BufferedStream result = streams.get(name);
      if (result == null) {
        result = new BufferedStream(name.toString(), bufferSize,
            codec == null ? codec : codec.modify(modifiers));
{code}
In case modifiers is null and codec is ZlibCodec, there would be NPE in ZlibCodec#modify(EnumSet<Modifier> modifiers) :
{code}
    for (Modifier m : modifiers) {
{code}"
HIVE-8078,ORC Delta encoding corrupts data when delta overflows long,"There is an issue with the integer encoding that can cause corruption in certain cases.
The following 3 longs cause this failure.
4513343538618202711
2911390882471569739
-9181829309989854913

I believe that even though the numbers are in decreasing order, the delta between the last two numbers overflows causing a positive delta, in this case the last digit ends up being corrupted (the delta is applied for the wrong sign resulting in -3442132998776557225 instead of -9181829309989854913.
"
HIVE-8008,NPE while reading null decimal value,"Say you have this table {{dec_test}}:
{code}
dec                 	decimal(10,0)       	                    
{code}

If the table has a row that is 9999999999.5, and if we do

{code}
select * from dec_test;
{code}

it will crash with NPE:

{code}
2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)
  at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)
  at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)
  at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)
  at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
  at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)
  at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)
  at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)
  at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
  at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)
  at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)
  at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)
  ... 12 more
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)
  at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)
  at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)
  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)
  at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)
  at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)
  ... 19 more
{code}"
HIVE-7992,StatsRulesProcFactory should gracefully handle overflows,"When StatsRulesProcFactory overflows it sets data size to 0 and as a result the Vertex will ask for a single task, this results in a fairly slow running query, most likely the overflow is a result of higher than usual number of rows.

The class should detect an overflow and set a flag when an overflow occurs, if an overflow occurs StatsRulesProcFactory should request the maximum number of tasks for the vertex."
HIVE-7991,Incorrect calculation of number of rows in JoinStatsRule.process results in overflow,"This loop results in adding the parent twice incase of a 3 way join of store_sales  x date_dim x store

{code}
         for (int pos = 0; pos < parents.size(); pos++) {
            ReduceSinkOperator parent = (ReduceSinkOperator) jop.getParentOperators().get(pos);

            Statistics parentStats = parent.getStatistics();
            List<ExprNodeDesc> keyExprs = parent.getConf().getKeyCols();

            // Parent RS may have column statistics from multiple parents.
            // Populate table alias to row count map, this will be used later to
            // scale down/up column statistics based on new row count
            // NOTE: JOIN with UNION as parent of RS will not have table alias
            // propagated properly. UNION operator does not propagate the table
            // alias of subqueries properly to expression nodes. Hence union20.q
            // will have wrong number of rows.
            Set<String> tableAliases = StatsUtils.getAllTableAlias(parent.getColumnExprMap());
            for (String tabAlias : tableAliases) {
              rowCountParents.put(tabAlias, parentStats.getNumRows());
            }
{code}

In the first join we have rowCountParents with {store_sales=120464862, date_dim=36524} which is correct.
For the second join result rowCountParents ends up with {store=212, store_sales=120464862, date_dim=120464862} where it should be {store=212, store_sales=120464862, date_dim=36524}.
The result of this is that computeNewRowCount ends up multiplying row count of store_sales x store_sales which makes the number of rows really high and eventually over flow.

Plan snippet : 
{code}
   Map 1
            Map Operator Tree:
                TableScan
                  alias: store_sales
                  filterExpr: (((ss_sold_date_sk is not null and ss_store_sk is not null) and ss_item_sk is not null) and ss_sold_date BETWEEN '1999-06-01' AND '2000-05-31') (type: boolean)
                  Statistics: Num rows: 110339135 Data size: 4817453454 Basic stats: COMPLETE Column stats: COMPLETE
                  Filter Operator
                    predicate: ((ss_sold_date_sk is not null and ss_store_sk is not null) and ss_item_sk is not null) (type: boolean)
                    Statistics: Num rows: 107740258 Data size: 2124353556 Basic stats: COMPLETE Column stats: COMPLETE
                    Map Join Operator
                      condition map:
                           Inner Join 0 to 1
                      condition expressions:
                        0 {ss_sold_date_sk} {ss_item_sk} {ss_store_sk} {ss_quantity} {ss_sales_price} {ss_sold_date}
                        1 {d_date_sk} {d_month_seq} {d_year} {d_moy} {d_qoy}
                      keys:
                        0 ss_sold_date_sk (type: int)
                        1 d_date_sk (type: int)
                      outputColumnNames: _col0, _col2, _col7, _col10, _col13, _col23, _col27, _col30, _col33, _col35, _col37
                      input vertices:
                        1 Map 6
                      Statistics: Num rows: 120464862 Data size: 26984129088 Basic stats: COMPLETE Column stats: COMPLETE
                      Map Join Operator
                        condition map:
                             Inner Join 0 to 1
                        condition expressions:
                          0 {_col0} {_col2} {_col7} {_col10} {_col13} {_col23} {_col27} {_col30} {_col33} {_col35} {_col37}
                          1 {s_store_sk} {s_store_id}
                        keys:
                          0 _col7 (type: int)
                          1 s_store_sk (type: int)
                        outputColumnNames: _col0, _col2, _col7, _col10, _col13, _col23, _col27, _col30, _col33, _col35, _col37, _col58, _col59
                        input vertices:
                          1 Map 5
                        Statistics: Num rows: 17886616227069518 Data size: 5866810122478801920 Basic stats: COMPLETE Column stats: COMPLETE
                        Map Join Operator
                          condition map:
                               Inner Join 0 to 1
                          condition expressions:
                            0 {_col0} {_col2} {_col7} {_col10} {_col13} {_col23} {_col27} {_col30} {_col33} {_col35} {_col37} {_col58} {_col59}
                            1 {i_item_sk} {i_brand} {i_class} {i_category} {i_product_name}
                          keys:
                            0 _col2 (type: int)
                            1 i_item_sk (type: int)
                          outputColumnNames: _col0, _col2, _col7, _col10, _col13, _col23, _col27, _col30, _col33, _col35, _col37, _col58, _col59, _col90, _col98, _col100, _col102, _col111
                          input vertices:
                            1 Map 7
                          Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                          Filter Operator
                            predicate: (((((_col0 = _col27) and (_col2 = _col90)) and (_col7 = _col58)) and _col30 BETWEEN 1193 AND 1204) and _col23 BETWEEN '1999-06-01' AND '2000-05-31') (type: boolean)
                            Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                            Select Operator
                              expressions: _col102 (type: string), _col100 (type: string), _col98 (type: string), _col111 (type: string), _col33 (type: int), _col37 (type: int), _col35 (type: int), _col59 (type: string), _col13 (type: float), _col10 (type: int)
                              outputColumnNames: _col102, _col100, _col98, _col111, _col33, _col37, _col35, _col59, _col13, _col10
                              Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                              Group By Operator
                                aggregations: sum(COALESCE((_col13 * _col10),0))
                                keys: _col102 (type: string), _col100 (type: string), _col98 (type: string), _col111 (type: string), _col33 (type: int), _col37 (type: int), _col35 (type: int), _col59 (type: string), '0' (type: string)
                                mode: hash
                                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
                                Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                                Reduce Output Operator
                                  key expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: int), _col7 (type: string), _col8 (type: string)
                                  sort order: +++++++++
                                  Map-reduce partition columns: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: int), _col7 (type: string), _col8 (type: string)
                                  Statistics: Num rows: -9223372036854775808 Data size: 0 Basic stats: NONE Column stats: COMPLETE
                                  value expressions: _col9 (type: double)
{code}"
HIVE-7987,Storage based authorization  - NPE for drop view,"When storage based authorization is enabled, NullPointerException is thrown for 'drop view'.
"
HIVE-7904,Missing null check cause NPE when updating join column stats in statistics annotation,Column stats updation in join stats rule annotation can cause NPE if column stats is missing from one relation. 
HIVE-7765,Null pointer error with UNION ALL on partitioned tables using Tez,"When executing a UNION ALL query in Tez over partitioned tables where at least one table is empty, Hive fails to execute the query, returning the message ""FAILED: NullPointerException null"".  No stack trace accompanies this message.  Removing partitioning solves this problem, as does switching to MapReduce as the execution engine.

This can be reproduced using a variant of the example tables from the ""Getting Started"" documentation on the Hive wiki.  To create the schema, use

CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);
CREATE TABLE empty_invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);

Then, load invites with data (e.g., using the instructions [here|https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-DMLOperations]) and execute the following:

SELECT * FROM invites
UNION ALL
SELECT * FROM empty_invites;"
HIVE-7738,tez select sum(decimal) from union all of decimal and null throws NPE,"if run this query using tez engine then hive will throw NPE
{code}
select sum(a) from (
  select cast(1.1 as decimal) a from dual
  union all
  select cast(null as decimal) a from dual
) t;
{code}

{code}
hive> select sum(a) from (
    >   select cast(1.1 as decimal) a from dual
    >   union all
    >   select cast(null as decimal) a from dual
    > ) t;
Query ID = apivovarov_20140814200909_438385b2-4147-47bc-98a0-a01567bbb5c5
Total jobs = 1
Launching Job 1 out of 1


Status: Running (application id: application_1407388228332_5616)

Map 1: -/-	Map 4: -/-	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 0/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 0/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Map 1: 0/1	Map 4: 1/1	Reducer 3: 0/1	
Status: Failed
Vertex failed, vertexName=Map 1, vertexId=vertex_1407388228332_5616_1_02, diagnostics=[Task failed, taskId=task_1407388228332_5616_1_02_000000, diagnostics=[AttemptID:attempt_1407388228332_5616_1_02_000000_0 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more

Container released by application, AttemptID:attempt_1407388228332_5616_1_02_000000_1 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more

Container released by application, AttemptID:attempt_1407388228332_5616_1_02_000000_2 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more

Container released by application, AttemptID:attempt_1407388228332_5616_1_02_000000_3 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:188)
	at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)
	at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:564)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)
	at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:553)
Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:145)
	at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:164)
	... 6 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableConstantHiveDecimalObjectInspector.precision(WritableConstantHiveDecimalObjectInspector.java:61)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum$GenericUDAFSumHiveDecimal.init(GenericUDAFSum.java:106)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:362)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:67)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:460)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:416)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:425)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376)
	at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:121)
	... 7 more
], Vertex failed as one or more tasks failed. failedTasks:1]
Vertex killed, vertexName=Reducer 3, vertexId=vertex_1407388228332_5616_1_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex killed as other vertex failed. failedTasks:0]
DAG failed due to vertex failure. failedVertices:1 killedVertices:1
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask
{code}
"
HIVE-7641,INSERT ... SELECT with no source table leads to NPE,"When no source table is provided for an INSERT statement Hive fails with NPE. 

{code}
0: jdbc:hive2://localhost:11050/default> create table test_tbl(i int);
No rows affected (0.333 seconds)
0: jdbc:hive2://localhost:11050/default> insert into table test_tbl select 1;
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)

-- Get a NPE even when using incorrect syntax (no TABLE keyword)
0: jdbc:hive2://localhost:11050/default> insert into test_tbl select 1;
Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)

-- Works when a source table is provided
0: jdbc:hive2://localhost:11050/default> insert into table test_tbl select 1 from foo;
No rows affected (5.751 seconds)
{code}"
HIVE-7574,CommonJoinOperator.checkAndGenObject calls LOG.Trace per row from probe side in a HashMap join consuming 4% of the CPU,"In Map join Log4JLogger.trace takes 4% of the CPU time as it gets called per row from the probe side by CommonJoinOperator.genAllOneUniqueJoinObject.

Fix is to remove the logging code code below from CommonJoinOperator.genAllOneUniqueJoinObject:
{code}
if (allOne) {
        LOG.info(""calling genAllOneUniqueJoinObject"");
        genAllOneUniqueJoinObject();
        LOG.info(""called genAllOneUniqueJoinObject"");
      } else {
        LOG.trace(""calling genUniqueJoinObject"");
        genUniqueJoinObject(0, 0);
        LOG.trace(""called genUniqueJoinObject"");
      }
{code}

And 
{code}
    if (!hasEmpty && !mayHasMoreThanOne) {
        LOG.trace(""calling genAllOneUniqueJoinObject"");
        genAllOneUniqueJoinObject();
        LOG.trace(""called genAllOneUniqueJoinObject"");
      } else if (!hasEmpty && !hasLeftSemiJoin) {
        LOG.trace(""calling genUniqueJoinObject"");
        genUniqueJoinObject(0, 0);
        LOG.trace(""called genUniqueJoinObject"");
      } else {
        LOG.trace(""calling genObject"");
        genJoinObject();
        LOG.trace(""called genObject"");
      }
{code}


This is the call stack 
{code}
Stack Trace	Sample Count	Percentage(%)
hadoop.hive.ql.exec.MapJoinOperator.processOp(Object, int)	388	75.486
   hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject()	121	23.541
      hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject()	92	17.899
      commons.logging.impl.Log4JLogger.trace(Object)	20	3.891
         log4j.Category.log(String, Priority, Object, Throwable)	20	3.891
            log4j.Category.getEffectiveLevel()	10	1.946
{code}"
HIVE-7393,Tez jobs sometimes fail with NPE processing input splits,"Input files are either ORC or RC format.  Only occurs on occasion - if the query is repeated it is likely to complete successfully.

{noformat}
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Grouping splits in Tez
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Desired splits: 408 too large.  Desired splitLength: 614866 Min splitLength: 16777216 New desired splits: 15 Total length: 250865685 Original splits: 13
2014-07-11 15:31:45,367 INFO [InputInitializer [Map 3] #0] org.apache.hadoop.mapred.split.TezMapredSplitsGrouper: Using original number of splits: 13 desired splits: 15
2014-07-11 15:31:45,381 INFO [AsyncDispatcher event handler] org.apache.tez.dag.history.HistoryEventHandler: [HISTORY][DAG:dag_1405114778353_0004_1][Event:VERTEX_INITIALIZED]: vertexName=Reducer 4, vertexId=vertex_1405114778353_0004_1_09, initRequestedTime=1405117905313, initedTime=1405117905381, numTasks=999, processorName=org.apache.hadoop.hive.ql.exec.tez.ReduceTezProcessor, additionalInputsCount=0
2014-07-11 15:31:45,381 INFO [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: vertex_1405114778353_0004_1_09 [Reducer 4] transitioned from NEW to INITED due to event V_INIT
2014-07-11 15:31:45,383 ERROR [AsyncDispatcher event handler] org.apache.tez.dag.app.dag.impl.VertexImpl: Vertex Input: csb initializer failed
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:275)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)
	at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)
	at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)
	at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)
	at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}
"
HIVE-7379,Beeline to fetch full stack trace for job (task) failures ,"When a query submitted via Beeline fails, Beeline displays a generic error message as below:

{quote}FAILED: Execution Error, return code 1 from …{quote}

This is expected, as Beeline is basically a regular JDBC client and is hence limited by JDBC's capabilities today. But it would be useful if Beeline can return the full remote stack trace and task diagnostics or job ID."
HIVE-7353,HiveServer2 using embedded MetaStore leaks JDOPersistanceManager,"While using embedded metastore, while creating background threads to run async operations, HiveServer2 ends up creating new instances of JDOPersistanceManager which are cached in JDOPersistanceManagerFactory. Even when the background thread is killed by the thread pool manager, the JDOPersistanceManager are never GCed because they are cached by JDOPersistanceManagerFactory."
HIVE-7313,Allow in-memory/ssd session-level temp-tables,"With HDFS storage policies implementation, temporary tables can be written with different storage/reliability policies. 

In-session temporary tables can be targetted at both SSD and memory storage policies, with fallbacks onto the disk and the associated reliability trade-offs."
HIVE-7264,TPCDS Query 78 throws NPE when Vectorization is turned on,"I get the same identical stack traceback found in TEZ-975 running TPC-DS query78 with vectorization turned while running on HDP 2.1 / Hive 0.13/Tez.
The HDP 2.1 release notes state that HIVE-6742 was fixed (and TEZ-975 was closed as fixed/duplicate)

When I turn off vectorization and the query completes. "
HIVE-7202,DbTxnManager deadlocks in hcatalog.cli.TestSematicAnalysis.testAlterTblFFpart(),"select * from HIVE_LOCKS produces

{noformat}
6                   |1                   |0                   |default                                                                                                                         |junit_sem_analysis                                                                                                              |NULL                                                                                                                            |w|r|1402354627716       |NULL                |unknown                                                                                                                         |ekoifman.local                                                                                                                  
6                   |2                   |0                   |default                                                                                                                         |junit_sem_analysis                                                                                                              |b=2010-10-10                                                                                                                    |w|e|1402354627716       |NULL                |unknown                                                                                                                         |ekoifman.local                                                                                                                  

2 rows selected
{noformat}

easiest way to repro this is to add
    hiveConf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
    hiveConf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, ""org.apache.hadoop.hive.ql.lockmgr.DbTxnManager"");

in HCatBaseTest.setUpHiveConf()"
HIVE-7155,WebHCat controller job exceeds container memory limit,"Submit a Hive query on a large table via WebHCat results in failure because the WebHCat controller job is killed by Yarn since it exceeds the memory limit (set by mapreduce.map.memory.mb, defaults to 1GB):
{code}
 INSERT OVERWRITE TABLE Temp_InjusticeEvents_2014_03_01_00_00 SELECT * from Stage_InjusticeEvents where LogTimestamp > '2014-03-01 00:00:00' and LogTimestamp <= '2014-03-01 01:00:00';
{code}

We could increase mapreduce.map.memory.mb to solve this problem, but this way we are changing this setting system wise.

We need to provide a WebHCat configuration to overwrite mapreduce.map.memory.mb when submitting the controller job.

"
HIVE-7128,Add direct support for creating and managing salted hbase tables,"Salting is a very important technique in order to avoid hot-spotting in hbase. It will be very beneficial if with current hbase integration we can provide a direct support for salting. More information on salting can be found here[1]

[1] http://blog.sematext.com/2012/04/09/hbasewd-avoid-regionserver-hotspotting-despite-writing-records-with-sequential-keys/"
HIVE-7109,Resource leak in HBaseStorageHandler,"The ""preCreateTable"" method in the HBaseStorageHandler checks that the HBase table is still online by creating a new instance of HTable

{code}
// ensure the table is online
new HTable(hbaseConf, tableDesc.getName());
{code}

However this instance is never closed. So if this test succeeds, we would have a resource leak in the code."
HIVE-7081,HiveServer/HiveServer2 leaks jdbc connections when network  interrupt,"HiveServer/HiveServer2 leaks jdbc connections when network between client and server is interrupted。
I test both use DBVisualizer and write JDBC code，when the network between client and hiveserver/hiverserver2 is interrupted，the tcp connection in the server side will be in ESTABLISH state forever util the server is stoped。By Using jstack to print out server's thread, I found thread is doing socketRead0()。
{quote}
""pool-1-thread-13"" prio=10 tid=0x00007fd00c0c6800 nid=0x5d21 runnable [0x00007fd000180000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:152)
	at java.net.SocketInputStream.read(SocketInputStream.java:122)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	- locked <0x00000000ebc24f28> (a java.io.BufferedInputStream)
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745) 
{quote}"
HIVE-7069,Zookeeper connection leak,We're using CDH 5.0.0 which ships with HIVE 0.12.0. We're running HiveServer2 and connect to it via JDBC. We have zookeeper support enabled. If a connection is made to HS2 and not explicitly closed via JDBC then a connection made to zookeeper is never released. It reaches a point where HS2 hangs and stops executing any new queries. It's easy to replicate with a simple script that connects to HS2 via JDBC and runs a simple query like 'show tables'. At the same time run this on the hive server machine to monitor zookeeper connections: 'while sleep 1; do netstat -anlp | grep 2181 | wc -l; done' .. If you close the connection explicitly the count will go down soon after the program exits.
HIVE-7021,HiveServer2 memory leak on failed queries,"The number of the following objects keeps increasing if a query causes an exception:
org.apache.hive.service.cli.HandleIdentifier
org.apache.hive.service.cli.OperationHandle
org.apache.hive.service.cli.log.LinkedStringBuffer
org.apache.hive.service.cli.log.OperationLog

The leak can be observed using a JDBCClient that runs something like this
                          connection = DriverManager.getConnection(""jdbc:hive2://"" + hostname + "":10000/default"", """", """");
                          statement   = connection.createStatement();
                          statement.execute(""CREATE TEMPORARY FUNCTION dummy_function AS 'dummy.class.name'"");

The above SQL will fail if HS2 cannot load ""dummy.class.name"" class. Each iteration of such query will result in +1 increase in instance count for the classes mentioned above.

This will eventually cause OOM in the HS2 service.
"
HIVE-7020,NPE when there is no plan file.,"Hive throws NPE when there is no plan file.

Exception message:
{code}
2014-05-06 18:03:17,749 INFO [main] org.apache.hadoop.hive.ql.exec.Utilities: No plan file found: file:/tmp/test/hive_2014-05-06_18-02-58_539_232619201891510265-1/-mr-10001/8cf1c965-b173-4482-a016-4a51a74b9324/map.xml
2014-05-06 18:03:17,750 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:255)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:437)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:430)
	at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587)
	at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:168)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:409)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
{code}

I looked through the code,
ql/exec/Utilities.java:
{code}
private static BaseWork getBaseWork(Configuration conf, String name) {
  ............
      } catch (FileNotFoundException fnf) {
      // happens. e.g.: no reduce work.
      LOG.info(""No plan file found: ""+path);
      return null;
    }
{code}

this code was called by HiveInputFormat.java:
{code}
  protected void init(JobConf job) {
    mrwork = Utilities.getMapWork(job);
    pathToPartitionInfo = mrwork.getPathToPartitionInfo();
  }
{code}

mrwork  is null, then NPE here.

"
HIVE-6984,Analyzing partitioned table with NULL values for the partition column failed with NPE,"The following describes how to produce the bug:
{code}
hive> desc test2;
name                	string              	                    
age                 	int                 	                    

hive> select * from test2;
6666666666666666666	NULL
5555555555555555555	NULL
tom	15
john	NULL
mayr	40
	30
	NULL

hive> create table test3(name string) partitioned by (age int);

hive> from test2 insert overwrite table test3 partition(age) select test2.name, test2.age;
Loading data to table default.test3 partition (age=null)
	Loading partition {age=40}
	Loading partition {age=__HIVE_DEFAULT_PARTITION__}
	Loading partition {age=30}
	Loading partition {age=15}
Partition default.test3{age=15} stats: [numFiles=1, numRows=1, totalSize=4, rawDataSize=3]
Partition default.test3{age=30} stats: [numFiles=1, numRows=1, totalSize=1, rawDataSize=0]
Partition default.test3{age=40} stats: [numFiles=1, numRows=1, totalSize=5, rawDataSize=4]
Partition default.test3{age=__HIVE_DEFAULT_PARTITION__} stats: [numFiles=1, numRows=4, totalSize=46, rawDataSize=42]

hive> analyze table test3 partition(age) compute statistics;
...
Task with the most failures(4): 
-----
Diagnostic Messages for this Task:
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
	at org.apache.hado

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
{code}

The following is the stack trace in mapper log:
{code}
2014-04-28 15:39:25,073 FATAL org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""name"":""6666666666666666666"",""age"":null,""raw__data__size"":19}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1499)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:539)
	... 9 more
{code}"
HIVE-6929,hcatalog packaging is not fully integrated with hive,"Currently, if you run {{mvn package}} hcatalog jars are in {{hcatalog/share/hcatalog}} and similarly webhcat jars. All other hive jars are in lib/ and thats where hcatalog jars should also be. Similar is the story for webhcat. To reduce confusion, its better that hcatalog follow hive's dir structure. "
HIVE-6866,Hive server2 jdbc driver connection leak with namenode,"1. Set 'ipc.client.connection.maxidletime' to 3600000 in core-site.xml and start hive-server2.
2. Connect hive server2 repetitively in a while true loop.
3. The tcp connection number will increase until out of memory, it seems that hive server2 will not close the connection until the time out, the error message is as the following:
{code}
2014-03-18 23:30:36,873 ERROR ql.Driver (SessionState.java:printError(386)) - FAILED: RuntimeException java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""hdm1.hadoop.local/192.168.2.101""; destination host is: ""hdm1.hadoop.local"":8020;
java.lang.RuntimeException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""hdm1.hadoop.local/192.168.2.101""; destination host is: ""hdm1.hadoop.local"":8020;
	at org.apache.hadoop.hive.ql.Context.getScratchDir(Context.java:190)
	at org.apache.hadoop.hive.ql.Context.getMRScratchDir(Context.java:231)
	at org.apache.hadoop.hive.ql.Context.getMRTmpFileURI(Context.java:288)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1274)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1059)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8676)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:278)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hive.service.cli.operation.SQLOperation.run(SQLOperation.java:95)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:181)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:148)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:203)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:40)
	at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:37)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1478)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:524)
	at org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:37)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""hdm1.hadoop.local/192.168.2.101""; destination host is: ""hdm1.hadoop.local"":8020;
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:761)
	at org.apache.hadoop.ipc.Client.call(Client.java:1239)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:202)
	at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at com.sun.proxy.$Proxy11.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2259)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2230)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:540)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1881)
	at org.apache.hadoop.hive.ql.Context.getScratchDir(Context.java:182)
	... 28 more
Caused by: java.io.IOException: Couldn't set up IO streams
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:662)
	at org.apache.hadoop.ipc.Client$Connection.access$2100(Client.java:253)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1288)
	at org.apache.hadoop.ipc.Client.call(Client.java:1206)
	... 42 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:713)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:655)
	... 45 more
{code} "
HIVE-6841,Vectorized execution throws NPE for partitioning columns with __HIVE_DEFAULT_PARTITION__,"If partitioning columns have __HIVE_DEFAULT_PARTITION__ or null, vectorized execution throws NPE."
HIVE-6755,Zookeeper Lock Manager leaks zookeeper connections.,"Driver holds instance for ZkHiveLockManager. In turn SqlQuery holds it too. So if we have many not closed queries we will get many zk sessions.
"
HIVE-6716,ORC struct throws NPE for tables with inner structs having null values ,"ORCStruct should return null when object passed to getStructFieldsDataAsList(Object obj) is null.

{code}
public List<Object> getStructFieldsDataAsList(Object object) {
      OrcStruct struct = (OrcStruct) object;
      List<Object> result = new ArrayList<Object>(struct.fields.length);
{code}

In the above code struct.fields will throw NPE if struct is NULL."
HIVE-6710,Deadlocks seen in transaction handler using mysql,When multiple clients attempt to obtain locks a deadlock on the mysql database occasionally occurs.
HIVE-6690,NPE in tez session state,"If hive.jar.directory isn't set hive will throw NPE in startup with tez:

Exception in thread ""main"" java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:344)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:682)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createHiveExecLocalResource(TezSessionState.java:303)
        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:130)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:342)
        ... 7 more"
HIVE-6682,nonstaged mapjoin table memory check may be broken,"We are getting the below error from task while the staged load works correctly. 
We don't set the memory threshold so low so it seems the settings are just not handled correctly. This seems to always trigger on the first check. Given that map task might have bunch more stuff, not just the hashmap, we may also need to adjust the memory check (e.g. have separate configs).

{noformat}
Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException: 2014-03-14 08:11:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	204001888	percentage:	0.197
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException: 2014-03-14 08:11:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	204001888	percentage:	0.197
	at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:104)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:150)
	at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:165)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)
	at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionException: 2014-03-14 08:11:21	Processing rows:	200000	Hashtable size:	199999	Memory usage:	204001888	percentage:	0.197
	at org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.checkMemoryStatus(MapJoinMemoryExhaustionHandler.java:91)
	at org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.processOp(HashTableSinkOperator.java:248)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:791)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:375)
	at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:346)
	at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:147)
	at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:82)
	... 15 more
{noformat}"
HIVE-6673,sql std auth - show grant statement for all principals throws NPE,"{code}
show grant on table t1;
2014-03-14 12:45:46,573 ERROR exec.DDLTask (DDLTask.java:execute(461)) - java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.DDLTask.getHivePrincipal(DDLTask.java:893)
        at org.apache.hadoop.hive.ql.exec.DDLTask.showGrantsV2(DDLTask.java:648)
        at org.apache.hadoop.hive.ql.exec.DDLTask.showGrants(DDLTask.java:553)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:426)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
...
...
{code}"
HIVE-6631,NPE when select a field of a struct from a table stored by ORC,"I have a table like this ...
{code:sql}
create table lineitem_orc_cg
(
CG1 STRUCT<L_PARTKEY:INT,
           L_SUPPKEY:INT,
           L_COMMITDATE:STRING,
           L_RECEIPTDATE:STRING,
           L_SHIPINSTRUCT:STRING,
           L_SHIPMODE:STRING,
           L_COMMENT:STRING,
           L_TAX:float,
           L_RETURNFLAG:STRING,
           L_LINESTATUS:STRING,
           L_LINENUMBER:INT,
           L_ORDERKEY:INT>,
CG2 STRUCT<L_QUANTITY:float,
           L_EXTENDEDPRICE:float,
           L_DISCOUNT:float,
           L_SHIPDATE:STRING>
)
row format serde 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
stored as orc tblproperties (""orc.compress""=""NONE"");
{code}
When I want to select a field from a struct by using
{code:sql}
select cg1.l_comment from lineitem_orc_cg limit 1;
{code}

I got 
{code}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ExprNodeFieldEvaluator.initialize(ExprNodeFieldEvaluator.java:61)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:928)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:954)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:65)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:459)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:415)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:189)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:409)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:133)
	... 22 more
{code}"
HIVE-6607,describe extended on a view fails with NPE,"STEPS TO REPRODUCE:
Create a table called 'sample_08'
Create a view of the table. From hive command line, please run:
hive> create view sample_09 as select * from sample_08 ;
ACTUAL BEHAVIOR:
Run the following command in the browser:
http://localhost:50111/templeton/v1/ddl/database/default/table/sample_09?format=extended
It fails with the following exception:
{""errorDetail"":""org.apache.hadoop.hive.ql.metadata.HiveException: Exception while processing show table status\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:2707)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:343)\n\tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)\n\tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)\n\tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1437)\n\tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1215)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1043)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)\n\tat org.apache.hive.hcatalog.cli.HCatDriver.run(HCatDriver.java:43)\n\tat org.apache.hive.hcatalog.cli.HCatCli.processCmd(HCatCli.java:259)\n\tat org.apache.hive.hcatalog.cli.HCatCli.processLine(HCatCli.java:213)\n\tat org.apache.hive.hcatalog.cli.HCatCli.main(HCatCli.java:172)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.lang.NullPointerException\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.putFileSystemsStats(JsonMetaDataFormatter.java:264)\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeOneTableStatus(JsonMetaDataFormatter.java:218)\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeAllTableStatus(JsonMetaDataFormatter.java:170)\n\tat org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showTableStatus(JsonMetaDataFormatter.java:153)\n\tat org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:2702)\n\t... 16 more\n"",""error"":""FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Exception while processing show table status"",""sqlState"":""08S01"",""errorCode"":40000,""database"":""default"",""table"":""sample_09""}"
HIVE-6532,ORC NPE on null Map,"On a select query map tasks will fail with npe for a where clause on a null map. on an ORC table 
example
 select * from my table where mymap['entry'] = 'something'
If my map where to be null in a line we will get an npe exception.
Maybe the line should just be ignored.
The same query on a text format table will resolve correctly"
HIVE-6524,Update ORC Filedump stripe sizes to match the memory manager changes,"The MemoryManager in ORC now resets to default whenever we close all open writers. 

This results in consistent (but different from test golden) stripe sizes for all files being written.

Fix the goldens."
HIVE-6518,Add a GC canary to the VectorGroupByOperator to flush whenever a GC is triggered,"The current VectorGroupByOperator implementation flushes the in-memory hashes when the maximum entries or fraction of memory is hit.

This works for most cases, but there are some corner cases where we hit GC ovehead limits or heap size limits before either of those conditions are reached due to the rest of the pipeline.

This patch adds a SoftReference as a GC canary. If the soft reference is dead, then a full GC pass happened sometime in the near past & the aggregation hashtables should be flushed immediately before another full GC is triggered."
HIVE-6468,HS2 & Metastore using SASL out of memory error when curl sends a get request,"We see an out of memory error when we run simple beeline calls.
(The hive.server2.transport.mode is binary)

curl localhost:10000

Exception in thread ""pool-2-thread-8"" java.lang.OutOfMemoryError: Java heap space
	at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:181)
	at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:189)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)"
HIVE-6450,Potential deadlock caused by unlock exceptions,"In the following two code snippets, unlock might fail with LockException. This
exception is not handled and thus the program might go on without releasing the lock, causing potential deadlock or starvations.

Line: 197, File: ""org/apache/hadoop/hive/ql/lockmgr/EmbeddedLockManager.java""
{noformat}
194:       try {
195:         unlock(locked, numRetriesForUnLock, sleepTime);
196:       } catch (LockException e) {
197:         LOG.info(e);
198:       }
{noformat}

Line: 276, File: ""org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java""
{noformat}
271:         try {
272:           LOG.info("" about to release lock for "" + hiveLock.getHiveLockObject().getName());
273:           unlock(hiveLock);
274:         } catch (LockException e) {
275:           // The lock may have been released. Ignore and continue
276:           LOG.warn(""Error when releasing lock"", e);
277:         }
{noformat}"
HIVE-6268,Network resource leak with HiveClientCache when using HCatInputFormat,"HCatInputFormat has a cache feature that allows HCat to cache hive client connections to the metastore, so as to not keep reinstantiating a new hive server every single time. This uses a guava cache of hive clients, which only evicts entries from cache on the next write, or by manually managing the cache.

So, in a single threaded case, where we reuse the hive client, the cache works well, but in a massively multithreaded case, where each thread might perform one action, and then is never used, there are no more writes to the cache, and all the clients stay alive, thus keeping ports open."
HIVE-6212,"Using Presto-0.56 for sql query,but HiveServer the console print java.lang.OutOfMemoryError: Java heap space","Hi friends:
Now I can't open the page https://groups.google.com/forum/#!forum/presto-users ,so show my question here.
I have started hiveserver and started presto-server on a machine with commands below:
hive --service hiveserver -p 9083
./launcher run
When I use the presto-client-cli command ./presto --server localhost:9083 --catalog hive --schema default ,the console shows presto:default>,input the command as show tables the console prints Error running command: java.nio.channels.ClosedChannelException,
and the hiveserver console print as below:
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread ""pool-1-thread-1"" java.lang.OutOfMemoryError: Java heap space
at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:353)
at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:215)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
at java.lang.Thread.run(Thread.java:662)

my configuration file below:
node.properties
node.environment=production
node.id=cc4a1bbf-5b98-4935-9fde-2cf1c98e8774
node.data-dir=/home/hadoop/cloudera-5.0.0/presto-0.56/presto/data

config.properties
coordinator=true
datasources=jmx
http-server.http.port=8080
presto-metastore.db.type=h2
presto-metastore.db.filename=/home/hadoop/cloudera-5.0.0/presto-0.56/presto/db/MetaStore
task.max-memory=1GB
discovery-server.enabled=true
discovery.uri=http://slave4:8080

jvm.config
-server
-Xmx16G
-XX:+UseConcMarkSweepGC
-XX:+ExplicitGCInvokesConcurrent
-XX:+CMSClassUnloadingEnabled
-XX:+AggressiveOpts
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-XX:PermSize=150M
-XX:MaxPermSize=150M
-XX:ReservedCodeCacheSize=150M
-Xbootclasspath/p:/home/hadoop/cloudera-5.0.0/presto-0.56/presto-server-0.56/lib/floatingdecimal-0.1.jar

log.properties
com.facebook.presto=DEBUG

catalog/hive.properties
connector.name=hive-cdh4
hive.metastore.uri=thrift://slave4:9083

HADOOP ENVIRONMENT IS CDH5+CDH5-HIVE-0.11+PRESTO-0.56

Last I had increased the Java heap size for the Hive metastore,but it still given me the same error informations ,please help me to check if that is a bug of CDH5.Now I have no idea,god !

please help me ,thanks.
**********************************************************************************************************************************************************************
========================================================================================================
**********************************************************************************************************************************************************************
Add some informations as below:

Help,help,help!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
I have test prest-server-0.55 and 0.56 and 0.57 on CDH4 +hive-0.10 or hive-0.11,but it still shown error informations above.
ON coordinator machine the directory etc and configuration files as below:
=================coordinator================================ 
 ----------------config.properties:
coordinator=true
datasources=jmx
http-server.http.port=8080
presto-metastore.db.type=h2
presto-metastore.db.filename=/home/hadoop/cloudera-5.0.0/presto-0.55/presto/db/MetaStore
task.max-memory=1GB
discovery-server.enabled=true
discovery.uri=http://name:8080
--------------jvm.config:
-server
-Xmx4G
-XX:+UseConcMarkSweepGC
-XX:+ExplicitGCInvokesConcurrent
-XX:+CMSClassUnloadingEnabled
-XX:+AggressiveOpts
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-XX:PermSize=150M
-XX:MaxPermSize=150M
-XX:ReservedCodeCacheSize=150M
-Xbootclasspath/p:/home/hadoop/cloudera-5.0.0/presto-0.55/presto-server-0.55/lib/floatingdecimal-0.1.jar
-----------------log.properties:
com.facebook.presto=DEBUG
-----------------node.properties:
node.environment=production
#node.id=0699bf8f-ac4e-48f4-92a9-28c0d8862923
node.id=name
node.data-dir=/home/hadoop/cloudera-5.0.0/presto-0.55/presto/data
---------------hive.properties:
connector.name=hive-cdh4
hive.metastore.uri=thrift://name:10000
---------------jmx.properties:
connector.name=jmx

=================coordinator================================ 
=================woker==================================== 
----------------config.properties:
coordinator=false
datasources=jmx,hive
http-server.http.port=8080
presto-metastore.db.type=h2
presto-metastore.db.filename=/home/hadoop/cloudera-5.0.0/presto-0.55/presto/db/MetaStore
task.max-memory=1GB
discovery-server.enabled=true
discovery.uri=http://name:8080
--------------jvm.config:
-server
-Xmx4G
-XX:+UseConcMarkSweepGC
-XX:+ExplicitGCInvokesConcurrent
-XX:+CMSClassUnloadingEnabled
-XX:+AggressiveOpts
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
-XX:PermSize=150M
-XX:MaxPermSize=150M
-XX:ReservedCodeCacheSize=150M
-Xbootclasspath/p:/home/hadoop/cloudera-5.0.0/presto-0.55/presto-server-0.55/lib/floatingdecimal-0.1.jar
-----------------log.properties:
com.facebook.presto=DEBUG
-----------------node.properties:
node.environment=production
#node.id=773a6b4e-9fd0-4342-8d96-59d1f58ac7cd
node.id=data1
node.data-dir=/home/hadoop/cloudera-5.0.0/presto-0.55/presto/data
---------------hive.properties:
connector.name=hive-cdh4
hive.metastore.uri=thrift://name:10000
---------------jmx.properties:
connector.name=jmx
=================woker====================================
=================discovery-server===========================
--------------config.properties:
http-server.http.port=8411
discovery.uri=http://name:8080
--------------node.properties:
node.environment=production
#node.id=b45edeee-4870-420f-919b-9f8487b9750e
#node.id=0699bf8f-ac4e-48f4-92a9-28c0d8862923
node.id=name
node.data-dir=/home/hadoop/cloudera-5.0.0/presto-discovery-data
----------------jvm.config
-server
-Xmx1G
-XX:+UseConcMarkSweepGC
-XX:+ExplicitGCInvokesConcurrent
-XX:+AggressiveOpts
-XX:+HeapDumpOnOutOfMemoryError
-XX:OnOutOfMemoryError=kill -9 %p
=================discovery-server===========================
I had started hiveserver and coordinator service and discovery-server on name machine,worker on data1 machine, all that ware Normal.When I started to use the command which was as below:
./presto --server name:10000 --catalog hive --schema default
the console of hiveserver that shown :
Exception in thread ""pool-1-thread-1"" java.lang.OutOfMemoryError: Java heap space
        at org.apache.thrift.protocol.TBinaryProtocol.readStringBody(TBinaryProtocol.java:353)
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:215)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:27)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)

MeanWhile,I had increased the java heap size of hive to  2g or 4g,however it still throwed exceptions as above.
Please give me some ideas because of you are Professional.
Thanks.
"
HIVE-5986,ORC SARG evaluation fails with NPE for UDFs or expressions in predicate condition,"{code}select s from orctable where length(substr(s, 1, 2)) <= 2 and s like '%';{code} kind of queries generate empty child expressions for the operator (AND in this case). When child expressions are empty evaluate(TruthValue[] leaves) functions returns null which results in NPE during orc split elimination or row group elimination. "
HIVE-5950,ORC SARG creation fails with NPE for predicate conditions with decimal/date/char/varchar datatypes,"When decimal or date column is used, the type field in PredicateLeafImpl will be set to null. This will result in NPE during predicate leaf generation because of null dereferencing in hashcode computation. SARG creation should be extended to support/handle decimal and date data types."
HIVE-5917,Hive packaging build is broken," mvn package -DskipTests  -Phadoop-1 -Pdist

yield the following error:
{code}[INFO] Hive Packaging .................................... FAILURE [1.224s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 12.559s
[INFO] Finished at: Mon Dec 02 15:24:58 PST 2013
[INFO] Final Memory: 70M/2933M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.3:single (assemble) on project hive-packaging: Assembly is incorrectly configured: bin: Assembly is incorrectly configured: bin:
[ERROR] Assembly: bin is not configured correctly: One or more filters had unmatched criteria. Check debug log for more information.
[ERROR] -> [Help 1]
{code}"
HIVE-5853,Hive Lock Manager leaks zookeeper connections,"Hive 0.10 leaks zookeeper connections from ZooKeeperHiveLockManager. HIVE-3723 describes a similar issue for cases of semantic errors and failures, but we're experiencing a consistent connection leak per query (even simple successful queries like ""select * from dual"").

Workaround: When turning off hive.support.concurrency, everything works fine - no leak (obviously, since the lock manager is not used).

Details:

OS: CentOS 5.9
Hive version: hive-server-0.10.0+67-1.cdh4.2.0.p0.10.el5 and hive-0.10.0+198-1.cdh4.4.0.p0.15.el5
Hadoop version: CDH4.2
Namenode uses HA. Hive's zookeeper configuration uses the NN zookeeper.

The problem occurs both when using the python thrift API, and the java thrift API. 

The leak happens even when we're running repeated ""select * from dual"" queries. We've checked the zookeeper connections using ""netstat -n | grep 2181 | grep ESTAB | wc -l"".

Eventually, the connection from the client reach the max connections per client limit in ZK, causing new queries to get stuck and never return.

We'll gladly provide more information if needed.




"
HIVE-5741,"Fix binary packaging build eg include hcatalog, resolve pom issues","There are a couple issues with our current binary tarball:

* HCatalog is not included
* We include many jars which we don't need and which break things"
HIVE-5730,"Beeline throws non-terminal NPE upon starting, after mavenization","In beeline project's SQLCompletor.java, it uses the local class in an attempt to load ""sql-keywords.properties"".  Java class behavior will prepend this file name with the class's package name.  This results in a NPE during Beeline initialization.

Before mavenization, the ""sql-keywords.properties"" lived in the same package as SQLCompletor.java, but now it is moved to src/main/resources."
HIVE-5660,Hive incorrecly handles data overflow/underflow,"Unlike other databases, Hive has only one server mode w.r.t error handling, in which NULL value is placed in case of errors such as divide-by-zero and data overflow/underflow. However, it appears that hive isn't consistent in this prospect. In case of data overflow or underflow, Hive returns a value that wraps around, which can produce erroneous result without user noticing it. For instance:
{code}
hive> select 2147483640 + 2147483645 from tmp2 limit 1;
...
OK
-11
Time taken: 6.549 seconds, Fetched: 1 row(s)
{code}

To be consistent in terms of error handling, Hive should produce NULL value instead."
HIVE-5601,NPE in ORC's PPD  when using select * from table with where predicate,"ORCInputFormat has a method findIncludedColumns() which returns boolean array of included columns. In case of the following query 
{code}select * from qlog_orc where id<1000 limit 10;{code}
 where all columns are selected the findIncludedColumns() returns null. This will result in a NPE when PPD is enabled. Following is the stack trace
{code}Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.planReadPartialDataStreams(RecordReaderImpl.java:2387)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readPartialDataStreams(RecordReaderImpl.java:2543)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:2200)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:2573)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:2615)
	at org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:132)
	at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rows(ReaderImpl.java:348)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$OrcRecordReader.<init>(OrcInputFormat.java:99)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:241)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:237)
	... 8 more{code}"
HIVE-5530,null pointer exception when case returns null,"The following expression will cause an NPE
 
select case when 1 = 1 then null end  from t"
HIVE-5492,Explain query fails with NPE if a client doesn't call getResultSetSchema(),"If a thrift client makes an explain query and fetches results without calling getResultSetSchema() first, NullPointerException will occur in Hive.

{code}
org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:262)
        at org.apache.hive.service.cli.operation.OperationManager.getOperationNextRowSet(OperationManager.java:179)
        at org.apache.hive.service.cli.session.HiveSessionImpl.fetchResults(HiveSessionImpl.java:422)
        at org.apache.hive.service.cli.CLIService.fetchResults(CLIService.java:333)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.FetchResults(ThriftCLIService.java:413)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1613)
        at org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults.getResult(TCLIService.java:1598)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:40)
        at org.apache.hive.service.auth.TUGIContainingProcessor$1.run(TUGIContainingProcessor.java:37)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1477)
        at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:527)
        at org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:37)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
        at org.apache.hive.service.cli.Row.<init>(Row.java:45)
        at org.apache.hive.service.cli.RowSet.addRow(RowSet.java:61)
        at org.apache.hive.service.cli.operation.SQLOperation.getNextRowSet(SQLOperation.java:254)
        ... 19 more
{code}"
HIVE-5418,Integer overflow bug in ConditionalResolverCommonJoin.AliasFileSizePair,"Sometimes, auto map join conversion unexpectedly fails to choose map join over a common join, even if the auto map join conversion's size criterion is satisfied.

This is caused by an integer overflow bug in the method {{compareTo}} of the class {{ConditionalResolverCommonJoin.AliasFileSizePair}}.

The bug is triggered only if the big table size exceeds the small table size by at least 2**31 bytes.
"
HIVE-5345,"Operator::close() leaks Operator::out, holding reference to buffers","When processing multiple splits on the same operator pipeline, the output collector in Operator has a held reference, which causes issues.

Operator::close() does not de-reference the OutputCollector object Operator::out held by the object.

This means that trying to allocate space for a new OutputCollector causes an OOM because the old one is still reachable."
HIVE-5323,NPE on executing query with order by 1 asc,"I created a table with the below DDL in Hive 0.11 and used order by 1 asc in a SQL query and got NPE. Details are below:

CREATE TABLE hive_npe(viewTime INT, userid BIGINT,
     page_url STRING, referrer_url STRING)
 COMMENT 'NPE for Order by 1 ASC'
 STORED AS SEQUENCEFILE
;

select sum(viewtime) , sum(distinct userid) from hive_npe order by 1 asc
;

I get the below error message
Query returned non-zero code: 40000, cause: FAILED: NullPointerException null"
HIVE-5307,NPE in JsonMetaDataFormatter  if Table Path is null,"When I try to get a table (actually a view) description from HCatalog in HUE, get a NPE thrown as: 
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:2758)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:347)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:144)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1355)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1139)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:945)
	at org.apache.hcatalog.cli.HCatDriver.run(HCatDriver.java:43)
	at org.apache.hcatalog.cli.HCatCli.processCmd(HCatCli.java:251)
	at org.apache.hcatalog.cli.HCatCli.processLine(HCatCli.java:205)
	at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:160)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.putFileSystemsStats(JsonMetaDataFormatter.java:303)
	at org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeOneTableStatus(JsonMetaDataFormatter.java:257)
	at org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.makeAllTableStatus(JsonMetaDataFormatter.java:209)
	at org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.showTableStatus(JsonMetaDataFormatter.java:192)
	at org.apache.hadoop.hive.ql.exec.DDLTask.showTableStatus(DDLTask.java:2745)
	... 15 more

Digg into the implementation of JsonMetaDataFormatter, I think org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.putFileSystemsStats should handle the case if Table Path is NULL (like a view)."
HIVE-5296,Memory leak: OOM Error after multiple open/closed JDBC connections. ,"Multiple connections to Hiveserver2, all of which are closed and disposed of properly show the Java heap size to grow extremely quickly. 

This issue can be recreated using the following code

{code}

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.Properties;

import org.apache.hive.service.cli.HiveSQLException;
import org.apache.log4j.Logger;

/*
 * Class which encapsulates the lifecycle of a query or statement.
 * Provides functionality which allows you to create a connection
 */

public class HiveClient {
	
	Connection con;
	Logger logger;
	private static String driverName = ""org.apache.hive.jdbc.HiveDriver"";	
	private String db;
	
	
	public HiveClient(String db)
	{		
		logger = Logger.getLogger(HiveClient.class);
		this.db=db;
		
		try{
			 Class.forName(driverName);
		}catch(ClassNotFoundException e){
			logger.info(""Can't find Hive driver"");
		}
		
		String hiveHost = GlimmerServer.config.getString(""hive/host"");
		String hivePort = GlimmerServer.config.getString(""hive/port"");
		String connectionString = ""jdbc:hive2://""+hiveHost+"":""+hivePort +""/default"";
		logger.info(String.format(""Attempting to connect to %s"",connectionString));
		try{			
			con = DriverManager.getConnection(connectionString,"""","""");									
		}catch(Exception e){
			logger.error(""Problem instantiating the connection""+e.getMessage());
		}		
	}
			
	public int update(String query) 
	{
		Integer res = 0;
		Statement stmt = null;
		try{			
			stmt = con.createStatement();
			String switchdb = ""USE ""+db;
			logger.info(switchdb);		
			stmt.executeUpdate(switchdb);
			logger.info(query);
			res = stmt.executeUpdate(query);
			logger.info(""Query passed to server"");	
			stmt.close();
		}catch(HiveSQLException e){
			logger.info(String.format(""HiveSQLException thrown, this can be valid, "" +
					""but check the error: %s from the query %s"",query,e.toString()));
		}catch(SQLException e){
			logger.error(String.format(""Unable to execute query SQLException %s. Error: %s"",query,e));
		}catch(Exception e){
			logger.error(String.format(""Unable to execute query %s. Error: %s"",query,e));
		}
		
		if(stmt!=null)
			try{
				stmt.close();
			}catch(SQLException e){
				logger.error(""Cannot close the statment, potentially memory leak ""+e);
			}
		
		return res;
	}
	
	public void close()
	{
		if(con!=null){
			try {
				con.close();
			} catch (SQLException e) {				
				logger.info(""Problem closing connection ""+e);
			}
		}
	}
	
	
	
}
{code}

And by creating and closing many HiveClient objects. The heap space used by the hiveserver2 runjar process is seen to increase extremely quickly, without such space being released."
HIVE-5196,"ThriftCLIService.java uses stderr to print the stack trace, it should use the logger instead.","ThriftCLIService.java uses stderr to print the stack trace, it should use the logger instead. Using e.printStackTrace is not suitable for production."
HIVE-4957,"Restrict number of bit vectors, to prevent out of Java heap memory","normally increase number of bit vectors will increase calculation accuracy. Let's say
{noformat}
select compute_stats(a, 40) from test_hive;
{noformat}
generally get better accuracy than
{noformat}
select compute_stats(a, 16) from test_hive;
{noformat}
But larger number of bit vectors also cause query run slower. When number of bit vectors over 50, it won't help to increase accuracy anymore. But it still increase memory usage, and crash Hive if number if too huge. Current Hive doesn't prevent user use ridiculous large number of bit vectors in 'compute_stats' query.

One example
{noformat}
select compute_stats(a, 999999999) from column_eight_types;
{noformat}
crashes Hive.

{noformat}
2012-12-20 23:21:52,247 Stage-1 map = 0%,  reduce = 0%
2012-12-20 23:22:11,315 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.29 sec
MapReduce Total cumulative CPU time: 290 msec
Ended Job = job_1354923204155_0777 with errors
Error during job, obtaining debugging information...
Job Tracking URL: http://cs-10-20-81-171.cloud.cloudera.com:8088/proxy/application_1354923204155_0777/
Examining task ID: task_1354923204155_0777_m_000000 (and more) from job job_1354923204155_0777

Task with the most failures(4): 
-----
Task ID:
  task_1354923204155_0777_m_000000

URL:
  http://0.0.0.0:8088/taskdetails.jsp?jobid=job_1354923204155_0777&tipid=task_1354923204155_0777_m_000000
-----
Diagnostic Messages for this Task:
Error: Java heap space
{noformat}"
HIVE-4883,TestHadoop20SAuthBridge tests fail sometimes because of race condition,"TestHadoop20SAuthBridge tests testSaslWithHiveMetaStore and testMetastoreProxyUser sometimes fail. I have seen this more often on mac and windows, but this can happen on linux as well.

The problem is that metastore is started in a different thread and these unit tests actually rely on the metastore having initialized DelegationTokenSecretManager in HadoopThriftAuthBridge20S as part of the metastore startup (HiveMetaStore.startMetaStore )

"
HIVE-4770,Null Pointer Exception in Group By Operator,"Table and data attached.


{noformat}
SELECT   cfloat,
         csmallint,
         cint,
         ctimestamp,
         (cfloat + 10),
         STDDEV_SAMP(cfloat),
         (-((cfloat + 10))),
         (cint / cfloat),
         MAX(cint),
         (-(cint)),
         (cint * STDDEV_SAMP(cfloat)),
         STDDEV_SAMP(cint),
         VAR_SAMP(cint),
         (-(MAX(cint))),
         ((-(MAX(cint))) / 0.00000000000000000000E+000)
FROM     alltypes_orc
WHERE    (((1 >= cfloat)
           OR (cstring2 LIKE '%b'))
          OR ((cint <= csmallint)
              OR (cstring2 LIKE '%ss')))
GROUP BY cfloat, csmallint, cint, ctimestamp
ORDER BY cint, cfloat;
{noformat}

{noformat}
java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""ctinyint"":null,""csmallint"":-3806,""cint"":-66533315,""cbigint"":null,""cdouble"":null,""cfloat"":152.95706,""cstring1"":null,""cstring2"":null,""ctimestamp"":""9131-01-01 16:52:03.53"",""cboolean"":null}
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:162)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:271)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.mapred.Child.main(Child.java:265)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""ctinyint"":null,""csmallint"":-3806,""cint"":-66533315,""cbigint"":null,""cdouble"":null,""cfloat"":152.95706,""cstring1"":null,""cstring2"":null,""ctimestamp"":""9131-01-01 16:52:03.53"",""cboolean"":null}
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:671)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)
	... 8 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:796)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:88)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:136)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:502)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:832)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:652)
	... 9 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.shouldBeFlushed(GroupByOperator.java:941)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processHashAggr(GroupByOperator.java:836)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:723)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:791)
	... 21 more
{noformat}
"
HIVE-4761,ZooKeeperHiveLockManage.unlockPrimitive has race condition with threads,"In unlockPrimitive, we check to see if children exist and if not delete the parent node. If two threads do this at the same time it's possible for two threads to call Zookeeper.delete() on the same node."
HIVE-4688,NPE in writing null values.,VectorExpressionWriter throws NPE when writing null values.
HIVE-4679,WebHCat can deadlock Hadoop if the number of concurrently running tasks if higher or equal than the number of mappers,"o In the current Templeton design, each time a Job is submitted thru the REST API (it can be Pig/Hive or MR job), it will consume one Hadoop map slot. Given that the number of map slots is finite in the cluster (16 node cluster will have 32 map slots), in some circumstances, a user can deadlock the cluster if Templeton job submission pipeline takes over all map slots (Templeton map tasks will wait for the actual underlying jobs to complete, what will never happen, given that Hadoop has no free map slots to schedule new tasks).

o HCat queries use a different mechanism and do not contribute to the deadlock."
HIVE-4631,Hive doesn't flush errors of multi-stage queries to STDERR until end of query,"When running a multi-stage query from the CLI for example 
say the query is:
SELECT  COUNT(*) FROM ( 
SELECT user from users
where datetime = 05-10-2013
UNION ALL
SELECT user from users
where datetime = 05-10-2013 
) a

and the command to execute it:
hive -e ""<query>"" 

If one of the jobs fails for example the 1st of 3, it should immediately write that to the standard error and flush it. Therefore if you are scripting a hive query you can catch the exception and terminate the other jobs rather then having to wait for them to run to handle the exception from the first job.

See here for more details:
http://stackoverflow.com/questions/16825066/hive-flush-errors-of-multi-stage-jobs-to-stderr-in-python

Thanks!"
HIVE-4558,mapreduce_stack_trace_hadoop20 in TestNegativeMinimrCliDriver fails on Windows,"testNegativeCliDriver_mapreduce_stack_trace_hadoop20 fails because group information is printed out on Windows. Here is the example of mapreduce_stack_trace_hadoop20.q.out.orig:
--------------------------------------------------------------------------
PREHOOK: query: FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exist' AS (key, value)
PREHOOK: type: QUERY
PREHOOK: Input: default@src
PREHOOK: Output: hdfs://127.0.0.1:25477/code/HWX/hive-monarch/build/ql/scratchdir/hive_2013-05-14_15-21-00_075_593034964465269090/-mr-10000
Ended Job = job_20130514152027587_0001 with errors
FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""key"":""238"",""value"":""val_238""}

groups found for user Administrators
Hive Runtime Error while processing row {""key"":""238"",""value"":""val_238""}
--------------------------------------------------------------------------

However, it is supposed to look like:
--------------------------------------------------------------------------
PREHOOK: query: FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exist' AS (key, value)
PREHOOK: type: QUERY
PREHOOK: Input: default@src
\#### A masked pattern was here ####
FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {""key"":""238"",""value"":""val_238""}
Hive Runtime Error while processing row {""key"":""238"",""value"":""val_238""}
--------------------------------------------------------------------------
"
HIVE-4540,JOIN-GRP BY-DISTINCT fails with NPE when mapjoin.mapreduce=true,"If the mapjoin.mapreduce optimization kicks in on a query of this form:

{noformat}
select count(distinct a.v) 
from a join b on (a.k = b.k)
group by a.g
{noformat}

The planer will NPE in the metadataonly optimizer."
HIVE-4502,NPE - subquery smb joins fails,Found this issue while running some SMB joins. Attaching test case that causes this error.
HIVE-4501,HS2 memory leak - FileSystem objects in FileSystem.CACHE,"org.apache.hadoop.fs.FileSystem objects are getting accumulated in FileSystem.CACHE, with HS2 in unsecure mode.

As a workaround, it is possible to set fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true.
Users should not have to bother with this extra configuration. 

As a workaround disable impersonation by setting hive.server2.enable.doAs to false."
HIVE-4398,HS2 Resource leak: operation handles not cleaned when originating session is closed,"
In HS2 closing of sessions doesn't lead to closing of all the operation handles that the session had opened. This JIRA is meant to address this issue."
HIVE-4375,Single sourced multi insert consists of native and non-native table mixed throws NPE,"CREATE TABLE src_x1(key string, value string);
CREATE TABLE src_x2(key string, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (""hbase.columns.mapping"" = "":key,cf:string"");

explain
from src a
insert overwrite table src_x1
select key,value where a.key > 0 AND a.key < 50
insert overwrite table src_x2
select key,value where a.key > 50 AND a.key < 100;

throws,

{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.addStatsTask(GenMRFileSink1.java:236)
	at org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.process(GenMRFileSink1.java:126)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:87)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:55)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
	at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:101)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapRedTasks(SemanticAnalyzer.java:8354)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8759)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:279)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
{noformat}"
HIVE-4342,NPE for query involving UNION ALL with nested JOIN and UNION ALL,"UNION ALL query with JOIN in first part and another UNION ALL in second part gives NPE.

bq. JOIN
UNION ALL
bq. UNION ALL

Attachments:
1. HiveCommands.txt : command script to setup schema for query under consideration.
2. sourceData1.txt and sourceData2.txt : required for above command script.
3. Query.txt : Exact query which produces NPE.

NOTE: you will need to update path to sourceData1.txt and sourceData2.txt in the HiveCommands.txt to suit your environment.

Attached files contain the schema and exact query which fails on Hive 0.9.
It is worthwhile to note that the same query executes successfully on Hive 0.7."
HIVE-4317,StackOverflowError when add jar concurrently ,"scenario: multiple thread add jar and do select operation by jdbc concurrently , when hiveserver serializeMapRedWork sometimes, it will throw StackOverflowError from XMLEncoder.
"
HIVE-4154,NPE reading column of empty string from ORC file,"If a String column contains only empty strings, a null pointer exception is throws from the RecordReaderImpl for ORC."
HIVE-4119,ANALYZE TABLE ... COMPUTE STATISTICS FOR COLUMNS fails with NPE if the table is empty,"ANALYZE TABLE ... COMPUTE STATISTICS FOR COLUMNS fails with NPE if the table is empty


{code}
hive -e ""create table empty_table (i int); select compute_stats(i, 16) from empty_table""


java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(PrimitiveObjectInspectorUtils.java:535)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFLongStatsEvaluator.iterate(GenericUDAFComputeStats.java:477)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:139)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1099)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:558)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1132)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:558)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(PrimitiveObjectInspectorUtils.java:535)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFLongStatsEvaluator.iterate(GenericUDAFComputeStats.java:477)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:139)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1099)
	... 15 more
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1132)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:558)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:567)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:193)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:231)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableIntObjectInspector.get(WritableIntObjectInspector.java:35)
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.getInt(PrimitiveObjectInspectorUtils.java:535)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats$GenericUDAFLongStatsEvaluator.iterate(GenericUDAFComputeStats.java:477)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:139)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.closeOp(GroupByOperator.java:1099)
	... 15 more

{code}
"
HIVE-4079,Altering a view partition fails with NPE,"Altering a view partition e.g. to add partition parameters, fails with a null pointer exception in the ObjectStore class.

Currently, this is only possible using the metastore Thrift API and there are no testcases for it."
HIVE-4033,NPE at runtime while selecting virtual column after joining three tables on different keys,This is due to hasVC flag in MapOperator is incorrectly initialized.
HIVE-4029,Hive Profiler dies with NPE,"Steps to reproduce:

{noformat}
$ git clone https://github.com/apache/hive.git hive-profiler-npe
Initialized empty Git repository in /home/brock/hive-profiler-npe/.git/
remote: Counting objects: 73654, done.
remote: Compressing objects: 100% (15383/15383), done.
remote: Total 73654 (delta 44331), reused 71338 (delta 43054)
Receiving objects: 100% (73654/73654), 42.78 MiB | 1.69 MiB/s, done.
Resolving deltas: 100% (44331/44331), done.
$ cd hive-profiler-npe/
$ ant clean package
$ cd build/dist/
$ ./bin/hive
hive> DROP TABLE IF EXISTS users;
hive> CREATE TABLE users (
user string,
passwd string,
uid int,
gid int,
name string,
home string,
shell string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
STORED AS TEXTFILE;

hive> LOAD DATA LOCAL INPATH '/etc/passwd' INTO TABLE users;

hive> set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler;
set hive.exec.operator.hooks=org.apache.hadoop.hive.ql.profiler.HiveProfiler
hive> set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.HiveProfilerResultsHook
hive> SET hive.exec.mode.local.auto=false;
SET hive.exec.mode.local.auto=false
hive> SET hive.task.progress=true;
SET hive.task.progress=true
hive> 
    > select count(1) from users;
select count(1) from users
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201302131617_0022, Tracking URL = http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201302131617_0022
Kill Command = /usr/local/hadoop-1.0.4/libexec/../bin/hadoop job  -kill job_201302131617_0022
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-02-16 10:24:14,215 Stage-1 map = 0%,  reduce = 0%
2013-02-16 10:24:44,354 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201302131617_0022 with errors
Error during job, obtaining debugging information...
Job Tracking URL: http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201302131617_0022
Examining task ID: task_201302131617_0022_m_000002 (and more) from job job_201302131617_0022

Task with the most failures(4): 
-----
Task ID:
  task_201302131617_0022_m_000000

URL:
  http://localhost.localdomain:50030/taskdetails.jsp?jobid=job_201302131617_0022&tipid=task_201302131617_0022_m_000000
-----
Diagnostic Messages for this Task:
java.lang.RuntimeException: Hive Runtime Error while closing operators
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:227)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.profiler.HiveProfilePublisher.publishStat(HiveProfilePublisher.java:85)
	at org.apache.hadoop.hive.ql.profiler.HiveProfiler.close(HiveProfiler.java:110)
	at org.apache.hadoop.hive.ql.exec.Operator.closeOperatorHooks(Operator.java:452)
	at org.apache.hadoop.hive.ql.exec.Operator.close(Operator.java:605)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.close(ExecMapper.java:194)
	... 8 more


FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   HDFS Read: 0 HDFS Write: 0 FAIL
Total MapReduce CPU Time Spent: 0 msec

{noformat}

"
HIVE-3996,Correctly enforce the memory limit on the multi-table map-join,"Currently with HIVE-3784, the joins are converted to map-joins based on checks of the table size against the config variable: hive.auto.convert.join.noconditionaltask.size. 

However, the current implementation will also merge multiple mapjoin operators into a single task regardless of whether the sum of the table sizes will exceed the configured value.
"
HIVE-3927,Potential overflow with new RCFileCat column sizes options,"The uncompressed/compressed sizes of columns may fit into ints for a single block of an RC file, but the same does not hold when they are summed across the file.  Should update the array which aggregates this sum to be an array of longs."
HIVE-3856,Authorization report NPE when table partition do not exits,"the following hql report npe:
use app;select a.name from( select profile['net'] as name from app.app_profile where p_day = 20130103 group by profile['net']) a left outer join app.app_network_mode b on a.name = b.name where b.name is null;

the errors are :
2013-01-04 11:10:05,905 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:625)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:486)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:917)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:689)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:557)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

    If i change the partition condition from ""p_day = 20130103"" to ""p_day = 20121228"" , it works. The ""p_day=20121228"" partition ensure exits ,but the ""p_pay=20130103"" partition do not exit.
    The statement should not report NPE !"
HIVE-3846,alter view rename NPEs with authorization on.,
HIVE-3697,External JAR files on HDFS can lead to race condition with hive.downloaded.resources.dir,"I've seen situations where utilizing JAR files on HDFS can cause job failures via CNFE or JVM crashes. 

This is difficult to replicate, seems to be related to JAR size, latency between client and HDFS cluster, but I've got some example stack traces below. Seems that the calls made to FileSystem (copyToLocal) which are static and will be executed to delete the current local copy can cause the file(s) to be removed during job processing.

We should consider changing the default for hive.downloaded.resources.dir to include some level of uniqueness per job. We should not consider hive.session.id however, as execution of multiple statements via the same user/session which might access the same JAR files will utilize the same session.

A proposal might be to utilize System.nanoTime() -- which might be enough to avoid the issue, although it's not perfect (depends on JVM and system for level of precision) as part of the default (/tmp/${user.name}/resources/System.nanoTime()/). 

If anyone else has hit this, would like to capture environment information as well. Perhaps there is something else at play here. 

Here are some examples of the errors:

for i in {0..2}; do hive -S -f query.q& done
[2] 48405
[3] 48406
[4] 48407
% #
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGBUS (0x7) at pc=0x00007fb10bd931f0, pid=48407, tid=140398456698624
#
# JRE version: 6.0_31-b04
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.6-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libzip.so+0xb1f0]  __int128+0x60
#
# An error report file with more information is saved as:
# /home/.../hs_err_pid48407.log
#
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
java.lang.NoClassDefFoundError: com/example/udf/Lower
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.getUdfClass(FunctionTask.java:105)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.createFunction(FunctionTask.java:75)
        at org.apache.hadoop.hive.ql.exec.FunctionTask.execute(FunctionTask.java:63)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1331)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1117)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:950)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:341)
        at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:439)
        at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:449)
        at org.apache.hadoop.hive.cli.CliDriver.processInitFiles(CliDriver.java:485)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:692)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:208)
Caused by: java.lang.ClassNotFoundException: com.example.udf.Lower
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
        ... 24 more
FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.FunctionTask

Another:
for i in {0..2}; do hive -S -f query.q& done
[1] 16294 
[2] 16295 
[3] 16296 
[]$ Couldn't create directory /tmp/ctm/resources/
Couldn't create directory /tmp/ctm/resources/"
HIVE-3659,TestHiveHistory::testQueryloglocParentDirNotExist Test fails on Windows because of some resource leaks in ZK,"Hive uses ZK for locking. In some test cases, ZK is not behaving well. In thread dumps, I saw it is waiting for locks to be released but they were not getting released. Hive tries to release locks but keeps failing, it eventually times out for its release attempts, which in default settings takes 10 mins. This is also the cause of why some queries take extra-ordinarily long to run. I suggest to disable ZK locking till ZK is certified for windows.

In this test case, I don’t see a requirement to use ZK so I am disabling the HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY to work around the issue.
"
HIVE-3636,Catch the NPe when using ^D to exit from CLI,The exit patch is just a quick hack to catch the NPE in order to allow ^D to exit hive without a stacktrace.
HIVE-3579,Queries in hive-0.8.1-cdh4.0.1 that invoke the Reducer results in Task Failed. The log file shows the ecxception: java.lang.Exception: java.lang.OutOfMemoryError: Java heap space,"hive> select count(*) from table1; 

   
       Total MapReduce jobs = 1    
       Launching Job 1 out of 1
       Number of reduce tasks determined at compile time: 1        
       12/10/15 23:07:02 WARN conf.Configuration: mapred.job.name is deprecated. Instead, use mapreduce.job.name     
       12/10/15 23:07:02 WARN conf.Configuration: mapred.system.dir is deprecated. Instead, use mapreduce.jobtracker.system.dir    
       12/10/15 23:07:02 WARN conf.Configuration: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir   
       12/10/15 23:07:02 WARN conf.HiveConf: hive-site.xml not found on CLASSPATH    
       WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.    
       Execution log at: /tmp/XXXX    
       /XXXX_20121015230707_c93521d0-4a97-4972-92b9-0fdd3ab42e5f.log    
       SLF4J: Class path contains multiple SLF4J bindings.    
       SLF4J: Found binding in [jar:file:/home/XXXX/hadoop-2.0.0-cdh4.0.1/share/hadoop/common/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]    
       SLF4J: Found binding in [jar:file:/home/XXXX/hive-0.8.1-cdh4.0.1/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]    
       SLF4J: See <http://www.slf4j.org/codes.html#multiple_bindings> for an explanation.
       Job running in-process (local Hadoop)     
       Hadoop job information for null: number of mappers: 0; number of reducers: 0    
       2012-10-15 23:07:04,721 null map = 0%,  reduce = 0%    
       Ended Job = job_local_0001 with errors    
       Error during job, obtaining debugging information...    
       **Execution failed with exit status: 2**    
       Obtaining error information    
       **Task failed!**    
       Task ID:    
       Stage-1    
       Logs:    
       /tmp/XXXX/hive.log    
       FAILED: Execution Error, return code 2 from      org.apache.hadoop.hive.ql.exec.MapRedTask  "
HIVE-3481,<Resource leak>: Hiveserver is not closing the existing driver handle before executing the next command. It results in to file handle leaks.,Close the driver object if it exists before creating another driver object. Bunch of HiveServer & JDBC related unit tests are failing because of these file handle leaks.
HIVE-3480,<Resource leak>: Fix the file handle leaks in Symbolic & Symlink related input formats.,Noticed these file handle leaks while fixing the Symlink related unit test failures on Windows.
HIVE-3431,Avoid race conditions while downloading resources from non-local filesystem,"""add resource <remote-uri>"" command downloads the resource file to location specified by conf ""hive.downloaded.resources.dir"" in local file system. But when the command above is executed concurrently to hive-server for same file, some client fails by VM crash, which is caused by overwritten file by other requests.

So there should be a configuration to provide per request location for add resource command, something like ""set hiveconf:hive.downloaded.resources.dir=temporary"""
HIVE-3425,NPE exception in Tests escape1 and escape2.,"Tests are executed with Postgres as metastore. These 2 test
testCliDriver_escape1 and 
testCliDriver_escape2 fails throwing NPE."
HIVE-3408,A race condition is caused within QueryPlan class,"Hive's threads are stopped at HashMap.getEntry(..) that is used within QueryPlan#extractCounters() and QueryPlan#updateCountersInQueryPlan().  It seems that a race condition problem of a HashSet object is caused when extractCounters() and updateCountersInQueryPlan() are concurrently executed.  I hit the problem with Hive 0.7.1 but, I think that it also is caused with 0.8.1.

The problem is reported by several persons on mailing list.
http://mail-archives.apache.org/mod_mbox/hive-dev/201201.mbox/%3CCAKTRiE+3x31FDy+3F0c+jZSXQrhxBgT4DOyfZddm7sdX+cu=Zg@mail.gmail.com%3E

http://mail-archives.apache.org/mod_mbox/hive-user/201202.mbox/%3CFC28CCD9-9C75-4F8D-B272-3D50F663A634@gmail.com%3E

The following is a part of my thread dump.

{quote}
""Thread-1091"" prio=10 tid=0x00007fd17112b000 nid=0x1100 runnable [0x00007fd175f60000]
   java.lang.Thread.State: RUNNABLE
   at java.util.HashMap.getEntry(HashMap.java:347)
   at java.util.HashMap.containsKey(HashMap.java:335)
   at java.util.HashSet.contains(HashSet.java:184)
   at org.apache.hadoop.hive.ql.QueryPlan.extractCounters(QueryPlan.java:342)
   at org.apache.hadoop.hive.ql.QueryPlan.getQueryPlan(QueryPlan.java:419)
   at org.apache.hadoop.hive.ql.QueryPlan.toString(QueryPlan.java:592)
   at org.apache.hadoop.hive.ql.history.HiveHistory.logPlanProgress(HiveHistory.java:493)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.progress(ExecDriver.java:395)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:686)
   at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:123)
   at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:130)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)

""Thread-1090"" prio=10 tid=0x00007fd17012f000 nid=0x10ff runnable [0x00007fd175152000]
   java.lang.Thread.State: RUNNABLE
   at java.util.HashMap.getEntry(HashMap.java:347)
   at java.util.HashMap.containsKey(HashMap.java:335)
   at java.util.HashSet.contains(HashSet.java:184)
   at org.apache.hadoop.hive.ql.QueryPlan.updateCountersInQueryPlan(QueryPlan.java:297)
   at org.apache.hadoop.hive.ql.QueryPlan.getQueryPlan(QueryPlan.java:420)
   at org.apache.hadoop.hive.ql.QueryPlan.toString(QueryPlan.java:592)
   at org.apache.hadoop.hive.ql.history.HiveHistory.logPlanProgress(HiveHistory.java:493)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.progress(ExecDriver.java:395)
   at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:686)
   at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:123)
   at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:130)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
   at org.apache.hadoop.hive.ql.exec.TaskRunner.run(TaskRunner.java:47)
{quote}"
HIVE-3317,Fix “TestDosToUnix” unit tests on Windows by closing the leaking file handle in DosToUnix.java.,Windows can’t delete the files if there are any open file handles on it so it is required to close them properly after completing the validation in DosToUnix utilities.
HIVE-3302,Race condition in query plan for merging at the end of a query,"In the query plan that's used to merge files at the end of a query, the dependency tree looks something like:
                   MoveTask(1)
                  /           \
...ConditionalTask             MoveTask(2)...
                  \           /
                   MergeTask

Here MoveTask(1) moves the partition data to a temporary location, and MoveTask(2) moves it to the final location.

However if there are dynamic partitions generated and some of these partitions are merged and others are moved, the dependency tree is changed at runtime to:
...ConditionalTask           MoveTask(2)...
                  \         /
                   MergeTask
                            \
                             MoveTask(1)

This produces a race condition between the two MoveTasks where if MoveTask(2) runs before MoveTask(1) the partitions moved by MoveTask(1) will get moved to an intermediate location and never moved to the final location.  In this case those partitions are quietly lost."
HIVE-3265,HiveHistory.printRowCount() throws NPE,
HIVE-3232,Resource Leak: Fix the File handle leak in EximUtil.java,"1) Not closing the file handle EximUtil after reading the metadata from the file.
2) Nit: Get the path from URI to handle the Windows paths.
"
HIVE-3225,NPE on a join query with authorization enabled,"when performing a join query which filters by a non-existent partition in the where clause (ie):

select t1.a as a1, t2.a as a2 from t1 join t2 on t1.a=t2.a where t2.part=""non-existent"";

It returns an NPE. It seems that the partition since non-existent is not part of the list of inputs (or maybe optimized out?). But the TableScanOperator still has a reference to it which causes an NPE after tableUsePartLevelAuth.get() returns null.


FAILED: Hive Internal Error: java.lang.NullPointerException(null)java.lang.NullPointerException        at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:617)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:486)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:336)        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:909)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:689)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:557)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
"
HIVE-3205,Bucketed mapjoin on partitioned table which has no partition throws NPE,"{code}
create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;
{code}

throws NPE
{noformat}
2012-06-28 08:59:13,459 ERROR ql.Driver (SessionState.java:printError(400)) - FAILED: NullPointerException null
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer$BucketMapjoinOptProc.process(BucketMapJoinOptimizer.java:269)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.transform(BucketMapJoinOptimizer.java:100)
	at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:87)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7564)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:50)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:245)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:430)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:258)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:215)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:406)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:744)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:607)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:186)
{noformat}"
HIVE-3203,Drop partition throws NPE if table doesn't exist,"ALTER TABLE t1 DROP PARTITION (part = '1');

This throws an NPE if t1 doesn't exist.  A SemanticException would be cleaner."
HIVE-3191,timestamp - timestamp causes null pointer exception,"select tts.rnum, tts.cts - tts.cts from cert.tts tts

Error: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)
SQLState:  42000
ErrorCode: 12

create table if not exists CERT.TTS ( RNUM int , CTS timestamp) 
stored as sequencefile;"
HIVE-3155,Memory leak in Hive,"Start the hive in server mode (Hive 0.9 with hadoop 0.23)
Run the sample application containing all queries 
After running the application for 20 hours ,it is found the each time new DFS client object is getting created .Due to this there are so many objects getting added into ArrayList maintained by LeaseRenewer.This internally leads to Memory leak.

"
HIVE-3154,Potential NPE in ql.metadata.Table.checkValidity(),
HIVE-3098,Memory leak from large number of FileSystem instances in FileSystem.CACHE,"The problem manifested from stress-testing HCatalog 0.4.1 (as part of testing the Oracle backend).

The HCatalog server ran out of memory (-Xmx2048m) when pounded by 60-threads, in under 24 hours. The heap-dump indicates that hadoop::FileSystem.CACHE had 1000000 instances of FileSystem, whose combined retained-mem consumed the entire heap.

It boiled down to hadoop::UserGroupInformation::equals() being implemented such that the ""Subject"" member is compared for equality (""==""), and not equivalence ("".equals()""). This causes equivalent UGI instances to compare as unequal, and causes a new FileSystem instance to be created and cached.

The UGI.equals() is so implemented, incidentally, as a fix for yet another problem (HADOOP-6670); so it is unlikely that that implementation can be modified.

The solution for this is to check for UGI equivalence in HCatalog (i.e. in the Hive metastore), using an cache for UGI instances in the shims.

I have a patch to fix this. I'll upload it shortly. I just ran an overnight test to confirm that the memory-leak has been arrested."
HIVE-3008,Memory leak in TUGIContainingTransport,Identical bug as in THRIFT-1468
HIVE-2933,analyze command throw NPE when table doesn't exists,analyze command throw NPE when table doesn't exists
HIVE-2821,union  with two mapjoin will throw NPE ,"create table src (key string, value string);
create table src1 (key string, value string);

select count(*) from (
select /+mapjoin(b)/ a.*
from src a
join 
src1 b
on a.key=b.key
where a.key=48
union all
select /+mapjoin(bb)/ aa.*
from src aa
join 
src1 bb
on aa.key=bb.key
where aa.key=100
) t;

FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.prune(PartitionPruner.java:156)
at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setTaskPlan(GenMapRedUtils.java:553)
at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.setTaskPlan(GenMapRedUtils.java:514)
at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.initPlan(GenMapRedUtils.java:125)
at org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.process(GenMRRedSink1.java:76)
at org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.process(GenMRRedSink3.java:64)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:55)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.parse.GenMapRedWalker.walk(GenMapRedWalker.java:67)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genMapRedTasks(SemanticAnalyzer.java:6946)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:7247)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:240)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:431)
at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)
at org.apache.hadoop.hive.ql.Driver.run(Driver.java:904)
at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:279)
at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:228)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:417)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:350)
at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:451)
at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:461)
at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)
at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:585)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.RunJar.main(RunJar.java:186)"
HIVE-2800,"NPE in ""create index"" without comment clause in external metastore","This happens only when using external metastore (with --hiveconf hive.metastore.uris=thrift://localhost:8088 --hiveconf hive.metastore.local=false). Also if I gave a comment in the statement, this exception go away.

Here is the statement:
create index test111 on table hcat_test(name) as 'compact' with deferred rebuild;

Here is the stack:
2012-02-10 17:07:42,612 ERROR exec.Task (SessionState.java:printError(380)) - FAILED: Error in metadata: java.lang.NullPointerException
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.metadata.Hive.createIndex(Hive.java:725)
        at org.apache.hadoop.hive.ql.exec.DDLTask.createIndex(DDLTask.java:822)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:231)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1291)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1082)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:933)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:185)
        at org.apache.hadoop.hive.metastore.api.Index.write(Index.java:1032)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$add_index_args.write(ThriftHiveMetastore.java:47518)
        at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.send_add_index(ThriftHiveMetastore.java:1675)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.add_index(ThriftHiveMetastore.java:1666)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createIndex(HiveMetaStoreClient.java:853)
        at org.apache.hadoop.hive.ql.metadata.Hive.createIndex(Hive.java:722)
        ... 17 more"
HIVE-2738,NPE in ExprNodeGenericFuncEvaluator,"Here is the query:
bq. {{SELECT t.lid, '2011-12-12', s_map2json(s_maxmap(UNION_MAP(t.categoryCount), 100)) FROM ( SELECT theme_lid AS theme_lid, MAP(s_host(referer), COUNT( * )) AS categoryCount FROM PageViewEvent WHERE day >= '20130104' AND day <= '20130112' AND date_ >= '2012-01-04' AND date_ < '2012-01-13' AND lid IS NOT NULL GROUP BY lid, s_host(referer) ) t GROUP BY t.lid}}

Removing the call s_map2json make it work but not by removing s_maxmap, but I don't understand what could be wrong with the implementation of my udf. And I don't know how to debug remote hadoop jobs.
"
HIVE-2627,NPE on MAP-JOIN with a UDF in an external JAR,"When a query is converted into a map join, and it depends on some UDF (ADD JAR...; CREATE TEMPORARY FUNCTION...), then an NPE may happen.  Here is an example.

SELECT
    some_udf(dummy1) as dummies
FROM (
    SELECT        
        a.dummy as dummy1,
        b.dummy as dummy2
    FROM        
        test a    
    LEFT OUTER JOIN
        test b
    ON
        a.dummy = b.dummy
) c;

My guess is that the JAR classes are not getting propagated to the hashmapjoin operator."
HIVE-2609,NPE when pruning partitions by thrift method get_partitions_by_filter,"It's a datanucleus bug indeed. 

try this code:
{code}
boolean open = false;
for (int i = 0; i < 5 && !open; ++i) {
  try {
    transport.open();
    open = true;
  } catch (TTransportException e) {
    System.out.println(""failed to connect to MetaStore, re-trying..."");
    try {
      Thread.sleep(1000);
    } catch (InterruptedException ignore) {}
  }
}

try {
  List<Partition> parts =
      client.get_partitions_by_filter(""default"", ""partitioned_nation"",
          ""pt < '2'"", (short) -1);
  for (Partition part : parts) {
    System.out.println(part.getSd().getLocation());
  }
} catch (Exception te) {
  te.printStackTrace();
}
{code}

A NPEexception would be thrown on the thrift server side
{noformat}
11/11/25 13:11:55 ERROR api.ThriftHiveMetastore$Processor: Internal error processing get_partitions_by_filter
java.lang.NullPointerException
        at org.datanucleus.store.mapped.mapping.MappingHelper.getMappingIndices(MappingHelper.java:35)
        at org.datanucleus.store.mapped.expression.StatementText.applyParametersToStatement(StatementText.java:194)
        at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getPreparedStatementForQuery(RDBMSQueryUtils.java:233)
        at org.datanucleus.store.rdbms.query.legacy.SQLEvaluator.evaluate(SQLEvaluator.java:115)
        at org.datanucleus.store.rdbms.query.legacy.JDOQLQuery.performExecute(JDOQLQuery.java:288)
        at org.datanucleus.store.query.Query.executeQuery(Query.java:1657)
        at org.datanucleus.store.rdbms.query.legacy.JDOQLQuery.executeQuery(JDOQLQuery.java:245)
        at org.datanucleus.store.query.Query.executeWithMap(Query.java:1526)
        at org.datanucleus.jdo.JDOQuery.executeWithMap(JDOQuery.java:334)
        at org.apache.hadoop.hive.metastore.ObjectStore.listMPartitionsByFilter(ObjectStore.java:1329)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1241)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$40.run(HiveMetaStore.java:2369)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler$40.run(HiveMetaStore.java:2366)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.executeWithRetry(HiveMetaStore.java:307)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:2366)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions_by_filter.process(ThriftHiveMetastore.j
ava:6099)
        at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor.process(ThriftHiveMetastore.java:4789)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$TLoggingProcessor.process(HiveMetaStore.java:3167)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

A null JavaTypeMapping was passed into org.datanucleus.store.mapped.mapping.MappingHelper.(int initialPosition, JavaTypeMapping mapping), that caused NPE.

After digged into the datanucleus source, I found that the null value was born in the constructor of org.datanucleus.store.mapped.expression.SubstringExpression. see
{code}
    /**
     * Constructs the substring
     * @param str the String Expression
     * @param begin The start position
     * @param end The end position expression
     **/   
    public SubstringExpression(StringExpression str, NumericExpression begin, NumericExpression end)
    {
        super(str.getQueryExpression());

        st.append(""SUBSTRING("").append(str).append("" FROM "")
            .append(begin.add(new IntegerLiteral(qs, mapping, BigInteger.ONE)))
            .append("" FOR "").append(end.sub(begin)).append(')');
    }
{code}

The field mapping hasn't been instanced at that moment.

How do you deal with such a external bug?"
HIVE-2563,OutOfMemory errors when using dynamic partition inserts with large number of partitions,"I'm trying to use dynamic partition inserts to mimic a legacy file generation process that creates a single file per combination of two record attributes, one with a low cardinality, and one with a high degree of cardinality.  In a small data set, I can do this successfully.  Using a larger data set on the same 11 node cluster, with a combined cardinality resulting in ~1600 partitions, I get out of memory errors in the reduce phase 100% of the time.  

I'm running with the following settings, writing to a textfile-backed table with two partitions of type string:

SET hive.exec.compress.output=true; 
SET io.seqfile.compression.type=BLOCK;
SET mapred.max.map.failures.percent=100;
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.max.dynamic.partitions=10000;
SET hive.exec.max.dynamic.partitions.pernode=10000;

(I've also tried gzip compression with the same result)


Here's an example of the error:

2011-11-09 00:51:52,425 INFO org.apache.hadoop.hive.ql.exec.FileSinkOperator: New Final Path: FS hdfs://ec2-50-19-131-121.compute-1.amazonaws.com/tmp/hive-hdfs/hive_2011-11-09_00-48-57_840_6003656718210084497/_tmp.-ext-10000/requestday=2011-09-29/clientname=XXXX-JA/000008_0.deflate
2011-11-09 00:51:52,461 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
2011-11-09 00:51:52,464 FATAL org.apache.hadoop.mapred.Child: Error running child : java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:640)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2931)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:544)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:219)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:584)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:565)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:472)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:464)
	at org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat.getHiveRecordWriter(HiveIgnoreKeyTextOutputFormat.java:80)
	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:247)
	at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:235)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:458)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutWriters(FileSinkOperator.java:599)
	at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:539)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.forward(GroupByOperator.java:959)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:798)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:724)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:469)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:270)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)
	at org.apache.hadoop.mapred.Child.main(Child.java:264)"
HIVE-2403,Possible chance of getting null pointer exception in these packages,"While reviewing the code we observed possible chance of getting null pointer exception in these packages

1) MetaStoreUtils.java
2) ExprProcFactory.java
3) GenMRTableScan1.java
4) GenMRUnion1.java
5) GenericUDFOpNotNull.java
6) GenericUDFOpNull.java
7) ColumnarStruct.java
8) thrift_grammar.java"
HIVE-2402,Function like with empty string is throwing null pointer exception,"select emp.ename from emp where ename like ''
This query is throwing null pointer exception"
HIVE-2345,NPE in reducer when using cross join with order by in sub select.,"When running the following query:
select  * from 
(select * from temp_table order by (id)) t
Join
(select * from temp_table) b


We get the exceptions below:

In stage 1:
2011-08-03 23:54:33,214 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.RuntimeException: Error in configuring object
at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)
at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)
at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:431)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)
at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)
... 9 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:144)
... 14 more
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.ExecReducer.configure(ExecReducer.java:129)
... 14 more
2011-08-03 23:54:33,217 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task


In stage 2:
2011-08-03 23:55:18,905 INFO org.apache.hadoop.hive.ql.exec.ExtractOperator: Initialization Done 3 OP
2011-08-03 23:55:18,908 FATAL ExecReducer: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.startGroup(CommonJoinOperator.java:341)
at org.apache.hadoop.hive.ql.exec.Operator.startGroup(Operator.java:489)
at org.apache.hadoop.hive.ql.exec.Operator.startGroup(Operator.java:489)
at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:213)
at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)
at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
at org.apache.hadoop.mapred.Child.main(Child.java:262)

2011-08-03 23:55:18,911 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1
2011-08-03 23:55:18,913 WARN org.apache.hadoop.mapred.Child: Error running child
java.lang.RuntimeException: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:268)
at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)
at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: java.lang.NullPointerException
at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.startGroup(CommonJoinOperator.java:341)
at org.apache.hadoop.hive.ql.exec.Operator.startGroup(Operator.java:489)
at org.apache.hadoop.hive.ql.exec.Operator.startGroup(Operator.java:489)
at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:213)
... 7 more
2011-08-03 23:55:18,916 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task


The query succeeds when:

We add the mapjoin :

select /*+ MAPJOIN(b) */  * from 
(select * from temp_table order by (id)) t
Join
(select * from temp_table) b

or 


select /*+ MAPJOIN(t) */  * from 
(select * from temp_table order by (id)) t
Join
(select * from temp_table) b


Or use equi join:

select   * from 
(select * from temp_table order by (id)) t
Join
(select * from temp_table) b
on (b.id=t.id)

Note: The size of the table temp_table doesn't matter.


"
HIVE-2341,SemanticAnalyzer.genJoinReduceSinkChild fails with NPE unless you specify an un-necessary table alias,"Following query:

        select * from
        V_EMF_LEARNING_ACTUAL_CHURN
        join
        (select count(*) as total from standard_connections) b

fails with the NPE listed below. It doesn't if you add an alias to the table like so:

        select * from
        V_EMF_LEARNING_ACTUAL_CHURN a
        join
        (select count(*) as total from standard_connections) b

The exception I'm getting:

java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinReduceSinkChild(SemanticAnalyzer.java:4440)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinOperator(SemanticAnalyzer.java:4545)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinPlan(SemanticAnalyzer.java:4704)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6011)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6603)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)

"
HIVE-2334,DESCRIBE TABLE causes NPE when hive.cli.print.header=true,
HIVE-2182,Avoid null pointer exception when executing UDF,"For using UDF's executed following steps

{noformat}
add jar /home/udf/udf.jar;
create temporary function grade as 'udf.Grade';
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;
{noformat}

But from the above steps if we miss the first step (add jar) and execute remaining steps

{noformat}
create temporary function grade as 'udf.Grade';
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;
{noformat}

In tasktracker it is throwing this exception
{noformat}
Caused by: java.lang.RuntimeException: Map operator initialization failed
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
		 ... 18 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
		 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:126)
		 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:878)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:904)
		 at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
		 at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
		 at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:444)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98)
		 ... 18 more
Caused by: java.lang.NullPointerException
		 at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107)
		 ... 31 more
{noformat}
Instead of null pointer exception it should throw meaning full exception"
HIVE-2152,Hive throws an NPE if hive-default.xml.,"If you don't have hive-default.xml in your classpath, you get the following error when you try to show tables in the the hive shell:

hive> show tables; 
FAILED: Error in metadata: java.lang.NullPointerException 
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask

It would be a lot more useful to print message indicating that hive-default.xml. This problem will become mute if HIVE-1530 gets accepted."
HIVE-2060,CLI local mode hit NPE when exiting by ^D,CLI gets an NPE when running in local mode and hit an ^D to exit it. 
HIVE-2045,TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() throws Null Pointer Exception in some cases,"1) In TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() is doing null check for the tokenizer.
If tokenizer is null, fillTokenizer() method is called to get the tokenizer object. But fillTokenizer() method also can update the tokenizer with NULL , so NULL check should be done before using the tokenizer.

2) Also improved some logging in TCTLSeparatedProtocol.java"
HIVE-2031,Correct the exception message for the better traceability for the scenario load into the partitioned table having 2  partitions by specifying only one partition in the load statement. ," Load into the partitioned table having 2 partitions by specifying only one partition in the load statement is failing and logging the following exception message.

{noformat}
 org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)
	at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{noformat}

This needs to be corrected in such a way what is the actual root cause for this."
HIVE-2009,Crtrl+D cause CLI throw NPE,"in HIVE CLI,enter Ctrl+D,it should exit the CLI,but throws NPE.

hive> Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.hive.cli.CliSessionState.close(CliSessionState.java:106)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:523)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
HIVE-1959,Potential memory leak when same connection used for long time. TaskInfo and QueryInfo objects are getting accumulated on executing more queries on the same connection.,*org.apache.hadoop.hive.ql.history.HiveHistory$TaskInfo* and *org.apache.hadoop.hive.ql.history.HiveHistory$QueryInfo* these two objects are getting accumulated on executing more number of queries on the same connection. These objects are getting released only when the connection is closed.
HIVE-1908,FileHandler leak on partial iteration of the resultset. ,"If the ""resultset"" is not iterated completely ,  one filehandler is leaking

Ex: We need only first row. This case one resource is leaking

{code}

ResultSet resultSet = createStatement.executeQuery(""select * from sampletable"");

if (resultSet.next())
{
	System.out.println(resultSet.getString(1)+""   ""+resultSet.getString(2));
} 

{code}


Command used for checking the filehandlers
{code}
lsof -p {hive_process_id} > runjarlsof.txt
{code}

"
HIVE-1884,Potential risk of resource leaks in Hive,"h3.There are couple of resource leaks.
h4.For example,

In CliDriver.java, Method :- processReader() the buffered reader is not closed.

h3.Also there are risk(s) of  resource(s) getting leaked , in such cases we need to re factor the code to move closing of resources in finally block.

h4. For Example :- 

In Throttle.java   Method:- checkJobTracker() , the following code snippet might cause resource leak.

{code}
InputStream in = url.openStream();
in.read(buffer);
in.close();
{code}


Ideally and as per the best coding practices it should be like below

{code}

InputStream in=null;
try   {
        in = url.openStream();
        int numRead = in.read(buffer);
}
finally {
       IOUtils.closeStream(in);
}

{code}

Similar cases, were found in ExplainTask.java, DDLTask.java etc.Need to re factor all such occurrences.


"
HIVE-1618,Expression in sort by throws NPE,"A query such as:
SELECT count(1) FROM src SORT BY count(1);

Will throw a null pointer exception:

2010-09-07 15:27:27,059 ERROR ql.Driver (SessionState.java:printError(277)) - FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:587)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:708)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6418)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:4029)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:5148)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5661)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6229)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:240)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:326)
"
HIVE-1553,NPE when using complex string UDF,"When executing this query: {code}select explode(split(city, """")) as char from users;{code} I get NPE: {code}java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.process(GenericUDTFExplode.java:70)
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.processOp(UDTFOperator.java:98)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:81)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:43)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:347)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:171)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:170){code}
But in case of this query:{code}select explode(split(city, """")) as char from users where id = 234234;{code} NPE does not occur, but in case of this query: {code}select explode(split(city, """")) as char from users where id > 0;{code}  Some mappers succed, but most of them fails, so whole task fails.
city is a string column and maximum users.id is about 30M.

I have run another query:{code}select explode(split(city, """")) as char from users where city is not null;{code}
and now the error I get is:{code}org.apache.hadoop.hive.ql.metadata.HiveException: UDTF's should not output rows on close
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.forwardUDTFOutput(UDTFOperator.java:111)
	at org.apache.hadoop.hive.ql.udf.generic.UDTFCollector.collect(UDTFCollector.java:40)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTF.forward(GenericUDTF.java:81)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode.process(GenericUDTFExplode.java:72)
	at org.apache.hadoop.hive.ql.exec.UDTFOperator.processOp(UDTFOperator.java:98)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:81)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:73)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:73)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:43)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:386)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:598)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:347)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:171)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)
	at org.apache.hadoop.mapred.Child.main(Child.java:170){code}"
HIVE-1547,Unarchiving operation throws NPE,"Unarchiving a partition throws a null pointer exception similar to the following:

2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)

This error seems to be DFS specific, as local file system in the unit tests don't catch this."
HIVE-1320,NPE with lineage in a query of union alls on joins.,"The following query generates a NPE in the lineage ctx code

EXPLAIN
INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

The stack trace is:

FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
at org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx$Index.mergeDependency(LineageCtx.java:116)
at org.apache.hadoop.hive.ql.optimizer.lineage.OpProcFactory$UnionLineage.process(OpProcFactory.java:396)
at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:59)
at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
at org.apache.hadoop.hive.ql.optimizer.lineage.Generator.transform(Generator.java:72)
at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:83)
at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5976)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:48)
at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
"
HIVE-1316,Increase the memory limit for CLI client,"Hive CLI client stores the plan in memory and serialize it to XML file. the serialization takes a lot of memory. for large queries, we've seen OOM exception and GC overhead limit reached. It would be good to increase the max heap size of CLI client and disable GC overhead limit. "
HIVE-1308,<boolean> = <boolean> throws NPE,"Workaround is to just use <boolean> or NOT <boolean>

{code}
hive> select true=true from src;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils$ConversionHelper.<init>(GenericUDFUtils.java:212)
        at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:138)
        at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:587)
        at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:708)
        at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:128)
        at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6136)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1831)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:1663)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:4911)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:5421)
        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5952)
        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:126)
        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:304)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:377)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:303)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
{code}"
HIVE-1188,NPE when running TestJdbcDriver/TestHiveServer,"{noformat}
% ant test -Dtestcase=TestJdbcDriver

BUILD FAILED
/Users/carl/Projects/hive/hd11/hive/build.xml:154: The following error occurred while executing this line:
/Users/carl/Projects/hive/hd11/hive/build.xml:93: The following error occurred while executing this line:
/Users/carl/Projects/hive/hd11/hive/contrib/build.xml:77: java.lang.NullPointerException
	at java.util.Arrays$ArrayList.<init>(Arrays.java:3357)
	at java.util.Arrays.asList(Arrays.java:3343)
	at org.apache.hadoop.hive.ant.QTestGenTask.execute(QTestGenTask.java:248)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:390)
	at org.apache.tools.ant.Target.performTasks(Target.java:411)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360)
	at org.apache.tools.ant.helper.SingleCheckExecutor.executeTargets(SingleCheckExecutor.java:38)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1212)
	at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:441)
	at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:302)
	at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:221)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:68)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.MacroInstance.execute(MacroInstance.java:398)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:390)
	at org.apache.tools.ant.Target.performTasks(Target.java:411)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360)
	at org.apache.tools.ant.Project.executeTarget(Project.java:1329)
	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1212)
	at org.apache.tools.ant.Main.runBuild(Main.java:801)
	at org.apache.tools.ant.Main.startAnt(Main.java:218)
	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)
{noformat}

TestHiveServer throws the same error.
"
HIVE-1139,GroupByOperator sometimes throws OutOfMemory error when there are too many distinct keys,"When a partial aggregation performed on a mapper, a HashMap is created to keep all distinct keys in main memory. This could leads to OOM exception when there are too many distinct keys for a particular mapper. A workaround is to set the map split size smaller so that each mapper takes less number of rows. A better solution is to use the persistent HashMapWrapper (currently used in CommonJoinOperator) to spill overflow rows to disk. "
HIVE-1097,groupby_bigdata.q sometimes throws out of memory exception,"I would get out of memory errors like the following when running groupby_bigdata.q.

{code}
  
    [junit] plan = /data/users/pyang/task2/trunk/VENDOR.hive/trunk/build/ql/scratchdir/plan38413.xml
    [junit] Exception in thread ""Thread-15"" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.util.Arrays.copyOf(Arrays.java:2882)
    [junit]     at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
    [junit]     at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
    [junit]     at java.lang.StringBuffer.append(StringBuffer.java:224)
    [junit]     at java.io.StringWriter.write(StringWriter.java:84)
    [junit]     at java.io.PrintWriter.newLine(PrintWriter.java:436)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:585)
    [junit]     at java.io.PrintWriter.println(PrintWriter.java:696)
    [junit]     at java.lang.Throwable.printStackTrace(Throwable.java:512)
    [junit]     at org.apache.hadoop.util.StringUtils.stringifyException(StringUtils.java:60)
    [junit]     at org.apache.hadoop.hive.ql.exec.ScriptOperator$StreamThread.run(ScriptOperator.java:561)
    [junit] Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
    [junit]     at java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:39)
    [junit]     at java.nio.CharBuffer.allocate(CharBuffer.java:312)
    [junit]     at java.nio.charset.CharsetEncoder.isLegalReplacement(CharsetEncoder.java:319)
    [junit]     at java.nio.charset.CharsetEncoder.replaceWith(CharsetEncoder.java:267)
    [junit]     at java.nio.charset.CharsetEncoder.<init>(CharsetEncoder.java:186)
    [junit]     at java.nio.charset.CharsetEncoder.<init>(CharsetEncoder.java:209)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.<init>(ISO_8859_1.java:116)
    [junit]     at sun.nio.cs.ISO_8859_1$Encoder.<init>(ISO_8859_1.java:113)
    [junit]     at sun.nio.cs.ISO_8859_1.newEncoder(ISO_8859_1.java:46)
    [junit]     at java.lang.StringCoding$StringEncoder.<init>(StringCoding.java:215)
    [junit]     at java.lang.StringCoding$StringEncoder.<init>(StringCoding.java:207)
    [junit]     at java.lang.StringCoding.encode(StringCoding.java:266)
    [junit]     at java.lang.String.getBytes(String.java:947)
    [junit]     at java.io.UnixFileSystem.getLength(Native Method)
    [junit]     at java.io.File.length(File.java:848)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem$RawLocalFileStatus.<init>(RawLocalFileSystem.java:375)
    [junit]     at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:359)
    [junit]     at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)
    [junit]     at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:643)
    [junit]     at org.apache.hadoop.hive.ql.exec.Utilities.clearMapRedWork(Utilities.java:114)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:680)
    [junit]     at org.apache.hadoop.hive.ql.exec.ExecDriver.main(ExecDriver.java:936)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
    [junit] Traceback (most recent call last):
    [junit]   File ""../data/scripts/dumpdata_script.py"", line 6, in <module>
    [junit]     print 20000 * i + k

{code}"
HIVE-1065,"When data is large,a reducer's memory can not hold the big data by using list to store the data, and get an error ""out of memory""","When data is large,I do a ""join"" operation ,a reducer's memory can not hold the big data by using list to store the data, and get an error ""out of memory"""
HIVE-1064,NPE when operating HiveCLI in distributed mode,"{code}
hive> select id, name from tab_a;
select id, name from tab_a;
10/01/18 03:55:59 INFO parse.ParseDriver: Parsing command: select id, name from tab_a
10/01/18 03:55:59 INFO parse.ParseDriver: Parse Completed
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Completed phase 1 of Semantic Analysis
10/01/18 03:55:59 INFO parse.SemanticAnalyzer: Get metadata for source tables
10/01/18 03:55:59 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
10/01/18 03:55:59 INFO metastore.ObjectStore: ObjectStore, initialize called
10/01/18 03:56:03 INFO metastore.ObjectStore: Initialized ObjectStore
10/01/18 03:56:03 INFO metastore.HiveMetaStore: 0: get_table : db=default tbl=tab_a
10/01/18 03:56:03 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for subqueries
10/01/18 03:56:03 INFO parse.SemanticAnalyzer: Get metadata for destination tables
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for FS(2)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for SEL(1)
10/01/18 03:56:04 INFO ppd.OpProcFactory: Processing for TS(0)
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO hive.log: DDL: struct tab_a { i32 id, string file, string name}
10/01/18 03:56:04 INFO parse.SemanticAnalyzer: Completed plan generation
10/01/18 03:56:04 INFO ql.Driver: Semantic Analysis Completed
10/01/18 03:56:04 INFO ql.Driver: Starting command: select id, name from tab_a
Total MapReduce jobs = 1
10/01/18 03:56:04 INFO ql.Driver: Total MapReduce jobs = 1
Launching Job 1 out of 1
10/01/18 03:56:04 INFO ql.Driver: Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
10/01/18 03:56:04 INFO exec.ExecDriver: Number of reduce tasks is set to 0 since there's no reduce operator
FAILED: Unknown exception : null
10/01/18 03:56:04 ERROR ql.Driver: FAILED: Unknown exception : null
java.lang.NullPointerException
	at org.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:288)
	at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:475)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:103)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:64)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:589)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:469)
	at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:329)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:317)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:155)
	at org.apache.hadoop.mapred.JobShell.run(JobShell.java:54)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.mapred.JobShell.main(JobShell.java:68)

hive> 
{code}"
HIVE-878,Update the hash table entry before flushing in Group By hash aggregation,"This is a newly introduced bug from r796133.
We should first update the aggregation, and then we can flush the hash table. Otherwise the entry that we update might be already out of the hash table.
"
HIVE-876,UDFOPNegative should deal with NULL gracefully,"UDFOPNegative is throwing out NullPointerException. It should return NULL for that.
"
HIVE-828,StackOverflowError When Creating Thirft Table,"When I try to create a thrift based table Hive goes into an infinite loop, and eventually overflows the stack:

hive> create table dxmsg
    > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.ThriftDeserializer'
    > WITH SERDEPROPERTIES (
    > ""serialization.class"" = ""*******REMOVED********"",
    > ""serialization.format"" = ""com.facebook.thrift.protocol.TBinaryProtocol""
    > ) STORED AS SEQUENCEFILE;
Exception in thread ""main"" java.lang.StackOverflowError
	at sun.reflect.generics.tree.TypeVariableSignature.make(TypeVariableSignature.java:19)
	at sun.reflect.generics.parser.SignatureParser.parseTypeVariableSignature(SignatureParser.java:329)
	at sun.reflect.generics.parser.SignatureParser.parseFieldTypeSignature(SignatureParser.java:230)
	at sun.reflect.generics.parser.SignatureParser.parseTypeArgument(SignatureParser.java:319)
	at sun.reflect.generics.parser.SignatureParser.parseTypeArguments(SignatureParser.java:284)
	at sun.reflect.generics.parser.SignatureParser.parseSimpleClassTypeSignature(SignatureParser.java:260)
	at sun.reflect.generics.parser.SignatureParser.parseClassTypeSignatureSuffix(SignatureParser.java:270)
	at sun.reflect.generics.parser.SignatureParser.parseClassTypeSignature(SignatureParser.java:244)
	at sun.reflect.generics.parser.SignatureParser.parseFieldTypeSignature(SignatureParser.java:228)
	at sun.reflect.generics.parser.SignatureParser.parseTypeSignature(SignatureParser.java:359)
	at sun.reflect.generics.parser.SignatureParser.parseTypeSig(SignatureParser.java:157)
	at sun.reflect.generics.repository.FieldRepository.parse(FieldRepository.java:34)
	at sun.reflect.generics.repository.FieldRepository.parse(FieldRepository.java:24)
	at sun.reflect.generics.repository.AbstractRepository.<init>(AbstractRepository.java:56)
	at sun.reflect.generics.repository.FieldRepository.<init>(FieldRepository.java:30)
	at sun.reflect.generics.repository.FieldRepository.make(FieldRepository.java:48)
	at java.lang.reflect.Field.getGenericInfo(Field.java:85)
	at java.lang.reflect.Field.getGenericType(Field.java:223)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspectorNoCache(ObjectInspectorFactory.java:149)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getReflectionObjectInspector(ObjectInspectorFactory.java:69)
.................



I have left out my thrift struct definition. I can say that it is quite simple and flat (No nested messages, no recursive definitions, not even any containers). "
HIVE-575,map join runs out of memory for a key with very large number of values,
HIVE-535,Memory-efficient hash-based Aggregation,"Currently there are a lot of memory overhead in the hash-based aggregation in GroupByOperator.
The net result is that GroupByOperator won't be able to store many entries in its HashTable, and flushes frequently, and won't be able to achieve very good partial aggregation result.

Here are some initial thoughts (some of them are from Joydeep long time ago):

A1. Serialize the key of the HashTable. This will eliminate the 16-byte per-object overhead of Java in keys (depending on how many objects there are in the key, the saving can be substantial).
A2. Use more memory-efficient hash tables - java.util.HashMap has about 64 bytes of overhead per entry.
A3. Use primitive array to store aggregation results. Basically, the UDAF should manage the array of aggregation results, so UDAFCount should manage a long[], UDAFAvg should manage a double[] and a long[]. The external code should pass an index to iterate/merge/terminal an aggregation result. This will eliminate the 16-byte per-object overhead of Java.

More ideas are welcome."
HIVE-435,Empty passwd param causing NPE in ExecDriver,HIVE-403 can cause NPE if the password param is empty. 
HIVE-320,Issuing queries with COUNT(DISTINCT) on a column that may contain null values hits a NPE,"When issuing a query that may contain a null value, I get a NPE. 

E.g. if 'middle_name' potentially holds null values,

select count(distinct middle_name) from people; will fail with the below exception.

Other queries that work with the same input set:
select distinct middle_name from people;
select count(1), middle_name from people group by middle_name;

java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:169)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:318)
	at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2198)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:424)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:164)
	... 2 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:376)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:477)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.process(GroupByOperator.java:420)
	... 3 more

"
