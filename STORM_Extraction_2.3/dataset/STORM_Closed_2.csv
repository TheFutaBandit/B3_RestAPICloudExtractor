Bug_ID,Bug_Summary,Bug_Description
STORM-4024,Bolt Input Stats are blank if topology.acker.executors is null or 0,"On StormUI (and via API) the bolt Input Stats do not work when topology.acker.executors is null or 0 (see attachements showing difference with and without ackers)

Also, some of the per-bolt instance Executed and latency fields are also not working"
STORM-4002,Security Vulnerability - Action Required: “Incorrect Permission Assignment for Critical Resource” vulnerability in some components of  org.apache.storm," I think the method org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache) may have an “Incorrect Permission Assignment for Critical Resource”vulnerability which is vulnerable in in some components of  org.apache.storm. It shares similarities to a recent CVE disclosure _CVE-2017-3166_ in the project _""apache/hadoop""_ project. The influencing components are listed below:
 # org.apache.storm:storm-kafka-examples in the versions between 1.1.0 and 1.2.4.
 # org.apache.storm:storm-starter in the versions of 1.1.2-1.1.3 and 1.2.0-1.2.2

The source vulnerability information is as follows: !https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.1&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ_bBS_0CMiL9kNUgnr95IJelNJAQJp906nnAonpFswrxMbSt1EVV1S2q6kq_ur-YE-1H49gOCjMGqFYtm5xBOS_EBOZci8ukIw2Hn8kM-9OIKVIxXrlhcRm6LA&disp=emb&realattid=ii_lmt56kbv0|width=1,height=1!!https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.2&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ-8wPNUdQ35WBKaadck2X1lP34blTQ_qiyhu5T7l0G8T4cboSCiFNgfxaCQZZsK-Pm3ebzj4JSWBs558OxWHJPM1uJqKlMvPMhpx9J0TiojhC85DNqeLu3dr2Q&disp=emb&realattid=ii_lmt6415i0|width=1,height=1!!https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.0.1&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ9XERxykP1zaB9Codaz3lisQ9gKwLHXnEIHP4p4oUcINmdFEWTJAWeDMfayncBsWIBj_kc2cAKHx4c7InMtKL98nDb2Dnt3TpfGLQCcJhdFsSBhemVA14CI0rA&disp=emb&realattid=ii_loxzzieb0|width=1,height=1!

*Vulnerability Detail:*

*CVE Identifier:* CVE-2017-3166

{*}Description{*}: In Apache Hadoop versions 2.6.1 to 2.6.5, 2.7.0 to 2.7.3, and 3.0.0-alpha1, if a file in an encryption zone with access permissions that make it world readable is localized via YARN's localization mechanism, that file will be stored in a world-readable location and can be shared freely with any application that requests to localize that file.

*Reference:*[ |http://goog_608275719/] [https://nvd.nist.gov/vuln/detail/CVE-2017-3166]

{*}Patch{*}: [https://github.com/apache/hadoop/commit/a47d8283b136aab5b9fa4c18e6f51fa799d91a29]
*Vulnerability Description:* The vulnerability is present in the class  org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager  of method  checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache)  , which is responsible for checking the permissions of other files in the distributed cache.. {*}But t{*}{*}he check snippet is similar to the vulnerable snippet for CVE-2017-3166{*} and may have the same consequence as CVE-2017-3166: {*}a file in an encryption zone with access permissions  will be stored in a world-readable location and can be freely shared with any application that requests the file to be localized{*}. Therefore, maybe you need to fix the vulnerability with much the same fix code as the CVE-2017-3166 patch. 
    Considering the potential risks it may have, I am willing to cooperate with you to verify, address, and report the identified vulnerability promptly through responsible means. If you require any further information or assistance, please do not hesitate to reach out to me. Thank you and look forward to hearing from you soon.
 "
STORM-3767,NPE on getComponentPendingProfileActions ,"When a topology is newly submitted, if the scheduling loop takes too long, the component UI might have error 500.

This is due to the NPE in nimbus code. An example:

1. When a scheduling loop finishes, nimbus will eventually update the assignmentsBackend. if a topology is newly submitted, its entry will be added to the idToAssignment map, otherwise, the entry will be updated with new assignments. The key point is the new topology Id doesn't exist in idToAssignment before it reaching here.

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2548-L2549
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java#L696
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/assignments/InMemoryAssignmentBackend.java#L63-L64

2. However, this assignmentsBackend update only started to happen at 2021-04-23 15:30:14.299


{code:java}
2021-04-23 15:30:14.299 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology
{code}

while this topology topo1-52-1619191499 has been scheduled at 2021-04-23 15:25:13.887. The scheduling loop took longer than 5mins.


{code:java}
2021-04-23 15:25:13.887 o.a.s.s.Cluster timer [INFO] STATUS - topo1-52-1619191499 Running - Fully Scheduled by DefaultResourceAwareStrategy (1297 states traversed in 1275 ms, backtracked 0 times)
other topologies were taking long time

2021-04-23 15:25:14.378 o.a.s.s.Cluster timer [INFO] STATUS - topo2-76-1612842912 Running - Fully Scheduled by DefaultResourceAwareStrategy (111 states traversed in 34 ms, backtracked 0 times)
...
2021-04-23 15:30:14.192 o.a.s.s.Cluster timer [INFO] STATUS - TrendingNowLES-11-1611713968 Not enough resources to schedule after evicting lower priority topologies. Additional Memory Required: 20128.0 MB (Available: 5411178.0 MB). Additional CPU Required: 1010.0% CPU (Available: 3100.0 % CPU).Cannot schedule by DefaultResourceAwareStrategy (65644 states traversed in 299804 ms, backtracked 65555 times, 89 of 150 executors scheduled)
...
2021-04-23 15:30:14.216 o.a.s.s.Cluster timer [INFO] STATUS - evaluateplus-dev-47-1605825401 Running - Fully Scheduled by GenericResourceAwareStrategy (41 states traversed in 10 ms, backtracked 0 times)
{code}

3. During this period, the idToAssignment map in assignmentsBackend wouldn't have the entry for topo1-52-1619191499, so when a component UI was visited,

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L3613-L3614
https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L3100
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java#L194
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/assignments/InMemoryAssignmentBackend.java#L69

it got a null value as the assignment, and hence NPE.

This can be produced easily by adding some sleep anywhere between 

{code:title=Nimbus.java}
            Map<String, SchedulerAssignment> newSchedulerAssignments =
                    computeNewSchedulerAssignments(existingAssignments, topologies, bases, scratchTopoId);
{code}

and
{code:title=Nimbus.java}
 state.setAssignment(topoId, assignment, td.getConf());
{code}

and submit a new topology and visit its component UI 
"
STORM-3765,NPE in DRPCSimpleACLAuthorizer.readAclFromConfig when drpc.authorizer.acl has no values,"When drpc.authorizer.acl has no values, for example:

{code:java}
-bash-4.2$ cat  drpc-auth-acl.yaml
drpc.authorizer.acl:
{code}

DRPCSimpleACLAuthorizer will have NPE
{code:java}
2021-04-22 15:22:48.795 o.a.s.t.ProcessFunction pool-9-thread-1 [ERROR] Internal error processing fetchRequest
java.lang.NullPointerException: null
        at org.apache.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer.readAclFromConfig(DRPCSimpleACLAuthorizer.java:59) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer.permitClientOrInvocationRequest(DRPCSimpleACLAuthorizer.java:108) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer.permitInvocationRequest(DRPCSimpleACLAuthorizer.java:150) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.authorizer.DRPCAuthorizerBase.permit(DRPCAuthorizerBase.java:51) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPC.checkAuthorization(DRPC.java:130) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPC.checkAuthorizationNoLog(DRPC.java:143) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPC.fetchRequest(DRPC.java:192) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPCThrift.fetchRequest(DRPCThrift.java:42) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.generated.DistributedRPCInvocations$Processor$fetchRequest.getResult(DistributedRPCInvocations.java:393) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.generated.DistributedRPCInvocations$Processor$fetchRequest.getResult(DistributedRPCInvocations.java:372) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:152) [storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_262]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Th
{code}
"
STORM-3763,Backpressure message ignored by the receiver caused the topology to not progress,"We have noticed a case where topology is stuck due to the mis-interpretation of backpressure messge:

At beginning, the topology ran fine but a downstream component had backpressure, so it sent backpressure signal to its upstream component, and the upstream component paused sending data to the downstream bolt.
Then the downstream component restarted (due to any reason, for example, killed by supervisor due to heartbeat timeout). When it came back up, it sends backpressure message to the upstream bolt. However, the upstream component didn't know how to interpret the backpressure message so it logs the below error and ignores the message.  
{code:java}
2021-01-28 19:41:37.175 o.a.s.m.n.SaslStormClientHandler client-worker-1 [ERROR] Unexpected message from server: {worker=4c38160a-3c66-4eff-8572-2d0c493bd6c1, bpStatusId=254, bpTasks=[], nonBpTasks=[546, 790, 863]}
{code}

Then the downstream component will not receive any data from the upstream component, so it won't have any backpressure (since no data is sent to it), hence it won't send any backpressure update message to the upstream component.  This leads to a dead situation that the upstream component thinks the downstream has backpressure so it paused sending data to it, while the downstream doesn't have backpressure but can't receive any data from upstream. The topology is stuck because of it.


Let's look at the code:

When the connection between the downstream (server) and upstream (client) is established,
server invokes

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/StormServerHandler.java#L39-L41

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java#L237

which sends backpressure messages to the client.

This is because in this pipeline, ""StormServerHandler"" is the only one with that implemented ""channelActive()"" method.

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/StormServerPipelineFactory.java#L56

However, the Client side expects authentication messages.

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/SaslStormClientHandler.java#L70-L75

so the client can't interpret the backpressure message at the beginning, hence ""unexpected message"".

This can be supported with an example. I have a wordcount topology running. At the startup, the client tries to connect to the server. Once connected, it sends a ""SASL_TOKEN_MESSAGE_REQUEST"".

client log

{code:java}
021-01-29 19:03:21.355 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] SASL credentials for storm topology wc is -8603731884381183101:-9091319821854384981
2021-01-29 19:03:21.359 o.a.s.m.n.Client client-worker-1 [DEBUG] successfully connected to openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702, [id: 0x29da2e9c, L:/10.215.73.209:45870 - R:openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702] [attempt 12]
2021-01-29 19:03:21.359 o.a.s.m.n.SaslStormClientHandler client-worker-1 [INFO] Connection established from /10.215.73.209:45870 to openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702
...
2021-01-29 19:03:21.362 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] Creating saslNettyClient now for channel: [id: 0x29da2e9c, L:/10.215.73.209:45870 - R:openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702]
2021-01-29 19:03:21.363 o.a.s.m.n.SaslNettyClient client-worker-1 [DEBUG] SaslNettyClient: Creating SASL DIGEST-MD5 client to authenticate to server
2021-01-29 19:03:21.368 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] Sending SASL_TOKEN_MESSAGE_REQUEST

...

2021-01-29 19:03:21.632 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] send/recv time (ms): 277
2021-01-29 19:03:21.633 o.a.s.m.n.SaslStormClientHandler client-worker-1 [ERROR] Unexpected message from server: {worker=cdf6f963-678c-45a4-91d2-e1067a9a8516, bpStatusId=1, bpTasks=[], nonBpTasks=[17, 1, 18, 3, 4, 22, 7, 8, 9, 12,
13]}
{code}

But the server sends the backpressure message first, before it deals with the SASL_TOKEN_MESSAGE_REQUEST message

server log

{code:java}
2021-01-29 19:03:21.473 o.a.s.m.n.SaslStormServerHandler Netty-server-localhost-6702-worker-1 [DEBUG] SASL credentials for storm topology wc is -8603731884381183101:-9091319821854384981
2021-01-29 19:03:21.482 o.a.s.u.Utils main [DEBUG] Using storm.yaml from resources
2021-01-29 19:03:21.490 o.a.s.d.w.WorkerState Netty-server-localhost-6702-worker-1 [INFO] Sending BackPressure status to new client. BPStatus: {worker=cdf6f963-678c-45a4-91d2-e1067a9a8516, bpStatusId=1, bpTasks=[], nonBpTasks=[17,
1, 18, 3, 4, 22, 7, 8, 9, 12, 13]}
2021-01-29 19:03:21.510 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] SASL credentials for storm topology wc is -8603731884381183101:-9091319821854384981
2021-01-29 19:03:21.572 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.maxCapacityPerThread: 4096
2021-01-29 19:03:21.573 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.maxSharedCapacityFactor: 2
2021-01-29 19:03:21.573 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.linkCapacity: 16
2021-01-29 19:03:21.574 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.ratio: 8
2021-01-29 19:03:21.575 o.a.s.v.ConfigValidation main [WARN] topology.backpressure.enable is a deprecated config please see class org.apache.storm.Config.TOPOLOGY_BACKPRESSURE_ENABLE for more information.
2021-01-29 19:03:21.593 o.a.s.s.i.n.b.AbstractByteBuf Netty-server-localhost-6702-worker-1 [DEBUG] -Dorg.apache.storm.shade.io.netty.buffer.checkAccessible: true
2021-01-29 19:03:21.594 o.a.s.s.i.n.b.AbstractByteBuf Netty-server-localhost-6702-worker-1 [DEBUG] -Dorg.apache.storm.shade.io.netty.buffer.checkBounds: true
2021-01-29 19:03:21.594 o.a.s.s.i.n.u.ResourceLeakDetectorFactory Netty-server-localhost-6702-worker-1 [DEBUG] Loaded default ResourceLeakDetector: org.apache.storm.shade.io.netty.util.ResourceLeakDetector@524a134b


....

2021-01-29 19:03:21.695 o.a.s.m.n.SaslStormServerHandler Netty-server-localhost-6702-worker-1 [DEBUG] No saslNettyServer for [id: 0x6fa65bc5, L:/10.215.73.209:6702 - R:/10.215.73.209:45870] yet; creating now, with topology token: wc
2021-01-29 19:03:21.697 o.a.s.m.n.SaslNettyServer Netty-server-localhost-6702-worker-1 [DEBUG] SaslNettyServer: Topology token is: wc with authmethod DIGEST-MD5
2021-01-29 19:03:21.698 o.a.s.m.n.SaslNettyServer Netty-server-localhost-6702-worker-1 [DEBUG] SaslDigestCallback: Creating SaslDigestCallback handler with topology token: wc
{code}

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/SaslStormServerHandler.java#L46-L52

This is a bug likely introduced in STORM-2306 (https://github.com/apache/storm/pull/2502). This willl happen on every topology when ""storm.messaging.netty.authentication"" is set true (It is false by default)"
STORM-3757,Update jackson version to 2.10.0,Update jackson version to 2.10.0 to avoid CVE-2019-14892 and CVE-2019-14893
STORM-3735,Kyro serialization fails on some metric tuples when topology.fall.back.on.java.serialization is false,"When a metric consumer is used, metrics will be sent from all executors to the consumer. In some of the metrics,  it includes NodeInfo object, and kryo serialization will fail if topology.fall.back.on.java.serialization is false.

{code:title=worker logs}
2021-01-13 20:16:37.017 o.a.s.e.ExecutorTransfer Thread-16-__system-executor[-1, -1] [INFO] TRANSFERRING tuple [dest: 5 tuple: source: __system:-1, stream: __metrics, id: {}, [TASK_INFO: { host: openstorm14blue-n4.blue.ygrid.yahoo.com:6703 comp: __system[-1]}, [
[CGroupCpuStat = {nr.throttled-percentage=46.544980443285525, nr.period-count=767, nr.throttled-count=357, throttled.time-ms=27208}], [CGroupMemoryLimit = 1342177280], [__recv-iconnection = {dequeuedMessages=0, enqueued={/10.215.73.210:47038=3169}}], [__send-ico
nnection = {NodeInfo(node:149a917b-bc75-49c8-b351-f74b8ae0fbed-10.215.73.210, port:[6701])={reconnects=1, src=/10.215.73.210:34938, pending=0, dest=openstorm14blue-n4.blue.ygrid.yahoo.com/10.215.73.210:6701, sent=1896, lostOnSend=0}, NodeInfo(node:149a917b-bc75-
49c8-b351-f74b8ae0fbed-10.215.73.210, port:[6702])={reconnects=8, src=/10.215.73.210:39476, pending=0, dest=openstorm14blue-n4.blue.ygrid.yahoo.com/10.215.73.210:6702, sent=2115, lostOnSend=0}, NodeInfo(node:b77b5ec6-15ee-4bd2-a9b8-12fcadde7744-10.215.73.211, po
rt:[6700])={reconnects=125, pending=0, dest=openstorm14blue-n5.blue.ygrid.yahoo.com/10.215.73.211:6700, sent=108, lostOnSend=1331}}], [CGroupMemory = 316485632], [CGroupCpu = {user-ms=36960, sys-ms=25860}], [memory.pools.Metaspace.usage = 0.9695890907929322], [m
emory.heap.max = 1073741824], [receive-queue-overflow = 0], [memory.pools.Compressed-Class-Space.used = 6237424], [memory.pools.Compressed-Class-Space.max = 1073741824], [memory.non-heap.init = 2555904], [worker-transfer-queue-overflow = 0], [memory.pools.Metasp
ace.committed = 42074112], [receive-queue-sojourn_time_ms = 0.0], [threads.waiting.count = 5], [memory.pools.G1-Eden-Space.usage = 0.2777777777777778], [memory.pools.Metaspace.used = 40798320], [memory.total.used = 101783888], [memory.pools.Code-Cache.init = 255
5904], [memory.non-heap.committed = 63832064], [GC.G1-Young-Generation.time = 677], [receive-queue-insert_failures = 0.0], [memory.total.init = 130482176], [GC.G1-Old-Generation.count = 0], [memory.pools.Metaspace.init = 0], [memory.pools.G1-Survivor-Space.commi
tted = 5242880], [worker-transfer-queue-population = 0], [memory.pools.Compressed-Class-Space.committed = 6684672], [threads.timed_waiting.count = 31], [memory.pools.G1-Eden-Space.init = 7340032], [memory.pools.Metaspace.max = -1], [memory.pools.G1-Survivor-Spac
e.used = 5242880], [memory.heap.init = 127926272], [memory.pools.G1-Old-Gen.used-after-gc = 0], [worker-transfer-queue-capacity = 1024], [memory.pools.G1-Survivor-Space.used-after-gc = 5242880], [memory.pools.G1-Old-Gen.committed = 47185920], [memory.pools.G1-Ed
en-Space.committed = 75497472], [receive-queue-arrival_rate_secs = 0.109421162052741], [memory.pools.Compressed-Class-Space.usage = 0.0058090537786483765], [TGT-TimeToExpiryMsecs = 71282993], [threads.runnable.count = 15], [worker-transfer-queue-insert_failures
= 0.0], [worker-transfer-queue-sojourn_time_ms = 0.0], [memory.heap.committed = 127926272], [memory.non-heap.max = -1], [threads.daemon.count = 29], [memory.pools.Code-Cache.max = 251658240], [worker-transfer-queue-arrival_rate_secs = 90.47776674390379], [memory
.heap.usage = 0.037109360098838806], [memory.pools.G1-Old-Gen.init = 120586240], [memory.pools.Code-Cache.committed = 15138816], [receive-queue-pct_full = 0.0], [worker-transfer-queue-pct_full = 0.0], [receive-queue-population = 0], [memory.pools.Compressed-Clas
s-Space.init = 0], [memory.pools.Code-Cache.usage = 0.059299468994140625], [worker-transfer-queue-dropped_messages = 0], [GC.G1-Young-Generation.count = 18], [memory.pools.Code-Cache.used = 14923200], [memory.pools.G1-Old-Gen.usage = 0.012695297598838806], [memo
ry.non-heap.usage = -6.196368E7], [memory.total.max = 1073741823], [threads.count = 51], [memory.heap.used = 39845872], [memory.pools.G1-Survivor-Space.init = 0], [memory.pools.G1-Old-Gen.used = 13631472], [receive-queue-dropped_messages = 0], [threads.terminate
d.count = 0], [memory.pools.G1-Eden-Space.max = -1], [uptimeSecs = 76], [threads.deadlock.count = 0], [threads.blocked.count = 0], [newWorkerEvent = 1], [receive-queue-capacity = 32768], [threads.new.count = 0], [startTimeSecs = 1610568920], [memory.pools.G1-Ede
n-Space.used-after-gc = 0], [memory.pools.G1-Eden-Space.used = 20971520], [GC.G1-Old-Generation.time = 0], [memory.non-heap.used = 61964384], [memory.pools.G1-Old-Gen.max = 1073741824], [memory.pools.G1-Survivor-Space.max = -1], [memory.pools.G1-Survivor-Space.u
sage = 1.0], [memory.total.committed = 191823872], [doHeartbeat-calls.count = 64], [doHeartbeat-calls.m1_rate = 1.0730202200365234E-6], [doHeartbeat-calls.m5_rate = 1.1636999000665182E-6], [doHeartbeat-calls.m15_rate = 1.1870955900857726E-6], [doHeartbeat-calls.
mean_rate = 1.0067076836696486E-6]]] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null]

...

2021-01-13 20:16:37.030 o.a.s.u.Utils Thread-16-__system-executor[-1, -1] [ERROR] Async loop died!
java.lang.RuntimeException: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at org.apache.storm.executor.Executor.accept(Executor.java:294) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:113) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:89) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:145) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.Utils$1.run(Utils.java:401) [storm-client-2.3.0.y.jar:2.3.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at org.apache.storm.executor.Executor.accept(Executor.java:294) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:113) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:89) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:145) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.Utils$1.run(Utils.java:401) [storm-client-2.3.0.y.jar:2.3.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[kryo-3.0.3.jar:?]
        at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:38) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:40) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:118) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:553) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:68) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.Task.sendUnanchored(Task.java:215) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.metricsTick(Executor.java:345) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:205) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.accept(Executor.java:290) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        ... 6 more
Caused by: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
        at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:488) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:622) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[kryo-3.0.3.jar:?]
        at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:38) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:40) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:118) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:553) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:68) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.Task.sendUnanchored(Task.java:215) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.metricsTick(Executor.java:345) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:205) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.accept(Executor.java:290) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        ... 6 more
{code}

The related metric is ""__send-iconnection"" from https://github.com/apache/storm/blob/7bef73a6faa14558ef254efe74cbe4bfef81c2e2/storm-client/src/jvm/org/apache/storm/daemon/metrics/BuiltinMetricsUtil.java#L40-L43

Note that this can only be reproduced when metrics are sent across workers (otherwise there is no serialization).

The work around is one of the following
1) add org.apache.storm.generated.NodeInfo to topology.kryo.register in topology conf
2) set topology.fall.back.on.java.serialization true or unset topology.fall.back.on.java.serialization since the default is true


The fix is to register NodeInfo class in kryo.
https://github.com/apache/storm/blob/7bef73a6faa14558ef254efe74cbe4bfef81c2e2/storm-client/src/jvm/org/apache/storm/serialization/SerializationFactory.java#L67-L77
"
STORM-3729,Assigning memory greater and equal than 2048m will make assgin memory for slot values to 1m,"Hi, everyone.

I set my topology over 2048m, the storm ui shows only 65m, so i found its error in [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java]  line 1089 that value cast to int instead of long, It goes wrong if I pass 2048m and results 1m.

Simply change this line to cast Long can solve this problem.:) "
STORM-3728,Workers are not able to connect to Pacemaker if pacemaker.auth.method is KERBEROS,"When pacemaker.auth.method is KERBEROS,  worker will fail to connect to KERBEROS because of exceptions like the following:
 
{code:java}
2020-12-21 20:07:00.786 o.a.s.c.PaceMakerStateStorage executor-heartbeat-timer [ERROR] Timed out waiting for channel ready. Failed to set_worker_hb. Will make 2 more attempts.
2020-12-21 20:07:00.902 o.a.s.m.n.KerberosSaslClientHandler openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [INFO] Connection established from /10.215.73.209:45548 to openstorm3blue-n10.blue.ygrid.yahoo.com/10.215.79.152:6699
2020-12-21 20:07:00.903 o.a.s.m.n.KerberosSaslNettyClient openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [INFO] Creating Kerberos Client.
2020-12-21 20:07:00.906 o.a.s.m.n.KerberosSaslNettyClient openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [INFO] Kerberos Client Callback Handler got callback: class javax.security.auth.callback.PasswordCallback
2020-12-21 20:07:00.906 o.a.s.m.n.Login openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [ERROR] Login using jaas conf /home/y/lib/storm/current/conf/storm_jaas.conf failed
2020-12-21 20:07:00.906 o.a.s.m.n.KerberosSaslNettyClient openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [ERROR] Client failed to login in principal:javax.security.auth.login.LoginException: No password provided
javax.security.auth.login.LoginException: No password provided
        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:923) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:764) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:618) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_262]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_262]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_262]
        at org.apache.storm.messaging.netty.Login.login(Login.java:301) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.Login.<init>(Login.java:83) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslNettyClient.<init>(KerberosSaslNettyClient.java:66) [storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:59) [storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
2020-12-21 20:07:00.907 o.a.s.m.n.KerberosSaslClientHandler openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [ERROR] Failed to authenticate with server due to error:
java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
        at org.apache.storm.messaging.netty.KerberosSaslNettyClient.<init>(KerberosSaslNettyClient.java:71) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:59) [storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: javax.security.auth.login.LoginException: No password provided
        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:923) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:764) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:618) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_262]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_262]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_262]
        at org.apache.storm.messaging.netty.Login.login(Login.java:301) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.Login.<init>(Login.java:83) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslNettyClient.<init>(KerberosSaslNettyClient.java:66) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        ... 20 more
2020-12-21 20:07:01.802 o.a.s.p.PacemakerClient executor-heartbeat-timer [ERROR] Error attempting to write to a channel to host openstorm3blue-n10.blue.ygrid.yahoo.com - Timed out waiting for channel ready.
2020-12-21 20:07:01.803 o.a.s.p.PacemakerClient executor-heartbeat-timer [WARN] Not getting response or getting null response. Making 9 more attempts for openstorm3blue-n10.blue.ygrid.yahoo.com.
{code}

Currently by design [https://github.com/apache/storm/blob/master/docs/Pacemaker.md#security] pacemaker allows writes by anyone (which should be improved in the future). 

So a quick work around is to make sure worker always has pacemaker.auth.method set to  NONE

 "
STORM-3725,DRPC spout will crash when any one of DRPC server is down,The root cause is DRPC Spout does not handle drpc connections really asynchrously. Spout worker will not work unless all DRPC servers are up and running which leads to the SPOF.
STORM-3701,Race Condition between cleanup thread and download tasks,"We captured a issue on our supervisor:

{code}

2020-06-09 23:30:08.723 o.a.s.l.LocalizedResource AsyncLocalizer Task Executor - 0 [INFO] completelyRemoveUnusedUser directu for directory /home/y/var/storm/supervisor/usercache/directu
 2020-06-09 23:30:08.724 o.a.s.l.AsyncLocalizer AsyncLocalizer Task Executor - 0 [WARN] Caught Exception While Downloading (rethrowing)...
 java.io.FileNotFoundException: File '/home/y/var/storm/supervisor/stormdist/dg_itp-605-1591745383/stormconf.ser' does not exist
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:316) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:477) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:311) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.get(AsyncLocalizer.java:698) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.get(AsyncLocalizer.java:683) [storm-server-2.3.0.y.jar:2.3.0.y]
         at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) [?:1.8.0_242]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_242]
         at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_242]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
         at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
 2020-06-09 23:30:08.725 o.a.s.d.s.Slot SLOT_6782 [ERROR] java.io.FileNotFoundException: File '/home/y/var/storm/supervisor/stormdist/dg_itp-605-1591745383/stormconf.ser' does not exist
 2020-06-09 23:30:08.725 o.a.s.l.AsyncLocalizer SLOT_6782 [INFO] Port and assignment info: PortAndAssignmentImpl\{dg_itp-605-1591745383 on 6782}
 2020-06-09 23:30:08.726 o.a.s.l.AsyncLocalizer SLOT_6782 [WARN] Local base blobs have not been downloaded yet.
 java.io.FileNotFoundException: File '/home/y/var/storm/supervisor/stormdist/dg_itp-605-1591745383/stormconf.ser' does not exist
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:316) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:477) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:311) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:362) ~[storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:472) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:549) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:298) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:1039) [storm-server-2.3.0.y.jar:2.3.0.y]

{code}
 The root issue is the delay at [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L641] which will cause the safeTopologyIds information out-of-date.

 

 

 "
STORM-3700,"Storm UI functionality ""Stop flight recording"" doesn't work properly","The code is implemented at
[https://github.com/apache/storm/blob/36204eda00bca7e03ac3979d9c0d3527d1f08330/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java#L841-L878]

 

JPROFILE_STOP is used for both start flight recording and stop flight recording. The logic in the code is that if there is already a JPROFILE_STOP with the same topoId and request content (host, port, timestamp), the JPROFILE_STOP becomes a stop command; otherwise, it is a start command.

But the problem is every time when we invoke stop on UI:

[https://github.com/apache/storm/blob/3fb289b87c7d72bfe01ee1c7028adbc69f012439/storm-webapp/src/main/java/org/apache/storm/daemon/ui/resources/StormApiResource.java#L621-L631]

 
the request is actually configured with timestamp=0:

[https://github.com/apache/storm/blob/bb199d574eae337d0512670dcc4957f3c7ef4922/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L2320]

so it will never equal to any request in the pending profile action. So stop will never happen
 "
STORM-3699,fix flight.bash to support flight recording on openJDK8u262 or newer,"[https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/comline.htm#JFRUH193]

[https://docs.oracle.com/javacomponents/jmc-5-5/jfr-command-reference/diagnostic-command-reference.htm#resourceid-15422-48C8362C]

 

flight recorder command changed between versions. so the current flight.bash doesn't work for flight recording on newer JFR version (e.g. on openJDK8u262)

[https://github.com/apache/storm/blob/eb27556f7669c7c966716e1aca4867da04ce6e08/bin/flight.bash#L86-L93]

 
{code:java}
bash-4.2$ java -version
openjdk version ""1.8.0_262""
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_262-b10)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.262-b10, mixed mode)
bash-4.2$ jcmd 20 JFR.stop recording=1
20:
java.lang.IllegalArgumentException: Unknown argument 'recording' in diagnostic command.
{code}
 

 

 "
STORM-3684,"receive-queue V2 metrics shouldn't have ""_system"" as componentId if it is from a system task","When storm registers receive-queue for executors,
[https://github.com/apache/storm/blob/v2.2.0/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java#L689]

it set componentId as ""__system"" for any task. This is wrong for V2 metrics.

V1 metrics are fine since the component Id used for V1 metric is not from the same place.

[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L342-L343]
It is from the member variable of the executor object."
STORM-3679,misuse of nodeId as hostname in LoadAwareShuffleGrouping," 
LoadAwareShuffleGrouping misuses nodeId as the hostname and it causes consistent invalid DNS queries for ""hostnames"" like 2a1f2cf3-c701-4621-9e93-640b4e63be48-<ip>.

This causes excessive unnecessary loads on nscd and DNS. Also because of this bug, every target tasks will be treated as at least RACK_LOCAL because if an ip address can't be determined, YahooDNSToSwitchMapping treats it as DEFAULT_RACK. This doesn't impact WORKER_LOCAL and HOST_LOCAL though.
 "
STORM-3677,Fix Worker Suicide Function if assignment is null,When topology is killed worker receives null for assignment. It should cause  worker restart.
STORM-3666,Validate component name in rebalance command and fix --executor option,"It appears rebalance does not change parallelism of components. It is advertised in storm help but not honoring it.
{quote}_storm rebalance --executors component-id:number topology-id_
{quote}"
STORM-3658,Problematic worker stays alive because of a deadlock and race condition caused by ShutdownHooks,"During worker startup, it starts many threads including executor threads, and then registers two shutdown hooks, first hook will invoke worker::shutdown, the second hook will sleep for 3 seconds before force halting the whole process.

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L141-L147

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L253-L255

We have seen a case where a deadlock happened. Imaging there is a bolt in a topology consistently fails at prepare stage and throws RuntimeException. This thread will eventually invoke Runtime.getRuntime().exit. The code path is:

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L406-L407

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/error/ReportErrorAndDie.java#L41

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L510-L514

There are three scenarios here.

*Scenario 1*
If two shutdown hooks are registered before this bolt's prepare method is invoked, when this bolt throws RuntimeException, it eventually invokes Runtime.getRuntime().exit, which triggers two shutdown hooks to start. And then this bolt executor thread waits for these two shutdown hooks to finish.

https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u242-b08/jdk/src/share/classes/java/lang/ApplicationShutdownHooks.java#L104-L111 (we use this openJDK8u242 version)

However, what the first shutdown hooks does is to invoke worker::shutdown method
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L467-L469
which will interrupt all executor threads and then wait for them to finish 

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java#L95-L100

However, this bolt executor thread ignores InterruptedException and continues to wait for the first hook to finish. Hence there is a dead lock between the first shutdown hook and the bolt executor thread. After 3 seconds, the second shutdown hook force halting the worker process. So in the log, you will see ""Forcing Halt...""

*Scenario 2*
If the bolt executor thread invokes prepare method earlier than the main thread registers these two shutdown hooks, because the bolt executor thread already triggers shutdown, the main thread will receive an IllegalStateException(""Shutdown in progress"")

{code:java}
2020-06-18 11:57:29.159 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.lang.IllegalStateException: Shutdown in progress
{code}

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L1011-L1018

https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u242-b08/jdk/src/share/classes/java/lang/ApplicationShutdownHooks.java#L63-L67

Since there is no shutdown hook registered, Runtime.getRuntime.exit invoked by the bolt executor continues; in the meantime, the main thread also invokes Runtime.getRuntime.exit because of IllegalStateException. Eventually the process dies.

*Scenario 3*
This is the worse case. In this scenario, after the first shutdown hook is registered, and before the second hook is registered, the bolt executor thread invokes prepare method and throws a RuntimeException. 
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L355-L356
 
In this case, we know have the deadlock between the first shutdown hook and the bolt executor thread (like in scenario 1). The main thread will also have IllegalStateException when it tries to register the second shutdown hook, so main thread also invokes Runtime.getRuntime.exit (like in scenario 2). But this time, since the bolt executor thread already obtained the shutdown lock (because it invokes Runtime.getRuntime.exit earlier), 
https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u242-b08/jdk/src/share/classes/java/lang/Shutdown.java#L208-L214

the main thread has to wait for the bolt executor thread to release this Shutdown.class lock. However, there is a deadlock between bolt executor thread and the first shutdown hook(thread), the main thread, the bolt executor thread and the shutdown hook are all BLOCKED. 

And in this case, since the second shutdown hook (sleepKill) is not registered, and every other threads like heartbeat timers still work (not yet closed), no one (not worker itself, not nimbus or supervisor) will kill this worker. 

So this worker stays alive but it does not function. And since this executor bolt is blocked, it doesn't consume tuples from the receiveQ, so the receiveQ can be quickly fill up by upstreams, the credential refresh thread is also blocked because it won't give up until the credential tuple is published to the receiveQ.

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java#L68-L69

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/JCQueue.java#L300-L313

*How to produce the deadlock*
It can be produced by modifying the [WordCountTopology|https://github.com/apache/storm/blob/v2.1.0/examples/storm-starter/src/jvm/org/apache/storm/starter/WordCountTopology.java] to add 

{code:java}
@Override
    public void prepare(Map<String, Object> topoConf, TopologyContext context) {
        throw new RuntimeException(""Runtime exception at WordCountBolt prepare"");
    }
{code}

in the WordCount bolt so everytime the prepare method is called, this bolt throws a RuntimeException. 


Optionally, add a delay in between registering two shutdown hooks can help reproduce scenario 3.

{code:java}
Runtime.getRuntime().addShutdownHook(wrappedFunc);
        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            LOG.info(""Sleep for 100ms between hooks"", e);
        }
        LOG.info(""Wait for 100ms"");
        Runtime.getRuntime().addShutdownHook(sleepKill);
{code}


*Solution*
There are better ways to deal with this issue. But a simple way is to register shutdown hooks before any executor threads are started to avoid IllegalStateException; And in 
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java#L98-L101
change it be a timed join so it won't wait indefinitely. This can prevent the deadlock from happening. 


"
STORM-3655,Worker should commit suicide On Change in its assignment.,"This is not urgent but essentially, it can take care of avoiding a lot of scenarios leading to left over rogue processes across the cluster. Two scenarios are: 
 * Worker still exists in assignment, but assignment is different
 * Worker is absent from assignments.
 "
STORM-3652, Last error not displayed  in Topology summary storm ui,"Under storm ui ""Topology summary"" lastError is not getting populated, you need to go into ""Component summary""
  
 Seems some body raised a similar question here [https://github.com/apache/storm/pull/2828#issuecomment-420790179]

STORM-3217 fixed the component page, but not the topology page
 is it because it is missing here [https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L1225]

 

I see it is in this section added [https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L1480]
  
h2. Topology summary ({color:#de350b}Last error appears empty, failed metric = 40{color})

*API response*
 localhost:8080/api/v1/topology/topology-test-1-1591927011
{code:java}
 
{
   ""bolts"":[
      {
         ""requestedCpu"":10.0,
         ""encodedBoltId"":""TopLevel.test"",
         ""transferred"":0,
         ""lastError"":"""",  <-- *empty*
         ""processLatency"":""0.000"",
         ""executeLatency"":""5.000"",
         ""executed"":40,
         ""failed"":40, <-- *failed count not zero*
         ""requestedMemOnHeap"":128.0,
         ""acked"":0,
         ""capacity"":""0.001"",
         ""emitted"":0,
         ""requestedMemOffHeap"":0.0,
         ""executors"":1,
         ""requestedGenericResourcesComp"":"""",
         ""boltId"":""TopLevel.test"",
         ""tasks"":1
      }
   ]
}
{code}
 
h2. Component summary ({color:#00875a}Displays the errors{color})

*API response*
 localhost:8080/api/v1/topology/topology-test-1-1591927011/component/TopLevel.test?sys=false
{code:java}
{
   ""requestedGenericResources"":"""",
   ""componentErrors"":[
      {
         ""errorTime"":1591928686,
         ""errorWorkerLogLink"":""http:\/\/localhost:8000\/api\/v1\/log?file=topology-test-1-1591927011%2F6700%2Fworker.log"",
         ""errorLapsedSecs"":16,
         ""errorPort"":6700,
         ""error"":""java.lang.IllegalArgumentException: Error with IllegalArgumentException\n\tat com.test.TestBolt.execute(TestBolt.java:35)\n\tat org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:236)\n\tat org.apache.storm.executor.Executor.accept(Executor.java:283)\n\tat org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131)\n\tat org.apache.storm.utils.JCQueue.consume(JCQueue.java:111)\n\tat org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:172)\n\tat org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159)\n\tat org.apache.storm.utils.Utils$1.run(Utils.java:394)\n\tat java.lang.Thread.run(Thread.java:748)\n"",
         ""errorHost"":""localhost""
      }
   ]
}
{code}
 Errors reported using
{code:java}
// Where outputCollector is the instance passed during BaseRichBolt::prepare

// public void prepare(Map stormConf, TopologyContext context, OutputCollector outputCollector) {

outputCollector().reportError(e);
outputCollector().fail(input);

{code}"
STORM-3649,Logic error regarding storm.supervisor.medium.memory.grace.period.ms,"Inside this chunk of code

https://github.com/apache/storm/blob/2.2.x-branch/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L758

{code:java}
if (systemFreeMemoryMb < mediumMemoryThresholdMb) {
                    if (memoryLimitExceededStart < 0) {
                        memoryLimitExceededStart = Time.currentTimeMillis();
                    } else {
                        long timeInViolation = Time.currentTimeMillis() - memoryLimitExceededStart;
                        if (timeInViolation > mediumMemoryGracePeriodMs) {
                            LOG.warn(
                                ""{} is using {} MB > memory limit {} MB for {} seconds"",
                                typeOfCheck,
                                usageMb,
                                memoryLimitMb,
                                timeInViolation / 1000);
                            return true;
                        }
                    }
                } 
{code}

At very beginning, memoryLimitExceededStart in BasicContainer is initialized as 0. :
https://github.com/apache/storm/blob/2.2.x-branch/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L80
{code:java}
protected volatile long memoryLimitExceededStart;
{code}

So once it hits this scenario, the grace period doesn't really take any effect because the timeInViolation will be very large (equals to currentTime)

The logs from a test:

{code:java}
2020-06-08 20:39:18.277 o.a.s.d.s.BasicContainer SLOT_6707 [WARN] WORKER 9c16e81e-4936-4029-bcda-ceb5b74b8f42 is using 167 MB > memory limit 158 MB for 1591648758 seconds
{code}

"
STORM-3637,Looping topology structure can cause backpressure to deadlock,"When you have a topology structure with loops in it (BoltA and BoltB send tuples to each other), it can cause backpressure to deadlock.

The scenario is that BoltA suddenly takes a long time to process a tuple (in our situation, it's doing a database operation). This causes the task input queue to fill up, setting the backpressure flag.

BoltB, which is sending a tuple to BoltA, then cannot send, and the tuple is held in the emit queue. This blocks any tuples behind it, and also stops BoltB from executing. This means the input queue to BoltB will build up, until that backpressure flag is also set - and then when BoltA next wants to send a tuple to BoltB, it will irrevocably deadlock."
STORM-3631,Wrong format of logs.users/groups in topology conf can cause supervisor/logviewer to terminate,"If users submit a topology with logs.users set as a single string, it will cause ClassCastException and cause Supervisor to terminate
{code:java}
2020-04-28 19:33:59.901 o.a.s.d.s.Slot SLOT_6707 [ERROR] Error when processing event
java.lang.ClassCastException: java.lang.String cannot be cast to java.util.List
        at org.apache.storm.daemon.supervisor.Container.writeLogMetadata(Container.java:)
{code}

Can be easily reproduced by 

{code:java}
storm jar storm-starter.jar org.apache.storm.starter.WordCountTopology wc -c logs.users=[null, ""fake-groups""] 
{code}


If users submit with logs.users set with a list with null member, for example, logs.users='[null, ""fake-2-users""]', it will cause NullPointerException and cause logviewer to terminate

{code:java}
Caused by: java.lang.NullPointerException
        at org.apache.storm.utils.ObjectReader.getStrings(ObjectReader.java:)
        at org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer.getLogUserGroupWhitelist(ResourceAuthorizer.java)
{code}

Can be easily reproduced by 

{code:java}
storm jar storm-starter.jar org.apache.storm.starter.WordCountTopology wc -c logs.users=""fake-users""
{code}


Ideally logs.users and logs.groups should be daemon config only and we should have something like topology.logs.users and topology.logs.groups for topology level config sit in Config.java (so it can be validated by ConfigValidation).   Because now there two configs are in DaemonConfig, it wont' be validated against ""@isStringList"" rule when it is from topo conf. 

But even with the rule, it doesn't validate when logs.users include a null member in the list. 

For backwards compatibility, we have to fix these configs instead of removing them from topo conf (by adding topology.logs.users).
"
STORM-3629, Logviewer should always allow admins to access logs,"https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java#L86-L89

Currently if there is any problems with reading worker.yaml, no one can access logs from logviewer, including admins. Admins should always be able to access."
STORM-3626,"storm-kafka-migration should pull in storm-client as ""provided"" dependency","https://github.com/apache/storm/blob/master/external/storm-kafka-migration/pom.xml#L34-L39


{code:java}
<dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
        </dependency>

{code}

it is ""compile"" dependency as of now."
STORM-3624,Race condition on ArtifactoryConfigLoader.load,"https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java#L100-L102

config() is called in multiple threads. But ArtifactoryConfigLoader.load is not thread-safe. For example, https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/utils/ArtifactoryConfigLoader.java#L181-L187
{code:java}
JSONObject returnValue;
        try {
            returnValue = (JSONObject) jsonParser.parse(metadataStr);
        } catch (ParseException e) {
            LOG.error(""Could not parse JSON string {}"", metadataStr, e);
            return null;
        }
{code}

Multiple threads use the same jsonParser and since JsonParser is not thread-safe, the return value will be corrupted. 


I propose to create a separate thread to load scheduler configs periodically. This also makes the config loading logic cleaner."
STORM-3623,v2 metrics tick reports all worker metrics within each executor,"see https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L335-L341


{code:java}
    private void addV2Metrics(List<IMetricsConsumer.DataPoint> dataPoints) {
        boolean enableV2MetricsDataPoints = ObjectReader.getBoolean(topoConf.get(Config.TOPOLOGY_ENABLE_V2_METRICS_TICK), false);
        if (!enableV2MetricsDataPoints) {
            return;
        }
        StormMetricRegistry stormMetricRegistry = workerData.getMetricRegistry();
{code}


This should be reporting just the metrics for the Executor."
STORM-3620,Data corruption can happen when components are multi-threaded because of non thread-safe serializer,"OutputCollector is not thread-safe in 2.x. 

It can cause data corruption if multiple threads in the same executor calls OutputCollector to emit data at the same time:

1. Every executor has an instance of ExecutorTransfer
https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L146

2. Every ExecutorTransfer has its own serializer

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L44

3. Every executor has its own outputCollector

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java#L146-L147

4. When outputCollector is called to emit to remote workers, it uses ExecutorTransfer to transfer data

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L66

5. which will try to serialize data

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerTransfer.java#L116

6. But serializer is not thread-safe

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L33-L43


----

But in the doc, http://storm.apache.org/releases/2.1.0/Concepts.html, it says outputCollector is thread-safe. 
{code:java}
Its perfectly fine to launch new threads in bolts that do processing asynchronously. OutputCollector is thread-safe and can be called at any time.
{code}


We should either fix it to make it thread-safe, or update the document to not mislead users"
STORM-3613,storm.py should include lib-worker instead of lib directory in the classpath while submitting a topology,"Currently the classpath is:
{code:java}
-cp /<path>/storm/2.2.0/*:/<path>/storm/2.2.0/lib/*:/<path>/storm/2.2.0/extlib/*:/tmp/storm-examples-1.0-SNAPSHOT.jar:/<path>/storm/2.2.0/conf:/<path>/storm/2.2.0/bin: 
{code}

 for ""storm jar"" command.

It should include lib-worker/ instead of lib/.

This can cause problems because we don't shade deps in lib/ so topology jar could conflict with jars in lib/."
STORM-3610,CLONE - Topology runtime exception - Error on initialization,"{code:java}
2018-03-14 13:28:41.399 o.a.s.d.worker main [INFO] Reading Assignments. 2018-03-14 13:28:41.511 o.a.s.m.TransportFactory main [INFO] Storm peer transport plugin:org.apache.storm.messaging.netty.Context 2018-03-14 13:28:41.935 o.a.s.m.n.Server main [INFO] Create Netty Server Netty-server-localhost-6712, buffer_size: 5242880, maxWorkers: 1 2018-03-14 13:28:41.980 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6712 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data$fn__5244.invoke(worker.clj:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:269) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544.run(worker.clj:613) ~[storm-core-1.1.0.jar:1.1.0] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_51] at javax.security.auth.Subject.doAs(Subject.java:415) ~[?:1.7.0_51] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto____5543.invoke(worker.clj:611) ~[storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?] at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0] Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:444) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:436) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_51] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.1.0.jar:1.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_51] at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_51] 2018-03-14 13:28:42.004 o.a.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"") at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0]
{code}"
STORM-3609,ClassCastException when credentials are updated for ICredentialsListener spout/bolt instances,"
{code:java}
2020-03-26 21:04:38.526 o.a.s.u.Utils Thread-14-spout-executor[2, 2] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.storm.generated.Credentials cannot be cast to java.util.Map
	at org.apache.storm.executor.Executor.accept(Executor.java:291) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:111) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:102) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:170) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:159) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:433) [storm-client-2.2.0.y.jar:2.2.0.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.lang.ClassCastException: org.apache.storm.generated.Credentials cannot be cast to java.util.Map
	at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:303) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.Executor.accept(Executor.java:287) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 7 more
{code}



note: ""2.2.0.y"" is our internal version, which is master branch."
STORM-3607,Document the exceptions topologies will see from TGT renewal thread,This is to document STORM-3606 in the code so users can be less confusing about the exceptions from TGT renewal thread.
STORM-3606,AutoTGT shouldn't invoke TGT renewal thread (from UserGroupInformation.loginUserFromSubject),"When hadoop security is enabled, 
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGT.java#L199-L209

AutoTGT will invoke ""loginUserFromSubject"", and it will spawn a TGT renewal thread (""TGT Renewer for <username>""). 
https://github.com/apache/hadoop/blob/branch-2.8.5/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L928-L957

which will eventually invoke system command ""kinit -R"", and then fail with the exception

{code:java}
org.apache.hadoop.util.Shell$ExitCodeException: kinit: Credentials cache file '/tmp/krb5cc_xxx' not found while renewing credentials

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1004) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:898) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:1011) [stormjar.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}


""kinit"" will never work from worker process since Storm don't keep TGT in local cache. Instead, TGT is saved in zookeeper and in memory of Worker process. 

This exception is confusing but not harmful to topologies. And the TGT renewal thread will eventually abort. 

It's better to find a real solution for it. But for now we can document what might happen in AutoTGT code.

To be clear, we still need loginUserFromSubject or some sort but we don't want to spawn TGT renewal thread.  This is found with hadoop-2.8.5. Other versions are similar. But it can also change in the future release."
STORM-3602,loadaware shuffle can overload local worker,"We were seeing a worker overloaded and tuples timing out with loadaware shuffle enabled.  From investigating, we found that the code allows switching from Host local to Worker local if the load average is lower than the low water mark.  It really should be checking the load on the worker instead. 

 

What's happening is the worker is overloaded with tons of idle host local tasks, so it switches to HOST_LOCAL.  Then the calculation across all the host tasks is below the low water mark and it immediately switches back to the overloaded worker local task.

 

 "
STORM-3598,Storm UI visualization throws NullPointerException,"We encountered an issue with visualization on UI. 

 
{code:java}
2020-03-09 19:59:01.756 o.a.s.d.u.r.StormApiResource qtp1919834117-167291 [ERROR] Failure getting topology visualization
java.lang.NullPointerException: null
        at org.apache.storm.stats.StatsUtil.mergeWithAddPair(StatsUtil.java:1855) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.expandAveragesSeq(StatsUtil.java:2308) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.aggregateAverages(StatsUtil.java:832) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.aggregateBoltStats(StatsUtil.java:731) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.boltStreamsStats(StatsUtil.java:900) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.ui.UIHelpers.getVisualizationData(UIHelpers.java:1939) ~[storm-webapp-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyVisualization(StormApiResource.java:423) ~[storm-webapp-2.2.0.y.jar:2.2.0.y]
{code}
This is a bug in the code. https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java#L1846-L1858
{code:java}
for (K kk : mm1.keySet()) {
                    List seq1 = mm1.get(kk);
                    List seq2 = mm2.get(kk);
                    List sums = new ArrayList();
                    for (int i = 0; i < seq1.size(); i++) {
                        if (seq1.get(i) instanceof Long) {
                            sums.add(((Number) seq1.get(i)).longValue() + ((Number) seq2.get(i)).longValue());
                        } else {
                            sums.add(((Number) seq1.get(i)).doubleValue() + ((Number) seq2.get(i)).doubleValue());
                        }
                    }
                    tmp.put(kk, sums);
                }
{code}
It assume mm1 and mm2 always have the same key, which is not true. 

And it can be reproduced by my example code: 

 {code:java}
public class  WordCountTopology extends ConfigurableTopology {
    private static final Logger LOG = LoggerFactory.getLogger(WordCountTopology.class);

    public static void main(String[] args) {
        ConfigurableTopology.start(new WordCountTopology(), args);
    }

    protected int run(String[] args) {
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(""spout1"", new RandomSpout(1), 1);
        builder.setSpout(""spout2"", new RandomSpout(2), 1);
        builder.setBolt(""bolt"", new RandomBolt(), 2).directGrouping(""spout1"", ""stream1"")
                .directGrouping(""spout2"", ""stream2"");

        String topologyName = ""word-count"";

        conf.setNumWorkers(3);

        if (args != null && args.length > 0) {
            topologyName = args[0];
        }
        return submit(topologyName, conf, builder);
    }

    static class RandomSpout extends BaseRichSpout {
        String stream;
        int id;

        public RandomSpout(int id) {
            this.id = id;
            stream = ""stream"" + id;
        }

        int taskId = 0;
        SpoutOutputCollector collector;
        public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
            taskId = context.getThisTaskId();
            this.collector = collector;
        }

        /**
         * Different spout send tuples to different bolt via different stream.
         */
        public void nextTuple() {
            LOG.info(""emitting {}"", id);
            if (id == 1) {
                Values val = new Values(""test a sentence"");
                collector.emitDirect(2, stream, val, val);
            } else {
                Values val = new Values(""test 2 sentence"");
                collector.emitDirect(3, stream, val, val);
            }
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declareStream(stream, new Fields(""word""));
        }
    }

    static class RandomBolt extends BaseBasicBolt {

        public void execute(Tuple input, BasicOutputCollector collector) {
            LOG.info(""executing:"" + input.getSourceComponent());
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {

        }
    }
}
{code}

 In this example, one of the bolt will only receive data from stream1 and another bolt will only receive data from stream2. So in the map, 

 {code:java}
                    List seq1 = mm1.get(kk);
                    List seq2 = mm2.get(kk);
{code}
seq1 is null if kk is stream1, seq2 is null if kk is stream2.

 

 We have other places aggregating executor stats without this problem because it's using different code https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java#L502-L513 and this problem has been taken cared of."
STORM-3591,Improve GRAS Strategy Log,"[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java#L123]
{code:java}
2020-02-24 14:53:59.652 o.a.s.s.r.s.s.GenericResourceAwareStrategy pool-21-thread-1 [WARN] Scheduling [[1, 1]] left over task (most likely sys tasks)
{code}
This message seems to be confusing on debugging.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java#L82]

Default Strategy actually uses debug level instead of warn."
STORM-3589,Iterator in BaseResourceStrategy is potentially buggy,"[https://github.com/apache/storm/blame/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java#L280]

We should probably only peek but not remove value from nodeIterator in hasNext() function.

 

[https://github.com/apache/storm/blame/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java#L296-L300]

 

And two consecutive next() call will cause problem."
STORM-3587,Allow Scheduler futureTask to gracefully exit and register message on timeout,"ResourceAwareScheduler creates a FutureTask with timeout specified in DaemonConfig.

ConstraintSolverStrategy uses the the another configuration variable to determine when to terminate its effort. Limit this value so that it terminates at most slightly before TimeoutException. This graceful exit allows result (and its error) to be available in ResourceAwareScheduler.

 "
STORM-3581,Change log level to info to show the config classes being used for validation,"[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java#L82]

This is trivial but since it's caused some confusion, it's better to have it in the log as INFO instead of DEBUG
{code:java}
LOG.debug(""Will use {} for validation"", ret);
{code}
 

Because the classes being used for validation depends on whether the following file is in the classpath or not

 

[https://github.com/apache/storm/blob/master/storm-server/src/main/resources/META-INF/services/org.apache.storm.validation.Validated]"
STORM-3580,Config overrides supplied using -c in storm.py not passed to all commands,"-c is used to supply configuration overide options. Storm.py in the client code converts these overrides into one -Dstorm.options system property. However, this jvm option is not handled properly when the actual command is executed. For example, Rebalance command completely ignores this setting.

Commands that currently process ""-c"" options:
 * Activate - Not Needed
 * *AdminCommands - Yes*
 * BasicDrpcClient - Not Needed
 * Blobstore - Not Needed
 * CLI - Not Needed
 * *ConfigValue - Yes*
 * Deactivate - Not Needed
 * *DevZookeeper - Yes*
 * *DRPCServer - Yes*
 * GetErrors - Not Needed
 * *HealthCheck - Yes*
 * *Heartbeats - Yes*
 * KillTopology - Not Needed
 * *KillWorkers - Yes*
 * ListTopologies - Not Needed
 * *LogViewerServer - Yes*
 * *LocalCluster - Yes*
 * Monitor - Not Needed
 * *Nimbus - Yes*
 * *Pacemaker - Yes*
 * Rebalance - {color:#ff0000}Added as part of this Jira{color}
 * SetLogLevel - Not Needed
 * *ShellSubmission - Yes*
 * *StormSqlRunner - Yes*
 * Supervisor -Not Needed
 * *UI - Yes*
 * *UploadCredentials - Yes, but specific options*
 * VersionsInfo - Not Needed

 "
STORM-3579,Fix Kerberos connection from Worker to Nimbus/Supervisor,BUG2 in the parent JIRA
STORM-3578,ClientAuthUtils.insertWorkerTokens removes exiting and new WorkerToken altogether if they are equal,BUG1 in the parent JIRA
STORM-3577,upload-credentials Breaks Topology in secure cluster,"*Background*

Worker uses WorkerToken to connect to Nimbus/Supervisor, (e.g. in Worker.doHeartBeat method). If WorkerToken is not in place, it will fall back to Kerberos.

 

*Issue:*

Users can submit topology and the topology is running fine.

But error shows up in worker log if ""storm upload-credentials"" is executed (with AutoTGT being used). (2.2.0.y is our internal version of apache-storm master branch)

 
{code:java}
2020-02-04 00:12:57.975 o.a.s.d.w.Worker heartbeat-timer [WARN] Exception when send heartbeat to local supervisor
2020-02-04 00:12:57.984 o.a.s.s.a.k.ClientCallbackHandler heartbeat-timer [WARN] Could not login: the client is being asked for a password, but the  client code does not currently support obtaining a password from the user. Make sure that the client is configured to use a ticket cache (using the JAAS configuration setting 'useTicketCache=true)' and restart the client. If you still get this message after that, the TGT in the ticket cache has expired and must be manually refreshed. To do so, first determine if you are using a password or a keytab. If the former, run kinit in a Unix shell in the environment of the user who is running this client using the command 'kinit <princ>' (where <princ> is the name of the client's Kerberos principal). If the latter, do 'kinit -k -t <keytab> <princ>' (where <princ> is the name of the Kerberos principal, and <keytab> is the location of the keytab file). After manually refreshing your cache, restart this client. If you continue to see this message after manually refreshing your cache, ensure that your KDC host's clock is in sync with this host's clock.
2020-02-04 00:12:57.984 o.a.s.s.a.k.KerberosSaslTransportPlugin heartbeat-timer [ERROR] Server failed to login in principal:javax.security.auth.login.LoginException: No password provided
javax.security.auth.login.LoginException: No password provided
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:919) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.storm.messaging.netty.Login.login(Login.java:300) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.messaging.netty.Login.<init>(Login.java:84) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:112) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:69) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:80) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:221) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:179) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.heartbeatToMasterIfLocalbeatFail(Worker.java:456) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.doHeartBeat(Worker.java:361) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.lambda$loadWorker$2(Worker.java:209) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.2.0.y.jar:2.2.0.y]
2020-02-04 00:12:57.985 o.a.s.u.NimbusClient heartbeat-timer [WARN] Ignoring exception while trying to get leader nimbus info from quadiumtan-ni.tan.ygrid.yahoo.com. will retry with a different seed host.
java.lang.RuntimeException: java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:108) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:69) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:80) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:221) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:179) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.heartbeatToMasterIfLocalbeatFail(Worker.java:456) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.doHeartBeat(Worker.java:361) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.lambda$loadWorker$2(Worker.java:209) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.2.0.y.jar:2.2.0.y]
Caused by: java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:117) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 10 more
Caused by: javax.security.auth.login.LoginException: No password provided
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:919) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.storm.messaging.netty.Login.login(Login.java:300) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.messaging.netty.Login.<init>(Login.java:84) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:112) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 10 more
{code}
It can be reproduced by
{code:java}
/storm jar /home/y/lib64/jars/storm-starter.jar  org.apache.storm.starter.WordCountTopology wc -c topology.debug=false

kinit -R # refresh TGT. This is must-have. So upload-credentials will do something and trigger the bug

storm upload-credentials wc

## Errors will show up in worker log in up to 30s (credential refresh period)
{code}
 

*BUGS*

 

*BUG1* When new credentials got uploaded, Worker will try to update credentials. But while it does it, it will also try to replace WorkerToken if it changes. But it has a bug in the code:

[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/ClientAuthUtils.java#L411-L416]

 

Here in the code, ""token"" could equal to ""previous"" if tokens didn't change because WorkerToken.equals() method only cares about the content of WorkerToken. The result of this function is the tokens got removed completely.

So in this case, because tokens are not present, Worker will fall back to use kerberos to connect to Nimbus/Supervisor. [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java#L122-L139]

And here comes the second bug

*BUG2*. Kerberos connection from Worker to Nimbus/Supervisor is not working properly, hence the error logs above. "
STORM-3575,Fix Scheduler Status on failure after multiple attempts,"The RAS on multiple attempts when fails to schedule a topology, it is overriding status as to with {color:#FF0000}_Failed to schedule within 5 attempts_{color}
But I think, it should append this message to existing reason/status on the topology.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java#L239]"
STORM-3562,"Storm code can not built repeatedly. It means that the same code builds different packages at different times. After comparison, the class file with the same name has the same content, but the location of the method is different. Is there any solution?",
STORM-3553,Upgrade JQuery to 3.5.1,"JQuery < 3.4.0 has some security issues ([https://snyk.io/vuln/npm:jquery)]

 JQuery 1.11.1 that currently being used is having this security issue:
 - Prototype Pollution"
STORM-3552,Storm CLI set_log_level no longer updates the log level,"Using the example StatefulWindowingTopology, when trying to update the log level via command line with the following command a NullPointer is thrown in the worker log and the log level is not updated.
{code:java}
storm set_log_level -l ROOT=DEBUG:0 test{code}
{code:java}
2019-12-09 17:16:02.600+0100 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl main-EventThread [ERROR] Event listener threw exception
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_131]
        at org.apache.logging.log4j.Level.getLevel(Level.java:261) ~[log4j-api-2.11.2.jar:2.11.2]
        at org.apache.storm.daemon.worker.LogConfigManager.setLoggerLevel(LogConfigManager.java:145) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.daemon.worker.LogConfigManager.processLogConfigChange(LogConfigManager.java:98) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.daemon.worker.Worker.checkLogConfigChanged(Worker.java:422) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl.issueMapCallback(StormClusterStateImpl.java:177) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl$1.changed(StormClusterStateImpl.java:122) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.ZKStateStorage$ZkWatcherCallBack.execute(ZKStateStorage.java:243) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.zookeeper.ClientZookeeper.lambda$mkClientImpl$0(ClientZookeeper.java:314) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$7.apply(CuratorFrameworkImpl.java:1048) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$7.apply(CuratorFrameworkImpl.java:1041) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.processEvent(CuratorFrameworkImpl.java:1040) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$000(CuratorFrameworkImpl.java:66) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$1.process(CuratorFrameworkImpl.java:126) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.ConnectionState.process(ConnectionState.java:185) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:533) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]{code}
This appears to be a regression from the migration from clojure to java in STORM-1267"
STORM-3533,Make a distinct storm-ras artifact,"Storm 2.0.0 release page [here|http://storm.apache.org/2019/05/30/storm200-released.html] suggest us to put storm-server as test scope for testing. But RAS packages are in storm-server package too, and we need it when we launch our topology. Some of RAS packages should be either integrated in storm-core or in a distinct package storm-ras, so that we do not have to put storm-server as compile test, which we do for now."
STORM-3529,Catch and log RetriableException in KafkaOffsetMetric,"When the KafkaOffsetMetric.getValueAndReset method calls the KafkaClient methods, exceptions may be thrown. When these exceptions are retriable, we should not crash the worker by letting them escape the method. We should instead catch and log the exception.

An example of the desired behavior can be seen at https://github.com/apache/storm/blob/7b1a98fc10fad516ef9ed0b3afc53a1d7be8a169/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L295"
STORM-3524,worker fails to launch due to missing parent directory for localized resource,"{code:java}
2019-10-14 14:59:29.839 o.a.s.l.LocalizedResource AsyncLocalizer Executor - 2 [WARN] Nothing to cleanup with badeDir /home/y/var/storm/supervisor/usercache/xxx/filecache/files even though we expected there to be something there 2019-10-14 14:59:29.839 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [WARN] Failed to download blob xxx:xxx.topology.yaml will try again in 100 ms java.nio.file.NoSuchFileException: /home/y/var/storm/supervisor/usercache/xxx/filecache/files at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[?:1.8.0_181] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_181] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_181] at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_181] at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_181] at org.apache.storm.localizer.LocalizedResource.lambda$fetchUnzipToTemp$4(LocalizedResource.java:257) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.LocallyCachedBlob.fetch(LocallyCachedBlob.java:92) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.LocalizedResource.fetchUnzipToTemp(LocalizedResource.java:250) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$10(AsyncLocalizer.java:277) ~[storm-server-2.0.1.y.jar:2.0.1.y] at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) [?:1.8.0_181] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_181] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}
 

A worker on a supervisor was failing to come up with this error continually presenting."
STORM-3523,supervisor restarts when releasing slot with missing file,"{code:java}
2019-10-03 16:25:32.809 o.a.s.d.s.Slot SLOT_6719 [ERROR] Error when processing event
java.io.FileNotFoundException: File 'x/storm/supervisor/stormdist/xxx-190213-004131-001-209-1550018519/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:308) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:469) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:303) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:359) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:460) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:435) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:229) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [storm-server-2.0.1.y.jar:2.0.1.y]
2019-10-03 16:25:32.810 o.a.s.u.Utils SLOT_6719 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:550) [storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:947) [storm-server-2.0.1.y.jar:2.0.1.y]
{code}"
STORM-3521,Storm CLI jar command doesn't handle topology arguments correctly,
STORM-3520,Storm CLI drpc-client incorrectly validating function args,
STORM-3519,Change ConstraintSolverStrategy::backtrackSearch to avoid StackOverflowException,"When ConstraintSolverStrategy::backtrackSearch recursively call itself - after approximately 20000 calls, there is a StackOverflowException. This can be replicated by running TestConstraintSolverStrategy::testScheduleLargeExecutorConstraintCount."
STORM-3515,Storm CLI config options are passed directly to underlying JAVA cli,
STORM-3513,Add note to install instructions for Windows users that they need to install Visual C++ redistributable,"See the note at the bottom of this section https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#maven

And also this stackoverflow question https://stackoverflow.com/questions/58123218/i-have-a-trouble-running-nimbus-apache-storm-2-0-0-on-windows"
STORM-3512,Nimbus failing on startup with `GLIBC_2.12' not found,"Nimbus failing to start with and exception (see below).

 
{code:java}
2019-09-25 17:21:56.013 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni3787537456845796855.so: /lib64/libpthread.so.0: version `GLIBC_2.12' not found (required by /tmp/librocksdbjni3787537456845796855.so)
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:647) ~[storm-client-2.0.0.jar:2.0.0]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:626) ~[storm-client-2.0.0.jar:2.0.0]
        at org.apache.storm.utils.Utils.lambda$createDefaultUncaughtExceptionHandler$2(Utils.java:982) ~[storm-client-2.0.0.jar:2.0.0]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_211]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_211]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_211]
Caused by: java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni3787537456845796855.so: /lib64/libpthread.so.0: version `GLIBC_2.12' not found (required by /tmp/librocksdbjni3787537456845796855.so)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_211]
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_211]
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_211]
        at java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_211]
        at java.lang.System.load(System.java:1086) ~[?:1.8.0_211]
        at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.RocksDB.<clinit>(RocksDB.java:35) ~[rocksdbjni-5.8.6.jar:?]
        at org.apache.storm.metricstore.rocksdb.RocksDbStore.prepare(RocksDbStore.java:67) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.metricstore.MetricStoreConfig.configure(MetricStoreConfig.java:33) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:528) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:471) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:465) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.launchServer(Nimbus.java:1282) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1307) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1312) ~[storm-server-2.0.0.jar:2.0.0]
 {code}
 

Environment:
{code:java}
>>> uname -a
Linux gctdwp03 3.0.101-108.98-default #1 SMP Mon Jul 15 13:58:06 UTC 2019 (262a94d) x86_64 x86_64 x86_64 GNU/Linux
 
>>> cat /etc/SuSE-release
SUSE Linux Enterprise Server 11 (x86_64)
VERSION = 11
PATCHLEVEL = 4{code}
 "
STORM-3511,Nimbus logs got flood with TTransportException Error messages (because of thrift 0.12.0),"Submitting a wordCountTopology works in secure cluster. But the following

{code:java}
2019-09-25 13:53:46.560 o.a.s.t.s.TThreadPoolServer pool-15-thread-1 [ERROR] Thrift error occurred during processing of message.
org.apache.storm.thrift.transport.TTransportException: null
        at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.read(TSaslTransport.java:433) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:27) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310) [shaded-deps-2.0.1.y.jar:2.0.1.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}
flood the nimbus log. (2.0.1.y is our internal version. The code is basically community 2.0.0)

This is similar to the issue found in Thrift community and got fixed in 0.13.0 but it's not released yet https://issues.apache.org/jira/browse/THRIFT-4805

"
STORM-3506,prevent topology from overriding STORM_CGROUP_HIERARCHY_DIR and WORKER_METRICS,We had an issue where users were using older versions of storm the set these differing values than the cluster supported.  These parameters don't make sense for topologies to override.
STORM-3504,AsyncLocalizerTest is stubbing file system operations,"AsyncLocalizerTest mocks AdvancedFSOps in order to avoid interacting with the real file system. This is most likely unnecessary, and could be replaced with using temporary files/directories. If possible, we should rewrite the tests to use temporary files, and use the real AdvancedFSOps."
STORM-3501,Local Cluster worker restarts,"I was trying to launch a topology that I'm developing (in 2.0.0) and noticed that the worker was getting restarted each ~30 seconds. 
 I placed a breakpoint in the _kill_ method of _LocalContainer_ ([https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainer.java#L66]) to try and understand why the worker was getting restarted. 
  
 The call stack was:
_kill:66, LocalContainer (org.apache.storm.daemon.supervisor)
killContainerFor:269, Slot (org.apache.storm.daemon.supervisor)
handleRunning:724, Slot (org.apache.storm.daemon.supervisor) 
stateMachineStep:218, Slot (org.apache.storm.daemon.supervisor)
run:931, Slot (org.apache.storm.daemon.supervisor) _
  
 With this I can understand that the worker is killed because a blob has changed ([https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java#L724]). In fact, there's a changing blob in the _dynamicState_ at that point.
  
 I checked the _AsyncLocalizer_ which downloads, caches blobs locally, and notifies the Slot state machine of a changing blob.
  
 I noticed this:
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L339]
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L265]
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java#L142]
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java#L192]
  

Which tell me that (correct me if I'm wrong):
 * Supervisor tries to update blobs each 30 seconds.
 * The topology jar blob requires extraction of the resources directory (either from a jar or directly in a classpath URL). It does so in _fetchUnzipToTemp_ and it's existence is checked in _isFullyDownloaded_.
 * The Slot is notified of a changing blob if:
 * the remote version is different from the local version (the code has changed).
 * OR the blob is not fully downloaded (the jar exists, and the extracted resources directory exists).

 
 Well, I did not have a resources folder under the root of the classpath, and that's why the worker was being restarted each ~30 seconds, as the Slot was being notified of a changing blob everytime _updateBlobs_ ran. 
 I created a resources folder (with dummy files) under the root of the classpath and the problem is now solved.
  
 However, if I understand correctly, the resources folder is only required for _multilang_. Our topologies do not use _multilang_ and this do not happen in Storm 1.1.3 for instance.

 

Happy to submit MR."
STORM-3494,Use UserGroupInformation to login to HDFS only once per process,"UserGroupInformation (UGI) loginUserFromKeytab should be used only once in a process to login to hdfs because it overrides static fields. Also loginUserFromKeytabAndReturnUGI function is also problematic according to hadoop team. So the correct way to connect to hdfs is to use UGI loginUserFromKeytab once and only in a process.

Currently we only use HDFS in hdfs-blobstore. It works correctly. But the code is implemented in the hdfs-blobstore plugin. It will be problematic if we want to add another plugin which also needs to connect to HDFS.

So the proposal here is to remove the login piece of code from hdfs-blobstore. And explicitly login to hdfs once and only once when the server (nimbus, supervisor, etc) or the client (storm cli command) launches. It can guarantee one login per process.

The plugins like hdfs-blobstore then simply assume the process has already logged in."
STORM-3493,Allow overriding python interpreter by environment variable,"$subj

https://github.com/apache/storm/pull/3111

 "
STORM-3486,Upgrade to Jersey 2.29,Jersey 2.29 supports Java 11 http://blog.supol.cz/?p=144
STORM-3481,IllegalArgumentException in ConstraintSolverStrategy,"We found this scheduling error based on our internal mirror. 
{code:java}
2019-08-06 13:00:20.344 o.a.s.s.r.ResourceAwareScheduler timer [ERROR] propane-0-170-1564778552 Internal Error - Exception thrown when scheduling. Please check logs for details
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Don't know how to convert null to int
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_181]
        at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_181]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.scheduleTopology(ResourceAwareScheduler.java:164) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.schedule(ResourceAwareScheduler.java:117) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.schedule(BlacklistScheduler.java:118) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2092) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2256) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2242) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2187) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$29(Nimbus.java:2890) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.1.y.jar:2.0.1.y]
Caused by: java.lang.IllegalArgumentException: Don't know how to convert null to int
        at org.apache.storm.utils.ObjectReader.getInt(ObjectReader.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.strategies.scheduling.ConstraintSolverStrategy.schedule(ConstraintSolverStrategy.java:263) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.lambda$scheduleTopology$3(ResourceAwareScheduler.java:161) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_181]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_181]
{code}
"
STORM-3478,Upgrade to JUnit 5.5.1,"https://github.com/apache/storm/pull/2990 upgraded to 5.5.0-M1, we should upgrade to a final version."
STORM-3477,HDFS blobstore isRemoteBlobExists performs unnecessary file opens,"isRemoteBlobExists eventually performs an HDFS file open and returns an input file stream, which is not closed.

 

We should just be calling file exists instead.  Lower HDFS overhead."
STORM-3476,LocalizedResourceRetentionSet cleanup causing excessive load on Hadoop namenode,"One of our local dev Hadoop devs noticed our storm user was by far creating the heaviest load on our production Hadoop cluster.  Looking at one of the heaviest supervisor nodes, and comparing debug logs to the Hadoop audit log, it looks like LocalizedResourceRetentionSet cleanup was constantly doing opens and never deleting any files.

 

The frequency can be addressed by supervisor.localizer.cleanup.interval.ms, but even so, it seems we will continually look for files to delete even when the target size is acceptable, resulting in unnecessary calls to Hadoop.

 

 "
STORM-3473,Hive can't read records written from HiveBolt,"I'm trying to stream items from storm into hive using the HiveBolt, but Hive does not seem to see the records at all.

Test program:
{code:java}
package com.datto.hivetest;

import org.apache.storm.Config;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.hive.bolt.HiveBolt;
import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.streams.StreamBuilder;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.Time;

import java.util.Map;
import java.util.Random;

public class MainStorm {
	public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
		HiveOptions hiveOptions = new HiveOptions(
			""<url>"",
			""default"",
			""test_table"",
			new JsonRecordHiveMapper()
				.withColumnFields(new Fields(""value""))
		)
			.withAutoCreatePartitions(true);

		StreamBuilder builder = new StreamBuilder();
		builder.newStream(new TestSpout())
			.map(tup -> tup.getStringByField(""word"").toLowerCase())
			.to(new HiveBolt(hiveOptions));

		Config config = new Config();
		config.setMessageTimeoutSecs(30);
		config.setMaxSpoutPending(1024);
		config.setClasspath(""/etc/hadoop/conf/"");

		StormSubmitter.submitTopology(""hive-test"", config, builder.build());
	}

	public static class TestSpout extends BaseRichSpout {
		private transient SpoutOutputCollector out;
		private transient Random random;

		@Override
		public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
			out = collector;
			random = new Random();
		}

		@Override
		public void nextTuple() {
			try {
				Time.sleep(100);
			} catch (InterruptedException e) {
				Thread.currentThread().interrupt();
				throw new RuntimeException(e);
			}

			final String[] words = new String[]{ ""nathan"", ""mike"", ""jackson"", ""golda"", ""bertels"" };
			final String word = words[random.nextInt(words.length)];
			out.emit(new Values(word));
		}

		@Override
		public void declareOutputFields(OutputFieldsDeclarer declarer) {
			declarer.declare(new Fields(""word""));
		}
	}
}
{code}
Table creation:
{code:sql}
CREATE TABLE test_table (value string) CLUSTERED BY (value) INTO 4 BUCKETS STORED AS ORC TBLPROPERTIES('orc.compress' = 'ZLIB', 'transactional' = 'true');

GRANT ALL ON test_table TO USER storm;{code}

Setting the ACL:

{code}
sudo -u hdfs hdfs dfs -setfacl -m user:storm:rwx /warehouse/tablespace/managed/hive/test_table
sudo -u hdfs hdfs dfs -setfacl -m default:user:storm:rwx /warehouse/tablespace/managed/hive/test_table
{code}

Hive results after running for around 10 minutes:

{code:java}
> SELECT COUNT(*) FROM test_table;
INFO  : Compiling command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403): SELECT COUNT(*) FROM test_table
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403); Time taken: 1.138 seconds
INFO  : Executing command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403): SELECT COUNT(*) FROM test_table
INFO  : Completed executing command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403); Time taken: 0.013 seconds
INFO  : OK
+------+
| _c0  |
+------+
| 0    |
+------+
{code}

So hive thinks there are no results, which isn't good. But if I look at hdfs, there are some files there:

{code}
# sudo -u hdfs hdfs dfs -ls -R -h /warehouse/tablespace/managed/hive/test_table
drwxrwx---+  - storm hadoop          0 2019-07-22 19:15 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100
-rw-rw----+  3 storm hadoop          1 2019-07-22 19:15 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/_orc_acid_version
-rw-rw----+  3 storm hadoop     74.4 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00001
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00001_flush_length
-rw-rw----+  3 storm hadoop     73.4 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00002
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00002_flush_length
-rw-rw----+  3 storm hadoop     84.9 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00003
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00003_flush_length
{code}

And they seem to have valid rows:

{code}
❯❯❯ ./orc-contents /tmp/bucket_00002  | head
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 0, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 1, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 2, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 3, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 4, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 5, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 6, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 7, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 8, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 9, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{code}

I can insert into the table manually, and I've also written a test java program that uses the hive streaming API to write one row, and hive sees those inserts. I don't see any errors in the storm logs; the tuples seem to be flushed and acked ok. I don't think I've seen any errors in the metastore logs either.

Anyone know what's up? I can get more info if needed."
STORM-3472,"STORM-3411 should have tests, and we shouldn't catch NPE for control flow","I think the code merged in STORM-3411 should have added tests that the new functionality works.

We should get rid of the new bit of code that try-catches an NPE to check whether the downloaded file is inside a worker dir. Instead, we should move the name generation up the call hierarchy to a place where we can tell whether we're inside a worker dir or not."
STORM-3470,Possible Null Dereference in SimpleSaslServer authentication function,"On line 183, nid could possible be null. By comparing using ""nid.equals()"" we will get a null pointer exception instead of comparing possibly null values."
STORM-3466,storm-kafka-monitor not found jar,"Hi, i'm a beginner to the storm. in order words, i'm a newbie.

 

I have tried to upgrade from Storm 1.2.2 to 2.0.0. and confirmed that kafka spout lag does not work. changed the value of ui.disable.spout.lag.monitoring to false, but it did not work.

So I checked the log of storm ui and got the following error message.

 

org.apache.storm.utils.ShellUtils$ExitCodeException: Error: Could not find or load main class .apache-storm-2.0.0.lib-tools.storm-kafka-monitor.audience-annotations-0.5.0.jar

at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:271) ~[storm-client-2.0.0.jar:2.0.0]
 at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:194) ~[storm-client-2.0.0.jar:2.0.0]...

 

In version 1.2.2, the storm-kafka-monitor works fine. However, version 2.0.0 throws an error."
STORM-3437,More license check automation,
STORM-3436,"TupleInfo.id is not set, making debugging more difficult than it should be","TupleInfo has an id field, which is supposed to contain the tuple root id. This is printed when we enable topology debug logging, and is supposed to make it easier to track down e.g. why a tuple timed out.

We currently don't set this field, so it's always null. The logs end up looking like

2019-06-29 11:26:31.990 o.a.s.e.s.SpoutExecutor Thread-14-kafka_spout-executor[4, 4] [INFO] SPOUT Acking message null {topic-partition=kafka-spout-test-0, offset=28, numFails=0, nullTuple=false}

The message doesn't contain the root id, and is therefore not useful for debugging."
STORM-3435,Use the Jetty BOM,"We should import the Jetty BOM, we're currently mixing different versions of Jetty jars together on the classpath because we only declare some of them, while others are pulled in transitively."
STORM-3422,TupleCaptureBolt is not thread-safe,"Marking this as Major because it's a crash. That said, the problem lies in testing code. This makes integration testing hard, but the issue does not affect any production code.

 

First, let me show you a stack trace for Storm 2.0.0:

{{java.lang.RuntimeException: java.lang.NullPointerException}}
{{at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:171) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:158) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.utils.Utils$1.run(Utils.java:388) [storm-client-2.0.0.jar:2.0.0]}}
{{at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]}}
{{Caused by: java.lang.NullPointerException}}
{{at org.apache.storm.testing.TupleCaptureBolt.execute(TupleCaptureBolt.java:45) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:234) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.Executor.accept(Executor.java:275) ~[storm-client-2.0.0.jar:2.0.0]}}
{{... 6 more}}

 

 Here's the same for Storm 1.2.2:

{{java.lang.RuntimeException: java.lang.NullPointerException}}
{{at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:74) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.daemon.executor$fn__10795$fn__10808$fn__10861.invoke(executor.clj:861) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [storm-core-1.2.2.jar:1.2.2]}}
{{at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]}}
{{at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]}}
{{Caused by: java.lang.NullPointerException}}
{{at org.apache.storm.testing.TupleCaptureBolt.execute(TupleCaptureBolt.java:50) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.daemon.executor$fn__10795$tuple_action_fn__10797.invoke(executor.clj:739) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.daemon.executor$mk_task_receiver$fn__10716.invoke(executor.clj:468) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.disruptor$clojure_handler$reify__10135.onEvent(disruptor.clj:41) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.2.jar:1.2.2]}}
{{... 6 more}}

 

This is a topology running as our integration test using {{Testing.completeTopology()}}. Both the stack traces point to the same code in the {{TupleCaptureBolt}} - its {{name}} field is not safely published (it should be marked {{final}}), and the internal {{HashMap}} does not safely store the data put in it. Perhaps it should be a {{ConcurrentHashMap}}?

Would you accept a PR with a more detailed analysis, or are you going to investigate on your side?"
STORM-3408,Rocks version shipped with Storm2 doesn't work on Windows 10,"The version of rocks referenced by storm 2 cannot be used on windows 10 due to https://github.com/facebook/rocksdb/issues/2531 - this means `LocalClusterRunner` cannot be instantiated.

The stack trace is
{code}java.lang.UnsatisfiedLinkError: C:\Users\...\AppData\Local\Temp\librocksdbjni5428427063666929934.dll: A dynamic link library (DLL) initialization routine failed

	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
	at java.lang.Runtime.load0(Runtime.java:809)
	at java.lang.System.load(System.java:1086)
	at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78)
	at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56)
	at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64)
	at org.rocksdb.RocksDB.<clinit>(RocksDB.java:35)
	at org.apache.storm.metricstore.rocksdb.RocksDbStore.prepare(RocksDbStore.java:67)
	at org.apache.storm.metricstore.MetricStoreConfig.configure(MetricStoreConfig.java:33)
	at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:528)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:244)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:159)
	...{code}

This issue is fixed by at least rocks version 5.17.2. I also note rocks v6 is out.

This is a blocker for us - we cannot upgrade to storm 2 as-is, as we cannot run our local unit tests using a local cluster runner on our Windows-based development machines."
STORM-3406,Allow customizing storm-hdfs FileReader ,Allow customizing storm-hdfs _FileReader_. The interface is currently package private preventing any possible custom implementation
STORM-3398,Closing socket for xxx because of error (kafka.network.Processor) kafka.network.InvalidRequestException,"deploy a topology on storm cluster, storm start a *org.apache.storm.kafka.monitor.KafkaOffsetLagUtil* process, this process error:
{code:java}
[2019-05-31 11:24:54,425] ERROR Closing socket for xxxx:9092-xxxxx:17547 because of error (kafka.network.Processor)
kafka.network.InvalidRequestException: Error getting request for apiKey: 3 and apiVersion: 2
at kafka.network.RequestChannel$Request.liftedTree2$1(RequestChannel.scala:95)
at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:87)
at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:488)
at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:483)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at kafka.network.Processor.processCompletedReceives(SocketServer.scala:483)
at kafka.network.Processor.run(SocketServer.scala:413)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid version for API key 3: 2
at org.apache.kafka.common.protocol.ProtoUtils.schemaFor(ProtoUtils.java:31)
at org.apache.kafka.common.protocol.ProtoUtils.requestSchema(ProtoUtils.java:44)
at org.apache.kafka.common.protocol.ProtoUtils.parseRequest(ProtoUtils.java:60)
at org.apache.kafka.common.requests.MetadataRequest.parse(MetadataRequest.java:96)
at org.apache.kafka.common.requests.AbstractRequest.getRequest(AbstractRequest.java:48)
at kafka.network.RequestChannel$Request.liftedTree2$1(RequestChannel.scala:92)
{code}
Too many kafka sockets lead to delayed consumption and production of kafka services"
STORM-3397,Upgrade to Zookeeper 3.5,Zookeeper 3.5 has a stable release now. We should upgrade I think.
STORM-3396,uploading dependency jars too slow when StormSubmitter and Nimbus located in different IDC,"     when storm client and server is locating in different IDC(one is in Beijing, while another in Shanghai), uploading dependency jars may take a very long long time(in my case, 31minutes!)...

    when I digged into this, I found that in DependencyUploader,  method ""uploadDependencyToBlobStore"" using JDK NIO's Files.copy to upload local jars to remote Blob server. In Files.copy(Path, OutputStream), the buffer size is 8k by default, given that latency between Beijing and Shanghai is about 20ms, a dependency fat jar of 360M finally cost me 'a lunch time' to finish uploading!!!

   "
STORM-3393,OffsetManager doesn't recover after missing offsets,"When missing offsets are encountered, but a committable offset exists after the missing offset, the condition is detected and logged but not properly processed.  You will see three log messages in this case:
{code:java}
Processed non-sequential offset.  The earliest uncommitted offset is no longer part of the topic.  Missing offset: [{}], Processed: [{}]
...
Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset
...
Topic-partition [{}] has no offsets ready to be committed{code}
However, this is not the proper handling.  While a committable offset has been found, the found flag is not set to true (resulting in the 3rd log message).

The fix is to add a found=true within this logic:

In OffsetManager.java
{code:java}
if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {
                        LOG.debug(""Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset"",
                            currOffset, nextCommitOffset);
                        nextCommitOffset = currOffset + 1;
                        found = true;       //  ADD THIS LINE TO FIX THIS BUG
                    }{code}
Because of this bug, offsets are not committed properly."
STORM-3391,"MongoMapState causes ""IllegalArgumentException: Invalid BSON field name _id"" while multiPut operation","MongoMapState causes ""IllegalArgumentException: Invalid BSON field name _id"" while multiPut operation. 

mongoDB server: 3.6.7
mongo-java-driver: 3.8.2"
STORM-3390,Lock python test dependencies so we don't get accidentally upgraded,"Tests are currently failing on Travis with

{code}
[INFO] --- exec-maven-plugin:1.6.0:exec (python2.7-test) @ storm-client ---
Traceback (most recent call last):
  File ""test_storm_cli.py"", line 20, in <module>
    import mock
  File ""/home/travis/.local/lib/python2.7/site-packages/mock/__init__.py"", line 2, in <module>
    import mock.mock as _mock
  File ""/home/travis/.local/lib/python2.7/site-packages/mock/mock.py"", line 69, in <module>
    from six import wraps
ImportError: cannot import name wraps
{code}

This is most likely because we're installing ""mock"" via pypi during the build, but we're not specifying a version. Since mock just released a new version, we're getting upgraded to that one on Travis.

I think we don't want this to happen automagically. "
STORM-3387,Test failure in integration test,"{code}
[ERROR] testTumbleTime(org.apache.storm.st.tests.window.TumblingWindowTest)  Time elapsed: 36.751 s  <<< FAILURE!
com.google.gson.JsonSyntaxException: com.google.gson.stream.MalformedJsonException: Unterminated string at line 1 column 32 path $.now
	at com.google.gson.internal.Streams.parse(Streams.java:60)
	at com.google.gson.internal.bind.TreeTypeAdapter.read(TreeTypeAdapter.java:65)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:129)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:220)
	at com.google.gson.Gson.fromJson(Gson.java:887)
	at com.google.gson.Gson.fromJson(Gson.java:852)
	at com.google.gson.Gson.fromJson(Gson.java:801)
	at com.google.gson.Gson.fromJson(Gson.java:773)
	at org.apache.storm.st.topology.window.data.TimeData.fromJson(TimeData.java:58)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:373)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.storm.st.wrapper.TopoWrap.deserializeLogData(TopoWrap.java:303)
	at org.apache.storm.st.wrapper.TopoWrap.getDeserializedDecoratedLogLines(TopoWrap.java:295)
	at org.apache.storm.st.tests.window.WindowVerifier.runAndVerifyTime(WindowVerifier.java:100)
	at org.apache.storm.st.tests.window.TumblingWindowTest.testTumbleTime(TumblingWindowTest.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:84)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:714)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:901)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1231)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)
	at org.testng.TestRunner.privateRun(TestRunner.java:767)
	at org.testng.TestRunner.run(TestRunner.java:617)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:334)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)
	at org.testng.SuiteRunner.run(SuiteRunner.java:240)
	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86)
	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149)
	at org.testng.TestNG.run(TestNG.java:1057)
	at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:135)
	at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:193)
	at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:94)
	at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:146)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: com.google.gson.stream.MalformedJsonException: Unterminated string at line 1 column 32 path $.now
	at com.google.gson.stream.JsonReader.syntaxError(JsonReader.java:1559)
	at com.google.gson.stream.JsonReader.nextQuotedValue(JsonReader.java:1017)
	at com.google.gson.stream.JsonReader.nextString(JsonReader.java:815)
	at com.google.gson.internal.bind.TypeAdapters$29.read(TypeAdapters.java:718)
	at com.google.gson.internal.bind.TypeAdapters$29.read(TypeAdapters.java:714)
	at com.google.gson.internal.Streams.parse(Streams.java:48)
	... 50 more
{code}"
STORM-3386,Set minimum Maven version for build to 3.5.0,"Alexandre Vermeerbergen found that the build doesn't work on Maven 3.3.9. This is likely because some plugin requires Maven version 3.5.0, but they forgot to specify that requirement in their POM.

We might as well bump our Maven version check to 3.5.0."
STORM-3384,storm set-log-level command throws wrong exception when the topology is not running,"https://github.com/apache/storm/blob/1.1.x-branch/storm-core/src/clj/org/apache/storm/command/set_log_level.clj#L31
will throw an exception like the following if the topology is not running
{code:java}
3396 [main] INFO  b.s.c.set-log-level - Sent log config LogConfig(named_logger_level:{ROOT=LogLevel(action:UPDATE, target_log_level:DEBUG, reset_log_level_timeout_secs:30)}) for topology w
Exception in thread ""main"" java.lang.IllegalArgumentException: No matching field found: IllegalArgumentException for class java.lang.String
	at clojure.lang.Reflector.getInstanceField(Reflector.java:271)
	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:315)
	at backtype.storm.command.set_log_level$get_storm_id.invoke(set_log_level.clj:31)
	at backtype.storm.command.set_log_level$_main.doInvoke(set_log_level.clj:75)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at backtype.storm.command.set_log_level.main(Unknown Source)
{code}
"
STORM-3381,Upgrading to Zookeeper 3.4.14 added an LGPL dependency,https://issues.apache.org/jira/browse/ZOOKEEPER-3367
STORM-3379,Intermittent NPE during worker boot in local mode,"{quote}
java.io.IOException: java.lang.NullPointerException
	at org.apache.storm.daemon.supervisor.LocalContainer.launch(LocalContainer.java:57) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.LocalContainerLauncher.launchContainer(LocalContainerLauncher.java:49) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobUpdate(Slot.java:536) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:230) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:931) [storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
Caused by: java.lang.NullPointerException
	at org.apache.storm.daemon.worker.WorkerState.readWorkerExecutors(WorkerState.java:623) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerState.<init>(WorkerState.java:156) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.worker.Worker.loadWorker(Worker.java:174) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.worker.Worker.lambda$start$0(Worker.java:166) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_201]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_201]
	at org.apache.storm.daemon.worker.Worker.start(Worker.java:165) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.LocalContainer.launch(LocalContainer.java:55) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	... 4 more
{quote}

The issue is that the WorkerState tries to read executor assignment from ZK, and gets null back. "
STORM-3378,"Clean up integration test a bit, and switch to JUnit 5",
STORM-3377,Scheduling may not fully utilize cluster with STORM_WORKER_MIN_CPU_PCORE_PERCENT set,I encountered a bug when testing STORM_WORKER_MIN_CPU_PCORE_PERCENT.  I need to add a unit test and fix for the case I encountered.
STORM-3376,Storm drops messages in the interval between server bind and callback registration,"This is one of the causes of unstable integration tests.

When a worker starts, WorkerState boots up a Server, which binds Netty to a port in order to receive messages from other workers. Slightly later, we register a callback with the server that defines where the Server should deliver received messages.

In the interim between binding the port and registering the callbacks, Storm quietly discards any received messages.

Other workers will happily send messages to the worker that is not ready, as the sending side considers an open connection to be sufficient to send messages.

We should change the setup so we set the receive callback before we start the Server."
STORM-3374,StormClientHandler exceptionCaught() log message does not warrant callstack,"Info level message spits out a large stack trace.  Since this is apparently not serious, let's not pollute the log."
STORM-3373,"Use Log4j BOM, ensure SLF4J dependencies use the same version, upgrade SLF4J to latest",
STORM-3372,HDFS bolt can throw NPE on shutdown if not using a TimedRotationPolicy,"{quote}42612 [SLOT_1024] ERROR o.a.s.d.s.Slot - Error when processing event
java.lang.NullPointerException: null
    at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.cleanup(AbstractHdfsBolt.java:261) ~[f083f1dc515311e9868bcf07babd3298.jar:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.executor$fn__9739.invoke(executor.clj:878) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at clojure.lang.MultiFn.invoke(MultiFn.java:233) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.executor$mk_executor$reify__9530.shutdown(executor.clj:437) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.worker$fn__10165$exec_fn__1369__auto__$reify__10167$shutdown_STAR___10187.invoke(worker.clj:684) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.worker$fn__10165$exec_fn__1369__auto__$reify$reify__10213.shutdown(worker.clj:724) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:67) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.LocalContainer.kill(LocalContainer.java:69) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.killContainerForChangedAssignment(Slot.java:311) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.handleRunning(Slot.java:527) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:265) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:752) [storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]{quote}
The error is due to a bug in storm-hdfs.

That variable in https://github.com/apache/storm/blob/v1.2.1/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L261 is only initialized if the rotation policy is a TimedRotationPolicy, which yours isn't."
STORM-3371,Metrics v2 is inaccessible in Trident StateFactory. ,"The injected IMetricsContext only grants access to metrics v1 methods. Metrics v2 is normally accessible via TopologyContext, but this is not supplied to StateFactory. We should add metrics v2 methods to IMetricsContext."
STORM-3370,Make StormMetricRegistry non-static,In STORM-3197 we made the StormMetricsRegistry for internal Storm components non-static. I think we should do the same for the worker-oriented StormMetricRegistry.
STORM-3369,Apache Storm 2.0.0 release artifacts are not avaible in maven central,"Hi All,

Couple of weeks ago, Apache storm 2.0.0 was released (based on github release branches). We have been eyeing the release for some time. With such great news, we wanted to start utilizing it.

However, the release artifacts have not been published yet into maven central: [https://search.maven.org/search?q=org.apache.storm]

We were wondering if it was a matter of time and the release process is not complete yet, or it was a missed step.

Thanks"
STORM-3367,Upgrade Dropwizard Metrics to 5.0.0-rc2,"Dropwizard Metrics 5 introduces metrics tagging. 

One of the goals of metrics v2 was to move worker metrics to dropwizard metrics instead of sending the metrics via Zookeeper. A good way to do this would be to register the metrics with Dropwizard and create a NimbusMetricsReporter that sends metrics to Nimbus. Since we don't want to send all metrics, tags would be useful to allow the reporter to send only metrics relevant for Storm UI.

In relation to https://issues.apache.org/jira/browse/STORM-3204, it will probably also be an easier upgrade, since Metrics 5 has a new package/artifact name, so there won't be conflicts with dependencies."
STORM-3363,Migrate Aether to maven-resolver as Aether was brought to ASF as a subproject of Apache Maven,"Looks like Aether is donated to ASF (in couple of years) and renamed as ""maven-resolver"" as a subproject of Apache Maven.

[https://github.com/apache/maven-resolver]
https://issues.apache.org/jira/browse/MNG-6007
[http://incubator.apache.org/ip-clearance/maven-aether.html]

As it is a kind of complete fork of Aether and further versions have been published here (the latest version of Aether is 1.1.0, whereas the latest version maven-resolver is 1.3.3), it should be easy to migrate to maven-resolver and reduce using EPLv1 license in Apache Storm."
STORM-3361,Add output of license plugin to git and distributions,"We should make it obvious to users which licenses our dependencies are under. In particular category B licensed dependencies need to be listed in a place users can find them.

We could use the license plugin to generate a list of dependencies along with their licenses, and include the generated file in our distributions."
STORM-3358,Upgrade Storm to Hadoop 3,"We should upgrade to Hadoop 3 at some point.

We are currently blocked by https://issues.apache.org/jira/browse/HBASE-22027, but I believe that is the only bit that prevents us from upgrading."
STORM-3357,Bump Clojure to 1.10,"Clojure 1.10 contains a few fixes for Java 9+, notably https://dev.clojure.org/jira/browse/CLJ-2284, and we're currently using 1.7.0. We should upgrade."
STORM-3356,Storm-hive should not pull in a compile-scope sfl4j binding,"Our Hive dependencies are leaking org.apache.logging.log4j:log4j-slf4j-impl into our compile-scope dependencies. This causes a multiple bindings warning when starting e.g. storm-starter, since the jar gets bundled into the topology jar, and there is also a log4j-slf4j-impl present in the Storm cluster. "
STORM-3355,Make force kill delay for workers follow the supervisor's SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS,"We currently have the supervisor.worker.shutdown.sleep.secs parameter allowing users to specify how long the supervisor should wait between starting the initial graceful shutdown of a worker, and sending the followup force kill. 

When workers are asked to shut down gracefully, they run a shutdown hook that allows 1 second of cleanup, before force halting the JVM. I think it would be good to make the delay between starting the shutdown hook and halting the JVM follow the same config as in the supervisor. 

I don't see why it is useful to specify the force kill delay in the supervisor, if the worker just suicides after one second anyway. Letting the user configure how long shutdown is allowed to take lets them make use of the bolt's cleanup method for cleaning up resources in non-crash scenarios.

Use case here https://stackoverflow.com/questions/55024919/resource-clean-up-after-killing-storm-topology"
STORM-3354,LeaderElector is not shut down properly,"Nimbus' LeaderElector is not shut down when Nimbus shuts down. This can cause test flakiness because the elector callback may be called after the Zookeeper client is closed.

Additionally the LeaderListenerCallback may in some cases quit the leadership election if Nimbus isn't ready to become master, but I don't see any code to re-enter the election.
 
{quote}java.lang.IllegalStateException: Client is not started
         at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:139) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:602) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1191) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1158) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:1155) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:605) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:595) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:360) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:308) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.zookeeper.ClientZookeeper.createNode(ClientZookeeper.java:98) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.nimbus.LeaderListenerCallback.setUpNimbusInfo(LeaderListenerCallback.java:154) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.nimbus.LeaderListenerCallback.leaderCallBack(LeaderListenerCallback.java:96) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.zookeeper.Zookeeper$1.isLeader(Zookeeper.java:123) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:660) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:539) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:590) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT] 
{quote}"
STORM-3353,Upgrade to Curator 4.2.0,Curator 4.2.0 removes an outdated version of Jackson that has some security holes https://issues.apache.org/jira/browse/CURATOR-481.
STORM-3352,Use Netty BOM to lock all Netty artifacts to same version,"We are not properly ensuring that Netty artifacts are the same version. I got a test failure in storm-cassandra, because netty-all is version 4.1.30, but cassandra pulls in Netty in version 4.0.37.

{quote}

java.lang.NoSuchMethodError: io.netty.util.internal.PlatformDependent.normalizedArch()Ljava/lang/String;
    at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:180) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]
    at io.netty.channel.epoll.Native.<clinit>(Native.java:61) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]
    at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:38) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]
    at org.apache.cassandra.transport.Server.run(Server.java:147) ~[cassandra-all-2.1.7.jar:2.1.7]

{quote}"
STORM-3350,Upgrade some old dependencies,"Jackson, ActiveMQ, commons-collections, commons-compress, Kafka and Maven plugins all have newer versions we should be able to upgrade to with little risk. We should do these upgrades."
STORM-3349,"Upgrade Hadoop, HBase and Hive to latest compatible","We should upgrade Hadoop, Hive, HDFS and HBase to the latest compatible versions. HBase in particular has fallen pretty far behind."
STORM-3348,Incorrect message when group id is not provided as kafka spout config on storm ui,"Steps to produce the issue - 
 # Use kafka as source for a spout.
 # Don't provide group id in spout configuration.
 # Start the topology and go to storm UI topology page.
 # Instead of showing kafka spout lags it shows the following message - ""Offset lags for kafka not supported for older versions. Please update kafka spout to latest version."", even though kafka spout is having correct version."
STORM-3347,Storm-starter should not suggest using maven-exec-plugin,"Storm-starter pom contains a maven-exec-plugin section for running examples via that plugin. This doesn't make sense, since the examples don't reference LocalCluster anymore."
STORM-3346,ClassNotFoundException: clojure.lang.persistentList whiile submitting topology to local cluster in storm,"Getting below exceptions while submitting storm topology to local cluster 

Exception in thread ""main"" java.lang.ExceptionInInitializerError
 at clojure.lang.Namespace.<init>(Namespace.java:34)
 at clojure.lang.Namespace.findOrCreate(Namespace.java:176)
 at clojure.lang.Var.internPrivate(Var.java:156)
 at org.apache.storm.LocalCluster.<clinit>(Unknown Source)
 at KafkaCEPTopology.main(KafkaCEPTopology.java:53)
Caused by: Syntax error compiling . at (clojure/core.clj:20:8).
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7114)
 at clojure.lang.Compiler.analyze(Compiler.java:6789)
 at clojure.lang.Compiler.access$300(Compiler.java:38)
 at clojure.lang.Compiler$DefExpr$Parser.parse(Compiler.java:596)
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7106)
 at clojure.lang.Compiler.analyze(Compiler.java:6789)
 at clojure.lang.Compiler.analyze(Compiler.java:6745)
 at clojure.lang.Compiler.eval(Compiler.java:7180)
 at clojure.lang.Compiler.load(Compiler.java:7635)
 at clojure.lang.RT.loadResourceScript(RT.java:381)
 at clojure.lang.RT.loadResourceScript(RT.java:372)
 at clojure.lang.RT.load(RT.java:463)
 at clojure.lang.RT.load(RT.java:428)
 at clojure.lang.RT.doInit(RT.java:471)
 at clojure.lang.RT.<clinit>(RT.java:338)
 ... 5 more
Caused by: java.lang.ClassNotFoundException: clojure.lang.PersistentList
 at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
 at clojure.lang.DynamicClassLoader.findClass(DynamicClassLoader.java:69)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at clojure.lang.DynamicClassLoader.loadClass(DynamicClassLoader.java:77)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at clojure.lang.RT.classForName(RT.java:2207)
 at clojure.lang.RT.classForNameNonLoading(RT.java:2220)
 at clojure.lang.Compiler$HostExpr.maybeClass(Compiler.java:1041)
 at clojure.lang.Compiler$HostExpr$Parser.parse(Compiler.java:982)
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7106)

 "
STORM-3344,blacklist scheduler causing nimbus restart,"{code:java}
2019-02-22 10:48:41.460 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.UnsupportedOperationException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$27(Nimbus.java:2872) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.1.y.jar:2.0.1.y]
Caused by: java.lang.UnsupportedOperationException
        at org.apache.storm.shade.com.google.common.collect.UnmodifiableIterator.remove(UnmodifiableIterator.java:43) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at java.util.AbstractCollection.remove(AbstractCollection.java:293) ~[?:1.8.0_102]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.removeLongTimeDisappearFromCache(BlacklistScheduler.java:216) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.schedule(BlacklistScheduler.java:110) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2070) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2234) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2220) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2165) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$27(Nimbus.java:2868) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        ... 2 more
2019-02-22 10:48:41.461 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:520) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$9(Nimbus.java:564) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:253) [storm-client-2.0.1.y.jar:2.0.1.y]
2019-02-22 10:48:41.462 o.a.s.u.Utils Thread-19 [INFO] Halting after 10 seconds
{code}"
STORM-3343,JCQueueTest can still be flaky,"Made a mistake in the fix for STORM-3310. The consumer in one of the tests check for interrupt in a place it shouldn't. 

{code}
[ERROR] Failures:
[ERROR]   JCQueueTest.lambda$testFirstMessageFirst$0:63 We expect to receive first published message first, but received null expected:<FIRST> but was:<null>

Exception in thread ""Thread-125"" java.lang.RuntimeException: java.lang.InterruptedException: ConsumerThd interrupted
	at org.apache.storm.utils.JCQueueTest$1.accept(JCQueueTest.java:48)
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133)
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110)
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101)
	at org.apache.storm.utils.JCQueueTest$ConsumerThd.run(JCQueueTest.java:207)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException: ConsumerThd interrupted
	... 6 more
{code}

The consumer accept method shouldn't check for interrupt, as that is handled by the ConsumerThd.run method. When the accept check for interrupt is hit, the consumer exits without draining the JCQueue, and the test may fail."
STORM-3342,Add plugin to generate list of dependency licenses to build,"I think it would be helpful if we could easily generate a list of the licenses used by our dependencies. When we do a release, we need to make sure we don't let dependencies with e.g. GPL license slip through, and I think it will be easier if we can list dependencies with their licenses."
STORM-3340,Nicer path handling in storm-webapp,Some of the path handling in storm-webapp is a little haphazard and differs between methods that do similar things. We should make the handling more uniform.
STORM-3339,Port all the AtomicReference to ConcurrentHashMap for Nimbus,"Now for many concurrent access resource in Nimbus.java, we use AtomicReference to make them multi thread safe. The resources summarized below:

1. heartbeatsCache
2. schedulingStartTimeNs
3. idToSchedStatus
4. nodeIdToResources
5. idToWorkerResources
6. idToExecutors

The 1, 4, 5 and 6 may grows huge if we have hundreds of topologies on cluster, when we update AtomicReference, actually we passed in a Function and use compareAndSet to update the whole val to the new returned by the Function, in that case, we must do a reference copy and merge the changes, which seems not necessary.

I think the reason to use AtomicReference is a legacy from old Clojure code, we can replace them totally with ConcurrentHashMap which supported better performance."
STORM-3335,timeout when scheduling topology runs too long,"We encountered an issue where a user submitted a topology and after a few minutes tried to kill it and failed.  After debugging we found that scheduling was running for topology for at least 30 minutes, causing the kill to be backlogged on the timer task. "
STORM-3333,storm-kafka-monitor - NoClassDefFoundError: org/apache/kafka/shaded/clients/consumer/KafkaConsumer," 

ui log contains errors trying to run storm-kafka-monitor.  I see a similar callstack when running from the command line:

 
{code:java}
org.apache.storm.utils.ShellUtils$ExitCodeException: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/kafka/shaded/clients/consumer/KafkaConsumer
	at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.getOffsetLags(KafkaOffsetLagUtil.java:145)
	at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.main(KafkaOffsetLagUtil.java:71)
Caused by: java.lang.ClassNotFoundException: org.apache.kafka.shaded.clients.consumer.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 2 more

	at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:271) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:194) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellUtils$ShellCommandExecutor.execute(ShellUtils.java:428) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellCommandRunnerImpl.execCommand(ShellCommandRunnerImpl.java:33) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellCommandRunnerImpl.execCommand(ShellCommandRunnerImpl.java:26) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.getLagResultForKafka(TopologySpoutLag.java:162) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.getLagResultForNewKafkaSpout(TopologySpoutLag.java:193) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.addLagResultForKafkaSpout(TopologySpoutLag.java:127) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.lag(TopologySpoutLag.java:61) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.daemon.ui.UIHelpers.getTopologyLag(UIHelpers.java:1620) ~[storm-webapp-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyLag(StormApiResource.java:359) ~[storm-webapp-2.0.1.y.jar:2.0.1.y]
	at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source) ~[?:?]
{code}"
STORM-3330,"Migrate parts of storm-webapp, and reduce use of mocks for files","Parts of storm-webapp are hard to modify due to overuse of mocks, e.g. for files. We should swap out the mock code with real temporary files."
STORM-3329,allow HttpForwardingMetricsConsumer to be used generally by topologies,
STORM-3328,Allow overriding function name in BasicDRPCTopology,
STORM-3327,Storm-webapp uses default charset for URL encoding in a bunch of places,"I think we should use UTF-8, the URLEncoder Javadoc recommends it, linking to https://www.w3.org/TR/html40/appendix/notes.html#non-ascii-chars."
STORM-3326,"FakeMetricsConsumer exposes internal lists, causing ConcurrentModificationException","{quote}
classname: org.apache.storm.metrics-test / testname: test-builtin-metrics-2
Uncaught exception, not in assertion.
expected: nil
  actual: java.util.ConcurrentModificationException: null
 at java.util.ArrayList$Itr.checkForComodification (ArrayList.java:907)
    java.util.ArrayList$Itr.next (ArrayList.java:857)
    com.google.common.collect.AbstractMapBasedMultimap$WrappedCollection$WrappedIterator.next (AbstractMapBasedMultimap.java:466)
    clojure.lang.PersistentVector.create (PersistentVector.java:105)
    clojure.lang.LazilyPersistentVector.create (LazilyPersistentVector.java:32)
    clojure.core$vec.invoke (core.clj:361)
    org.apache.storm.util$clojurify_structure$fn__219.invoke (util.clj:85)
    clojure.walk$prewalk.invoke (walk.clj:64)
    clojure.core$partial$fn__4527.invoke (core.clj:2493)
    clojure.core$map$fn__4553.invoke (core.clj:2622)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
    clojure.core.protocols/fn (protocols.clj:101)
    clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
    clojure.core$reduce.invoke (core.clj:6519)
    clojure.core$into.invoke (core.clj:6600)
    clojure.walk$walk.invoke (walk.clj:49)
    clojure.walk$prewalk.invoke (walk.clj:64)
    clojure.core$partial$fn__4527.invoke (core.clj:2493)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
    clojure.core.protocols/fn (protocols.clj:101)
    clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
    clojure.core$reduce.invoke (core.clj:6519)
    clojure.core$into.invoke (core.clj:6600)
    clojure.walk$walk.invoke (walk.clj:49)
    clojure.walk$prewalk.invoke (walk.clj:64)
    org.apache.storm.util$clojurify_structure.invoke (util.clj:83)
    org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_$reify__2950.exec (metrics_test.clj:79)
    org.apache.storm.Testing.whileTimeout (Testing.java:103)
    org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_.invoke (metrics_test.clj:77)
    org.apache.storm.metrics_test$assert_metric_running_sum_BANG_.invoke (metrics_test.clj:98)
    org.apache.storm.metrics_test/fn (metrics_test.clj:333)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval4721$iter__4722__4726$fn__4727$fn__4728$fn__4729.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval4721$iter__4722__4726$fn__4727$fn__4728.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval4721$iter__4722__4726$fn__4727.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval4721.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
{quote}"
STORM-3325,Storm-webapp should not be part of Externals on travis,"Storm-webapp is built as part of ""Externals"" in Travis. I think it should be part of Server instead."
STORM-3323,Make storm.py work without wrapper scripts,"I think it would be good if we could get rid of the platform specific shell scripts wrapping storm.py.

I think we should be able to move all the functionality in the shell scripts into storm.py. The only change we'd likely have to make is to have a storm-env.py file people can modify in storm/conf, instead of having the storm-env.sh/storm-env.ps1 as we do now."
STORM-3321,Tests are flaky due to long timeouts in Nimbus and supervisor when using LocalCluster,"Tests will sometimes fail with timeout when using e.g. Testing.completeTopology.

The issue is that the timeout is 10 seconds, and Nimbus and the supervisor both have timers that monitor for new deployments that are also set to 10 seconds. This causes tests to time out because a lot of the test time is wasted waiting for Nimbus/the supervisors to catch that the test topology is deployed.

We should reduce these timeouts to their minimums.

There is also a race in Nimbus that can cause test failures 
{quote}
2019-01-21 02:00:19.587 [main] WARN  org.apache.storm.daemon.nimbus.Nimbus - Topology submission exception. (topology name='topologytest-45f5ad59-ec16-45a4-ba4a-eea992411cc1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='DESKTOP-AGC8TKM', port=6627, isLeader=true}
	at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1525) ~[classes/:?]
	at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2982) ~[classes/:?]
	at org.apache.storm.daemon.nimbus.Nimbus.submitTopology(Nimbus.java:2965) ~[classes/:?]
	at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:444) ~[classes/:?]
	at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:125) ~[classes/:?]
	at org.apache.storm.Testing.completeTopology(Testing.java:424) ~[classes/:?]
{quote}

The issue is that Nimbus has to acquire leadership in order to submit topologies, but LocalCluster doesn't wait for the Nimbus instance it creates to gain leadership.

We should make LocalCluster wait for Nimbus to gain leadership."
STORM-3320,Executors should start when all worker connections are ready,"We conflate ""being activated"" with ""all workers are ready"" in WorkerState, by making isWorkerActivated a part of isTopologyActivated.

The issue with this is that isTopologyActivated is used to communicate activation/deactivation to the executors, and is updated on a timer (default only every 10 seconds). isWorkerActivated is really meant to be a one-way switch, which lets us delay executor initialization until all other workers in the topology are also started.

Since we mix the two up, if a worker is started in the topology and all other connections aren't ready immediately (e.g. as happens every time you deploy a topology, some workers will boot faster than others), the worker may have to wait up to 10 seconds to start.

We should make sure the wait for isWorkerActivated happens via CountDownLatch instead, so the executor will start as soon as the connections are ready."
STORM-3319,Slot can fail assertions in some cases,"{quote}
2019-01-19 22:47:03.045 [SLOT_1024] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.lang.AssertionError: null
	at org.apache.storm.daemon.supervisor.Slot.handleEmpty(Slot.java:781) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:217) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [classes/:?]
2019-01-19 22:47:03.045 [SLOT_1025] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.lang.AssertionError: null
	at org.apache.storm.daemon.supervisor.Slot.handleEmpty(Slot.java:781) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:217) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [classes/:?]
{quote}

The issue is that Slot tries to go from WAITING_FOR_LOCALIZATION to EMPTY when there's an exception downloading a blob. It then fails one of the assertions in EMPTY because it doesn't clear its pendingChangingBlobsAssignment field.

There's no reason to go back to EMPTY. The Slot still wants to download some blobs, so it should just restart the downloads and go back to WAITING_FOR_LOCALIZATION."
STORM-3318,Complete information in Class NewKafkaSpoutOffsetQuery,"Just complete information in three methods(toString , equals,  hashCode)"
STORM-3317,upload credentials fails when using different java.security.auth.login.config file,Our launcher box has a differing version of java.security.auth.login.config from the system property.  Having this property set differently causes upload-credentials to fail with the current code.
STORM-3315,Upgrade to Kryo 4,"A user seems to be hitting https://github.com/EsotericSoftware/kryo/issues/462, which is fixed in Kryo 4.

We should upgrade. It seems like we can ensure compatibility with current Kryo by setting kryo.getFieldSerializerConfig().setOptimizedGenerics(true), going by the Kryo 4 release notes."
STORM-3312,Upgrade Guava to latest,"As part of STORM-3311, I want to use https://google.github.io/guava/releases/23.0/api/docs/com/google/common/io/MoreFiles.html to replace Guava's Files class. We're currently on Guava 16.0.1, which is too old. Since we're shading Guava, there shouldn't be an issue with upgrading it. Modules like storm-cassandra that require old Guava versions can depend directly on unshaded Guava in the version they like."
STORM-3311,Use Java 7 Files API for IO instead of the older API,"We should try to use the NIO Files API for file IO. The older file API causes issues on Windows, since it doesn't set the FILE_SHARE_DELETE flag when opening files. This causes Storm to be unable to delete files that have open handles, which is unlike the behavior on Linux. This can cause e.g. unnecessary supervisor crashes because one thread tries to delete a file that is open in another.

For the same reason, we should get rid of uses of common-io FileUtils, and get rid of uses of Guava's Files class that opens IO streams."
STORM-3310,JCQueueTest is flaky,"The JCQueueTest is flaky

{quote}
[ERROR]   JCQueueTest.testFirstMessageFirst:61 We expect to receive first published message first, but received null expected:<FIRST> but was:<null>
{quote}

The issue is that the test has a race condition. There is no check that the consumer thread has read all (or any) of the produced messages before the test terminates."
STORM-3309,TickTupleTest is still flaky,"{quote}
 testTickTupleWorksWithSystemBolt  Time elapsed: 6.802 s  <<< FAILURE!
java.lang.AssertionError: Iteration 1 expected:<52000> but was:<51000>
{quote}

The test runs a topology in a local cluster with time simulation. One of the bolts has tick tuples enabled, and the test tries to check that the ticks arrive with 1 second intervals.

As far as I can tell, the problem is that time simulation doesn't cover the bolt and spout executors. When the test increases simulated time by 1 second and waits for the cluster to idle, the test expects that to mean that the bolt will at that point have consumed the tick. In some cases this doesn't happen, and multiple tick tuples may end up queued before the bolt consumes them. Since the bolt is responsible for generating the timestamp, the test will fail."
STORM-3308,o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key,"Related to issue STORM-2736

We run a single instance of storm-nimbus (Non HA). Due to issues with connections issues with zookeeper, the ephemeral child nodes at  /storm.104/blobstore/key/  were lost that was created by the instance. Nimbus constantly logs the following line which was fixed in the issue mentioned above - 
{code:java}
o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<key>
{code}

But, Shouldn't nimbus automatically create the children with hostname:port-sequencenumber automatically if it is up and running ? In my case, nimbus did not crash but the ephemeral nodes( the children in the case) vanished  when the connection was reset between nimbus and zookeeper. I don't see any code path that creates the children if they are missing in the /blobstore zk path."
STORM-3307,0 timestamp in component errors in Storm UI,
STORM-3306,Some tests in storm-core/test/jvm/org/apache/storm/integration/TopologyIntegrationTest.java are using Thrift to build topologies. They should use TopologyBuilder instead. ,
STORM-3304,Storm-hdfs tests don't run on Java 11,
STORM-3301,The KafkaSpout can in some cases still replay tuples that were already committed,"In the fix for STORM-2666 and followups, we added logic to handle cases where the spout received the ack for an offset after the following offsets were already acked. The issue was that the spout might commit all the acked offsets, but not adjust the consumer position forward, or clear out waitingToEmit properly. If the acked offset was sufficiently far behind the log end offset, the spout might end up polling for offsets it had already committed.

The fix is slightly wrong. When the consumer position drops behind the committed offset, we make sure to adjust the position forward, and clear out any waitingToEmit messages that are behind the committed offset. We don't clear out waitingToEmit unless we adjust the consumer position, which turns out to be a problem.

For example, say offset 1 has failed, offsets 2-10 have been acked and maxPollRecords is 10. Say there are 11 records (1-11) in Kafka. If the spout seeks back to offset 1 to replay it, it will get offsets 1-10 back from the consumer in the poll. The consumer position is now 11. The spout emits offset 1. Say it gets acked immediately. On the next poll, the spout will commit offset 1-10 and check if it should adjust the consumer position and waitingToEmit. Since the position (11) is ahead of the committed offset (10), it doesn't clear out waitingToEmit. Since waitingToEmit still contains offsets 2-10 from the previous poll, the spout will end up emitting these tuples again."
STORM-3300,Potential NPE in Acker when using reset timeout,
STORM-3297,NimbusMetricProcessor.processWorkerMetrics() can cause supervisor restart,NimbusClient.getConfiguredClient() can throw a (runtime) NimbusLeaderNotFoundException.  This can cause a supervisor restart when there is no nimbus leader.
STORM-3296,Upgrade Curator-test to resolve CURATOR-409,"I'd like to preemptively upgrade curator-test, so we don't get affected by CURATOR-409 once we upgrade Zookeeper to the latest 3.4 version."
STORM-3295,Blacklist scheduling doesn't handle multiple supervisors on a host properly,"If two supervisors on a host are blacklisted, the RasBlacklistStrategy code can release one supervisor and keep the other blacklisted.  The BlacklistScheduler then sees the one blacklisted supervisor and considers that host still blacklisted, preventing scheduling from operating properly.

 

 

 "
STORM-3290,Split storm-kafka-client KafkaSpoutConfig into a config for the Storm spout and a config for the Trident spouts,"The KafkaSpoutConfig class is being used for configuration of both the Trident and non-Trident spouts. I think we should split it up, because about half the properties are only used by the non-Trident spout. It is confusing for users to have a lot of settings that don't do anything."
STORM-3289,Add note about KAFKA-7044 to storm-kafka-client compatibility docs,"We should add a note about KAFKA-7044 to the storm-kafka-client docs, since it can cause crashes in the spout. "
STORM-3288,Extracting jar dirs with resources in them are corrupted,"The current code when it tries to remove resources/ from the beginning of the path ends up removing it everywhere it exists in the path, which messes up anything with a directory named
{code:java}
*resources/{code}"
STORM-3286,MENIFEST.MF may loss by use of storm-rename-hack,"Function shadeJarStream in class DefaultShader may produce lost  of MENIFEST.MF,because of JarOutputStream created by the contructor without manifest.For some situation, such as mongodb connection, may lead a NullPointerException."
STORM-3280,Trident-based windowing does not appear to guarantee at-least-once,"[~shaikasifullah] mentioned that he was experiencing lost tuples when restarting a Trident topology that uses windowing alongside the opaque Kafka spout.

I think this is due to a bug in the Trident windowing implementation.

Trident doesn't use the regular acking mechanism to keep track of all tuples in a batch. Instead, the bolt executors in Trident send ""coordinator"" tuples downstream following each batch, indicating how many tuples were in the batch. These coordinator tuples are anchored to the initial ""emit batch"" tuple at the master batch coordinator (MBC). The next bolt executor in line checks if it received all the expected tuples, and fails the ""emit batch"" tree if not. Otherwise, the entire batch is considered acked when the coordinator tuple is acked, which happens as soon as it is received (purposefully ignoring the commit mechanism here).

The bolt executor notifies the wrapped bolt when a batch starts, and when it finishes. The expectation is that the bolt will emit any new tuples it wants anchored to the coordinator tuple before the bolt executor considers the batch finished. See https://github.com/apache/storm/blob/19fbfb9ac8f82719cf70fedf6a024acaeec4e804/storm-client/src/jvm/org/apache/storm/trident/topology/TridentBoltExecutor.java#L127.

The windowing mechanism in Trident is implemented via a processor https://github.com/apache/storm/blob/19fbfb9ac8f82719cf70fedf6a024acaeec4e804/storm-client/src/jvm/org/apache/storm/trident/windowing/WindowTridentProcessor.java#L147. The processor collects received tuples grouped by batch, and only passes them to the WindowManager when a batch is considered complete. At this point, it will also check if any triggers have fired (e.g. due to timeout), and will emit any resulting windows.

The issue here is that there is no correlation between the finished batch and which tuples the window processor chooses to emit during the finishBatch call. Unless it emits exactly the tuples from the received batch, there is a risk of losing the at-least-once property, since the bolt executor will ack the coordinator tuple immediately following finishBatch.

Just to give a concrete example:

MBC starts txid 1 by emitting an ""emit batch"" tuple
Spout executor receives the tuple, emits tuple 1-10, then emits coordinator tuple containing expected count of 10 tuples.
Bolt executor receives tuple 1-10
Bolt executor receives coordinator tuple from upstream spout, containing an expected count of 10 tuples
Bolt executor calls finishBatch
Window processor is configured with a window of 10 seconds, and decides not to emit the 10 tuples. Since nothing is emitted, no new tuples are anchored at the coordinator tuple.
Bolt executor acks the coordinator tuple at the MBC
The MBC sees that the ""emit batch"" tuple has been acked, and starts the commit process. At this point Trident is free to assume the 10 tuples have been correctly processed and e.g. write to Zookeeper that the Kafka spout should pick up at offset 10 next time it starts."
STORM-3279,Kafka trident spout could loose its position with EARLIEST or LATEST FirstPollOffsetStrategy,"In KafkaTridentSpoutEmitter emitPartitionBatch() function, when kafkaConsumer.poll(pollTimeoutMs) returns 0 records for the very first transaction where FirstPollOffsetStrategy is set to EARLIEST or LATEST, the spout fails to move to EARLIEST or LATEST, and continues from the last metadata position.

 

The flow of events which would cause this bug :

 

1. FirstPollOffsetStrategy set to EARLIEST or LATEST

2. For first transaction after restart txid1 Based on [link L164|https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutEmitter.java#L164] ,

The currentBatch is initialized to lastBatchMeta (which need not be null);

3. Later in L171, the consumer seeks to ""start"" OR ""end""

4. Then consumer.poll(pollTimeoutMs) is called.

5. If poll returns non 0 records , currentBatch is set to a new metadata . *If poll returns 0 records,*

*currentBatch is not reset ie, currentBatch is still lastBatchMeta (which need not be null)*

 

So now in transaction txid2 after txid1, isFirstPoll() returns false, and the spout continues from lastBatchMeta.

 

 "
STORM-3276,Can't run Flux with Storm 2.0.0,"I try to run a Flux-based topology with Storm 2.0.0

 

_apache-storm-2.0.0/bin/storm local target/2-1.0-SNAPSHOT.jar org.apache.storm.flux.Flux crawler.flux --local-ttl 9999999_

 

Am getting 

 

_17:41:22.191 [main] ERROR o.a.s.f.Flux - To run in local mode run with 'storm local' instead of 'storm jar'_
_17:41:22.191 [main] INFO o.a.s.LocalCluster -_

_RUNNING LOCAL CLUSTER for 9999999 seconds._

and nothing happens after that. 

 

The documentation for Flux [http://storm.apache.org/releases/2.0.0-SNAPSHOT/flux.html] still mentions using 'storm jar' as well as --local and --sleep.

My test topology can be found at [https://github.com/DigitalPebble/storm2] and requires the branch 2.x of StormCrawler [https://github.com/DigitalPebble/storm-crawler/tree/2.x] to by installed.

 

 

 

 "
STORM-3273,Don't pass storm.local.hostname to topology conf,"We have found that if we set storm.local.hostname on nimbus it gets put into the topology conf which in turn causes the topology to report all metrics as coming from nimbus, which is not what we want."
STORM-3270,"Build Storm with JDK 11, excluding incompatible modules",
STORM-3269,storm-client and storm-server indirectly depend on storm-core,"When trying to get the version information for nimbus it looks for storm-core, which requires the storm-core class to be on the classpath.  We need to fix this, because VersionInfo is in storm-client so it is possible for someone who uses it from storm-client to load the wrong thing."
STORM-3268,Try to make the integration test more stable,"The integration test is still flaky, and most of the time it's not because of bugs in Storm. Try to make it more stable."
STORM-3256,"If all thread counts exceed 32767, the system will generate errors","If all thread counts exceed 32767, the system will generate errors, MessageBatch.java Line141.

 

Storm systems are used for low latency and a linear increase in the number of concurrencies as servers increase.
If you set up four workers per server, it's easy to achieve 4000 degrees of parallelism, that is, 4000 threads per server, then 10 servers will exceed 32767.
Supporting only 32767 threads is a disaster for large-scale computing.
It is hoped that this function can be improved and repaired as soon as possible."
STORM-3249,Nimbus Shutdown Faster,Nimbus takes for ever to shut down in 2.x.  It would really be nice to fix that. (and it is really our own fault why it takes so long)
STORM-3247,remove BLOBSTORE_SUPERUSER,BLOBSTORE_SUPERUSER doesn't appear to be used. 
STORM-3245,Log viewer cleanup failes if multiple empty worker-artifact dirs,"If a drive fills up for various reasons it is possible that we can create the directory for a worker in worker-artifacts, but not able to create the worker.yaml.  If we get more than one of these on a node we get an exception like.

 
{code:java}
o.a.s.d.l.u.LogCleaner logviewer-cleanup [ERROR] Exception while cleaning up old log.
java.lang.IllegalStateException: Duplicate key ...
        at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133) ~[?:1.8.0_102]
        at java.util.HashMap.merge(HashMap.java:1253) ~[?:1.8.0_102]
        at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320) ~[?:1.8.0_102]
        at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:1.8.0_102]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_102]
        at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2746) ~[?:1.8.0_102]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_102]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_102]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_102]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_102]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_102]
        at org.apache.storm.daemon.logviewer.utils.WorkerLogs.identifyWorkerLogDirs(WorkerLogs.java:214) ~[storm-webapp-...{code}

After that we cannot clean up any directories any longer..."
STORM-3240,healthchecks fail if scripts return 0 exit code,
STORM-3239,Worker profile actions don't work,"I just noticed that all worker profile actions don't work in 2.0.0-RC1. access-ui logged the message that the action happened, but there's no further log message as well as no action.

Assigning to [~govindmenon] since he said he is working on this in mailing list."
STORM-3238,The result of log search for topology doesn't show in UI page,Please refer the attachments to see the symptom. The result seems to be returned from Logviewer but UI doesn't reflect the output.
STORM-3237,create metric to track mkAssignments exceptions,We had an issue in the past where scheduling was throwing exceptions on nimbus.  I'd like to be able to monitor this issue going forward.
STORM-3236,DRPC meterShutdownCalls marked after metrics stopped,"If we want to track shutdown calls, we need to mark before the metricsRegistry.stopMetricsReporters() call occurs."
STORM-3234,Document Cluster Metrics,We need better docs on what cluster metrics we have in storm.
STORM-3233,Upgrade zookeeper client to newest version (3.4.13),"Hi,

I would like to see new zookeeper client (3.4.13) used in storm.

New release contains an important fix for cloud environments where zookeeper servers have dynamic ips ([https://jira.apache.org/jira/browse/ZOOKEEPER-2184]).

If possible, it would be nice to see updated zookeeper also on older storm versions (1.2.x, 1.1.x)"
STORM-3232,Display other versions of storm offered on the cluster in the UI,In Storm 2.0.0 we have the option to have older versions of storm installed on the cluster for backwards compatibility.  If this happens we should display it on the UI do others know everything that is installed on the cluster.
STORM-3231,TopologyBySubmissionTimeComparator does not consider priority,"TopologyBySubmissionTimeComparator indicates ""Comparator that sorts topologies by priority and then by submission time"", but the code only considers uptime.

 

I am not sure what the intent should be.  Either the code or comment should be fixed."
STORM-3230,Small race with worker tokens.,There is a small race in zookeeper I missed when trying to get the secret out of ZK for worker tokens.
STORM-3229,Better error reporting in WorkerTokenAuthorizer,"The error handling code in WorkerTokenAuthorizer logs some errors at a debug level, that should be logged at a much higher level.  Specifically ZK auth errors, where we can have the Zk client not authenticated because it is using an older jaas conf.  When this happens DRPC auth just does not work and there is no indication as to why."
STORM-3228,Supervisor blobstore ref counting not working properly,"When supervisors release slots, they can continue to indefinitely download the blob.
{code:java}
2018-09-17 16:18:00.955 o.a.s.l.AsyncLocalizer SLOT_6703 [INFO] Releasing slot for logviewer-ui-groups-test-11-1537201071 6703
2018-09-17 16:18:00.956 o.a.s.l.LocallyCachedBlob SLOT_6703 [WARN] {logviewer-ui-groups-test-11-1537201071 on 6703} had no reservation for logviewer-ui-groups-test-11-1537201071 stormjar.jar

2018-09-17 16:22:50.198 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_CONF logviewer-ui-groups-test-11-1537201071 will try again in 100 ms{code}"
STORM-3226,Improve Supervisor authorization handler error message,The error in the supervisor about the authorization handler is confusing and does not explain how to fix the issue.
STORM-3225,AuthorizedUserFilter should not convert the media type to a string,The AuthorizedUserFilter is converting the media type to a string so it can do a comparison check.  There is a better way.
STORM-3223,RAS can get an NPE if entire rack is blacklisted,If an entire rack is blacklisted the RAS scheduler can end up getting an NPE.  This is a more extreme version of a fix that was merged in previously for single nodes that are blacklisted causing NPEs.
STORM-3222,Fix KafkaSpout internals to use LinkedList instead of ArrayList,"KafkaSpout internally maintains a waitingToEmit list per topic partition and keeps removing the first item to emit during each nextTuple. The implementation uses an ArrayList which results in un-necessary traversal and copy for each tuple.

Also I am not sure why the nextTuple only emits a single tuple wheres ideally it should emit whatever it can emit in a single nextTuple call which is more efficient.  However the logic appears too complicated to refactor."
STORM-3221,Utilization in clusterSummary is inverted - shows free instead of used,
STORM-3219,Storm UI javascript needs better error reporting,"In many places on the UI we report errors by placing them in a section at the bottom of the page, but not all of the sections of the pages do this.  Which can make it hard to debug what is happening, and not very obvious to our end users that there is a problem."
STORM-3218,AuthorizedUserFilter should handle authorization exceptions better.,"Sorry I missed this before when I added back in impersonation.  The code that gets the topology conf to validate if the user is allowed to make the given REST call should not be doing impersonation because.  I tested the code as a single user, but the issue is that because the ReqContext is tied to a thread if we don't clear/clean up the impersonation code properly the old user is still in the ReqContext so when we try to get the conf we are doing it as the wrong user and get an error."
STORM-3217,Component errors missing in /api/v1/component API call,"componentErrors and other component stats were missed by me in the migration.

 "
STORM-3215,New UI is not impersonating user,The new UI switched APIs that it used to get the nimbus client.  Turns out that the APIs are inconsistent on using ReqContext or not.
STORM-3211,WindowedBoltExecutor NPE if wrapped bolt returns null from getComponentConfiguration,"{code}
Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.storm.topology.WindowedBoltExecutor.declareOutputFields(WindowedBoltExecutor.java:309)
    at org.apache.storm.topology.TopologyBuilder.getComponentCommon(TopologyBuilder.java:432)
    at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:120)
    at Main.main(Main.java:23)
{code}"
STORM-3208,supervisor NPE trying to kill workers,"{code:java}
2018-08-29 15:37:47.891 o.a.s.u.Utils main [INFO] UNNAMED:main : user is gstorm
2018-08-29 15:37:47.893 o.a.s.d.s.Supervisor main [ERROR] Error trying to kill 7f4dd1bb-ea77-4f13-a785-0299e81bf5a5
java.lang.NullPointerException: null
        at org.apache.storm.daemon.supervisor.BasicContainer.cleanUpForRestart(BasicContainer.java:216) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Container.cleanUp(Container.java:360) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.killWorkers(Supervisor.java:482) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.ReadClusterState.<init>(ReadClusterState.java:111) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.launch(Supervisor.java:282) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.launchDaemon(Supervisor.java:312) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.main(Supervisor.java:185) [storm-server-2.0.0.y.jar:2.0.0.y]
2018-08-29 15:37:47.904 o.a.s.d.s.Supervisor main [INFO] Starting supervisor with id 03ee87f5-28ca-491b-95cb-15b841f249e1-10.215.76.240 at host openqe74blue-n1.blue.ygrid.yahoo.com.

{code}"
STORM-3205,Optimization in TuplImpl,"Wrapping {{TuplImpl.values}} with Collections.unmodifiableList() turns out be very expensive. Its intention is obviously to check and prevent accidental tweaking TuplImpl once created. Given the high cost, if needed, we can limit this extra checking mechanism in debug/dev mode. Being in the critical path it means several thousand/million additional allocations per second of the List wrapper object .... proportional to the number of bolt/spout instances.

*TVL :*
| |throughput (k/sec)|cores|mem (mb)|
|*master (#f5a410ba3)*|412 |4.26|103|
|*storm-3205*|547  (+33%)|4.09|132|

{{+cmd:+ bin/storm jar topos/storm-loadgen-2.0.0-SNAPSHOT.jar org.apache.storm.loadgen.ThroughputVsLatency *--rate 550000* --spouts 1 --splitters 3 --counters 2 -c topology.acker.executors=0}}

*+ConstSpoutIdentityBoltNullBolt :+*
| |throughput|
|*master*|4.25 mill/sec|
|*storm-3205*|5.4 mill/sec (+27%)|

+cmd+: {{bin/storm jar topos/storm-perf-2.0.0-SNAPSHOT.jar org.apache.storm.perf.ConstSpoutIdBoltNullBoltTopo -c topology.acker.executors=0 -c topology.producer.batch.size=1000 400}}

*Note:* The perf gains are more evident when operating at high thoughputs w/o backpressure occurring (i.e. some bolts have not yet become a bottleneck)"
STORM-3204,Upgrade Dropwizard Metrics to 4.0.3,"Since we've removed metrics-ganglia, we can upgrade to the latest Metrics version. It has some fixes for Java 9+ https://github.com/dropwizard/metrics/pull/1236."
STORM-3203,AsyncLocalizer is not updating permissions for storm.conf storm.ser and storm.jar,"Only in 2.0 the AsyncLocalizer is not updating the permissions at all.  It looks like it is some code I missed when refactoring things, but I have a fix for it."
STORM-3202,Include offset information to spout metrics and remove storm-kafka-monitor,"To provide offset information on Kafka spout (old and new), we have storm-kafka-monitor module which is being run by UI (shell). This approach requires UI doing too many things - basically UI process does most of things via interacting with Nimbus - and also running external shell process in UI process per opening topology page doesn't look right.

We could just let Spout include offset information into spout metric, and let UI leverage the information. I have been thinking about this approach but forgot about addressing it while thinking about generalizing the format. Now I think we don't have to put too much effort to generalize format, because Kafka spout is used mainly.

 "
STORM-3199,"Metrics-ganglia depends on an LGPL library, so we shouldn't depend on it","https://issues.apache.org/jira/browse/STORM-2153 introduced a dependency on metrics-ganglia, which depends on remotetea-oncrpc. This library appears to be licensed under LGPL.

The Dropwizard metrics project removed it for the same reason https://github.com/dropwizard/metrics/issues/1319. I think we also need to get rid of it."
STORM-3197,Make StormMetricsRegistry a regular instance class rather than a static utility,Having the registry be a static utility makes fixing some issues harder than it should be. Preventing daemons from accidentally registering metrics for other daemons (STORM-3101) and flushing reporters on shutdown without breaking the tests (STORM-3173) for example.
STORM-3194,Reduce logging level of FIFOSchedulingPriorityStrategy,I'm seeing FIFOSchedulingPriorityStrategy eating up a ton of logging lines on larger clusters.  I'd like to switch the logging to debug to allow a more readable log (as well as preserve a longer log history).
STORM-3192,ClusterSummaryMetrics is activating/deactivating by polling for whether Nimbus is leader. It should be notified instead.,See https://github.com/apache/storm/pull/2764#discussion_r209333494.
STORM-3190,Unnecessary null check of directory stream in LogCleaner,This should be using try-with-resources https://github.com/apache/storm/blob/a1b3e02aab57b4e458b8b5763a0d467852906bb7/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogCleaner.java#L263
STORM-3189,Remove unused data file LogViewer api,"Discovered in STORM-3133.

`findNMatches` in LogviewerLogSearchHandler returns a `Matched` object which contains a field `fileOffset`. However, in current implementation, `fileOffset` behaves a bit odd and is not being used anywhere in the app. I'm wondering if we should remove this field altogether

Specifically, the difference in behavior follows,
`fileOffset is passed in as the desired amount of file to skip in search (equiv to index of first file to search)

if desired amount of matches found, fileOffset will be the index of last scanned file (starting from 0).
if not enough matches found in all logs, fileOffset will be number of all logs (equiv to one past the index of last file)

See 
https://github.com/apache/storm/pull/2754#discussion_r208691016
https://github.com/apache/storm/pull/2754#discussion_r208726809"
STORM-3188,Removing try-catch block from getAndResetWorkerHeartbeats,"After refactoring, SupervisorUtils.readWorkerHeartbeats no longer throws checked Exceptions. I'm wondering if we still want to keep the try-catch block to wrap around its invocation in getAndResetWorkerHeartbeats in ReportWorkerHeartbeats.java."
STORM-3187,Nimbus code refactoring and cleanup,"Nimbus.java is bloated with many legacy code that are convoluted and inefficient. It would be nice if we can clean up the code a bit, especially now that we're moving away from Clojure.

Several suggestion are made in STORM-3133, including,

1. Remove logging that is of the same purpose of some metrics: https://github.com/apache/storm/pull/2764#discussion_r203727117

2. Refactor data type of return values/parameters to improve readability: https://github.com/apache/storm/pull/2764#discussion_r208699933
https://github.com/apache/storm/pull/2764#discussion_r208721202
https://github.com/apache/storm/pull/2764#discussion_r208707855

3. Other performance improvement
https://github.com/apache/storm/pull/2764#discussion_r208714561"
STORM-3186,Customizable configuration for metric reporting interval,"In current implementation, all subclass of ScheduledReporter are hard coded report interval of 10 seconds. However I think it would make sense to make this an item in configuration so user can change the reporting frequency to fit their needs.

See discussion https://github.com/apache/storm/pull/2764#discussion_r203726617"
STORM-3184,Storm supervisor log showing keystore and truststore password in plaintext,"When we enable SSL for Apache storm, the superviosr log shows the keystore and truststore password in the plaintext



log name : /var/log/storm/supervisor.log 
{code}

2018-05-28 16:21:12.594 o.a.s.d.s.Supervisor main [INFO] Starting supervisor for storm version '1.1.1.3.1.1.0-35'. 
2018-05-28 16:21:12.595 o.a.s.d.s.Supervisor main [INFO] Starting Supervisor with conf {storm.messaging.netty.min_wait_ms=100, storm.zookeeper.auth.user=null, storm.messaging.netty.buffer_s 
ize=5242880, client.jartransformer.class=org.apache.storm.hack.StormShadeTransformer, storm.exhibitor.port=8080, pacemaker.auth.method=NONE, ui.filter=null, worker.profiler.enabled=false 
ui.https.key.password=pass123
ui.https.keystore.password=pass123 

{code}


For the below properties created in custom-storm-site section in Ambari while enabling SSL. 
{code}

ui.https.key.password=pass123 
ui.https.keystore.password=pass123

{code}"
STORM-3183,"Topology Visualization is broken: shows components at most, but no more information","Regardless of enabling ack it doesn't show stream between components.
 
Please attach screenshot.

cc. [~govindmenon]"
STORM-3182,Owner summary page in non-secured cluster shows 500 server error because of NullPointerException,"When opening owner summary page in non-secured cluster, all contents become empty except html template, and shows 500 error with below stack trace:

{code}
java.lang.NullPointerException
	at org.apache.storm.daemon.ui.filters.AuthorizedUserFilter.makeResponse(AuthorizedUserFilter.java:83)
	at org.apache.storm.daemon.ui.filters.AuthorizedUserFilter.filter(AuthorizedUserFilter.java:121)
	at org.glassfish.jersey.server.ContainerFilteringStage.apply(ContainerFilteringStage.java:132)
	at org.glassfish.jersey.server.ContainerFilteringStage.apply(ContainerFilteringStage.java:68)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:197)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:269)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
{code}

I'll attach the screenshot.

I'm not sure it would work normally in secured cluster, since I don't have secured cluster.

cc. [~govindmenon]"
STORM-3181,Topology stats doesn't show complete latency even for topology which ack is enabled,"Please see attached screenshot.

cc. [~govindmenon]"
STORM-3180,Total executors in Cluster Summary in main UI page is not exposed even a topology is running,"After STORM-1311, Total executors in Cluster Summary is showing nothing (blank) even one topology is running.

Will attach screenshot which shows the behavior.

cc. [~govindmenon]"
STORM-3179,No data is available for Nimbus Summary in main UI page even Nimbus is running,"After STORM-1311, ""Nimbus Summary"" is showing ""No data available in table"", even it runs with Nimbus.

Will attach screenshot which shows the behavior.

cc. [~govindmenon]"
STORM-3178,Decouple `ClientSupervisorUtils` and refactor metrics registration,See conversation https://github.com/apache/storm/pull/2710#discussion_r207576736
STORM-3177,MockRemovableFile returns true on `#exists` even after `#delete` is called.,See conversation in https://github.com/apache/storm/pull/2788#pullrequestreview-142918985
STORM-3176,KafkaSpout commit offset occurs CommitFailedException which leads to worker dead,"KafkaSpout use the commitAsync api of Consumer, if the interval time between the call of consumer.poll() more than _max.poll.interval.ms_ or the heartbeat of consumer timeout, that will occur CommitFailedException,  and then the worker will die, the log like this: 
{code:java}
// 2018-07-31 19:19:03.341 o.a.s.util [ERROR] Async loop died!
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer th
an the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in
poll() with max.poll.records.
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:698) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:577) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1126) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:XXX) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:430) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:264) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$fn__10936$fn__10951$fn__10982.invoke(executor.clj:647) ~[XXX.jar:?]
at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [XXX.jar:?]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-31 19:19:03.342 o.a.s.d.executor [ERROR]
{code}
I find it will catch the Exception in auto-commit mode of consumer, the source code is:
{code:java}
// private void maybeAutoCommitOffsetsSync(long timeoutMs) {
    if (autoCommitEnabled) {
        Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets = subscriptions.allConsumed();
        try {
            log.debug(""Sending synchronous auto-commit of offsets {} for group {}"", allConsumedOffsets, groupId);
            if (!commitOffsetsSync(allConsumedOffsets, timeoutMs))
                log.debug(""Auto-commit of offsets {} for group {} timed out before completion"",
                        allConsumedOffsets, groupId);
        } catch (WakeupException | InterruptException e) {
            log.debug(""Auto-commit of offsets {} for group {} was interrupted before completion"",
                    allConsumedOffsets, groupId);
            // rethrow wakeups since they are triggered by the user
            throw e;
        } catch (Exception e) {
            // consistent with async auto-commit failures, we do not propagate the exception
            log.warn(""Auto-commit of offsets {} failed for group {}: {}"", allConsumedOffsets, groupId,
                    e.getMessage());
        }
    }
}
{code}
I think KafkaSpout should do like this, catch the Exception avoid to worker die. And when the msg ack failed, Spout should judge the offset of the msgID is larger than the last commit offset(Spout can guarantee that these msgs which offset less than the last commit offset are all ack), if not, the msg should not retry.

 "
STORM-3174,Standardize exit codes,"Exit codes are hard-coded.  It would be better to centralize and tie them to a meaningful constant.

 

 "
STORM-3173,flush metrics to ScheduledReporter on shutdown,"We lose shutdown related metrics that we should alert on at shutdown. We should flush metrics on a shutdown.

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L4497"
STORM-3171,java.lang.NoSuchMethodError in org.apache.storm:storm-kafka-monitor:jar:1.1.2 caused by dependency conflict issue,"Hi, we found a dependency conflict issue in *org.apache.storm:storm-kafka-monitor:jar:1.1.2*, *caused by org.apache.zookeeper:zookeeper:jar*. As shown in the following dependency tree, due to Maven version management, *org.apache.zookeeper:zookeeper:jar:3.4.6* will be loaded, during the packaging process.

 

However, method *<org.apache.zookeeper.server.quorum.flexible.QuorumMaj: void <init>(java.util.Map)>* only defined in *org.apache.zookeeper:zookeeper:jar 3.5.3-beta*, so that there is a crash with the following stack trace when your project referencing the missing method.

 

*Stack trace:*

Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V

         at org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57)

         at org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159)

         at org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:158)

         at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:109)

 

*Dependency tree:*

org.apache.storm:storm-kafka-monitor:jar:1.1.2

+- org.apache.kafka:kafka-clients:jar:0.10.1.0:compile
|  +- net.jpountz.lz4:lz4:jar:1.3.0:compile|
|  +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile|
|  - org.slf4j:slf4j-api:jar:1.7.21:compile|

+- org.apache.curator:curator-framework:jar:4.0.0:compile
|  - org.apache.curator:curator-client:jar:4.0.0:compile|
|     +- *org.apache.zookeeper:zookeeper:jar:3.4.6:compile (version managed from 3.5.3-beta)*|
|     +- jline:jline:jar:0.9.94:compile| |
|      - io.netty:netty:jar:3.9.9.Final:compile (version managed from 3.7.0.Final)| |
|     +- com.google.guava:guava:jar:16.0.1:compile (version managed from 20.0)|
|     - (org.slf4j:slf4j-api:jar:1.7.21:compile - version managed from 1.7.6; omitted for duplicate)|

+- com.googlecode.json-simple:json-simple:jar:1.1:compile

+- commons-cli:commons-cli:jar:1.3.1:compile
 - junit:junit:jar:4.11:test

   - org.hamcrest:hamcrest-core:jar:1.3:test

 

*Solution:*

One choice is to upgrade *org.apache.zookeeper:zookeeper:jar to 3.5.3-beta,* but it is not the best solution, as 3.5.3-beta is not a release version.**

 

Thanks a lot!

Regards,

Leo"
STORM-3170,DirectoryCleaner may not correctly report correct number of deleted files,"In DirectoryCleaner#deleteOldestWhileTooLarge, the original implementation calls file#delete without checking if it succeeds or not, and they're always reported as deleted. This prevents DirectoryCleaner from clean up other files and invalidates any metrics built on top of this."
STORM-3169,Misleading logviewer.cleanup.age.min,"Config specification logviewer.cleanup.age.min labels the duration in minutes passed since a log file is modified before we consider the log to be old. However in the actual use it's been subtracted by nowMills, which is the current time in milliseconds. We should convert it to milliseconds."
STORM-3168,AsyncLocalizer cleanup appears to crash,"I was investigating these blobstore download messages which keep repeating for hours in the supervisor (and nimbus logs).  I turned on debug logging, and was expecting a cleanup debug message every 30 seconds ([https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L606).]  It did not log.  I restarted the supervisor, and it started logging again.  It appears to have crashed with some error.  

We should make sure the cleanup runs continuously and logs any failures to investigate.

 
{code:java}
2018-07-30 23:25:35.691 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [ERROR] Could not update blob, will retry again later

java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not download...

        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_131]

        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_131]

        at org.apache.storm.localizer.AsyncLocalizer.updateBlobs(AsyncLocalizer.java:303) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]

        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_131]

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]

        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

Caused by: java.lang.RuntimeException: Could not download...

        at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:268) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_131]

        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_131]

        ... 3 more

Caused by: org.apache.storm.generated.KeyNotFoundException

        at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25853) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25821) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25752) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:798) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:785) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:85) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:122) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:252) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_131]

        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_131]

        ... 3 more
{code}"
STORM-3167,Flaky test in metrics_test.clj,"{code}
classname: org.apache.storm.metrics-test / testname: test-builtin-metrics-2
Uncaught exception, not in assertion.
expected: nil
 actual: java.util.ConcurrentModificationException: null
 at java.util.ArrayList$Itr.checkForComodification (ArrayList.java:907)
 java.util.ArrayList$Itr.next (ArrayList.java:857)
 com.google.common.collect.AbstractMapBasedMultimap$WrappedCollection$WrappedIterator.next (AbstractMapBasedMultimap.java:486)
 clojure.lang.PersistentVector.create (PersistentVector.java:105)
 clojure.lang.LazilyPersistentVector.create (LazilyPersistentVector.java:32)
 clojure.core$vec.invoke (core.clj:361)
 org.apache.storm.util$clojurify_structure$fn__206.invoke (util.clj:85)
 clojure.walk$prewalk.invoke (walk.clj:64)
 clojure.core$partial$fn__4527.invoke (core.clj:2493)
 clojure.core$map$fn__4553.invoke (core.clj:2622)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.RT.seq (RT.java:507)
 clojure.core/seq (core.clj:137)
 clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
 clojure.core.protocols/fn (protocols.clj:101)
 clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
 clojure.core$reduce.invoke (core.clj:6519)
 clojure.core$into.invoke (core.clj:6600)
 clojure.walk$walk.invoke (walk.clj:49)
 clojure.walk$prewalk.invoke (walk.clj:64)
 clojure.core$partial$fn__4527.invoke (core.clj:2493)
 clojure.core$map$fn__4553.invoke (core.clj:2624)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.RT.seq (RT.java:507)
 clojure.core/seq (core.clj:137)
 clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
 clojure.core.protocols/fn (protocols.clj:101)
 clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
 clojure.core$reduce.invoke (core.clj:6519)
 clojure.core$into.invoke (core.clj:6600)
 clojure.walk$walk.invoke (walk.clj:49)
 clojure.walk$prewalk.invoke (walk.clj:64)
 org.apache.storm.util$clojurify_structure.invoke (util.clj:83)
 org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_$reify__1258.exec (metrics_test.clj:79)
 org.apache.storm.Testing.whileTimeout (Testing.java:103)
 org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_.invoke (metrics_test.clj:77)
 org.apache.storm.metrics_test$assert_metric_running_sum_BANG_.invoke (metrics_test.clj:98)
 org.apache.storm.metrics_test/fn (metrics_test.clj:326)
 clojure.test$test_var$fn__7670.invoke (test.clj:704)
 clojure.test$test_var.invoke (test.clj:704)
 clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
 clojure.test$default_fixture.invoke (test.clj:674)
 clojure.test$test_vars$fn__7692.invoke (test.clj:722)
 clojure.test$default_fixture.invoke (test.clj:674)
 clojure.test$test_vars.invoke (test.clj:718)
 clojure.test$test_all_vars.invoke (test.clj:728)
 clojure.test$test_ns.invoke (test.clj:747)
 clojure.core$map$fn__4553.invoke (core.clj:2624)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.Cons.next (Cons.java:39)
 clojure.lang.RT.boundedLength (RT.java:1735)
 clojure.lang.RestFn.applyTo (RestFn.java:130)
 clojure.core$apply.invoke (core.clj:632)
 clojure.test$run_tests.doInvoke (test.clj:762)
 clojure.lang.RestFn.invoke (RestFn.java:408)
 org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132$fn__5133.invoke (test_runner.clj:107)
 org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132.invoke (test_runner.clj:53)
 org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131.invoke (test_runner.clj:52)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.RT.seq (RT.java:507)
 clojure.core/seq (core.clj:137)
 clojure.core$dorun.invoke (core.clj:3009)
 org.apache.storm.testrunner$eval5125.invoke (test_runner.clj:52)
 clojure.lang.Compiler.eval (Compiler.java:6782)
 clojure.lang.Compiler.load (Compiler.java:7227)
 clojure.lang.Compiler.loadFile (Compiler.java:7165)
 clojure.main$load_script.invoke (main.clj:275)
 clojure.main$script_opt.invoke (main.clj:337)
 clojure.main$main.doInvoke (main.clj:421)
 clojure.lang.RestFn.invoke (RestFn.java:421)
 clojure.lang.Var.invoke (Var.java:383)
 clojure.lang.AFn.applyToHelper (AFn.java:156)
 clojure.lang.Var.applyTo (Var.java:700)
 clojure.main.main (main.java:37)
{code}
 

It looks to me like the issue is that the FakeMetricsConsumer.getTaskIdToBuckets returns a view of a map that may be modified at any time (the getTaskIdToBuckets is synchronized on the map). When the method returns, the lock is released, but the return value is a view of the map, rather than a copy. This makes iteration over the return value unsafe. The method should instead copy the map before returning it."
STORM-3166,Utils.threadDump does not account for dead threads,"Saw this test failure
{code}
classname: integration.org.apache.storm.integration-test / testname: test-validate-topology-structure
Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.NullPointerException: null
 at org.apache.storm.utils.Utils.threadDump (Utils.java:1191)
    org.apache.storm.Testing.whileTimeout (Testing.java:107)
    org.apache.storm.Testing.completeTopology (Testing.java:437)
    integration.org.apache.storm.integration_test$try_complete_wc_topology.invoke (integration_test.clj:247)
    integration.org.apache.storm.integration_test/fn (integration_test.clj:259)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132$fn__5133.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval5125.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
{code}

Utils.threadDump needs to check whether ThreadInfo objects are null before trying to dereference them, since the ThreadMxBean.getThreadInfo method will return null for threads that are dead."
STORM-3165,Better Unified Metrics API with Dimensions,"The current metrics system is really painful.  We have multiple different metrics APIs for both daemon and worker metrics.  We don't support dimensions consistently because we hacked them on top of only one of the metrics APIs in a way that is not extensible compatible.

 

We need a real final solution.  Internally at Oath we have a new metrics client API that fulfills a lot of these, and I am working with the author to open source it, but at the same time I don't think everyone wants to use this API, some will want to use dropwizard or some other reporter so I am going to put up a thin API that will allow us to have multiple different back ends, probably similar to how bookkeeper does it."
STORM-3162,Race condition at updateHeartbeatCache,"This is discovered during testing for STORM-3133. Travis-CI log can be found [here|https://travis-ci.org/apache/storm/jobs/408719153#L1897].

Specifically, updateHeartbeatCache can be invoked both by Nimbus (at `Nimbus#updateHeartBeats`) and by Supervisor (at `Nimbubs#updateCachedHeartbeatsFromWorker` at `Nimbus#updateCachedHeartbeatsFromSupervisor`), causing ConcurrentModificationException."
STORM-3161,Local mode should force setting min replication count to 1,"When topology.min.replication.count is set to more than 1, nimbus in local mode never achieve condition for replication, hence stuck on handling blobs. We should force set it to 1 in local mode."
STORM-3159,Fixed potential file resource leak,"`zipFileSize()` in ServerUtils is not correctly wrapped in try-with-resource block, which could lead to resource leak."
STORM-3158,improve login failure message,"We had an issue where the DRPC server (on an older version of storm) was misconfigured and constantly restarting without logging shutdown messages.  One of the warning messages that did log should have been an error and could provide more useful information about fixing the issue.

warning message: 

2018-07-20 15:09:30.191 clojure-agent-send-off-pool-0 b.s.s.a.k.ServerCallbackHandler [WARN] No password found for user: null

 

hopefully more helpful:

2018-07-20 15:44:39.835 clojure-agent-send-off-pool-0 b.s.s.a.k.ServerCallbackHandler [ERROR] No password found for user: null, validate klist matches jaas conf"
STORM-3157,General improvement to StormMetricsRegistry,The solution contains general improvement and clean up to StormMetricsRegistry. Therefore this may affect all current and future changes to server-side metrics
STORM-3156,Remove the transactional topology API,
STORM-3155,IOutputSerializer implementations always allocates a new ByteBuffer,"The IOutputSerializer javadoc specifies that the user may optionally provide a ByteBuffer to serialize into.

https://github.com/apache/storm/blob/af42f434f4a4c3d9087c6058b359033736d3b5e8/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/IOutputSerializer.java#L26

None of the IOutputSerializer implementations we ship with actually do this. They all ignore the ByteBuffer parameter.

If this is a useful feature, I think that we should update them to use the supplied ByteBuffer if not null. If it isn't a useful feature, we should instead remove the ByteBuffer parameter from the API."
STORM-3153,Restore storm sql provider tests affected via STORM-2406,"Current proposed patch for STORM-2406 gets rid of major parts of tests for provider, because we can't do same test newly changed code. To restore tests additional code change is needed.

This issue is to track the effort of making change of Storm SQL to be able to restore previous tests.

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-hdfs/src/test/org/apache/storm/sql/hdfs/TestHdfsDataSourcesProvider.java#L90]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-kafka/src/test/org/apache/storm/sql/kafka/TestKafkaDataSourcesProvider.java]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-mongodb/src/test/org/apache/storm/sql/mongodb/TestMongoDataSourcesProvider.java]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-redis/src/test/org/apache/storm/sql/redis/TestRedisDataSourcesProvider.java]

 "
STORM-3151,Negative Scheduling Resource/Overscheduling issue,"Possible overscheduling captured when follow steps are performed (Logging is added in STORM-3147)

1) launch nimbus & zookeeper

2) launch supervisor 1

3) launch topology 1 (I used org.apache.storm.starter.WordCountTopology)

4) launch supervisor 2

5) launch topology 2 (I used org.apache.storm.starter.ExclamationTopology)

{noformat}
2018-07-13 12:58:43.196 o.a.s.d.n.Nimbus timer [WARN] Memory over-scheduled on 176ec6d4-2df3-40ca-95ca-c84a81dbcc22-172.130.97.212
{noformat}

Indicating there may be issues inside scheduler.
It is discovered when I ported ClusterSummay to StormMetrics"
STORM-3150,Improve Gauge Registration in StormMetricsRegistry,Make #registerGauge and #registerProvidedGauge generic and clean up other code.
STORM-3149,Why did an exception in the client read not bring down the entire worker,"[https://github.com/apache/storm/pull/2762]

 

ran into some issues where we got an array index out of bounds, but it didn't bring down the worker, just caused issues with the one message being sent.  We should understand what was happening and if there is anything we should fix."
STORM-3148,StormServerPipelineFactory can deserialize messages incorrectly,"We recently ran into an integration test failure. (TestingTest).

It looks like the only way for this error to happen would be if there was an internal bug in kryo, or if we were using Output from multiple threads.  It is the latter.

 

org.apache.storm.messaging.netty.Server creates a single KryoValuesSerializer and KryoValuesDeserializer instance.  These get passed through StormServerPipelineFactory and added to each channel (if there are multiple channels there are multiple threads) and we can mess up both encoding and decoding messages.

 
{code:java}
2018-07-12 17:41:49.408 [Netty-server-localhost-1030-worker-1] ERROR org.apache.storm.messaging.netty.StormServerHandler - server errors in handling the request
org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.ArrayIndexOutOfBoundsException
    at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.Server.channelActive(Server.java:261) [storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.StormServerHandler.channelActive(StormServerHandler.java:40) [storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:518) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:465) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.ArrayIndexOutOfBoundsException
    at java.lang.System.arraycopy(Native Method) ~[?:1.8.0_60]
    at com.esotericsoftware.kryo.io.Output.toBytes(Output.java:130) ~[kryo-3.0.3.jar:?]
    at org.apache.storm.serialization.KryoValuesSerializer.serializeObject(KryoValuesSerializer.java:50) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.BackPressureStatus.buffer(BackPressureStatus.java:68) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.BackPressureStatusEncoder.encode(BackPressureStatusEncoder.java:34) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.BackPressureStatusEncoder.encode(BackPressureStatusEncoder.java:24) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
    ... 27 more
   {code}"
STORM-3147,Port ClusterSummary as Metrics to StormMetricsRegistry,
STORM-3146,dependencies conflict,"I have configure apache hadoop cluster (2.9.1) . storm hdfs by default it takes 2.6.1 hadoop dependencies  , I have excluded it from dependencies and add hadoop 2.9.1 dependencies. I have attached my pom along with it. 

I am finding following error:

java.lang.NoSuchMethodError: org.apache.hadoop.security.authentication.util.KerberosUtil.hasKerberosTicket(Ljavax/security/auth/Subject;)Z
 at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:666) ~[hadoop-common-2.9.1.jar:?]
 at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:861) ~[hadoop-common-2.9.1.jar:?]

 

When I change the hadoop version to 2.6.1 ,I am not finding that error.

 

 

 "
STORM-3144,Extend metrics on Nimbus,"Metrics include:
 # File upload time
 # Nimbus restart count
 # Nimbus loss of leadership: meter marking when a nimbus node gains or loses leadership
 # Excessive scheduling time (both duration distribution and current longest)"
STORM-3143,Unnecessary inclusion of empty match result in Json,"`FindNMatches()` didn't correctly filter out empty match result in `substringSearch()` and hence send back an empty map to user. I don't know if this the desired behavior but a fix to current behavior will make metrics for logviewer easier to implement. 

An example of current behavior:

{code:json}
{
    ""fileOffset"": 1,
    ""searchString"": ""sdf"",
    ""matches"": [
        {
            ""searchString"": ""sdf"",
            ""fileName"": ""word-count-1-1530815972/6701/worker.log"",
            ""matches"": [],
            ""port"": ""6701"",
            ""isDaemon"": ""no"",
            ""startByteOffset"": 0
        }
    ]
}
{code}

Desired behavior:

{code:json}
{
    ""fileOffset"": 1,
    ""searchString"": ""sdf"",
    ""matches"": []
}
{code}"
STORM-3142,Add support for JUnit 5 tests,"I think it would be nice if we could use the new JUnit 5 APIs for testing. Since JUnit 5 can run JUnit 4 tests, it shouldn't be too much work to add support."
STORM-3141,NPE in WorkerState.transferLocalBatch when receiving messages for a task that isn't the first task assigned to the executor,"{code}
2018-07-02 20:32:28.944 [Worker-Transfer] ERROR org.apache.storm.utils.Utils - Async loop died!
java.lang.NullPointerException: null
	at org.apache.storm.daemon.worker.WorkerState.transferLocalBatch(WorkerState.java:538) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:71) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.local.Context$LocalClient.send(Context.java:194) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.TransferDrainer.send(TransferDrainer.java:53) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.flush(WorkerTransfer.java:100) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:146) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.lambda$makeTransferThread$0(WorkerTransfer.java:82) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:353) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2018-07-02 20:32:28.945 [Worker-Transfer] ERROR org.apache.storm.utils.Utils - Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.storm.utils.Utils$2.run(Utils.java:368) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: java.lang.NullPointerException
	at org.apache.storm.daemon.worker.WorkerState.transferLocalBatch(WorkerState.java:538) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:71) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.local.Context$LocalClient.send(Context.java:194) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.TransferDrainer.send(TransferDrainer.java:53) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.flush(WorkerTransfer.java:100) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:146) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.lambda$makeTransferThread$0(WorkerTransfer.java:82) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:353) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 1 more
{code}

When tuples are received, the method looks up which JCQueue to send to. It does this with the tuple destination task id. The map it looks in only has the queues by the starting task id of the executor's range, so if the destination is e.g. task 4 for an executor with assignment [3, 4], we hit an NPE."
STORM-3140,Duplicated method in Logviewer REST API?,"{code:java}
    /**
     * Handles '/searchLogs' request.
     */
    @GET
    @Path(""/searchLogs"")
    public Response searchLogs(@Context HttpServletRequest request) throws IOException {
        String user = httpCredsHandler.getUserName(request);
        String topologyId = request.getParameter(""topoId"");
        String portStr = request.getParameter(""port"");
        String callback = request.getParameter(""callback"");
        String origin = request.getHeader(""Origin"");

        return logviewer.listLogFiles(user, portStr != null ? Integer.parseInt(portStr) : null, topologyId, callback, origin);
    }

    /**
     * Handles '/listLogs' request.
     */
    @GET
    @Path(""/listLogs"")
    public Response listLogs(@Context HttpServletRequest request) throws IOException {
        meterListLogsHttpRequests.mark();

        String user = httpCredsHandler.getUserName(request);
        String topologyId = request.getParameter(""topoId"");
        String portStr = request.getParameter(""port"");
        String callback = request.getParameter(""callback"");
        String origin = request.getHeader(""Origin"");

        return logviewer.listLogFiles(user, portStr != null ? Integer.parseInt(portStr) : null, topologyId, callback, origin);
    }{code}

These two methods are identical although they seem to serve different functions."
STORM-3139,worker fails to start - KeeperErrorCode = NoAuth for /credentials/topologyname," 

Seeing a sporadic test failure internally for us with a worker that won't come up.  The test schedules a bunch of topologies, kills the supervisors, restarts nimbus, and then starts up the supervisors and validates the topologies are all fully running.

 

I've seen this test failure twice in the last two weeks.  The worker has migrated and cannot come up:

 
{code:java}
2018-06-30 10:15:24.102 b.s.util main [WARN] Expecting exception of class: class java.nio.channels.ClosedByInterruptException, but exception chain only contains: (#<RuntimeException java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966> #<NoAuthException org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966>) 2018-06-30 10:15:24.102 b.s.d.worker main [ERROR] Error on initialization of server mk-worker java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966 at backtype.storm.util$wrap_in_runtime.invoke(util.clj:53) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.zookeeper$get_data.invoke(zookeeper.clj:135) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4249.get_data(zookeeper_state_factory.clj:125) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__4296.get_data(pacemaker_state_factory.clj:175) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at backtype.storm.cluster$mk_storm_cluster_state$reify__3910.credentials(cluster.clj:563) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$fn__7710$exec_fn__1599__auto____7711.invoke(worker.clj:623) ~[storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$fn__7710$mk_worker__7803.doInvoke(worker.clj:598) [storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$_main.invoke(worker.clj:810) [storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?] at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.2.y.jar:0.10.2.y] Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966 at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:113) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.zookeeper$get_data.invoke(zookeeper.clj:131) ~[storm-core-0.10.2.y.jar:0.10.2.y] ... 31 more 2018-06-30 10:15:24.199 b.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"")
{code}"
STORM-3138,dev-zookeeper logging to stdout is annoying,
STORM-3137,Flaky test in nimbus_test,"Saw a test failure in storm-core

{code}
313081 [main] INFO  o.a.s.d.n.Nimbus - Cleaning up topo3
313081 [main] INFO  o.a.s.d.n.Nimbus - Exception {}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:54) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils.deserialize(Utils.java:717) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:684) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2424) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.rmDependencyJarsInTopology$accessor$ivuy1xAW(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824$auxiliary$0HJjHtWw.call(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.mockito.internal.invocation.RealMethod$FromCallable$1.call(RealMethod.java:40) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.RealMethod$FromBehavior.invoke(RealMethod.java:62) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.InterceptedInvocation.callRealMethod(InterceptedInvocation.java:127) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:43) [mockito-core-2.19.0.jar:?]
	at org.mockito.Answers.answer(Answers.java:100) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:104) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:63) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:110) [mockito-core-2.19.0.jar:?]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.rmDependencyJarsInTopology(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2478) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.doCleanup$accessor$ivuy1xAW(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824$auxiliary$g48aKOaZ.call(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.mockito.internal.invocation.RealMethod$FromCallable$1.call(RealMethod.java:40) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.RealMethod$FromBehavior.invoke(RealMethod.java:62) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.InterceptedInvocation.callRealMethod(InterceptedInvocation.java:127) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:43) [mockito-core-2.19.0.jar:?]
	at org.mockito.Answers.answer(Answers.java:100) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:104) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:63) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:110) [mockito-core-2.19.0.jar:?]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.doCleanup(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) [clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) [clojure-1.7.0.jar:?]
	at org.apache.storm.nimbus_test$fn__4768$fn__4771.invoke(nimbus_test.clj:1899) [?:2.0.0-SNAPSHOT]
	at clojure.core$with_redefs_fn.invoke(core.clj:7209) [clojure-1.7.0.jar:?]
	at org.apache.storm.nimbus_test$fn__4768.invoke(nimbus_test.clj:1895) [?:2.0.0-SNAPSHOT]
	at clojure.test$test_var$fn__7670.invoke(test.clj:704) [clojure-1.7.0.jar:?]
	at clojure.test$test_var.invoke(test.clj:704) [clojure-1.7.0.jar:?]
	at clojure.test$test_vars$fn__7692$fn__7697.invoke(test.clj:722) [clojure-1.7.0.jar:?]
	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]
	at clojure.test$test_vars$fn__7692.invoke(test.clj:722) [clojure-1.7.0.jar:?]
	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]
	at clojure.test$test_vars.invoke(test.clj:718) [clojure-1.7.0.jar:?]
	at clojure.test$test_all_vars.invoke(test.clj:728) [clojure-1.7.0.jar:?]
	at clojure.test$test_ns.invoke(test.clj:747) [clojure-1.7.0.jar:?]
	at clojure.core$map$fn__4553.invoke(core.clj:2624) [clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]
	at clojure.lang.Cons.next(Cons.java:39) [clojure-1.7.0.jar:?]
	at clojure.lang.RT.boundedLength(RT.java:1735) [clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.applyTo(RestFn.java:130) [clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:632) [clojure-1.7.0.jar:?]
	at clojure.test$run_tests.doInvoke(test.clj:762) [clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.invoke(RestFn.java:408) [clojure-1.7.0.jar:?]
	at org.apache.storm.testrunner$eval5473$iter__5474__5478$fn__5479$fn__5480$fn__5481.invoke(test_runner.clj:107) [?:2.0.0-SNAPSHOT]
	at org.apache.storm.testrunner$eval5473$iter__5474__5478$fn__5479$fn__5480.invoke(test_runner.clj:53) [?:2.0.0-SNAPSHOT]
	at org.apache.storm.testrunner$eval5473$iter__5474__5478$fn__5479.invoke(test_runner.clj:52) [?:2.0.0-SNAPSHOT]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]
	at clojure.lang.RT.seq(RT.java:507) [clojure-1.7.0.jar:?]
	at clojure.core$seq__4128.invoke(core.clj:137) [clojure-1.7.0.jar:?]
	at clojure.core$dorun.invoke(core.clj:3009) [clojure-1.7.0.jar:?]
	at org.apache.storm.testrunner$eval5473.invoke(test_runner.clj:52) [?:2.0.0-SNAPSHOT]
	at clojure.lang.Compiler.eval(Compiler.java:6782) [clojure-1.7.0.jar:?]
	at clojure.lang.Compiler.load(Compiler.java:7227) [clojure-1.7.0.jar:?]
	at clojure.lang.Compiler.loadFile(Compiler.java:7165) [clojure-1.7.0.jar:?]
	at clojure.main$load_script.invoke(main.clj:275) [clojure-1.7.0.jar:?]
	at clojure.main$script_opt.invoke(main.clj:337) [clojure-1.7.0.jar:?]
	at clojure.main$main.doInvoke(main.clj:421) [clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.invoke(RestFn.java:421) [clojure-1.7.0.jar:?]
	at clojure.lang.Var.invoke(Var.java:383) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyToHelper(AFn.java:156) [clojure-1.7.0.jar:?]
	at clojure.lang.Var.applyTo(Var.java:700) [clojure-1.7.0.jar:?]
	at clojure.main.main(main.java:37) [clojure-1.7.0.jar:?]
Caused by: java.lang.NullPointerException
	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106) ~[?:1.8.0_151]
	at org.apache.storm.utils.Utils.gunzip(Utils.java:826) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:51) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 80 more
{code}

Full run is here https://travis-ci.org/apache/storm/jobs/398415336."
STORM-3136,"Fix flaky integration test, and make it more readable","The integration test appears flaky, e.g. https://travis-ci.org/apache/storm/jobs/397471420.

We should try to fix this. I also find the integration test code hard to read and understand, so I'd like to do some cleanup."
STORM-3135,JCQueueTest is flaky,"Saw the JCQueueTest fail a couple of times on Travis.

{code}
testFirstMessageFirst(org.apache.storm.utils.JCQueueTest)  Time elapsed: 0.86 sec  <<< FAILURE!
java.lang.AssertionError: Unable to send halt interrupt
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.storm.utils.JCQueueTest.run(JCQueueTest.java:151)
	at org.apache.storm.utils.JCQueueTest.run(JCQueueTest.java:127)
	at org.apache.storm.utils.JCQueueTest.testFirstMessageFirst(JCQueueTest.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}

I think we should just keep trying to send the interrupt until we succeed, or we hit a reasonable timeout."
STORM-3134,upload-credentials imporvements,"There are some things that would be nice to do with AutoCredentials to make debugging and usage simpler.

 

First it would be ideal if we could use the topology conf as a starting point for AutoCredentials instead of requiring the user to provide all configs from the get go.

 

It would also be nice to warn users if they are overriding the list of AutoCreds in a way that makes it so some of the creds might not be interpreted properly (meaning they are uploading creds that there is no plugin on the worker to interpret them)

 

At the same time it would also be really nice to have some help with debugging, both as an admin, but also as a developer to be able to see what creds currently exist (not necessarily the content of the credentials, but the keys)."
STORM-3133,Extend metrics on Nimbus and LogViewer,"Include but not limited to

Logviewer

1. Clean-up time
 2. Time to complete one clean up loop Time. 
 3. Disk usage by logs before cleanup and After cleanup loop. ( Just like GC.?)
 4. Failures/exceptions.
 5. Search request Cnt: By category - Archived/non-archived
 6. Search Request - Response time
 7. Search Request - 0 result Cnt
 8. Search Result - open files
 9. File partial read count
 10. File Download request Cnt/ And Size served
 11. Disk IO by logviewer
 12. CPU usage ( for unzipping files)

Nimbus Additional:
 * Topology stormjar.ser/stormconf.ser/stormser.ser file upload time.
 * Scheduler related metrics would be a long list generic and specific to different strategies.
 * Most if not all cluster summary can be pushed as Metrics.
 * Restart cnt
 * Nimbus loss of leadership !/jira/images/icons/emoticons/help_16.png|width=16,height=16,align=absmiddle!
 * UI not responding ([https://jira.ouroath.com/browse/YSTORM-4838])
 * Negative resource scheduling events ([https://jira.ouroath.com/browse/YSTORM-4940])
 * Excessive scheduling time  !/jira/images/icons/emoticons/help_16.png|width=16,height=16,align=absmiddle!"
STORM-3130,Add Timer registration and Timed object wrapper to Storm metrics util.,This allows us to time method running duration or variable/resource lifespan.
STORM-3129,Worker state machine does not use correct time util to get start time,Current implementation uses System.currentTimeMillis() instead of Time.currentTimeMillis() to get state start time. This may create problem in unit test as it uses simulated time controlled by Storm Time util.
STORM-3128,Connection refused error in AsyncLocalizerTest,"In AsyncLocalizerTest testKeyNotFoundException, a localBlobStore is created and tries but failed to connect to zookeeper due to connection error. I'm not sure if this compromises the test even though it is passed after connection retry timeout. But it's nice to keep in mind.

{noformat}
2018-06-27 13:05:28.005 [main-SendThread(localhost:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-06-27 13:05:28.032 [main] INFO  org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl - Default schema
2018-06-27 13:05:28.035 [main-SendThread(localhost:2181)] WARN  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_171]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_171]
	at org.apache.storm.shade.org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
{noformat}

I managed to track down the source where the exception is thrown, but it's really strange that this is called by a StormTimer inside Supervisor, which is not declared anywhere in this test. I'm completely lost by now.


{noformat}
2018-08-08 11:45:30.217 [heartbeatTimer] ERROR org.apache.storm.zookeeper.ClientZookeeper - e: {}
org.apache.storm.shade.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /supervisors
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:268) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:257) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForegroundStandard(ExistsBuilderImpl.java:254) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:247) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:206) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:35) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:145) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:292) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.cluster.ZKStateStorage.set_ephemeral_node(ZKStateStorage.java:129) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl.supervisorHeartbeat(StormClusterStateImpl.java:522) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.timer.SupervisorHeartbeat.run(SupervisorHeartbeat.java:96) [classes/:?]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0-SNAPSHOT.jar:?]
{noformat}
"
STORM-3127,Avoid potential race condition ,"PortAndAssignment and its call back is added after update to a blob is invoked asynchronously. It is not guaranteed that the new dependent worker will be registered before blob informs its update to listening workers. 

This can be fixed by moving addReference call up."
STORM-3126,Avoid unnecessary force kill when invoking storm kill_workers,"Supervisor tries to force kill a worker before checking if it has died, leading to unnecessary force kill call. This is minor but does help clean up logs a little bit. "
STORM-3125,Refactoring methods in components for Supervisor and DRPC,"This is a supplement issue page to STORM-3099, separating out refactoring work from metrics addition.

A few misc bug discovered during refactoring have been incorporate in this issue as well. See links for more information."
STORM-3124,Failures talking to Pacemaker,"{code:java}
2018-06-25 20:21:05.220 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 7 more attempts.
2018-06-25 20:21:06.220 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:06.220 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 6 more attempts.
2018-06-25 20:21:07.220 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:07.221 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 5 more attempts.
2018-06-25 20:21:08.221 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:08.221 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 4 more attempts.
2018-06-25 20:21:09.222 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:09.222 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 3 more attempts.
2018-06-25 20:21:10.222 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:10.222 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 2 more attempts.
2018-06-25 20:21:11.223 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:11.223 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 1 more attempts.
2018-06-25 20:21:12.223 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:12.223 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 0 more attempts.
2018-06-25 20:21:13.224 o.a.s.p.PacemakerClientPool timer [WARN] Failed to connect to the pacemaker server openqe74blue-n2.blue.ygrid.yahoo.com
2018-06-25 20:21:13.229 o.a.s.d.n.Nimbus pool-37-thread-481 [INFO] uploadedJar /home/y/var/storm/nimbus/inbox/stormjar-c5893ba3-21c6-4397-84e2-54aab8e091a9.jar
2018-06-25 20:21:13.225 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2773) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:214) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:482) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:897) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2469) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2771) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
Caused by: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:71) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:199) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:482) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:897) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2469) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2771) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-06-25 20:21:13.231 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:470) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:490) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:253) [storm-client-2.0.0.y.jar:2.0.0.y]
2018-06-25 20:21:13.232 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master
2018-06-25 20:21:13.232 o.a.s.u.Utils Thread-13 [INFO] Halting after 10 seconds


2018-06-25 20:21:13.677 o.a.s.d.n.Nimbus pool-37-thread-481 [INFO] desired replication count 1 achieved, current-replication-count for conf key = 1, current-replication-count for code key = 1, current-replication-count for jar key = 1
2018-06-25 20:21:13.678 o.a.s.d.n.Nimbus pool-37-thread-481 [WARN] Topology submission exception. (topology name='run')
java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3024) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3511) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3490) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-06-25 20:21:13.680 o.a.s.t.ProcessFunction pool-37-thread-481 [ERROR] Internal error processing submitTopologyWithOpts
java.lang.RuntimeException: java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3049) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3511) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3490) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3024) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more

{code}
We're having sporadic failures talking to Pacemaker.  This callstack shows us unable to launch topologies."
STORM-3122,"FNFE due to race condition between ""async localizer"" and ""update blob"" timer thread","There's race condition between ""async localizer"" and ""update blob"" timer thread.

When worker is shutting down, reference count for blob will be 0 and supervisor will remove actual blob file. There's also ""update blob"" timer thread which tries to keep blobs updated for downloaded topologies. While updating topology it should read some of blob files already downloaded assuming these files should be downloaded before, and the assumption is broken because of async localizer.

[~arunmahadevan] suggested an approach to fix this: ""updateBlobsForTopology"" can just catch the FIleNotFoundException and skip updating the blobs in case it can't find the stormconf, and the approach looks simplest fix so I'll provide a patch based on suggestion.

Btw, it doesn't apply to master branch, since in master branch all blobs are synced up separately (no need to read stormconf to enumerate topology related blobs), and update logic is already fault-tolerance (skip to next sync when it can't pull the blob)."
STORM-3121,Fix flaky metrics tests in storm-core,"The tests are flaky, but only rarely fail. I've only seen them fail on Travis when Travis is under load.

Example failures:
{code}
classname: org.apache.storm.metrics-test / testname: test-custom-metric-with-multi-tasks
expected: (clojure.core/= [1 0 0 0 0 0 2] (clojure.core/subvec (org.apache.storm.metrics-test/lookup-bucket-by-comp-id-&-metric-name! ""2"" ""my-custom-metric"") 0 N__3207__auto__))
  actual: (not (clojure.core/= [1 0 0 0 0 0 2] [1 0 0 0 0 0 0]))
      at: test_runner.clj:105
{code}
{code}
classname: org.apache.storm.metrics-test / testname: test-builtin-metrics-2
expected: (clojure.core/= [1 1] (clojure.core/subvec (org.apache.storm.metrics-test/lookup-bucket-by-comp-id-&-metric-name! ""myspout"" ""__emit-count/default"") 0 N__3207__auto__))
  actual: (not (clojure.core/= [1 1] [1 0]))
      at: test_runner.clj:105
{code}

The problem is that the tests increment metrics counters in the executor async loops, then expect the counters to end up in exact metrics buckets. The creation of a bucket is triggered by the metrics timer. The timer is included in time simulation and LocalCluster.waitForIdle, but the executor async loop isn't. There isn't any guarantee that the executor async loop gets to run when the test does a sequence like
{code}
Time.advanceClusterTime
cluster.waitForIdle
{code}
because the waitForIdle check doesn't know about the executor async loop."
STORM-3120,"Clean up leftover null checks in Time, ensure idle threads get to run when cluster time is advanced",
STORM-3119,Build Storm with JDK 10,"I think we should add Java 10 to the build matrix.

Storm-cassandra and storm-hive are expected not to work for now, because Cassandra and Hive aren't compatible with Java 9 yet, but the rest of Storm should work (Hadoop-based components may or may not work though, I believe they are still working on compatibility, see https://issues.apache.org/jira/browse/HADOOP-11123). We can exclude those two components from the Java 10 build for now."
STORM-3118,Netty incompatibilities with Pacemaker,"Nimbus has issues with Pacemaker:
{code:java}
2018-06-21 08:55:17.762 o.a.s.p.PacemakerClientHandler client-worker-2 [ERROR] Exception occurred in Pacemaker.
org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)
        at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:65) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:635) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:582) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:461) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)
        at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.ensureWritable0(AbstractByteBuf.java:276) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.writeShort(AbstractByteBuf.java:966) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.messaging.netty.SaslMessageToken.write(SaslMessageToken.java:104) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.codec.ThriftEncoder.encodeNettySerializable(ThriftEncoder.java:44) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.codec.ThriftEncoder.encode(ThriftEncoder.java:77) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        ... 26 more
{code}
Prevents topology submission:

 
{code:java}
2018-06-21 09:10:46.343 o.a.s.d.n.Nimbus pool-37-thread-250 [WARN] Topology submission exception. (topology name='testStormKafkaNewApi')
java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3009) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
{code}"
STORM-3117,Deleting blobs for running topologies hoses Nimbus,"The following test pseudo-code causes issues:
{code:java}
cluster.submitTopology(cluster.getTopologiesJarFile(), topoName, config, topology);
cluster.waitTopologyUp(topoName);
cluster.deleteAllBlobs();
{code}
This causes nimbus to get stuck and restart:

 
{code:java}
2018-06-20 15:48:14.273 o.a.s.d.n.Nimbus pool-27-thread-694 [INFO] Received topology submission for wc-topology-test (storm-0.10.2.y.251 JDK-1.8.0_131) 
2018-06-20 15:48:14.629 o.a.s.d.n.Nimbus pool-27-thread-694 [INFO] Activating wc-topology-test: wc-topology-test-1-1529509694
2018-06-20 15:48:14.724 o.a.s.d.n.Nimbus pool-27-thread-703 [INFO] TRANSITION: wc-topology-test-1-1529509694 KILL null true
2018-06-20 15:48:14.812 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormconf.ser
2018-06-20 15:48:14.830 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormcode.ser
2018-06-20 15:48:14.863 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormjar.jar
2018-06-20 15:48:18.449 o.a.s.s.r.s.p.DefaultSchedulingPriorityStrategy timer [INFO] SIM Scheduling wc-topology-test-1-1529509694 with score of 0.3125
2018-06-20 15:48:18.492 o.a.s.s.Cluster timer [INFO] STATUS - wc-topology-test-1-1529509694 Running - Fully Scheduled by DefaultResourceAwareStrategy
2018-06-20 15:48:18.527 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology id wc-topology-test-1-1529509694:

2018-06-20 15:48:18.979 o.a.s.d.n.Nimbus pool-27-thread-722 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormjar.jar
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

2018-06-20 15:48:22.884 o.a.s.d.n.Nimbus timer [INFO] Renewing Creds For wc-topology-test-1-1529509694 with org.apache.storm.security.auth.kerberos.AutoTGT@4482469c owned by hadoopqa@DEV.YGRID.YAHOO.COM


2018-06-20 15:48:37.947 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: KeyNotFoundException(msg:wc-topology-test-1-1529509694-stormcode.ser)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2822) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) [storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormcode.ser
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:420) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1517) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2675) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.sendClusterMetricsToExecutors(Nimbus.java:2686) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2819) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-06-20 15:48:37.948 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:468) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:488) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) [storm-client-2.0.0.y.jar:2.0.0.y]
2018-06-20 15:48:37.950 o.a.s.d.n.Nimbus Thread-11 [INFO] Shutting down master
2018-06-20 15:48:37.950 o.a.s.u.Utils Thread-12 [INFO] Halting after 10 seconds

2018-06-20 15:48:46.672 o.a.s.d.n.Nimbus pool-27-thread-798 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormconf.ser
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-06-20 15:48:47.950 o.a.s.u.Utils Thread-12 [WARN] Forcing Halt...

{code}
Nimbus then continually restarts:
{code:java}
2018-06-20 15:48:54.635 o.a.s.u.Utils main [ERROR] Received error in main thread.. terminating server...
java.lang.Error: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:603) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:582) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils$5.uncaughtException(Utils.java:931) [storm-client-2.0.0.y.jar:2.0.0.y]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_131]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_131]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_131]
Caused by: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.
        at org.apache.storm.zookeeper.AclEnforcement.getTopoAcl(AclEnforcement.java:194) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithTopoChildren(AclEnforcement.java:250) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithReadOnlyTopoChildren(AclEnforcement.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyAcls(AclEnforcement.java:136) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1155) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1162) ~[storm-server-2.0.0.y.jar:2.0.0.y]
{code}"
STORM-3116,"storm.home is not set in Clojure tests, nor is it guaranteed to be set when using LocalCluster","Several of the Clojure tests are littering directories like storm-core/null/storm-local, because storm.home isn't set. The root pom sets storm.home through Surefire, but that isn't used when running Clojure tests.

Also LocalCluster doesn't set storm.home, so it has the same issue."
STORM-3115,Add admin command to get a zookeeper shell,"At times ZK might get messed up, or a user may just want to see what storm is doing with it and it would be nice to have a simple command line tool that pops us into ZK where we can start to look for issues."
STORM-3114,Ban jdk.tools,jdk.tools snuck back in after STORM-2799 got merged. We should use Enforcer to ban it.
STORM-3113,Upgrade Mockito to fix Java 10 incompatibility,"https://github.com/mockito/mockito/issues/1243

I see no harm in upgrading to the latest Mockito version, which is 2.19.0 at the time of writing."
STORM-3112,Incremental scheduling supports,"As https://issues.apache.org/jira/browse/STORM-3093 described, now the scheduling work for a round is a complete scan and computation for all the topologies on cluster, which is a very heavy work when topologies increment to hundreds.

So this JIRA is to refactor the scheduling logic that only care about topologies that need to.

Promotions list:
1. Cache the id to storm base mapping which reduce the pressure to ZooKeeper.
2. Only schedule the topologies that need to: with dead executors or not enough running workers.
3. For some schedulers we still need a full scheduling, i.e. IsolationScheduler.
4. Cache the scheduling resource bestride multi scheduling round, i.e. nodeId -> used slot, nodeId -> used resource, nodeId -> totalResource.

Cause in https://issues.apache.org/jira/browse/STORM-3093 i already cache the storm-id -> executors mapping, now for a scheduling round, thing we will do:
1. Scan all the active storm bases( cached ) and local storm-conf/storm-topology, then to refresh the heartbeats cache, and we will know which topologies need to schedule.
2. Compute scheduleAssignment only for need scheduling topologies.

About robustness when nimbus restarts:
1. The cached storm-bases are taken care of by ILocalAssignmentsBackend.
2. the scheduling cache will be refresh for the first time scheduling through a full topologies scheduling.
"
STORM-3111,StormSubmitter localNimbus field is unused,"The localNimbus field in StormSubmitter appears to always be null. It looks like this has been the case since at least 1.0.0. Local mode runs fine without it, so we should remove it."
STORM-3109,Wrong canonical path set to STORM_LOCAL_DIR in storm kill_workers,"When `STORM_LOCAL_DIR` is set as a relative path, the original implementation incorrectly append the `STORM_LOCAL_DIR` after the current working directory upon invocation of `storm kill_workers`. If the current working directory is not the home directory for storm, in this `STORM_LOCAL_DIR` points to the incorrect location, so `storm kill_workers` can't actually kill workers at all.

See pull request for implementation."
STORM-3107,Nimbus confused about leadership after crash,"Nimbus crashed and restarted without shutting down zookeeper due to a deadlock in the timer shutdown code.  This could however also happen for various other issues.  

 

The problem is that once Nimbus restarted, it was really confused about who the leader was:

 
{code:java}
2018-05-24 09:27:21.762 o.a.s.z.LeaderElectorImp main [INFO] Queued up for leader lock.
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping assignments
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping cleanup
2018-05-24 09:27:22.633 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping credential renewal.

2018-05-24 09:27:40.771 o.a.s.d.n.Nimbus pool-37-thread-63 [WARN] Topology submission exception. (topology name='topology-testOverSubscribe-1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-05-24 09:27:40.771 o.a.s.b.BlobStoreUtils Timer-1 [ERROR] Could not download the blob with key: topology-testOverCapacityScheduling-2-1519992333-stormcode.ser
2018-05-24 09:27:40.771 o.a.t.s.TThreadPoolServer pool-37-thread-63 [ERROR] Error occurred during processing of message.
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2961) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more
{code}
The session timeout was set to 20 seconds, but we're exceeding this period, and Nimbus did not recover leadership.  It needed to be restarted manually to recover.

 "
STORM-3104,Delayed worker launch due to accidental transitioning in state machine,"In Slot.java, there is a comparison in 
{code:java}
handleWaitingForBlobUpdate()
{code}
 between dynamic state's current assignment and new assignment, which accidentally route back state machine just transitioned from WAIT_FOR_BLOB_LOCALIZATION back to WAIT_FOR_BLOB_LOCALIZATION, because the current assignment in this case is highly likely to be null and different from new assignment (I'm not sure if it's guaranteed). This causes delay for a worker to start/restart.

The symptom can be reproduced by launching an empty storm server and submit any topology. Here's the log sample (relevant transition starting from 2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [DEBUG]):

{code:sh}
2018-06-13 16:57:10.254 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 6024 -> EMPTY msInState: 6024
2018-06-13 16:57:10.255 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE EMPTY
2018-06-13 16:57:10.257 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from EMPTY to WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:10.257 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 6027 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2018-06-13 16:57:10.258 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:10.258 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingChangingBlobs are []
2018-06-13 16:57:11.259 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 1003 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 1003
2018-06-13 16:57:11.260 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:11.260 o.a.s.d.s.Slot SLOT_6700 [DEBUG] found changing blobs [BLOB CHANGING LOCAL TOPO BLOB TOPO_CONF test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02), BLOB CHANGING LOCAL TOPO BLOB TOPO_CODE test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)] moving them to pending...
2018-06-13 16:57:12.262 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 2005 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 2005
2018-06-13 16:57:12.263 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.263 o.a.s.d.s.Slot SLOT_6700 [DEBUG] found changing blobs [BLOB CHANGING LOCAL TOPO BLOB TOPO_JAR test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)] moving them to pending...
2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02) == current null ? false
2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [INFO] There are pending changes, waiting for them to finish before launching container...
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from WAITING_FOR_BLOB_LOCALIZATION to WAITING_FOR_BLOB_UPDATE
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 2018 -> WAITING_FOR_BLOB_UPDATE msInState: 1
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_UPDATE
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization: null, new: LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02), current: null, pdchanging: LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)
2018-06-13 16:57:12.276 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Assignment Changed from null to LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)
2018-06-13 16:57:12.278 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from WAITING_FOR_BLOB_UPDATE to WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.278 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_UPDATE msInState: 4 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingChangingBlobs are []
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02) == current null ? false
2018-06-13 16:57:12.280 o.a.s.d.s.Slot SLOT_6700 [DEBUG] launch container for the first time
2018-06-13 16:57:12.284 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Created Worker ID 4155b2bb-ebd1-431d-907f-d8a4ff1e1da4

{code}

I would like to know if this is actually the desired behavior of the state machine, or I can help fix the bug. The implementation would be to redesign the if statement."
STORM-3103,nimbus stuck shutting down causing leadership issues on startup,"When debugging an Nimbus NPE that caused restarts, I noticed that a forced halt occurred:

 
{code:java}
2018-05-24 09:27:05.569 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Opening socket connection to server openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-05-24 09:27:05.570 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Socket connection established to openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181, initiating session
2018-05-24 09:27:05.571 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Session establishment complete on server openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181, sessionid = 0x1624a86300f7f6b, negotiated timeout = 40000
2018-05-24 09:27:05.571 o.a.c.f.s.ConnectionStateManager main-EventThread [INFO] State change: CONNECTED
2018-05-24 09:27:05.636 o.a.s.d.n.Nimbus main [INFO] Starting nimbus server for storm version '2.0.0.y'
2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-05-24 09:27:06.023 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-05-24 09:27:06.032 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master
2018-05-24 09:27:06.032 o.a.s.u.Utils Thread-13 [INFO] Halting after 5 seconds
{code}
At times this would cause leadership confusion:

 
{code:java}
2018-05-24 09:27:21.762 o.a.s.z.LeaderElectorImp main [INFO] Queued up for leader lock.
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping assignments
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping cleanup
2018-05-24 09:27:22.633 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping credential renewal.

2018-05-24 09:27:40.771 o.a.s.d.n.Nimbus pool-37-thread-63 [WARN] Topology submission exception. (topology name='topology-testOverSubscribe-1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-05-24 09:27:40.771 o.a.s.b.BlobStoreUtils Timer-1 [ERROR] Could not download the blob with key: topology-testOverCapacityScheduling-2-1519992333-stormcode.ser
2018-05-24 09:27:40.771 o.a.t.s.TThreadPoolServer pool-37-thread-63 [ERROR] Error occurred during processing of message.
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2961) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more
{code}
We should endeavor to shutdown cleanly.

 

 

 

 "
STORM-3102,Storm Kafka Client performance issues with Kafka Client v1.0.0,"Recently I upgraded our storm topology to use the storm-kafka-client instead of storm-kafka.  After the upgrade in our production environment we saw a significant (2x) reduction in our processing throughput.

We process ~20000 kafka messages per second, on a 10 machine kafka 1.0.0 server cluster.

After some investigation, it looks like the issue only occurs when using kafka clients 0.11 or newer.

In kafka 0.11, the kafka consumer method commited always blocks to make an external call o get the last commited offsets

[https://github.com/apache/kafka/blob/e18335dd953107a61d89451932de33d33c0fd207/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1326-L1351]

In kafka 0.10.2 the kafka consumer only made the blocking remote call if the partition is not assigned to the consumer

[https://github.com/apache/kafka/blob/695596977c7f293513f255e07f5a4b0240a7595c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1274-L1311]

 

The impact of this is to require every tuple to make blocking remote calls before being emitted.  

[https://github.com/apache/storm/blob/2dc3d53a11aa3fea621666690d1e44fa8b621466/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L464-L473]

Removing this check returns performance to expected levels.

Looking through the storm-kafka-client code, it is not clear to me the impact of ignoring the check.  In our case we want at least once processing, but for other processing gurantees the call to kafkaConsumer.commited(tp) is not needed, as the value is only looked at if the processing mode is at least once."
STORM-3101,Fix unexpected metrics registration in StormMetricsRegistry,"Metrics that are registered using StormMetricRegistry all added through static method from the registry class, and attached to a singleton MetricRegistry object per process. Currently most metrics are bound to classes (static), so the issue occurs when metrics from irrelevant components are accidentally registered in class initialization phase. 

For example, a process running supervisor daemon will incorrectly register metrics from nimbus when BasicContainer class is initialized and statically imports ""org.apache.storm.daemon.nimbus.Nimbus.MIN_VERSION_SUPPORT_RPC_HEARTBEAT"", which triggers initialization of Nimbus class and all metrics registration, even though no functionalities of nimbus daemon will be used and no nimbus metrics will be updated. 

This creates many garbage metrics and makes metrics hard to read. Therefore we should filter metrics registration by the type of daemon that the process actually runs.

For implementation please see the pull request."
STORM-3099,"Extend metrics on supervisor, workers, and DRPC","This patch serves to extend metrics on supervisor and worker. Currently the following metrics are being implemented, including but not limited to:

Worker:
# Kill Count by Category - Assignment Change/HB too old/Heap Space
# Time spent in each state
# Time to Actually Kill worker (from identifying need by supervisor and actual change in the state of the worker) - per worker?
# Time to start worker for topology from reading assignment for the first time.
# Worker cleanup Time/Worker cleanup Retries
# Worker Suicide Count - category: internal error or Assignment Change

Supervisor:
# Supervisor restart Count 
# Blobstore (Request to download time) 
    - # Download time individual blob (inside localizer) localizer gettting requst to actually download hdfs request to finish
    - # Download rate individual blob (inside localizer)
    - # Supervisor localizer thread blob download - how long (outside localizer)
# Blobstore Update due to Version change Cnts
# Blobstore Storage by users

DRPC:
#  Avg/Max Time to respond to Http Request

There might be more metrics added later. 

This patch will also refactor code in relevant files. Bugs found during the process will be reported in other issues and handled separately."
STORM-3098,Fix bug in filterChangingBlobsFor() in Slot.java,"The following method is not implemented correctly

{code:java}
 private static DynamicState filterChangingBlobsFor(DynamicState dynamicState, final LocalAssignment assignment) {
        if (!dynamicState.changingBlobs.isEmpty()) {
            return dynamicState;
        }

        HashSet<BlobChanging> savedBlobs = new HashSet<>(dynamicState.changingBlobs.size());
        for (BlobChanging rc : dynamicState.changingBlobs) {
            if (forSameTopology(assignment, rc.assignment)) {
                savedBlobs.add(rc);
            } else {
                rc.latch.countDown();
            }
        }
        return dynamicState.withChangingBlobs(savedBlobs);
    }

{code}

It doesn't modify dynamicState in anyway.
The solution is to remove the negation in the first if statement."
STORM-3097,Remove storm-druid in 2.x and deprecate support for it in 1.x,"Trying again at this.

 

storm-druid depends on tranquility, which is not currently very well supported.  The druid community is moving in the direction of ingesting streaming data directly from Kafka, as such we are going to deprecate storm-druid in 1.x and remove it in the 2.x releases."
STORM-3094,Topology name needs to be validated at storm-client,"*Current Behavior :* Execute topology with invalid topology name is throwing exception after uploading the jar.

*Improvement :* Validating topology name at client side before uploading the jar.

 

 
{code:java}
2018-06-05 16:16:19.461 o.a.s.d.n.Nimbus pool-21-thread-53 [INFO] Uploading file from client to /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar
2018-06-05 16:16:20.596 o.a.s.d.n.Nimbus pool-21-thread-35 [INFO] Finished uploading file from client: /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar
2018-06-05 16:16:20.624 o.a.s.d.n.Nimbus pool-21-thread-29 [INFO] Received topology submission for test-[123] (storm-2.0.0-SNAPSHOT JDK-1.8.0_162) with conf {topology.users=[null], topology.acker.executors=null, storm.zookeeper.superACL=null, topology.workers=3, topology.submitter.principal=, topology.debug=true, topology.name=test-[123], topology.kryo.register={}, storm.id=test-[123]-7-1528195580, topology.kryo.decorators=[], topology.eventlogger.executors=0, topology.submitter.user=mvanam, topology.max.task.parallelism=null}
2018-06-05 16:16:20.624 o.a.s.d.n.Nimbus pool-21-thread-29 [INFO] uploadedJar /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar
2018-06-05 16:16:20.624 o.a.s.b.BlobStore pool-21-thread-29 [ERROR] 'test-[123]-7-1528195580-stormjar.jar' does not appear to be valid. It must match ^[\w \t\._-]+$. And it can't be ""."", "".."", null or empty string.
2018-06-05 16:16:20.625 o.a.s.b.BlobStore pool-21-thread-29 [ERROR] 'test-[123]-7-1528195580-stormconf.ser' does not appear to be valid. It must match ^[\w \t\._-]+$. And it can't be ""."", "".."", null or empty string.
2018-06-05 16:16:20.626 o.a.s.d.n.Nimbus pool-21-thread-29 [WARN] Topology submission exception. (topology name='test-[123]')
java.lang.IllegalArgumentException: test-[123]-7-1528195580-stormconf.ser does not appear to be a valid blob key
 at org.apache.storm.blobstore.BlobStore.validateKey(BlobStore.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]{code}
 

 "
STORM-3093,Cache the storm id to executors mapping on master to avoid repeat computation,"Now nimbus will collect all the topologies's conf/topology-ser/storm-base to compute in a scheduling round, which is a very heavy work. The scheduling will still take to minutes even we now change to RPC heartbeats and assignment distribution.

So i decide to redesign the scheduler, so we can only schedule the topologies that need to: that have dead workers or not enough number workers.

Here i checkout out the code and found that the id->executors mapping is computed every time for every topology, which is really a heavy computation and totally not that necessary, because this mapping is fixed invariable for a topology unless we rebalance or kill it.

So i refactor the code a little here, and this is more powerful after the scheduler is resigned for delta-scheduling[  which is very lightweight even there are thousands of topologies on one cluster.]

For now this is enough for us."
STORM-3092,Metrics Reporter and Shutdown Hook on Supervisor not properly set up at launchDaemon,"The bug was introduced in commit 0dac58b0aa82133df242b3b2ebeb65bfea7d63cc, when launchSupervisorThriftServer method is invoked in the launchDaemon method in Supervisor class. launchSupervisorThriftServer() invokes a blocking call to thrift server under the hood, hence preventing Utils.addShutdownHookWithForceKillIn1Sec and StormMetricsRegistry.startMetricsReporters from correctly called. 

 

The bug can be solved by moving launchSupervisorThriftServer to the end of the code block."
STORM-3091,worker heartbeat directory recreated after killing,"We see occasional instances where we kill the workers, wait for them to die, force delete the worker directory successfully, and the heartbeat directory is then re-created.  

 

It looks like when new local states get created, these will create the directory.  There is probably a race condition between the worker calling LocalState VersionStore mkdirs() and being killed and the supervisor directory cleanup.

 

Containers already guarantee this directory exists, so a fix could be to add an option to allow the LocalStates to create the dir or not.

 "
STORM-3090,The same offset value is used by the same partition number of different topics.,"In the current implementation of `ZkCoordinator` deleted partition managers are used as state holders for newly created partition managers. This behaviour was introduced in the scope of [this|https://issues-test.apache.org/jira/browse/STORM-2296] ticket. However existing lookup is based on only on partition number.
{code:java}
Map<Integer, PartitionManager> deletedManagers = new HashMap<>();
for (Partition id : deletedPartitions) {
 deletedManagers.put(id.partition, _managers.remove(id));
}
for (PartitionManager manager : deletedManagers.values()) {
 if (manager != null) manager.close();
}
LOG.info(taskPrefix(_taskIndex, _totalTasks, _taskId) + "" New partition managers: "" + newPartitions.toString());

for (Partition id : newPartitions) {
 PartitionManager man = new PartitionManager(
 _connections,
 _topologyInstanceId,
 _state,
 _topoConf,
 _spoutConfig,
 id,
 deletedManagers.get(id.partition));
 _managers.put(id, man);
{code}
Which is definitely incorrect as the same task is able to manage multiple partitions with the same number but for different topics. In this case all new partition managers obtain the same offset value from a random deleted partition manager (as `HashMap` is used). And all fetch requests for the new partition managers fail with `TopicOffsetOutOfRangeException`. Some of them are recovered via this logic if assigned offset is smaller than the real one, but other continue to repetitively fail with offset out of range exception preventing fetching messages from Kafka.
{code:java}
if (offset > _emittedToOffset) {
 _lostMessageCount.incrBy(offset - _emittedToOffset);
 _emittedToOffset = offset;
 LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
}
{code}
I assume that state holder lookup should be based both on topic and partition number."
STORM-3089,Document worker hooks on the hooks page,The hooks page http://storm.apache.org/releases/2.0.0-SNAPSHOT/Hooks.html only mentions task hooks. We should also describe worker hooks.
STORM-3087,FluxBuilder.canInvokeWithArgs is too permissive when the method parameter type is a primitive,"One of the clauses in canInvokeWithArgs is too permissive. It returns true if the declared method parameter type is a primitive, regardless of what the type of the actual parameter value is. This causes Flux to attempt to invoke the wrong methods in certain cases, which will trigger an IllegalArgumentException."
STORM-3086,Update Flux documentation to demonstrate static factory methods (STORM-2796),"I think we should add examples for static factory methods to the Flux documentation, the STORM-2796 changes don't seem to be mentioned."
STORM-3084,2.x NPE on Nimbus startup,"{code:java}
2018-05-24 09:27:05.636 o.a.s.d.n.Nimbus main [INFO] Starting nimbus server for storm version '2.0.0.y' 2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event java.lang.RuntimeException: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y] Caused by: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y] ... 2 more 2018-05-24 09:27:06.023 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event java.lang.RuntimeException: Halting process: Error while processing event at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y] 2018-05-24 09:27:06.032 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master 2018-05-24 09:27:06.032 o.a.s.u.Utils Thread-13 [INFO] Halting after 5 seconds
{code}"
STORM-3083,Upgrade HikariCP version to 2.4.7,"I've encountered the issue which reason was version conflict regarding disruptor via Storm and Phoenix 5. I've filed STORM-3077 and resolved the root reason, but I also found that HikariCP doesn't give proper error message on this. It just complains about timeout while initializing pool, but the actual error was NoSuchMethodError.

The issue in HikariCP version we use is that it leverages default implementation of ThreadPoolExecutor even while it checks fail-fast in initialization. So fail-fast works anyway, but it says ""timed-out"", not exposing the error.

One thing to note is that HikariCP moved the main version line to support JDK 8 and change artifact name to ""HikariCP-java7"" on JDK7 compatible maintenance version. If we would like to provide same user experience, we should keep using ""HikariCP"" artifact, not ""HikariCP-java7"", since Maven might not recognize they're same.

Hopefully I found there's another HikariCP version (2.4.7) which all of below conditions are met:
 * artifact name is ""HikariCP"", not ""HikariCP-java7""
 * fail-fast works correctly
 * supports Java 7

 

So upgrading HikariCP to 2.4.7 would resolve the issue in simplest and safest way."
STORM-3082,NamedTopicFilter can't handle topics that don't exist yet,"[~aniket.alhat] reported on the mailing list that he got an NPE when trying to start the Trident spout.

{code}
2018-05-22 06:23:02.318 o.a.s.util [ERROR] Async loop died!
java.lang.NullPointerException: null
        at org.apache.storm.kafka.spout.NamedTopicFilter.getFilteredTopicPartitions(NamedTopicFilter.java:57) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.ManualPartitionSubscription.refreshAssignment(ManualPartitionSubscription.java:54) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.ManualPartitionSubscription.subscribe(ManualPartitionSubscription.java:49) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutManager.createAndSubscribeKafkaConsumer(KafkaTridentSpoutManager.java:59) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.<init>(KafkaTridentSpoutEmitter.java:84) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.<init>(KafkaTridentSpoutEmitter.java:100) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutOpaque.getEmitter(KafkaTridentSpoutOpaque.java:50) ~[stormjar.jar:?]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.<init>(OpaquePartitionedTridentSpoutExecutor.java:97) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor.getEmitter(OpaquePartitionedTridentSpoutExecutor.java:221) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor.getEmitter(OpaquePartitionedTridentSpoutExecutor.java:39) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.TridentSpoutExecutor.prepare(TridentSpoutExecutor.java:60) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.topology.TridentBoltExecutor.prepare(TridentBoltExecutor.java:245) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.daemon.executor$fn__5043$fn__5056.invoke(executor.clj:803) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.2.1.jar:1.2.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]
{code}

It looks to me like the partitionsFor method on the consumer will return null if the specified topic doesn't exist. We didn't account for this in the filter, because the return type of the method is a List, and we assumed it wouldn't be null.

I think it's reasonable that people should be able to subscribe to topics that don't exist yet, and the spout should pick up the new topics eventually.

We should check for null here https://github.com/apache/storm/blob/93ed601425a79759c0189a945c6b46266e5c9ced/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/NamedTopicFilter.java#L55, and maybe log a warning if the returned value is null."
STORM-3079,improve getMessage support for ThriftExceptions,"I've seen error callstacks similar to this and been confused as to the null message.  The generated thrift code does not support getMessage().  We should try and improve the log messages.

 
2018-05-16 21:15:04.596 o.a.s.d.n.Nimbus timer [INFO] Exception {}
org.apache.storm.generated.KeyNotFoundException: null        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:393) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:670) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2333) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2387) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2674) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]"
STORM-3078,HBAuthorizationException appears unused,Looks like this class is safe to remove.
STORM-3075,NPE starting nimbus,"{code:java}
2018-05-15 14:14:59.873 o.a.c.f.l.ListenerContainer main-EventThread [ERROR] Listener (org.apache.storm.zookeeper.Zookeeper$1@26d820eb) threw an exception
java.lang.NullPointerException: null
        at org.apache.storm.nimbus.LeaderListenerCallback.leaderCallBack(LeaderListenerCallback.java:118) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.Zookeeper$1.isLeader(Zookeeper.java:124) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) ~[curator-client-4.0.1.jar:?]
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:660) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:539) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:590) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]

{code}"
STORM-3074,Inconsistent null checking in SaslMessageToken,"The SaslMessageToken class will throw an NPE if buffer() is called and the payload is null. While the buffer method checks whether the token is null in a few places before dereferencing, the encodedLength method is called right off the bat, and it doesn't check for null.

The payload is always generated by either https://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslServer.html#evaluateResponse(byte[]) or https://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslClient.html#evaluateChallenge(byte[]). The javadoc indicates that if these return null, authentication has succeeded and it is unnecessary to send any more messages to the other party.

I think if null SaslMessageToken payloads are never sent over the wire, we should remove all the null checking in SaslMessageToken and MessageDecoder, and ensure that the SASL handlers check for null before deciding to write tokens."
STORM-3073,In some cases workers may crash because pendingEmits is full,"Saw this while running the https://github.com/apache/storm/blob/master/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/ThroughputVsLatency.java topology.

{code}
2018-05-15 11:35:28.365 o.a.s.u.Utils Thread-16-spout-executor[8, 8] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: Queue full
	at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:168) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:157) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:349) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: java.lang.IllegalStateException: Queue full
	at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_144]
	at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:516) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:140) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:70) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:42) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:360) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:120) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:63) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:295) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.Executor.accept(Executor.java:278) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 7 more
{code}

The executor's pendingEmits queue is full, and the executor then tries to add another tuple. It looks to me like we're preventing the queue from filling by emptying it between calls to nextTuple at https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java#L184.

The TVL topology reemits failed tuples directly from the fail method, which can be triggered by tick tuples. If the pendingEmits queue is already close to full when this happens, we might hit the error above. I think it can also happen if nextTuple emits too many tuples in a call, or if too many metrics ticks happen between pendingEmit flushes, since metrics ticks also trigger emits."
STORM-3072,Frequent test failures in storm-sql-core,"Seeing test failures in storm-sql-core, sometimes regular test failures, other times JVM crashes.
{code}
testExternalUdf(org.apache.storm.sql.TestStormSql)  Time elapsed: 8.177 sec  <<< ERROR!
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='DESKTOP-AGC8TKM.localdomain', port=6627, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2952)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopology(Nimbus.java:2761)
        at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:378)
        at org.apache.storm.sql.StormSqlLocalClusterImpl.runLocal(StormSqlLocalClusterImpl.java:68)
        at org.apache.storm.sql.TestStormSql.testExternalUdf(TestStormSql.java:214)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='DESKTOP-AGC8TKM.localdomain', port=6627, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1302)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2798)
        ... 41 more

testExternalNestedArrayOutOfBoundAccess(org.apache.storm.sql.TestStormSql)  Time elapsed: 0.598 sec  <<< ERROR!
KeyNotFoundException(msg:storm-sql-1-1526308521-stormcode.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:422)
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1443)
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2593)
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfo(Nimbus.java:4183)
        at org.apache.storm.LocalCluster.getClusterInfo(LocalCluster.java:470)
        at org.apache.storm.sql.StormSqlLocalClusterImpl.runLocal(StormSqlLocalClusterImpl.java:71)
        at org.apache.storm.sql.TestStormSql.testExternalNestedArrayOutOfBoundAccess(TestStormSql.java:170)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
{code}

The cause seems to be similar to https://issues.apache.org/jira/browse/STORM-3065, some of the tests are interfering with each other. Reducing to fork count 1 fixes it."
STORM-3070,"MessageDecoder forgets to rewind buffer position if BackpressureStatus code is received, but the rest of the message is pending","It looks to me like https://github.com/apache/storm/blob/5deba40fca0f88f61d2086cb902318ad9bb044f1/storm-client/src/jvm/org/apache/storm/messaging/netty/MessageDecoder.java#L108 should reset the buffer position before returning, since it has read the message code at this point."
STORM-3068,STORM_JAR_JVM_OPTS are not passed to storm-kafka-monitor  properly,STORM_JAR_JVM_OPTS are not being passed to storm-kafka-monitor properly which can limit the user to pass java configuration such as ssl truststore etc.
STORM-3067,Kafka Spout has no active members in consumer group,"Using Kafka 10.2.1, two different behaviours were observed:

Storm 1.1.0 =>
{code}
/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test.topic.consumer-group --describe
Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).


TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID
test.topic 0 85186604 85186607 3 consumer-2-27a77d1b-e851-47a4-954e-4953ea612b72 /X.X.X.X consumer-2

{code}
 

Storm 1.1.2 =>
{code:java}
/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test.topic.consumer-group --describe
Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).



Consumer group 'test.topic.consumer-group' has no active members.

TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID
test.topic 0 85202473 85202475 2 -{code}

Despite this behaviour, the topology continues to consume messages and commit offsets to Kafka. It's unclear if this lack of active clients on the consumer group affects the normal functioning of the Spout."
STORM-3065,Very frequent test failures in storm-server,"I'm seeing the following intermittent test failures in storm-server when I run locally

{code}
2018-05-09 16:32:23.377 [main] ERROR org.apache.storm.blobstore.KeySequenceNumber - Exception {}
java.lang.NullPointerException: null
	at java.lang.String.contains(String.java:2133) ~[?:1.8.0_152]
	at org.apache.storm.blobstore.KeySequenceNumber.checkIfStateContainsCurrentNimbusHost(KeySequenceNumber.java:206) ~[classes/:?]
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:159) [classes/:?]
	at org.apache.storm.daemon.nimbus.Nimbus.getVersionForKey(Nimbus.java:655) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:223) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStore$MockitoMock$1067706995.createBlob$accessor$Ub7aO1Cr(Unknown Source) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStore$MockitoMock$1067706995$auxiliary$x0GVZISq.call(Unknown Source) [classes/:?]
	at org.mockito.internal.invocation.RealMethod$FromCallable.invoke(RealMethod.java:48) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.InterceptedInvocation.callRealMethod(InterceptedInvocation.java:129) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:43) [mockito-core-2.10.0.jar:?]
	at org.mockito.Answers.answer(Answers.java:100) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:97) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:65) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:51) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:135) [mockito-core-2.10.0.jar:?]
	at org.apache.storm.blobstore.LocalFsBlobStore$MockitoMock$1067706995.createBlob(Unknown Source) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasic(LocalFsBlobStoreTest.java:325) [test-classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasicLocalFs(LocalFsBlobStoreTest.java:114) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_152]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_152]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_152]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_152]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) [junit-4.11.jar:?]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.11.jar:?]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) [junit-4.11.jar:?]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.11.jar:?]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.11.jar:?]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) [junit-4.11.jar:?]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) [junit-4.11.jar:?]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) [junit-4.11.jar:?]
	at org.junit.runners.Suite.runChild(Suite.java:127) [junit-4.11.jar:?]
	at org.junit.runners.Suite.runChild(Suite.java:26) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) [junit-4.11.jar:?]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290) [surefire-booter-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242) [surefire-booter-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121) [surefire-booter-2.19.1.jar:2.19.1]
{code}
and 
{code}
java.lang.NullPointerException
	at java.lang.String.startsWith(String.java:1405)
	at java.lang.String.startsWith(String.java:1434)
	at org.apache.storm.zookeeper.ClientZookeeper.deleteNodeBlobstore(ClientZookeeper.java:86)
	at org.apache.storm.cluster.ZKStateStorage.delete_node_blobstore(ZKStateStorage.java:93)
	at org.apache.storm.cluster.StormClusterStateImpl.setupBlob(StormClusterStateImpl.java:704)
	at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:223)
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasic(LocalFsBlobStoreTest.java:325)
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasicLocalFs(LocalFsBlobStoreTest.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
{code}

This seems to be because the LocalFsBlobStoreTest is mocking NimbusInfo, and the properties on that object aren't stubbed. Occasionally the test will try to e.g. do a string contains with the null value from the stub, which throws an NPE. Since NimbusInfo is just a POJO, I don't see why we would need to mock it, we could just supply dummy values.

I'm also seeing test failures on nearly every run locally, seemingly due to interference between the tests. Example:
{code}
testLocalTransport(org.apache.storm.MessagingTest)  Time elapsed: 8.197 sec  <<< ERROR!
java.lang.RuntimeException: java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2952)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopology(Nimbus.java:2761)
        at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:378)
        at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:121)
        at org.apache.storm.Testing.completeTopology(Testing.java:424)
        at org.apache.storm.MessagingTest.testLocalTransport(MessagingTest.java:57)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?
        at org.apache.storm.zookeeper.Zookeeper.toNimbusInfo(Zookeeper.java:109)
        at org.apache.storm.zookeeper.LeaderElectorImp.getLeader(LeaderElectorImp.java:108)
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1301)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2798)
        ... 39 more
{code}
as well as occasional connection losses to Zookeeper. Reducing storm-server to run one test at a time, rather than with a forkCount of 1 per core eliminates these issues."
STORM-3064,PartitionedTridentSpoutExecutor should use getPartitionsForTask,"https://issues.apache.org/jira/browse/STORM-2407 added a method to the IOpaquePartitionedTridentSpout.Emitter interface called getPartitionsForTask, which is now used to delegate partitioning between tasks (previously, the partitioning was hard coded to round robin).

If we want to be able to delegate partitioning, I don't see a reason not to make the same change on the IPartitionedTridentSpout.Emitter interface, where partitioning is still hard coded to use round robin. E.g. compare https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/storm-client/src/jvm/org/apache/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L131 to https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/storm-client/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L131"
STORM-3063,Minor POM issues,"storm-core has a duplicate Zookeeper declaration.

The parent pom uses the prerequisites tag to set minimum Maven version. Maven 3.5.3 gives the following warning:
{quote}
The project org.apache.storm:storm:pom:2.0.0-SNAPSHOT uses prerequisites which is only intended for maven-plugin projects but not for non maven-plugin projects. For such purposes you should use the maven-enforcer-plugin. See https://maven.apache.org/enforcer/enforcer-rules/requireMavenVersion.html
{quote}"
STORM-3062,Document JMXStormReporter configuration,"discussion on the PR for STORM-2988(https://issues.apache.org/jira/browse/STORM-2988) to document JMXStormReporter.

 "
STORM-3061,Upgrade Dependencies before 2.x release,Storm has a lot of dependencies.  It would be great to upgrade many of them to newer versions ahead of a 2.x release.
STORM-3059,KafkaSpout throws NPE when hitting a null tuple if the processing guarantee is not AT_LEAST_ONCE,"Introduced with STORM-2994

{quote}
java.lang.NullPointerException: null
        at org.apache.storm.kafka.spout.KafkaSpout.emitOrRetryTuple(KafkaSpout.java:507)
~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.KafkaSpout.emitIfWaitingNotEmitted(KafkaSpout.java:440)
~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:308)
~[stormjar.jar:?]
{quote}"
STORM-3058,set TOPOLOGY_MAX_SPOUT_PENDING to one，but can get more than one tuple in bolt,"Spout in the nextTuple may pull more than one kafka message, will remain in the _waitingToEmit, and then launch one by one, even if the TOPOLOGY_MAX_SPOUT_PENDING set to 1, when the first tuple is defeated, the bolt will still receive follow-up Message until _waitingToEmit is completely transmitted.

This and I think TOPOLOGY_MAX_SPOUT_PENDING is not the same meaning, I hope that the message can be strictly processed one by one, my topology worker number, spout and bolt parallelism are set to 1"
STORM-3056,Add a test for quickly rebinding to a port,We need to add a test for the bug fix of STORM-3039. We try to rebind to port 6700 a few times and expect it to be usable quickly.
STORM-3055,never refresh connection,"in our enviroment some worker's connection to other worker being closed and never reconnect,

the log show's that 

2018-05-02 10:28:49.302 o.a.s.m.n.Client Thread-90-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.31.1:6800 is being closed

......
2018-05-02 11:00:29.540 o.a.s.m.n.Client Thread-90-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.31.1:6800 is being closed

the log shows that it never can reconnect again. i can only fix it after restart the topo, "
STORM-3053,blobstores deleted before topologies can be submitted,"We have integration tests failing that create a blobstore and then fail to submit a topology.  Digging into the logs, it looks like there is a nimbus thread that runs every 10 seconds and deletes blobstores if there is no associated topology.  We're hitting a race between the two calls periodically.  

 

 

A possible fix could be to save the timestamp when the blobstore AtomicOutputStream closes and give some grace period before cleaning up based on this timestamp.

 

 

 "
STORM-3047,Ensure Trident emitter refreshPartitions is only called with partitions assigned to the emitter,"This is a backport of the changes made to OpaquePartitionedTridentSpoutExecutor in https://github.com/apache/storm/pull/2300/files.

The description of the issue is copied here for convenience:

The changes in https://github.com/apache/storm/pull/2009 released in 1.1.0 made some changes to the OpaquePartitionedTridentSpoutExecutor that likely broke IOpaquePartitionedTridentSpout implementations other than storm-kafka-client. The changed code used to request sorted partitions from the spout via getOrderedPartitions, do a round-robin partitioning, and assign partitions via refreshPartitions https://github.com/apache/storm/blob/v1.0.4/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L100. The new code just passes the output of getOrderedPartitions into refreshPartitions https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L120. It looks to me like refreshPartitions is passed the list of all partitions assigned to any spout task, rather than just the partitions assigned to the current task.

The proposed fix will use getOrderedPartitions to get the sorted partitions list, pass the list into getPartitionsForTask, and pass the resulting list of assigned partitions back into refreshPartitions.
"
STORM-3045,Microsoft Azure EventHubs: Storm Spout and Bolt improvements,
STORM-3043,NullPointerException thrown in SimpleRecordTranslator.apply(),"When using a SimpleRecordTranslator with a user-defined translator Func, a NullPointerException will be thrown if Func.apply() returns null. A null List object is a valid return value from apply() if the ConsumerRecord is invalid.

SimpleRecordTranslator does not check for a null result before attempting to call the addAll method of the List."
STORM-3041,worker-launcher setup is confusing,"I had a misconfigured worker-launcher and multiple things confused me:

 

1) I wasn't immediately finding the config file location

2) config failures result in info messages in the logs.  My search for ERROR failed to find anything.

3) worker-launcher messages indicate node managers.

 

 "
STORM-3040,RAS scheduling performance improvements,Even after fixing a lot of the loops in RAS scheduling there is still more we can do to make the performance even better.  This is especially true for the generic resource aware strategy.
STORM-3039,Ports of killed topologies remain in TIME_WAIT state preventing to start new topology,"When topology is killed the slot ports (supervisor.slots.ports) remain in TIME_WAIT state. In that case new topology can not be started, because workers throw the following error:
{code:java}
2018-04-20 08:37:08.742 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker
org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6700
 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.worker$worker_data$fn__10395.invoke(worker.clj:285) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:282) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.worker$fn__10693$exec_fn__3301__auto__$reify__10695.run(worker.clj:626) ~[storm-core-1.2.1.jar:1.2.1]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_161]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_161]
 at org.apache.storm.daemon.worker$fn__10693$exec_fn__3301__auto____10694.invoke(worker.clj:624) ~[storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
 at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
 at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
 at org.apache.storm.daemon.worker$fn__10693$mk_worker__10784.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
 at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
 at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
 at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
Caused by: java.net.BindException: Address already in use
 at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_161]
 at sun.nio.ch.Net.bind(Net.java:433) ~[?:1.8.0_161]
 at sun.nio.ch.Net.bind(Net.java:425) ~[?:1.8.0_161]
 at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:1.8.0_161]
 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.8.0_161]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.2.1.jar:1.2.1]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
 at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_161]

{code}
 

This exception occurs often when topologies stopped and started automatically."
STORM-3036,Add isRemoteBlobExists RPC interface for deciding if remote blob exists,"As https://github.com/apache/storm/pull/2618 has described, now we try catch a KeyNotFoundException to decide remote blob does not exist.

It is confusing because user had removed it/them on their own initiative but we still got a KeyNotFoundException at both server and client point.

So i add a new api for this case."
STORM-3031,size of particules in StormUI visualization,"In StormUI visualization, the size of particules is driven by the length of the component name in my opinion it's counter-intuitive.
I think that a fixed size or may be a configurable size would be a better choice

May be a good choice would be that the size be linked with some kind of performance or latency.
Currently the color is the capacity, we could set the size as a reflect of the component latency (with an lower and upper limit)?"
STORM-3026,Upgrade ZK instance for security,"It would be great to have the ability to move an existing cluster with it's ZK from insecure to secure without wiping everything clean.  This does not allow for a rolling upgrade because the running topologies will not have the credentials that they need, but you don't need to do the manual step of deleting the root ZK node."
STORM-3024,Allow scheduling for RAS to happen in the background,"We have run into some issues recently where occasionally a strategy on a very large cluster will take an extra long amount of time finish scheduling.  This slowness cascades into other issues, like topologies not being able to be killed because the timer thread is still in use trying to run scheduling.

The plan is to make scheduling happen in a thread pool.  The main thread will wait for up to a configurable amount of time for the topology to be scheduled, but if it does not complete in that time it will be left to keep running in the background thread in hopes that later on it will be scheduled.

If for some reason the state of the cluster changes while scheduling is happening in the background we will cancel the scheduling, as any scheduling it produced may not be able to fit on the cluster.  The next time the scheduler runs it will restart the scheduling and hopefully allow the cluster to reach a steady state even if it takes a while, but without blocking kills and other critical operations from happening.

Note that we are also working on optimizing scheduling as well so that these issues don't happen in the first place."
STORM-3020,Fix race condition is async localizer,"I think this impacts all of the code that uses asynclocalizer, but I need to check to be sure.

As part of a review of a different pull request against AsyncLocalizer I noticed that requestDownloadTopologyBlobs is synchronized, but everything it does is async, but there is a race in one of the async pieces where we read from a map, and then try to update the map later, all outside of a lock."
STORM-3019,StormReporter doesn't have information on where it's running,"Metrics2 StormReporter implementations don't have a lot of information on where they're running. In particular, they are missing:
 * Whether they are running for nimbus, supervisor, or worker, and what the worker is.
 * The full deployed config - it's just provided with the basic topology configuration, not the full effective configuration as specified at topology deployment
 * A TopologyContext object"
STORM-3018,Fix integration test DemoTest#testExclamationTopology fail problem,"For https://github.com/apache/storm/pull/2433 i changed the task metrics reporting interval default to 60 seconds, which is same with the  DemoTest#testExclamationTopology check time.

So fix it here."
STORM-3016,Nimbus gets down when job has large amount of parallelism components,"When a job having large amount of parallelism components( total parallelism rises to 5000 for example) been submmited to storm cluster, Nimubs might get crashed, the work flow is as below:

1)  Nimbus computting assignment

2) Nimbus sending assignment to zk

{color:#ff0000}3) When assignment mapping info string is too long due to  total parallelism of job being too large, sending this info to zk will fail (zNode datalength set default is 1M ){color}

{color:#333333}4) Nimbus keeps trying sending this assignment info, after some times, it gives up and crashed, with that happend, the stablity of the cluster will be greatly impacted{color}"
STORM-3015,storm-kafka-client-examples should not depend on the Kafka server jar,"The kafka_2.10 dependency is unnecessary, and adds a little more hassle than necessary to switching the Kafka client version. E.g. To switch to Kafka 1.1.0, you have to also replace kafka_2.10 with kafka_2.11"
STORM-3014,TickTupleTest.testTickTupleWorksWithSystemBolt fails intermittently in Travis CI,"[https://travis-ci.org/apache/storm/jobs/359417643]

 
{code:java}
classname: org.apache.storm.TickTupleTest / testname: testTickTupleWorksWithSystemBolt
java.lang.AssertionError: took over 110000 ms of simulated time to get a message back...
	at org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt(TickTupleTest.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121){code}
 

Note that it is not failing consistently. Here's the build which the test passed.

[https://travis-ci.org/HeartSaVioR/storm/builds/359567639]

 "
STORM-3013,Deactivated topology restarts if data flows into Kafka,"Hi, I have deactivated the storm topology & then if I produce any records into Kafka, Storm throws an exception. Exception follows,
{code:java}
2018-03-28 09:50:23.804 o.a.s.d.executor Thread-83-kafkaLogs-executor[130 130] [INFO] Deactivating spout kafkaLogs:(130)
2018-03-28 09:51:01.289 o.a.s.util Thread-17-kafkaLogs-executor[139 139] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: This consumer has already been closed.
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:634) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
Caused by: java.lang.IllegalStateException: This consumer has already been closed.
at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:1787) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1622) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$metrics_tick$fn__4899.invoke(executor.clj:345) ~[storm-core-1.2.1.jar:1.2.1]
at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
at clojure.core$filter$fn__4580.invoke(core.clj:2679) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.7.0.jar:?]
at clojure.lang.RT.next(RT.java:674) ~[clojure-1.7.0.jar:?]
at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?]
at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]
at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]
at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$tuple_action_fn__4981.invoke(executor.clj:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:471) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]
... 7 more
{code}"
STORM-3009,Storm-webapp has multiple SLF4j bindings on the classpath,"Storm-webapp has both logback and log4j2 bindings for SLF4j on the classpath. The logback binding comes from our Dropwizard dependency. As of Dropwizard 1.2.0 (https://github.com/dropwizard/dropwizard/pull/1900), Dropwizard isn't tightly coupled to logback, so we can exclude that dependency."
STORM-3008,Add Windows CI coverage,"I'd like to see us add test runs on Windows to our CI. Tests occasionally break on Windows, and we don't catch it during PR review because Travis only tests on Linux. 

Since Travis doesn't offer Windows support, we could look at using https://www.appveyor.com/, which is also free for open source projects. It's already used by a few other Apache projects, like Thrift and Spark."
STORM-3005,[DRPC] LinearDRPCTopologyBuilder shouldn't be deprecated ,"Apache Storm provides DRPC functionality. However LinearDRPCTopologyBuilder is deprecated due to: ""Trident subsumes the functionality provided by this class, so it's deprecated"". I think it shouldn't be so, because you may still want to use DRPC without Trident.

LinearDRPCTopologyBuilder is also mentioned as a part of example in current documentation: [http://storm.apache.org/releases/1.2.1/Distributed-RPC.html]"
STORM-3002,Checkstyle plugin failure,"I followed the directions on this page: [https://github.com/apache/storm/tree/master/examples/storm-starter#build-and-install-storm-jars-locally] working on ref 8c8c0c31c70f3e73b85083d84f5ab9475c6f1e2b

 

When I run:
{code:java}
mvn clean install -DskipTests=true{code}
 

I get the following error:
{code:java}
...[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:49 min
[INFO] Finished at: 2018-03-20T21:54:28-05:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project storm-starter: Execution validate of goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check failed: Plugin org.apache.maven.plugins:maven-checkstyle-plugin:2.17 or one of its dependencies could not be resolved: Could not find artifact org.apache.storm:storm-checkstyle:jar:2.0.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR]{code}"
STORM-2998,Wrong className in LoggerFactory.getLogger method,"If we use LoggerFactory.getLogger method in a class,the most accurate approach is use this class as a parameter."
STORM-2995,Topology runtime exception - Error on initialization,"{code:java}
2018-03-14 13:28:41.399 o.a.s.d.worker main [INFO] Reading Assignments. 2018-03-14 13:28:41.511 o.a.s.m.TransportFactory main [INFO] Storm peer transport plugin:org.apache.storm.messaging.netty.Context 2018-03-14 13:28:41.935 o.a.s.m.n.Server main [INFO] Create Netty Server Netty-server-localhost-6712, buffer_size: 5242880, maxWorkers: 1 2018-03-14 13:28:41.980 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6712 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data$fn__5244.invoke(worker.clj:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:269) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544.run(worker.clj:613) ~[storm-core-1.1.0.jar:1.1.0] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_51] at javax.security.auth.Subject.doAs(Subject.java:415) ~[?:1.7.0_51] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto____5543.invoke(worker.clj:611) ~[storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?] at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0] Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:444) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:436) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_51] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.1.0.jar:1.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_51] at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_51] 2018-03-14 13:28:42.004 o.a.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"") at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0]
{code}"
STORM-2994,KafkaSpout consumes messages but doesn't commit offsets,"A topology that consumes from two different Kafka clusters: 0.10.1.1 and 0.10.2.1.

Spouts consuming from 0.10.2.1 have a low lag (and regularly commit offsets) 

The Spout that consumes from 0.10.1.1 exhibits either:

1- Unknown lag

2- Lag that increments as the Spout reads messages from Kafka

 

In DEBUG, Offset manager logs: ""topic-partition has NO offsets ready to be committed"", despite continuing to consume messages.

Several configuration tweaks were tried, including setting maxRetries to 1, in case messages with a lower offset were being retried (logs didn't show it, though)

offsetCommitPeriodMs was also  lowered to no avail.

The only configuration that works is to have ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG=true, but this is undesired   since we lose processing guarantees.

 "
STORM-2992,Add support for starting storm-kafka-client spout consumer at a specific timestamp ,"The 0.10.1.0 KafkaConsumer has support for getting the offset corresponding to a given timestamp (the offsetsForTimes method). We could provide a new FirstPollOffsetStrategy to allow topologies to start consumption at a specific timestamp, instead of earliest or latest offset."
STORM-2991,Use MockConsumer for tests where possible in storm-kafka-client instead of using a raw Mockito mock,"MockConsumer seems like it will be less brittle for testing than using a raw Mockito mock, e.g. we don't have to manually stub the Consumer.position call. We should try to replace the mocks in existing tests."
STORM-2990,Make the storm-kafka-client Trident spout FirstPollOffsetStrategy behavior consistent with the regular spout,The EARLIEST and LATEST semantics were changed in the regular spout so the spout only starts over when the topology is redeployed. The Trident spout should behave the same way.
STORM-2989,LogCleaner should preserve current worker.log.metrics,"LogCleaner cleans up logs based on LastModified timestamp. Like 

[https://github.com/apache/storm/blob/master/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/DirectoryCleaner.java#L45]

 

I think we should at least preserve the current worker.log.metrics"
STORM-2988,"""Error on initialization of server mk-worker"" when using org.apache.storm.metrics2.reporters.JmxStormReporter on worker","As per documentation, I configured metrics v2 in my storm.yaml using the following configuration:
 
{code:yaml}
storm.metrics.reporters:

  - class: ""org.apache.storm.metrics2.reporters.JmxStormReporter""
    daemons:
        - ""supervisor""
        - ""nimbus""
        - ""worker""
    report.period: 10
    report.period.units: ""SECONDS""
{code}

When I start nimbus and supervisors everything works properly, I can see metrics reported to JMX, and logs (for nimbus in this example) report:

{code}
2018-03-07 15:35:22.201 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
2018-03-07 15:35:22.203 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2018-03-07 15:35:22.221 o.a.s.d.common main [INFO] Started statistics report plugin...
{code}

When I submit a topology, workers cannot initialize and report this error

{code:java}
2018-03-07 15:39:19.136 o.a.s.d.worker main [INFO] Launching worker for stp_topology-1-1520433551 on [... cut ...]
2018-03-07 15:39:19.169 o.a.s.m.StormMetricRegistry main [INFO] Starting metrics reporters...
2018-03-07 15:39:19.172 o.a.s.m.StormMetricRegistry main [INFO] Attempting to instantiate reporter class: org.apache.storm.metrics2.reporters.JmxStormReporter
2018-03-07 15:39:19.175 o.a.s.m.r.JmxStormReporter main [INFO] Preparing...
2018-03-07 15:39:19.182 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker
java.lang.IllegalArgumentException: Don't know how to convert {""class"" ""org.apache.storm.metrics2.reporters.JmxStormReporter"", ""daemons"" [""supervisor"" ""nimbus"" ""worker""], ""report.period"" 10, ""report.period.units"" ""SECONDS""} + to String
	at org.apache.storm.utils.Utils.getString(Utils.java:848) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain(JmxStormReporter.java:70) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.reporters.JmxStormReporter.prepare(JmxStormReporter.java:51) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.StormMetricRegistry.startReporter(StormMetricRegistry.java:119) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.StormMetricRegistry.start(StormMetricRegistry.java:102) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.daemon.worker$fn__5545$exec_fn__1369__auto____5546.invoke(worker.clj:611) ~[storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
2018-03-07 15:39:19.195 o.a.s.util main [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
{code}

Looking at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain() I found that it passes ""reporterConf"" map to Utils.getString() instead of a string:
{code:java}
    public static String getMetricsJMXDomain(Map reporterConf) {
        return Utils.getString(reporterConf, JMX_DOMAIN);
}
{code}

The ""prepare"" method in org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter used by nimbus and supervisor correctly passes a string to Utils.getString():

{code:java}
public void prepare(MetricRegistry metricsRegistry, Map stormConf) {
        LOG.info(""Preparing..."");
        JmxReporter.Builder builder = JmxReporter.forRegistry(metricsRegistry);
        String domain = Utils.getString(stormConf.get(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_DOMAIN), null);
        if (domain != null) {
            builder.inDomain(domain);
}
[...]
{code}

Is this a bug or am I missing something in configuration?

Regards,
Federico Chiacchiaretta"
STORM-2987,PaceMakerStateStorage should deal with InterruptedException correctly,"We found an issue that when nimbus restarted, it can only get leadership after a few seconds (15~20s). 

 
{code:java}
2018-02-27 08:18:43.420 main o.a.s.z.LeaderElectorImp [INFO] Queued up for leader lock.
2018-02-27 08:18:43.481 main o.a.s.d.m.MetricsUtils [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
2018-02-27 08:18:43.483 main o.a.s.d.m.r.JmxPreparableReporter [INFO] Preparing...
2018-02-27 08:18:43.499 main o.a.s.m.StormMetricsRegistry [INFO] Started statistics report plugin...
2018-02-27 08:18:43.543 main o.a.s.m.n.Login [INFO] successfully logged in.
2018-02-27 08:18:43.551 main o.a.s.z.ClientZookeeper [INFO] Staring ZK Curator
2018-02-27 08:18:43.551 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
2018-02-27 08:18:43.552 Refresh-TGT o.a.s.m.n.Login [INFO] TGT refresh thread started.
2018-02-27 08:18:43.553 Refresh-TGT o.a.s.m.n.Login [INFO] TGT valid starting at:        Tue Feb 27 08:18:43 UTC 2018
2018-02-27 08:18:43.553 Refresh-TGT o.a.s.m.n.Login [INFO] TGT expires:                  Wed Feb 28 08:18:43 UTC 2018
2018-02-27 08:18:43.553 Refresh-TGT o.a.s.m.n.Login [INFO] TGT refresh sleeping until: Wed Feb 28 04:35:55 UTC 2018
2018-02-27 08:18:43.553 main o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=openqe74blue-gw.blue.ygrid.yahoo.com:2181 sessionTimeout=60000 watcher=org.apache.
curator.ConnectionState@2e185cd7
2018-02-27 08:18:43.559 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Default schema
2018-02-27 08:18:43.560 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.c.ZooKeeperSaslClient [INFO] Client will use GSSAPI as SASL mechanism.
2018-02-27 08:18:43.561 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Opening socket connection to server openqe74blue-gw.blue.ygrid.yahoo.co
m/10.215.68.156:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-02-27 08:18:43.562 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Socket connection established to openqe74blue-gw.blue.ygrid.yahoo.com/1
0.215.68.156:2181, initiating session
2018-02-27 08:18:43.565 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Session establishment complete on server openqe74blue-gw.blue.ygrid.yah
oo.com/10.215.68.156:2181, sessionid = 0x161d5f1ae970099, negotiated timeout = 40000
2018-02-27 08:18:43.565 main-EventThread o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2018-02-27 08:18:43.605 Curator-Framework-0 o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
2018-02-27 08:18:43.625 main o.a.z.ZooKeeper [INFO] Session: 0x161d5f1ae970099 closed
2018-02-27 08:18:43.625 main-EventThread o.a.z.ClientCnxn [INFO] EventThread shut down
2018-02-27 08:18:43.626 main o.a.s.z.ClientZookeeper [INFO] Staring ZK Curator
2018-02-27 08:18:43.626 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
2018-02-27 08:18:43.635 main o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=openqe74blue-gw.blue.ygrid.yahoo.com:2181/storm_ystormQE_CI sessionTimeout=60000 w
atcher=org.apache.curator.ConnectionState@46cc127b
2018-02-27 08:18:43.654 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.c.ZooKeeperSaslClient [INFO] Client will use GSSAPI as SASL mechanism.
2018-02-27 08:18:43.660 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Opening socket connection to server openqe74blue-gw.blue.ygrid.yahoo.co
m/10.215.68.156:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-02-27 08:18:43.663 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Default schema
2018-02-27 08:18:43.663 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Socket connection established to openqe74blue-gw.blue.ygrid.yahoo.com/1
0.215.68.156:2181, initiating session
2018-02-27 08:18:43.666 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Session establishment complete on server openqe74blue-gw.blue.ygrid.yah
oo.com/10.215.68.156:2181, sessionid = 0x161d5f1ae97009a, negotiated timeout = 40000
2018-02-27 08:18:43.669 main-EventThread o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2018-02-27 08:18:43.790 main o.a.s.d.n.Nimbus [INFO] Starting nimbus server for storm version '2.0.0.y'
2018-02-27 08:18:44.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping assignments
2018-02-27 08:18:44.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping cleanup
2018-02-27 08:18:44.300 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518767144-stormcode.ser
2018-02-27 08:18:44.301 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: TestZkErrorNodesHaveCorrectAcls-3-1519540302-stormcode.ser
2018-02-27 08:18:44.302 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: logviewer-ui-groups-test-1-1518940914-stormcode.ser
2018-02-27 08:18:44.303 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518800831-stormcode.ser
2018-02-27 08:18:44.304 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518767144-stormconf.ser
2018-02-27 08:18:44.306 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: logviewer-ui-groups-test-1-1518940914-stormconf.ser
2018-02-27 08:18:44.307 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: TestZkErrorNodesHaveCorrectAcls-3-1519540302-stormconf.ser
2018-02-27 08:18:44.308 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518800831-stormconf.ser
2018-02-27 08:18:44.367 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping credential renewal.
2018-02-27 08:18:54.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping assignments
2018-02-27 08:18:54.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping cleanup
2018-02-27 08:18:59.367 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping credential renewal.
2018-02-27 08:19:02.059 main-EventThread o.a.s.z.Zookeeper [INFO] active-topology-blobs [] local-topology-blobs [] diff-topology-blobs []
2018-02-27 08:19:02.059 main-EventThread o.a.s.z.Zookeeper [INFO] active-topology-dependencies [] local-blobs [] diff-topology-dependencies []
2018-02-27 08:19:02.059 main-EventThread o.a.s.z.Zookeeper [INFO] Accepting leadership, all active topologies and corresponding dependencies found locally.
2018-02-27 08:19:04.754 timer o.a.s.d.n.Nimbus [INFO] Scheduling took 442 ms for 0 topologies
{code}
 

This can be re-produced by the following steps:
{code:java}
1. restart pacemaker;
2. before pacemaker is up, restart nimbus{code}
When we restart nimbus process,  it runs ShutDownHooks and stuck on timer.close(). 

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L4227]

The timer is not able to close because it's waiting for  doCleanup() to stop. However, the interruptedException is caught and ate in : [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java#L180-L192]

 

 

 

 "
STORM-2985,Add jackson-annotations to dependency management," 

We recently upgraded to jackson version 2.9.4. However different versions of jackson-annotation dependencies are inherited via transitive dependencies of other jars. Its best to keep it in sync."
STORM-2981,Upgrade Curator to lastest patch version,"Looks like the fix for https://issues.apache.org/jira/browse/STORM-2706 (the fix is https://issues.apache.org/jira/browse/CURATOR-436) is resolved in 4.0.1, not 4.0.0. We should upgrade."
STORM-2980,The storm-starter documentation incorrectly states that the examples can be run in a local cluster via a command line flag.,
STORM-2979,WorkerHooks EOFException during run_worker_shutdown_hooks,"Hi,

I'm trying to use the BaseWorkerHook but an exception is thrown after I killed the topology.

The issue is exactly the same as : [http://user.storm.apache.narkive.com/uchOrwlH/workerhook-deserialization-problem|http://user.storm.apache.narkive.com/uchOrwlH/workerhook-deserialization-problem]

An extract of my code :

 
{code:java}
// topology
final TridentTopology topology = new TridentTopology();
// ... I skip all the topology configuration part
final StormTopology topo = topology.build();

// hook
final BaseWorkerHook hook = new BaseWorkerHook();
final ByteBuffer serializedHook = ByteBuffer.wrap(Utils.javaSerialize(hook ));
topo.add_to_worker_hooks(hook);

// submit topology
LocalCluster cluster = new LocalCluster();
cluster.submitTopology(name,config,topo);
Utils.sleep(60000);

// kill topology
final KillOptions killOptions = new KillOptions();
killOptions.set_wait_secs(0);
cluster.killTopologyWithOpts(name, killOptions);
Utils.sleep(10000);
cluster.shutdown();
{code}
 

I have the following error :
{code:java}
java.lang.RuntimeException: java.io.EOFException
    at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:254)
    at org.apache.storm.daemon.worker$run_worker_shutdown_hooks$iter__5456__5460$fn__5461.invoke(worker.clj:578)
    at clojure.lang.LazySeq.sval(LazySeq.java:40)
    at clojure.lang.LazySeq.seq(LazySeq.java:49)
    at clojure.lang.RT.seq(RT.java:507)
    at clojure.core$seq__4128.invoke(core.clj:137)
    at clojure.core$dorun.invoke(core.clj:3009)
    at clojure.core$doall.invoke(core.clj:3025)
    at org.apache.storm.daemon.worker$run_worker_shutdown_hooks.invoke(worker.clj:576)
    at org.apache.storm.daemon.worker$fn__5471$exec_fn__1371__auto__$reify__5473$shutdown_STAR___5493.invoke(worker.clj:693)
    at org.apache.storm.daemon.worker$fn__5471$exec_fn__1371__auto__$reify$reify__5519.shutdown(worker.clj:706)
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:67)
    at org.apache.storm.daemon.supervisor.LocalContainer.kill(LocalContainer.java:59)
    at org.apache.storm.daemon.supervisor.Slot.killContainerForChangedAssignment(Slot.java:311)
    at org.apache.storm.daemon.supervisor.Slot.handleRunning(Slot.java:527)
    at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:265)
    at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:741)
Caused by: java.io.EOFException
    at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2680)
    at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3155)
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:864)
    at java.io.ObjectInputStream.<init>(ObjectInputStream.java:360)
    at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:245)
    ... 16 more
{code}
 

Maybe it is related to log4j shutdown hooks (https://issues.apache.org/jira/browse/STORM-2176) so I tried to disable the hook in my src/test/resources/log4j2.xml.

 
{code:java}
<Configuration monitorInterval=""60"" shutdownHook=""disable"">
    <Appenders>
        <Console name=""Console"" target=""SYSTEM_OUT"">
            <PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"" />
        </Console>
    </Appenders>

    <Loggers>
        <Root level=""debug"">
            <AppenderRef ref=""Console"" />
        </Root>
    </Loggers>
</Configuration>
{code}
But it does not change anything.

 

Of course the purpose of my work is to use my own worker hook extending the BaseWorkerHook.

 

 

 

 "
STORM-2978,"The fix for STORM-2706 is broken, and adds a transitive dependency on Zookeeper 3.5.3-beta for projects that depend on e.g. storm-kafka","Shinhyung Yang wrote on the mailing list:

{quote}I have been running the Yahoo streaming benchmarks on Storm 0.9.7 [...] With the introduction of Storm 1.2.0, I decided to upgrade from 0.9.7 to 1.2.0. Currently I'm testing Yahoo streaming benchmark's topology on the new setup and I end up getting the following exceptions:

[...]
Caused by: org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /ad-events/7183b5b2-4971-41a1-b86d-0788f646bc64/partition_0
[...]
{quote}

When fixing STORM-2706, I used the Storm parent's DependencyManagement section to force Zookeeper to version 3.4.6 everywhere in Storm. Sadly it turns out that this mechanism doesn't extend to external projects that depend on Storm components. While e.g. storm-kafka will use Zookeeper 3.4.6 when built as part of Storm, it will have a transitive dependency on Zookeeper 3.5.3-beta when an external project declares a dependency on storm-kafka.

A quick google indicates that the ""proper"" way to export the transitive dependency versions to downstream projects would be with a BOM pom, i.e. we'd create a separate BOM project for Storm that exports our DependencyManagement, and users would then import the BOM. I'm not sure if we want to do that on master, but since it is a breaking change I don't think we should do this on 1.x.

For 1.x (and maybe master?), we'll have to make sure that Curator dependencies always exclude Zookeeper, and all projects depending on Curator will have to explicitly declare the right Zookeeper dependency version."
STORM-2975,Worker died if KafkaSpout catched a kafka CommitFailedException,"org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:184) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:605) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1173) ~[stormjar.jar:?]
 at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:?]
 at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:220) ~[stormjar.jar:?]
 at org.apache.storm.daemon.executor$fn__4962$fn__4977$fn__5008.invoke(executor.clj:646) ~[storm-core-1.1.1.jar:1.1.1]
 at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.1.jar:1.1.1]
 at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]"
STORM-2974,Add a transactional non-opaque spout to storm-kafka-client,
STORM-2973,Replace storm-perf storm-kafka topologies with storm-kafka-client versions,
STORM-2972,Replace storm-kafka in storm-sql-kafka with storm-kafka-client,
STORM-2971,Replace Flux storm-kafka example with an equivalent example for storm-kafka-client,
STORM-2966,Nimbus fails to log errors when shutting down at startup,"While testing, I encountered a NoClassDefFoundError for Nimbus at startup.  This caused Nimbus to shutdown with 0 lines in the log file.

 

Adding test code to catch the exception, I was able to see the log messages and fix the issue (see callstack below).

 

We should be logging the error on shutdown for debugging.

 

2018-02-19 14:56:28.842 o.a.s.d.n.Nimbus main [ERROR] Failed to initialize metric store

org.apache.storm.metricstore.MetricException: Failed to create metric store

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:41) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1113) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1103) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1098) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.launchServer(Nimbus.java:1008) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1026) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1031) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/io/RawComparator

        at org.apache.storm.hbasemetricstore.HBaseStore.<clinit>(HBaseStore.java:87) ~[storm-hbasemetricstore-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]

        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        ... 6 more

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.RawComparator

        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_131]

        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_131]

        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) ~[?:1.8.0_131]

        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_131]

        at org.apache.storm.hbasemetricstore.HBaseStore.<clinit>(HBaseStore.java:87) ~[storm-hbasemetricstore-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]

        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        ... 6 more"
STORM-2965,Interpret wildcard in classpath correctly when reading config from classpath,"While reading configuration file from classpath at Utils.getConfigFromClasspath(), it doesn't interpret wildcard (\*) as Java classpath does. It should behave same as Java."
STORM-2963,Updates to Performance.md ,
STORM-2958,Use new wait strategies for Spout as well,"STORM-2306 introduced a new configurable wait strategy system for these situations
 * BackPressure Wait (used by spout & bolt)
 * No incoming data (used by bolt)

There is another wait situation in the spout when there are no emits generated in a nextTuple() or if max.spout.pending has been reached. This Jira is to transition the spout wait strategy from the old model to the new model. Thereby we have a uniform model for dealing with wait strategies."
STORM-2957,Offsets are not committed when no ackers set (ackers=0) and KafkaSpout stops polling messages once it reaches the max uncommitted offsets ,"A KafkaSpout stops polling messages from topic when topology has no ackers set (""topology.acker.executors""= 0). This happens after KafkaSpout polls (and emits) messages equal to the max uncommitted offsets (KafkaspoutConfig.maxUncommittedOffsets).

This happens because the KafkaSpout.ack() is called even before the emit method adds the message to ""emitted"" collection (Set). In such cases, the message is not added ""acked"" collection (Map), and hence never committed. This seem to be bug introduced in a version later than storm-kafka-client 1.0.1 and current code (1.0.5) has a check in ack() that checks if the acked message is available in ""emitted"" collection and if not, ignores it (never adds it to ""acked"" collection)

Steps to reproduce issue:
1. Need a topology with KafkaSpout from storm-kafka-client
2. Set number of ackers (""topology.acker.executors"") to o (org.apache.storm.Config.setNumAckers(0)
3. While creating KafkaSpout instance, set SpoutConfig.setMaxUncommittedOffsets to small number (50 or 100)
4. Start topology, see to it that KafkaSpout polls more messages from Kafka that setMaxUncommittedOffsets
5. KafkaSpout stops polling messages from topic"
STORM-2953,Remove storm-kafka in 2.0.0,Remove storm-kafka from master. See the thread at http://mail-archives.apache.org/mod_mbox/storm-dev/201802.mbox/%3CCAF5108hZBRf7Y%2BOFPif9kSw9%2BuZuPx5iBgSt1TOX5zMvF0-NfQ%40mail.gmail.com%3E
STORM-2952,Deprecate storm-kafka in 1.x,Add deprecation notice to storm-kafka for the 1.x branch. See the thread at http://mail-archives.apache.org/mod_mbox/storm-dev/201802.mbox/%3CCAF5108hZBRf7Y%2BOFPif9kSw9%2BuZuPx5iBgSt1TOX5zMvF0-NfQ%40mail.gmail.com%3E. 
STORM-2951,Storm binaries packages oncrpc jar which is LGPL ,"With the recent storm metrics changes storm packages oncrpc-1.0.7.jar which is LGPL licence.

 

[https://mvnrepository.com/artifact/org.acplt/oncrpc/1.0.7]

 

I am not sure if its ok to package libraries with LGPL license in storm distribution. 

 

Its coming from metrics-ganglia dependency in storm-core.

 [~ptgoetz], can you provide inputs ? If this needs to be excluded, I can craft a patch and push it."
STORM-2949,Download Storm release link in 1.0.x and 1.1.1 points to github(source code) rather than the storm.apache.org link. ,"I am following the documentation to [Setup a Storm cluster for 1.0.5|http://storm.apache.org/releases/1.0.4/Setting-up-a-Storm-cluster.html] and I have some comments/ errors to point out.
 
- The documentation only exists for 1.0.4. Replacing 1.0.4 in the url with 1.0.5 gives a 404 error.
 
- The section to [Download and extract a Storm release to Nimbus and worker machines|http://storm.apache.org/releases/1.0.4/Setting-up-a-Storm-cluster.html#download-and-extract-a-storm-release-to-nimbus-and-worker-machineshttp://storm.apache.org/releases/1.0.4/Setting-up-a-Storm-cluster.html%23install-dependencies-on-nimbus-and-worker-machines] provides a link to download Storm releases from GitHub. 
 * The tar.gz download from [github|https://github.com/apache/storm/releases/tag/v1.0.5] is 8.5MB and is labeled as ""*Source code* (tar.gz)"". This is confusing for a release download.
 * Trying to run nimbus from the extracted folder gives the following error, which is consistent with the above labeling. 
 * [vagrant@node1 storm-1.0.6]$ bin/storm nimbus

******************************************

The storm client can only be run from within a release. You appear to be trying to run the client from a checkout of Storm's source code.

 

You can download a Storm release at [http://storm.apache.org/downloads.html]

******************************************

This link is incorrect in documentation for v1.0.x and v1.1.x.
 My understanding is the documentation is pointing to the wrong link. This link should be updated to point to the [releases|http://storm.apache.org/downloads.html] from [http://storm.apache.org/] as mentioned in the error message. I was able to setup the storm cluster using this download path. "
STORM-2947,Review and fix/remove deprecated things in Storm 2.0.0,"We've been deprecating the things but haven't have time to replace/get rid of them. It should be better if we have time to review and address them.
"
STORM-2945,Nail down and document how to support background emits in Spouts and Bolts,
STORM-2943,Binary distribution includes storm-kafka-monitor source/javadoc in toollib directory,"Quoting Alexandre's RC3 vote:

 
{quote}I hate to be the one who always give bad news, but as a matter of
 facts, Storm 1.2.0 RC3 installation from binary artifacts (both
 apache-storm-1.2.0-src.tar.gz and apache-storm-1.2.0.zip) leads to ""by
 default KO Kafka monitor"" in Nimbus UI (which dirty exceptions in
 ui.log)

Here's for example what I get from apache-storm-1.2.0-src.tar.gz
 downloaded from
[https://dist.apache.org/repos/dist/dev/storm/apache-storm-1.2.0-rc3/apache-storm-1.2.0-src.tar.gz]:

$ tar ztvf apache-storm-1.2.0.tar.gz apache-storm-1.2.0/toollib
 -rwxrwxrwx ptgoetz/staff 16999 2018-02-06 21:22
 apache-storm-1.2.0/toollib/storm-kafka-monitor-1.2.0-sources.jar
 -rwxrwxrwx ptgoetz/staff 93461 2018-02-06 21:22
 apache-storm-1.2.0/toollib/storm-kafka-monitor-1.2.0-javadoc.jar
 -rwxrwxrwx ptgoetz/staff 21591320 2018-02-06 21:22
 apache-storm-1.2.0/toollib/storm-kafka-monitor-1.2.0.jar

And here's what I see in ui.log:

 org.apache.storm.kafka.spout.KafkaSpout
 2018-02-07 16:49:57.153 o.a.s.u.TopologySpoutLag qtp1997623038-18
[WARN] Exception message:Error: Could not find or load main class
 .usr.local.Storm.storm-stable.toollib.storm-kafka-monitor-1.2.0-javadoc.jar

org.apache.storm.utils.ShellUtils$ExitCodeException: Error: Could not
 find or load main class
 .usr.local.Storm.storm-stable.toollib.storm-kafka-monitor-1.2.0-javadoc.jar

        at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:231)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:161)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils$ShellCommandExecutor.execute(ShellUtils.java:371)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils.execCommand(ShellUtils.java:461)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils.execCommand(ShellUtils.java:444)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.TopologySpoutLag.getLagResultForKafka(TopologySpoutLag.java:163)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.TopologySpoutLag.getLagResultForNewKafkaSpout(TopologySpoutLag.java:189)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.TopologySpoutLag.lag(TopologySpoutLag.java:57)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.ui.core$topology_lag.invoke(core.clj:805)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.ui.core$fn__9586.invoke(core.clj:1165)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$make_route$fn__5979.invoke(core.clj:100)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$if_route$fn__5967.invoke(core.clj:46)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$if_method$fn__5960.invoke(core.clj:31)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$routing$fn__5985.invoke(core.clj:113)
 ~[storm-core-1.2.0.jar:1.2.0]
         at clojure.core$some.invoke(core.clj:2570) ~[clojure-1.7.0.jar:?]
         at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
 ~[storm-core-1.2.0.jar:1.2.0]
         at clojure.lang.RestFn.applyTo(RestFn.java:139) ~[clojure-1.7.0.jar:?]
         at clojure.core$apply.invoke(core.clj:632) ~[clojure-1.7.0.jar:?]
         at org.apache.storm.shade.compojure.core$routes$fn__5989.invoke(core.clj:118)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.cors$wrap_cors$fn__8894.invoke(cors.clj:149)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__8841.invoke(json.clj:56)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6621.invoke(multipart_params.clj:118)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__7904.invoke(reload.clj:22)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.ui.helpers$requests_middleware$fn__6874.invoke(helpers.clj:52)
 ~[storm-core-1.2.0.jar:1.2.0]

Deleting the extraneous storm-kafka-monitor-1.2.0-sources.jar and
 storm-kafka-monitor-1.2.0-javadoc.jar file, then restarting Nimbus
 solves the issue.

However, binaries artifacts should be as clean as possible, isn't it ?
{quote}
 "
STORM-2942,Remove javadoc and source jars from toollib directory in binary distribution,Need to update the assembly to only include the classes jar.
STORM-2941,checkstyle failing on master,Got some checkstyle failures and build will not pass
STORM-2939,Create interface for processing worker metrics,"In Container.java, we send worker metrics to Nimbus to store to RocksDB.  Other implementations (HBase, etc) may want to process in different fashions.  "
STORM-2937,Overwrite storm-kafka-client 1.x-branch into 1.0.x-branch,"This is to track the effort of syncing up divergence between storm-kafka-client 1.x-branch and 1.0.x-branch so that critical fixes can be go in 1.0.x-branch as well.

Note that it can modify storm-core as well (unlikely in a backwards-incompatible way but not 100% sure), so we should make a decision whether we allow the change in bugfix version line.

Linking discussion thread:

[https://lists.apache.org/thread.html/0451fed132bb982b618d9e0780282a87554f1bc5747827599f276944@%3Cdev.storm.apache.org%3E]"
STORM-2936,Overwrite storm-kafka-client 1.x-branch into 1.1.x-branch,"This is to track the effort of syncing up divergence between storm-kafka-client 1.x-branch and 1.1.x-branch so that critical fixes can be go in 1.1.x-branch as well.

Linking discussion thread:

[https://lists.apache.org/thread.html/0451fed132bb982b618d9e0780282a87554f1bc5747827599f276944@%3Cdev.storm.apache.org%3E]

 "
STORM-2934,Logviewer ClassNotFoundException validating configs,"Caused by: java.lang.ClassNotFoundException: org.rocksdb.RocksDBException
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_131]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_131]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) ~[?:1.8.0_131]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_131]
        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]
        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]
        at org.apache.storm.validation.ConfigValidation$ImplementsClassValidator.validateField(ConfigValidation.java:587) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.validation.ConfigValidation.validateField(ConfigValidation.java:758) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.validation.ConfigValidation.validateFields(ConfigValidation.java:811) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.validation.ConfigValidation.validateFields(ConfigValidation.java:772) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.ConfigUtils.readStormConfigImpl(ConfigUtils.java:397) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.ConfigUtils.readStormConfig(ConfigUtils.java:136) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.logviewer.LogviewerServer.main(LogviewerServer.java:158) ~[storm-webapp-2.0.0.y.jar:2.0.0.y]"
STORM-2933,"Add a storm-perf topology that uses storm-kafka-client, so we can benchmark that module",
STORM-2928,(For 2.0.0) In a secure cluster with storm-autocreds enabled storm-druid can fail with NoSuchMethodError,"storm-autocreds brings in the curator 4.0 via transitive dependency of storm-core, even though storm-core is listed as provided scope, the app assembler plugin puts the dependency (curator 4.0) into external/storm-autocreds directory. This conflicts with the storm-druid tranquility library which depends on curator 2.6.0
2018-01-22 08:43:54.047 o.a.s.d.executor Thread-15-54-Dashboard-Violation-Predicted-executor[20 20] [ERROR]
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:125)"
STORM-2927,"Storm supervisor displaying message ""kill: sending signal to 23543 failed: No such process""","I started storm cluster and all one nimbus and two supervisors started fine and I could view them fine on ""Storm UI""  then I deployed example topology ""storm-opentsdb-2.0.0-SNAPSHOT.jar"", it is successfully deployed on the cluster but after few minutes supervisor console displays message and no data is inserted in Opentsdb.
kill: sending signal to 23543 failed: No such process
kill: sending signal to 23615 failed: No such process
kill: sending signal to 23612 failed: No such process
kill: sending signal to 23706 failed: No such process
kill: sending signal to 23776 failed: No such process.

The same topology run perfectly fine in Local Cluster mode and inserts data in Opentsdb.

To resolve this issue I killed the toplogy, killed the nimbus and supervisor nodes. I manuall deleted files under ""storm.local.dir: ""/tmp/storm-data"""" and also deleted the files in zookeeper znode /storm. Again started the cluster and deployed the same topology but got the same error.


"
STORM-2926,org.apache.storm.st.tests.window.SlidingWindowTest.testWindowCount in integration test fails intermittently,"Lost a build link... From what I've seen is, one of 13th trials in test had failed. So not often, but really intermittent."
STORM-2925,org.apache.storm.sql.TestStormSql consistently fails,"1. java.lang.RuntimeException: java.lang.IllegalStateException: It took over 60000ms to shut down slot Thread[SLOT_1024,5,main]

[https://travis-ci.org/apache/storm/jobs/335344937]

 

2. [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project storm-sql-core: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?

[https://travis-ci.org/apache/storm/jobs/334308676]

 

I've skimmed a bit and looks like twos are same issue."
STORM-2924,testTickTupleWorksWithSystemBolt in org.apache.storm.TickTupleTest fails intermittently,"java.lang.AssertionError: took over 110000 ms of simulated time to get a message back...

[https://travis-ci.org/apache/storm/jobs/335075657]"
STORM-2923,org.apache.storm.st.tests.window.SlidingWindowTest in integration test fails intermittently with 1.x version lines,"Unfortunately lost the build link... The following message was presented:

java.lang.RuntimeException: Failed to kill topology SlidingWindowTest-window200-slide100. Subsequent tests may fail because worker slots are occupied"
STORM-2922,org.apache.storm.hdfs.spout.TestDirLock is consistently stuck with JDK 7 in Travis CI,"[https://travis-ci.org/apache/storm/jobs/333477869]

 "
STORM-2921,integration.org.apache.storm.testing4j-test fails intermittently on 1.x version line,[https://travis-ci.org/apache/storm/jobs/332445614]
STORM-2920,org.apache.storm.trident.tuple-test fails intermittently on 1.x version line,[https://travis-ci.org/apache/storm/jobs/332147829]
STORM-2919,integration.org.apache.storm.integration-test fails intermittently on 1.x version line,"testname ""test-submit-inactive-topology"" failed occasionally: I've seen various kinds of failures, refer the links to see detail:

[https://travis-ci.org/apache/storm/jobs/333477868]

[https://travis-ci.org/apache/storm/jobs/333477867]"
STORM-2918,Upgrade Netty version,"netty 3.9.0 has been out since June 2014, netty 3.9.9 has been released in July 2015. On top of it, there are two known CVEs for netty below 3.9.2
CVE-20140193 [https://www.us-cert.gov/ncas/bulletins/SB14-132]
CVE-20143488 [https://www.cvedetails.com/cve/CVE-2014-3488/]"
STORM-2915,How could I to get the fail Number   in Bolt When I use  Kafka Spout,"I want to get fail num in bolt , how could  I  to get it? 

if  fail it  retry, I see This 

if (!isScheduled || retryService.isReady(msgId)) {
 final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;

 if (!isAtLeastOnceProcessing()) {
 if (kafkaSpoutConfig.isTupleTrackingEnforced()) {
 collector.emit(stream, tuple, msgId);
 LOG.trace(""Emitted tuple [{}] for record [{}] with msgId [{}]"", tuple, record, msgId);
 } else {
 collector.emit(stream, tuple);
 LOG.trace(""Emitted tuple [{}] for record [{}]"", tuple, record);
 }
 } else {
 emitted.add(msgId);
 offsetManagers.get(tp).addToEmitMsgs(msgId.offset());
 if (isScheduled) { // Was scheduled for retry and re-emitted, so remove from schedule.
 retryService.remove(msgId);
 }
 collector.emit(stream, tuple, msgId);
 tupleListener.onEmit(tuple, msgId);
 LOG.trace(""Emitted tuple [{}] for record [{}] with msgId [{}]"", tuple, record, msgId);
 }
 return true;
}"
STORM-2914,Remove enable.auto.commit support from storm-kafka-client,"The enable.auto.commit option causes the KafkaConsumer to periodically commit the latest offsets it has returned from poll(). It is convenient for use cases where messages are polled from Kafka and processed synchronously, in a loop. 

Due to https://issues.apache.org/jira/browse/STORM-2913 we'd really like to store some metadata in Kafka when the spout commits. This is not possible with enable.auto.commit. I took at look at what that setting actually does, and it just causes the KafkaConsumer to call commitAsync during poll (and during a few other operations, e.g. close and assign) with some interval. 

Ideally I'd like to get rid of ProcessingGuarantee.NONE, since I think ProcessingGuarantee.AT_MOST_ONCE covers the same use cases, and is likely almost as fast. The primary difference between them is that AT_MOST_ONCE commits synchronously.

If we really want to keep ProcessingGuarantee.NONE, I think we should make our ProcessingGuarantee.NONE setting cause the spout to call commitAsync after poll, and never use the enable.auto.commit option. This allows us to include metadata in the commit."
STORM-2913,"STORM-2844 made autocommit and at-most-once storm-kafka-client spouts log warnings on every emit, because those modes don't commit the right metadata to Kafka","The mechanism added in https://issues.apache.org/jira/browse/STORM-2844 to allow us to check whether a committed offset was committed by the currently running topology requires that we commit some metadata along with the offset.

We are using this metadata for two things: Only applying the FirstPollOffsetStrategy when the topology is deployed, rather than when the worker is restarted, and an (IMO fairly unimportant) runtime check that the spout offset tracking is not in a bad state.

Autocommit spouts don't include this metadata, and we also don't include it when committing offsets in at-most-once mode. We can fix at-most-once by switching to committing a custom OffsetAndMetadata, rather than using the no-arg commitSync variant. 

I'm not sure what we should do to fix the autocommit case. There doesn't seem to be a way to include metadata in autocommits, so I don't think we can support this mechanism for autocommits. 

If we can't fix the autocommit case, I see two options for fixing this:
* Make doSeek have the old behavior for autocommits only (i.e. apply the FirstPollOffsetStrategy on every worker restart), and keep the new behavior for at-least-once/at-most-once. I think this behavior could be a little confusing.
* Revert doSeek to the old behavior in all cases, and throw out the runtime check that uses the metadata. This also isn't a great option, because the new seek behavior is more useful than restarting on every worker reboot.

What do you think [~hmclouro]? I'm leaning toward the first option."
STORM-2912,Tick tuple is being shared without resetting start time and incur side-effect to break metrics,"In STORM-2786 we have applied small optimization: create tick tuple only once and reuse. The optimization completely makes sense, but when measuring built-in metrics, when reused tick tuple is selected for sampling, we never reset start time unless it is selected for sampling again, hence further tick tuple is always considered as sampled with start time unchanged.

What I've observed is that delta for tick tuple is gradually increasing for some time-period, and reset to 0, which messes up latencies. It also messes up executed as well because it is always considered as selected tuple (hence recorded to 20x for tick tuple), but unless interval of tick tuple is super small, the effect is much smaller then latency.

 

Here's part of log denoting this issue. Please take a look at DELTA values, which shouldn't be such huge.
{code:java}
2018-01-25 13:34:41.464 o.a.s.d.executor Thread-14-__acker-executor[1 1] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [30] TASK: 1 DELTA: 0
2018-01-25 13:34:41.658 o.a.s.d.executor Thread-12-counter-executor[3 3] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 3 DELTA: 87083
2018-01-25 13:34:41.658 o.a.s.d.executor Thread-8-counter-executor[2 2] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 2 DELTA: 6003
2018-01-25 13:34:41.728 o.a.s.d.executor Thread-26-counter-executor[5 5] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 5 DELTA: 30036
2018-01-25 13:34:41.728 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 4001
2018-01-25 13:34:41.729 o.a.s.d.executor Thread-32-counter-executor[4 4] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 4 DELTA: 156155
2018-01-25 13:34:41.813 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 24043
2018-01-25 13:34:41.813 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 14025
2018-01-25 13:34:41.813 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 52091
2018-01-25 13:34:41.886 o.a.s.d.executor Thread-28-intermediateRanker-executor[10 10] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 10 DELTA: 18025
2018-01-25 13:34:43.731 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 6004
2018-01-25 13:34:43.817 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 26047
2018-01-25 13:34:43.817 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 16029
2018-01-25 13:34:43.817 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 1
2018-01-25 13:34:43.890 o.a.s.d.executor Thread-28-intermediateRanker-executor[10 10] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 10 DELTA: 20029
2018-01-25 13:34:44.661 o.a.s.d.executor Thread-12-counter-executor[3 3] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 3 DELTA: 90086
2018-01-25 13:34:44.662 o.a.s.d.executor Thread-8-counter-executor[2 2] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 2 DELTA: 9007
2018-01-25 13:34:44.734 o.a.s.d.executor Thread-26-counter-executor[5 5] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 5 DELTA: 33042
2018-01-25 13:34:44.734 o.a.s.d.executor Thread-32-counter-executor[4 4] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 4 DELTA: 159160
2018-01-25 13:34:45.735 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 8008
2018-01-25 13:34:45.820 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 2004
2018-01-25 13:34:45.821 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 28051
2018-01-25 13:34:45.821 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 18033
2018-01-25 13:34:45.892 o.a.s.d.executor Thread-28-intermediateRanker-executor[10 10] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 10 DELTA: 22031
2018-01-25 13:34:47.663 o.a.s.d.executor Thread-12-counter-executor[3 3] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 3 DELTA: 93088
2018-01-25 13:34:47.664 o.a.s.d.executor Thread-8-counter-executor[2 2] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 2 DELTA: 12009
2018-01-25 13:34:47.734 o.a.s.d.executor Thread-32-counter-executor[4 4] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 4 DELTA: 162160
2018-01-25 13:34:47.736 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 10009
2018-01-25 13:34:47.736 o.a.s.d.executor Thread-26-counter-executor[5 5] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 5 DELTA: 36044
2018-01-25 13:34:47.825 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 20037
2018-01-25 13:34:47.826 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 30056
2018-01-25 13:34:47.826 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 4010{code}"
STORM-2911,SpoutConfig is serializable but does not declare a serialVersionUID field,See https://stackoverflow.com/questions/48420085/kafkaspout-is-not-emmiting-message
STORM-2910,Tests taking a long time to run,"The test for me went form 17 mins to run everything up to 30 mins to run everything.

I did some debugging and found that the slot is not very responsive if it cannot connect to nimbus (to send metrics back).  In those cases the connection has to time out before the slot can respond.

I think we want to do 2 things here.  First we don't want slot blocking for long periods of time if it cannot talk to nimbus, so I am going to background the sync, with a queue that will throw out old metrics if the new ones want to be sent.

Next I am going to figure out why the local override is not happening in the nimbus client in this situation."
STORM-2909,New Metrics Reporting API - for 2.0.0,"This is a proposal to provide a new metrics reporting API based on [Coda Hale's metrics library | http://metrics.dropwizard.io/3.1.0/] (AKA Dropwizard/Yammer metrics).

h2. Background
In a [discussion on the dev@ mailing list | http://mail-archives.apache.org/mod_mbox/storm-dev/201610.mbox/%3cCAGX0URh85NfH0Pbph11PMc1oof6HTycjCXSxgwP2nnofuKq0pQ@mail.gmail.com%3e]  a number of community and PMC members recommended replacing Storm’s metrics system with a new API as opposed to enhancing the existing metrics system. Some of the objections to the existing metrics API include:

# Metrics are reported as an untyped Java object, making it very difficult to reason about how to report it (e.g. is it a gauge, a counter, etc.?)
# It is difficult to determine if metrics coming into the consumer are pre-aggregated or not.
# Storm’s metrics collection occurs through a specialized bolt, which in addition to potentially affecting system performance, complicates certain types of aggregation when the parallelism of that bolt is greater than one.


In the discussion on the developer mailing list, there is growing consensus for replacing Storm’s metrics API with a new API based on Coda Hale’s metrics library. This approach has the following benefits:

# Coda Hale’s metrics library is very stable, performant, well thought out, and widely adopted among open source projects (e.g. Kafka).
# The metrics library provides many existing metric types: Meters, Gauges, Counters, Histograms, and more.
# The library has a pluggable “reporter” API for publishing metrics to various systems, with existing implementations for: JMX, console, CSV, SLF4J, Graphite, Ganglia.
# Reporters are straightforward to implement, and can be reused by any project that uses the metrics library (i.e. would have broader application outside of Storm)

As noted earlier, the metrics library supports pluggable reporters for sending metrics data to other systems, and implementing a reporter is fairly straightforward (an example reporter implementation can be found here). For example if someone develops a reporter based on Coda Hale’s metrics, it could not only be used for pushing Storm metrics, but also for any system that used the metrics library, such as Kafka.

h2. Scope of Effort
The effort to implement a new metrics API for Storm can be broken down into the following development areas:

# Implement API for Storms internal worker metrics: latencies, queue sizes, capacity, etc.
# Implement API for user defined, topology-specific metrics (exposed via the {{org.apache.storm.task.TopologyContext}} class)
# Implement API for storm daemons: nimbus, supervisor, etc.

h2. Relationship to Existing Metrics
This would be a new API that would not affect the existing metrics API. Upon completion, the old metrics API would presumably be deprecated, but kept in place for backward compatibility.

Internally the current metrics API uses Storm bolts for the reporting mechanism. The proposed metrics API would not depend on any of Storm's messaging capabilities and instead use the [metrics library's built-in reporter mechanism | http://metrics.dropwizard.io/3.1.0/manual/core/#man-core-reporters]. This would allow users to use existing {{Reporter}} implementations which are not Storm-specific, and would simplify the process of collecting metrics. Compared to Storm's {{IMetricCollector}} interface, implementing a reporter for the metrics library is much more straightforward (an example can be found [here | https://github.com/dropwizard/metrics/blob/3.2-development/metrics-core/src/main/java/com/codahale/metrics/ConsoleReporter.java].

The new metrics capability would not use or affect the ZooKeeper-based metrics used by Storm UI.

h2. Relationship to JStorm Metrics
[TBD]

h2. Target Branches
[TBD]

h2. Performance Implications
[TBD]

h2. Metrics Namespaces
[TBD]

h2. Metrics Collected
*Worker*
|| Namespace || Metric Type || Description ||

*Nimbus*
|| Namespace || Metric Type || Description ||

*Supervisor*
|| Namespace || Metric Type || Description ||

h2. User-Defined Metrics
[TBD]
"
STORM-2908,Document Metrics V2 for 2.0.0,"This issue is to track follow-up task on Metrics V2: documentation.

The purpose for 'blocker' is to ensure this to be finished before releasing Storm 2.0.0."
STORM-2907,In a secure cluster with storm-autocreds enabled storm-druid can fail with NoSuchMethodError,"storm-autocreds brings in the curator 4.0 via transitive dependency of storm-core, even though storm-core is listed as provided scope, the app assembler plugin puts the dependency (curator 4.0) into external/storm-autocreds directory. This conflicts with the storm-druid tranquility library which depends on curator 2.6.0
2018-01-22 08:43:54.047 o.a.s.d.executor Thread-15-54-Dashboard-Violation-Predicted-executor[20 20] [ERROR]
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:125)"
STORM-2905,Supervisor still downloads storm blob files when the topology was killed.,"When we kill a topology, at the moment of topology blob-files be removed, Supervisor executor still request blob-files and get an KeyNotFoundException.

I stepped in and found the reason:
1. We do not add a guarded lock on `topologyBlobs` of AsyncLocalizer which is singleton to a supervisor node.
2. And we remove jar/code/conf blob keys in `topologyBlobs` of killed storm only in a timer task: cleanUp() method of AsyncLocalizer, the remove condition is :[no one reference the blobs] AND [ blobs removed by master OR exceeds the max configured size ], the default scheduling interval is 30 seconds.
3. When we kill a storm on a node[ which means that the slot container are empty], the AsyncLocalizer will do: releaseSlotFor, which only remove reference on the blobs [topologyBlobs keys are still there.]
4. Then the container is empty, and Slot.java will do: cleanupCurrentContainer, which will invoke AsyncLocalizer #releaseSlotFor to release the slot.
5. AsyncLocalizer have a timer task: updateBlobs to update base/user blobs every 30 seconds, which based on the AsyncLocalizer#`topologyBlobs`
6. We know that AsyncLocalizer#`topologyBlobs` overdue keys are only removed by its AsyncLocalizer#cleanUp which is also a timer task.
7. So when we kill a storm, AsyncLocalizer#updateBlobs will update based on a removed jar/code/conf blob-key and fire a exception, then retried until the configured max times to end.

Here is how i fixed it:
1. just remove the base blob keys eagerly when we do AsyncLocalizer #releaseSlotFor when there is no reference [no one used] on the blobs, and remove the overdue keys in AsyncLocalizer#`topologyBlobs`
2. Guard the AsyncLocalizer#updateBlobs and AsyncLocalizer #releaseSlotFor on the same lock.
3. When container is empty, we do not need to exec AsyncLocalizer #releaseSlotFor[because we have already deleted them].
4. I also add a new RPC api for decide if there exists a remote blob, we can use it to decide it the blob could be removed instead of use getBlobMeta and catch an confusing KeyNotFoundException [both on supervisors and master log for every base blobs].

This is the partial of Supervisor log:

2018-03-31 13:41:17.089 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/pids/1115
 2018-03-31 13:41:17.090 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/heartbeats
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/pids
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/tmp
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.103 o.a.s.d.s.Container SLOT_6700 [INFO] REMOVE worker-user b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.103 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers-users/b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.104 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Removed Worker ID b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.105 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE KILL msInState: 25 topo:word_count_fk_11-2-1522472558 worker:null -&gt; EMPTY msInState: 0
 2018-03-31 13:41:17.105 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Changing current assignment from LocalAssignment(topology_id:word_count_fk_11-2-1522472558, executors:[ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:6, task_end:6), ExecutorInfo(task_start:5, task_end:5), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:3, task_end:3), ExecutorInfo(task_start:2, task_end:2), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:896.0, mem_off_heap:0.0, cpu:70.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=896.0, cpu.pcore.percent=70.0}, shared_resources:{}), owner:danny0405) to null
 2018-03-31 13:41:18.001 o.a.s.d.s.t.SupervisorHealthCheck timer [INFO] Running supervisor healthchecks...
 2018-03-31 13:41:18.002 o.a.s.h.HealthChecker timer [INFO] The supervisor healthchecks succeeded.
 2018-03-31 13:41:36.837 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 1 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_JAR word_count_fk_11-2-1522472558 will try again in 100 ms
 org.apache.storm.generated.KeyNotFoundException: null
         at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25225) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25193) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25124) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[libthrift-0.9.3.jar:0.9.3]
         at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:825) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:812) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:318) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:176) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$5(AsyncLocalizer.java:249) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) [?:1.8.0_151]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_151]
         at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_151]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_151]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_151]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_151]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_151]
         at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]"
STORM-2904,Document Metrics V2,"This issue is to track follow-up task on Metrics V2: documentation.

The purpose for 'blocker' is to ensure this to be finished before releasing Storm 1.2.0."
STORM-2903,Fix possible NullPointerException in AbstractAutoCreds,"Observed below exception while testing Hive token mechanism.

    ```
    Caused by: java.lang.NullPointerException
            at org.apache.storm.common.AbstractAutoCreds.addTokensToUGI(AbstractAutoCreds.java:219) ~[storm-autocreds-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]
            at org.apache.storm.common.AbstractAutoCreds.populateSubject(AbstractAutoCreds.java:118) ~[storm-autocreds-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]
            at org.apache.storm.security.auth.AuthUtils.populateSubject(AuthUtils.java:228) ~[storm-core-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]
            ... 10 more
    2018-01-19 16:23:26.157 o.a.s.util main [ERROR] Halting process: (""Error on initialization"")
    ```"
STORM-2902,Some improvements for storm-rocketmq module,"* Upgraded RocketMQ version to 4.2.0 which brings improvements and new features like batch sending
* Imporved retry policy for RocketMQ consumer push mode to avoid data loss in some scenes
* Batch sending supported for bolt and trident state
* Allow running several consumer instances in one process, that is to say, different topics in one worker is possible

 "
STORM-2901,Reuse ZK connection for getKeySequenceNumber,"Now when our nimbus restarts, many zookeeper connections will be made in minutes, and it's really a pressure for our zookeeper server.

I checkout the log and code to find that when nimbus restart, in order to sync local storm keys[ actually valid storms ], it will:
 # check storm keys diff of local storm and zk remote.
 # set up path for all the valid storm keys with a keySequenceNumber.
 # In order to get the keySequenceNumber, now blobstore will make a new zk-client and connect to zk-server.

This is the reason why thousands of connections are made. For our cluster, there are about 800+ topologies running, which means that at least 800 connections will be made which totally can be reused.

 

This is part of nimbus re-starting log:

2018-01-18 12:51:57.031 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
 2018-01-18 12:51:57.032 o.a.s.s.o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=dx-data-rt-zk01:2181,dx-data-rt-zk02:2181,dx-data-rt-zk04:2181/mtstorm_101_dx_storm01 sessionTimeout=30000 watcher=org.apache.storm.shade.org.apache.curator.ConnectionState@76513a57
 2018-01-18 12:51:57.032 o.a.s.s.o.a.z.ClientCnxn [INFO] Opening socket connection to server dx-data-rt-zk04.dx.sankuai.com/10.32.157.254:2181. Will not attempt to authenticate using SASL (unknown error)
 2018-01-18 12:51:57.033 o.a.s.s.o.a.z.ClientCnxn [INFO] Socket connection established to dx-data-rt-zk04.dx.sankuai.com/10.32.157.254:2181, initiating session
 2018-01-18 12:51:57.034 o.a.s.s.o.a.z.ClientCnxn [INFO] Session establishment complete on server dx-data-rt-zk04.dx.sankuai.com/10.32.157.254:2181, sessionid = 0x45cd92f0cc7e938, negotiated timeout = 30000
 2018-01-18 12:51:57.034 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
 2018-01-18 12:51:57.037 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
 2018-01-18 12:51:57.039 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x45cd92f0cc7e938 closed
 2018-01-18 12:51:57.039 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
 2018-01-18 12:51:57.040 o.a.s.cluster [INFO] setup-path/blobstore/app_waimairank_wm_recsys_user_block-4-1504509174-stormconf.ser/dx-data-rt-nimbus05.dx.sankuai.com:9827-1
 2018-01-18 12:51:57.051 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
 2018-01-18 12:51:57.051 o.a.s.s.o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=dx-data-rt-zk01:2181,dx-data-rt-zk02:2181,dx-data-rt-zk04:2181/mtstorm_101_dx_storm01 sessionTimeout=30000 watcher=org.apache.storm.shade.org.apache.curator.ConnectionState@69c222d6
 2018-01-18 12:51:57.052 o.a.s.s.o.a.z.ClientCnxn [INFO] Opening socket connection to server dx-data-rt-zk02.dx.sankuai.com/10.32.108.46:2181. Will not attempt to authenticate using SASL (unknown error)
 2018-01-18 12:51:57.053 o.a.s.s.o.a.z.ClientCnxn [INFO] Socket connection established to dx-data-rt-zk02.dx.sankuai.com/10.32.108.46:2181, initiating session
 2018-01-18 12:51:57.055 o.a.s.s.o.a.z.ClientCnxn [INFO] Session establishment complete on server dx-data-rt-zk02.dx.sankuai.com/10.32.108.46:2181, sessionid = 0x25cd386f245eb72, negotiated timeout = 30000
 2018-01-18 12:51:57.055 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
 2018-01-18 12:51:57.058 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
 2018-01-18 12:51:57.061 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x25cd386f245eb72 closed
 2018-01-18 12:51:57.061 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
 2018-01-18 12:51:57.061 o.a.s.cluster [INFO] setup-path/blobstore/app_waimairank_waimai_rank_rt_pipeline_user_feature-12-1507516853-stormconf.ser/dx-data-rt-nimbus05.dx.sankuai.com:9827-1"
STORM-2900,Subject is not populated and NPE is thrown while populating credentials in nimbus.,"Nimbus Auto Creds[1.x, 2.0] may get into NPE while populating credentials when there is no config for the given key.

1.x - [https://github.com/apache/storm/blob/1.x-branch/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java#L75]
 2.0 - [https://github.com/apache/storm/blob/master/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java#L55]"
STORM-2899,Remove/replace the contributors lists in the README and on https://storm.apache.org/contribute/People.html,"We've talked a few times on the mailing list about removing or replacing the contributors lists, because they are not really being maintained because keeping them up to date is a manual process.

The last time this was discussed http://mail-archives.apache.org/mod_mbox/storm-dev/201712.mbox/browser, there seemed to be some consensus for using Spark's scripted solution to generate the contributors list. It was also suggested that having the contributors list at all was maybe unnecessary.

In order to avoid having this discussion again in a few months, I'll submit a PR that removes the contributors list from the README and site, because it's the simplest solution in terms of release process (no extra step to generate the contributors list).

If there are strong feelings for using Spark's solution to generate a contributors list instead, people will hopefully chime in on the PR review, and we can make changes as necessary."
STORM-2898,Storm should support auth through delegation tokens for workers,"There are a lot of cases where it would be great for a worker to be able to communicate directly to nimbus, supervisors, or drpc servers in a secure way out of the box.

This is currently a pain to make work.  The user has to ship a TGT with their topology, and continually keep it up to date with credentials-push.  They also need a kind of hacked up jaas.conf to grab the TGT from AutoTGT and put it in the place that he client wants it.

We should just generate a signed data structure (aka delegation token from hadoop) that we can had off to the topologies to use when talking to nimbus, a supervisor, or drpc servers.

We may want to split up the different services from each other to make an attack against one not hit all of them, but that is something we can think about with the design of this.

I will try to come up with a design shortly.
"
STORM-2896,Support automatic migration of offsets from storm-kafka to storm-kafka-client KafkaSpout,"I think we can ease migration for people looking to move from storm-kafka to storm-kafka-client. We should be able to support migrating offsets from the old spout by setting some extra configuration in KafkaSpoutConfig, and by adding a new FirstPollOffsetStrategy (e.g. something like FirstPollOffsetStrategy.UNCOMMITTED_MIGRATE_FROM_STORM_KAFKA).

The old spout stores offsets in Storm's Zookeeper at one of two paths. The storm-kafka SpoutConfig has two parameters we'll also need, namely zkRoot and id. In addition we need to know if the storm-kafka subscription was a wildcard subscription or not.

The zookeeper path for commit info is 
{code}
zkRoot + ""/"" + id + ""/"" + topicName + ""partition_"" + partition
{code}
if the subscription was a wildcard. Otherwise it is 
{code}
zkRoot + ""/"" + id + ""/"" + ""partition_"" + partition
{code}

We can get topicName and partition numbers from the KafkaConsumer.assignment. When we run initialize, we should be able to read the old offset structure from Zookeeper when the strategy is UNCOMMITTED_MIGRATE_FROM_STORM_KAFKA, and seek the consumer to those offsets. We can just crash if the offsets are not present.

I'd be okay with this feature not being permanent, but I think this feature would make it a lot easier for people to move off the old spout."
STORM-2894,fix some random typos in tests,"* MultilangEnvirontmentTest
** There is no ""t"" between n and m in Environment.
* getOffsetFromConfigAndFroceFromStart
** The r and o are transposed in Force."
STORM-2893,maven-assembly-plugin upgrade is breaking distribution build,"{code: title=failure}
% mvn clean package && cd storm-dist/binary && mvn -Dgpg.skip=true install
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single (default) on project final-package: Execution default of goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single failed: group id '906175167' is too big ( > 2097151 ). Use STAR or POSIX extensions to overcome this limit -> [Help 1]
{code}

This is a [known issue with the maven-assembly-plugin version 2.5+|https://stackoverflow.com/a/30246880/318428].

Prior to [a change a few months ago|https://github.com/apache/storm/commit/84a4314d96b9e4e377a3d5d81d0a042d96a0625e], the maven-assembly-plugin was [version 2.2.2|https://github.com/apache/storm/blob/de1d468592b402fb862f98ce357cbd1268d13f8c/pom.xml#L1195-L1199], now it's 2.6, as inherited from the [apache pom version 18|https://github.com/apache/maven-pom/blob/apache-18/pom.xml#L119-L123]."
STORM-2892,Flux test fails to parse valid PATH environment variable,"The flux tests rely on substituting the {{PATH}} environment variable into the [{{substitution-test.yaml}}|https://github.com/apache/storm/blob/466a7ad74da27c1250eedf412a487db409e42c19/flux/flux-core/src/test/resources/configs/substitution-test.yaml#L44-L45] file.

I noticed that the tests were failing when my {{PATH}} had a trailing colon, despite that being a valid {{PATH}} \[1].

h2. Existing error

{code: title=mvn test output}
Running org.apache.storm.flux.TCKTest
Tests run: 18, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.852 sec <<< FAILURE! - in org.apache.storm.flux.TCKTest
testVariableSubstitution(org.apache.storm.flux.TCKTest)  Time elapsed: 0.012 sec  <<< ERROR!
org.yaml.snakeyaml.scanner.ScannerException: null; mapping values are not allowed here;  in 'string', line 45, column 890:
     ... /usr/local/bin:/usr/bin:
                                ^
        at org.yaml.snakeyaml.scanner.ScannerImpl.fetchValue(ScannerImpl.java:866)
        at org.yaml.snakeyaml.scanner.ScannerImpl.fetchMoreTokens(ScannerImpl.java:360)
        at org.yaml.snakeyaml.scanner.ScannerImpl.checkToken(ScannerImpl.java:226)
        at org.yaml.snakeyaml.parser.ParserImpl$ParseBlockMappingKey.produce(ParserImpl.java:558)
        at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)
        at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)
        at org.yaml.snakeyaml.composer.Composer.composeMappingNode(Composer.java:230)
        at org.yaml.snakeyaml.composer.Composer.composeNode(Composer.java:159)
        at org.yaml.snakeyaml.composer.Composer.composeMappingNode(Composer.java:237)
        at org.yaml.snakeyaml.composer.Composer.composeNode(Composer.java:159)
        at org.yaml.snakeyaml.composer.Composer.composeDocument(Composer.java:122)
        at org.yaml.snakeyaml.composer.Composer.getSingleNode(Composer.java:105)
        at org.yaml.snakeyaml.constructor.BaseConstructor.getSingleData(BaseConstructor.java:120)
        at org.yaml.snakeyaml.Yaml.loadFromReader(Yaml.java:481)
        at org.yaml.snakeyaml.Yaml.load(Yaml.java:400)
        at org.apache.storm.flux.parser.FluxParser.loadYaml(FluxParser.java:121)
        at org.apache.storm.flux.parser.FluxParser.parseInputStream(FluxParser.java:75)
        at org.apache.storm.flux.parser.FluxParser.parseResource(FluxParser.java:59)
        at org.apache.storm.flux.TCKTest.testVariableSubstitution(TCKTest.java:224)


Results :

Tests in error: 
  TCKTest.testVariableSubstitution:224 » Scanner null; mapping values are not al...
{code}

h2. Proposed solution

Just wrap the {{PATH}} variable's contents in the yaml file.

h3. \[1] PATH validity

{code: title=man bash}
...

       PATH   The  search  path  for  commands.  It is a colon-separated
              list of directories in which the shell looks for  commands
              (see  COMMAND  EXECUTION  below).   A  zero-length  (null)
              directory name in the value of PATH indicates the  current
              directory.   A null directory name may appear as two adja-
              cent colons, or as an  initial  or  trailing  colon.   The
              default path is system-dependent, and is set by the admin-
              istrator  who  installs   bash.    A   common   value   is
              ``/usr/gnu/bin:/usr/local/bin:/usr/ucb:/bin:/usr/bin''.
...
{code}"
STORM-2891,Upgrade Checkstyle plugin to version 3.0.0,It would be good to upgrade the maven-checkstyle-plugin to version 3.0.0. Some pretty nice quality of life improvements there https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317223&version=12333072.
STORM-2890,Update the UI to use the new metrics query API,"Once we have a query API and at least the same metric that go into ZK are going into the new metrics engine we should update the UI to display the metrics from the new query API where possible, and fall back on ZK when they are not available.

We want to get rid of the metrics in the heartbeats at some point soon, but to be able to support older topologies running under the newer versions of storm we will keep this code around until a 3.x build."
STORM-2889,Add Metrics Query API (Thrift and REST),"Once we have a storage engine and even if it is just a few metrics we want to add in an API to query the metrics.

It should be able to do simple aggregations (Min, Max, Mean, and Sum) across different time ranges, with different roll up windows.  It should also be able to filter based off of things like topology id, component name, component id, stream name, and host name.  It is fine to have some of these required, such as topology id, at the beginning.

It should be able to return results either all together or split up by the different topology ids, component names, component ids, stream names, or host names.

For Example the UI shows things like

SUM of {tuples_emitted, tuples_transferred, acked, failed}  in the last {10 mins, 3 hours, 1 day, all time} for all spouts a.k.a a set of component names that we got from the topology itself.
AVERAGE of {complete_latency} in the last {10 mins, 3 hours, 1 day, all time} for a set of component names.

SUM of {tuples_emitted, tuples_transferred, acked, failed}  in the last {10 mins, 3 hours, 1 day, all time} for all components separated by each component name.
AVERAGE of {complete_latency} in the last {10 mins, 3 hours, 1 day, all time} for all components separated by each component name.

And there are others too.



The API should be open to expansion for other types of filters, aggregates, and even data types.  The following are not things that should be implemented, but are things that we should think about how we can leave the API open for expansion alter on so we could support them in a clean way.

Specifically some filters I would like to see in the future include topology name or even better a basic pattern we can use to match topology ids and the owner of a topology.

I would also like to eventually support percentile sketches as a data type so instead of just having an average of the 99th%ile collected by each component, we get an much more accurate approximation of what the 99th%ile really is in aggregate.
"
STORM-2888,Integrate storage with metrics V2.,Once STORM-2887 and STORM-2153 are done and in this is to add in the glue code that allows the supervisor to forward all of the metrics to the storage engine.
STORM-2887,Add RocksDB storage and Basic Metrics forwarding from the supervisor,"This portion is just to put in the rocksdb storage engine, the APIs to be able to store the metrics, and to update the supervisor to send the few metrics that it knows about.

Memory usage when using cgroups.

This is by no means complete but should be able to unblock some work on elasticity in the scheduler."
STORM-2886,Look into better phemeral port support in LocalCluster.withNimbusDaemon,"We have a few tests that use a LocalCluster and then launch an actual Nimbus Thrift Server instance.  This is mostly to test that the thrift code works properly.

In some cases this can cause issues because by default it is going to get a single hard coded port.

In most cases we work around this, like in STORM-2885 by finding a free port and then using that for both client and server configs.

This does not eliminate the race, but it makes it very unlikely.  It would be great if we could eliminate the race by somehow bringing up the thrift server on an ephemeral port, and then providing a way to get that port through an API in LocalCluster.

One of the issues we would have to overcome is that we explicitly check for positive port numbers, and we would only want to turn that off for this particular use case."
STORM-2885,TickTupleTest can fail because it uses NimbusDaemon,"When STORM-2876 went it it made some of the tests run in parallel.  I have seen testTickTupleWorksWithSystemBolt fail, but rarely, when other tests are running that also want to launch a LocalCluster withNimbusDaemon enabled.

This test has no reason to have withNimbusDaemon enabled so I will just turn it off.

There are a few other tests that want to use this same functionality.  I will also look to see if I can reduce it's usage or find a good way to have it use an ephemeral port.

"
STORM-2882,Relocate dependencies on storm-client (and more),"STORM-2441 broke down storm-core to several parts which reduced actual dependencies for storm-client (worker), but it also got rid of relocations which may be considered as regression.

We should handle relocation before releasing Storm 2.0.0.

Earlier discussion link: https://mail-archives.apache.org/mod_mbox/incubator-storm-dev/201703.mbox/%3CCAF5108gjjQzSyYWCP99bgPGEc7TUfE-tbt9Fi0M78ah8RkMQCQ@mail.gmail.com%3E"
STORM-2880,Minor optimisation about kafka spout,"Based on the single responsibility principle, method isAtLeastOnceProcessing() should reside in KafkaSpoutConfig rather than KafkaSpout. This patch removes the dependency of KafkaSpoutConfig.ProcessingGuarantee from KafkaSpout."
STORM-2879,Supervisor collapse continuously when there is a expired assignment for overdue storm,"For now, when a topology is reassigned or killed for a cluster, supervisor will delete 4 files for an overdue storm:
- storm-code
- storm-ser
- storm-jar
- LocalAssignment

Slot.java
static DynamicState cleanupCurrentContainer(DynamicState dynamicState, StaticState staticState, MachineState nextState) throws Exception {
        assert(dynamicState.container != null);
        assert(dynamicState.currentAssignment != null);
        assert(dynamicState.container.areAllProcessesDead());
        
        dynamicState.container.cleanUp();
        staticState.localizer.releaseSlotFor(dynamicState.currentAssignment, staticState.port);
        DynamicState ret = dynamicState.withCurrentAssignment(null, null);
        if (nextState != null) {
            ret = ret.withState(nextState);
        }
        return ret;
    }

But we do not make a transaction to do this, if an exception occurred during deleting storm-code/ser/jar, an overdue local assignment will be left on disk.

Then when supervisor restart from the exception above, the slots will be initial and container will be recovered from LocalAssignments, the blob store will fetch the files from Nimbus/Master, but will get a KeyNotFoundException, and supervisor collapses again.

This will happens continuously and supervisor will never recover until we clean up all the local assignments manually.

This is the stack:
2017-12-27 14:15:04.434 o.a.s.l.AsyncLocalizer [INFO] Cleaning up unused topologies in /opt/meituan/storm/data/supervisor/stormdist
2017-12-27 14:15:04.434 o.a.s.d.s.AdvancedFSOps [INFO] Deleting path /opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785
2017-12-27 14:15:04.445 o.a.s.d.s.Slot [INFO] STATE EMPTY msInState: 109 -> WAITING_FOR_BASIC_LOCALIZATION msInState: 1
2017-12-27 14:15:04.471 o.a.s.d.s.Supervisor [INFO] Starting supervisor with id 255d3fed-f3ee-4c7e-8a08-b693c9a6a072 at host gq-data-rt48.gq.sankuai.com.
2017-12-27 14:15:04.502 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.611 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.718 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.825 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.932 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.039 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.140 o.a.s.u.Utils [INFO] Could not extract resources from /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar
2017-12-27 14:15:05.142 o.a.s.d.s.Slot [INFO] STATE WAITING_FOR_BASIC_LOCALIZATION msInState: 697 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-12-27 14:15:05.142 o.a.s.l.AsyncLocalizer [WARN] Caught Exception While Downloading (rethrowing)... 
java.io.FileNotFoundException: File '/opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785/stormconf.ser' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:264) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:376) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:370) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:226) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:213) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]"
STORM-2878,Supervisor collapse continuously when there is a expired assignment for overdue storm,"For now, when a topology is reassigned or killed for a cluster, supervisor will delete 4 files for an overdue storm:
- storm-code
- storm-ser
- storm-jar
- LocalAssignment

Slot.java
static DynamicState cleanupCurrentContainer(DynamicState dynamicState, StaticState staticState, MachineState nextState) throws Exception {
        assert(dynamicState.container != null);
        assert(dynamicState.currentAssignment != null);
        assert(dynamicState.container.areAllProcessesDead());
        
        dynamicState.container.cleanUp();
        staticState.localizer.releaseSlotFor(dynamicState.currentAssignment, staticState.port);
        DynamicState ret = dynamicState.withCurrentAssignment(null, null);
        if (nextState != null) {
            ret = ret.withState(nextState);
        }
        return ret;
    }

But we do not make a transaction to do this, if an exception occurred during deleting storm-code/ser/jar, an overdue local assignment will be left on disk.

Then when supervisor restart from the exception above, the slots will be initial and container will be recovered from LocalAssignments, the blob store will fetch the files from Nimbus/Master, but will get a KeyNotFoundException, and supervisor collapses again.

This will happens continuously and supervisor will never recover until we clean up all the local assignments manually.

This is the stack:
2017-12-27 14:15:04.434 o.a.s.l.AsyncLocalizer [INFO] Cleaning up unused topologies in /opt/meituan/storm/data/supervisor/stormdist
2017-12-27 14:15:04.434 o.a.s.d.s.AdvancedFSOps [INFO] Deleting path /opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785
2017-12-27 14:15:04.445 o.a.s.d.s.Slot [INFO] STATE EMPTY msInState: 109 -> WAITING_FOR_BASIC_LOCALIZATION msInState: 1
2017-12-27 14:15:04.471 o.a.s.d.s.Supervisor [INFO] Starting supervisor with id 255d3fed-f3ee-4c7e-8a08-b693c9a6a072 at host gq-data-rt48.gq.sankuai.com.
2017-12-27 14:15:04.502 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.611 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.718 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.825 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.932 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.039 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.140 o.a.s.u.Utils [INFO] Could not extract resources from /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar
2017-12-27 14:15:05.142 o.a.s.d.s.Slot [INFO] STATE WAITING_FOR_BASIC_LOCALIZATION msInState: 697 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-12-27 14:15:05.142 o.a.s.l.AsyncLocalizer [WARN] Caught Exception While Downloading (rethrowing)... 
java.io.FileNotFoundException: File '/opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785/stormconf.ser' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:264) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:376) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:370) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:226) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:213) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]"
STORM-2877,Introduce an option to configure pagination in Storm UI ,"The current pagination default value for Storm UI is hard-coded to be 20. Pagination has been introduced in Storm 1.x. Having 20 items in the list restricts searching through the page. It will be beneficial to have a configuration option, say {{ui.pagination}}, which will set the default pagination value when Storm UI loads. This option can be added to {{storm.yaml}} along with other configurations."
STORM-2876,Some storm-hdfs tests fail with out of memory periodically,"In our 2.x automated testing we have noticed that every so often we will see TestFileLock fail with out of memory errors.

{code}
java.lang.OutOfMemoryError: Java heap space
...
{code}

Which then appears to trigger other failures.  Not sure if there is a memory leak involved or if the tests really need 1.5GB of memory periodically."
STORM-2875,Hadoop-auth dependency in storm-core results in exception,"Storm-core 1.1.1 ships with hadoop-auth dependency for version 2.6.1 which results in exceptions while working with hadoop version 2.8.x and above due to changes in KerberosUtil class.


_java.lang.NoSuchMethodError: org.apache.hadoop.security.authentication.util.KerberosUtil.hasKerberosKeyTab(Ljavax/security/auth/Subject;)Z
	at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:715) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:925) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:873) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:740) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3472) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3462) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3304) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476) ~[stormjar.jar:?]
	at org.apache.storm.hdfs.bolt.HdfsBolt.doPrepare(HdfsBolt.java:106) ~[stormjar.jar:?]
	at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.prepare(AbstractHdfsBolt.java:124) ~[stormjar.jar:?]
	at org.apache.storm.daemon.executor$fn__5030$fn__5043.invoke(executor.clj:793) ~[storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.1.jar:1.1.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]_"
STORM-2874,Minor style improvements to backpressure code,"The WorkerBackpressureThreadTest uses looped sleep-and-check on an AtomicLong to verify that the callback is called after notifying the trigger. We should use a countdown latch for this instead.

The WorkerBackpressureThreadTest extends TestCase, which is a bad idea because it makes the JUnit 4 annotations non-functional."
STORM-2871,Performance optimizations for getOutgoingTasks ,"Task.getOutgoingTasks() is in critical messaging path. Two observed bottlenecks in it :

- Looking up HashMap 'streamToGroupers'. Need to look into converting HashMap into Array lookup ?
- [outTasks.addAll(compTasks)|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/Task.java#L139]  seems to be impacting throughput as well. Identified by .. running ConstSpoutNullBoltTopo with 1 spout & bolt paralllelism (no Ack) and replacing this line with hard coded logic to add the single known bolt's taskID. "
STORM-2870,FileBasedEventLogger leaks non-daemon ExecutorService which prevents process to be finished,"{code}
    private void setUpFlushTask() {
        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor();
        Runnable task = new Runnable() {
            @Override
            public void run() {
                try {
                    if(dirty) {
                        eventLogWriter.flush();
                        dirty = false;
                    }
                } catch (IOException ex) {
                    LOG.error(""Error flushing "" + eventLogPath, ex);
                    throw new RuntimeException(ex);
                }
            }
        };

        scheduler.scheduleAtFixedRate(task, FLUSH_INTERVAL_MILLIS, FLUSH_INTERVAL_MILLIS, TimeUnit.MILLISECONDS);
}
{code}

The code block initializes ExecutorService locally, which served threads are not daemons so it can prevent JVM to be exit successfully.

Moreover it should be considered as bad case: not labeling thread name. I observed the process hung and got jstack, but hard to know where is the root, because leaked thread has default thread name."
STORM-2869,KafkaSpout discards all pending records when adjusting the consumer position after a commit,"As part of the STORM-2666 fix the spout clears out waitingToEmit when the consumer position falls behind the committed offset during a commit. We only need to do it for the affected partition, and then only for the records that are behind the committed offset.

Also the validation check in emitOrRetryTuple is slightly too permissive, it should check whether the current record is behind the committed offset, not whether the consumer position is behind the committed offset."
STORM-2868,Address handling activate/deactivate in multilang module files,"The multilang modules in Javascript, ruby and python are missing activate and deactivate handling from the last multilang protocol change."
STORM-2867,Add Consumer lag metrics to Kafka Spout,
STORM-2866,ImportError: No module named shlex,"Hello Team,

Am trying to get Stormcrawler working on my Debian VM. I get this error when I do the following command:

root@demo76:/opt# storm

Traceback (most recent call last):

 File ""/opt/storm/apache-storm-1.1.0/bin/storm.py"", line 23, in <module>

 import shlex

ImportError: No module named shlex

 I have python 2.7.9 version. I tried a simple python program with shlex and it works fine but am not sure why the storm is not recognizing shlex. Can you please help?

Kind Regards,
Sai"
STORM-2865,KafkaSpout constructor with KafkaConsumerFactory shoud be public,"When we use custom implement of interface ""Deserializer""，KafkaSpout constructor can only use class ""KafkaConsumerFactoryDefault""，the method “configure” of Interface “Deserializer” will not be called。

We need change the ""KafkaSpout"" constructor with ""KafkaConsumerFactory"" to be public, so we can create custom custom implement of interface ""KafkaConsumerFactory""."
STORM-2864,Minor optimisation about trident kafka state,"Make TridentKafkaState a template class to eliminate warning messages in eclipse, and a minor optimisation that use StringBuilder.append instead of string concat operation."
STORM-2863,Some ras tests fail because of static resource caching,"We noticed that depending on the ordering of tests, some of the RAS tests can fail because of metrics being cached.  We really should clear this static cache before each test."
STORM-2862,"More flexible logging in multilang (Python, Ruby, JS)","We're running a Storm topology written in Python, using storm.py from storm-multilang. As well as human-readable logs, the topology is also configured to write JSON logs which are sent to ELK.

At the moment, when storm-core receives a ""log"" command, it outputs the pid, component name, and the message it received, like so:

{{ShellLog pid:<pid>, name:<component name> <message>}}

The code that does this is (currently) in [ShellBolt line 254|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/task/ShellBolt.java#L254] and [ShellSpout line 227|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/spout/ShellSpout.java#L227].

As well as the pid and component name, it would be great to have the task ID, tuple ID, and the ID of the originating tuple - but this would make parsing the string even more laborious than it is now, and would make the default log message too long. 

Would it be possible to put contextual information like this in the [ThreadContext|https://logging.apache.org/log4j/2.x/manual/thread-context.html] instead? Then our JSON layout could read from the context instead of parsing the string, and human-readable logs could use ""%mdc"" in the PatternLayout format string."
STORM-2861,Explicit reference kafka-schema-registry-client,"storm-hdfs compile failure due to io.confluent.kafka.schemaregistry.client.* not found.
Changing the dependence from kafka-avro-serializer to kafka-schema-registry-client fixed this issue."
STORM-2859,NormalizedResources has some bugs in special cases where 0 of a resource is available.,
STORM-2858,Fix worker-launcher build,I got an error when building with -Pnative because GCC has marked asprintf as a function where you shouldn't ignore the return value. I'm guessing it's the same issue that's preventing Travis from building storm-core.
STORM-2853,Deactivated topologies cause high cpu utilization,"The issue is there is high cpu usage for deactivated apache storm topologies.  I can reliably re-create the issue using the steps below but I haven't identified the exact cause or a solution yet.

The environment is a storm cluster on which 1 topology is running (The topology is extremely simple, I used the exclamation example).  It is INACTIVE.  Initially there is normal CPU usage.  However, when I kill all topology JVM processes on all supervisors and let Storm restart them again, I find that some time later (~9 hours) the CPU usage per JVM process rises to nearly 100%.  I have tested an ACTIVE topology and this does not happen with it.  I have also tested more than one topology and observe the same results when they're in the INACTIVE state.

***Steps to re-create:***

 1. Run 1 topology on an Apache Storm cluster
 2. Deactivate it
 3. Kill **all** topology JVM processes on all supervisors (Storm will restart them)
 4. Observe the CPU usage on Supervisors rise to nearly 100% for all **INACTIVE** topology JVM processes.

***Environment***

Apache Storm 1.1.0 running on 3 VMs (1 nimbus and 2 supervisors).

Cluster Summary:

 - Supervisors: 2 
 - Used Slots: 2 
 - Available Slots: 38 
 - Total Slots: 40
 - Executors: 50 
 - Tasks: 50

the topology has 2 workers and 50 executors/tasks (threads).


***Investigation so far:***

Apart from being able to reliably re-create the issue, I have identified, for the affected topology JVM process, the threads using the most CPU.  There are 102 threads total in the process, 97 blocked, 5 IN_NATIVE.  The threads using the most CPU are identical and there are 23 of them (all in BLOCKED state):

    Thread 28558: (state = BLOCKED)
     - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
     - java.util.concurrent.locks.LockSupport.parkNanos(long) @bci=11, line=338 (Compiled frame)
     - com.lmax.disruptor.MultiProducerSequencer.next(int) @bci=82, line=136 (Compiled frame)
     - com.lmax.disruptor.RingBuffer.next(int) @bci=5, line=260 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue.publishDirect(java.util.ArrayList, boolean) @bci=18, line=517 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue.access$1000(org.apache.storm.utils.DisruptorQueue, java.util.ArrayList, boolean) @bci=3, line=61 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue$ThreadLocalBatcher.flush(boolean) @bci=50, line=280 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue$Flusher.run() @bci=55, line=303 (Interpreted frame)
     - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
     - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
     - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
     - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
     - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)


I identified this thread by using `jstack` to get a thread dump for the process:
 

    jstack -F <pid> > jstack<pid>.txt

and `top` to identify the threads within the process using the most CPU:

    top -H -p <pid> "
STORM-2852,Dup log4j jars in storm-local,"After upgrading to 1.0.2, every time there was ""Could not initialize class org.apache.log4j.Log4jLoggerFactory"" error when deploying a topology, but nothing suspicious found against the dependency tree.

Turned out there are dup log4j implementation jars under storm lib dir
log4j-over-slf4j-1.6.6.jar
log4j-slf4j-impl-2.1.jar

After removing log4j-over-slf4j-1.6.6.jar from lib, the whole world is running well.

Could you someone help fix? It's quite tricky to figure out the problem"
STORM-2851,org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions sometimes throws ConcurrentModificationException,"Hello,

We have been running Storm 1.2.0 preview on our pre-production supervision system.
We noticed that in the logs of our topology to logs persistency in Hbase, we got the following exceptions (about 4 times in a 48 hours period):

java.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442) 
at java.util.HashMap$KeyIterator.next(HashMap.java:1466) 
at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:347) 
at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:320) 
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:245) 
at org.apache.storm.daemon.executor$fn__4963$fn__4978$fn__5009.invoke(executor.clj:647) 
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) 
at clojure.lang.AFn.run(AFn.java:22) 
at java.lang.Thread.run(Thread.java:748) 

It looks like there's something to fix here, such as making the map thread-safe, or managing the exclusivity of modification of this map at a caller level.

Note: this topology is using Storm Kafka Client spout with default properties (unlike other topologies we have based on autocommit). However, it's the one which deals with highest rate of messages (line of logs coming from about 10000 VMs, a nice scale test for Storm :))

Could it be fixed in Storm 1.2.0 final version?

Best regards,
Alexandre Vermeerbergen
"
STORM-2850,ManualPartitionSubscription assigns new partitions before calling onPartitionsRevoked,"ManualPartitionSubscription does partition assignment updates in the wrong order. It calls KafkaConsumer.assign, then onPartitionsRevoked and last onPartitionsAssigned. The order should be onPartitionsRevoked, then assign, then onPartitionsAssigned.

onPartitionsRevoked has to be called before we reassign partitions, because we try to commit offsets for the revoked partitions. If we try to commit to a partition the consumer is not assigned, it will throw an exception. The onRevoke, assign, onAssign order is also more in line with the javadoc for ConsumerRebalanceListener, which specifies that onRevoke should be called before the partition rebalance begins.
"
STORM-2848,[storm-sql] Separate the concept of STREAM and TABLE,"Currently we only support STREAM type of table, and don't provide type selection for users.

We have future plan of supporting join on stream and table, which requires tables to be defined either stream or table, because left join which left side is table and right side is stream doesn't make sense and vice versa.

This issue tracks the effort of separating STREAM and TABLE. We may want to also apply the constraint that only STREAM supports SELECT statement."
STORM-2847,Exception thrown after rebalance IllegalArgumentException,"After rebalance the storm-kafka-client spout attempts to check the current position of partitions that are no longer assigned to the current spout. This occurs in a topology with multiple spout instances.

java.lang.IllegalArgumentException: You can only check the position for partitions assigned to this consumer. at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1262) at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:473)"
STORM-2845,Drop standalone mode of Storm SQL,"Quoting discussion again:

{quote}
We have been exposing ""standalone mode"" of Storm SQL which leverages Storm SQL in a JVM process rather than composing topology and run.
At a start we implemented both standalone and trident modes with same approach, but while we improved Storm SQL by leveraging more features on Calcite, we addressed only trident mode, and now twos are diverged.

I guess there is likely no actual user on standalone mode since its classes are exposed but we didn't document it. I know a case, but the source codes on standalone mode code are migrated to the project (and modified to conform to the project) and the project no longer depends on Storm SQL.

If we all don't have any other case, how about dropping it and only concentrate to trident mode?
(Btw, I'm trying to replace the backend on Storm SQL from Trident to Streams API, which may make the mode name obsolete, but after dropping standalone mode we don't even need the name for mode since there will be only one mode.)
{quote}

Discussion link:
https://mail-archives.apache.org/mod_mbox/storm-dev/201712.mbox/%3CCAF5108g3yEQPWO-UPQaJmoqkN4%2BoZgnH64pvKs2ARju4ySUB4Q%40mail.gmail.com%3E
"
STORM-2844,KafkaSpout Throws IllegalStateException After Committing to Kafka When First Poll Strategy Set to EARLIEST,"This [code|https://github.com/apache/storm/blob/1.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L407-L409], which was committed to fix [STORM-2666|https://issues.apache.org/jira/browse/STORM-2666] throws IllegalStateException when the KafkaSpout commits to Kafka and is restarted with the same consumer group id and first poll strategy is set to EARLIEST.

For example consider the following sequence:
# KafkaSpout with consumer_group_id=TEST polls and commits offsets 1-5 
# KafkaSpout with consumer_group_id=TEST is restarted with first poll strategy set to EARLIEST

==> IllegalStateException will be thrown

This bug could be a blocker. I am setting it to Critical because assigning a different consumer id serves as a workaround to the problem.

"
STORM-2843,Flux: properties file not found when loading resources from classpath,"Scenario: The CI auto-build a fat jar including filter properties file and topology yaml file. Filter properties file not found when load resources from classpath.

example:
{code}storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --remote --resource --filter dev.properties my_config.yaml{code}

The dev.properties file cannot be found in classpath.

After this patch, the FluxParser will load filter properties file as same as the way of topology yaml.
"
STORM-2842,Fixed links for YARN&Kubernetes Integration,
STORM-2837,RAS Constraint Solver Strategy,"We have a use case where a user has some old native code and they need it to work with storm, but sadly the code is not thread safe so they need to be sure that each instance of a specific bolt is in a worker without other instances of the same bolt.  It also cannot co-exist with other bolts for a similar reason.  I know that this is a fairly strange use case, but to help fix the issue we wrote a strategy for RAS that can do a simple search of the state space trying to honor these constrains and we thought it best to push it back then to keep it internal."
STORM-2835,storm-kafka-client KafkaSpout can fail to remove all tuples from waitingToEmit,
STORM-2833,Cached Netty Connections can have different keys for the same thing.,"It turns out that if you set {{storm.local.hostname}} on your supervisors that the netty caching code might not work.  The issue is that when we go to add a netty connection to the cache we use the host name provided by the scheduling.  Which ultimately comes from the {{storm.local.hostname}} setting on each of the nodes.  But when we go to remove it from the cache, we use the resolved INetSocket address for the destination.  If the two do not match exactly then we can close a connection, but not have it removed from the cache, so when we go to try and use it again, the connection is closed."
STORM-2830,Upgrade Jackson to 2.9.2,"We recently hit an issue (https://issues.apache.org/jira/browse/STORM-2829) because Jackson can't serialize some Java 7 classes. In order to avoid this kind of problem in the future, I think we should upgrade Jackson.

Jackson has had support for the JDK7 additions since 2.8.0, so upgrading past this point should prevent this kind of bug from popping up again."
STORM-2829,Logviewer deepSearch not working,"
{code:java}
2017-11-21 21:06:19.369 o.e.j.s.HttpChannel qtp1471948789-17 [WARN] /api/v1/deepSearch/wc-1-1511188542
javax.servlet.ServletException: java.lang.RuntimeException: com.fasterxml.jackson.databind.JsonMappingException: Direct self-reference leading to cycle (through reference chain: org.apache.storm.daemon.logviewer.handler.Matched[""matches""]->java.util.ArrayList[0]->java.util.HashMap[""port""]->sun.nio.fs.UnixPath[""fileSystem""]->sun.nio.fs.LinuxFileSystem[""rootDirectories""]->sun.nio.fs.UnixPath[""root""])
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:489) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
{code}
"
STORM-2827,Logviewer search returns incorrect logviewerUrl,"Code in LogviewerLogSearchHandler
{code:java}
  @VisibleForTesting
    String urlToMatchCenteredInLogPage(byte[] needle, Path canonicalPath, int offset, Integer port) throws UnknownHostException {
        final String host = Utils.hostname();
        final Path truncatedFilePath = truncatePathToLastElements(canonicalPath, 3);

        Map<String, Object> parameters = new HashMap<>();
        parameters.put(""file"", truncatedFilePath.toString());
        parameters.put(""start"", Math.max(0, offset - (LogviewerConstant.DEFAULT_BYTES_PER_PAGE / 2) - (needle.length / -2)));
        parameters.put(""length"", LogviewerConstant.DEFAULT_BYTES_PER_PAGE);

        return UrlBuilder.build(String.format(""http://%s:%d/api/v1/log"", host, port), parameters);
    }

    @VisibleForTesting
    String urlToMatchCenteredInLogPageDaemonFile(byte[] needle, Path canonicalPath, int offset, Integer port) throws UnknownHostException {
        final String host = Utils.hostname();
        final Path truncatedFilePath = truncatePathToLastElements(canonicalPath, 1);

        Map<String, Object> parameters = new HashMap<>();
        parameters.put(""file"", truncatedFilePath.toString());
        parameters.put(""start"", Math.max(0, offset - (LogviewerConstant.DEFAULT_BYTES_PER_PAGE / 2) - (needle.length / -2)));
        parameters.put(""length"", LogviewerConstant.DEFAULT_BYTES_PER_PAGE);

        return UrlBuilder.build(String.format(""http://%s:%d/api/v1/daemonlog"", host, port), parameters);
    }
{code}
only returns http url. This url will be invalid if logviewer https port is configured, in which case the http url will be not found"
STORM-2826,KafkaSpoutConfig.builder doesn't set key/value deserializer properties in storm-kafka-client,"STORM-2548 replaced the KafkaSpoutConfig.builder() implementations with ones that don't set the key/value deserializer fields in KafkaSpoutConfig, but instead just sets the corresponding property in the kafkaProps map. This is a breaking change for applications that assume those properties are set after the builder is created.

Code like the following would break.
{quote}
this.keyDeserializer = config.getKeyDeserializer().getClass();
this.valueDeserializer = config.getValueDeserializer().getClass();
{quote}"
STORM-2825,"storm-kafka-client configuration fails with a ClassCastException if ""enable.auto.commit"" is present in the consumer config map, and the value is a string","{quote}
Exception in thread ""main"" java.lang.ClassCastException: java.lang.String
cannot be cast to java.lang.Boolean
        at
org.apache.storm.kafka.spout.KafkaSpoutConfig.setAutoCommitMode(KafkaSpoutConfig.java:721)
        at
org.apache.storm.kafka.spout.KafkaSpoutConfig.<init>(KafkaSpoutConfig.java:97)
        at
org.apache.storm.kafka.spout.KafkaSpoutConfig$Builder.build(KafkaSpoutConfig.java:671)
{quote}"
STORM-2824,Ability to configure topologies for exactly once processing,"The default implementation of a spout  (Kafka) is to wait for acknowledgement, if an acknowledgement is not provided the tuple is replayed leading to an at least once processing model.

Can an option be provided to always acknowledge even in the event of error in any spout or bolt and the user decide which mode the topology should be configured.

There are cases like multiple bolts (B) inserting to persistent stores (PS) like B1 - PS1, B2-PS2, B3-PS3, the fact that B2-PS2 bolt fail doesn't mean that the tuple needs to be replayed leading to complexity on the logic of bolts, it would be easier if this was configurable and the user of the topology decides which style to choose.
"
STORM-2823,Ability to have an option to combine topologies at run time in a single process space,"Unlike an API server which service multiple APIs within the same process space, the Topologies needs to run in separate processes.
Lets say we have Topology TP-1 which use 1 GB of memory.
Now we create the same for n Topologies of TP-1..... TP-n
As the topologies increase the memory allocation is now multiplied by the number of topologies.
This design though scalable is not similar to the API route we have before which was within the same process space.

So in a micros services world, each topology would be responsible for a similar set of objects, like employee, customer, product, order, order details etc.

As the number of topologies increase the worker allocation is not sufficient. Most topologies are not utilized fully but since these are in different process space the memory allocated can't be used.

If we have an ability to say that TP-1 --- TP 10 Can run within the same process space but behave like individual topologies we could conserve the resource usage.

Now user are forced to combine topologies to the hardware provided with ""if"" logics to route the correct object that needs to be processed.

This way one can still configure topologies as API in the same API server and reuse resources collectively for related group of topologies acting as micro services.




"
STORM-2822,Remove LinearDRPCTopologyBuilder,We should look into removing the deprecated LinearDRPCTopologyBuilder ibn 2.0.0.  But we need to make sure first that all of the use cases for it are clearly documented and covered by the other ways of using DRPC.
STORM-2821,Remove TransactionalTopologySupport.,"Transactional Topologies have been in storm for a long time, but Trident overrides it and the former is deprecated.  We should remove all of the code that supports it and from the code base."
STORM-2819,Ability to natively support JSON serialization in topologies,Now that the world is moving towards NoSQL and most of the data is in JSON. Can a native JSON Serializer be implemented similar to support for Kryo. 
STORM-2818,Storm UI doesn't show which version of the code was used to run the topology,"Lets say we create a Topology 1 with version 1 of the uber Jar namely Topololog1V1.jar
We submit this jar and the Topology1 is shown in Storm UI

We then do changes and now have another version of the Topology 1 which is Topology1V2.jar 
We submit this jar and the Topology1 is again shown in the Storm UI

In both case we can't find which version of the code are we running without actually logging in to the Storm Supervisor instance and find the process start parameters.

Can this start parameters be made available in the UI so that we can easily find which version of the code are we running for the topology?
"
STORM-2817,Topology Restart Counts are not maintained in Storm UI,"On the Storm UI, we need an ability to have a Topology Submission Time, Topology Uptime as well as how many times a Topology worker process has restarted since last Submission.

The reason been, lets say we have a Supervisor with 8 GB RAM.
We also have 4 Slots on this Supervisor.
We submit 4 Topologies each with worker memory of 3 GB leading to a total of 12GB / 8 GB utilization assuming not all topologies would use up all the memory at the same time.

Now, we find that topologies are dying behind the scenes due to out of memory and Storm Nimbus keeps restarting these topologies again.

The uptime requests as part of  [STORM-2816] (https://issues.apache.org/jira/browse/STORM-2816) we can address the uptime but it still won't say we have a deeper issue and the topologies are restarting behind the scene. Adding this counter would help to flag issues.

The counts should be at both per topology level like

Topology 1 
     Submission Time T1
     Uptime T2
     Restarts 4 (Possible log links to why restarted)

The other should be at the Storm UI level

Total Topologies : 20
Total Topologies Restart since Submission : 12 (Possible links to topologies that got restarted)

This way monitoring and alerting systems can hook into these counts and alert when things go wrong.
"
STORM-2816,Topology Summary Uptime is not reflecting worker restarts,"The Storm UI , Topology Summary Uptime is not reflecting the Worker Process restarts and always gives the initial topology submission uptime.

So if we submitted the Topology1 at time T1
The topology was running in 1 worker instance.
If the worker instance goes down at time T2 and a new worker instance was started at time T3.
The uptime always show time T1 and not starting with time T3

We might need to split the Topology Submission Time and Topology Uptime

"
STORM-2813,Clean up RAS resource Map.,"Under the new Generic RAS code we use a Map<String, Number> or Map<String, Double> for the resources.  This is really inefficient and we should look at normalizing the Maps into an array, which will be faster, and hopefully will make the code cleaner."
STORM-2811,"Nimbus may throw NPE if the same topology is killed multiple times, and the integration test kills the same topology multiple times","{quote}
2017-11-12 08:45:50.353 o.a.s.d.n.Nimbus pool-14-thread-47 [WARN] Kill topology exception. (topology name='SlidingWindowTest-window20-slide10')
java.lang.NullPointerException: null
	at org.apache.storm.cluster.IStormClusterState.getTopoId(IStormClusterState.java:171) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConfFromName(Nimbus.java:1970) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.killTopologyWithOpts(Nimbus.java:2760) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3226) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3210) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.10.0.jar:0.10.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.10.0.jar:0.10.0]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:167) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[libthrift-0.10.0.jar:0.10.0]
	at org.apache.thrift.server.Invocation.run(Invocation.java:18) ~[libthrift-0.10.0.jar:0.10.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
{quote}"
STORM-2810,Storm-hdfs tests are leaking resources,"The Storm-hdfs tests are leaking resources, and it seems to be making the tests fail. "
STORM-2809,Integration test is failing consistently and topologies sometimes fail to start workers,"The integration test has been failing fairly consistently since https://github.com/apache/storm/pull/2363. I tried running the test outside a VM with a locally installed Storm setup, and it has failed every time for me.

Most runs seem to fail in ways that make it look like the integration test is just flaky (e.g. tuple windows not matching the calculated window), but in at least a few tests I saw the topology get submitted to Nimbus followed by about 3 minutes of nothing happening. The workers never started and the supervisor didn't seem aware of the scheduling. The only evidence that the topology was submitted was in the Nimbus log. This still happens even if the test topologies are killed with a timeout of 0, so there should be slots free for the next test immediately.

I tried reverting https://github.com/apache/storm/pull/2363 and it seems to make the integration test pass much more often. Over 5 runs there was still an instance of a supervisor failing to start the workers, but the other 4 passed.

We should try to fix whatever is causing the supervisor to fail to start workers, and get the integration test more stable."
STORM-2807,Integration test should shut down topologies immediately after the test,"The integration test kills topologies with the default 30 second timeout. This is unnecessary and delays the following tests, because the killed topology is still occupying worker slots.

When the integration test kills topologies, it tries sending the kill message to Nimbus once, and may fail quietly. This breaks following tests, because the default Storm install has only 4 worker slots, and the test topologies each take up 3. When a topology is not shut down, it prevents the following topologies from being assigned."
STORM-2803,SlotTest failing on travis frequently,"I have seen SlotTest fail way too frequently on travis, but it never fails off of travis.

My guess is that there is some kind of a race condition happening and on slower hardware (aka VMs or Containers on overloaded build machines) that the tests tend to fail.

I'll try to find some time to look at this, but if someone else wants to steal it from me feel free to.  I don't know exactly when I will find time to do it."
STORM-2802,Storm-cassandra tests don't run on JDK 9,"The storm-cassandra tests don't run on JDK 9. 

{quote}
opaqueStateTest(org.apache.storm.cassandra.trident.MapStateTest)  Time elapsed: 0.627 sec  <<< ERROR!
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Error writing))
        at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:233)
        at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:79)
        at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1473)
        at com.datastax.driver.core.Cluster.init(Cluster.java:159)
        at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:330)
        at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:305)
        at com.datastax.driver.core.Cluster.connect(Cluster.java:247)
        at org.apache.storm.cassandra.trident.MapStateTest.setUp(MapStateTest.java:163)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:564)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)

{quote}

Java 9 support for Cassandra is tracked at https://issues.apache.org/jira/browse/CASSANDRA-9608"
STORM-2801,Storm-Hive tests don't run on JDK 9,"The Storm-Hive tests error out when running on JDK 9. 

{quote}
java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444)
	at org.apache.storm.hive.bolt.TestHiveBolt.<init>(TestHiveBolt.java:110)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1449)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2661)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2680)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	... 34 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	... 39 more
Caused by: javax.jdo.JDOFatalInternalException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
NestedThrowables:
org.datanucleus.exceptions.NucleusException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:591)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy27.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	... 44 more
Caused by: org.datanucleus.exceptions.NucleusException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
	at org.datanucleus.store.rdbms.mapping.RDBMSMappingManager.getDatastoreMappingClass(RDBMSMappingManager.java:1215)
	at org.datanucleus.store.rdbms.mapping.RDBMSMappingManager.createDatastoreMapping(RDBMSMappingManager.java:1378)
	at org.datanucleus.store.rdbms.table.AbstractClassTable.addDatastoreId(AbstractClassTable.java:392)
	at org.datanucleus.store.rdbms.table.ClassTable.initializePK(ClassTable.java:1087)
	at org.datanucleus.store.rdbms.table.ClassTable.preInitialize(ClassTable.java:247)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTable(RDBMSStoreManager.java:3118)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTables(RDBMSStoreManager.java:2909)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3182)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	... 62 more
{quote}

Someone asked about this error on Stack Overflow (https://stackoverflow.com/questions/43086904/error-with-nucleusexception-and-jdofatalexception-when-starting-hive), and it sounds like we'll need to wait for Hive to release a JDK 9 compatible version (https://issues.apache.org/jira/browse/HIVE-17632)"
STORM-2800,Use JAXB api dependency from Maven instead of relying on that API being available in the standard JDK,JDK 9 doesn't expose the javax.xml.bind package by default anymore. We should use the Maven package to get the APIs instead.
STORM-2799,Ensure jdk.tools is not being included transitively since tools.jar doesn't exist in JDK 9 and we don't need it.,"A few of our dependencies are leaking a jdk.tools dependency to us from hbase-annotations and hadoop-annotations. It seems like those projects use jdk.tools to run a custom doclet for generating their own Javadoc. We shouldn't need jdk.tools since we don't run custom doclets, and tools.jar doesn't exist in JDK 9."
STORM-2798,Build Storm with JDK 11,"Track what we need to do to make Storm build on Java 11 (i.e. fix issues introduced in Java 9, 10 and 11)."
STORM-2797,LogViewer worker logs broken on Windows,"LogViewer worker logs are broken on Windows. Attempting to access the log (e.g. http://localhost:8000/log?file=word-topo-5-1509750559%5C6701%5Cworker.log) leads to a 500 Server Error.

I've attached the LogViewer logs which show the stack trace. The issue is pretty clear from the log: on line 123 of logviewer.clj, the path is split using the path separator as a regex. This is fine on Posix systems as / is a normal character in regex; however, on Windows, backslash is the path separator. As this is also the regex escape character, it is not a valid regular expression."
STORM-2796,Flux: Provide means for invoking static factory methods and improve non-primitive number handling,"Provide a means to invoke static factory methods for flux components. E.g:

Java signature:
{code}
public static MyComponent newInstance(String... params)
{code}

Yaml:

{code}
    className: ""org.apache.storm.flux.test.MyComponent""
    factory: ""newInstance""
    factoryArgs: [""a"", ""b"", ""c""]
{code}

Also include a fix for non-primitive numbers, so constructs like the following work:

Java constructor:
{code}
public TestBolt(Long l){}
{code}

Yaml:
{code}
  - id: ""bolt-4""
    className: ""org.apache.storm.flux.test.TestBolt""
    constructorArgs:
      - 10
    parallelism: 1
{code}

(Before fix the above would fail because snakeyaml would convert `10` to an Integer.)
"
STORM-2795,Race in downloading resources can cause failure,Recently had a failure/hang in the async localizer test.  Turns out that there is a race when downloading dependencies and there is a race in trying to create the parent directory.
STORM-2788,supervisor.worker.version.classpath.map should support regex,"To enable 0.10 (or other version) topology to run on 2.x cluster, we need to set supervisor.worker.version.classpath.map and something else. But now the classpath.map doesn't support regex yet. We have to list all the jar file path.

{code:java}
supervisor.worker.version.classpath.map:
    0.10.2.y: ""/home/y/lib64/ystorm_compatibility/current/lib/asm-4.0.jar:/home/y/lib64/ystorm_compatibility/current/lib/auth_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/bcpkix.jar:/home/y/lib64/ystorm_compatibility/current/lib/bcprov.jar:/home/y/lib64/ystorm_compatibility/current/lib/bouncer_auth_java.jar:/home/y/lib64/ystorm_compatibility/current/lib/clojure-1.6.0.jar:/home/y/lib64/ystorm_compatibility/current/lib/data_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/disruptor-3.3.2.jar:/home/y/lib64/ystorm_compatibility/current/lib/junixsocket.jar:/home/y/lib64/ystorm_compatibility/current/lib/kryo-2.21.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-1.2-api-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-api-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-core-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-slf4j-impl-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/servlet-api-2.5.jar:/home/y/lib64/ystorm_compatibility/current/lib/sia_java_client.jar:/home/y/lib64/ystorm_compatibility/current/lib/slf4j-api-1.7.7.jar:/home/y/lib64/ystorm_compatibility/current/lib/storm-core-0.10.2.y.jar:/home/y/lib64/ystorm_compatibility/current/lib/storm_yahoo-0.10.2.y.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_byauth.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_filter_logic.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_servlet.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_servlet_filters.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_yca.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_ysecure.jar:/home/y/lib64/ystorm_compatibility/current/lib/zts_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/zts_java_client.jar:""
{code}

We want to have the following configs working.

{code:java}
supervisor.worker.version.classpath.map:
    0.10.2.y: ""/home/y/lib64/ystorm_compatibility/current/lib/*""
{code}
"
STORM-2787,storm-kafka-client KafkaSpout should set 'initialized' flag independently of processing guarantees,"Currently the method 


{code:java}
public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
{code}

has the following condition

{code:java}
if (isAtLeastOnceProcessing() && initialized) {
                initialized = false;
                ...
}
{code}

initialized should be set to false independently of the processing guarantee"
STORM-2786,Ackers leak tracking info on failure and lots of other cases.,"Over the weekend we had an incident where ackers were running out of memory at a really scary rate.  It turns out that they were having a lot of failures, for an unrelated reason, but each of the failures were resulting in tuple tracking being lost because... 

We don't send ticks to any system components ever...

https://github.com/apache/storm/blob/124acb92dff04a57b530ab4d95a698abc8ff46d9/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L384

and ackers are system components.

So the tracking map was never rotated and all failed tuples

https://github.com/apache/storm/blob/124acb92dff04a57b530ab4d95a698abc8ff46d9/storm-client/src/jvm/org/apache/storm/daemon/Acker.java#L97-L103

Were never deleted from the map.

This leak eventually made the ackers crash, and when they came back up the other components kept blasting them with messages that would never be fully acked which also leaked because of the tick problem.

Looking back this has been in every release since 0.9.1-incubating.  It appears to have been introduced by https://github.com/apache/storm/commit/483ce454a3b2cd31b5d1c34e9365346459b358a8

So every apache release has this problem (which is the only reason I have not marked this as a blocker, because apparently it is not so bad that anyone has noticed in the past 4 years)."
STORM-2784, storm-kafka-client KafkaTupleListener method onPartitionsReassigned() should be called after initialization is complete,
STORM-2783,"De-couple ""Spout Lag"" metrics on Storm UI from Kafka Spout and StormKafkaMonitor","As a developer of a spout, I'd love to be able to publish lag metrics to the storm UI.  After digging into the source code for how the UI interacts with storm-kafka-monitor to get these metrics, it appears to be strongly coupled.  I believe that the concept of ""Spout Lag"" extends beyond the scope of just consuming from Kafka. 

I'd like to propose the idea of restructuring how these metrics are queried by StormUI to a way that allows developers of other spouts to be able to ""plug into"" the UI.  The easiest way that springs to mind is to provide an interface that allows developers to code against.

"
STORM-2781,Refactor storm-kafka-client KafkaSpout  Processing Guarantees,
STORM-2779,NPE on shutting down WindowedBoltExecutor,"STORM-2724 introduced a bug on WindowedBoltExecutor which throws NPE when shutting down WindowedBoltExecutor which has waterMarkEventGenerator field as null.

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java#L330

"
STORM-2775,Improve KafkaPartition Metric Names,"The _storm-kafka_ `KafkaSpout` emits a metric group called _kafkaPartition_

These metric names are prefixed with 
{noformat}
Partition{host=some.broker.host.mycompany.com:9092,-topic=some/topic/name,-partition=40}
{noformat}

Which makes for ugly, difficult to discover metrics on systems like Graphite.

The metric prefix should match the metrics emitted by the _kafkaOffset_ metric group that look like:

_topicName/partition_<partitionNum>_"
STORM-2774,Workers get killed with FileNotFoundException on stormjar.jar,"Hi,

 Worker processes sometimes get killed unable to find stormjar.jar in tmp directory. The stacktrace looks as below. 

10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO] Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be c
reated
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308)
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO]       ... 85 more
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO] Caused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Error reading config
uration file
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.fail(ServiceLoader.java:232)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.parse(ServiceLoader.java:309)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.access$200(ServiceLoader.java:185)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:293)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.security.AccessController.doPrivileged(Native Method)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       ... 85 more
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO] Caused by: java.io.FileNotFoundException: /var/log/storm/tmp/supervisor/stormdist/R2Topology-80-1507783551/stormjar.jar (No such file or directory)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.open(Native Method)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.<init>(ZipFile.java:219)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.<init>(ZipFile.java:149)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.jar.JarFile.<init>(JarFile.java:166)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.jar.JarFile.<init>(JarFile.java:103)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:93)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at java.net.URL.openStream(URL.java:1045)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.parse(ServiceLoader.java:304)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       ... 92 more"
STORM-2773,"If a drpcserver node in cluster is down,drpc cluster won't work if we don't modify the drpc.server configuration and restart the cluster","There is a cluster which includes three nodes named storm1,storm2,storm3.And there is a drpcserver in every node,a worker which has been started on strom1.When strom1 was down with hardware failure,my drpc topology won't work,when I send request from drpcclient.
As storm1 was down,so the worker will be restarted on another node,but it can't Initialize successfully because the call method of Adder will throw a RuntimeException,when drpcspout try to connect to storm1,so the worker will restart again. 

In conclusion,If a drpcserver node in cluster is down,drpc cluster won't work until we modify the drpc.server configuration and restart the cluster,but in production,it's difficult to restart whole cluster.

So I think we should catch the RuntimeException and log it,and the drpc topology will work normally."
STORM-2772,"In the DRPCSpout class, when the fetch from the DRPC server fails, the log should return to get the DRPC request failed instead of getting the DRPC result failed","In the DRPCSpout class, when the fetch from the DRPC server fails, the log error should return to get the DRPC request failed instead of getting the DRPC result failed.
for example, in line 216 of DRPCSpout class,
  LOG.error(""Not authorized to fetch DRPC result from DRPC server"", aze);
this should be modified to 
 LOG.error(""Not authorized to fetch DRPC request from DRPC server"", aze);"
STORM-2771,Some tests are being run twice,"I noticed that at least for storm-servier, and possibly others, that we have both the surefire plugin and the failsafe plugin.  Both of these run the unit tests.  They run them in slightly different ways, but we have not configured them to be exclusive, so most of the time we are running the storm-server tests twice."
STORM-2767,Surefire now truncates too much of the stack trace,"Surefire is truncating so much of the stack trace when tests fail that we often can't easily spot the error. As an example I manually threw an NPE from storm-kafka-client's KafkaSpout.commit() method, and here are the stack traces with trimStackTrace enabled and disabled:

trimmed
{code}
testCommitSuccessWithOffsetVoids(org.apache.storm.kafka.spout.KafkaSpoutCommitTest)  Time elapsed: 0.714 sec  <<< ERROR!
java.lang.NullPointerException: This is an NPE from inside nextTuple
	at org.apache.storm.kafka.spout.KafkaSpoutCommitTest.testCommitSuccessWithOffsetVoids(KafkaSpoutCommitTest.java:87)
{code}

not trimmed
{code}
testCommitSuccessWithOffsetVoids(org.apache.storm.kafka.spout.KafkaSpoutCommitTest)  Time elapsed: 0.78 sec  <<< ERROR!
java.lang.NullPointerException: This is an NPE from inside nextTuple
	at org.apache.storm.kafka.spout.KafkaSpout.commit(KafkaSpout.java:266)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:235)
	at org.apache.storm.kafka.spout.KafkaSpoutCommitTest.testCommitSuccessWithOffsetVoids(KafkaSpoutCommitTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
{code}

Note how the trimmed stack trace is also removing the trace lines from inside KafkaSpout.

As part of fixing https://issues.apache.org/jira/browse/STORM-2734 we upgraded to Surefire 2.19.1. It seems like 2.19 switched to a different interpretation of trimStackTrace, which trims all lines outside the test. It's my impression that it used to only trim lines before the trace reached a line inside the test. Going by https://issues.apache.org/jira/browse/SUREFIRE-1226?focusedCommentId=15140710&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15140710, this change seems intentional.

We should either downgrade Surefire, or disable stack trace trimming."
STORM-2764,HDFSBlobStore leaks file system objects,"This impacts all of the releases.  Each time we create a new HDFSBlobStore instance we call 

https://github.com/apache/storm/blob/v1.0.0/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java#L140

loginUserFromKeytab.

This results in a new subject being created each time when ends up causing a FileSystem object to leak each time."
STORM-2762,Sort out multiple places of codes of manipulation of collections,"Storm has codes of manipulation of collections at multiple places. One of example would be blacklist/Sets.java which will be merged in via STORM-2083.
https://github.com/apache/storm/pull/2343

It would be better to sort out and make them common utils, or if possible, replace them with specific library Storm already depends on."
STORM-2756,STORM-2548 on 1.x-branch broke setting key/value deserializers with the now deprecated setKey/setValue methods,"When STORM-2548 was backported, the setKey/setValue methods on KafkaSpoutConfig.Builder were deprecated, and users were directed to use setProp along with the relevant ConsumerConfig constants for setting deserializers instead.

As part of this change, the KafkaConsumerFactoryDefault switched from using the KafkaConsumer(props, keyDes, valDes) constructor to using the KafkaConsumer(props) constructor. Unfortunately I forgot to update the KafkaSpoutConfig.Builder constructor properly, so if the user configures the deserializer via either the Builder constructor parameters or setKey/setValue, the setting is not put in the kafkaProps map and the deserializer is not used."
STORM-2752,Nimbus crashes silently if scheduler is not found,"When nimbus is started and the custom scheduler specified in storm.yaml is not in the classpath, nimbus hangs and exits with status 13 about 10s later. No errors are logged.

Affected versions 1.0.3-5, I did not test any other. OpenJDK 8."
STORM-2750,fix double_checked locking,"update HBaseSecurityUtil singleton to fix double_checked locking

Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.
Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. When implemented in other languages, such as C++, it depends on the memory model of the processor, the reorderings performed by the compiler and the interaction between the compiler and the synchronization library. Since none of these are specified in a language such as C++, little can be said about the situations in which it will work. Explicit memory barriers can be used to make it work in C++, but these barriers are not available in Java.
See url link for details: http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
"
STORM-2748,TickTupleTest is useless,"The test starts up a small topology on a simulated time cluster with TOPOLOGY_TICK_TUPLE_FREQ_SECS set to 1.  Then it simulates 2 seconds of cluster time.  This is not enough time to even launch the topology.  How do I know this?  Because the Bolt and Spout in the topology override `writeObject` so the resulting serialized bolt and spout are empty and trying to deserialize them results in an exception.

Just running a topology that does nothing and never verifies that the ticks showed up is a really horrible test.  We should either delete it entirely or actually verify that ticks are showing up once a second.  I am leaning towards just removing it totally."
STORM-2747,"Make the windowing classes use long instead of int for time parameters, and rename millisecond based time parameters so it's clear which unit they are.","Some parameters in the windowing classes are using int instead of long for describing time, which requires a bunch of casting to/from long in the code. Some variables are always in milliseconds but are named e.g. ""value"", which makes it easy to accidentally use the wrong timeunit. Such variables should be named e.g. ""valueMs"" to disambiguate the unit."
STORM-2746,Max Open Files does not close files for the oldest entry,"Description:

AbstractHDFSBolt has WritersMap. This evicts least recently used AbstractHDFSWriter out of the writers map, however, does not close the file in open state by the oldest entry.

Steps to reproduce  error: 

1) Use new Max open files feature and set the value to 1.
2) Write data to two or three different files in hdfs using AvroBolt.
3) Check output directory using fsck in hdfs.
   
Expected: only one file open in output directory.
Actual: > 1 files are in open state."
STORM-2745,Hdfs Open Files problem,"Issue:

Problem exists when there are multiple HDFS writers in writersMap. Each writer keeps an open hdfs handle to the file. Incase of Inactive writer(i.e. one which is not consuming any data from long period), the files are not closed and always remain in open state.

Ideally, these files should get closed and Hdfs writers removed from the WritersMap.

Solution:
Implement a ClosingFilesPolicy that is based on Tick tuple intervals. At each tick tuple all Writers are checked and closed if they exist for a long time."
STORM-2739,Storm UI fails to bind to ui.host when using https,"When using https with the Storm UI, it ignores the value of ui.host, and binds to 0.0.0.0.

Starting with this config:


{code}
storm.local.dir: ""/opt/storm""
storm.zookeeper.servers:
    - ""bigstorm.porcupineracing.com""
nimbus.seeds: [""bigstorm.porcupineracing.com""]
nimbus.childopts: ""-Xmx1024m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
ui.childopts: ""-Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
supervisor.childopts: ""-Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
storm.thrift.transport: ""org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin""
java.security.auth.login.config: ""/keytabs/jaas.conf""
storm.zookeeper.superACL: ""sasl:storm@PORCUPINERACING.COM""

ui.host: 127.0.0.1

nimbus.authorizer: ""org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer""
nimbus.admins:
  - ""storm/bigstorm.porcupineracing.com@PORCUPINERACING.COM""
  - ""storm@PORCUPINERACING.COM""
  - ""storm""
nimbus.supervisor.users:
  - ""storm/bigstorm.porcupineracing.com@PORCUPINERACING.COM""
  - ""storm@PORCUPINERACING.COM""
  - ""storm""
nimbus.users:
   - ""steven.miller""
   - ""steven.miller@PORCUPINERACING.COM""
{code}

I can start the UI and verify using lsof that it's only listening on localhost:


{code}
[root@bigstorm bin]# ps axuww | grep ui.core
root      5080  0.1  5.6 2850232 217688 pts/1  Sl   Sep14   1:31 java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/opt/apache-storm-1.1.1 -Dstorm.log.dir=/opt/apache-storm-1.1.1/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /opt/apache-storm-1.1.1/lib/asm-5.0.3.jar:/opt/apache-storm-1.1.1/lib/clojure-1.7.0.jar:/opt/apache-storm-1.1.1/lib/disruptor-3.3.2.jar:/opt/apache-storm-1.1.1/lib/kryo-3.0.3.jar:/opt/apache-storm-1.1.1/lib/log4j-api-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-core-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-over-slf4j-1.6.6.jar:/opt/apache-storm-1.1.1/lib/log4j-slf4j-impl-2.8.2.jar:/opt/apache-storm-1.1.1/lib/minlog-1.3.0.jar:/opt/apache-storm-1.1.1/lib/objenesis-2.1.jar:/opt/apache-storm-1.1.1/lib/reflectasm-1.10.1.jar:/opt/apache-storm-1.1.1/lib/ring-cors-0.1.5.jar:/opt/apache-storm-1.1.1/lib/servlet-api-2.5.jar:/opt/apache-storm-1.1.1/lib/slf4j-api-1.7.21.jar:/opt/apache-storm-1.1.1/lib/storm-core-1.1.1.jar:/opt/apache-storm-1.1.1/lib/storm-rename-hack-1.1.1.jar:/opt/apache-storm-1.1.1:/opt/apache-storm-default/conf -Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf -Dlogfile.name=ui.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/opt/apache-storm-1.1.1/log4j2/cluster.xml org.apache.storm.ui.core
root     19913  0.0  0.0 112648   972 pts/1    R+   09:26   0:00 grep --color=auto ui.core

[root@bigstorm bin]# lsof -p 5080 -P | grep LISTEN
java    5080 root   27u     IPv6             597116       0t0      TCP localhost:8080 (LISTEN)
{code}


Now if I add the https config:

{code}
ui.https.host: ""localhost""
ui.https.port: 8443
ui.https.keystore.type: ""jks""
ui.https.keystore.path: ""/keytabs/keystore.jks""
ui.https.keystore.password: ""sooper-sekrit""
ui.https.key.password: ""sooper-sekrit""
{code}

and I restart the UI, I can see that it's listening on *:8443:

{code}
[root@bigstorm bin]# ps axuww | grep ui.core
root     19921 17.2  5.4 2849188 210896 pts/1  Sl   09:26   0:04 java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/opt/apache-storm-1.1.1 -Dstorm.log.dir=/opt/apache-storm-1.1.1/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /opt/apache-storm-1.1.1/lib/asm-5.0.3.jar:/opt/apache-storm-1.1.1/lib/clojure-1.7.0.jar:/opt/apache-storm-1.1.1/lib/disruptor-3.3.2.jar:/opt/apache-storm-1.1.1/lib/kryo-3.0.3.jar:/opt/apache-storm-1.1.1/lib/log4j-api-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-core-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-over-slf4j-1.6.6.jar:/opt/apache-storm-1.1.1/lib/log4j-slf4j-impl-2.8.2.jar:/opt/apache-storm-1.1.1/lib/minlog-1.3.0.jar:/opt/apache-storm-1.1.1/lib/objenesis-2.1.jar:/opt/apache-storm-1.1.1/lib/reflectasm-1.10.1.jar:/opt/apache-storm-1.1.1/lib/ring-cors-0.1.5.jar:/opt/apache-storm-1.1.1/lib/servlet-api-2.5.jar:/opt/apache-storm-1.1.1/lib/slf4j-api-1.7.21.jar:/opt/apache-storm-1.1.1/lib/storm-core-1.1.1.jar:/opt/apache-storm-1.1.1/lib/storm-rename-hack-1.1.1.jar:/opt/apache-storm-1.1.1:/opt/apache-storm-default/conf -Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf -Dlogfile.name=ui.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/opt/apache-storm-1.1.1/log4j2/cluster.xml org.apache.storm.ui.core
root     20018  0.0  0.0 112648   968 pts/1    R+   09:27   0:00 grep --color=auto ui.core
[root@bigstorm bin]# lsof -p 19921 -P | grep LISTEN
java    19921 root   38u  IPv6             677914       0t0      TCP *:8443 (LISTEN)
{code}

I have a situation in which I'm trying to limit access to the UI on a per-user basis.  The UI seems, as far as I can tell, only to support limiting access to users with valid Kerberos tickets (which is everyone here :) ), so I was trying to put a proxy in front of the UI and run it just on localhost, and rely on the proxy to do the authentication.

This bug means that if I was to do that, I'd have to run the UI without https, which means that people's credentials would be bouncing around in the clear (again, as far as I can tell; I tcpdumped that and I could see, say, storm@PORCUPINERACING.COM in the base64 decode of the Authorization: HTTP header, at least, which I figure was a bad sign).

I looked at the code and didn't see anything obvious but since I don't know Clojure or Netty it was probably staring me in the face. :) . But if you could fix this that'd be awesome, and it'd let me secure this in a way that I'd find much more reassuring.  Thanks!"
STORM-2736,o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key,"Sometimes, after our topologies have been running for a while, Zookeeper does not respond within an appropriate time and we see
{code}
2017-08-16 10:18:38.859 o.a.s.zookeeper [INFO] ip-10-181-20-70.ec2.internal lost leadership.
2017-08-16 10:21:31.144 o.a.s.zookeeper [INFO] ip-10-181-20-70.ec2.internal gained leadership, checking if it has all the topology code locally.
2017-08-16 10:21:46.201 o.a.s.zookeeper [INFO] Accepting leadership, all active topology found localy.
{code}

That's fine, and we probably need to allocate more resources. But after a new leader is chosen, we then see:
{code}
o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<key>
{code}
over and over.

I can't figure out yet how to cause the conditions that lead to Zookeeper becoming unresponsive, but it is possible to reproduce the {{BlobStoreUtils}} error by restarting Zookeeper.

The problem, I think, is that the loop [here|https://github.com/apache/storm/blob/v1.1.1/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L175] never executes because the {{nimbusInfos}} list is empty. If I add a check similar to [this|https://github.com/apache/storm/blob/v1.1.1/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L244] for a node which exists but has no children, the error goes away."
STORM-2735,LocalCluster and other testing classes are not documented in Javadoc because they are part of storm-server,"We don't publish any Javadoc about LocalCluster or the other testing tools because they are part of the storm-server module. Since users are expected to interact with these classes directly, we should figure out a way to publish Javadoc for them. 
"
STORM-2734,The master branch cannot release due to crash in Checkstyle,"The current master branch produces a stack overflow when mvn release:prepare is run. Checkstyle is executed multiple times per module, and in storm-sql-core it ends up checking generated files.

There are also several other minor issues I'd like to resolve:

* Checkstyle could be upgraded to latest version
* storm-integration-test is ""dangling"" in the sense that it's not attached to the rest of the project. This causes its version to get out of sync regularly (e.g. currently on 1.0.x-branch) because we have to update it manually.
* storm-hive declares calcite-core twice in different versions."
STORM-2733,Make Load Aware Shuffle much better at really bad situations,"We recently had an issue where some bolts got really backed up and started to die from OOMs.  The issue ended up being 2 fold.

First the GC really slowed down the worker so much that it could not keep up even with < 1% of the traffic that was still being sent to it.  Which made it almost impossible to recover.

The second issue was that the serialization of the tuples took a lot longer than the processing, which resulted in the send queue filling up much more quickly than the receive queue.

To help fix this issue I plan to address this in 2 ways.  First we need a better algorithm that can actually shut off the flow entirely to a very slow bolt and second we need to take the send queue into account when shuffling.

This is not a full set of changes needed by STORM-2686 but it is a step in that direction.  I am going to try and set it up so that the two algorithms would work nicely together."
STORM-2724,ExecutorService in WaterMarkEventGenerator never shutdown,"I have seen a topology with event time windowing never terminated on local mode. While looking into detail on thread dump I found only one non-daemon thread prevents process to be not finished: executorService in WaterMarkEventGenerator. 

Btw, I dumped thread via jstack but impossible to find from jstack result because it doesn't have thread factory hence thread name is pool-*."
STORM-2723,Nimbus crashes on joining cluster,"Cluster with N nodes and with running topologies. N new nodes join and the old machines start to be disconnected.
Some of the new nimbus fail with this message:

{code:java}
2017-09-06T16:30:53.551Z cluster [INFO] setup-path/blobstore/Topology-1-1504685635-stormconf.
ser/node02:6627-1
2017-09-06T16:30:53.608Z nimbus [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: or
g.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:98) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$fn__10607.invoke(nimbus.clj:1458) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:233) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$fn__11005$exec_fn__1364__auto____11006$fn__11021.invoke(nimbus.clj:2460) ~[storm-core-1.1.0.j
ar:1.1.0]
	at org.apache.storm.timer$schedule_recurring$this__1737.invoke(timer.clj:105) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:50) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.storm.th
rift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:120) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.storm.thrift.transport.TTransportExc
eption
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:266) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.lang.RuntimeException: java.io.IOException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:194) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.io.IOException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:156) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:182) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:186) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.generated.Nimbus$Client.recv_downloadBlobChunk(Nimbus.java:866) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.generated.Nimbus$Client.downloadBlobChunk(Nimbus.java:853) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.readMore(NimbusBlobStore.java:168) ~[storm-core-1.1.0.j
ar:1.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:146) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:182) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:186) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
2017-09-06T16:30:53.618Z util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__9808.invoke(nimbus.clj:212) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:71) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
2017-09-06T16:30:53.619Z nimbus [INFO] Shutting down master

{code}
"
STORM-2722,JMSSpout test fails way too often,"{code}
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.storm.jms.spout.JmsSpoutTest.testFailure(JmsSpoutTest.java:62)
{code}

Which corresponds to 

https://github.com/apache/storm/blob/d6e5e6d4e0a20c4c9f0ce0e3000e730dcb4700da/external/storm-jms/src/test/java/org/apache/storm/jms/spout/JmsSpoutTest.java?utf8=%E2%9C%93#L62
"
STORM-2721,Add mapping for KafkaConsumer metrics to storm metrics in KafkaTridentSpoutOpaque,"The current KafkaTridentSpoutOpque, does not have any metrics. We can use the metrics() call of the KafkaConsumer https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#metrics(), to get various metrics and map that to the storm metrics.

Eg:
We can add a generic KafkaClientMetric which would implement IMetric and has a getValueAndReset(), where the consumer metric calls are made.
requiredMetric can be initialized to any metrics like records-lag-max.

{code}
        @Override
        public Object getValueAndReset() {

            for (Map.Entry<MetricName, ? extends Metric> metricKeyVal : ((Map<MetricName, ? extends Metric>) kafkaConsumer.metrics()).entrySet()) {

                // Sample structure of Metric
                // MetricName [name=records-lag-max, group=consumer-fetch-manager-metrics, description=The maximum lag in terms of number of records for any partition in this window, tags={client-id=consumer-1}] metric.name()=MetricName [name=records-lag-max, group=consumer-fetch-manager-metrics, description=The maximum lag in terms of number of records for any partition in this window, tags={client-id=consumer-1}] metric.value()=-Infinity

                Metric metric = metricKeyVal.getValue();
                if(metric.metricName().name().equals(requiredMetric)) {
                    return metric.value();
                }

            }

            return null;
        }
{code}
"
STORM-2720,Add timestamp based FirstPollOffsetStrategy in KafkaTridentSpoutOpaque,"Offsets for a given partition at a particular timestamp can now be found using offsetsForTimes API. https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes(java.util.Map).

One way to make use of this api would be to :
Add a new option for FirstPollOffsetStrategy called TIMESTAMP 
Add a new startTimeStamp option to KafkaSpoutConfig, which would be used only when FirstPollOffsetStrategy is set to TIMESTAMP.

Later in the KafkaTridentSpoutEmitter, when we do the first seek, we can do something like :

{code}
            if(firstPollOffsetStrategy.equals(TIMESTAMP)) {
                try {
                    startTimeStampOffset =
                        kafkaConsumer.offsetsForTimes(Collections.singletonMap(tp, startTimeStamp)).get(tp).offset();
                } catch (IllegalArgumentException e) {
                    LOG.error(""Illegal timestamp {} provided for tp {} "",startTimeStamp,tp.toString());
                } catch (UnsupportedVersionException e) {
                    LOG.error(""Kafka Server do not support offsetsForTimes(), probably < 0.10.1"",e);
                }

                if(startTimeStampOffset!=null) {
                    LOG.info(""Kafka consumer offset reset for TopicPartition {}, TimeStamp {}, Offset {}"",tp,startTimeStamp,startTimeStampOffset);
                    kafkaConsumer.seek(tp, startTimeStampOffset);
                } else {
                    LOG.info(""Kafka consumer offset reset by timestamp failed for TopicPartition {}, TimeStamp {}, Offset {}. Restart with a different Strategy "",tp,startTimeStamp,startTimeStampOffset);
                }
            }
{code}

"
STORM-2719,Trident Kafka Spout Emitters do not get full partition information in getOrderedPartitions(),"The storm kakfa trident spout uses the KafkaTridentSpoutTopicPartitionRegistry, to get partition information. The coordinator calls the getTopicPartitions() method to get partition information and passes it to the emitters. But this partition information will not be accurate as all instances of KafkaTridentSpoutTopicPartitionRegistry will not be updated with full partition information.

The update to the registry is done when the consumer subscribes using KafkaSpoutConsumerRebalanceListener. This calls the KafkaTridentSpoutTopicPartitionRegistry.INSTANCE.addAll(partitions); These calls would only update the registry in that particular worker with partition information for consumers in that worker.

So when the coordinator calls the getOrderedPartitions() and passes it to each emitter by calling getOrderedPartitions(), the full partition information will not be present. The only probable case this would work is if the emitters and coordinators were on the same worker."
STORM-2716,Storm-webapp tests don't work on Windows,"Several storm-webapp tests don't work on Windows because file paths like ""/tmp"" are used, and paths are sometimes constructed by String concatenation instead of using Path.resolve. The logviewer also doesn't seem to work on Windows, probably for the same reason.

I think there might be a few similar issues in other parts of the code, which I'd like to also fix as part of this. 

I haven't checked whether this is a problem in 1.x, but it's likely."
STORM-2712,accept arbitrary number of rows per tuple in storm-cassandra,"Current implementation in `TridentResultSetValuesMapper::map` restricts a SELECT query to return one row. In `StateQueryProcessor::finishBatch`, it checks the equality between the result size of `batchRetrieve` and input tuple size. When the number of result rows is less than 1 or greater than 1, it breaks the condition and an exception is thrown.

We should accept arbitrary number of rows by adjusting List dimensions."
STORM-2711,use KafkaTridentSpoutOpaque poll msg slowly.,"At first I run producer examples to make msgs in kafka, which topic is 5 partition 1 replication, then the number of total message was about 4000, per partition almost 800. Then I {color:red}run the part of consumer example in TridentKafkaClientWordCountNamedTopics in storm-kafka-client-examples{color}, First pull messages at a certain speed, when each partition to more than 500, significantly {color:red}slower speed {color}. I wonder why"
STORM-2707,Nimbus loops forever with getClusterInfo error if it looses storm.local.dir contents,"Hello,

Short issue description:
* Remove storm.local.dir directory
* Storm UI isn't anymore able to query anything from Nimbus
* Nimbus process prints getClusterInfo exceptions in its log whenever it gets a query from Nimbus UI or from ""storm"" command line
* To fix this issue, we have to stop all Storm processes, cleanup the content of Zookeeper nodes, then restart & redeploy our topologies

Excepted behavior:
* In such case, Storm should cleanup the content of Zookeeper and recover in a mode allowing to kill & restart all topologies

More details:
===========
Sometimes we loose the content of storm.local.dir on our single-node Nimbus production cluster.

We haven't yet considered deploying Nimbus in HA because this is a relatively modest deployment with budget constrains on the number of the number of IaaS resources which can be used for this application. So far so good, because in our environment, Nimbus & Nimbus UI (hosted on same VM) are supervized, and we also have self-healing crons to automatically kill & restart topologies blocked in Kafka consumption or having too many failed tuples (because Storm back pressure has some fuzzy limits, so we use this by-pass, as approved by Roshan in a past discussion, but that's not the point here).

Our problem is that sometime, we loose the content of storm.local.dir.

When it happens, our supervision detects the issue because it cannot anymore query Nimbus REST services on Nimbus-UI process.

In such case it tries to restart Storm-UI but this doesn't help because queries to Storm-UI fails with the following stack trace when it tries to list all topologies:

org.apache.storm.thrift.TApplicationException: Internal error processing getClusterInfo
	at org.apache.storm.thrift.TApplicationException.read(TApplicationException.java:111)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.storm.generated.Nimbus$Client.recv_getClusterInfo(Nimbus.java:1168)
	at org.apache.storm.generated.Nimbus$Client.getClusterInfo(Nimbus.java:1156)
	at org.apache.storm.ui.core$cluster_summary.invoke(core.clj:356)
	at org.apache.storm.ui.core$fn__9556.invoke(core.clj:1113)
	at org.apache.storm.shade.compojure.core$make_route$fn__5976.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__5964.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__5957.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__5982.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__5986.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.cors$wrap_cors$fn__8891.invoke(cors.clj:149)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__8838.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6618.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__7901.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__6871.invoke(helpers.clj:50)
	at org.apache.storm.ui.core$catch_errors$fn__9758.invoke(core.clj:1428)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__6538.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__6581.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__6510.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6618.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__6833.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__6819.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__6368.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__6372.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.ui.helpers$x_frame_options_filter_handler$fn__6964.invoke(helpers.clj:189)
	at org.apache.storm.ui.helpers.proxy$java.lang.Object$Filter$abec9a8f.doFilter(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:748)

In Nimbus.log, we also have this kind of exception each time Nimbus UI is queried:

org.apache.storm.generated.KeyNotFoundException: null
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:147) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:299) ~[storm-core-1.1.0.jar:1.1.0]
        at sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:489) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.daemon.nimbus$get_cluster_info$iter__10687__10691$fn__10692.invoke(nimbus.clj:1550) ~[storm-core-1.1.0.jar:1.1.0]
        at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
        at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
        at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
        at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
        at clojure.core$dorun.invoke(core.clj:3009) ~[clojure-1.7.0.jar:?]
        at clojure.core$doall.invoke(core.clj:3025) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1524) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10782.getClusterInfo(nimbus.clj:1971) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3920) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3904) ~[storm-core-1.1.0.jar:1.1.0]

Even if Storm was shutting down in such case, this wouldn't help because we have to cleanup all Zookeepers to put our Storm cluster back to life.

Ideally, we would like that when storm.local.dir is lost, Nimbus will re-create it ""blank"" and cleanups Zookeeper nodes (that's the tricky part); then we expect that Supervisors (which are unaffected by this issue when it occurs) will re-register themselves to make Nimbus aware that topologies are Running. 

Also, Topologies restart from UI should consistently fail until topologies JARs are re-submitted (please make the error message very clear and easy to ""grep"" when such case occurs);

This is my first JIRA, I hope I provided everything to let Storm developers dig this issue ; otherwise please let me know if more information is required: I will be glad to help as much as I can... Storm rocks!

Best regards,
Alexandre Vermeerbergen

"
STORM-2706,Nimbus stuck in exception and does not fail fast,"We experience a problem in nimbus which leads it to get stuck in a retry and fail loop. When I manually restart the nimbus it works again as expected. However, it would be great if nimbus would shut down so our monitoring can automatically restart the nimbus. 

The nimbus log. 

{noformat}
24.8.2017 15:39:1913:39:19.804 [pool-13-thread-51] ERROR org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer - Unexpected throwable while invoking!
24.8.2017 15:39:19org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:151) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:133) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:453) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:19	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:19	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at org.apache.storm.zookeeper$zk_leader_elector$reify__1043.getLeader(zookeeper.clj:296) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:19	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:19	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10780.getLeader(nimbus.clj:2412) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.generated.Nimbus$Processor$getLeader.getResult(Nimbus.java:3944) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.generated.Nimbus$Processor$getLeader.getResult(Nimbus.java:3928) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:162) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
24.8.2017 15:39:19	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
24.8.2017 15:39:19	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
24.8.2017 15:39:2713:39:27.205 [pool-13-thread-52] ERROR org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer - Unexpected throwable while invoking!
24.8.2017 15:39:27org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:151) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:133) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:453) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:27	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:27	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at org.apache.storm.zookeeper$zk_leader_elector$reify__1043.getLeader(zookeeper.clj:296) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:27	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:27	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1544) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10780.getClusterInfo(nimbus.clj:2006) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3920) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3904) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:162) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
24.8.2017 15:39:27	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
24.8.2017 15:39:27	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
24.8.2017 15:39:2913:39:29.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:2913:39:29.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:39:3913:39:39.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:3913:39:39.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:39:4913:39:49.271 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:4913:39:49.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:39:5913:39:59.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:5913:39:59.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:40:0913:40:09.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:40:0913:40:09.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:40:1313:40:13.806 [timer] INFO  org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting
24.8.2017 15:40:1313:40:13.807 [timer] INFO  org.apache.storm.shade.org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181/storm sessionTimeout=20000 watcher=org.apache.storm.shade.org.apache.curator.ConnectionState@f90354
24.8.2017 15:40:1313:40:13.808 [timer-SendThread(10.42.174.214:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Opening socket connection to server 10.42.174.214/10.42.174.214:2181. Will not attempt to authenticate using SASL (unknown error)
24.8.2017 15:40:1313:40:13.862 [timer-SendThread(10.42.174.214:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Socket connection established to 10.42.174.214/10.42.174.214:2181, initiating session
24.8.2017 15:40:1313:40:13.865 [timer-SendThread(10.42.174.214:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Session establishment complete on server 10.42.174.214/10.42.174.214:2181, sessionid = 0x15e14456dc70045, negotiated timeout = 20000
24.8.2017 15:40:1313:40:13.910 [timer] INFO  org.apache.storm.shade.org.apache.zookeeper.ZooKeeper - Session: 0x15e14456dc70045 closed
24.8.2017 15:40:1313:40:13.910 [timer-EventThread] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - EventThread shut down
{noformat}
"
STORM-2699,Put all the version information of third party components into the main pom ,I think it's better to put all the version information of third party components into the main pom for more efficient version control
STORM-2698,Upgrade to newest Mockito and Hamcrest versions,"We are currently depending on Mockito 1.9.5, which is from 2012. I think we should upgrade to the latest version, since some APIs have become a little nicer to work with."
STORM-2697,Failed to cleanup worker when GET worker-user failed,"""2017-08-15 11:25:53,554"" | INFO  | [Thread-4] | Shutting down and clearing state for id f5906569-41db-4c7f-9048-b3c551603fb4. Current supervisor time: 1502767553. State: :not-started, Heartbeat: nil | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,554"" | INFO  | [Thread-4] | Shutting down 136d9652-7b8b-4e3d-8d45-33d72dfe1462:f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | INFO  | [Thread-4] | GET worker-user f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.config (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | WARN  | [Thread-4] | Failed to get worker user for f5906569-41db-4c7f-9048-b3c551603fb4. #<FileNotFoundException java.io.FileNotFoundException: /var/streaming_data/stormdir/workers-users/f5906569-41db-4c7f-9048-b3c551603fb4 (No such file or directory)> | backtype.storm.config (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | WARN  | [Thread-4] | Failed to cleanup worker f5906569-41db-4c7f-9048-b3c551603fb4. Will retry later #<IllegalArgumentException java.lang.IllegalArgumentException: User cannot be blank when calling worker-launcher.> | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | INFO  | [Thread-4] | Shut down 136d9652-7b8b-4e3d-8d45-33d72dfe1462:f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) "
STORM-2694,Create a listener to handle tuple state changes of the KafkaSpout,"We had a couple of use cases where we needed the KafkaSpout to put failed tuples into a dead letter queue.

The pull request proposes a listener which is called every time a tuple in the KafkaSpout is emitted/acked/failed/retried.  

"
STORM-2693,Topology submission or kill takes too much time when topologies grow to a few hundred,"Now for a storm cluster with 40 hosts [with 32 cores/128G memory] and hundreds of topologies, nimbus submission and killing will take about minutes to finish. For example, for a cluster with 300 hundred of topologies，it will take about 8 minutes to submit a topology, this affect our efficiency seriously.

So, i check out the nimbus code and find two factor that will effect nimbus submission/killing time for a scheduling round:
* read existing-assignments from zookeeper for every topology [will take about 4 seconds for a 300 topologies cluster]
* read all the workers heartbeats and update the state to nimbus cache [will take about 30 seconds for a 300 topologies cluster]
the key here is that Storm now use zookeeper to collect heartbeats [not RPC], and also keep physical plan [assignments] using zookeeper which can be totally local in nimbus.

So, i think we should make some changes to storm's heartbeats and assignments management.

For assignment promotion:
1. nimbus will put the assignments in local disk
2. when restart or HA leader trigger nimbus will recover assignments from zk to local disk
3. nimbus will tell supervisor its assignment every time through RPC every scheduling round
4. supervisor will sync assignments at fixed time


For heartbeats promotion:
1. workers will report executors ok or wrong to supervisor at fixed time
2. supervisor will report workers heartbeats to nimbus at fixed time
3. if supervisor die, it will tell nimbus through runtime hook
    or let nimbus find it through aware supervisor if is survive 
4. let supervisor decide if worker is running ok or invalid , supervisor will tell nimbus which executors of every topology are ok
"
STORM-2692,Load only configs specific to the topology in populateCredentials,"Theres a single instance of AutoCredentials plugin in Nimbus and right now we load all the config keys in ""populateCredentials"". This can cause issues when multiple topologies are submitted. The second one tries to load the keys of the first topology."
STORM-2691,storm-kafka-client Trident spout implements the Trident interface incorrectly,"The Trident Kafka spout uses the KafkaTridentSpoutTopicPartitionRegistry enum to pass existing topic partitions from the spout to the coordinator. This only works when those components happen to be in the same JVM, because the coordinator gets the topic information from the KafkaConsumer started by KafkaTridentSpoutEmitter. 

The coordinator runs in the TridentSpoutCoordinator bolt here https://github.com/apache/storm/blob/4c8a986f519cdf3e63bed47e9c4f723e4867267a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java#L162, while the spout instances (emitters) run in TridentSpoutExecutors here https://github.com/apache/storm/blob/4c8a986f519cdf3e63bed47e9c4f723e4867267a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java#L176.

We should replace the registry enum with writes to Zookeeper or something similar.

Edit: The fix for this is likely to be a broader change where we split the Subscription API into a few parts so the assignment process can be split across the coordinator and emitter instead of the emitter doing everything."
STORM-2690,resurrect invocation of ISupervisor.assigned() & make Supervisor.launchDaemon() accessible,"As [discussed in STORM-2018|https://issues.apache.org/jira/browse/STORM-2018?focusedCommentId=16108307&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16108307], that change subtly broke the storm-mesos integration framework because of the removal of the invocation of [{{ISupervisor.assigned()}}|https://github.com/apache/storm/blob/v1.0.4/storm-core/src/jvm/org/apache/storm/scheduler/ISupervisor.java#L44].

So this ticket is tracking the reinstatement of that invocation from the supervisor core code.

Also, the [{{launchDaemon()}}|https://github.com/apache/storm/blob/v1.0.4/storm-core/src/jvm/org/apache/storm/daemon/supervisor/Supervisor.java#L248] method of the {{Supervisor}} is not public, so we had to use reflection to allow calling it from the storm-mesos integration.  That should be changed too."
STORM-2689,storm-kafka-examples and storm-kafka-client-examples are difficult for new users to run,"The storm-kafka-examples and storm-kafka-client-examples projects configure their dependencies in a way that makes them difficult to run for a new user. The other example projects set up a provided dependency on storm-client, and otherwise include all their dependencies in a shaded jar. 

storm-kafka(-client) by default produce jars without several necessary dependencies, e.g. the Kafka client libraries. The provided.scope Maven parameter was intended to be used to allow users to produce a shaded jar with all dependencies, but if provided scope is set to compile, the resulting jar will also contain storm-client. This prevents the jar from running on a real cluster.

While users can work around this by producing the slim jar and using --artifacts when submitting the topology, this is unnecessarily tedious. We should just produce a fat jar by default, then mention in the example documentation that --artifacts is there for users that want to make slimmer jars.

Edit:
This issue now includes simplifying storm-kafka-examples and storm-kafka-client-examples in general. The examples demonstrate use of State and DRPC when the focus should be on how to use storm-kafka(-client). It also causes the modules to have some undesirable dependencies, e.g. they both depend on storm-starter. "
STORM-2683,Storm UI - Topology action button error response hardcoded,"Right now on confirming the topology ui action buttons (https://github.com/apache/storm/blob/4966d7a69318d2ca690c47dd43466b03574e5e9e/storm-core/src/ui/public/templates/topology-page-template.html#L607) the UI behaviour is to reload the page always (https://github.com/apache/storm/blob/10d381b303c9176ede0d1260428ad61c7757e396/storm-core/src/ui/public/js/script.js#L169) and in case of an error display a hardcoded error message _""Error while communicating with Nimbus.""_

While this behaviour is okay for workflows with no authorization it gets confusing when the action buttons are put behind some form of authorization. It will be much clearer to define an error message format and have the UI display the correct error message on authorization and other non-nimbus related failures. "
STORM-2682,Supervisor crashes with NullPointerException,"When supervisor is started, it dies after about 30s like so:

{code:java}
...
2017-08-07 17:12:04.606 o.a.s.d.s.Slot main [WARN] SLOT 192.168.10.21:6701 Starting in state EMPTY - assignment null
2017-08-07 17:12:04.607 o.a.s.d.s.Slot main [WARN] SLOT 192.168.10.21:6702 Starting in state EMPTY - assignment null
2017-08-07 17:12:04.607 o.a.s.l.AsyncLocalizer main [INFO] Cleaning up unused topologies in /home/storm/data/supervisor/stormdist
2017-08-07 17:12:04.617 o.a.s.d.s.Supervisor main [INFO] Starting supervisor with id 65a0f977-474c-4938-a4f5-bc99939e96ff at host 192.168.10.
21.
2017-08-07 17:12:04.619 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPrep
arableReporter
2017-08-07 17:12:04.620 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2017-08-07 17:12:04.624 o.a.s.m.StormMetricsRegistry main [INFO] Started statistics report plugin...
2017-08-07 17:12:34.620 o.a.s.e.EventManagerImp Thread-4 [ERROR] {} Error when processing event
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_121]
        at org.apache.storm.localizer.Localizer.updateBlobs(Localizer.java:332) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.updateBlobsForTopology(UpdateBlobs.java:99) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run(UpdateBlobs.java:72) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:54) ~[storm-core-1.0.4.jar:1.0.4]
2017-08-07 17:12:34.620 o.a.s.u.Utils Thread-4 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1750) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:63) ~[storm-core-1.0.4.jar:1.0.4]
2017-08-07 17:12:34.631 o.a.s.d.s.Supervisor Thread-5 [INFO] Shutting down supervisor 65a0f977-474c-4938-a4f5-bc99939e96ff
{code}"
STORM-2680,The switch to turn on-off the cgroup in the doc should be “storm.resource.isolation.plugin.enable”,
STORM-2677,consider all sampled tuples which took greater than 0 ms processing time,"In Storm 1.x , tuples that aren't sampled shouldn't be considered for execute  latency calculations. Need to consider all sampled tuples which took greater than  0 ms  processing time"
STORM-2675,KafkaTridentSpoutOpaque not committing offsets to Kafka,"Every time I restart the topology the spout was picking the earliest message even though poll strategy is set UNCOMMITTED_EARLIEST.  I looked at Kafka's  __consumer_offsets topic to see if spout (consumer) is committing the offsets but did not find any commits. I am not even able to locate the code in the KafkaTridentSpoutEmitter class where we are updating the commits?

    conf.put(Config.TOPOLOGY_DEBUG, true);
    conf.put(Config.TOPOLOGY_WORKERS, 1);
    conf.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 4); //tried with1 as well
    conf.put(Config.TRANSACTIONAL_ZOOKEEPER_ROOT, ""/aggregate"");
    conf.put(Config.TRANSACTIONAL_ZOOKEEPER_SERVERS, Arrays.asList(new String[]{""localhost""}));
    conf.put(Config.TRANSACTIONAL_ZOOKEEPER_PORT, 2181);

 protected static KafkaSpoutConfig<String, String> getPMStatKafkaSpoutConfig() {
    ByTopicRecordTranslator<String, String> byTopic =
        new ByTopicRecordTranslator<>((r) -> new Values(r.topic(), r.key(), r.value()),
            new Fields(TOPIC, PARTITION_KEY, PAYLOAD), SENSOR_STREAM);

    return new KafkaSpoutConfig.Builder<String, String>(Utils.getBrokerHosts(),
        StringDeserializer.class, null, Utils.getKafkaEnrichedPMSTopicName())
            .setMaxPartitionFectchBytes(10 * 1024) // 10 KB
            .setRetry(getRetryService())
            .setOffsetCommitPeriodMs(10_000)
            .setFirstPollOffsetStrategy(FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST)
            .setMaxUncommittedOffsets(250)
            .setProp(""value.deserializer"", ""io.confluent.kafka.serializers.KafkaAvroDeserializer"")
            .setProp(""schema.registry.url"",""http://localhost:8081"")
            .setProp(""specific.avro.reader"",true)
            .setGroupId(AGGREGATION_CONSUMER_GROUP)
            .setRecordTranslator(byTopic).build();
  }

Stream pmStatStream =
        topology.newStream(""statStream"", new KafkaTridentSpoutOpaque<>(getPMStatKafkaSpoutConfig())).parallelismHint(1)

storm-version - 1.1.0"
STORM-2673,For debugging allow users to tell the scheduler which nodes they would prefer,"In some cases with debugging it would be nice to let the user tell the scheduler that it wants to run on host X and not run on host Y.

This is mostly for the case where we saw an odd issue with a topology and it was running on a specific host.  So to unblock the user giving them the ability to avoid a specific host is helpful, at the same time we may want to reproduce the issue and causing the topology to be scheduled on that bad node is helpful. "
STORM-2670,move storm-client-misc to external/http-forwarding-metrics-consumer,"storm-client-misc is not named very well, we should fix it."
STORM-2669,Extend the BinaryEventDataScheme in storm-eventhubs to include MessageId in addition to system properties,"Currently there are two types of EventDataScheme included with the storm-eventhubs spout.

The default is the StringEventDataScheme that emits a single output field, the message itself as a string.

There is an additional BinaryEventDataScheme that passes the message as is, but also has two additional fields: metadata and system_metadata that is passed by eventhubs-client.

The system_metadata only contains the sequence number, offset and enqeued time of an event.

As part of recent requirements by certain applications for tracking an event, they also need the partition id. The partition id is NOT sent by the eventhubs-client, instead the partition manager in the spout already has this information.

The goal of this JIRA is to introduce another output field in BinaryEventDataScheme that contains the MessageId for an event. The messageId will contain: partitionId, sequence number and the offset information for any downstream bolt to be able to locate where the message arrived from.

I will also be fixing any maven checkstyle warnings/errors in the files that I will be committing changes in."
STORM-2668,org.apache.storm.kafka.FailedFetchException,"2017-08-01 11:34:42.446 o.a.s.k.KafkaUtils [ERROR] Error fetching data from [Partition{host=xxx, topic=ABC, partition=5}] for topic [ABC]: [UNKNOWN]
2017-08-01 11:34:42.446 o.a.s.k.KafkaSpout [WARN] Fetch failed
org.apache.storm.kafka.FailedFetchException: Error fetching data from [Partition{host=xxx, topic=ABC, partition=5}] for topic [ABC]: [UNKNOWN]"
STORM-2666,Storm-kafka-client spout can sometimes emit messages that were already committed. ,"Under a certain heavy load, for failed/timeout tuples, the retry service will ack tuple for failed max times. Kafka Client Spout will commit after reached the commit interval. However seems some 'on the way' tuples will be failed again, the retry service will cause Spout to emit again, and acked eventually to OffsetManager.

In some cases such offsets are too many, exceeding the max-uncommit, causing org.apache.storm.kafka.spout.internal.OffsetManager#findNextCommitOffset unable to find next commit point, and Spout for this partition will not poll any more.

By the way I've applied STORM-2549 PR#2156 from Stig Døssing to fix STORM-2625, and I'm using Python Shell Bolt as processing bolt, if this information helps.

resulting logs like below. I'm not sure if the issue has already been raised/fixed, glad if anyone could help to point out existing JIRA. Thank you.


2017-07-27 22:23:48.398 o.a.s.k.s.KafkaSpout Thread-23-spout-executor[248 248] [INFO] Successful ack for tuple message [{topic-partition=kafka_bd_trigger_action-20, offset=18204, numFails=0}].
2017-07-27 22:23:49.203 o.a.s.k.s.i.OffsetManager Thread-23-spout-executor[248 248] [WARN] topic-partition [kafka_bd_trigger_action-18] has unexpected offset [16002]. Current committed Offset [16003]

Edit:
See https://issues.apache.org/jira/browse/STORM-2666?focusedCommentId=16125893&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16125893 for the current best guess at the root cause of this issue.
"
STORM-2664,Fix for the distribution packaged on Windows OS can't run on Linux,"I made a distribution from source code on windows OS, but when I want to launch this on linux. I found there are some mistake for line break in the scripts. And then I found the settings for line ending in the binary.xml was not set obviously, so it is decided by the OS where run the maven command.
I think its better to set the line ending obviously for unix. because we mostly use linux system to deploy storm and, at the same time, this distribution will run well in the windows OS."
STORM-2663,Backport STORM-2558 and deprecate storm.cmd on 1.x-branch,"The storm.cmd script has been removed from master, but it's still very difficult to work with on 1.x-branch, due to several deficiencies compared to storm.py. It is missing certain commands (pacemaker at least) and not setting some Java properties correctly (-DdaemonName and the Java classpath are set incorrectly). Rather than spending time fixing these issues, I'd like to deprecate storm.cmd in future 1.x releases and add the Powershell script as an alternative for Windows users."
STORM-2662,"Fix for STORM-2659 is incomplete, some commands still don't have daemonName set correctly","daemonName must be set for all commands that execute a Java class, it should be an empty string for commands other than those fixed in STORM-2659. "
STORM-2661,JmsSpout should support additional ack mode or should have option for child classes,"Currently , JmsSpout supports 3 acknowledge modes. But some of the queue Providers (like Solace) supports some more ack modes. Like :
Sol_client_ack which acknowledges each message individually to support Guaranteed Delivery.
As JmsSpout's 'jmsAcknowledgeMode' is private and setter has validations, so we can't simply extend JmsSpout(and override required APIs only) to support this additional features. We need to take the complete code.

JmsSpout reference Url :https://github.com/apache/storm/blob/v1.1.0/external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java

So, either there should be support for enhancements by child classes or 
this additional ack mode(which seems generic requirement) should be included in JmsSpout itself - we've some reference code which is working fine. can share the same if required.  
"
STORM-2660,"The Nimbus storm-local directory is relative to the working directory of the shell executing ""storm nimbus""","When the storm.local.dir property is set to a relative directory, it should be interpreted as relative to STORM_HOME. This is how it works for ""storm supervisor"". For ""storm nimbus"" it is instead relative to the working directory of the shell, so running ""storm nimbus"" from STORM_HOME/bin will put a storm-local directory in STORM_HOME/bin/storm-local."
STORM-2659,"storm.cmd does not set -Ddaemon.name, which prevents Log4j2 from logging","When trying to start any daemon from storm.cmd the following error occurs:
{code}
main ERROR Unable to create file E:\apache-storm-1.1.1\logs/access-web-${sys:daemon.name}.log java.io.IOException: The filename, directory name, or volume label syntax is incorrect
{code}

It looks like the daemon-name variable was introduced at some point, but only the storm.py script was updated. This prevents Storm from logging when started from this script."
STORM-2658,Provide storm-kafka-client spout examples,"There are a few example topologies in storm-kafka-client, but trying them out as a new user requires you to modify the storm-kafka-client pom to add shading, then rebuild storm-kafka-client and copy the jar-with-dependencies into Storm's extlib. After that you can take the test jar and run the topology. I think this is needlessly complicated.

We should move the example topologies to examples/storm-kafka-client, and make the storm-kafka-client pom produce a jar with all dependencies. Since we are only including the example source with Storm distributions, I don't see a reason to try to minimize jar size at the cost of adding more steps for the user to try out the examples. storm-starter is a good example of a user friendly example test topology, since it contains all its dependencies. If we want to make the user aware that extlib can be used to reduce jar size, we can add notes and commented out provided scopes to the example pom."
STORM-2652,Exception thrown in JmsSpout open method,"Due to a bug, the message timeout configuration property is read as an integer even though it is of type long.

```
7317 [Thread-18-a-executor[2 2]] ERROR o.a.s.util - Async loop died!
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at org.apache.storm.jms.spout.JmsSpout.open(JmsSpout.java:175) ~[storm-jms-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.executor$fn__4976$fn__4991.invoke(executor.clj:600) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
7320 [Thread-18-a-executor[2 2]] ERROR o.a.s.d.executor - 
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at org.apache.storm.jms.spout.JmsSpout.open(JmsSpout.java:175) ~[storm-jms-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.executor$fn__4976$fn__4991.invoke(executor.clj:600) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
```"
STORM-2651,Executor stopped working after connectivity issues with ZK. Executor is not restarted  by nimbus scheduler. ,"After connectivity issues, nimbus scheduler assigned the appropriate executor to the slots(custom scheduler). 


{code:java}

{panel:title=My title}
o.a.s.d.nimbus [INFO] Setting new assignment for topology id <<topology_name>>-1499356635: #org.apache.storm.daemon.common.Assignment{:master-code-dir ""/opt/storm_datadir"", :node->host {""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" ""test1"", ""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" ""test2"", ""c13b0fc8-d5c1-4335-8339-17b3c048b160"" ""test3"", ""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" ""test4""}, :executor->node+port {[8 8] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [12 12] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [2 2] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [7 7] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [22 22] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [3 3] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [24 24] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [1 1] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [18 18] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [6 6] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [20 20] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [9 9] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [23 23] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [11 11] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [16 16] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [13 13] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [19 19] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [21 21] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [5 5] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [10 10] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [14 14] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [4 4] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [15 15] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [17 17] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702]}, :executor->start-time-secs {[8 8] 1499356646, [12 12] 1499356646, [2 2] 1499356646, [7 7] 1499356646, [22 22] 1499356646, [3 3] 1499356646, [24 24] 1499356646, [1 1] 1499356646, [18 18] 1499356646, [6 6] 1499356646, [20 20] 1499356646, [9 9] 1499356646, [23 23] 1499356646, [11 11] 1499356646, [16 16] 1499356646, [13 13] 1499356646, [19 19] 1499356646, [21 21] 1499356646, [5 5] 1499356646, [10 10] 1499356646, [14 14] 1499356646, [4 4] 1499356646, [15 15] 1499356646, [17 17] 1499356646}, :worker->resources {[""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702] [0.0 0.0 0.0], [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702] [0.0 0.0 0.0], [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703] [0.0 0.0 0.0], [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703] [0.0 0.0 0.0]}}
{panel}

{code}

Then all the executor are started working properly.  

When I checked in-depth I found that,  one of the spout executor has not started and also found  that nimbus stopped logging after this issue.

o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<<topology-name >>-1499356635-stormconf.ser


"
STORM-2650,Add test for non-string property substitution in Flux tests,"When discussing https://issues.apache.org/jira/browse/STORM-2646, it was unclear to me whether Flux was changing the types of properties substituted in via the --filter option. It would be good to add a check for this to the unit tests."
STORM-2649,Update config validation check to give better information,"As part of submitting a topology we need to serialize the config as JSON.  The check right now is rather bad. it just calls Utils.isValidConf and throws some generic Exception about problems.

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L978-L980

It would be much better to have a new function/method that would check if they are equal and if not it would walk through them looking for which configs are different when it finds one it includes which configs and how they are different in the exception."
STORM-2648,Kafka spout can't show acks/fails and complete latency when auto commit is enabled,"The storm-kafka-client spout currently emits tuples with no message ids if auto commit is enabled. This causes the ack/fail/complete latency counters in Storm UI to be 0. In some cases this is desirable because the user may not care, and doesn't want the overhead of Storm tracking tuples. [~avermeerbergen] expressed a desire to be able to use auto commit without these counters being disabled, presumably to monitor topology performance.

We should add a toggle that allows users to enable/disable tuple anchoring in the auto commit case. "
STORM-2646,NimbusClient Class cast exception when nimbus seeds is not an array of hosts,
STORM-2645,update storm.py to be python3 compatible ,"Function emitBolt() emits a map, which is not json serializeble in python3. It should be changed to return a list in order to be json serializeble.
"
STORM-2642,Storm-kafka-client spout cannot be serialized when using manual partition assignment,"The ManualPartitioner interface isn't serializable, which prevents topology submission."
STORM-2641,storm.py and storm.ps1 don't handle errors correctly on Windows,"The powershell script doesn't return the exit code it gets when running the python script. This causes it to always return 0.

When the python script executes a java class on Windows, it throws away the subprocess output if there's an error. This makes it difficult to figure out why the process failed (e.g. ""storm jar"" with an incorrect class name)"
STORM-2640,"Deprecate KafkaConsumer.subscribe APIs on 1.x, and make KafkaConsumer.assign the default","I thought it made sense to have a separate issue for this, so we can list the deprecation in the 1.2.0 changelog, and the removal separately in 2.0.0"
STORM-2639,Kafka Spout incorrectly computes numCommittedOffsets due to voids in the topic (topic compaction),"This is a followup to STORM-2505 to fix a minor issue with the computation of numUncommittedOffsets in OffsetManager. 
"
STORM-2635,Deep log search doesn’t work when there’s no topology in topology history,"When doing deep search with cluster which doesn't ever killed topology (hence no history on topology histories), Nimbus throws error on getting topology histories."
STORM-2625,KafkaSpout is not calculating uncommitted correctly,"This happens when:
1. KafkaSpout has already committed offsets to a topic before, and is not running/activated now;
2. There're messages in topic after the committed offsets;
3. The same consumer group topology with multi works is started/activated again;

The same issue may happen when running topology gets consumer group partition re-assignment with offsets not being able to be committed in time.

The underlying issue is:

a. Because workers are registering kafka consumers one by one, when the first consumer A registers itself with kafka broker with the consumer group, it's assigned all the partitions, say partition 0 & 1. Consumer A then retrieves messages from all the assigned partitions if possible, and started processing. With every tuple KafkaSpout A emits, UNCOMMITTED count numUncommittedOffsets++ (KafkaSpout#emitTupleIfNotEmitted());

b. At this point a second consumer B registers with the broker for the same consumer group. the broker then re-assigns the partitions among existing consumers, say consumer A is assigned partition 0, and consumer B assigned partition 1. 

b.1 At this point KafkaSpout A will try committing acked offsets, and remove the partition 1 offsets it's tracking (KafkaSpout.KafkaSpoutConsumerRebalanceListener#onPartitionsRevoked()); However because the tuples are not all acked, KafkaSpout is not able to commit full list of offsets to kafka broker.

b.2 Then KafkaSpout A will remove tracked partition 1 offsets in offsetManagers as well as emitted (
org.apache.storm.kafka.spout.KafkaSpout.KafkaSpoutConsumerRebalanceListener#onPartitionsAssigned()
org.apache.storm.kafka.spout.KafkaSpout.KafkaSpoutConsumerRebalanceListener#initialize()), resulting the not acked tuples won't be acked for ever (org.apache.storm.kafka.spout.KafkaSpout#ack()), also the UNCOMMITTED count numUncommittedOffsets will never be reduced back to a correct result.

"
STORM-2624,Kafka Storm Spout: Got fetch request with offset out of range,"If partition offset is out of range then kafka spout stops emitting new messages and keeps logging following warning:
2016-10-26 11:11:31.070 o.a.s.k.KafkaUtils [WARN] Partition{host=somehost.org:9092, topic=my-topic, partition=0} Got fetch request with offset out of range: [3]
2016-10-26 11:11:31.078 o.a.s.k.KafkaUtils [WARN] Partition{host=somehost.org:9092, topic=my-topic, partition=0} Got fetch request with offset out of range: [3]
...

I believe the trivial fix is in PartitonManager.java in fill method 
line 237:
{code:java}
            long partitionLatestOffset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.LatestTime());
            if (partitionLatestOffset < offset) {
                offset = partitionLatestOffset;
            } else {
                offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.EarliestTime());
            }
{code}
change to:
{code:java}
            offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, _spoutConfig.startOffsetTime);
{code}

line 259:
{code:java}
            if (offset > _emittedToOffset) {
                _lostMessageCount.incrBy(offset - _emittedToOffset);
                _emittedToOffset = offset;
                LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
            }
{code}
change to:
{code:java}
            if (offset > _emittedToOffset) {
                _lostMessageCount.incrBy(offset - _emittedToOffset);
            }
            _emittedToOffset = offset;
            LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
{code}
"
STORM-2621,STORM-2557 broke sojourn time estimation,"STORM-2557 updated the arrival_rate disruptor queue metric to go off of individual tuples, and not batches of tuples like it did before.  But the sojourn time is also calculated from the arrival rate and the population.  But the population is computed based off of slots, not tuples.  So it is now off proportionally to how many tuples are in a given slot.

We either need a way to compute the tuple population and use that (which would be a new metric), or we need to keep the old arrival rate metric around to, just for the sojourn time.

I think the first one is more likely to work out."
STORM-2617,log4j2 RollingFile rotation failing,"I noticed that the default log rotation configuration isn't working as expected.  A specific example is worker.log.  Here is the default log4j2 configuration for worker.log (log4j2/worker.xml):
{code:xml}
    <RollingFile name=""A1""
                fileName=""${sys:workers.artifacts}/${sys:storm.id}/${sys:worker.port}/${sys:logfile.name}""
                filePattern=""${sys:workers.artifacts}/${sys:storm.id}/${sys:worker.port}/${sys:logfile.name}.%i.gz"">
        <PatternLayout>
            <pattern>${pattern}</pattern>
        </PatternLayout>
        <Policies>
            <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
        </Policies>
        <DefaultRolloverStrategy max=""9""/>
    </RollingFile>
{code}

Even thought the DefaultRolloverStrategy is set to 9, I only ever see worker.log and worker.log.1.gz.  It seems like rotation is continually overwriting worker.log.1.gz .  I expect this is either an issue with log4j2 or an issue related to the RollingFileAppender and the associated filePattern."
STORM-2611,a batched kafkaspout with offsets in zookeeper,"There are some issues with org.apache.storm.kafka.spout.KafkaSpout.
1. When the topology is running in multi workers in different supervisors, it is very often to trigger kafkaspout rebalance. And so the streaming is not stable. And it will cause massive retransmission of lost packets.
2. When max.uncommitted.offsets is less than 200000 (for limited flow), sometimes there is deadlock. The phenomenon is the heartbeat between spout and kafka can not be performed.
3. When the data is from storm to hbase,  batch is used to improve writing productivity. So using batch from spout to bolt is better for special scene.
4. So a batched kafkaspout and bolt with offsets in zookeeper will be valuable."
STORM-2610,Spout throtteling metrics are unusable,"When helping someone debug an issue with backpressure I realized that the metrics we are collecting in the spout are mistakenly being multiplied by the rate, even though we are not sub-sampling them.  This results in the values being, by default, 20 times higher then they should be.  Thinking about how I would use the metrics to debug an issue also showed that some of them.  skipped-max-spout and skipped-throttle correspond to about 1 ms of sleep, but skipped-inactive corresponds to about 100 ms of sleep.  And the 1 ms sleep is configurable so it could be different from one topology to another, and even the code around it is pluggable, so it could be doing anything from not sleeping to sleeping a random amount of time.

I think we just need to scrap what we have been doing and record how long we sleep for and use that as the metric instead.

These metrics also don't appear to be documented anywhere so I am going to change what they mean and document them to actually be useful, and correct."
STORM-2609,We need a drpc-client command line.,"We have no simple way to send a DRPC request from the command line.  We really should have one for debugging/testing at a minimum, and also as an example of what works to use DRPC."
STORM-2608,Out Of Range Offsets Should Be Removed From Pending Queue,"There is a bug that occurs when failed tuples are invalidated due to Kafka throwing a {{TopicOffsetOutOfRangeException}}. 

Below is what happens:

- Spout emits tuples
- Offsets are added to the _pending_ tree
- Some tuples fail and are added to the _failedMsgRetryManager_
- On the next fetch request, a {{TopicOffsetOutOfRangeException}} is thrown and the new offset is _after_ the offset that are currently sitting in both the _pending_ tree and the _failedMsgRetryManager_
- All offsets smaller than the the new offset are removed from the _failedMsgRetryManager_ but *not* the _pending_ tree.
- Since those offsets were removed from the _failedMsgRetryManager_ they will never be retried and thus never get removed from _pending_
- {{lastCommittedOffset()}} will always return the same value which means that offset in zookeeper for that partition will never get updated.
"
STORM-2607,[kafka-client] Consumer group every time with lag 1,"When i put a message a partition, the storm-kafka-client consume this message.
But storm-kafka-client commit the offset -1.


storm-kafka-client: 1.1.0
storm-core : 1.1.0
kafka: 0.10.2.0

Steps to bug

#1 - Insert message in kafka
#2 - Read with storm Spout this topic
#3 - Get the offset for the consumer group and the offset is always offset -1

The KafkaSpoutConfig

protected static KafkaSpoutConfig<String, String> newKafkaSpoutConfig() {
		return KafkaSpoutConfig.builder(""192.168.57.11:9092"", ""topic"").
                          setGroupId(""storm"").setOffsetCommitPeriodMs(10_000).
                          setMaxUncommittedOffsets(100_0000).setRetry(newRetryService())
                          .build();
	}

"
STORM-2606,Bolt execute() called many times ( 2 ~ 4 times ),"Hello~

I am getting some problem.
The problem is that My Develop Bolt execute method is called over twice..
The Bolt Function is logging to HBASE.. So.. If execute method is called twice over, The Data is logging to HBASE twice over..

It is disaster in my project..

Please Recommand this problem to me.."
STORM-2603,url encoding issue when submit topology name with space,"How to reproduce: 
Use 1.0.3
LocalCluster lc = new LocalCluster();
lc.submitTopology(""Hello Storm"", conf, tb.createTopology());

I got following error:
Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/var/folders/1t/3jbxpldd5dgd34fvtd74mc6r0000gp/T/f1e5f859-895c-4767-b586-cd84ba2d0fc7/supervisor/stormdist/Hello%20Storm-1-1498499584/stormconf.ser' does not exist

I looked at my directory: I have the directory Hello+Storm-1-1498499584 instead of Hello%20Storm-1-1498499584

full log file and full code: https://github.com/richardxin/hello_storm/blob/master/err.log

"
STORM-2600,Improve or replace storm-kafka-monitor,"The storm-kafka-monitor module, which is used by Storm UI to show offset lag for topologies with Kafka spouts, has some shortcomings:

* The Storm UI integration code doesn't seem to be able to support topic subscriptions that change after topology submission. The UI code (https://github.com/apache/storm/blob/64e29f365c9b5d3e15b33f33ab64e200345333e4/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java#L91) gets the topic list it should request offset lag for via the spout's getComponentConfiguration method, as far as I can tell through this call https://github.com/apache/storm/blob/9e31509d47c4e91c1009f55c7ccf321d7d7e63aa/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java#L541. It seems like the component configuration is intended to be static once the topology has started running. This prevents us from showing the right topic list for subscriptions that are not known at submission time, which is currently the case for Pattern subscriptions. The topic list for that type of subscription isn't known until the spout has started the KafkaConsumer in {{ISpout.open()}}. I don't see a way to fix this, unless there is some way to update the component configuration when the subscription changes.
* The jar is installed along with the cluster, and depends on the Kafka version specified in Storm's root POM. Kafka guarantees backwards compatible client-server communication for one release only, so there's a potential coupling between Storm cluster version and Kafka version. If users want to update the Kafka version in storm-kafka-monitor, they have to rebuild that module and replace the jar in their Storm install.
* The UI integration uses the storm-kafka-monitor Bash script to start the monitoring code, in order to avoid a dependency between storm-core and storm-kafka-monitor. This prevents the UI integration from working on Windows. We could supply a Windows script as well, but then we'd need to keep the two in sync.

I am wondering if these problems could be solved by implementing offset lag monitoring via the metrics system instead. The spout could periodically seek to the log end offset and submit a metric for how far behind the committed offset is, then seek back to where it left off.
"
STORM-2599,"BasicContainer.getWildcardDir tries to resolve the wildcard character with Paths.get, which prevents workers from booting on Windows","STORM-2191 shortens the worker classpath by substituting in wildcards for the full list of jars. The path is constructed using Paths.get(dir, ""*""), but this doesn't work on Windows. It seems like Windows checks that the path is valid. 

{code}
Paths.get(new File(""."").toString(), ""*"");

Exception in thread ""main"" java.nio.file.InvalidPathException: Illegal char <*> at index 2: .\*
	at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182)
	at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153)
	at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)
	at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)
	at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)
	at java.nio.file.Paths.get(Paths.java:84)
{code}

Paths doesn't guarantee support for globs, and we don't want the OS to examine the path in any case, since the wildcard isn't a ""real"" wildcard (including all files) but a special syntax for including jars in the Java classpath. The path should be constructed with String concatenation instead."
STORM-2596,Storm Worker not reconnect the Netty Client,"I have report the simliar bugs at [STORM-2561|https://issues.apache.org/jira/browse/STORM-2561] on the version of 0.10.1.

And these days I upgrade the storm to 1.1.0, but today the bug is appeared agagin.

The worker.log shows
{code:java}
$ cat worker.log|grep '10.24.40.254:6812'|more
2017-06-22 15:14:25.295 o.a.s.m.n.Client main [INFO] creating Netty Client, connecting to 10.24.40.254:6812, bufferSize: 5242880
2017-06-23 11:23:32.570 o.a.s.m.n.StormClientHandler client-worker-1 [INFO] Connection to /10.24.40.254:6812 failed:
2017-06-23 11:23:35.654 o.a.s.m.n.Client refresh-connections-timer [INFO] closing Netty Client Netty-Client-/10.24.40.254:6812
2017-06-23 11:23:35.655 o.a.s.m.n.Client refresh-connections-timer [INFO] waiting up to 600000 ms to send 0 pending messages to Netty-Client-/10.24.40.254
:6812
2017-06-23 14:57:03.352 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 14:57:59.777 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 14:59:16.038 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:01:27.092 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:04:08.654 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:06:59.777 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
{code}
The worker close the netty client on 2017-06-23 11:23:35.654, and never start the netty client. So the messages later on that worker are been discarded.
 
On that time Storm Node(10.24.40.254:6812) is OOM.

{code:java}
2017-06-23 11:22:59.623 g.a.s.s.t.SolrPersistApi pool-10-thread-8 [INFO] write 200 doc at:invoketrace success cost 228060
2017-06-23 11:22:59.625 g.a.s.s.t.SolrPersistApi pool-10-thread-5 [INFO] write 66 doc at:invoketrace success cost 226739
2017-06-23 11:22:59.626 g.a.s.s.t.SolrPersistApi pool-10-thread-7 [INFO] write 200 doc at:invoketrace success cost 167869
2017-06-23 11:23:32.242 STDERR Thread-2 [INFO] java.lang.OutOfMemoryError: Java heap space
2017-06-23 11:23:32.253 STDERR Thread-2 [INFO] Dumping heap to artifacts/heapdump ...
@
{code}

"
STORM-2595,Apply new code style to storm-solr,
STORM-2594,Apply new code style to storm-rocketmq,
STORM-2593,Apply new code style to storm-redis,
STORM-2592,Apply new code style to storm-pmml,
STORM-2591,Apply new code style to storm-opentsdb,
STORM-2590,Apply new code style to storm-mqtt,
STORM-2589,Apply new code style to storm-mongodb,
STORM-2588,Apply new code style to storm-metrics,
STORM-2587,Apply new code style to storm-kinesis,
STORM-2586,Apply new code style to storm-kafka-monitor,
STORM-2585,Apply new code style to storm-kafka,
STORM-2584,Apply new code style to storm-jms,
STORM-2583,Apply new code style to storm-jdbc,
STORM-2582,Apply new code style to storm-hive,
STORM-2581,Apply new code style to storm-hdfs,
STORM-2580,Apply new code style to storm-hbase,
STORM-2579,Apply new code style to storm-eventhubs,
STORM-2578,Apply new code style to storm-elasticsearch,
STORM-2577,Apply new code style to storm-druid,
STORM-2576,Apply new code style to storm-caasandra,
STORM-2575,Apply new code style to storm-autocreds,
STORM-2574,Apply new code style to storm-sql,
STORM-2573,Apply new code style to flux,
STORM-2572,Apply new code style to storm-submit-tools,
STORM-2571,Apply new code style to storm-webapp,
STORM-2570,Apply new code style to storm-core,
STORM-2569,Apply new code style to storm-client-misc,
STORM-2568,'api/vi/topology/:id/lag' returns empty json {},"Hello

I've tried to use storm-kafka-monitor, and it works fine on command line If I changed 'toollib/storm-kafka-monitor-*.jar' to 'toollib/storm-kafka-monitor-1.1.0.jar'.

{code}
{""my-kafka-topic-name"":{""0"":{""consumerCommittedOffset"": 74804998, ""logHeadOffset"": 74805483, ""lag"": 485},""1"":{""consumerCommittedOffset"": 74804998, ""logHeadOffset"": 74805485, ""lag"": 487},""2"":{""consumerCommittedOffset"": 74804995, ""logHeadOffset"": 74805485, ""lag"": 490},""3"":{""consumerCommittedOffset"": 74805001, ""logHeadOffset"": 74805488, ""lag"": 487},""4"":{""consumerCommittedOffset"": 74805011, ""logHeadOffset"": 74805484, ""lag"": 473},""5"":{""consumerCommittedOffset"": 74805009, ""logHeadOffset"": 74805485, ""lag"": 476},""6"":{""consumerCommittedOffset"": 74805008, ""logHeadOffset"": 74805483, ""lag"": 475},""7"":{""consumerCommittedOffset"": 74805010, ""logHeadOffset"": 74805484, ""lag"": 474},""8"":{""consumerCommittedOffset"": 73641446, ""logHeadOffset"": 74805488, ""lag"": 1164042},""9"":{""consumerCommittedOffset"": 73641448, ""logHeadOffset"": 74805489, ""lag"": 1164041},""10"":{""consumerCommittedOffset"": 73641443, ""logHeadOffset"": 74805483, ""lag"": 1164040},""11"":{""consumerCommittedOffset"": 73641445, ""logHeadOffset"": 74805487, ""lag"": 1164042},""12"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805486, ""lag"": 483},""13"":{""consumerCommittedOffset"": 74804999, ""logHeadOffset"": 74805482, ""lag"": 483},""14"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805483, ""lag"": 481},""15"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805484, ""lag"": 482},""16"":{""consumerCommittedOffset"": 74804994, ""logHeadOffset"": 74805482, ""lag"": 488},""17"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805489, ""lag"": 487},""18"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805488, ""lag"": 485},""19"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805489, ""lag"": 486}}}
{code}

but it gives empty result when I call below api.

{code}
/api/v1/topology/:id/lag
...

{
""MySpoutName"": {
""spoutLagResult"": {},
""spoutId"": ""MySpoutName"",
""spoutType"": ""KAFKA""
}
}
{code}

-I think that needs to fix ""groupid"" to ""group.id"" in TopologySpoutLag.java I debug it, but groupid is right.-

the reason was topics has square brackets in command. 



{code}
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] json configuration: {config.security.protocol=null, config.bootstrap.servers=kafka.xxx.com:9092, config.topics=[my-kafka-topic], config.groupid=my-storm-kafka-spout-groupid, topology.tasks=5}

2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] /my/program/storm/bin/storm-kafka-monitor
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -t
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] [my-kafka-topic]
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -g
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] my-storm-kafka-spout-groupid
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -b
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] kafka.xxx.com:9092
{code}

the square brackets automatically added because of this
{code}
package org.apache.storm.kafka.spout;
public class NamedSubscription extends Subscription {
...
    @Override
    public String getTopicsString() {
        return String.valueOf(topics);
    }
{code}
topics is Collections. so String.valueOf returns value with square brackets.

I fixed the code that remove square brackets in TopologySpoutLag.java for my case. 
but I think that fixing 'getTopicsString of NamedSubscription.java in org.apache.storm.kafka.spout' is might be better. 
"
STORM-2567,Apply new code style to storm-server,"Put effort to reduce max allowed violation count greatly, ideally 0, but even can't get rid of all, do as many as possible."
STORM-2566,Apply new code style to storm-client,"Put effort to reduce max allowed violation count greatly, ideally 0, but even can't get rid of all, do as many as possible."
STORM-2565,Apply new code style to current codebase,"We've introduced code style and also introduced checkstyle, but also set max allowed violation count for each modules to let build pass.

We should put effort to reduce max allowed violation count greatly, ideally 0, but even we can't get rid of all, we should do as many as possible."
STORM-2560,Storm-Kafka on CDH 5.11 with kerberos security enabled.,"Hi,
 
I have installed Apache Storm 1.1.0 manually on CDH 5.11 cluster. This cluster is secured with kerberos. 
I have storm sample written which ingest data from kafka topic and inserts into HDFS directory in real time. So, this sample uses storm-kafka as well as storm-hdfs. 
When I run the storm topology it gives the following error in kafka-spout.

 {color:#d04437}2017-06-18 22:29:31.297 o.a.z.ClientCnxn Thread-14-kafka-spout-executor[5 5]-SendThread(localhost:2181) [INFO] Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error){color}
 
{color:#d04437}2017-06-18 22:29:31.571 k.c.SimpleConsumer Thread-14-kafka-spout-executor[5 5] [INFO] Reconnect due to error:
java.nio.channels.ClosedChannelException: null
        at kafka.network.BlockingChannel.send(BlockingChannel.scala:110) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:85) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) [stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) [stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:129) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) [storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]{color}
 
Kafka version: 2.1.1-1.2.1.1.p0.18
 
There is no storm-kafka*.jat present in - ""/usr/local/storm""
But this sample was workin fine before kerberizing the cluster, even in this case.
 
 
I have tried the same example on Hortonworks and after adding the below code to set security protcol, the topology runs fine:
*spoutConfig.securityProtocol = ""SASL_PLAINTEXT"";*
After Adding above code in case of cloudera it gives error: ""Symbol not found""
 
Please let me know if you nedd any other information...
Thanks in advance.."
STORM-2554,Trident Kafka Spout Refactoring to Include Manual Partition Assignment,Incorporate changes done in STORM-2541 and do some refactoring to internal state partition management to make it cleaner and more properly handle partitions reassignment.
STORM-2553,JedisCluster does not support password,"Now jediscluster for storm-redis is based on jedis-2.8.1, which does not support password for cluster. We can update to jedis-2.9.0 and do some modify for JedisClusterConfig and JedisCommandsContainerBuilder to support password for cluster."
STORM-2550,Supervisor dies if a worker dies in Windows,"When both the Supervisor and Workers are running on a Windows Server 2012 R2, killing a worker using the task manager will cause the Supervisor to die a while after.

It's important to mention that the supervisor.log shows nothing to explain this before it dies. Here are the final logs:

{code:java}
2017-06-13 11:28:40.460 o.a.s.u.Utils SLOT_6705 [INFO] Error when trying to kill 16152. Process is probably already dead.
2017-06-13 11:28:40.460 o.a.s.u.Utils SLOT_6700 [INFO] Error when trying to kill 12048. Process is probably already dead.
2017-06-13 11:28:40.461 o.a.s.u.Utils SLOT_6704 [INFO] Error when trying to kill 6796. Process is probably already dead.
2017-06-13 11:28:40.476 o.a.s.u.Utils SLOT_6709 [INFO] Error when trying to kill 6264. Process is probably already dead.
2017-06-13 11:28:40.673 o.a.s.u.Utils SLOT_6706 [INFO] Error when trying to kill 3932. Process is probably already dead.
2017-06-13 11:28:40.674 o.a.s.u.Utils SLOT_6707 [INFO] Error when trying to kill 1324. Process is probably already dead.
2017-06-13 11:28:42.144 o.a.s.d.s.Container SLOT_6708 [INFO] Found 3868 running as SYSTEM, but expected it to be root
2017-06-13 11:28:42.144 o.a.s.d.s.Slot SLOT_6708 [WARN] SLOT 6708 all processes are dead...
{code}
"
STORM-2549,"The fix for STORM-2343 is incomplete, and the spout can still get stuck on failed tuples","Example:
Say maxUncommittedOffsets is 10, maxPollRecords is 5, and the committedOffset is 0.
The spout will initially emit up to offset 10, because it is allowed to poll until numNonRetriableTuples is >= maxUncommittedOffsets
The spout will be allowed to emit another 5 tuples if offset 10 fails, so if that happens, offsets 10-14 will get emitted. If offset 1 fails and 2-14 get acked, the spout gets stuck because it will count the ""extra tuples"" 11-14 in numNonRetriableTuples.

An similar case is the one where maxPollRecords doesn't divide maxUncommittedOffsets evenly. If it were 3 in the example above, the spout might just immediately emit offsets 1-12. If 2-12 get acked, offset 1 cannot be reemitted.

The proposed solution is the following:
* Enforce maxUncommittedOffsets on a per partition basis (i.e. actual limit will be multiplied by the number of partitions) by always allowing poll for retriable tuples that are within maxUncommittedOffsets tuples of the committed offset. Pause any non-retriable partitions if the partition has passed the maxUncommittedOffsets limit, and some other partition is polling for retries while also at the maxUncommittedOffsets limit. 

Example of this functionality:
MaxUncommittedOffsets is 100
MaxPollRecords is 10
Committed offset for partition 0 and 1 is 0.
Partition 0 has emitted 0
Partition 1 has emitted 0...95, 97, 99, 101, 103 (some offsets compacted away)
Partition 1, message 99 is retriable
We check that message 99 is within 100 emitted tuples of offset 0 (it is the 97th tuple after offset 0, so it is)
We do not pause partition 0 because that partition isn't at the maxUncommittedOffsets limit.
Seek to offset 99 on partition 1 and poll
We get back offset 99, 101, 103 and potentially 7 new tuples. Say the lowest of these is at offset 104.
The spout emits offset 99, filters out 101 and 103 because they were already emitted, and emits the 7 new tuples.
If offset 104 (or later) become retriable, they are not retried until the committed offset moves. This is because offset 104 is the 101st tuple emitted after offset 0, so it isn't allowed to retry until the committed offset moves."
STORM-2548,Simplify KafkaSpoutConfig,"Some suggestions for simplifying KafkaSpoutConfig off the mailing list:

* We should not duplicate properties that users would normally set in the KafkaConsumer properties map. We should just have a setter (setProp) for setting properties in that map. For instance, setGroupId is just duplicating a setting that the user should be able to set directly in the consumer properties.

* We should get rid of the key/value deserializer setters. Setting the deserializers as classes is something the user can just as well do by using setProp. The SerializableDeserializer class should be removed. It is only offering extra type safety in the case where the user is defining their own deserializer type, and has the opportunity to subclass SerializableDeserializer. The setters don't work with the built in Kafka deserializers."
STORM-2546,Kafka spout can stall / get stuck due to edge case with failing tuples,"The mechanism for replaying a failed tuple involves seeking the kafka consumer to the failing offset and then re-emitting it into the topology. A tuple, when emitted the first time, will have an entry created in OffsetManager. This entry will be removed only after the tuple is successfully acknowledged and its offset successfully committed. Till then, commits for offsets beyond the failing offset for that TopicPartition will be blocked.

It is possible that when the spout seeks the consumer to the failing offset, the corresponding kafka message is not returned in the poll response. This can happen due to that offset being deleted or compacted away. In this scenario that partition will be blocked from committing and progressing."
STORM-2544,Bugs in the Kafka Spout retry logic when using manual commit,"Situation: Spout configured to use manual commit with a finite number of retries.

In the above scenario if and when a tuple fails repeatedly and hits the retry limit, it will neither be scheduled for an attempt again nor properly accounted for in the ack() method and in OffsetManager. This will block commits for the partition that the tuple belongs to."
STORM-2542,Deprecate storm-kafka-client KafkaConsumer.subscribe API subscriptions on 1.x and remove them as options in 2.x,"Most of this is copied off the mailing list post:

We've recently seen some issues raised by users using the default subscription API in the new KafkaSpout (https://issues.apache.org/jira/browse/STORM-2514, https://issues.apache.org/jira/browse/STORM-2538).

A while ago an alternative subscription implementation was added (https://github.com/apache/storm/pull/1835), which uses the KafkaConsumer.assign API instead.

The {{subscribe}} API used by default causes Kafka to assign partitions to available consumers automatically. It allows a consumer group to keep processing even in the presence of crashes because partitions are reassigned when a consumer becomes unavailable.

The {{assign}} API used in the alternative subscription implementation leaves it up to the consuming code to figure out a reasonable partition distribution among a consumer group. The {{assign}} API is essentially equivalent to how the old storm-kafka spout distributes partitions across spout instances, and as far as I know it has worked well there.

Storm already ensures that all spout instances are running, and restarts them if they crash, so we're not really gaining much by using the subscribe API.

The disadvantages to using the subscribe API are:

* Whenever an executor crashes, the Kafka cluster reassigns all partitions. This causes all KafkaSpout instances in that consumer group to pause until reassignment is complete.

* The partition assignment is random, so it is difficult for users to predict which partitions are assigned to which spout task.

* The subscribe API is extremely likely to cause hangs and other weird behavior if the KafkaSpout is configured to run multiple tasks in an executor. When KafkaConsumer.poll is called during partition reassignment, it will block until the reassignment is complete. If there are multiple consumers in a thread, the first consumer to get called will block, and the other consumer will get ejected from the list of active consumers after a timeout, because it didn't manage to call poll during the rebalance. See the example code in https://issues.apache.org/jira/browse/STORM-2514, which runs two KafkaConsumers in one thread. The result is that they flip flop between being active, and most polls take ~30 seconds (the Kafka session timeout)

* The random assignment of partitions causes more message duplication than is necessary. When an executor crashes, all the other executors have their partitions reassigned. This makes it likely that some of them will lose a partition they had in-flight tuples on, which they will then be unable to commit to Kafka. The message is then reemitted by whichever KafkaSpout instance was assigned the partition. See https://issues.apache.org/jira/browse/STORM-2538

I'd like to drop support for the subscribe API, and switch to using the assign API by default.

The KafkaConsumer Javadoc even mentions applications like Storm as a case where the {{subscribe}} API doesn't really add value.

{quote}
If the process itself is highly available and will be restarted if it fails (perhaps using a cluster management framework like YARN, Mesos, or AWS facilities, or as part of a stream processing framework). In this case there is no need for Kafka to detect the failure and reassign the partition since the consuming process will be restarted on another machine. 
{quote}"
STORM-2541,Manual partition assignment doesn't work,"The manual partition assignment logic in ManualPartitionNamed/PatternSubscription is broken. The spout is unable to start. The subscription needs to call onPartitionsAssigned even if the current assignment is null, otherwise it becomes impossible to initialize the spout. The order of KafkaConsumer.assign and the calls to onPartitionsAssigned/Revoked is also wrong. The assignment must happen first, otherwise onPartitionsAssigned will get an IllegalStateException when it tries to call KafkaConsumer.seek on a partition the consumer is not yet assigned."
STORM-2540,Get rid of window compaction in WindowManager,"Storm's windowing support uses trigger and eviction policies to control the size of the windows passed to WindowingBolts. The WindowManager has a hard coded limit of 100 tuples before tuples will start getting evicted from the window, probably as an attempt to avoid overly huge windows when using time based eviction policies. Whenever a tuple is added to the window, the hard cap is checked, and if the number of tuples in the window exceeds the cap the WindowManager evaluates the EvictionPolicy for the tuples to figure out if some can be removed.

This hard cap is ineffective in most configurations, and has a surprising interaction with the count based policy.

If the windowing bolt is configured to use timestamp fields in the tuples to determine the current time, the WatermarkingXPolicy classes are used. In this configuration, the compaction isn't doing anything because tuples cannot be evicted until the WatermarkGenerator sends a new watermark, and when it does the TriggerPolicy causes the WindowManager to evict any expired tuples anyway.

If the windowing bolt is using the count based policy, compaction has the unexpected effect of hard capping the user's configured max count to 100. If the configured count is less than 100, the compaction again has no effect.

When the bolt is configured to use the tuple arrival time based policy, the compaction only has an effect if there are tuples older than the configured window duration, which only happens if the window happens to trigger slightly late. This can cause tuples to be evicted from the window before the user's bolt sees them. Even when tuples are evicted with the compaction mechanism they are kept in memory until the next time a window is presented to the user's bolt.

I think the compaction mechanism should be removed. The only policy that benefits is the time based policy, and in that case it would be better to just add a configurable max tuple count to that policy. "
STORM-2538,New kafka spout emits duplicate tuples,"Currently, KafkaSpout in storm-kafka-client can cause duplicate tuples to be emitted. Reason is the implementation of ConsumerRebalanceListener interface is called by kafka everytime a new executor comes up. However, on PartitionsRevoked we already have some in flight tuples and are emitting the same ones from the new executor on which the onPartitionsAssigned was called. We need to make sure that we emit only one tuple per kafka message."
STORM-2536,storm-autocreds adds jersey 1.x to worker classpath,"While storm-autocreds module excludes some critical (easy to make version conflict) dependencies but still contains jersey 1.9 which makes conflict against jersey 2.x.

We should exclude jersey and additional unneeded libraries if any."
STORM-2534,"Visualization API missing stats/instances for ""system"" components","The topology visualization api end point ( /api/v1/topology/TOPOLOGY-ID/visualization ) does not return correct ""stats"" values for ""system"" components __system and __acker.

See the following example *correct* response for a spout or bolt within a topology, shorten for brevity.  Under the stats key it lists all of the instances of that component that is deployed.

{code}
{
	""spout"": {
		...
		"":stats"": [{
			"":host"": ""e54bb273-2a8a-4320-b23f-7c7ace52c961-10.153.0.30"",
			"":port"": 6700,
			"":uptime_secs"": 0,
			"":transferred"": {
				...
			}
		}],
		...
	},
{code}

See the following response for the __system and __acker components.  They do *not* correctly list any entries under the stats key.

{code}
{
	""__system"": {
		"":type"": ""spout"",
		"":capacity"": 0,
		"":latency"": null,
		"":transferred"": null,
		"":stats"": [],
		"":link"": ""\/component.html?id=__system&topology_id=test-1-1495630798"",
		"":inputs"": []
	},
	""__acker"": {
		"":type"": ""spout"",
		"":capacity"": 0,
		"":latency"": null,
		"":transferred"": null,
		"":stats"": [],
		"":link"": ""\/component.html?id=__acker&topology_id=test-1-1495630798"",
		"":inputs"": [...]
	}
}
{code}"
STORM-2533,"Visualization API returns ""spout"" for system components","Hitting the visualization api end point ( /api/v1/topology/TOPOLOGY-ID/visualization ) returns system components labeled as spouts.  These should not be labeled as spouts, but instead 'system' or some other more appropriate label.

See the following example response:

{code}
{
	...
	""__system"": {
		"":type"": ""spout"",
		...
	},
	""__acker"": {
		"":type"": ""spout"",
		 ...
	}
}
{code}"
STORM-2532,Get rid of most uses of Utils.getAvailablePort,Utils.getAvailablePort is a racy way to acquire a free port. We should only use it in cases where there is no other option (e.g. tests that need a client to try connecting before the server is started)
STORM-2531,CheckPointState.Action should have a serializer registered,CheckPointStae.Action does is not registered in Kryo so users of stateful topologies must register it.  Like other core classes this should be registered by Storm at startup.
STORM-2529,KeyNotFoundException on topology undeploy,"On undeploying a topology, the following is logged in the nimbus log:

{code}2017-05-23T11:59:17,785 o.a.s.d.nimbus [INFO] Killing topology: <topology>
2017-05-23T11:59:18,284 o.a.s.d.nimbus [INFO] Cleaning up <topology>
2017-05-23T11:59:18,359 o.a.s.d.nimbus [INFO] Removing dependency jars from blobs - 
2017-05-23T11:59:28,498 o.a.s.d.nimbus [INFO] Cleaning up <topology>
2017-05-23T11:59:28,515 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:<topology>-stormcode.ser){code}

This causes the client making the thrift call to nimbus to undeploy the topology to block indefinitely"
STORM-2525,Fix flaky integration tests,"The integration tests fail fairly often, e.g. https://travis-ci.org/apache/storm/jobs/233690012. The tests should be fixed so they're more reliable."
STORM-2523,Trident : Cannot join two streams if only one is passed through a persistant aggregator.,"When i define a topology such :
Spout1 -> persistantAggregator -> Stream ->  join
Spout2 ->  Join (the same one)

Join never produces data.

To produce data i need to add a persistantAggregator in other stream.
This  persistantAggregator aggregate nothing and just repeat input tuple.


I join a java class testing each configuration

thx
                    "
STORM-2522,examples in package do not build with checkstyle issues,"{code}
$ mvn clean package
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ storm-starter ---
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-starter ---
[INFO]
[INFO] --- maven-checkstyle-plugin:2.17:check (validate) @ storm-starter ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1.468 s
[INFO] Finished at: 2017-05-18T15:58:20-05:00
[INFO] Final Memory: 26M/437M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project storm-starter: Failed during checkstyle execution: Unable to find configuration file at location: storm-buildtools/storm_checkstyle.xml: Could not find resource 'storm-buildtools/storm_checkstyle.xml'. -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}"
STORM-2521,"""storm sql"" fails since '--jars' can't handle wildcard","STORM-2191 changes the approach to include jars for classpath: before the patch, storm script lists jars in directory and add them to classpath one by one. After the patch, we just use wildcard which is great to shorten the classpath.

storm-sql-runtime jars use '--jars' to be uploaded to blobstore and '--jars' doesn't support wildcard. So unfortunately, STORM-2191 breaks ""storm sql"".

We should choose whether making --jars supporting wildcard, or have an exceptional case, lists jars in directory only for above case."
STORM-2520,AutoHDFS should prefer cluster-wise hdfs kerberos principal to global hdfs kerberos principal,"After STORM-2482, we can set cluster-wise principal and keytab (and configurations) instead of setting global principal and keytab for HDFS and HBase. 
(Hive will be supported via STORM-2501.)

In AutoHDFS there's a missed spot which always uses global principal, and it throws some errors when global principal is not set.

It should prefer cluster-wise principal to global principal."
STORM-2518,NPE during uploading dependency artifacts with secured cluster,"While adding ACL to USER from uploading artifacts, ""name"" field is actually optional for thrift specification, but Nimbus reads the value without checking null while fixing ACL.

{code}
2017-05-16 14:57:02.527 o.a.s.t.s.TThreadPoolServer pool-45-thread-136 [ERROR] Error occurred during processing of message.
java.lang.NullPointerException: null
        at org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser(BlobStoreAclHandler.java:382) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs(BlobStoreAclHandler.java:357) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableBlobMeta(BlobStoreAclHandler.java:306) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:103) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9064.beginCreateBlob(nimbus.clj:2047) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3430) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3414) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{code}

Uploading artifacts fails and topology submission also fails."
STORM-2517,storm-hdfs writers can't be subclassed,"{{HdfsBolt.makeNewWriter()}} returns an AbstractHDFSWriter instead of an interface. AbstractHDFSWriter is public but its constructor is package-private so it can't actually be subclassed unless your class is in the {{org.apache.storm.hdfs.common}} package. I subclassed HDFSWriter but it required some workarounds.

Also {{AbstractHDFSWriter.offset}} is package-private and write() is final, so there is no way for a subclass to control or update the offset after {{doWrite()}}"
STORM-2516,WindowedBoltExecutorTest.testExecuteWithLateTupleStream is flaky,See https://travis-ci.org/apache/storm/jobs/232571820.
STORM-2515,Fix most checkstyle violations for storm-kafka-client,
STORM-2513,NPE possible in getLeader call,"The getLeader call actually reads data from two different locations

https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L2371-L2385

One is /leader-lock and the other is /nimbuses.  There is a really rare possibility that these two can get out of sync when the leader crashes and we read from leader election saying it is still the leader, but after that it's entry is removed from ZK for /nimbuses.  So we either need to make them not be separate entries, or we need to add in some kind of a retry when this happens.

Also NimbusClient has not retry built in.  Not all operations are idempotent, but we really should look at adding a retry with possibly switching to a new nimbus on idempotent operations."
STORM-2511,Submitting a topology with name containing unicode getting failed.,"
Below error occurs when a topology name contains  unicode characters.

{quote}
$ storm jar WordCountTopology-1.0-SNAPSHOT.jar examples.WordCountTopology ""wordcount-中文topology""

2624 [main] INFO  o.a.s.StormSubmitter - Submitting topology wordcount-中文topology in distributed mode with conf {""storm.zookeeper.topology.auth.scheme"":""digest"",""storm.zookeeper.topology.auth.payload"":""-8594815830934962206:-8598394253140221278"",""topology.workers"":2,""topology.debug"":true}
Exception in thread ""main"" java.lang.RuntimeException: AuthorizationException(msg:wordcount-中文topology-4-1483689231-stormconf.ser does not appear to be a valid blob key)
        at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:255)
        at org.apache.storm.StormSubmitter.submitTopology(StormSubmitter.java:310)
        at org.apache.storm.StormSubmitter.submitTopologyWithProgressBar(StormSubmitter.java:346)
        at org.apache.storm.StormSubmitter.submitTopologyWithProgressBar(StormSubmitter.java:327)
        at com.microsoft.example.KafkaReaderTop.main(KafkaReaderTop.java:39)
Caused by: AuthorizationException(msg:wordcount-中文topology-4-1483689231-stormconf.ser does not appear to be a valid blob key)
        at org.apache.storm.generated.Nimbus$submitTopology_result$submitTopology_resultStandardScheme.read(Nimbus.java:7628)
        at org.apache.storm.generated.Nimbus$submitTopology_result$submitTopology_resultStandardScheme.read(Nimbus.java:7596)
        at org.apache.storm.generated.Nimbus$submitTopology_result.read(Nimbus.java:7530)
        at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
        at org.apache.storm.generated.Nimbus$Client.recv_submitTopology(Nimbus.java:294)
        at org.apache.storm.generated.Nimbus$Client.submitTopology(Nimbus.java:278)
        at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:243)
        ... 4 more
{quote}"
STORM-2510,adjust checkstyle configurations to decrease violations,"Adjust Storm's checkstyle configuration:
* 4-space indent instead of 2-space indent that is default with google_checks.xml
** More hand-written code in Storm is 4-space indented than 2-space indented.
* exclude the thrift generated code from checkstyle
** since we shouldn't be touching it anyways
* go with 120 character line-length limits instead of the default of 100 from google_checks.xml
** This cuts ~70% of the line-length violations.  We might wanna increase it even more.  140 would cut out ~90% of the line-length violations.

With those adjustments, the total number of violations will shrink by ~100,000 (~140,000 -> ~40,000).

We can decrease the existing violations even more if we upgrade the checkstyle version from 6.11.2 to 7.7.   I figured this out after noticing that IntelliJ had different (& fewer) violations when I ran the checkstyle plugin on the same module in IntelliJ as compared to with cmdline maven.  A further benefit of the newer checkstyle version is that it runs *way* faster.  As in ~6+ times faster."
STORM-2509,Writers in AbstractHDFSBolt are not closed/rotated when evicted from WritersMap,"When the eldest entry in the WritersMap in the AbstractHDFSBolt gets removed due to the number of writers exceeding maxWriters (see below), the writer is not closed and rotation actions are not executed (doRotationAndRemoveWriter is not called).

{code}
static class WritersMap extends LinkedHashMap<String, AbstractHDFSWriter> {
    final long maxWriters;

    public WritersMap(long maxWriters) {
        super((int)maxWriters, 0.75f, true);
        this.maxWriters = maxWriters;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<String, AbstractHDFSWriter> eldest) {
        return this.size() > this.maxWriters;
    }
}
{code}"
STORM-2505,Kafka Spout doesn't support voids in the topic (topic compaction not supported),"Kafka maintains the spout progress (offsets for partitions) which can hold a value which no longer exists (or offset+1 doesn't exist) in the topic due to following reasons
* Topology stopped processing (or died) & topic got compacted (cleanup.policy=compact) leaving offset voids in the topic.
* Topology stopped processing (or died) & Topic got cleaned up (cleanup.policy=delete) and the offset.

When the topology starts processing again (or restarted), the spout logic suggests that the next offset has to be (committedOffset+1) for the spout to make progress, which will never be the case as (committedOffset+1) has been removed from the topic and will never be acked.

{code:title=OffsetManager.java|borderStyle=solid}
 if (currOffset == nextCommitOffset + 1) {            // found the next offset to commit
      found = true;
      nextCommitMsg = currAckedMsg;
      nextCommitOffset = currOffset;
} else if (currOffset > nextCommitOffset + 1) {
      LOG.debug(""topic-partition [{}] has non-continuous offset [{}]. It will be processed in a subsequent batch."", tp, currOffset);
}
{code}

A smart forwarding mechanism has to be built so as to forward the spout pivot to the next logical location, instead of a hardcoded single forward operation.
"
STORM-2504,"At the start of topology, the KafkaTridentSpoutOpaque will sometimes emit the first batch twice ","The unit test in the attachment can reproduce the problem, and there is my simple fix."
STORM-2502,Use PID in file name for heap dumps,"The default JVM options for the workers specify the path to use for the heap dumps, see 
[https://github.com/apache/storm/blob/1.x-branch/conf/defaults.yaml#L171], however when a memory error happens more than once for the same worker, only the first dump is kept as the file can't be overridden. Instead, would it make sense to use something like 
_""-XX:HeapDumpPath=artifacts/heapdump_<pid>.hprof""_ so that a different dump is generated for each JVM instance? Or is the current pattern used on purpose to avoid too much disk space being used?"
STORM-2500,waitUntilReady in PacemakerClient cannot be invoked,"Heartbeat fails due to waitUntilReady not being invoked correctly.

storm.yaml includes: storm.cluster.state.store: ""org.apache.storm.pacemaker.pacemaker_state_factory"" and pacemaker.servers: <someip>

Here's a strack trace:
{code}
[executor-heartbeat-timer] ERROR org.apache.storm.pacemaker.pacemaker-state-factory - Failed to set_worker_hb. Will make [6] more attempts.
"" java.lang.IllegalArgumentException: No matching field found: waitUntilReady for class org.apache.storm.pacemaker.PacemakerClient
    at clojure.lang.Reflector.getInstanceField(Reflector.java:271) ~[clojure-1.7.0.jar:?]
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:315) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.pacemaker.pacemaker_state_factory$get_pacemaker_write_client.invoke(pacemaker_state_factory.clj:110) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__12511$fn__12512.invoke(pacemaker_state_factory.clj:187) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$pacemaker_retry_on_exception$fn__12506.invoke(pacemaker_state_factory.clj:139) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$pacemaker_retry_on_exception.invoke(pacemaker_state_factory.clj:139) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__12511.set_worker_hb(pacemaker_state_factory.clj:183) [storm-core-1.1.0.jar:1.1.0]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) [clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) [clojure-1.7.0.jar:?]
	at org.apache.storm.cluster$mk_storm_cluster_state$reify__4395.worker_heartbeat_BANG_(cluster.clj:468) [storm-core-1.1.0.jar:1.1.0]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) [clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$do_executor_heartbeats.doInvoke(worker.clj:76) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:439) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544$fn__5547.invoke(worker.clj:624) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$schedule_recurring$this__1737.invoke(timer.clj:105) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:50) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
{code}"
STORM-2498,Download Full File link broken in 1.x branch,"The download link points to ""download?file=%5BLjava.lang.Object%3B%406d6db3b"" instead of something like ""download?file=wordcount-1-1493298799%2F6701%2Fworker.log""
"
STORM-2497,Support Shared Memory Scheduling in RAS,"In some cases bolt and or spouts can share memory, but the scheduler has not good way to express that.  We should be able to support this."
STORM-2496,Dependency artifacts should be uploaded to blobstore with READ permission for all,"When we submit topology via specific user with dependency artifacts, submitter uploads artifacts to the blobstore with user which runs the submission.

Since uploaded artifacts are uploaded once and shared globally, other user might need to use uploaded artifact. (This is completely fine for non-secured cluster.) In this case, Supervisor fails to get artifact and crashes in result.

{code}
2017-04-28 04:56:46.594 o.a.s.l.AsyncLocalizer Async Localizer [WARN] Caught Exception While Downloading (rethrowing)...
org.apache.storm.generated.AuthorizationException: null
	at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-04-28 04:56:46.597 o.a.s.d.s.Slot SLOT_6701 [ERROR] Error when processing event
java.util.concurrent.ExecutionException: AuthorizationException(msg:<user> does not have READ access to dep-org.apache.curator-curator-framework-jar-2.10.0.jar)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_112]
	at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_112]
	at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:380) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:740) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
Caused by: org.apache.storm.generated.AuthorizationException
	at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-04-28 04:56:46.597 o.a.s.u.Utils SLOT_6701 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
	at org.apache.storm.utils.Utils.exitProcess(Utils.java:1774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
2017-04-28 04:56:46.599 o.a.s.d.s.Supervisor Thread-7 [INFO] Shutting down supervisor 775c158b-0a2d-40be-9e02-a9662d8bc5c4
{code}

So we need to upload artifacts with READ permission to all, or at least supervisor should be able to read them at all."
STORM-2494,KafkaSpout does not handle CommitFailedException,"In situations when tuple processing takes longer than session timeout, we get CommitFailedException and instead of recovering from it Storm worker dies.

{code}
2017-04-26 11:07:04.902 o.a.s.util [ERROR] Async loop died!
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:578) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:519) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:679) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:658) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:426) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:278) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:360) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:192) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:404) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1058) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:219) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) ~[storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-04-26 11:07:04.909 o.a.s.d.executor [ERROR] 
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:578) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:519) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:679) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:658) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:426) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:278) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:360) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:192) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1058) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:404) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:219) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) ~[storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-04-26 11:07:04.953 o.a.s.util [ERROR] Halting process: (\""Worker died\"")
java.lang.RuntimeException: (\""Worker died\"")
\tat org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
\tat org.apache.storm.daemon.worker$fn__5646$fn__5647.invoke(worker.clj:763) [storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.daemon.executor$mk_executor_data$fn__4863$fn__4864.invoke(executor.clj:274) [storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.util$async_loop$fn__557.invoke(util.clj:494) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

2017-04-26 11:07:44.507 o.a.s.k.s.KafkaSpoutRetryExponentialBackoff [DEBUG] Instantiated KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=2, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=10, timeUnit=SECONDS}}
2017-04-26 11:07:44.516 o.a.s.k.s.KafkaSpoutRetryExponentialBackoff [DEBUG] Instantiated KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=0, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=0, timeUnit=MILLISECONDS}}
2017-04-26 11:07:45.048 o.a.s.k.s.KafkaSpout [INFO] Kafka Spout opened with the following configuration: KafkaSpoutConfig{kafkaProps={enable.auto.commit=false, request.timeout.ms=30000, group.id=Group1, bootstrap.servers=192.168.1.143:9092, session.timeout.ms=20000}, key=org.apache.kafka.common.serialization.StringDeserializer@1b5080fd, value=org.apache.kafka.common.serialization.StringDeserializer@2720873b, pollTimeoutMs=200, offsetCommitPeriodMs=5000, maxUncommittedOffsets=1000, firstPollOffsetStrategy=UNCOMMITTED_EARLIEST, subscription=org.apache.storm.kafka.spout.NamedSubscription@7f068c1f, translator=org.apache.storm.kafka.spout.SimpleRecordTranslator@1f1ca6a2, retryService=KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=2, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=10, timeUnit=SECONDS}}}
2017-04-26 11:07:45.111 o.a.s.k.s.KafkaSpout [INFO] Kafka Spout opened with the following configuration: KafkaSpoutConfig{kafkaProps={enable.auto.commit=false, request.timeout.ms=30000, group.id=Group2, bootstrap.servers=192.168.1.143:9092, session.timeout.ms=20000}, key=org.apache.kafka.common.serialization.StringDeserializer@45ffa954, value=org.apache.kafka.common.serialization.StringDeserializer@4b384f9b, pollTimeoutMs=200, offsetCommitPeriodMs=5000, maxUncommittedOffsets=1000, firstPollOffsetStrategy=UNCOMMITTED_EARLIEST, subscription=org.apache.storm.kafka.spout.NamedSubscription@4f07c224, translator=org.apache.storm.kafka.spout.SimpleRecordTranslator@a0545a0, retryService=KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=2, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=10, timeUnit=SECONDS}}}
2017-04-26 11:07:45.297 o.a.s.k.s.NamedSubscription [INFO] Kafka consumer subscribed topics [topic-1]
2017-04-26 11:07:45.302 o.a.s.k.s.NamedSubscription [INFO] Kafka consumer subscribed topics [topic-2]
2017-04-26 11:07:45.456 o.a.s.k.s.KafkaSpout [INFO] Partitions revoked. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@32cbdbb0, topic-partitions=[]]
2017-04-26 11:07:45.463 o.a.s.k.s.KafkaSpout [INFO] Partitions revoked. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@275d5222, topic-partitions=[]]
2017-04-26 11:07:45.545 o.a.s.k.s.KafkaSpout [INFO] Partitions reassignment. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@275d5222, topic-partitions=[topic-1]]
2017-04-26 11:07:45.546 o.a.s.k.s.KafkaSpout [INFO] Partitions reassignment. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@32cbdbb0, topic-partitions=[topic-2]]
2017-04-26 11:07:45.551 o.a.s.k.s.i.OffsetManager [DEBUG] Instantiated OffsetManager{topic-partition=topic-1, fetchOffset=11803, committedOffset=11802, ackedMsgs=[]}
2017-04-26 11:07:45.551 o.a.s.k.s.i.OffsetManager [DEBUG] Instantiated OffsetManager{topic-partition=topic-2, fetchOffset=11801, committedOffset=11800, ackedMsgs=[]}
2017-04-26 11:07:45.552 o.a.s.k.s.KafkaSpout [INFO] Initialization complete
2017-04-26 11:07:45.552 o.a.s.k.s.KafkaSpout [INFO] Initialization complete
{code}

I think expected behaviour would be that KafkaSpout would recover from exception (client will reconnect and get partitions reassigned) without worker getting killed."
STORM-2489,Overlap and data loss on WindowedBolt based on Duration,"The attachment is my test script, one of my test results is:
```
expired=1...55
get=56...4024
new=56...4024
Recived=3969,RecivedTotal=3969
expired=56...4020
get=4021...8191
new=4025...8191
Recived=4171,RecivedTotal=8140
SendTotal=12175
expired=4021...8188
get=8189...12175
new=8192...12175
Recived=3987,RecivedTotal=12127
```
This test result shows that some tuples appear in the expired list directly, we lost these data if we just use get() to get tuples, this is the first bug.
The second: the tuples of get() has overlap, the getNew() seems alright.

The problem not happen definitely, may need to try several times.

Actually, I'm newbie about storm, so I'm not sure this is a bug indeed, or, I use it in wrong way?"
STORM-2488,The UI user Must be HTTP,"The UI user Must be HTTP. Otherwise, the UI page can not be authorized"
STORM-2487,getting com.mongodb.MongoBulkWriteException while trying to save bulk messages using apache storm mongo,"While trying to save bulk numbers of messages by using storm-mongo, we are getting below exception

com.mongodb.MongoBulkWriteException: Bulk write operation error on server mongoserver:27017. Write errors: [BulkWriteError{index=0, code=11000, message='E11000 duplicate key error collection: NextMDC.EMAIL index: _id_ dup key: { : ""22596
079-1260-44f1-b4df-a5857f48f22d"" }', details={ }}].
        at com.mongodb.connection.BulkWriteBatchCombiner.getError(BulkWriteBatchCombiner.java:176) ~[stormjar.jar:?]
        at com.mongodb.connection.BulkWriteBatchCombiner.throwOnError(BulkWriteBatchCombiner.java:205) ~[stormjar.jar:?]
        at com.mongodb.connection.BulkWriteBatchCombiner.getResult(BulkWriteBatchCombiner.java:146) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:190) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:168) ~[stormjar.jar:?]
        at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:230) ~[stormjar.jar:?]
        at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:221) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:168) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:74) ~[stormjar.jar:?]
        at com.mongodb.Mongo.execute(Mongo.java:781) ~[stormjar.jar:?]
        at com.mongodb.Mongo$2.execute(Mongo.java:764) ~[stormjar.jar:?]
        at com.mongodb.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:323) ~[stormjar.jar:?]
        at org.apache.storm.mongodb.common.MongoDBClient.insert(MongoDBClient.java:61) ~[stormjar.jar:?]
        at org.apache.storm.mongodb.bolt.MongoInsertBolt.execute(MongoInsertBolt.java:85) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:464) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]

But for less number of messages it is working fine.
"
STORM-2485,re-include example jars in storm distribution,"Beginning with Apache 1.1.0, it appears the example jar ""examples/storm-starter/storm-starter-topologies-$STORMVERSION.jar"" is no longer included in the distribution.

I maintain a project that has a series of ""sanity tests"" when new versions of projects came out.  The WordCountTopology example in the storm-start topology was the ""sanity test"" we used.

It would certainly be nice to have the jar re-included as it would automatically allow the tests to be run.  I'm sure for others, it is a quick way to try out Storm rather than have to build the jars by hand."
STORM-2484,Flux: support bolt+spout memory configuration,"Storm has features to tune memory and CPU settings on a per-bolt or per-spout basis, with the setMemoryLoad and setCPULoad functions: https://storm.apache.org/releases/1.1.0/javadocs/index.html

Flux doesn't appear to support these features"
STORM-2483,wrong parameters order,"org.apache.storm.utils.Utils#getGlobalStreamId has wrong parameters order:
    
public static GlobalStreamId getGlobalStreamId(String streamId, String componentId) {
        if (componentId == null) {
            return new GlobalStreamId(streamId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(streamId, componentId);
    }

but GlobalStreamId constructor is:   public GlobalStreamId(
    String componentId,
    String streamId)

so i think the nice code is:
    public static GlobalStreamId getGlobalStreamId(String streamId, String componentId) {
        if (streamId == null) {
            return new GlobalStreamId(componentId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(componentId, streamId);
    }
"
STORM-2481,Upgrade Aether version to resolve Aether bug BUG-451566,"I received a report that storm-submit-tools throws NPE.

{code}
 /usr/hdf/current/storm-client/bin/storm: line 2: /usr/hdf/3.0.0.0-179/etc/default/hadoop: No such file or directory
Resolving dependencies on demand: artifacts (['org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12', 'org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12', 'org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12', 'org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12', 'org.apache.storm:storm-druid:1.0.2.3.0.0.0-179', 'org.scala-lang:scala-library:2.11.8']) with repositories (['hwx-public^http://repo.hortonworks.com/content/groups/public/', 'hwx-private^http://nexus-private.hortonworks.com/nexus/content/groups/public/'])
DependencyResolver input - artifacts: org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12,org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12,org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12,org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12,org.apache.storm:storm-druid:1.0.2.3.0.0.0-179,org.scala-lang:scala-library:2.11.8
DependencyResolver input - repositories: hwx-public^http://repo.hortonworks.com/content/groups/public/,hwx-private^http://nexus-private.hortonworks.com/...
Exception in thread ""main"" java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.submit.command.DependencyResolverMain.main(DependencyResolverMain.java:82)
Caused by: java.lang.NullPointerException
        at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
        at org.apache.storm.submit.dependency.DependencyResolver.resolve(DependencyResolver.java:95)
        at org.apache.storm.submit.command.DependencyResolverMain.main(DependencyResolverMain.java:71)
Traceback (most recent call last):
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 884, in <module>
    main()
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 881, in main
    (COMMANDS.get(COMMAND, unknown_command))(*ARGS)
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 295, in jar
    artifact_to_file_jars = resolve_dependencies(DEP_ARTIFACTS_OPTS, DEP_ARTIFACTS_REPOSITORIES_OPTS)
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 182, in resolve_dependencies
    raise RuntimeError(""dependency handler returns non-zero code: code<%s> syserr<%s>"" % (p.returncode, errors))
RuntimeError: dependency handler returns non-zero code: code<1> syserr<None>
{code}

There was also an issue filed to Eclipse bug tracker.
https://bugs.eclipse.org/bugs/show_bug.cgi?id=451566

The issue was fixed for newer version of Aether so we just need to upgrade Aether version to get over."
STORM-2477,Configs should have generics,"Config since the beginning has not really had generics it has just been a Map.  We should really have it be consistent everywhere a {{Map<String, Object>}}

This will reduce the number of warnings in the code base by a lot."
STORM-2475,NimbusInfo does not handle IPv6 addresses,"This is probably fairly minor, but NimbusInfo tries to parse {{host:port}} but I found myself in a situation where I was getting an IPv6 address and not a host name so the "":"" parsing became a problem."
STORM-2473,KafkaTridentSpoutOpaque's implementation is incorrect.,"The coordinator relies on emitter to start subscription, this is in correct since coordinator and emitter may run on different machines."
STORM-2468,Remove Clojure from storm-client,"It would be great to remove clojure as a dependency of storm-client.  We should start looking at moving as much out of storm-client as possible, as the initial separation left some things in there that should not be part of the client, but are not easy to separate just yet."
STORM-2467,Encoding issues in Kafka consumer,"The StringScheme of the storm-kafka consumer does not set an explicit charset in all cases, which leads to messages being decoded in the environment specific default charset. 

I have a PR for that to fix it
https://github.com/apache/storm/pull/2055"
STORM-2460,Test with Storm testings completeTopology and Maven surefire fail,"Running tests that use Storm testings completeTopology fail when running with Maven surefire in some environments.

Some tests are run successfully and it is not always the same phase of tests that fail.

It seems to be issue similar to STORM-130."
STORM-2459,Support SSL for Redis (Jedis 2.9.0),"Jedis 2.9.0 added SSL support.  This helps with connecting to hosted Redis environments, such as in Azure, which are SSL-only by default.

However, the Redis support in Storm doesn't currently expose an option to use this.  I would hope for something like:

JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
    .setHost(host)
    .setPort(port)
    .useSSL(true)
    .build();

Thanks."
STORM-2456,Error when running Flux on Windows in non-elevated command prompt,"Running:
{code}
mvn compile exec:java -Dexec.args=""--local -R /topology.yaml""
{code}

I get the following errors.

It works if I run in an elevated command prompt though.

{code}
17:22:48 [SLOT_1027] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.nio.file.FileSystemException: C:\Users\maurgi\AppData\Local\Temp\0924ce14-5d18-4da2-ab49-abfe37e59742\workers\c487e249-7225-4836-b7f0-b840f1a05732\artifacts: A required privilege is
 not held by the client.
        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsFileSystemProvider.createSymbolicLink(WindowsFileSystemProvider.java:585) ~[?:1.8.0_121]
        at java.nio.file.Files.createSymbolicLink(Files.java:1043) ~[?:1.8.0_121]
        at org.apache.storm.daemon.supervisor.AdvancedFSOps.createSymlink(AdvancedFSOps.java:354) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Container.createArtifactsLink(Container.java:383) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Container.setup(Container.java:321) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.LocalContainerLauncher.launchContainer(LocalContainerLauncher.java:44) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:387) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:741) [storm-core-1.0.3.jar:1.0.3]
17:22:48 [SLOT_1027] ERROR org.apache.storm.utils.Utils - Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1749) [storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) [storm-core-1.0.3.jar:1.0.3]
{code}"
STORM-2453,Move non-connectors into the top directory,"This issue is being discussed from dev@ mailing list.

http://mail-archives.apache.org/mod_mbox/storm-dev/201703.mbox/%3CCAF5108iYx7rxKWqzqncP_8un9OTv-a3wEgo90v0MZ1pfYzH25w%40mail.gmail.com%3E

We have a consensus to move non-connectors into out of ""external"", and most of participants are OK to move them to the top directory. Unless there's no further objection, we can just put them into the top directory."
STORM-2451,windows storm.cmd does not set log4j2 config file correctly by default,"When running e.g. nimbus with the default storm.yaml, the log config file is set to file:///log4j2\cluster.xml since the start script does not convert the relative dir into an absolute path. It works when setting the storm.log4j2.conf.dir explicitly.
"
STORM-2450,supervisor v2 broke ShellBolt/Spout in local mode from storm jar,"In local mode from the command line the localizer was placing the resources from the jar in the wrong directory.  resources/resource instead of resources.  It looks like this was broken when we ""fixed"" doing it for copying resources from outside the jar (unit tests)."
STORM-2449,"Iterator of Redis State may return same key multiple time, with different values","Redis state iterator iterates pending prepare -> pending commit -> external storage (state) sequentially. While iterating, more of them are subject to change, so it can provide inconsistent result.

While we can't provide consistent result (since states are changing continuously), at least iterator needs to provide same key only once."
STORM-2448,Support running workers using older JVMs/storm versions,"As a part of STORM-2441 we are separating out the classpaths for the client+worker process from everything else in storm.  This is great but it really will make some of our users upset, because it is not a rolling upgrade, and because they will need to recompile their topologies (again).

We have done a really good job in maintaining compatibility with older versions of storm because we use thrift for all communication and state storage.  This means that a new supervisor and or nimbus should be able to talk to just about any existing client/worker out there.  So we should explicitly support this.

We should add in config options to supervisors.

{{storm.supported.jvms}} which is a map of the version of the JVM to the JAVA_HOME path for it.

and

{{storm.legacy.worker.classpath}} which is a map of the version of storm to a CLASSPATH that can be used to launch a legacy worker process.

They should be set for all supervisors and nimbus.

Then we also add in some metadata that the client submits to nimbus along with a topology.  Namely the version of the storm client they are running on and the version of the JVM they are running on.

Nimbus can then decide (possibly through another config, but probably just through convention with some config overrides) if the version of the client + JVM is compatible with the version of storm + JVM currently on the cluster.  If so it should just let it through.  If not it should pick a version of the JVM + storm that is compatible.  If there are none available it will reject the request.  We should also allow end users to set these configs.

For this to work well we need some good version matching/sorting code that is lenient, even during rolling upgrades.

For example if a user submits a topology with a 1.0.3 client to a 2.0.0 cluster. Nimbus sees that it has 1.0.3 installed great it will start running with that, but then we do a rolling upgrade of the cluster and upgrade move to 1.0.4.  The supervisors should be able to launch the workers for that topology with a 1.0.4 classpath.

As such part of the worker heartbeat should also include the version of storm + JVM that the worker is running with.

We should display that on the UI and display the version it was submitted with on the UI too.

"
STORM-2447,Make local cluster transparent,"As part of the work for STORM-2441.  We would like to split the storm classpath down so the client jar only has what it needs.  Everything else is in a separate classpath(s) for daemon processes.  One of the issues with this is that local mode is built into almost all examples because it uses a separate API from the normal storm client API.

To work around this we really should add in a new option to {{storm jar}} that will include everything on the classpath, set a SystemProperty and call into a special Main Method.   The new main method will 

1) start up the LocalCluster
2) configure SotrmSubmitter, NimbusClinet and DRPCClient to talk to the LocalCluster instead.
3) run the regular main method
4) optionally sleep for a configurable amount of time
5) shut down the local cluster."
STORM-2446,Add more ml algorithm to storm-ml as a new external,"Create a new external named storm-ml that contained not only pmml but also other ml algorithm like recommend ,linear and so on."
STORM-2445,Topology log search refers supervisor ID as host of worker which contains UUID,"It seems to take supervisor ID as worker's host in topology log search, so API request to logviewer fails.

Please check attachment."
STORM-2444,Nimbus sometimes throws NPE when clicking show topology visualization button,"Here's error message from Nimbus (containing stack trace): 

{code}
{""error"":""Internal Server Error"",""errorMessage"":""java.lang.NullPointerException\n\tat org.apache.storm.stats.StatsUtil.mergeWithAddPair(StatsUtil.java:1997)\n\tat org.apache.storm.stats.StatsUtil.expandAveragesSeq(StatsUtil.java:2511)\n\tat org.apache.storm.stats.StatsUtil.aggregateAverages(StatsUtil.java:877)\n\tat org.apache.storm.stats.StatsUtil.aggregateBoltStats(StatsUtil.java:776)\n\tat org.apache.storm.stats.StatsUtil.boltStreamsStats(StatsUtil.java:942)\n\tat org.apache.storm.ui.core$visualization_data$iter__3002__3006$fn__3007.invoke(core.clj:239)\n\tat clojure.lang.LazySeq.sval(LazySeq.java:40)\n\tat clojure.lang.LazySeq.seq(LazySeq.java:49)\n\tat clojure.lang.Cons.next(Cons.java:39)\n\tat clojure.lang.RT.next(RT.java:674)\n\tat clojure.core$next__4112.invoke(core.clj:64)\n\tat clojure.core$dorun.invoke(core.clj:3010)\n\tat clojure.core$doall.invoke(core.clj:3025)\n\tat org.apache.storm.ui.core$visualization_data.invoke(core.clj:268)\n\tat org.apache.storm.ui.core$build_visualization.invoke(core.clj:591)\n\tat org.apache.storm.ui.core$fn__3641.invoke(core.clj:1204)\n\tat org.apache.storm.shade.compojure.core$make_route$fn__324.invoke(core.clj:100)\n\tat org.apache.storm.shade.compojure.core$if_route$fn__312.invoke(core.clj:46)\n\tat org.apache.storm.shade.compojure.core$if_method$fn__305.invoke(core.clj:31)\n\tat org.apache.storm.shade.compojure.core$routing$fn__330.invoke(core.clj:113)\n\tat clojure.core$some.invoke(core.clj:2570)\n\tat org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)\n\tat clojure.lang.RestFn.applyTo(RestFn.java:139)\n\tat clojure.core$apply.invoke(core.clj:632)\n\tat org.apache.storm.shade.compojure.core$routes$fn__334.invoke(core.clj:118)\n\tat org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1383.invoke(json.clj:56)\n\tat org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__918.invoke(multipart_params.clj:118)\n\tat org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__747.invoke(reload.clj:22)\n\tat org.apache.storm.ui.helpers$requests_middleware$fn__2903.invoke(helpers.clj:54)\n\tat org.apache.storm.ui.core$catch_errors$fn__3813.invoke(core.clj:1462)\n\tat org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2632.invoke(keyword_params.clj:35)\n\tat org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2675.invoke(nested_params.clj:84)\n\tat org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2604.invoke(params.clj:64)\n\tat org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__918.invoke(multipart_params.clj:118)\n\tat org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2890.invoke(flash.clj:35)\n\tat org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2876.invoke(session.clj:98)\n\tat org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2498.invoke(servlet.clj:127)\n\tat org.apache.storm.shade.ring.util.servlet$servlet$fn__2502.invoke(servlet.clj:136)\n\tat org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)\n\tat org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)\n\tat org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)\n\tat org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)\n\tat org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\n""}
{code}"
STORM-2443,Nimbus throws error when changing log level on UI topology page,"Here's stacktrace from Nimbus log:

{code}
2017-03-30 16:53:26.954 o.a.s.d.n.Nimbus pool-14-thread-56 [WARN] set log config topology exception. (topology id='rolling-1-1490860365')
java.lang.NullPointerException: null
        at org.apache.storm.daemon.nimbus.Nimbus.setLogConfig(Nimbus.java:2688) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3295) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3280) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]
{code}"
STORM-2442,modify the Usage Examples in storm-kafka-client's README.md,"The Usage Example in storm-kafka-client's README.md is Incorrect in some place.For example, This variables ,“kafkaSpoutConfig,kafkaSpoutStreams,kafkaConsumerProps,tuplesBuilder,retryService”, should be defined before it is used.And I think ""props"" should be named kafkaConsumerProps as there is a Map named kafkaConsumerProps ."
STORM-2440,Kafka outage can lead to lockup of topology,"During two somewhat extended outages of our Kafka cluster, we experienced a problem with our Storm topologies consuming data from that Kafka cluster.

Almost all our topologies just silently stopped processing data from some of the topics/partitions, an the only way to fix this situation was to restart those topologies.

I tracked down one occurrence of the failure to this worker, which was running one the KafkaSpouts:

{noformat}
2017-03-18 04:06:15.389 o.a.s.k.KafkaUtils [ERROR] Error fetching data from [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}] for topic [tagging_log]: [NOT_LEADER_FOR_PARTITION]
2017-03-18 04:06:15.389 o.a.s.k.KafkaSpout [WARN] Fetch failed
org.apache.storm.kafka.FailedFetchException: Error fetching data from [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}] for topic [tagging_log]: [NOT_LEADER_FOR_PARTITION]
        at org.apache.storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:213) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.fill(PartitionManager.java:189) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:138) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2017-03-18 04:06:15.390 o.a.s.k.ZkCoordinator [INFO] Task [1/1] Refreshing partition manager connections
2017-03-18 04:06:15.395 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=tagging_log, partitionMap={0=kafka-03:9092, 1=kafka-12:9092,
 2=kafka-08:9092, 3=kafka-05:9092}}
2017-03-18 04:06:15.395 o.a.s.k.KafkaUtils [INFO] Task [1/1] assigned [Partition{host=kafka-03:9092, topic=tagging_log, partition=0}, Partition{host=kafka-12:9092, topic=tagging_log, partit
ion=1}, Partition{host=kafka-08:9092, topic=tagging_log, partition=2}, Partition{host=kafka-05:9092, topic=tagging_log, partition=3}]
2017-03-18 04:06:15.395 o.a.s.k.ZkCoordinator [INFO] Task [1/1] Deleted partition managers: [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}]
2017-03-18 04:06:15.396 o.a.s.k.ZkCoordinator [INFO] Task [1/1] New partition managers: [Partition{host=kafka-12:9092, topic=tagging_log, partition=1}]
2017-03-18 04:06:15.398 o.a.s.k.PartitionManager [INFO] Read partition information from: /log_processing/tagging/kafka-tagging-spout/partition_1  --> {""partition"":1,""off
set"":40567174332,""topology"":{""name"":""tagging-aerospike-1"",""id"":""tagging-aerospike-1-3-1489587827""},""topic"":""tagging_log"",""broker"":{""port"":9092,""host"":""kafka-08""}}
2017-03-18 04:06:25.408 k.c.SimpleConsumer [INFO] Reconnect due to error:
java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:86) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) [stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) [stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2017-03-18 04:06:35.416 o.a.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.net.SocketTimeoutException
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) ~[storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:99) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) ~[stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) ~[stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[stormjar.jar:?]
        ... 5 more
2017-03-18 04:06:35.419 o.a.s.d.executor [ERROR] 
java.lang.RuntimeException: java.net.SocketTimeoutException
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) ~[storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:99) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) ~[stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) ~[stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[stormjar.jar:?]
        ... 5 more
2017-03-18 04:06:35.442 o.a.s.d.executor [INFO] Got interrupted excpetion shutting thread down...
{noformat}

There were no more outputs in the log after that until the toplogy was manually killed.

As you can see the {{java.net.SocketTimeoutException}} escapes the storm-kafka code (probably a problem in and of itself), but the worker is not killed. The thread that calls the {{.nextTuple}} method of the spout is exited on the other hand.
This is the culprit line: https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L270

I see that this has been fixed in the Java port of the executor code by explicitly excluding {{java.net.SocketTimeoutException}} from the condition.
I will open a pull request with a backport tomorrow."
STORM-2439,HealthCheck feature does not work,"There are a few issues with this feature:

1. The default timeout value produces `java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long at org.apache.storm.command.HealthCheck.processScript(HealthCheck.java:79)` because the value, 5000, is automatically deserialized by Jackson as an Integer, but we attempt to cast it to a long. (I successfully worked around this by setting a timeout greater than the maximum int.)

2. The documentation says that a script should print ""ERROR"" if the node is unhealthy, but in fact the script must *also* exit with a non-zero exit code. This appears to be the opposite of what is intended, given a comment that says ""We treat non-zero exit codes as indicators that the scripts failed to execute properly, not that the system is unhealthy"". I believe the test in this line is inverted: https://github.com/apache/storm/blob/70102643e74d577728adf5f8719920d1bf60e98a/storm-core/src/jvm/org/apache/storm/command/HealthCheck.java#L97

3. Even with workarounds for the above two bugs, a failing health check does not cause workers to shut down in my testing with Storm 1.0.3. I have not determined the cause, but because the previous two issues suggest to me that this code is rarely if ever tested, I do not plan to investigate further at the moment.

If this feature is, as it appears, untested and non-functional, I would suggest that it be removed from the code and documentation."
STORM-2438,on-demand resource requirement scaling,"As a first step towards true elasticity in a storm topology we propose allowing rebalance to also modify the resource requirements for each bolt/spout in the topology.  It will not be automatic, but it will let users scale up and down the CPU/memory needed for a component."
STORM-2437,LocalCluster in Unit Test crash the VM,"When unit testing Storm, we use LocalCluster. There is nothing to say when the Unit Test is working, the Unit Test ends gracefully.

However, when there are RuntimeException, for instance in the prepare functions, Storm crash and calls in Utils.mkSuicideFn, which calls Runtime.getRuntime().exit. So the VM crash and this is contradictory to Maven Surefire design (http://maven.apache.org/surefire/maven-surefire-plugin/faq.html#vm-termination).

I searched many ways to either prevent Storm from exiting (using SecurityManager), or make Unit Test accept the crash of the forked process.

If the Unit Test s VM crash, surefire will be unable to continue. My suggestion is to allow a configuration of LocalCluster that avoids System.exit, but just kills the topology (and closes all ressources if possible, but in the short term, this is not really important in a forked process)."
STORM-2432,Storm-Kafka-Client Trident Spout Seeks Incorrect Offset With UNCOMMITTED_LATEST Strategy,"With UNCOMMITED_LATEST offset, Storm-Kafka-Client Trident Spout doesn't read all the data.
See worker logs for of the same topology with UNCOMMITED_EARLIEST as well as UNCOMITTED_LATEST.
The source topic has 3 partition and I publish data k000000-k000499(k000000, v000000 etc.)
In particular for k000000 value the data is not picked by spout at all."
STORM-2430,Potential Race condition in Kafka Spout,"Kafka spout hangs when the number of uncommitted messages exceeds the max allowed uncommitted messages and some intermediate tuples have failed in down stream bolt.

Steps of reproduction.
Create a simple topology with one kafka spout and a slow bolt. 
In kafka spout set the maximum uncommitted messages to a small number like 100.
Bolt should process 10 tuples in second. And program it to fail on some random tuples. For eg: say tuple number 10 fails. Also assume  there is only 1 Kafka partition the spout reads from.

Spout on first execution of nextTuple() gets 110 records and emits them. At this point number of uncommitted message would be 110.
First 9 tuples are acked by the bolt. 10th tuple is failed by the bolt. KafkaSpout puts it on retry queue.
Tuple number 11 to 110 are acked by bolt . But spout only commits till offset 9.[link | https://github.com/apache/storm/blob/1.0.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L510]

Now, the number of uncommitted  messages = 110 - 9 = 101 > 100 (max allowed uncommitted messages)
No new records are polled from kafka.[link | https://github.com/apache/storm/blob/1.0.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L239]. The spout is stuck as the nothing is polled. 

Solution is to explicitly go through retry queue explicitly and emit tuples that are ready on every nextTuple().
"
STORM-2429,non-string values in supervisor.scheduler.meta cause crash,"The type of values in supervisor.scheduler.meta is not validated, but if one is not a string, supervisors crash during Thrift serialization with:

java.lang.ClassCastException: java.lang.Boolean cannot be cast to java.lang.String
        at org.apache.storm.generated.SupervisorInfo$SupervisorInfoStandardScheme.write(SupervisorInfo.java:1241)
        at org.apache.storm.generated.SupervisorInfo$SupervisorInfoStandardScheme.write(SupervisorInfo.java:1049)
        at org.apache.storm.generated.SupervisorInfo.write(SupervisorInfo.java:923)
        at org.apache.storm.thrift.TSerializer.serialize(TSerializer.java:79)
        at org.apache.storm.serialization.GzipThriftSerializationDelegate.serialize(GzipThriftSerializationDelegate.java:40)
        at org.apache.storm.utils.Utils.serialize(Utils.java:210)
        at org.apache.storm.cluster.StormClusterStateImpl.supervisorHeartbeat(StormClusterStateImpl.java:419)
        at org.apache.storm.daemon.supervisor.timer.SupervisorHeartbeat.run(SupervisorHeartbeat.java:85)
        at org.apache.storm.daemon.supervisor.Supervisor.launch(Supervisor.java:202)
        at org.apache.storm.daemon.supervisor.Supervisor.launchDaemon(Supervisor.java:243)
        at org.apache.storm.daemon.supervisor.Supervisor.main(Supervisor.java:362)

I will attach a PR with a simple fix"
STORM-2428,Flux-core jar contains unpacked dependencies,"The jar file for flux-core contains classes from /org/apache/http/. This was not the case before and causes problems with projects which rely on a different version of http-client. 
I can't see any references to http-client in the pom though."
STORM-2427,Event logger enable/disable UI is not working as expected in master branch,Need to pull missing commits from 1.x branch
STORM-2426,First tuples fail after worker is respawn,"Topology with two Kafka spouts (org.apache.storm.kafka.spout.KafkaSpout) reading from two different topics with same consumer group ID. 

1. Kill the only worker process for topology
2. Storm creates new worker
3. Kafka starts rebalancing (log line 15-16)
4. Kafka rebalancing done (log line 18-19)
5. Kafka topics read and tuples emitted (log line 28-29)
6. Tuples immediately fail (log line 30-33)

The delay between tuples emitted and tuples failing is just some 10 ms. No bolts in topology received the tuples.

What could cause this? The assumption is that there are uncommitted messages in Spout when it is killed and those are retried.
"
STORM-2425,Storm Hive Bolt not closing open transactions,"Hive bolt will close connection only if parameter ""max_connections"" is exceeded or bolt dies. So if we open a connection to Hive via Hive bolt and some time later we stop producing messages to Hive bolt, connection will be maintained and corresponding transactions will be opened. This can be a problem if we launch two topologies and one of them will maintain open transactions doing nothing, and other will work writing messages to hive. At some point hive will launch compactions to collapse small delta files generated by Hive Bolt into one base file. But compaction wont launch if we have opened transactions."
STORM-2424,Supervisor fails silently if started with old supervisor/localstate content,"If the following method in LocalState encounters an Exception and throws a RuntimeException, the supervisor quits silently without generating an error. I had to debug this by connecting with a remote debugger. Instead the method should generate an error to the user as to the source of the error. In my case, because I was upgrading my installation, the problem was due to a missing parameter in the content under the supervisor/localstate directory.

    private Map<String, ThriftSerializedObject> partialDeserializeLatestVersion(TDeserializer td) {
        try {
            String latestPath = _vs.mostRecentVersionPath();
            Map<String, ThriftSerializedObject> result = new HashMap<>();
            if (latestPath != null) {
                byte[] serialized = FileUtils.readFileToByteArray(new File(latestPath));
                if (serialized.length == 0) {
                    LOG.warn(""LocalState file '{}' contained no data, resetting state"", latestPath);
                } else {
                    if (td == null) {
                        td = new TDeserializer();
                    }
                    LocalStateData data = new LocalStateData();
                    td.deserialize(data, serialized);
                    result = data.get_serialized_parts();
                }
            }
            return result;
        } catch(Exception e) {
            throw new RuntimeException(e);
        }
    }"
STORM-2423,Join Bolt : Use explicit instead of default window anchoring of emitted tuples,"Default anchoring will anchor each emitted tuple to every tuple in current window. This requires a very large numbers of ACKs from any downstream bolt.  If topology.debug is enabled, it also worsens the load on the system significantly. 

Letting the topo run in this mode (in particular with max.spout.pending disabled), could lead to the worker running out of memory and crashing.

Fix: Join Bolt should avoid using default window anchoring, and explicitly anchor each emitted tuple with the exact matching tuples form each inputs streams. This reduces the complexity of the tuple trees and consequently the reduces burden on the ACKing & messaging subsystems. "
STORM-2421,Support lists of childopts beyond just worker,"The following worker childopts configuration options all support both a string value and a list of strings value:
{code}
WORKER_CHILDOPTS
WORKER_PROFILER_CHILDOPTS
WORKER_GC_CHILDOPTS
TOPOLOGY_WORKER_CHILDOPTS
TOPOLOGY_WORKER_GC_CHILDOPTS
TOPOLOGY_WORKER_LOGWRITER_CHILDOPTS
{code}

Currently the following childopts configuration options only support strings:
{code}
NIMBUS_CHILDOPTS
LOGVIEWER_CHILDOPTS
UI_CHILDOPTS
PACEMAKER_CHILDOPTS
DRPC_CHILDOPTS
SUPERVISOR_CHILDOPTS
{code}

Please could lists be supported across all childopts options as it makes configuration management and building easier using automated tools such as Chef and Puppet."
STORM-2415,Storm fails to properly handle Zookeeper hosts going down,"We run a storm cluster (v.1.0.3) on AWS and have 3 Zookeepers supporting it. Because AWS sometimes terminates VMs, we sometimes lose a Zookeeper instance. When this happens, the hostname cannot be resolved for that zookeeper instance as AWS has taken the VM away. We noticed that in this case storm fails to connect to zookeeper – even though there are still 2 Zookeeper instances running. It fails with an exception something like:
{noformat}
java.net.UnknownHostException: zookeeper3
  at java.net.InetAddress.getAllByName0(InetAddress.java:1280) 
  at java.net.InetAddress.getAllByName(InetAddress.java:1192) 
  at java.net.InetAddress.getAllByName(InetAddress.java:1126) 
  at org.apache.storm.shade.org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61) 
  at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445) 
  at org.apache.storm.shade.org.apache.curator.utils.DefaultZookeeperFactory.newZooKeeper(DefaultZookeeperFactory.java:29) 
  at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$2.newZooKeeper(CuratorFrameworkImpl.java:150) 
  at org.apache.storm.shade.org.apache.curator.HandleHolder$1.getZooKeeper(HandleHolder.java:94) 
  at org.apache.storm.shade.org.apache.curator.HandleHolder.getZooKeeper(HandleHolder.java:55) 
  at org.apache.storm.shade.org.apache.curator.ConnectionState.reset(ConnectionState.java:218) 
  at org.apache.storm.shade.org.apache.curator.ConnectionState.start(ConnectionState.java:103) 
  at org.apache.storm.shade.org.apache.curator.CuratorZookeeperClient.start(CuratorZookeeperClient.java:190) 
  at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:259) 
  at org.apache.storm.zookeeper$mk_client.doInvoke(zookeeper.clj:86) 
  at clojure.lang.RestFn.invoke(RestFn.java:494)
  at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState.invoke(zookeeper_state_factory.clj:28) 
  at org.apache.storm.cluster_state.zookeeper_state_factory.mkState(Unknown Source) 
  <SNIP REST OF STACKTRACE>
{noformat}
Having done some research it looks like this error is caused by a bug in the Zookeeper client library. There is an issue for it here:
[https://issues.apache.org/jira/browse/ZOOKEEPER-1576]
This issue has been resolved in the version 3.5.x branch of Zookeeper. However, after 2.5 years and 3 releases the 3.5.x branch of Zookeeper is still in Alpha .
Despite the fact that it is in alpha, there is a branch of Curator (v.3.x.x) that uses it, but Storm uses Curator version 2.x.x – possibly because it doesn’t rely on alpha code. So the bug is still unpatched in Storm.
I realise that an upgrade to alpha code may be too much of a risk, but this problem is a serious issue for those running Storm in a containerised or cloud environment - so perhaps it may be worth considering?"
STORM-2414,Skip checking meta's ACL when subject has write privileges for any blobs,"When BlobStore.deleteBlob is called, it always tries to get blobs if not existed on local because the logic needs to check ACL with given subject. That is not necessary when syncing up blobs in follower Nimbuses.

More generically, some subjects have write privilege for any blobs (say, superuser or admin) and for them BlobStore doesn't need to check (even download) meta's ACL and just deletes them from storage."
STORM-2410,modify Trident-state.md,"The sentence, ""(One side note – once Kafka supports replication, it will be possible to have transactional spouts that are fault-tolerant to node failure, but that feature does not exist yet.)"" ,should be removed,because Kafka has supported replication."
STORM-2409,Storm-Kafka-Client KafkaSpout Support for Failed and Null Tuples,
STORM-2407,KafkaTridentSpoutOpaque Doesn't Poll Data From All Topic-Partitions When Parallelism Hint Not a Multiple Total Topic-Partitions,
STORM-2406,[Storm SQL] Change underlying API to Streams API (for 2.0.0),"Since we dropped features which conform to the Trident semantic, Storm SQL doesn't need to rely on Trident, which is micro-batch.

Both core API and Streams API are candidates, but we should implement some bolts when we decide to rely on core API, whereas we don't need to do that for Streams API. (If we need to, that's the point to improve Streams API.)

Streams API also provides windowing feature via tuple-to-tuple semantic, so it's ready for STORM-2405 too."
STORM-2403,Fix KafkaBolt test failure: tick tuple should not be acked,"From STORM-2387, I changed KafkaBolt to make sure it doesn't ack tick tuples. 
(Tick tuples are generated from each executor and don't trigger ACK_INIT so actually it should not be acked. Acker will keep them and remove some for message timeout so not a big deal though.) 

But I forgot to fix unit test for that, and also missed to check test result. This issue is for making quick fix for that. "
STORM-2402,KafkaSpout sub-classes should be able to customize tuple processing,"We need a {{KafkaSpout}} that writes unprocessable records to a ""dead-letter-topic"". For this to function we sub-classed {{KafkaSpout}} and added the corresponding code. Without the incoming patch sub-classses can not have access to the actual tuples/records but just the {{KafkaSpoutMessageId}} in {{ack()}} and {{fail()}}."
STORM-2401,org.apache.storm.deamon.supervisor can not be found,"org.apache.storm.deamon.supervisor can not be found.
:supervisor
  set CLASS=org.apache.storm.daemon.supervisor

 in storm-core-1.0.3.jar . I can not find org.apache.storm.deamon.supervisor class, but can find org.apache.storm.deamon.supervisor$_main.class."
STORM-2397,modify storm-kafka.md,"KafkaBolt doesn't have a static variable named ""KAFKA_BROKER_PROPERTIES"" in storm 1.0.2.So storm-kafka.md should be modified."
STORM-2395,storm.cmd supervisor calls the wrong class name,"When running storm supervisor in Windows, the script tries to call org.apache.storm.daemon.supervisor. The class was renamed since it was changed from Clojure to Java recently, storm.cmd has not been changed accordingly.

current behaviour:

{quote}
λ storm supervisor

Error: Could not find or load main class org.apache.storm.daemon.supervisor
{quote}

expected behaviour:
... starts the daemon

"
STORM-2394,KafkaSpout: Has no leader of partitions for a short time,"In our case, there is something wrong with network for a short time. So some partitions of Kafka have no leaders.
The nextTuple of KafkaSpout throw an exception of ""No leader found for partition 0"" at the position of ""_coordinator.refresh();"". The exception is from the function getLeaderFor in DynamicBrokersReader.java. So the spout is hanged.
The partitions of Kafka have recover for a short time. But the spout can not deal with this problem. This problem appears several times on our server. Such as:
Feb 25 06:31:19 CST 2017, KafkaSpout threw the exception.
Feb 25 06:31:21 CST 2017, Kafka partitions recoverd.
To be stronger, I think that the ""_coordinator.refresh();"" can try times. At the last time, throw the exception. Anyway, it will die, why not try one more time?"
STORM-2392,Thrift source code generated by Storm not found,"In Maven, we can find storm-core-sources.jar, and this file contains Storm sources files. But it does not contains the Thrift source code, that Storm renamed in order to have multiple Thrift in Storm. We need source code to be in this Jar, so something should be done by storm assembly that generated Thrift code, so that these considered as source code by Maven when it deploys to Nexus.

Use case: I had to dig inside of Thrift Storm github page, download the matching branch, etc., rename the package because of thrift7 instead of thrift package etc. There are many things to be done in order to investigate Storm code, while it is much much simpler if Storm includes Thrift source natively."
STORM-2391,HdfsSpoutTopology example needs to be moved into storm-hdfs-examples from storm-starter,
STORM-2390,The storm-*-examples jars are missing in the binary distro,
STORM-2389,Event Logger bolt is instantiated even if topology.eventlogger.executors=0,
STORM-2388,JoinBolt breaks compilation against JDK 7,"STORM-2334 introduces compilation error on JDK 7, and it's included to 1.1.0 RC2.
We should fix it shortly."
STORM-2386,Fail-back Blob deletion also fails in BlobSynchronizer.syncBlobs,"This is a bug introduced from STORM-2321.

{code}
            for (String key : keySetToDownload) {
                try {
                    Set<NimbusInfo> nimbusInfoSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);
                    if (BlobStoreUtils.downloadMissingBlob(conf, blobStore, key, nimbusInfoSet)) {
                        BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);
                    }
                } catch (KeyNotFoundException e) {
                    LOG.debug(""Detected deletion for the key {} - deleting the blob instead"", key);
                    // race condition with a delete, delete the blob in key instead
                    blobStore.deleteBlob(key, BlobStoreUtils.getNimbusSubject());
                }
            }
{code}

'keySetToDownload' are keys which exist in Zookeeper, and do not exist in local. So deleting blob in local doesn't make sense. (Seems like I was confused at that time.)

if downloading throws KeyNotFoundException, it means that the blob is not available neither Zookeeper nor local, so just skipping would be OK."
STORM-2385,pacemaker_state_factory.clj does not compile on branch-1.0.x,"Assigning to Kyle because it looks like a fix for another issue broke this.

{code}
commit a6b9668c98b46a7acfb9d4f39c6e90342b2f41f7
Author: Kyle Nusbaum <knusbaum at yahoo dash inc dot com>
Date:   Tue Feb 21 14:18:31 2017 -0600

    Fixing pacemaker delete-path bug.
{code}"
STORM-2382,log4j and slf4j conflicting libraries issue,"my project storm 1.0.1 job's dependencies (log4j & slf4j) conflict with apache&hdp storm 1.0.1 default libraries (STORM_HOME/lib) and is preventing submitting new storm job.

* 1. shadow my storm job jar *
I get the following error:

{code:language=java}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.5.0.0-1245/storm/lib/log4j-slf4j-impl-2.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/tmp/356865dafc1a11e69341ecb1d7ac1510.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread ""main"" java.lang.IllegalAccessError: tried to access method org.apache.logging.log4j.core.lookup.MapLookup.newMap(I)Ljava/util/HashMap; from class org.apache.logging.log4j.core.lookup.MainMapLookup
	at org.apache.logging.log4j.core.lookup.MainMapLookup.<clinit>(MainMapLookup.java:37)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.logging.log4j.core.util.ReflectionUtil.instantiate(ReflectionUtil.java:185)
	at org.apache.logging.log4j.core.lookup.Interpolator.<init>(Interpolator.java:65)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:346)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:161)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:359)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:420)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:138)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:147)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:175)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:102)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:43)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:29)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:277)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)
	at org.apache.storm.utils.LocalState.<clinit>(LocalState.java:45)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at org.apache.storm.config__init.__init7(Unknown Source)
	at org.apache.storm.config__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at clojure.lang.RT.loadClassForName(RT.java:2182)
	at clojure.lang.RT.load(RT.java:436)
	at clojure.lang.RT.load(RT.java:412)
	at clojure.core$load$fn__5448.invoke(core.clj:5866)
	at clojure.core$load.doInvoke(core.clj:5865)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5671)
	at clojure.core$load_lib$fn__5397.invoke(core.clj:5711)
	at clojure.core$load_lib.doInvoke(core.clj:5710)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:632)
	at clojure.core$load_libs.doInvoke(core.clj:5753)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:634)
	at clojure.core$use.doInvoke(core.clj:5843)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at org.apache.storm.command.config_value$loading__5340__auto____12764.invoke(config_value.clj:16)
	at org.apache.storm.command.config_value__init.load(Unknown Source)
	at org.apache.storm.command.config_value__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at clojure.lang.RT.loadClassForName(RT.java:2182)
	at clojure.lang.RT.load(RT.java:436)
	at clojure.lang.RT.load(RT.java:412)
	at clojure.core$load$fn__5448.invoke(core.clj:5866)
	at clojure.core$load.doInvoke(core.clj:5865)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.lang.Var.invoke(Var.java:379)
	at org.apache.storm.command.config_value.<clinit>(Unknown Source)
{code}


* 2. upgrade STORM_HOME/lib *
I get this warning.  looks like a racing condition (?)

{code:language=java}
log4j:WARN No appenders could be found for logger (org.apache.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
log4j:WARN No appenders could be found for logger (org.apache.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{code}"
STORM-2381,Add logging of JDBC connection string in storm-jdbc integration for debugging failures,"If there is an issue with the jdbc connection string, the user has to go back to the code and determine what it could be. They won't be able to determine if there is a bug in user code that is setting the jdbc connection string.

In order to make debugging easier we should print the jdbc connection string."
STORM-2380,worker.childopts with whitespace inside one param will be split into pieces,"worker.childopts params with whitespace inside, like -XX:OnError=""pstack %p >~/pstack%p.log"", will be split into pieces for supervisor use string.split(""\\s+"") to split params."
STORM-2379,[storm-elasticsearch] switch ES client to Java REST API,"following documentation:
https://storm.apache.org/releases/1.0.1/storm-elasticsearch.html



https://github.com/apache/storm/blob/master/external/storm-elasticsearch/pom.xml#L40

this causes errors while writing to elastic 5.x

{code:language=java}
java.lang.NoClassDefFoundError: org/elasticsearch/common/base/Preconditions
	at org.apache.storm.elasticsearch.common.EsConfig.<init>(EsConfig.java:62) ~[storm-elasticsearch-1.0.2.jar:1.0.2]
	at org.apache.storm.elasticsearch.common.EsConfig.<init>(EsConfig.java:49) ~[storm-elasticsearch-1.0.2.jar:1.0.2]

Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.base.Preconditions
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_112]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_112]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_112]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_112]


261538 [elasticsearch[Ringleader][generic][T#2]] INFO  o.e.c.transport - [Ringleader] failed to get node info for [#transport#-1][svaddi][inet[localhost/127.0.0.1:9200]], disconnecting...
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[localhost/127.0.0.1:9200]][cluster:monitor/nodes/info] request_id [26] timed out after [5005ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529) ~[elasticsearch-1.6.0.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{code}


elastic logs:
{code:language=java}
[2017-02-23T15:47:04,487][WARN ][o.e.t.n.Netty4Transport  ] [Qt9qlNV] exception caught on transport layer [[id: 0x8f15e875, L:/127.0.0.1:9300 - R:/127.0.0.1:52031]], closing connection
java.lang.IllegalStateException: Received message from unsupported version: [1.0.0] minimal compatible version is: [5.0.0]
	at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1199) ~[elasticsearch-5.0.0.jar:5.0.0]
	at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74) ~[transport-netty4-5.0.0.jar:5.0.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:610) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:513) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:467) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:437) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-common-4.1.5.Final.jar:4.1.5.Final]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
{code}"
STORM-2374,Storm Kafka Client Func Interface Must be Serializable,
STORM-2372,Pacemaker client doesn't clean up heartbeats properly.,"Paths are not deleted correctly. Pacemaker's delete-path operates by matching a prefix against all the keys in the map.

The issue here is that the prefix is given a '/' on the end, but keys don't have a trailing '/' if there is no 'subkey'.

i.e. delete path /foo/bar/baz/ doesn't match the key /foo/bar/baz
The path has to have the trailing '/' so that delete path /foo/bar/baz doesn't also delete /foo/bar/bazoo

The solution here is to tack on a '/' to every key when checking against the prefix.

We also want to send the delete command to *every* pacemaker server rather than just the normal write client.
"
STORM-2366,SpoutTracker does not delegate to all methods,"h2. Problem
{{SpoutTracker}} does not implement and delegate to the following methods:

- {{activate}}
- {{deactivate}}
- {{getComponentConfiguration}}

h2. Effect
This causes problems for spouts that require initialization in or to be operational.

h2. Solution
The recommended fix is adding the following:

{code}
    @Override
    public void activate() {
        _delegate.activate();
    }

    @Override
    public void deactivate() {
        _delegate.deactivate();
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return _delegate.getComponentConfiguration();
    }
{code}
"
STORM-2364,Ensure kafka-monitor jar is included in classpath for UI process,The kafka-monitor jar has been moved into  toollib/ and not featuring in UI process's classpath. Leading to ClassNotFoundException for KafkaOffsetLagUtil class ... seen in the ui.log
STORM-2362,Cassandra Spout,
STORM-2361,"Kafka spout - after topic leader change, it stops committing offsets to ZK","After STORM-2296 although Kafka spouts do not generate duplicates, the offsets committment to ZK may stop on recreated PartitionManagers.

This is because ack's for messages emitted by already destroyed PartitionManagers are not routed properly to the new PartitionManagers handling that partition.

E.g: 
{code:java} public void ack(Object msgId) {
        KafkaMessageId id = (KafkaMessageId) msgId;
        PartitionManager m = _coordinator.getManager(id.partition);
        if (m != null) {
            m.ack(id.offset);
        }
{code}
id.partition is Partition(host, partition, topic), which is different if Kafka broker changed."
STORM-2360,Storm-Hive: Thrift version mismatch with storm-core,"Storm-Hive's libthrift version (0.9.0) is not in sync with storm-core's libthrift version (0.9.3) on branch-1.0.x. 

This issue has been resolved for master and 1.x-branch but was not put in the 1.0.x-branch. 

Related commits:
* master: [5df06bf523754d2bae35c23c24a6c6ffb99e4d9f|https://github.com/apache/storm/commit/5df06bf523754d2bae35c23c24a6c6ffb99e4d9f]
* branch-1.0.x: [60506cf8432366d0e5799974f374b4de95da8abe| https://github.com/apache/storm/commit/60506cf8432366d0e5799974f374b4de95da8abe]

Credit goes to [~ptgoetz] for discovering this issue."
STORM-2359,Revising Message Timeouts,"A revised strategy for message timeouts is proposed here.

Design Doc:
 https://docs.google.com/document/d/1am1kO7Wmf17U_Vz5_uyBB2OuSsc4TZQWRvbRhX52n5w/edit?usp=sharing"
STORM-2358,Update storm hdfs spout to remove specific implementation handlings,"I was looking at storm hdfs spout code in 1.x branch, I found below
improvements can be made in below code.

  1.  Make org.apache.storm.hdfs.spout.AbstractFileReader as public so
that it can be used in generics.

  2.  org.apache.storm.hdfs.spout.HdfsSpout requires readerType as
String. It will be great to have class<? extends AbstractFileReader>
readerType; So we will not use Class.forName at multiple places also it
will help in below point.

  3.  HdfsSpout also needs to provide outFields which are declared as
constants in each reader(e.g.SequenceFileReader). We can have abstract
API AbstractFileReader in which return them to user to make it generic."
STORM-2356,Storm-HDFS: NPE on empty & stale lock file,"In HDFSSpout a NPE can occur if a stale lock file is empty.

{{LogEntry.deserialize}} tries to split the line by colons.
If the line is null, the split will cause a NPE:
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java#L179

Moreover the callee of {{getLastEntry}} is also mishandling empty log files.
The {{lastEntry.eventTime}} could also cause a NPE if the above scenario is passed and the log file is empty:
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java#L149-L160"
STORM-2354,Upgrade to Driver 3.1,"Added a pull request at https://github.com/apache/storm/pull/1936

"
STORM-2352,New Kafka spout retries for ever even with retries of 5,"v1.0.0 and above

KafkaSpout is created with a KafkaSpoutConfig having maxRetries of 5. Still the KafkaSpout retries the failed Tuple forever. 

Reason:
The numFails are incremented in fail() method of KafkaSpout.
{code}
public void fail(Object messageId) {
        final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;
        emitted.remove(msgId);
        if (msgId.numFails() < maxRetries) {
            msgId.incrementNumFails();
            retryService.schedule(msgId);
        } else { // limit to max number of retries
            LOG.debug(""Reached maximum number of retries. Message [{}] being marked as acked."", msgId);
            ack(msgId);
        }
    }
{code}

However the emitTupleIfNotEmitted() creates a new KafkaSpoutMessageId  and checks if the msgId is ready to be emitted (in the case of failure) and if so emits the new msgId instance (thus losing the numFails from the previous time)

{code}
    private void emitTupleIfNotEmitted(ConsumerRecord<K, V> record) {
        final TopicPartition tp = new TopicPartition(record.topic(), record.partition());
        final KafkaSpoutMessageId msgId = new KafkaSpoutMessageId(record);

        if (acked.containsKey(tp) && acked.get(tp).contains(msgId)) {   // has been acked
            LOG.trace(""Tuple for record [{}] has already been acked. Skipping"", record);
        } else if (emitted.contains(msgId)) {   // has been emitted and it's pending ack or fail
            LOG.trace(""Tuple for record [{}] has already been emitted. Skipping"", record);
        } else if (!retryService.isScheduled(msgId) || retryService.isReady(msgId)) {   // not scheduled <=> never failed (i.e. never emitted) or ready to be retried
            final List<Object> tuple = tuplesBuilder.buildTuple(record);
            kafkaSpoutStreams.emit(collector, tuple, msgId);
            emitted.add(msgId);
            numUncommittedOffsets++;
            if (retryService.isReady(msgId)) { // has failed. Is it ready for retry ?
                retryService.remove(msgId);  // re-emitted hence remove from failed
            }
            LOG.trace(""Emitted tuple [{}] for record [{}]"", tuple, record);
        }
    }
{code}

isReady() is not a side-effect. It just looks up and returns true. Fix is to either modify the RetryService interface to convey back the msgId in the RetryService or make the isReady() a side-effect to attach the numFails from the previous time OR to add 'failed' to KafkaSpout to keep track of failed msgs (similar to acked) and use the msgId from the failed to emit if isReady() is true"
STORM-2351,Unable to build native code on OS X,"Compilation of Storm fails under OS X if native profile is enabled for multiple reasons:

1)
{code}
~/w/storm ❯❯❯ mvn clean install -Pnative -DskipTests
....
....
[INFO] --- exec-maven-plugin:1.2.1:exec (default) @ storm-core ---
cp: illegal option -- u
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory
{code}

The problem is caused by the lack of ""u"" (upgrade) flag of cp.

2)
{code}
[INFO] 	gcc -DPACKAGE_NAME=\""worker-launcher\"" -DPACKAGE_TARNAME=\""worker-launcher\"" -DPACKAGE_VERSION=\""1.0.0\"" -DPACKAGE_STRING=\""worker-launcher\ 1.0.0\"" -DPACKAGE_BUGREPORT=\""user@storm.apache.org\"" -DPACKAGE_URL=\""\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -D__EXTENSIONS__=1 -D_ALL_SOURCE=1 -D_GNU_SOURCE=1 -D_POSIX_PTHREAD_SEMANTICS=1 -D_TANDEM_SOURCE=1 -DPACKAGE=\""worker-launcher\"" -DVERSION=\""1.0.0\"" -DHAVE_UNISTD_H=1 -DHAVE__BOOL=1 -DHAVE_STDBOOL_H=1 -DHAVE_DECL_STRERROR_R=1 -DHAVE_STRERROR_R=1 -DHAVE_MKDIR=1 -DHAVE_UNAME=1 -I.    -I./impl -Wall -g -Werror -DEXEC_CONF_DIR=/etc/storm  -MT impl/worker-launcher.o -MD -MP -MF $depbase.Tpo -c -o impl/worker-launcher.o impl/worker-launcher.c &&\
[INFO] 	mv -f $depbase.Tpo $depbase.Po
[INFO] impl/worker-launcher.c:60:15: error: use of undeclared identifier 'PATH_MAX'
[INFO]   char buffer[PATH_MAX];
[INFO]               ^
[INFO] impl/worker-launcher.c:61:20: error: use of undeclared identifier 'PATH_MAX'
[INFO]   snprintf(buffer, PATH_MAX, ""/proc/%u/exe"", getpid());
[INFO]                    ^
[INFO] impl/worker-launcher.c:62:27: error: use of undeclared identifier 'PATH_MAX'
[INFO]   char *filename = malloc(PATH_MAX);
[INFO]                           ^
[INFO] impl/worker-launcher.c:67:44: error: use of undeclared identifier 'PATH_MAX'
[INFO]   ssize_t len = readlink(buffer, filename, PATH_MAX);
[INFO]                                            ^
[INFO] impl/worker-launcher.c:72:21: error: use of undeclared identifier 'PATH_MAX'
[INFO]   } else if (len >= PATH_MAX) {
[INFO]                     ^
[INFO] impl/worker-launcher.c:74:13: error: use of undeclared identifier 'PATH_MAX'
[INFO]             PATH_MAX, filename, PATH_MAX);
[INFO]             ^
[INFO] impl/worker-launcher.c:74:33: error: use of undeclared identifier 'PATH_MAX'
[INFO]             PATH_MAX, filename, PATH_MAX);
[INFO]                                 ^
[INFO] impl/worker-launcher.c:99:9: error: unused variable 'binary_euid' [-Werror,-Wunused-variable]
[INFO]   uid_t binary_euid = filestat.st_uid; // Binary's user owner
[INFO]         ^
[INFO] impl/worker-launcher.c:450:42: error: use of undeclared identifier 'PATH_MAX'
[INFO]     char *(paths[]) = {strndup(local_dir,PATH_MAX), 0};
[INFO]                                          ^
[INFO] impl/worker-launcher.c:597:42: error: use of undeclared identifier 'PATH_MAX'
[INFO]     char *(paths[]) = {strndup(full_path,PATH_MAX), 0};
[INFO]                                          ^
[INFO] impl/worker-launcher.c:725:3: error: implicit declaration of function 'fcloseall' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
[INFO]   fcloseall();
[INFO]   ^
[INFO] impl/worker-launcher.c:725:3: note: did you mean 'fclose'?
[INFO] /usr/include/stdio.h:232:6: note: 'fclose' declared here
[INFO] int      fclose(FILE *);
[INFO]          ^
[INFO] 11 errors generated.
[INFO] make: *** [impl/worker-launcher.o] Error 1
{code}"
STORM-2350,Storm-HDFS's listFilesByModificationTime is broken,"Storm-HDFS module tries to locate the oldest locks. To sort based on the modification time the {{ModifTimeComparator}} is used which is broken:
The comparator currently compares the first object ({{o1}}) to itself.

Fix is trivial: compare {{o1}} to {{o2}}."
STORM-2347,JS errors in Topology Visualization,"On a freshly downloaded 1.0.2 after starting ZK, UI, NM, and SP, submitting a topology the Topology Visualization isn't working and 300+ JS errors are appearing on the console.
No changes to config files were made.
Any help would be appreciated.



{code}
...
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at screenUpdate (arbor.js:33)
gather_stream_count @ visualization.js:314
(anonymous) @ visualization.js:268
(anonymous) @ arbor.js:35
each @ jquery-1.11.1.min.js:2
eachEdge @ arbor.js:35
calculate_total_transmitted @ visualization.js:257
redraw @ visualization.js:54
screenUpdate @ arbor.js:33
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at screenUpdate (arbor.js:33)
gather_stream_count @ visualization.js:314
(anonymous) @ visualization.js:268
(anonymous) @ arbor.js:35
each @ jquery-1.11.1.min.js:2
eachEdge @ arbor.js:35
calculate_total_transmitted @ visualization.js:257
redraw @ visualization.js:54
screenUpdate @ arbor.js:33
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at Object.success (visualization.js:420)
    at j (jquery-1.11.1.min.js:2)
    at Object.fireWith [as resolveWith] (jquery-1.11.1.min.js:2)
...
{code}"
STORM-2345,Type mismatch in ReadClusterState's ProfileAction processing Map,"Discovered during reading STORM-2018's review comments:
{{ReadClusterState.run()}} method loads all the profiling requests from Zk then filters the ones designated to that particular node it runs on.  

The filtered profiling requests are stored in a Map: {{Map<Integer, Set<TopoProfileAction>>}} on a per Slot basis.
For some reason the TCP Port is serialized as i64/Long in NodeInfo, which is later used as a key in the Map.

The Map.put is converted properly to Integer, but the Map.get does not, causing the lookups to report a miss in the {{filtered}} Map.

This could cause TopoProfileActions ignored if one would send multiple TopoProfileActions through Zk. 
I don't think that could normally happen (using the UI), but the fix is trivial."
STORM-2343,New Kafka spout can stop emitting tuples if more than maxUncommittedOffsets tuples fail at once,"It doesn't look like the spout is respecting maxUncommittedOffsets in all cases. If the underlying consumer returns more records in a call to poll() than maxUncommittedOffsets, they will all be added to waitingToEmit. Since poll may return up to 500 records by default (Kafka 0.10.1.1), this is pretty likely to happen with low maxUncommittedOffsets.

The spout only checks for tuples to retry if it decides to poll, and it only decides to poll if numUncommittedOffsets < maxUncommittedOffsets. Since maxUncommittedOffsets isn't being respected when retrieving or emitting records, numUncommittedOffsets can be much larger than maxUncommittedOffsets. If more than maxUncommittedOffsets messages fail, this can cause the spout to stop polling entirely."
STORM-2342,storm-kafka-client consumer group getting stuck consuming from kafka 0.10.1.1,"I've created a topology that will read from kafka using storm-kafka-client but when it reaches the last message on kafka log it stops consuming and get stuck, new messages are never consumed, here are the kafka logs:
{quote}
[2017-02-03 19:15:26,865] INFO [GroupCoordinator 1002]: Preparing to restabilize group kafka-spout with old generation 29 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:15:26,865] INFO [GroupCoordinator 1002]: Stabilized group kafka-spout generation 30 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:15:26,868] INFO [GroupCoordinator 1002]: Assignment received from leader for group kafka-spout for generation 30 (kafka.coordinator.GroupCoordinator)
{quote}
========= here storm starts consuming messages, then, when it hits the last message, I can see this log in kafka == >
{quote}
[2017-02-03 19:16:01,266] INFO [GroupCoordinator 1002]: Preparing to restabilize group kafka-spout with old generation 30 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:16:01,266] INFO [GroupCoordinator 1002]: Group kafka-spout with generation 31 is now empty (kafka.coordinator.GroupCoordinator)
{quote}
=====
and then storm consumer group is stuck, no new messages are read from kafka. my topology/ spout are configured that way:

*Topology:*

      c.put(SConfig.TOPOLOGY_MAX_SPOUT_PENDING, 1000)
      c.put(SConfig.NIMBUS_SEEDS, ""my nimbus seeds"")
      c.put(SConfig.NIMBUS_THRIFT_PORT, 6627)
      c.put(SConfig.TOPOLOGY_WORKERS, 2)      c.put(SConfig.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS, 100)

*Spout:*

    props.put(KafkaSpoutConfig.Consumer.ENABLE_AUTO_COMMIT, true)
    props.put(KafkaSpoutConfig.Consumer.BOOTSTRAP_SERVERS, ...)
    props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""kafka-spout"")
    props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, keyDeserializer)
    props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, valueDeserializer)
 
Also the offsets are not seeming to be committed, despite I've set *enable.auto.commit* and *auto.commit.interval.ms* properties, because if I kill the topology and send it again, the same messages are being reprocessed...

 any hints?"
STORM-2341,worker-launcher is not included in binary distribution,Even though the documentation refers to [worker-launcher|http://storm.apache.org/releases/1.0.2/SECURITY.html] and the Travis builds with -Pnative the {{worker-launcher}} binary is not included in apache-storm-xyz.tar.gz files.
STORM-2339,Python code format cleanup in storm.py,"{{bin/storm.py}} has multiple stylistic shortcomings:
 - PEP8 standard is not followed
 - the python interpreter is hard-wired to /usr/bin/python
 - unnecessary global statements are posted before reading globals

These issues shadows error reporting by modern IDEs (such as PyCharm).
"
STORM-2338,Subprocess exception handling is broken in storm.py on Windows environment,"There is typo in the exception handling branch in {{storm.py:: exec_storm_class()}}:
{code}
        try:
            ret = sub.check_output(all_args, stderr=sub.STDOUT)
            print(ret)
        except sub.CalledProcessor as e:
            sys.exit(e.returncode)
{code}

There is no ""CalledProcessor"" type of exception exists in subprocess module.
The correct exception name is CalledProcessError."
STORM-2337,Broken documentation generation for storm-metrics-profiling-internal-actions.md and windows-users-guide.md,"The generated documentation is broken for {{storm-metrics-profiling-internal-actions.md}} and {{windows-users-guide.md}}. 

The format of these documents do not conform with Jekyll's standard and they are silently ignored. This causes 404 errors when someone want's to access those documents through the webpage.

Additionally the exclusion filter in _config.yml has a typo (READ*E*ME.md)"
STORM-2334,Bolt for Joining streams,"Create a general purpose windowed bolt that performs Joins on multiple data streams.

Since, depending on the topo config,  the bolt could be receiving data either on 'default' streams or on named streams .... join bolt should be able to differentiate the incoming data based on names of upstream components as well as stream names.

*Example:*

The following SQL style join involving 4 tables :

{code}
select  userId, key4, key2, key3
from stream1 
join       stream2  on stream2.userId =  stream1.key1
join       stream3  on stream3.key3   =  stream2.userId
left join  stream4  on stream4.key4   =  stream3.key3
{code}

Could be expressed using the Join Bolt over 4 named streams as :

{code}
new JoinBolt(STREAM, ""stream1"", ""key1"") //'STREAM' arg indicates that stream1/2/3/4 are names of streams. 'key1' is the key on which 
     .join     (""stream2"", ""userId"",  ""stream1"") //join stream2 on stream2.userId=stream1.key1
     .join     (""stream3"", ""key3"",    ""stream2"") //join stream3 on stream3.key3=stream2.userId   
     .leftjoin (""stream4"", ""key4"",    ""stream3"") //left join stream4 on stream4.key4=stream3.key3
     .select(""userId, key4, key2, key3"")         // chose output fields
     .withWindowLength(..)
     .withSlidingInterval(..);
{code}

Or based on named source components :

{code}
new JoinBolt(SOURCE, ""kafkaSpout1"", ""key1"") //'SOURCE' arg indicates that kafkaSpout1, hdfsSpout3 etc are names of upstream components 
     .join     (""kafkaSpout2"", ""userId"",    ""kafkaSpout1"" )    
     .join     (""hdfsSpout3"",  ""key3"",      ""kafkaSpout2"")
     .leftjoin (""mqttSpout1"",  ""key4"",      ""hdfsSpout3"")
     .select (""userId, key4, key2, key3"")
     .withWindowLength(..)
     .withSlidingInterval(..);
{code}


In order for the tuples to  be joined correctly, 'fields grouping' should be employed on the incoming streams. Each stream should be grouped on the same key using which it will be joined against other streams.  This is a restriction compared to SQL which allows join a table with others on any key and any number of keys.

*For example:* If a 'Stream1' is Fields Grouped on 'key1', we cannot use a different 'key2' on 'Stream1' to join it with other streams. However, 'Stream1' can be joined using the same key with multiple other streams as show in this SQL.

{code}
select ....
from stream1 
join  stream2  on stream2.userId =  stream1.key1
join  stream3  on stream3.key3   =  stream1.key2  // not supportable in Join Bolt 
{code}

Consequently the join bolt's syntax is a bit simplified compared to SQL. The key name for any given stream only appears once, as soon the stream is introduced for the first time in the join. Thereafter that key is implicitly used for joining. See the case of 'stream3' being joined with both 'stream2' and 'stream4' in the first example.
"
STORM-2329,Topology halts when getting HDFS writer in a secure environment,"Simple topologies writing to Kerberized HDFS will sometimes stop while getting a new writer (storm-hdfs) in a Kerberized environment: 

java.io.IOException: Failed on local exception: java.io.IOException: Couldn't setup connection for principal@realm to nn1/nn1IP; Host Details : local host is: ""hostname/ip""; destination host is: ""nn hostname"":8020; at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772) at org.apache.hadoop.ipc.Client.call(Client.java:1473) at org.apache.hadoop.ipc.Client.call(Client.java:1400) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) at com.sun.proxy.$Proxy26.create(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296) at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy27.create(Unknown Source) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1726) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1668) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1593) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775) at org.apache.storm.hdfs.bolt.AvroGenericRecordBolt.makeNewWriter(AvroGenericRecordBolt.java:115) at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.getOrCreateWriter(AbstractHdfsBolt.java:222) at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.execute(AbstractHdfsBolt.java:154) at backtype.storm.daemon.executor$fn_3697$tuple_action_fn3699.invoke(executor.clj:670) at backtype.storm.daemon.executor$mk_task_receiver$fn3620.invoke(executor.clj:426) at backtype.storm.disruptor$clojure_handler$reify3196.onEvent(disruptor.clj:58) at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) at backtype.storm.daemon.executor$fn3697$fn3710$fn3761.invoke(executor.clj:808) at backtype.storm.util$async_loop$fn_544.invoke(util.clj:475) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Couldn't setup connection for principal@realm to nn1/nn1IP:8020 at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:673) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:644) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:731) at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:369) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1522) at org.apache.hadoop.ipc.Client.call(Client.java:1439) ... 35 more Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:554) at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:369) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:719) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:718) ... 38 more Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ... 47 more

Typically seen on low throughput topologies but recently witnessed in a topology that rotates files within minutes.  

From the trace it happens here: https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L151

My suspicion is this happens only when opening a new file otherwise I don't see why there would be a Kerberos context to complain about until a flush/sync perhaps. 

My shoot from the hip reaction is to pull that out of the current try and simply let the bolt fail and restart to establish a security context.  Thoughts? "
STORM-2328,Batching And Vector Operations,"Sub-topic of Storm Worker redesign. 
Design doc:  https://docs.google.com/document/d/13n0omjkc04h6KObC9-h7l4h4OL8Cp9Qnqz0XXtNJtno/edit?usp=sharing"
STORM-2327,Abstract class ConfigurableTopology,"Classes which run topologies often repeat the same code and pattern to:
* populate the configuration from a file instead of ~/.storm
* determine whether to run locally or remotely
* set a TTL for a topology

Flux provides an elegant way of dealing with these but sometimes it is simpler to define a topology in Java code. 

In [StormCrawler|http://stormcrawler.net], we implemented an abstract class named ConfigurableTopology which can be extended and saves users the hassle of having to write code for the things above. I will open a PR containing this class so that we can discuss and comment whether it is of any use at all."
STORM-2326,Upgrade log4j and slf4j,"The dependencies to log4j could be upgraded from 2.1 to 2.7, same for slf4j to 1.7.21.

This would help fix [STORM-1386]

BTW any idea why we need log4j-over-slf4j?
"
STORM-2322,Could not find or load main class blobstore ,I got this error (Could not find or load main class blobstore) on Windows machine while I'm trying to run a command: storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1. This error occurs for other commands that I'm trying to use and for different machines.
STORM-2321,Nimbus did not come up after restart,"The nimbus was restarted during HA testing. After the restart the nimbus failed to come up. 
{code}
2017-01-18 04:57:58.231 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2017-01-18 04:57:58.247 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyKillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar
2017-01-18 04:57:58.273 o.a.s.b.KeySequenceNumber [ERROR] Exception {}
org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:149)
	at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)
	at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58.274 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
2017-01-18 04:57:58.296 o.a.s.m.n.Login [INFO] successfully logged in.
2017-01-18 04:57:58.309 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x359afc1eaa2009b closed
2017-01-18 04:57:58.309 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
2017-01-18 04:57:58.310 o.a.s.t.s.TThreadPoolServer [ERROR] Error occurred during processing of message.
java.util.NoSuchElementException
	at java.util.TreeMap.key(TreeMap.java:1327)
	at java.util.TreeMap.lastKey(TreeMap.java:297)
	at java.util.TreeSet.last(TreeSet.java:401)
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:206)
	at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)
	at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58.311 o.a.s.d.nimbus [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:92)
	at org.apache.storm.daemon.nimbus$fn__9373.invoke(nimbus.clj:1452)
	at clojure.lang.MultiFn.invoke(MultiFn.java:233)
	at org.apache.storm.daemon.nimbus$fn__9770$exec_fn__3656__auto____9771$fn__9786.invoke(nimbus.clj:2452)
	at org.apache.storm.timer$schedule_recurring$this__2188.invoke(timer.clj:105)
	at org.apache.storm.timer$mk_timer$fn__2171$fn__2172.invoke(timer.clj:50)
	at org.apache.storm.timer$mk_timer$fn__2171.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:114)
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:76)
	... 8 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:252)
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:111)
	... 9 more
Caused by: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:349)
	at org.apache.storm.blobstore.BlobStoreUtils.createStateInZookeeper(BlobStoreUtils.java:217)
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:249)
	... 10 more
Caused by: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)
	at org.apache.storm.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)
	at org.apache.storm.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)
	at org.apache.storm.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_createStateInZookeeper(Nimbus.java:1000)
	at org.apache.storm.generated.Nimbus$Client.createStateInZookeeper(Nimbus.java:987)
	at org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:346)
	... 12 more
2017-01-18 04:57:58.314 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__8579.invoke(nimbus.clj:212)
	at org.apache.storm.timer$mk_timer$fn__2171$fn__2172.invoke(timer.clj:71)
	at org.apache.storm.timer$mk_timer$fn__2171.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,318 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,318 FATAL Ignoring log event after log4j was shut down
{code}"
STORM-2320,DRPC client printer class reusable for local and remote DRPC,"The Trident Kafka examples in the modules storm-kafka-examples and storm-kafka-client-examples use DRPC to print the results of DRPC computation. In local mode the results output is printed as illustrated in the screenshot attached.

For the DRPC results to be printable when running in distributed mode,it is necessary to connect a DRPC client to  retrieve the results. There was no DRPC client running in remote mode prior to this change, hence nothing was getting printed in distributed mode. This fact mislead users into believing that the examples were not working properly in remote mode.

This JIRA addresses the issue by printing the DRPC results in the log files. Thus, it allows users to query the results and investigate their validity. Furthermore, it serves as a good example on how to print results of DRPC computation in local and distributed mode."
STORM-2316,Enumeration support for properties configuration,It would be great if a Flux builder will resolve enumeration within properties configuration. This feature is only available for constructor arguments.
STORM-2315,New kafka spout can't commit offset when ack is disabled. ,"When ack is disabled, kafka spout failed to commit offsets."
STORM-2314,Workers is dead or locked when netty connection is timeout,"Storm is running ,but some workers can not emit data when throw the following exception,  the exception offen occurs in heigh pressure

2017-01-22 18:23:25.137 s.k.CollectorZkCoordinator [INFO] Task [3/3] Finished refreshing
2017-01-22 18:24:08.170 o.a.s.m.n.StormClientHandler [INFO] Connection to xxxxxxx/192.168.175.25:6703 failed:
java.io.IOException: Connection timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.7.0_80]
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.7.0_80]
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.7.0_80]
	at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384) ~[?:1.7.0_80]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.0.2.jar:1.0.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_80]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_80]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80]"
STORM-2313,CPU Pinning,"Design Document:
https://docs.google.com/document/d/1zI0ax-8dE9SmLkF8ZoJaLnrM11EL7NV1uO3VymWG3OA/edit?usp=sharing"
STORM-2312,Memory Management,"Refer to this Doc for details

https://docs.google.com/document/d/1a-RLv1KKBn2vVliztLcdfC-5vDStxpNlR0ZLu33lwxg/edit?usp=sharing"
STORM-2311,A browser which is outside the cluster cant's access  storm ui when storm cluster in a security mode,"when storm cluster in a security mode such as kerberos,We can not use a browser which is outside the cluster to access  storm ui,even if we remove the configuration item which named ui.filter.There is a mistake like ""server 500"",because those methods to get cluster's info for ui can't access the cluster which is in  a security mode"
STORM-2310,Handling Back Pressure,"Design Doc:
https://docs.google.com/document/d/1btmpBpFeEl-bh1uhQ_W4Ao-cQK7muNBYbXXMwuc4YaU/edit?usp=sharing"
STORM-2309,Elasticity - for Storm topologies,
STORM-2308,Support for Non-replayable Sources,"In order to recover from failures without data loss, Storm (and other streaming systems) places the responsibility of buffering events on the source system. In the event of a crash or other failure, in-flight events can be re-fetched from the source and their processing can be retried on recovery. A nice benefit of this approach is that it keeps Storm’s architecture simple. 

While it is desirable to avoid the complexities of creating an internal reliable buffering system, it is not necessary to restrict Spouts to accept data only from persistent sources such Kafka, Hdfs or databases. Some amount of data loss is acceptable in many uses cases. Storm already supports such use cases by allowing ACK-ing to be disabled. 

Users who can tolerate data loss, benefit from having spouts that can accept data directly from a wider variety of sources such as HTTP, TCP/UDP, Syslog, Flume etc. For such use cases, by not forcing all data to go through a system like Kafka, end-to-end latency improves in addition to simplifying management and reducing cost of the data pipeline. Users who care about not losing data can always funnel the incoming data via Kafka or another persistent store and enable ACKs.
"
STORM-2307,Revised Threading and Execution Model,"Design Doc:
https://docs.google.com/document/d/1PBGQomJQ67gsLR0CNZlYfVWjGyzAEJMsQjpKumuyHuQ/edit?usp=sharing"
STORM-2306,"Redesign Messaging Subsystem, switch to JCTools Queues and introduce new Backpressure model","Details in these documents:

1) *Redesign of the messaging subsystem*
https://docs.google.com/document/d/1NK1DJ3aAkta-Im0m-2FObQ4cSRp8xSa301y6zoqcBeE/edit?usp=sharing
This doc discusses the new design for the messaging system. Plus some of the optimizations being made.

2) *Choosing a high performance messaging queue:*
https://docs.google.com/document/d/1PpQaWVHg06-OqxTzYxQlzg1yEhzA4Y46_NC7HMO6tsI/edit?usp=sharing
This doc looks into how fast hardware can do inter-thread messaging and why we chose the JCTools queues.

3) *Backpressure Model*
https://docs.google.com/document/d/1Z9pRdI5wtnK-hVwE3Spe6VGCTsz9g8TkgxbTFcbL3jM/edit?usp=sharing
Describes the Backpressure model integrated into the new messaging subsystem."
STORM-2305,STORM-2279 calculates task index different from grouper code,"Arun reported this from https://github.com/apache/storm/pull/1866#discussion_r95326528

Quoting his comment:

{quote}
This should match the task selected by fields grouping https://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/daemon/GrouperFactory.java#L159.
The modulo technique will return a different value than Math.abs. Probably we should move it to some common utility function and use it in both places.

In 1.x branch the fields grouper doesn't seem to account for negative hashCode, so not sure why didn't it come up before. https://github.com/apache/storm/blob/1.x-branch/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L52
{quote}

So we should fix Nimbus code to use Math.abs on master, 1.x, 1.0.x branches, and also fix executor.clj to use Math.abs on 1.x, 1.0.x branches."
STORM-2303,[storm-opentsdb] Fix list invariant issue for JDK 7,"From STORM-2297 I also fixed an issue where storm-opentsdb refers TupleOpenTsdbDatapointMapper (implemented one) to List/Iterable, not ITupleOpenTsdbDatapointMapper (interface). 
I just replaced TupleOpenTsdbDatapointMapper to ITupleOpenTsdbDatapointMapper, and it works with JDK 8, but later I realized it doesn't work with JDK 7 because generic is invariant.

While I don't know why it worked with JDK 8 (I googled about generic covariance change on JDK 8 but no luck.) it should be fixed."
STORM-2302,New Kafka spout doesn't support seek to given offset,"I was looking at code of current KafkaTridentSpoutEmitter & KafkaSpout class. Can we add functionality based on user provided offset to start from particular offset? This would be useful incase user wants to reprocess particular data set. Another example user has changed the group id & aware where old offset committed & he wants to start processing from same position.

Please refer attachment for further discussion happened over mail."
STORM-2301,[storm-cassandra] upgrade cassandra driver to 3.1.2,"Currently, storm-cassandra refers cassandra driver 2.1.7.1 which is not compatible with Cassandra 3.x. 

Fortunately cassandra driver 3.1.2 is compatible with various Cassandra versions (Apache Cassandra 1.2, 2.0, 2.1, 2.2 and 3.0), so upgrade driver should be safe for users who uses prior version of Cassandra 3.0.
http://docs.datastax.com/en/developer/java-driver/3.1/#compatibility"
STORM-2300,[Flux] support list of references,"There're many methods which receive list of objects (also array of objects with varargs) which object is not basic type of yaml.
(STORM-2297 is one of the case, though STORM-2297 is just going to fix storm-opentsdb itself.)

It would be better to support reference list so that it can support method which argument is List<Type>, Type[], Type... (varargs).
(Flux can automatically convert List to Array while assigning.)"
STORM-2299,Stop user from killing topology before X (configured) amount of time,"Currently user can kill topology directly without waiting for some amount of time so that all inflight messages will get processed.  For example, storm is writing to file & user kills topology, file is not closed or moved to proper location. We need to educate operation guys to do the right things also there are some chances that it will be not followed causing system to go in inconsistent state.
 
Can we set mandatory timeout (configurable) when user kills storm topology? User should not be allowed kill topology with time less than mentioned time.

Some case: 
1) If topology is long running don't allow user to kill but time not less than mentioned one
2) If topology is just deployed allow him to kill instantly (as it might be some mistake)
3) Handle same cases from command-line.
"
STORM-2298,Don't kill Nimbus when ClusterMetricsConsumer is failed to initialize,"ClusterMetricsConsumerExecutor doesn't pass errors to Nimbus when sending metrics to ClusterMetricsConsumer, but passes errors to Nimbus when initializing ClusterMetricsConsumer and kills Nimbus.

Nimbus should have fault tolerance on ClusterMetricsConsumer so that it could drop cluster metrics but still be alive.

It might be also ideal to retry initialization if ClusterMetricsConsumerExecutor is about to send metrics but initialization of ClusterMetricsConsumer is failed."
STORM-2297,[storm-opentsdb] Support Flux for OpenTSDBBolt,"Due to some limitations of Flux, we can't use storm-opentsdb with Flux.

- Flux doesn't support static factory method
- Flux doesn't support List of references

While it would be great to support these via Flux, fixing storm-opentsdb to support Flux would be easier and no harm."
STORM-2296,Kafka spout - no duplicates on topic leader changes,"Current behavior of Kafka spout emits duplicate tuples whenever Kafka topic leader's change.
In case of exception caused by leader changes, PartitionManagers are simply recreated losing the state about which tuples were already emitted and new PartitionManager re-emits them again.

This is fine as at-least-once is fulfilled, but still it would be better to not emit duplicate data if possible.
Moreover this could be easily avoided by moving the state related to emitted tuples from old PartitionManager to new one.

Pull requests implementing this: 
1.0.x-branch - https://github.com/apache/storm/pull/1873
1.x-branch - https://github.com/apache/storm/pull/1888

Pull request for related bugfix: https://github.com/apache/storm/pull/1940"
STORM-2295,KafkaSpoutStreamsNamedTopics changing the sequence of fields name while emitting data,"If you look at below code *allFields* variable is HashSet. To which we have added the o/p of *kafkaSpoutStream.getOutputFields().toList()*. That sort data on hash basis rather than keeping same sequence.
{code:java}
	@Override
	public Fields getOutputFields() {
		final Set<String> allFields = new HashSet<>();
		for (KafkaSpoutStream kafkaSpoutStream : topicToStream.values()) {
			allFields.addAll(kafkaSpoutStream.getOutputFields().toList());
		}
		return new Fields(new ArrayList<>(allFields));
	}
{code}

Changes needed is below
{code:java}
final Set<String> allFields = new LinkedHashSet<>();
{code}"
STORM-2294,Send activate and deactivate command from ShellSpout,"When deactivate and activate are called on ShellSpout those calls should be send to the corresponding ShellProcess as multilang commands. 

*For Example:*
When a ShellSpout polls some data from any source those resouces can be gracefully allocated or deallocated on (de)activation. Otherwise there is no possibility to react on those events via multilang support."
STORM-2293,hostname should only refer node's 'storm.local.hostname',"This is reported bug from user mailing list.
https://lists.apache.org/thread.html/206dfc6aefda13f27bb8d41a86da98355767d522882841296e09070b@%3Cuser.storm.apache.org%3E

{code}
Hi Guys,

I'm running a topology on 3 supervisor. I've registred a custom
MetricsConsumer class that output metrics on Elasticsearch.

My bug is that the taskInfo.srcWorkerHost that always the same values as
the first supervisor (nimbus as well), Even when I put a parallelismHint to
3...

I'm using storm 1.0.2

Thanks for your feedback
{code}

The reason is that `hostname` refers topology configuration when finding storm.local.hostname. It should always refer node's configuration."
STORM-2291,A Hash Collision Problem of Fields Grouping in Windowing Method,"I‘d like to discuss the hash collision issue that occurs when applying the _Grouping_ method [http://storm.apache.org/releases/current/Concepts.html] to _Windowing_ method [http://storm.apache.org/releases/current/Windowing.html] in Storm.

I first assume the following situation. Spout constantly emits tuples to Bolt. At this time, Bolt tries to perform operations while moving tuples of a certain interval. e.g. moving average, etc. To solve this situation, Storm provides _Windowing_ method.

However, consider the following complex situation. Two problems are added in the above situation.
# As a first problem, the tuple emitted by Spout is multidimensional with multiple pieces of information. For example, Alice, Bob, and Clark are mapped to random real numbers. That is, the tuples emitted from Spout are {[Alice, 0.18322], [Clark, 0.57833], [Bob, 0.27902], [Clark, 0.24553], [Alice, 0.50164], [Alice, 0.06463], ...}. While those tuples are transmitted to the next windowed bolt, they must be necessarily separated by keys such as Alice, Bob, and Clark. In other words, each tuples in which Alice, Bob, and Clark are mapped must belong to different windows.
# The second problem is Storm's parallelism. Spout and Bolt can be operated as multiple objects on multiple servers. This problem is that the tuples with the same key must be emit in the same window even if they are created by a different Spout object.

Storm can specify the Bolt objects which the tuples is to be input as a _Grouping_ method. Storm provides various _Grouping_ methods, but a fields grouping is best suited as a way to solve the above problems. The fields grouping is a way of partitioning an input stream by a specified field. With the fields grouping, tuples of the same field can only be passed to the same Bolt object. However, the fields grouping has been implemented as a hash method. ([http://storm.apache.org/releases/current/Tutorial.html]) Therefore, it can be cause *a hash collision problem* that can include the tuples in the same window although they have different fields. So I am interested in solving the hash collision.

The source code of [https://github.com/dke-knu/i2am/tree/master/i2am-app/fields-window-grouping/src/main/java/org/fields/window/grouping/as_is] is a situation where the hash collision occurs. First, Spout randomly emits the tuples mapping Alice, Bob, and Clark on random real numbers. Next, Bolt which extends BaseWindowedBolt prints the TupleWindow objects received from Spout. At this time, Bolt uses the fields grouping. Spout emits three fields: Alice, Bob, and Clark. If the parallelism of Bolt is set less than 3, it surely cause the hash collision. Conversely, the greater the parallelism of Bolt than 3, the lower the probability of the hash collision. But, it can not be guaranteed that the hash collision does not occur.

As an alternative to this hash collision, I used two-step IRichBolt instead of BaseWindowedBolt. The source code for this is [https://github.com/dke-knu/i2am/tree/master/i2am-app/fields-window-grouping/src/main/java/org/fields/window/grouping/to_be]. The first Bolt is important. This Bolt takes tuples from Spout and manages them through a hash map of list according to the field. If the list is as filled as a predefined window size, the oldest tuple is removed and a new tuple is added. And then, Bolt emits this list to the next Bolt.

Using this method, the above problems can be solved. That is, the tuples of the same fields is always managed in the same window regardless of the number of fields and the number of parallelism. 

I'm concerned that this problem will frequently happen to many Storm users who use the Windowing method. If there is not a better way than the one I presented, I think there should be a new grouping method for Windowing method."
STORM-2290,Upgrading zookeeper to 3.4.9 for stability,"We should upgrade zookeeper to 3.4.9 as it brings in a lot of stability improvements (http://zookeeper.apache.org/releases.html) and storm is still using 3.4.6 (https://github.com/apache/storm/blob/master/pom.xml)

One serious issue affecting zookeeper 3.4.6 is https://issues.apache.org/jira/browse/ZOOKEEPER-1506 which prohibits zookeeper from getting a quorum and hence affects storm's stability as well."
STORM-2289,Intermittent failure on DRPCtest (high chance on Travis CI),"DRPCtest is failing with high chance on Travis CI. 
Here're the builds for master from recent pull requests:

- https://travis-ci.org/apache/storm/jobs/189442311 (failed)
- https://travis-ci.org/apache/storm/jobs/189324650 (succeed)
- https://travis-ci.org/apache/storm/jobs/188740322 (failed)
- https://travis-ci.org/apache/storm/jobs/191431175 (failed)

More than 50% are failing."
STORM-2288,Nimbus client can timeout in log running tests,
STORM-2287,DemoTest fails intermittently on 1.x-branch & master branch,"See the following runs:
https://travis-ci.org/apache/storm/builds/191507849"
STORM-2286,Storm Rebalance command should support arbitrary component parallelism,"For legacy reasons, config TOPOLOGY-TASKS is considered first when schedule a topology, for a component, if user don’t specify TOPOLOGY-TASKS, storm just override it to be equal to component parallelism hint, and schedule based on TOPOLOGY-TASKS later on.

This works for the most cases, but not Rebalance command. Now, when do Rebalance, the StormBase :component->executors attribute will be overridden in Zookeeper which is used to partition component tasks into executors, as we said above, the TOPOLOGY-TASKS is considered here as the real tasks number for components, something goes weird here:

If we override a bigger executor numbers for a component when do rebalance, it just don’t work because smaller TOPOLOGY-TASKS [ not changed since first submitted at all ]is partitioned into bigger number of executors which read from ZooKeeper overridden by Rebalance command, but for smaller task, it works fine.

I see that storm support a command like this now: [storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*] which indicate that user can override a component parallelism freely, i think it’s more sensible to support this and it's meaningless to have a restriction like before."
STORM-2284,Storm Worker Redesign,"Much has been learnt from evolving the 1.x line. We can now use the benefit of hindsight and apply these learnings into the future work on 2.x line. 

The goal is to rethink the Worker to improve performance, enhance its abilities and also retain compatibility.


*Overview Document*:
Also covers results from experiments that motivate this work.
https://docs.google.com/document/d/1EzeHL3d7EE-RyyBEpN7CwRmWz3oqjbbKiVVAlzFp2Nc/edit?usp=sharing    "
STORM-2283,Fix DefaultStateHandler kryo multithreading issues,
STORM-2279,Unable to open bolt page of storm ui,"With latest storm code, I am unable to open ui and see bolt information. I am using the vagrant setup. On the ui page that open, I see the following error.
{code}
Internal Server Error
org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1369)
	at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1353)
	at org.apache.storm.ui.core$component_page.invoke(core.clj:1026)
	at org.apache.storm.ui.core$fn__4308.invoke(core.clj:1214)
	at org.apache.storm.shade.compojure.core$make_route$fn__789.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__777.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__770.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__795.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__799.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__3573.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__3102.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__2152.invoke(helpers.clj:54)
	at org.apache.storm.ui.core$catch_errors$fn__4474.invoke(core.clj:1460)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__1844.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__1887.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__1816.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2139.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2125.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__1674.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__1678.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{code}
Url: http://node1:8080/component.html?id=SlidingTimeCorrectness-winSec1slideSec1VerificationBolt&topology_id=SlidingWindowTestw1s1-2-1483646178

There is a stacktrace corresponding to this in nimbus.log showing IndexOutOfBound error:
{code}
2017-01-05 19:57:26.934 pool-15-thread-41 o.a.s.d.n.Nimbus [WARN] getComponentPageInfo exception. (topo id='SlidingWindowTestw1s1-2-1483646178')
java.lang.ArrayIndexOutOfBoundsException: -2
        at java.util.ArrayList.elementData(ArrayList.java:418)
        at java.util.ArrayList.get(ArrayList.java:431)
        at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:3606)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4097)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4081)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

The problem is that we expect the index to be positive, but since it is a mod of hashcode it can be negative.
{code}
                int taskIndex = TupleUtils.listHashCode(Arrays.asList(componentId)) %
                        tasks.size();
                int taskId = tasks.get(taskIndex);
{code}
https://github.com/apache/storm/blob/2b82fc8b5328fd4fbd680998c6051d9496c102d7/storm-core/src/jvm/org/apache/storm/daemon/nimbus/Nimbus.java#L3605
"
STORM-2276,Remove twitter4j usages due to license issue (JSON.org is catalog X),"The ASF recently made the determination that the json.org license is category x. Storm doesn't depend on it directly, but Storm depends on twitter4j for storm-starter, and twitter4j depends on json.org.

This is a blocker for any releases. Please refer mail thread for detail: https://www.mail-archive.com/dev@storm.apache.org/msg40060.html"
STORM-2275,Nimbus crashed during state transition of topology,"I am copying last few lines of the nimbus logs including stack trace.
{code}
2017-01-04 22:18:10.106 pool-15-thread-47 o.a.s.d.n.Nimbus [INFO] Activating DemoTest: DemoTest-21-1483568289
2017-01-04 22:18:11.646 timer o.a.s.s.EvenScheduler [INFO] Available slots: [f0ea57ab-86d6-401f-9429-52f479b1d69f:6704, f0ea57ab-86d6-401f-9429-52f479b1d69f:6705, f0ea57ab-86d6-401f-9429-52f479b1d69f:670\
6, f0ea57ab-86d6-401f-9429-52f479b1d69f:6707, f0ea57ab-86d6-401f-9429-52f479b1d69f:6708, f0ea57ab-86d6-401f-9429-52f479b1d69f:6709, f0ea57ab-86d6-401f-9429-52f479b1d69f:6700, f0ea57ab-86d6-401f-9429-52f4\
79b1d69f:6701, f0ea57ab-86d6-401f-9429-52f479b1d69f:6702, f0ea57ab-86d6-401f-9429-52f479b1d69f:6703]
2017-01-04 22:18:11.648 timer o.a.s.d.n.Nimbus [INFO] Setting new assignment for topology id DemoTest-21-1483568289: Assignment(master_code_dir:storm-local, node_host:{f0ea57ab-86d6-401f-9429-52f479b1d69\
f=node1}, executor_node_port:{[10, 10]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [14, 14]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [16, 16]=NodeInfo(node:\
f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [12, 12]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [8, 8]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [6,\
 6]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [20, 20]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [4, 4]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f,\
 port:[6700]), [2, 2]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [18, 18]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [11, 11]=NodeInfo(node:f0ea57ab-86d6-401\
f-9429-52f479b1d69f, port:[6701]), [15, 15]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [7, 7]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [9, 9]=NodeInfo(node\
:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [21, 21]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [5, 5]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [3\
, 3]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [19, 19]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [17, 17]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d6\
9f, port:[6701]), [1, 1]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [13, 13]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700])}, executor_start_time_secs:{[12, 12]=1\
483568291, [6, 6]=1483568291, [18, 18]=1483568291, [2, 2]=1483568291, [8, 8]=1483568291, [14, 14]=1483568291, [16, 16]=1483568291, [20, 20]=1483568291, [4, 4]=1483568291, [10, 10]=1483568291, [9, 9]=1483\
568291, [3, 3]=1483568291, [15, 15]=1483568291, [21, 21]=1483568291, [5, 5]=1483568291, [11, 11]=1483568291, [13, 13]=1483568291, [17, 17]=1483568291, [19, 19]=1483568291, [1, 1]=1483568291, [7, 7]=14835\
68291}, worker_resources:{NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0), NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f,\
 port:[6701])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0), NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)})
2017-01-04 22:18:11.660 timer o.a.s.d.n.Nimbus [INFO] Cleaning up DemoTest-20-1483567429
2017-01-04 22:18:11.668 timer o.a.s.d.n.Nimbus [INFO] Removing dependency jars from blobs - []
2017-01-04 22:18:12.420 pool-15-thread-51 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormjar.jar
2017-01-04 22:18:12.990 pool-15-thread-38 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormcode.ser
2017-01-04 22:18:12.995 pool-15-thread-59 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormconf.ser
2017-01-04 22:18:20.303 timer o.a.s.d.n.Nimbus [INFO] TRANSITION: DemoTest-20-1483567429 REMOVE null false
2017-01-04 22:18:20.304 timer o.a.s.d.n.Nimbus [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1174)
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83)
Caused by: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.transition(Nimbus.java:1215)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1172)
        ... 1 more
2017-01-04 22:18:20.304 timer o.a.s.u.Utils [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$15(Nimbus.java:1107)
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:104)
2017-01-04 22:18:20.315 Thread-9 o.a.s.d.n.Nimbus [INFO] Shutting down master
{code}

The problem is that we are assuming that the base will be non-null which is incorrect leading to NPE."
STORM-2274,Support named output streams in Hdfs Spout,Currently it emits only to default output stream
STORM-2273,Starting nimbus from arbitrary dir fails,"Here is the output that I got:
{code}
storm@node1:/home/vagrant$ storm nimbus
Running: java -server -Ddaemon.name=nimbus -Dstorm.options= -Dstorm.home=/usr/share/apache-storm-2.0.0-SNAPSHOT -Dstorm.log.dir=/usr/share/apache-storm-2.0.0-SNAPSHOT/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64 -Dstorm.conf.file= -cp /usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-api-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-slf4j-impl-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/clojure-1.7.0.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/slf4j-api-1.7.7.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/asm-5.0.3.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/minlog-1.3.0.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/reflectasm-1.10.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-over-slf4j-1.6.6.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-core-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/kryo-3.0.3.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/storm-rename-hack-2.0.0-SNAPSHOT.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/disruptor-3.3.2.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/storm-core-2.0.0-SNAPSHOT.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/objenesis-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/servlet-api-2.5.jar:/usr/share/storm/conf -Xmx1024m -Dlogfile.name=nimbus.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/usr/share/apache-storm-2.0.0-SNAPSHOT/log4j2/cluster.xml org.apache.storm.daemon.nimbus.Nimbus
{code}
Log added to nimbus.log.
{code}
2017-01-04 21:44:33.089 main o.a.s.n.NimbusInfo [INFO] Nimbus figures out its name to node1
{code}
"
STORM-2272,LocalCluster can leak simulated time,If the constructor for LocalCluster throws an exception while configured for simulated time it can leak the simulated time and leave it on.
STORM-2269,Dynamic reconfiguration for the nodes in Nimbus/Pacemaker clusters,"Reference: https://zookeeper.apache.org/doc/trunk/zookeeperReconfig.html

It would be nice to have a similar functionality for Nimbus/Pacemaker clusters too.
As that would eliminate the need for restarting servers in the Nimbus/Pacemaker clusters whenever a node exits or joins these clusters.


----------------------------------------------------------
Reply from Bobby Evans on the dev group:
----------------------------------------------------------

There is nothing for that right now on pacemaker.
You can do it with nimbus so long as at least one of the original nodes is still up.

But in either case it would not be too difficult to make it all fully functional.
The two critical pieces would be in giving the workers and daemons a way to reload these specific configs dynamically.
Then it would be documenting the order of operations to be sure nothing goes wrong.

*Adding Pacemaker Node(s)*
# bring up the new node(s).
# update nimbus configs to start reading from the new nodes.
# update all of the worker nodes to let workers start writing to the new node.

*Removing Pacemaker Node(s)*
# Shut down pacemaker nodes/update configs on workers (order should not matter so long as there are enough pacemaker nodes up to handle the load)
# update the nimbus configs to not try and read from the old nodes


*Adding new Nimbus Node(s)*
# Bring up the new nimbus with the new config.
# update all of the other nodes (including any machines that clients come from) with new config (order does not matter)

*Removing Nimbus Node(s)*
# Shut down the old nodes and update the configs on all the boxes in any order you want.
# This should just work so long as you have at least one nimbus node still up.
"
STORM-2267,[storm-submit-tools] Use user's local maven repo. directory to local repo.,"We've found that dependency resolver fetches old SNAPSHOT artifacts from remote repository other than new SNAPSHOT artifacts from local. 
While this only affects SNAPSHOT artifacts, using SNAPSHOT artifacts is sometimes inevitable for developer. 
If possible, it would be better to resolve this so that developer can use their own SNAPSHOT artifacts for their works."
STORM-2265,Incorrectly Serialized JSON in TransactionalState causes Worker to Die,"TransactionalState uses JSONValue to serialize / deserialize objects. However, the object GlobalPartitionInformation is incorrectly serialized by default, causing the exception bellow. To get around this problem, GlobalPartitionInformation must implement JSONAware.

2016-12-23 14:37:26.980 o.a.s.e.e.ReportError Thread-21-$spoutcoord-spout-spout1-executor[2, 2] [ERROR] Error
java.lang.RuntimeException: java.lang.RuntimeException: Unexpected character (G) at position 1.
        at org.apache.storm.utils.Utils$6.run(Utils.java:2190) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.RuntimeException: Unexpected character (G) at position 1.
        at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:174) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$6.run(Utils.java:2179) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 1 more
Caused by: org.apache.storm.shade.org.json.simple.parser.ParseException
        at org.apache.storm.shade.org.json.simple.parser.Yylex.yylex(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.nextToken(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.JSONValue.parseWithException(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:167) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$6.run(Utils.java:2179) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 1 more
2016-12-23 14:37:26.987 o.a.s.u.Utils Thread-21-$spoutcoord-spout-spout1-executor[2, 2] [ERROR] Halting process: Worker died
java.lang.RuntimeException: Halting process: Worker died
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$4.run(Utils.java:1800) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException(ReportErrorAndDie.java:45) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1956) [?:1.8.0_112]
2016-12-23 14:37:26.987 o.a.s.d.w.Worker Thread-38 [INFO] Shutting down worker tkst-consumer-4-1482532570 556a1e7b-49f7-4dc2-a936-d17e5e4ba9de 6700
2016-12-23 14:37:26.988 o.a.s.d.w.Worker Thread-38 [INFO] Terminating messaging context
"
STORM-2264,OpaqueTridentKafkaSpout failing after STORM-2216,"I've seen OpaqueTridentKafkaSpout failing after STORM-2216.

{code}
java.lang.RuntimeException: Unexpected character (G) at position 1.
	at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:172) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$6.run(Utils.java:2179) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]
Caused by: org.apache.storm.shade.org.json.simple.parser.ParseException
	at org.apache.storm.shade.org.json.simple.parser.Yylex.yylex(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.nextToken(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.JSONValue.parseWithException(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 9 more
{code}

There's no bug on STORM-2216, but there has been limitation on TransactionalState which doesn't have type information and uses simple-json to serde, in result it can't serde GlobalPartitionInformation properly.

Previously we just ignored parse error but STORM-2216 exposes the issue.

We could make TransactionalState ignoring parse error again, but anyway it has been not intended behavior so better to be fixed properly."
STORM-2253,Storm PMML Bolt - Unit Tests,"Currently the patch has integration tests through the form of a test topology that loads a PMML Model and raw input data from a CSV file. The {@link RawInputFromCSVSpout}
creates a stream of tuples with raw inputs, and the {@link PMMLPredictorBolt} computes the predicted scores.

The main focus of the initial patch was to design the classes in such a way that they can accommodate arbitrary runtime environments. The default implementation provided uses one such runtime execution library, which is more suited to be tested using integration tests.

Will add some unit tests around to assert for edge and some common cases"
STORM-2250,Kafka Spout Refactoring to Increase Modularity and Testability,"Per the discussion here https://github.com/apache/storm/pull/1826 the KafkaSpout class should be split up a bit, and the unit tests should be improved to use time simulation and not break encapsulation on the spout to test."
STORM-2249,Make Distribution Scripts Put Examples to the Correct Locations,Make binary.xml put all the examples that exist in source packages with name pattern COMPONENT-NAME-examples-x.y.x.jar to STORM_HOME/examples/storm-COMPONENT-NAME-examples/COMPONENT-NAME-examples-x.y.x.jar
STORM-2248,Storm UI in Apache storm 1.0.2 does not update Executors and Tasks after Rebalance .Is anyone else facing this issue ,
STORM-2245,integration-test constant compilation failure,"The travis-ci constant build failure due to the following error:

{code}
[ERROR] COMPILATION ERROR : 
[ERROR] /home/travis/build/apache/storm/integration-test/src/main/java/org/apache/storm/ExclamationTopology.java:[80,11] error: cannot find symbol
{code}
"
STORM-2242,Trident state persisting does not honor batch.size.rows configuration,"Persisting the Trident state in {{org.apache.storm.cassandra.trident.state.CassandraState}} with batching enabled does not honor the configuration for {{cassandra.batch.size.rows}}.

This results in a warning at least:
{code}
10:33:33.720 [SharedPool-Worker-16] WARN  o.a.c.cql3.statements.BatchStatement - Batch of prepared statements for [gin.ngram_count] is of size 5200, exceeding specified threshold of 5120 by 80.
{code}

An exception like this is also possible:
{code}
10:30:54.287 [SharedPool-Worker-1] ERROR o.a.c.cql3.statements.BatchStatement - Batch of prepared statements for [gin.df] is of size 103428, exceeding specified threshold of 51200 by 52228. (see batch_size_fail_threshold_in_kb)
10:30:54.295 [Thread-29-b-1-executor[7 7]] WARN  o.a.s.c.trident.state.CassandraState - Batch write operation is failed.
10:30:54.297 [Thread-29-b-1-executor[7 7]] ERROR org.apache.storm.daemon.executor -
com.datastax.driver.core.exceptions.InvalidQueryException: Batch too large
    at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:50) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:64) ~[cassandra-driver-core-3.1.0.jar:na]
    at org.apache.storm.cassandra.trident.state.CassandraState.updateState(CassandraState.java:159) ~[storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.cassandra.trident.state.CassandraStateUpdater.updateState(CassandraStateUpdater.java:34) [storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.cassandra.trident.state.CassandraStateUpdater.updateState(CassandraStateUpdater.java:30) [storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.trident.planner.processor.PartitionPersistProcessor.finishBatch(PartitionPersistProcessor.java:98) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.trident.planner.SubtopologyBolt.finishBatch(SubtopologyBolt.java:151) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.trident.topology.TridentBoltExecutor.finishBatch(TridentBoltExecutor.java:266) [storm-core-1.0.2.jar:1.0.2]
{code}

This effectivly disables the usage of batching."
STORM-2240,STORM PMML Bolt - Add Support to Load Models from Blob Store,This PR follows from a [suggestion/request|https://github.com/apache/storm/pull/1816#discussion_r91586225] made by [~ptgoetz] during the PR review.
STORM-2239,New Kafka spout does not properly handle interrupts,"The KafkaConsumer underlying the new Kafka spout had a bug that meant that it could enter an infinite loop if certain methods were called on it from an interrupted thread. This can cause local mode clusters to hang on shutdown. It is fixed in the next Kafka release (https://issues.apache.org/jira/browse/KAFKA-4387), but a side effect is that some blocking calls on the consumer can now throw an unchecked InterruptedException variant. This will cause the executor to crash. The spout should instead catch the exception if it occurs and set thread interrupted state before returning control to Storm."
STORM-2236,storm kafka client should support manual partition management.,"Currently storm kafka client relies on kafka to assign partition to each spout. This may cause unnecessary rebalance in cases where storm itself, e.g. worker restart, slow processing of tuples."
STORM-2235,Introduce new option: 'add remote repositories' for dependency resolver,"Sometimes we need to pull the artifacts from other than maven central. For now dependency resolver in storm-submit-tool doesn't support adding remote repositories, so it would be better to add the feature."
STORM-2234,heartBeatExecutorService in shellSpout don't work well with deactivate ,"When using the activate and deactivate of a shellSpout (using the client):
1 .First we deactivate -which calls :  {quote}
heartBeatExecutorService.shutdownNow();{quote}
2. Then we actiavate which calls: {quote}         heartBeatExecutorService.scheduleAtFixedRate(new SpoutHeartbeatTimerTask(this), 1, 1, TimeUnit.SECONDS);
{quote}
3.This results in an {quote} RejectedExecutionException {quote} as we *already* shutdown the heartBeatExecutorService.
4.Simple test to prove this:
 {quote}{noformat}    
private class TestTimerTask extends TimerTask {


        @Override
        public void run() {
            System.out.println(""Im running now"");
        }
    }

    @Test(expectedExceptions = RejectedExecutionException.class)
    public void heartBeatShutdownTest() {
        ScheduledExecutorService heartBeatExecutorService = MoreExecutors.getExitingScheduledExecutorService(new ScheduledThreadPoolExecutor(1));
        heartBeatExecutorService.scheduleAtFixedRate(new TestTimerTask(), 1, 1, TimeUnit.SECONDS);
        heartBeatExecutorService.shutdownNow();
        heartBeatExecutorService.scheduleAtFixedRate(new TestTimerTask(), 1, 1, TimeUnit.SECONDS);
    }{noformat}{quote}

5.Already created a fix and opening a PR with it: https://github.com/apache/storm/pull/1813"
STORM-2233,BlobStore Cleanup/Optimization,"KeySequenceNumber creates a new connection to zookeeper each time.  This really does not need to happen.  At a minimum we should cache the connection and/or reuse it.

Also everywhere that we check for LocalBlobStore so we can store some blob state should be refactored so it can be inside by LocalBlobStore itself. "
STORM-2231,NULL in DisruptorQueue while multi-threaded ack,"I use simple topology with one spout (9 workers) and one bolt (9 workers).
I have topology.backpressure.enable: false in storm.yaml.
Spouts send about 10 000 000 tuples in 10 minutes. Pending for spout is 80 000.
Bolts buffer theirs tuples for 60 seconds and flush to database and ack tuples in parallel (10 threads).
I read that OutputCollector can be used in many threads safely, so i use it.
I don't have any bottleneck in bolts(flushing to database) or spouts(kafka spout), but about 2% of tuples fail due to tuple processing timeout (fails are recordered in spout stats only).
I am sure that bolts ack all tuples. But some of acks don't come to spouts.

While multi-threaded acking i see many errors in worker logs like that:
2016-12-01 13:21:10.741 o.a.s.u.DisruptorQueue [ERROR] NULL found in disruptor-executor[3 3]-send-queue:853877

I tried to use synchronized wrapper around OutputCollector to fix the error. But it didn't help.

I found the workaround that helps me: i do all processing in bolt in multiple threads but call OutputCollector.ack methods in a one single separate thread.

I think Storm has an error in the multi-threaded use of OutputCollector.

If my topology has much less load, like 500 000 tuples per 10 minutes, then  i don't lose any acks."
STORM-2230,Unable to send same topic in different streams using KafkaSpoutStreamsNamedTopics,
STORM-2229,KafkaSpout does not resend failed tuples,"When the topology fails a tuple, it is never resent by the KafkaSpout. This can easily be shown by constructing a small topology failing every tuple.

Apparent reason:

{code}
public class KafkaSpout<K, V> extends BaseRichSpout {
//...
private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                kafkaConsumer.seekToEnd(toArrayList(rtp));    // Seek to last committed offset <== Does seek to end of partition
            }
        }
    }
{code}

The code seeks to the end of the partition instead of seeking to the first uncommited offset.

Preliminary fix (worked for me, but needs to be checked by an expert)

{code}
    private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                OffsetAndMetadata committed = kafkaConsumer.committed(rtp);
                if(committed == null) {
                    // No offsets commited yet for this partition - start from beginning 
                    kafkaConsumer.seekToBeginning(toArrayList(rtp));
                } else {
                   // Seek to first uncommitted offset
                    kafkaConsumer.seek(rtp, committed.offset() + 1);
                }
            }
        }
    }
{code}
"
STORM-2228,KafkaSpout does not replay properly when a topic maps to multiple streams,"In the example.

KafkaSpoutTopologyMainNamedTopics.java

The code creates a TuplesBuilder and a KafkaSpoutStreams

{code}
protected KafkaSpoutTuplesBuilder<String, String> getTuplesBuilder() {
    return new KafkaSpoutTuplesBuilderNamedTopics.Builder<>(
            new TopicsTest0Test1TupleBuilder<String, String>(TOPICS[0], TOPICS[1]),
            new TopicTest2TupleBuilder<String, String>(TOPICS[2]))
            .build();
}

protected KafkaSpoutStreams getKafkaSpoutStreams() {
    final Fields outputFields = new Fields(""topic"", ""partition"", ""offset"", ""key"", ""value"");
    final Fields outputFields1 = new Fields(""topic"", ""partition"", ""offset"");
    return new KafkaSpoutStreamsNamedTopics.Builder(outputFields, STREAMS[0], new String[]{TOPICS[0], TOPICS[1]})  // contents of topics test, test1, sent to test_stream
            .addStream(outputFields, STREAMS[0], new String[]{TOPICS[2]})  // contents of topic test2 sent to test_stream
            .addStream(outputFields1, STREAMS[2], new String[]{TOPICS[2]})  // contents of topic test2 sent to test2_stream
            .build();
}
{code}

Essentially the code is trying to take {{TOPICS\[0]}}, {{TOPICS\[1]}}, and {{TOPICS\[2]}} translate them to {{Fields(""topic"", ""partition"", ""offset"", ""key"", ""value"")}} and output them on {{STREAMS\[0]}}. Then just for {{TOPICS\[2]}} they want it to be output as {{Fields(""topic"", ""partition"", ""offset"")}} to {{STREAMS\[2]}}.  (Don't know what happened to {{STREAMS\[1]}})

There are two issues here.  First with how the TupleBuilder and the SpoutStreams are split up, but coupled {{STREAMS\[2]}} is actually getting the full ""topic"" ""partition"" ""offset"" ""key"" ""value"", but this minor.  The real issue is that the code uses the same KafkaSpoutMessageId for all the tuples emitted to both {{STREAMS\[1]}} and {{STREAMS\[2]}}.

https://git.corp.yahoo.com/storm/storm/blob/5bcbb8d6d700d0d238d23f8f6d3976667aaedab9/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L284-L304

The code, however, is written to assume that it will only ever get one ack/fail for a given KafkaSpoutMessageId.  This means that if one of the emitted tuple trees succeed and then the other fails, the failure will not result in anything being replayed!  This violates how storm is intended to work.

I discovered this as a part of STORM-2225, and I am fine with fixing it on STORM-2225 (I would just remove support for that functionality because there are other ways of doing this correctly).  But that would not maintain backwards compatibility and I am not sure it would be appropriate for 1.x releases.  I really would like to have feedback from others on this.

I can put something into 1.x where it will throw an exception if acking is enabled and this situation is present, but I don't want to spend the time tying to do reference counting on the number of tuples actually emitted.  If someone else wants to do that I would be happy to turn this JIRA over to them."
STORM-2227,Better User Control of GC Options,"As a user, I would like to override certain JVM garbage connection options instead of overriding them all, so that I can make simpler, safer changes.

Currently, if a user wants to add some gc option in a topology, the user must copy everything from {{worker.gc.childopts}} to {{topology.worker.gc.childopts}} and make needed edits/additions.  This is error prone, since the provided cluster-wide options can change, and because they are overwritten by default.

A user can easily override settings unwittingly by adding new options if they forget to also copy the cluster-wide settings."
STORM-2226,New kafka spout offset lag tool does not work for secured kafka setup,
STORM-2225,Kafka New API make simple things simple,"The Kafka spouts in storm-kafka-client use the new API and are very extendable, but doing very simple things take way too many lines of code.

For example to create a KafkaTridentSpoutOpaque you need the following code (from the example).

{code}
    private KafkaTridentSpoutOpaque<String, String> newKafkaTridentSpoutOpaque() {
        return new KafkaTridentSpoutOpaque<>(new KafkaTridentSpoutManager<>(
                        newKafkaSpoutConfig(
                        newKafkaSpoutStreams())));
    }

    private KafkaSpoutConfig<String,String> newKafkaSpoutConfig(KafkaSpoutStreams kafkaSpoutStreams) {
        return new KafkaSpoutConfig.Builder<>(newKafkaConsumerProps(),
                    kafkaSpoutStreams, newTuplesBuilder(), newRetryService())
                .setOffsetCommitPeriodMs(10_000)
                .setFirstPollOffsetStrategy(EARLIEST)
                .setMaxUncommittedOffsets(250)
                .build();
    }

    protected Map<String,Object> newKafkaConsumerProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(KafkaSpoutConfig.Consumer.BOOTSTRAP_SERVERS, ""127.0.0.1:9092"");
        props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""kafkaSpoutTestGroup"");
        props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, ""org.apache.kafka.common.serialization.StringDeserializer"");
        props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, ""org.apache.kafka.common.serialization.StringDeserializer"");
        props.put(""max.partition.fetch.bytes"", 200);
        return props;
    }

    protected KafkaSpoutTuplesBuilder<String, String> newTuplesBuilder() {
        return new KafkaSpoutTuplesBuilderNamedTopics.Builder<>(
                new TopicsTupleBuilder<String, String>(TOPIC_1, TOPIC_2))
                .build();
    }

    protected KafkaSpoutRetryService newRetryService() {
        return new KafkaSpoutRetryExponentialBackoff(new KafkaSpoutRetryExponentialBackoff.TimeInterval(500L, TimeUnit.MICROSECONDS),
                KafkaSpoutRetryExponentialBackoff.TimeInterval.milliSeconds(2),
                Integer.MAX_VALUE, KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(10));
    }

    protected KafkaSpoutStreams newKafkaSpoutStreams() {
        return new KafkaSpoutStreamsNamedTopics.Builder(new Fields(""str""), new String[]{""test-trident"",""test-trident-1""}).build();
    }

    protected static class TopicsTupleBuilder<K, V> extends KafkaSpoutTupleBuilder<K,V> {
        public TopicsTupleBuilder(String... topics) {
            super(topics);
        }
        @Override
        public List<Object> buildTuple(ConsumerRecord<K, V> consumerRecord) {
            return new Values(consumerRecord.value());
        }
    }
{code}

All of this so I can have a trident spout that reads <String, String> values from ""localhost:9092"" on the topics ""test-trident"" and ""test-trident-1"" and outputting the value as the field ""str"".

I shouldn't need 50 lines of code for something I can explain in 3 lines of test.  It feels like we need to have some better defaults, and less overhead on a lot of these things."
STORM-2224,Expose a method to override in computing the field from given tuple in FieldSelector,org.apache.storm.cassandra.query.selector.FieldSelector should give a way to customize computing field value from tuple.
STORM-2223,Storm PMML Bolt,This JIRA is to build a Storm PMML bolt which uses JPMML library to load PMML doc and evaluate the incoming tuples based on the user provided PMML doc.
STORM-2222,Repeated NPEs thrown in nimbus if rebalance fails,"If the nimbus daemon crashed during a rebalance (rebalance didn't finish yet) and the daemon is restarted, it will always throw NPEs afterwards due to the wait time secs being gone after the restart. "
STORM-2221,Update DRPC Example,"Provide an example of how to do DRPC properly, and not use the deprecated LinearDRPCTopologyBuilder."
STORM-2220,Adding config keys for CassandraBolts instead of taking at topology level configuration.,Currently Cassandra bolts takes cassandra cluster configuration fro storm topology configuration. This is restrictive once it has two different cassandra bolts talking to different cassandra endpoints. Give a way to pass cassandra conf to any cassandra bolt. 
STORM-2211,KafkaSpout error after recreating kafka topic,"I have a storm topology with a KakfaSpout which was processing messages successfully. I shut down the topology, deleted the kafka topic and recreated it. After restarting the topology, it appears storm saved the partition in zookeeper and was unable to identify that the partition no longer existed. I saw the following error message in the storm logs:

o.a.s.k.KafkaUtils [WARN] Partition{host=<removed>, topic=<removed>, partition=0} Got fetch request with offset out of range: [10650]"
STORM-2210,ShuffleGrouping does not produce even distribution,"When testing the ShuffleGrouping in a multithreaded environment, it produces an extremely uneven distribution.

This appears to be a result of the Collection.shuffle call here. https://github.com/apache/storm/blob/1.0.x-branch/storm-core/src/jvm/org/apache/storm/grouping/ShuffleGrouping.java#L58

Because current was set to zero before the shuffle, other threads are able to access the arrayList while it is being shuffled.

Stephen's gist here includes a test that results in a very uneven distribution of taskIds from the ShuffleGrouping: https://gist.github.com/Crim/61537958df65a5e13b3844b2d5e28cde

I would have expected the taskIds from the ShuffleGrouping to be almost uniformly distributed."
STORM-2208,HDFS State Throws FileNotFoundException in Azure Data Lake Store file system (adl://),This is caused by the fact that AFS does not keep a handle to the file when the file is deleted.
STORM-2207,Kafka Spout NullPointerException during ack,"This occurs on startup of the topology.  There should be some null check safeguards, but i'm not sure what's causing it to occur in the first place...my guess is  the topic partition is not found in the ack map.

2016-11-17 23:11:05.366 o.a.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:420) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:69) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:628) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]
Caused by: java.lang.NullPointerException
    at org.apache.storm.kafka.spout.KafkaSpout.ack(KafkaSpout.java:316) ~[stormjar.jar:?]
    at org.apache.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:448) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$tuple_action_fn__7996.invoke(executor.clj:536) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$mk_task_receiver$fn__7979.invoke(executor.clj:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$clojure_handler$reify__7492.onEvent(disruptor.clj:40) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:451) ~[storm-core-1.0.2.jar:1.0.2]
    ... 7 more
2016-11-17 23:11:05.379 o.a.s.d.executor [ERROR] 
java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:420) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:69) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:628) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]
Caused by: java.lang.NullPointerException
    at org.apache.storm.kafka.spout.KafkaSpout.ack(KafkaSpout.java:316) ~[stormjar.jar:?]
    at org.apache.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:448) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$tuple_action_fn__7996.invoke(executor.clj:536) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$mk_task_receiver$fn__7979.invoke(executor.clj:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$clojure_handler$reify__7492.onEvent(disruptor.clj:40) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:451) ~[storm-core-1.0.2.jar:1.0.2]
    ... 7 more
2016-11-17 23:11:05.473 o.a.s.util [ERROR] Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
    at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.worker$fn__8663$fn__8664.invoke(worker.clj:765) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$mk_executor_data$fn__7875$fn__7876.invoke(executor.clj:274) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:494) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]

The method and line number in question below:

@Override
    public void ack(Object messageId) {
        final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;
        if (!consumerAutoCommitMode) {  // Only need to keep track of acked tuples if commits are not done automatically
            acked.get(msgId.getTopicPartition()).add(msgId);
        }
        emitted.remove(msgId);
}
"
STORM-2205,Racecondition in getting nimbus summaries while ZK connections are reconnected.,
STORM-2204,Add caching to HBaseLookupBolt,"Add capability to cache Results from HBase lookup in HBaseLookupBolt. 

- Caching is disabled by default.
- Enabled by configuration
   hbase.cache.enable = true
   hbase.cache.ttl.seconds = 300 
   hbase.cache.size = 1000 

Using Guava LoadingCache implementation to create an LRU cache.

Also marking OutputCollector as transient in AbstractHbaseBolt (it should be marked as transient)

https://github.com/apache/storm/blob/cd5c9e8f904205a6ca6eee9222ca954ca8b37ec3/external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java

https://github.com/apache/storm/pull/1783"
STORM-2200,[Storm SQL] Drop Aggregate & Join support on Trident mode,"This was already discussed on dev@ mailing list. [1] 

As aggregation and join don't make sense for streaming without windowing, we're not ready to handle them on SQL semantic. Unless we're ready, it would be better to drop them to not making any confusions. (Aggregation and join on Trident semantic could be different from what users expect.) 

[1] http://mail-archives.apache.org/mod_mbox/storm-dev/201610.mbox/%3CCAF5108hNLfSJq+pUvD_RRMtGKmrGV7yQSwqn3j8dA9SaN6HcMg@mail.gmail.com%3E"
STORM-2198,perform RotationAction when stopping HdfsBolt,"I have a _HdfsBolt_ with _TimedRotationPolicy_ and _MoveFileAction_. I found the bolt didn't move files when I stopped the HdfsBolt, then _RotationPolicy_ was not triggered, 
Look at the code, the _rotateOutputFile_ method just be called when _RotationPolicy_ is triggered or _writer.needsRotation_. I will add some logic in _cleanup_ method."
STORM-2197,NimbusClient connectins leak due to leakage in ThriftClient.,"Nimbus client connections are not closed when there are errors while connecting to nimbus. Created TSocket in ThriftClient should have been closed in case of errors.

2016-11-03 08:09:37.766 b.s.s.a.k.KerberosSaslTransportPlugin [ERROR] Client failed to open SaslClientTransport to interact with a server during session initiation: org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed
org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed
	at org.apache.thrift7.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:277) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_60]
	at javax.security.auth.Subject.doAs(Subject.java:415) [?:1.7.0_60]
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:103) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:72) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:106) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:82) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.ui.core$nimbus_summary.invoke(core.clj:584) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.ui.core$fn__10334.invoke(core.clj:1009) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$make_route$fn__7476.invoke(core.clj:93) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$if_route$fn__7464.invoke(core.clj:39) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$if_method$fn__7457.invoke(core.clj:24) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$routing$fn__7482.invoke(core.clj:106) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at clojure.core$some.invoke(core.clj:2515) [clojure-1.6.0.jar:?]"
STORM-2194,ReportErrorAndDie doesn't always die,"I've been trying to track down a cause of some of our issues with some exceptions leaving Storm workers in a zombified state for some time. I believe I've isolated the bug to the behaviour in :report-error-and-die/reportErrorAndDie in the executor. Essentially:

{code}
     :report-error-and-die (fn [error]
                             (try
                               ((:report-error <>) error)
                               (catch Exception e
                                 (log-message ""Error while reporting error to cluster, proceeding with shutdown"")))
                             (if (or
                                    (exception-cause? InterruptedException error)
                                    (exception-cause? java.io.InterruptedIOException error))
                               (log-message ""Got interrupted excpetion shutting thread down..."")
                               ((:suicide-fn <>))))
{code}

has the grouping for the if statement slightly wrong. It shouldn't log OR die from InterruptedException/InterruptedIOException, but it should log under that condition, and ALWAYS die. 

Basically:

{code}
     :report-error-and-die (fn [error]
                             (try
                               ((:report-error <>) error)
                               (catch Exception e
                                 (log-message ""Error while reporting error to cluster, proceeding with shutdown"")))
                             (if (or
                                    (exception-cause? InterruptedException error)
                                    (exception-cause? java.io.InterruptedIOException error))
                               (log-message ""Got interrupted excpetion shutting thread down...""))
                             ((:suicide-fn <>)))
{code}

After digging into the Java port of this code, it looks like a different bug was introduced while porting:

{code}
        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
            LOG.info(""Got interrupted exception shutting thread down..."");
            suicideFn.run();
        }
{code}

Was how this was initially ported, and STORM-2142 changed this to:

{code}
        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
            LOG.info(""Got interrupted exception shutting thread down..."");
        } else {
            suicideFn.run();
        }
{code}

However, I believe the correct port is as described above:

{code}
        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
            LOG.info(""Got interrupted exception shutting thread down..."");
        }
        suicideFn.run();
{code}

I'll look into providing patches for the 1.x and 2.x branches shortly."
STORM-2193,Storm UI/Logviewer passing in params in wrong order to FilterConfiguration,"FilterConfiguration has a few constructors and the order of the params on one of them seems off, but both the ui and logviewer are passing the params in the wrong order to it."
STORM-2191,shorten classpaths in worker and LogWriter commands,"When launching the worker daemon and its wrapping LogWriter daemon, the commands can become so long that they eclipse the default Linux limit of 4096 bytes. That results in commands that are cut off in {{ps}} output, and prevents easily inspecting the system to see even what processes are running.

The specific scenario in which this problem can be easily triggered: *running Storm on Mesos*.

h5. Details on why it happens:
# using the default Mesos containerizer instead of Docker containers, which causes the storm-mesos package to be unpacked into the Mesos executor sandbox.
# The [""expand all jars on classpath""|https://github.com/apache/storm/blob/6dc6407a01d032483edebb1c1b4d8b69a304d81c/bin/storm.py#L114-L140] functionality in the {{bin/storm.py}} script causes every one of the jars that storm bundles into its lib directory to be explicitly listed in the command.
#* e.g., say the mesos work dir is {{/var/run/mesos/work_dir/}}
#* and say that the original classpath argument in the supervisor cmd includes the following for the {{lib/}} dir in the binary storm package:
#** {{/var/run/mesos/work_dir/slaves/2357b762-6653-4052-ab9e-f1354d78991b-S12/frameworks/20160509-084241-1086985738-5050-32231-0000/executors/STORM_TOPOLOGY_ID/runs/e6a1407e-73fd-4be4-8d00-e882117b3391/storm-mesos-0.1.7-storm0.9.6-mesos0.28.2/lib/*}}
#* That leads to a hugely expanded classpath argument for the LogWriter and Worker daemons that get launched:
#** {{/var/run/mesos/work_dir/slaves/2357b762-6653-4052-ab9e-f1354d78991b-S12/frameworks/20160509-084241-1086985738-5050-32231-0000/executors/STORM_TOPOLOGY_ID/runs/e6a1407e-73fd-4be4-8d00-e882117b3391/storm-mesos-0.1.7-storm0.9.6-mesos0.28.2/lib/asm-4.0.jar:/var/run/mesos/work_dir/slaves/2357b762-6653-4052-ab9e-f1354d78991b-S12/frameworks/20160509-084241-1086985738-5050-32231-0000/executors/STORM_TOPOLOGY_ID/runs/e6a1407e-73fd-4be4-8d00-e882117b3391/storm-mesos-0.1.7-storm0.9.6-mesos0.28.2/lib/carbonite-1.4.0.jar:...}}"
STORM-2190,Topology submission blocked behind scheduling,"The submit-lock in nimbus seems to protect some very large and slow sections of code.  As more and more topologies are submitted scheduling can take longer and longer to complete making submitting a topology take increasingly longer.  But most of scheduling does not need to be protected by this lock.  Only a small section of the scheduler pulls state from zookeeper that the lock protects elsewhere.

We should split this lock up and protect scheduling separate from protecting StormBase stored in zk."
STORM-2183,BaseStatefulBoltExecutor does not handle cyclic graphs,"BaseStatefulBoltExecutor::getCheckpointInputTaskCount() returns the number of sources that a state transaction must wait for to process a transaction.  In a graph where there is a loop (e.g. A->B->C->D->C) components 'C' and 'D' the required number of tuples can will never be received.  The function shouldProcessTransaction will never receive the correct number of tuples, because the set required to come back form 'D' to 'C' will never be forwarded from 'C' to  'D' to begin with.  

Bolt 'C' and 'D' never finish the state initialization step and as such will never pass tuples to their wrapped bolt."
STORM-2182,Refactor Storm Kafka Examples Into Own Modules,Refactor storm-kafka-client and storm-kafka examples similarly to what was done in STORM-1970
STORM-2179,Storm.py doesn't detect JAVA_HOME properly on Windows,"line 92 in storm.py, should be inclusing a check for OS and using java.exe (Windows) vs java (Linux).

JAVA_CMD = 'java' if not JAVA_HOME else os.path.join(JAVA_HOME, 'bin', 'java.exe')"
STORM-2178,version not being returned on Windows,"""storm.cmd version"" returns blank string."
STORM-2176,Workers do not shutdown cleanly and worker hooks don't run when a topology is killed,"This appears to have been introduced in the 1.0.0 release. The issues does not seem to affect 0.10.2.

When a topology is killed and workers receive the notification to shutdown, they do not shutdown cleanly, so worker hooks never get invoked.

When a worker shuts down cleanly, the worker logs should contain entries such as the following:

{code}
2016-10-28 18:52:06.273 b.s.d.worker [INFO] Shut down transfer thread
2016-10-28 18:52:06.279 b.s.d.worker [INFO] Shutting down default resources
2016-10-28 18:52:06.287 b.s.d.worker [INFO] Shut down default resources
2016-10-28 18:52:06.351 b.s.d.worker [INFO] Disconnecting from storm cluster state context
2016-10-28 18:52:06.359 b.s.d.worker [INFO] Shut down worker exclaim-1-1477680593 61bddd66-0fda-4556-b742-4b63f0df6fc1 6700
{code}

In the 1.0.x line of releases (and presumably 1.x, though I haven't checked) this does not happen -- the worker shutdown process appears to get stuck shutting down executors (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/daemon/worker.clj#L666), no further log messages are seen in the worker log, and worker hooks do not run.

There are two properties that affect how workers exit. The first is the configuration property {{supervisor.worker.shutdown.sleep.secs}}, which defaults to 1 second. This corresponds to how long the supervisor will wait for a worker to exit gracefully before forcibly killing it with {{kill -9}}. When this happens the supervisor will log that the worker terminated with exit code 137 (128 + 9).

The second property is a hard-coded 1 second delay (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/util.clj#L463) added as a shutdown hook that will call {{Runtime.halt()}} if the delay is exceeded. When this happens, the supervisor will log that the worker terminated with exit code 20 (hard-coded).

Side Note: The hardcoded halt delay in worker.clj and the default value for {{supervisor.worker.shutdown.sleep.secs}} both being 1 second should probably be changed since it creates a race to see whether the supervisor delay or the worker delay wins.


To test this, I set {{supervisor.worker.shutdown.sleep.secs}} to 15 to allow plenty of time for the worker to exit gracefully, and deployed and killed a topology. In this case the supervisor consistently reported exit code 20 for the worker, indicating the hard-coded shutdown hook caused the worker to exit.

I thought the hard-coded 1 second shutdown hook delay might not be long enough for the worker to shutdown cleanly. To test that hypothesis, I changed the hard-code delay to 10 seconds, leaving {{supervisor.worker.shutdown.sleep.secs}} at 15 seconds. Again supervisor reported an exit code of 20 for the worker, and there were no log messages indicating the worker had exited cleanly and that the worker hook had run.
"
STORM-2175,Supervisor V2 can possibly shut down workers twice in local mode,"See https://github.com/apache/storm/pull/1697#issuecomment-256456889

{code}
java.lang.NullPointerException
    at org.apache.storm.utils.DisruptorQueue$FlusherPool.stop(DisruptorQueue.java:110)
    at org.apache.storm.utils.DisruptorQueue$Flusher.close(DisruptorQueue.java:293)
    at org.apache.storm.utils.DisruptorQueue.haltWithInterrupt(DisruptorQueue.java:410)
    at org.apache.storm.disruptor$halt_with_interrupt_BANG_.invoke(disruptor.clj:77)
    at org.apache.storm.daemon.executor$mk_executor$reify__4923.shutdown(executor.clj:412)
    at sun.reflect.GeneratedMethodAccessor303.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify__5552$shutdown_STAR___5572.invoke(worker.clj:668)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify$reify__5598.shutdown(worker.clj:706)
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:66)
    at org.apache.storm.ProcessSimulator.killAllProcesses(ProcessSimulator.java:79)
    at org.apache.storm.testing$kill_local_storm_cluster.invoke(testing.clj:207)
    at org.apache.storm.testing4j$_withLocalCluster.invoke(testing4j.clj:93)
    at org.apache.storm.Testing.withLocalCluster(Unknown Source)
{code}

and

{code}
java.lang.IllegalStateException: Timer is not active
    at org.apache.storm.timer$check_active_BANG_.invoke(timer.clj:87)
    at org.apache.storm.timer$cancel_timer.invoke(timer.clj:120)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify__5552$shutdown_STAR___5572.invoke(worker.clj:682)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify$reify__5598.shutdown(worker.clj:706)
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:66)
    at org.apache.storm.ProcessSimulator.killAllProcesses(ProcessSimulator.java:79)
    at org.apache.storm.testing$kill_local_storm_cluster.invoke(testing.clj:207)
    at org.apache.storm.testing4j$_withLocalCluster.invoke(testing4j.clj:93)
    at org.apache.storm.Testing.withLocalCluster(Unknown Source)
{code}

[~Srdo] is still working on getting a reproducible use case for us. But I will try to reproduce/fix it myself in the mean time."
STORM-2171,blob recovery on a single host results in deadlock,"It might be more versions but I have only tested this on 2.x.

Essentially when trying to find replicas to copy blobs from LocalFSBlobStore does not exclude itself.  This results in a deadlock where it is holding a lock trying to download the blob, and at the same time has done a request back to itself trying to download the blob, but it will never finish because it is blocked on the same lock."
STORM-2158,OutOfMemoryError in Nimbus' SimpleTransportPlugin,"{{OutOfMemoryError}} is thrown by Nimbus' {{SimpleTransportPlugin}} if malformed Thrift request is posted:
{code}
echo ""Hello"" | nc localhost 6627
{code}

In nimbus.log:
{noformat}
2016-10-20 12:54:09.978 b.s.d.nimbus [INFO] Starting Nimbus server...
2016-10-20 12:54:42.926 o.a.t.s.THsHaServer [ERROR] run() exiting due to uncaught error
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[?:1.8.0_92-internal]
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) ~[?:1.8.0_92-internal]
	at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:207) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:158) [storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
2016-10-20 12:54:42.942 b.s.d.nimbus [INFO] Shutting down master
2016-10-20 12:54:43.003 b.s.d.nimbus [INFO] Shut down master
{noformat}

The problem is caused by the lack of specification of the {{maxReadBufferBytes}} of {{THsHaServer}}'s arguments."
STORM-2152,Upgrade Curator Framework to Latest Version (3.2.0),
STORM-2150,ShellBolt raise subprocess heartbeat timeout Exception,"I've got a simple topology running with Storm 1.0.1. The topology consists of a KafkaSpout and several python multilang ShellBolt. I frequently got the following exceptions. 

{code}
java.lang.RuntimeException: subprocess heartbeat timeout at org.apache.storm.task.ShellBolt$BoltHeartbeatTimerTask.run(ShellBolt.java:322) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
{code}

More information here:
1. Topology run with ACK mode.
2. Topology had 40 workers.
3. Topology emitted about 10 milliom tuples every 10 minutes. 


Every time subprocess heartbeat timeout, workers would restart and python processes exited with exitCode:-1, which affected processing capacity and stability of the topology. 

I've checked some related issues from Storm Jira. I first found STORM-1946 reported a bug related to this problem and said bug had been fixed in Storm 1.0.2. However I got the same exception even after I upgraded Storm to 1.0.2.

I checked other related issues. Let's look at history of this problem.
DashengJu first reported this problem with Non-ACK mode in STORM-738. STORM-742 discussed the approach of this problem with ACK mode, and it seemed that bug had been fixed in 0.10.0. I don't know whether this patch is included in storm-1.x branch. In a word, this problem still exists in the latest stable version."
STORM-2144,Fix Storm-sql group-by behavior in standalone mode,
STORM-2142,ReportErrorAndDie runs suicide function only when InterruptedException or InterruptedIOException is thrown,"When EvaluationFilter / EvaluationFunction throws Exception, async loop for the executor is died but others will continue to work.

{code}
2016-10-08 14:12:29.597 o.a.s.u.Utils Thread-23-b-0-LOGICALFILTER_6-LOGICALPROJECT_7-executor[5 5] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:468) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
...
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]
        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]
        at org.codehaus.janino.ScriptEvaluator.evaluate(ScriptEvaluator.java:982) ~[dep-janino-2.7.6-dcb5bd18-a5dd-4976-a967-0108dcf46df0.jar.1475903522000:2.7.6]
...
Caused by: java.lang.RuntimeException: Cannot convert null to int
        at org.apache.calcite.runtime.SqlFunctions.cannotConvert(SqlFunctions.java:1023) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]
        at org.apache.calcite.runtime.SqlFunctions.toInt(SqlFunctions.java:1134) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]
        at SC.eval0(Unknown Source) ~[?:?]
{code}

While looking into detail, I found that ReportErrorAndDie implementation seems odd - completely opposite behavior compared to 1.x :report-error-and-die.
When InterruptedException or InterruptedIOException is thrown, it should just leave a log and shouldn't run suicide function. For others it should run suicide function."
STORM-2139,Let ShellBolts and ShellSpouts run with scripts from blobs,"It would be nice to be able to use the scripts and executable files distributed through the blob store rather then through the resources directory in a jar.

This is nice because it allows you to use a tgz that preserves the execute bit on files like scripts. 

The biggest issue here is that ShellProcess switches the current working directory for the process over to the code dir (where the storm jar is extracted).  And there is not simple way for a child process to find its way back to the blobs.  I will add in a new option to not change the CWD."
STORM-2138,java.io.FileNotFoundException: stormconf.ser does not exist,"We are seeing problems in our storm topology whereby all our workers crash.

The errors we see are

2016-10-07 09:49:33.599 o.a.s.d.supervisor [ERROR] Error on initialization of server mk-supervisor
java.io.FileNotFoundException: File '/opt/storm_local/supervisor/stormdist/production_2016_09_13-1-1475831938/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292)
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815)
        at org.apache.storm.config$read_supervisor_storm_conf_given_path.invoke(config.clj:142)
        at org.apache.storm.config$read_supervisor_storm_conf.invoke(config.clj:221)
        at org.apache.storm.daemon.supervisor$add_blob_references.invoke(supervisor.clj:495)
        at org.apache.storm.daemon.supervisor$fn__9307$exec_fn__2466__auto____9308.invoke(supervisor.clj:795)
        at clojure.lang.AFn.applyToHelper(AFn.java:160)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at clojure.core$apply.invoke(core.clj:630)
        at org.apache.storm.daemon.supervisor$fn__9307$mk_supervisor__9352.doInvoke(supervisor.clj:763)
        at clojure.lang.RestFn.invoke(RestFn.java:436)
        at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1200)
        at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1233)
        at clojure.lang.AFn.applyToHelper(AFn.java:152)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at org.apache.storm.daemon.supervisor.main(Unknown Source)
2016-10-07 09:49:33.608 o.a.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
        at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
        at clojure.lang.RestFn.invoke(RestFn.java:423)
        at org.apache.storm.daemon.supervisor$fn__9307$mk_supervisor__9352.doInvoke(supervisor.clj:763)
        at clojure.lang.RestFn.invoke(RestFn.java:436)
        at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1200)
        at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1233)
        at clojure.lang.AFn.applyToHelper(AFn.java:152)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at org.apache.storm.daemon.supervisor.main(Unknown Source)
2016-10-07 09:49:34.668 o.a.s.d.supervisor [INFO] Removing code for storm id production_2016_09_13-1-1475831938


We have looked at https://github.com/apache/storm/pull/418 and https://issues.apache.org/jira/browse/STORM-130, which both show the first issue as being fixed - however we are still experiencing it in 1.0.2. The changes from the fixing commit (https://github.com/apache/storm/pull/418/commits/ccd28f8a356f468e66865fa9d9901b0a2628ec74) don't seem to be in the current version of the file (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj).

We get this often when resubmitting a topology, and our only workaround is to stop the topology, delete the whole /opt/storm_local directory (which is our storm.local.dir) and resubmit the topology. Often, the workers seem to be looking for stormconf.ser in the local directory of an old topology that isn't even running at the time."
STORM-2134,improving the current scheduling strategy for RAS,
STORM-2132,NullPointerException in KafkaSpout,"Received a null pointer exception using storm-kafka-client.  Not sure what caused it, was just sitting idle in my IDE.  It did crash my topology.

Here's the consumer information:
topic 	0	1,546	2,802	2,798	4
topic 	1	1,663	2,856	2,856	0
topic 	2	1,671	3,031	3,022	9
topic 	3	1,648	2,760	2,760	0
topic 	4	1,618	2,828	2,824	4
topic 	5	1,537	2,599	2,595	4
topic 	6	1,469	2,522	2,522	0

java.lang.NullPointerException
	at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:249)
	at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:237)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:203)
	at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
[Thread-34-kafka-spout-executor[4 5]] ERROR org.apache.storm.daemon.executor - 
java.lang.NullPointerException
	at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:249)
	at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:237)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:203)
	at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
[Thread-34-kafka-spout-executor[4 5]] ERROR org.apache.storm.util - Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.worker$fn__8659$fn__8660.invoke(worker.clj:761)
	at org.apache.storm.daemon.executor$mk_executor_data$fn__7875$fn__7876.invoke(executor.clj:274)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:494)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)"
STORM-2130,1.0.x does not compile (BaseConfigurationDeclarer),Looks like we missed pulling something into 1.0.x-branch because if I use the version of storm-core/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java from 1.x-branch everything works.
STORM-2128,SimpleSqlTridentConsumer missing license headder,Looks like STORM-2089 added in this file and it needs a header
STORM-2127,Storm-eventhubs should use latest amqp and eventhubs-client versions,"Storm eventhub jar needs to use the latest eventhubs-client version available (1.0.1). The latest version introduces several bug fixes, and resilient sender/receivers.

New event data schemes (how message content is handled) that were added to HDInsight based storm-eventhubs jar (Binary, and String) would be good features to merge into this offering. 

"
STORM-2120,Emit to outputStreamId configured in SpoutConfig for KafkaSpout,"Even though KafkaSpout.declareOutputFields declaresStream using outputStreamId (if present), the message gets emitted to a stream matching the Kafka topic it was read from. Looks like it may have been a merge conflict between the fix for STORM-1210 and STORM-1379."
STORM-2119,bug in log message printing to stdout,"https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/ui/core.clj#L987

Can cause stdout buffers to fill up thus causing threads calling this function to hang"
STORM-2118,A few fixes for storm-sql standalone mode,
STORM-2110,in supervisor v2 filter out empty command line args,"Just found this in Staging as well.  In the old supervisor code we would filter out all empty command line args.  This was missed in the new supervisor, but only when we split a String opts on white space."
STORM-2109,Under supervisor V2 SUPERVISOR_MEMORY_CAPACITY_MB and SUPERVISOR_CPU_CAPACITY must be Doubles,"Just found this rolling out Supervisor V2 to staging env, but it is a simple fix."
STORM-2108,Spout unable to recover after worker crashes...continuously see discarding messages errors...,"Hello:

We have a new situation that occurred after we upgraded to storm 1.0.2 (from 0.9.2). We had a worker crash due to a bug in our code that caused a stack overflow exception. The supervisor detected the issue and restarted the worker as expected.

After the worker crashed, the many of the tuples the spout sends out continuously time out and our throughput slows to a crawl. The spout seems to send out tuples until it hits the max spout pending. Then some of the tuples time out and it sends the next batch.

We saw this exception in the spout log when the worker crashed:

2016-09-21T01:54:32,749 [refresh-connections-timer] [org.apache.storm.messaging.netty.Client] [INFO]> closing Netty Client Netty-Client-/10.103.16.14:31437
2016-09-21T01:54:32,750 [refresh-connections-timer] [org.apache.storm.messaging.netty.Client] [INFO]> waiting up to 600000 ms to send 0 pending messages to Netty-Client-/10.103.16.14:31437
2016-09-21T01:55:35,925 [Netty-server-localhost-31009-worker-1] [org.apache.storm.messaging.netty.StormServerHandler] [ERROR]> server errors in handling the request
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_77]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_77]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_77]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_77]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[?:1.8.0_77]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.0.2.jar:1.0.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
2

And now we just continuously see these messages in the spout logs:

2016-09-21T02:03:35,513 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,644 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,774 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,817 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,849 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,073 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,141 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,169 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,340 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,365 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,416 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,560 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,607 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,660 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,865 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,894 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,026 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,051 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,065 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,219 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed

The worker that died (10.103.16.14.31437) was restarted by the supervisor, but I don't see any log messages in the logs indicating that it is receiving any tuples. The ""is being closed"" messages in the spout logs make me think that storm has failed to close its connection.

This has happened to us repeatedly since the upgrade. Does anyone have any suggestions about how to fix this issue? Also, I originally thought it might be related to STORM-1560, but I don't see the exception that is mentioned in that ticket.

Thanks, and I would greatly appreciate any help
"
STORM-2104,New Kafka spout crashes if partitions are reassigned while tuples are in-flight,"The new KafkaSpout may throw NPEs if partitions are reassigned while tuples are in-flight. The ack function assumes that the spout instance is always responsible for tuples it emitted, which isn't true if partitions were reassigned since the tuple was emitted. The fail function also assumes that failed tuples should be replayed, which is useless if the tuple is for a partition the spout isn't assigned, since it then can't commit the tuple if it succeeds. Both functions should check that the spout instance is responsible for the incoming tuple before scheduling it for retry or adding it to the acked list."
STORM-2100,Few tests are getting failed in external/sql module with JDK 7 ,"

Tests run: 11, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 31.095 sec <<< FAILURE! - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
testCompileEquiJoinWithRightOuterJoin(org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler)  Time elapsed: 2.434 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: arrays first differed at element [0]; expected:<[2, null]> but was:<[3, null]>
	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
	at org.junit.Assert.internalArrayEquals(Assert.java:473)
	at org.junit.Assert.assertArrayEquals(Assert.java:265)
	at org.junit.Assert.assertArrayEquals(Assert.java:280)
	at org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.testCompileEquiJoinWithRightOuterJoin(TestPlanCompiler.java:161)

testCompileEquiJoinWithFullOuterJoin(org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler)  Time elapsed: 2.447 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: arrays first differed at element [0]; expected:<[null, dept-2]> but was:<[null, dept-3]>
	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
	at org.junit.Assert.internalArrayEquals(Assert.java:473)
	at org.junit.Assert.assertArrayEquals(Assert.java:265)
	at org.junit.Assert.assertArrayEquals(Assert.java:280)
	at org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.testCompileEquiJoinWithFullOuterJoin(TestPlanCompiler.java:179)

testCompileEquiJoinWithLeftOuterJoin(org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler)  Time elapsed: 2.381 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: arrays first differed at element [0]; expected:<[2, null]> but was:<[3, null]>
	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
	at org.junit.Assert.internalArrayEquals(Assert.java:473)
	at org.junit.Assert.assertArrayEquals(Assert.java:265)
	at org.junit.Assert.assertArrayEquals(Assert.java:280)
	at org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.testCompileEquiJoinWithLeftOuterJoin(TestPlanCompiler.java:143)
"
STORM-2097,Improve logging in trident core and examples,Improve logging to make code easier to debug and extend.
STORM-2096,Error creating jar file with maven,"I tried to alter the Wordcount file inside storm in order to make it take a file path externally in the command line. Then, I tried to make the jar file for operation using maven. 

First i ran mvn compile. The output is:-

[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 1.0.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 52 source files to /opt/storm/examples/storm-starter/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-starter ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 9.178 s
[INFO] Finished at: 2016-09-15T11:18:13+00:00
[INFO] Final Memory: 52M/691M
[INFO] ------------------------------------------------------------------------

The i ran mvn test. The output is:-

[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 1.0.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 52 source files to /opt/storm/examples/storm-starter/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /opt/storm/examples/storm-starter/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 8 source files to /opt/storm/examples/storm-starter/target/test-classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
  symbol:   class Rankings
  location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
  symbol:   class Rankings
  location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,52] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,44] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[52,9] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[65,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,25] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[89,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[93,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,25] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[97,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[122,9] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,56] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,94] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,22] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,87] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,13] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,51] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,89] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[153,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[174,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[188,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[203,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[228,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[232,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,5] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,23] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[INFO] 141 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.608 s
[INFO] Finished at: 2016-09-15T10:33:18+00:00
[INFO] Final Memory: 51M/706M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project storm-starter: Compilation failure: Compilation failure:
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,52] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,44] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[52,9] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[65,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,25] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[89,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[93,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,25] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[97,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[122,9] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,56] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,94] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,22] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,87] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,13] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,51] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,89] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[153,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[174,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[188,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[203,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[228,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[232,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,5] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,23] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException

I tried removing the rank files. Even that didn't rectify the problem. Please advice how I can make a jar file to run the modified wordcount file."
STORM-2095,Nimbus dies and never recovers due to java.nio.file.DirectoryNotEmptyException,"To Recreate:
--------------------------------------
1) Create a blobstore key for a large file (1 or 2 GB). Size of the file does not matter if nimbus can be killed while the blob is being created.
2) while the blob is being created, restart nimbus (this is easiest way to regenerate, there can be various reasons due to which a blob couldn't be successfully created in nimbus)
3) When nimbus tries to start on restart, it will keep dying due to DirectoryNotEmptyException and never come up.

Expected Behavior
--------------------------------------
Partial blobstore key is deleted cleanly and doesn't affect nimbus.

The actual, incorrect behavior.
--------------------------------------
2016-09-14 15:07:48.518 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-09-14 15:07:48.576 o.a.s.zookeeper [INFO] xxx gained leadership
2016-09-14 15:07:48.581 o.a.s.d.nimbus [ERROR] Error on initialization of server service-handler
java.lang.RuntimeException: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file
	at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:229)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.daemon.nimbus$setup_blobstore.invoke(nimbus.clj:1196)
	at org.apache.storm.daemon.nimbus$fn__7064$exec_fn__2461__auto____7065.invoke(nimbus.clj:1416)
	at clojure.lang.AFn.applyToHelper(AFn.java:156)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at clojure.core$apply.invoke(core.clj:630)
	at org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)
	at clojure.lang.RestFn.invoke(RestFn.java:421)
	at org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)
	at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)
	at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)
	at clojure.lang.AFn.applyToHelper(AFn.java:152)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at org.apache.storm.daemon.nimbus.main(Unknown Source)
Caused by: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file
	at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at org.apache.storm.blobstore.FileBlobStoreImpl.delete(FileBlobStoreImpl.java:239)
	at org.apache.storm.blobstore.FileBlobStoreImpl.deleteKey(FileBlobStoreImpl.java:178)
	at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:226)
	... 19 more
2016-09-14 15:07:48.588 o.a.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)
	at clojure.lang.RestFn.invoke(RestFn.java:421)
	at org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)
	at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)
	at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)
	at clojure.lang.AFn.applyToHelper(AFn.java:152)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at org.apache.storm.daemon.nimbus.main(Unknown Source)


[root]# ls -l  /opt/storm/storm-local/blobs/955/some_big_file
total 591060
-rw-r--r-- 1 storm storm 605241344 Sep 14 15:07 1473865562841.tmp"
STORM-2090,Add integration test for storm windowing,"We want to add integration test for storm windowing feature.
It will be nice to have this running with existing travis-ci setup."
STORM-2088,"Typos in documentation ""Guaranteeing Message Processing""","Minor typos in ""Guaranteeing Message Processing"" page."
STORM-2087,Storm-kafka-client: Failed tuples are not always replayed ,"I am working with kafka 10 and the storm-kafka-client from master. It appears that tuples are not always being replayed when they are failed.

With a topology that randomly fails tuples a small percentage of the time I found that the committed kafka offset would get stuck and eventually processing would stop even though the committed offset was no where near the end of the topic. 

I have also replicated the issue in unit tests with this PR: 
https://github.com/apache/storm/pull/1679

It seems that increasing the number of times I call nextTuple for the in order case will make it work, but it doesn't seem to help the case where tuples are failed out of order from which they were emitted. "
STORM-2084,after supervisor v2 merge async localizer and localizer,"Once we mere in STORM-2018 
https://github.com/apache/storm/pull/1642 

we should look into merging the two localizers into a single class."
STORM-2080,storm-submit-tools license check failure,
STORM-2077,KafkaSpout doesn't retry failed tuples,"KafkaSpout does not retry all failed tuples.

We used following Configuration:
        Map<String, Object> props = new HashMap<>();
        props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""c1"");
        props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, ByteArrayDeserializer.class.getName());
        props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, ByteArrayDeserializer.class.getName());
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker.bootstrapServer());

        KafkaSpoutStreams kafkaSpoutStreams = new KafkaSpoutStreams.Builder(FIELDS_KAFKA_EVENT, new String[]{""test-topic""}).build();

        KafkaSpoutTuplesBuilder<byte[], byte[]> kafkaSpoutTuplesBuilder = new KafkaSpoutTuplesBuilder.Builder<>(new KeyValueKafkaSpoutTupleBuilder(""test-topic"")).build();
        KafkaSpoutRetryService retryService = new KafkaSpoutLoggedRetryExponentialBackoff(KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.milliSeconds(1), KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.milliSeconds(1), 3, KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.seconds(1));

        KafkaSpoutConfig<byte[], byte[]> config = new KafkaSpoutConfig.Builder<>(props, kafkaSpoutStreams, kafkaSpoutTuplesBuilder, retryService)
                .setFirstPollOffsetStrategy(UNCOMMITTED_LATEST)
                .setMaxUncommittedOffsets(30)
                .setOffsetCommitPeriodMs(10)
                .setMaxRetries(3)
                .build();

kafkaSpout = new org.apache.storm.kafka.spout.KafkaSpout<>(config);


The downstream bolt fails every tuple and we expect, that those tuple will all be replayed. But that's not the case for every tuple."
STORM-2076,Supervisor sync-processes and sync-supervisor race when downloading new topology code.,"The fix for https://issues.apache.org/jira/browse/STORM-1934 moved the cleanup of topology code to sync-processes. The cleanup is based on ls-local-assignment, but this is not called from sync-supervisor until all the new topology code has been downloaded. As a result, sync-processes may delete new topology code before sync-supervisor has had a chance to update ls-local-assignment."
STORM-2071,nimbus-test test-leadership failing with Exception,"When running unit tests on my Mac, I get repeated failures in test-leadership.

~~~~
73752 [main] INFO  o.a.s.l.ThriftAccessLogger - Request ID: 1 access from: null principal: null operation: deactivate
]]>            </system-out>
            <error message=""Uncaught exception, not in assertion."">Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.RuntimeException: No transition for event: :inactivate, status: {:type :rebalancing} storm-id: t1-1-1472598899
 at org.apache.storm.daemon.nimbus$transition_BANG_$get_event__4879.invoke (nimbus.clj:365)
    org.apache.storm.daemon.nimbus$transition_BANG_.invoke (nimbus.clj:373)
    clojure.lang.AFn.applyToHelper (AFn.java:165)
    clojure.lang.AFn.applyTo (AFn.java:144)
    clojure.core$apply.invoke (core.clj:636)
    org.apache.storm.daemon.nimbus$transition_name_BANG_.doInvoke (nimbus.clj:391)
    clojure.lang.RestFn.invoke (RestFn.java:467)
    org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__5850.deactivate (nimbus.clj:1773)
    sun.reflect.NativeMethodAccessorImpl.invoke0 (NativeMethodAccessorImpl.java:-2)
    sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    java.lang.reflect.Method.invoke (Method.java:497)
    clojure.lang.Reflector.invokeMatchingMethod (Reflector.java:93)
    clojure.lang.Reflector.invokeInstanceMethod (Reflector.java:28)
    org.apache.storm.nimbus_test$fn__1203$fn__1209.invoke (nimbus_test.clj:1222)
    org.apache.storm.nimbus_test/fn (nimbus_test.clj:1210)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
(test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval8358$iter__8359__8363$fn__8364$fn__8365$fn__8366.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval8358$iter__8359__8363$fn__8364$fn__8365.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval8358$iter__8359__8363$fn__8364.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval8358.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
~~~~"
STORM-2070,Sigar native binary download link went 404,"{code}
  <properties>
    <!-- settings for downloading the sigar native binary complete archive, which is not available in Maven central-->
    <sigar.version>1.6.4</sigar.version>
    <sigar.download.url>https://magelan.googlecode.com/files/hyperic-sigar-${sigar.version}.zip</sigar.download.url>
    <sigar.SHA1>8f79d4039ca3ec6c88039d5897a80a268213e6b7</sigar.SHA1>
    <!-- this will download the sigar ZIP to the local maven repository next to the sigar dependencies,
         so we only download it once -->
    <sigar.download.path>${settings.localRepository}/org/fusesource/sigar/${sigar.version}</sigar.download.path>
  </properties>
{code}

Sigar download url is set to https://magelan.googlecode.com/files/hyperic-sigar-1.6.4.zip which is not working. 
Google Code seems changed their download link. Current link of sigar binary 1.6.4 is https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/magelan/hyperic-sigar-1.6.4.zip"
STORM-2067,"""array element type mismatch"" from compute-executors in nimbus.clj","In some scenarios, Nimbus throws ""java.lang.IllegalArgumentException: array element type mismatch"".

{noformat}
08:49:35.321 [timer] ERROR o.a.s.d.nimbus - Error when processing event
java.lang.IllegalArgumentException: array element type mismatch
	at java.lang.reflect.Array.set(Native Method) ~[?:1.8.0_66]
	at clojure.lang.RT.seqToTypedArray(RT.java:1719) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.seqToTypedArray(RT.java:1692) ~[clojure-1.7.0.jar:?]
	at clojure.core$into_array.invoke(core.clj:3319) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$compute_executors$fn__4307.doInvoke(nimbus.clj:645) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:408) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$compute_executors.invoke(nimbus.clj:645) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$compute_executor__GT_component.invoke(nimbus.clj:655) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:565) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$mk_assignments$iter__4668__4672$fn__4673.invoke(nimbus.clj:967) ~[classes/:?]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
	at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
	at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.7.0.jar:?]
	at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?]
	at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?]
	at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]
	at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:966) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:410) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$fn__5354$exec_fn__579__auto____5355$fn__5366$fn__5367.invoke(nimbus.clj:2409) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$fn__5354$exec_fn__579__auto____5355$fn__5366.invoke(nimbus.clj:2408) ~[classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:190) ~[classes/:?]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83) [classes/:?]
{noformat}

The exception is thrown from into-array, which is called from below line:
{code}
((fn [ & maps ] (Utils/joinMaps (into-array (into [component->executors] maps)))))
{code}"
STORM-2065,Add tooltip descriptions for config keys to the UI,"As an admin/a user, I would like to know the purpose of various config settings I see on the UI, so that I can make the correct changes to my cluster/topology.



This could be accomplished with a simple annotation for each key in our config classes, similar to those we use for validation, but just a simple string.  The annotations could simply duplicate the text we already have in the javadoc comments.

The UI could send this text to the browser in as a tooltip pop-up when the mouse hovers over one of the config keys."
STORM-2062,Hive streaming doesn't support non string partition fields,"create hive table with an int partition column

CREATE TABLE CDRDWH.CDR_FACT (
geo_id int,
time_id smallint,
cust_id smallint,
vend_id smallint,
cust_rel_id smallint,
vend_rel_id smallint,
route tinyint,
connect boolean,
earlyEvent boolean,
Call_duration_cust double,
I_PDD double,
E_PDD double,
orig_number string,
term_number string
)
partitioned by (date_id int)
clustered by (geo_id, time_id) into 16 buckets
stored as ORC
tblproperties (""orc.compress""=""SNAPPY”);

When i try to stream my topolgy output to Hive I get the following exception:

11829 [Thread-31-hivewriter-executor[5 5]] ERROR o.a.s.d.executor - 
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
at org.apache.storm.tuple.TupleImpl.getStringByField(TupleImpl.java:153) ~[storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper.mapPartitions(DelimitedRecordHiveMapper.java:92) ~[storm-hive-1.0.1.jar:1.0.1]
at org.apache.storm.hive.bolt.HiveBolt.execute(HiveBolt.java:112) [storm-hive-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]


Line 92 of DelimtedRecordHiveMapper is attempting to access my integer field as a String and the subsequent exception is thrown
 @Override
    public List<String> mapPartitions(Tuple tuple) {
        List<String> partitionList = new ArrayList<String>();
        if(this.partitionFields != null) {
            for(String field: this.partitionFields) {
                partitionList.add(tuple.getStringByField(field));
            }
        }
        if (this.timeFormat != null) {
            partitionList.add(getPartitionsByTimeFormat());
        }
        return partitionList;
    }"
STORM-2055,Exception when running topology from Maven exec with Flux,"When running a topology from Maven with Flux as a dependency, we get

{code}
11335 [Thread-8] ERROR o.a.s.event - Error when processing event
java.io.FileNotFoundException: Source 'file:/home/julien/.m2/repository/org/apache/storm/flux-core/1.0.1/flux-core-1.0.1.jar!/resources' does not exist
    at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.daemon.supervisor$fn__9359.invoke(supervisor.clj:1194) ~[storm-core-1.0.1.jar:1.0.1]
    at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078$fn__9096.invoke(supervisor.clj:582) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078.invoke(supervisor.clj:581) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.event$event_manager$fn__8630.invoke(event.clj:40) [storm-core-1.0.1.jar:1.0.1]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]
{code}

The same topology runs fine when executed with Eclipse or via the storm command.

See [https://github.com/DigitalPebble/storm-crawler/issues/324]"
STORM-2054,DependencyResolver should be aware of relative path and absolute path,"DependencyResolver always create directory based on storm.home or current working directory which is intended for relative path but not intended for absolute path. 

Furthermore, DependencyResolverTest doesn't remove temporary directory after testing. Test creates a new temporary absolute path but due to this bug, temporary directory is created in working directory which prevents cleaning up, and finally making RAT error on all builds."
STORM-2052,Kafka Spout New Client API - Log Improvements and Parameter Tuning for Better Performance,"Tune Kafka Spout parameters 
Improve Logging to show more meaningful messages, and print detail appropriate to logging level."
STORM-2051,Flux should support the builder pattern,"While trying to work with {{Flux}} and the {{storm-kafka-client}} package we noticed that they are incompatible, unfortunately, as the needed {{KafkaSpoutConfig}} is based on the builder pattern. Unless some hacky method is used it will not be possible to configure a {{KafkaSpout}} and instantiate/use it with a Flux-based topology.

Flux could be enhanced to support the builder pattern with the following yaml configuration as a proposal:

{code}
builder:
  - id: ""spoutConfigBuilder""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig.Builder""
    builderMethod: ""build""
    constructorArgs:
      - [...]
    properties:
      - [...]
    configMethods:
      - [...]
components:
  - id: ""spoutConfig""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig""
    builderRef: ""spoutConfigBuilder""
spouts:
  - id: ""kafkaSpout""
    className: ""org.apache.storm.kafka.spout.KafkaSpout""
    constructorArgs:
      - ref: ""spoutConfig""
{code}

Unfortunately, for now, we are busy with other tasks so we cannot work on a patch for Flux. But we thought it's better to report / suggest this enhancement nevertheless."
STORM-2048,Refactor code blocks which are ported to for-loop to Java Stream API,"We just changed minimum requirement for master branch to Java 1.8 from STORM-2041. 

Thanks for the change we can change ported code block which was functional style to similar style again.

We could even broaden the boundary of this issue for applying other benefits from Java 8, or file separate issues."
STORM-2046,Errors when using TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE in local mode.,"When using a LocalCluster during tests, if {{TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE}} is specified, {{assert-can-serialize}} attempts to destructure a Java model object and throws, killing the worker. A minimal-ish case and the full logs are here: https://gist.github.com/ckolbeck/557734429e62b097efa9382a714122b0"
STORM-2045,NPE in SpoutExecutor in 2.0 branch,"This issue was raised in [STORM-1949], but since the original issue mainly discusses about whether to disable ABP by default, I'd like to pick this NPE as another issue."
STORM-2044,Nimbus should not make assignments crazily when Pacemaker goes down and up,"        Now pacemaker is a stand-alone service and no HA is supported. When it goes down, all the workers's heartbeats will be lost. It will take a long time to recover even if pacemaker goes up immediately if there are dozens GB of heartbeats. During the time worker heartbeats are not restored completely, Nimbus will think these workers are dead because of heartbeats timeout and reassign these ""dead"" workers continuously until heartbeats restore to normal. So, during recovery time, many topologies will be reassigned continuously and the throughout will goes very down.  
        This is not acceptable. 
        So i think, pacemaker is not suitable for production if the problem above exists.
               i think several ways to solve this problem:
              1. pacemaker HA
              2. when pacemaker does down, notice nimbus not to reassign any more until it recover"
STORM-2043,Nimbus should not make assignments crazy when Pacemaker down,"When pacemaker goes down, all the heartbeats of workers are lost. These heartbeats will need a long time to recover even if pacemaker goes up immediately if it costs dozens of GB memory. During the time worker heartbeats are not complete，Nimbus will think the workers are died( heartbeat time out ),  and reassign these workers crazily. But actually the workers are healthy, the reassignment will move in cycles until pacemaker heartbeats recover. During this time, all the topologies's throughout will goes down. We should avoid this, because Pacemaker has no HA."
STORM-2040,Config.TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE=true causes j.l.UnsupportedOperationException: nth not supported on this type: AddressedTuple,"When Config.TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE is enabled for a topology, the components fail with the following exception:

26168 [Thread-13-disruptor-executor[1 1]-send-queue] ERROR o.a.s.d.executor - 
java.lang.RuntimeException: java.lang.UnsupportedOperationException: nth not supported on this type: AddressedTuple
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__7509.invoke(disruptor.clj:83) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
Caused by: java.lang.UnsupportedOperationException: nth not supported on this type: AddressedTuple
	at clojure.lang.RT.nthFrom(RT.java:933) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.nth(RT.java:883) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$assert_can_serialize.invoke(worker.clj:130) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.worker$mk_transfer_fn$fn__8214.invoke(worker.clj:202) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__7898.invoke(executor.clj:312) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.disruptor$clojure_handler$reify__7492.onEvent(disruptor.clj:40) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:451) ~[storm-core-1.0.2.jar:1.0.2]
	... 6 more
26171 [Thread-15-__acker-executor[163 163]] INFO  o.a.s.d.executor - Preparing bolt __acker:(163)
26171 [Thread-15-__acker-executor[163 163]] INFO  o.a.s.d.executor - Prepared bolt __acker:(163)"
STORM-2038,Provide an alternative to using symlinks,"As of Storm 1.0 and above, some functionality (such as the worker-artifacts directory) require the use of symlinks. On Windows platforms, this requires that Storm either be run as an administrator or that certain group policy settings are changed.

In locked-down environments, both of these solutions are not suitable.

Where possible, an alternative option should be provided to the use of symlinks. For example, it may be possible to create additional copies of the worker artifacts directory for each worker (possibly inefficient) or provide the workers with the canonical path to the real directory.

See the [brief discussion|http://mail-archives.apache.org/mod_mbox/storm-dev/201608.mbox/%3C1293850887.13165119.1471022901569.JavaMail.yahoo%40mail.yahoo.com%3E] on the mailing list."
STORM-2037,debug operation should be whitelisted in SimpleAclAuthorizer,"For topology event logging to work in secure mode, the ""debug"" operation should be whitelisted."
STORM-2032,"""not fast enough"" metrics WARN message in netty client can be misinterpreted","Example:

bq. WARN Messages are not being delivered fast enough, got 3 metrics messages at once

It appears to some users to be a good signal for monitoring/alerting, but really this is not part of the design.

We should remove it, change it, or lower the log level."
STORM-2028,Exceptions in JDBCClient are hidden by subsequent SQL-Exception in close(),"When an Exception is triggered in JdbcClient.executeInsertQuery there is the potential for a follow-up Exception in close() to take precedence over the previously thrown Exception, when triggered in the finally block. This makes debugging the actual Exception impossible.

As far as I can tell it would be better to catch the Exception form close() in the finally-block, and to combine it with the existing Exception, so that the key information for debugging purposes isn't lost.

For data consistency purposes we have to make sure that the Exception from closing the connection is thrown (or do we? can we be sure that a successful commit has persisted the data?) but ""overlapping"" Exceptions have to be dealt with.

Alternatively it might be a good idea to log the Exceptions before throwing them, so that the stack trace isn't lost. This is probably easier than tracking in the finally block whether a previous Exception has been thrown, and what to do with it.

If there's a workaround for this, that I might have missed, to get to the root of the Exception, I would also be interested in hearing, I'm currently looking at a situation where jdbc fails, and there being no indication of what's going on.

I labelled this newbie-level, since the implementation is pretty trivial; but the decision of which way to pursue isn't as clear to me."
STORM-2026,"Inconsistency between (SpoutExecutor, BoltExecutor) and (spout-transfer-fn, bolt-transfer-fn)","As I left the comment from https://github.com/apache/storm/pull/1445#discussion_r73255197 for pull request, there's some difference between SpoutExecutor / BoltExecutor and spout-transfer-fn / bolt-transfer-fn.

While it's not that big to fix, I just want to not block port work and just address from here."
STORM-2025,dropping messages in withTumblingWindow,"when i use {{withTumblingWindow}} and process the input messages, if the processing time is longer than input rate, we will not get all input messages.

{code}
int count=0;
	@Override
	public void execute(TupleWindow inputWindow) {
		try {
			List<Event> windowEvenets = new ArrayList<>();
			for(Tuple tuple: inputWindow.get()) {
				count++;
				/* some operation here */
			}
			logger.info(count + ""======= Process event "");
			Thread.sleep(4000);
		}
		catch (Exception ex) {
			ex.printStackTrace();
		}
	}
{code}

The topology is as follow:
{code}
 TopologyBuilder builder = new TopologyBuilder();
            builder.setSpout(""KafkaSpout"", new KafkaSpout(kafkaConfig), 1);
            builder.setBolt(""WindowInputTest"", new WindowInputTest(zookeeperHosts).withTumblingWindow(new BaseWindowedBolt.Duration(4,TimeUnit.SECONDS)), 1).shuffleGrouping(""KafkaSpout"");
{code}"
STORM-2022,FieldsTest.selectingUnknownFieldThrowsTest is failing,"{code}
<testcase name=""selectingUnknownFieldThrowsTest"" classname=""org.apache.storm.tuple.FieldsTest"" time=""0.007"">
    <error message=""Unexpected exception, expected&lt;java.lang.NullPointerException&gt; but was&lt;java.lang.IllegalArgumentException&gt;"" type=""java.lang.Exception""><![CDATA[java.lang.Exception: Unexpected exception, expected<java.lang.NullPointerException> but was<java.lang.IllegalArgumentException>
        at org.apache.storm.tuple.Fields.fieldIndex(Fields.java:104)
        at org.apache.storm.tuple.Fields.select(Fields.java:63)
        at org.apache.storm.tuple.FieldsTest.selectingUnknownFieldThrowsTest(FieldsTest.java:124)
]]></error>
  </testcase>
{code}"
STORM-2021,storm-kinesis missing licenses,"{code}
Unapproved licenses:

  external/storm-kinesis/src/test/java/org/apache/storm/kinesis/spout/test/KinesisBoltTest.java
  external/storm-kinesis/src/test/java/org/apache/storm/kinesis/spout/test/KinesisSpoutTopology.java
  external/storm-kinesis/src/test/java/org/apache/storm/kinesis/spout/test/TestRecordToTupleMapper.java
{code}"
STORM-2020,Stop using sun internal classes,"sun.reflect.generics.reflectiveObjects.NotImplementedException, sun.misc.BASE64Decoder, and sun.misc.BASE64Encoder are not public APIs we should not be using them."
STORM-2019,NullPointerException in Kafka-Spout,"KafkaSpout reports following error:
java.lang.NullPointerException
        at java.util.TreeMap.rotateLeft(TreeMap.java:2220) ~[?:1.8.0_77]
        at java.util.TreeMap.fixAfterInsertion(TreeMap.java:2287) ~[?:1.8.0_77]
        at java.util.TreeMap.put(TreeMap.java:582) ~[?:1.8.0_77]
        at org.apache.storm.kafka.PartitionManager.fill(PartitionManager.java:235) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:138) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[stormjar.jar:?]
        at applications.spout.KafkaSpoutWrapper.nextTuple(KafkaSpoutWrapper.java:64) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7885$fn__7900$fn__7931.invoke(executor.clj:645) ~[storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
"
STORM-2018,Simplify Threading Model of the Supervisor,"We have been trying to roll out CGROUP enforcement and right now are running into a number of race conditions in the supervisor.  When using CGROUPS the timing of some operations are different and are exposing issues that we would not see without this.

In order to make progress with testing/deploying CGROUP and RAS we are going to try and refactor the supervisor to have a simpler threading model, but likely with more threads.  We will base the code off of the java code currently in master, and may replace that in the 2.0 release, but plan on having it be a part of 1.x too, if it truly is more stable.

I will try to keep this JIRA up to date with what we are doing and the architecture to keep the community informed.  We need to move quickly to meet some of our company goals but will not just shove this in.  We welcome any feedback on the design and code before it goes into the community."
STORM-2017,ShellBolt stops reporting task ids,"After running enough flow throw ShellBolt in some cases after tens of minutes ShellBolt stopped reporting task ids. After this error condition no new task ids where reported back. When acking of the tuples processed by the bolt where set in callback related to arrival of the task ids all tuple trees going through the bolt would fail after reporting stopped. ShellBolt will continue to operate new tuples and respond to heartbeats.

After running some tests and making some changes to the code. I have following hypothesis for the reason:

org.apache.storm.utils.ShellBoltMessageQueue has two queues one being for taskIds and the other for bolt messages.
taskIds queue is implemented by LinkedList and bolt msg queue LinkedBlockingQueue. Both of the queues are operated similarly.
One major difference between the structures is that LinkedList is not synchronized.

In the code:

ShellBoltMessageQueue.java:58 add method is used without holding the lock. Where as ShellBoltMessageQueue.java:110 uses the poll method with the lock. 
As in ShellBolt BoltReaderRunnable and BoltWriterRunnable are run concurrently this can lead to race condition.

If I move the ShellBoltMessageQueue.java:58 inside the lock and run the test in similar fashion it seems to solve the issue."
STORM-2006,Storm metrics feature improvement: support per-worker level metrics aggregation,"Storm provides per-task level metrics which could be huge when topology has a number of tasks. 
Task level metric is useful for determining load balance between tasks, but it doesn't need to be time-series fashion.

Before introducing topology level component like TopologyMaster for JStorm, we can utilize SystemBolt to aggregate task level metrics to per-worker level metrics.

We should provide options and this feature should be turned off by default to keep backward compatibility. "
STORM-2001,New Kafka Spout Consume Max Records,"Kafka 0.10 have add a new parameter (max.poll.records) that use to control the number of messages returned in a single call to poll(). It's useful to control topology QPS . 

[Implement max.poll.records for new consumer (KIP-41)|https://issues.apache.org/jira/browse/KAFKA-3007]
"
STORM-2000,Add opentsdb libs to external dir in installation.,storm-opentsdb is missing in $storm-installation-dir/external dir. This module should be packaged in external directory of the installation.
STORM-1999,Update docs with OutputCollector's threadsafety,"From 1.x branch, Updated docs about OutputCollector;s threadsafety."
STORM-1996,Supplement Kafka Test Case,Storm Kafka Client's test package seems not a test case . 
STORM-1995,downloadChunk in nimbus.clj should close the input stream,
STORM-1992,Deploy multilang-javascript code as node package,"Now that storm includes Flux, it is easier than ever to deploy a topology with javascript components. If the Bolt and Spout base classes defined in storm/multi-lang/javascript were available in a node.js package on https://www.npmjs.com/ it would allow node.js storm users to take advantage of node's built in package manager to develop their own bolts and spouts.

It would be relatively trivial to add some maven tasks to storm/multi-lang/javascript/pom.xml to take the storm.js resource and package it in a node module and submit it to npm.

This could be added to the pom as a separate profile so it wouldn't impact the normal storm build process.

This integration will also make it easier to add unit tests for storm.js

For additional background see this discussion: 

http://mail-archives.apache.org/mod_mbox/storm-dev/201607.mbox/%3CCAD8EKPHc6O1LCnoQUUoYoDuMQ3uSaNpD5gR4onK%2B0EL5_qcZ3Q%40mail.gmail.com%3E

If this sounds like a worthwhile addition to the project I would be happy to submit a PR.
"
STORM-1988,Kafka Offset not showing due to bad classpath,"STORM-1950 breaks classpath of storm-kafka-monitor. 

Classpath doesn't work with wildcard and filename prefix/postfix. It was added for purposing to prevent other libs to also included as classpath, but it just doesn't work. My bad.

We should fix classpath to specify full filename path or directory/* pattern."
STORM-1986,Local BlobStore only lists blobs in local rather than all available blobs on nimbuses.,"This is follow up issue from [~revans2]'s comment on STORM-1977

https://github.com/apache/storm/pull/1574#issuecomment-233638403

Quote part of the comment describing bug:
{quote}
The one bug I saw while going through the code is that when we list keys, we are doing it only from the local storage, not from ZK.
{quote}

This is not same behavior for HDFS backed so it is definitely a bug which should be addressed."
STORM-1983,"Topology Page: Visualization form is generated for each pushing of the 'Show Visualization' button, and only first one shows graph properly","In topology page, visualization form is generated for each pushing of the 'Show Visualization' button, and only first one shows graph properly.

I'll attach screenshot on this.
"
STORM-1982,Topology running Local Cluster is not properly destroyed,"The process which run LocalCluster is not properly destroyed.

I'll attach the console log after kill is triggered, and jstack dump."
STORM-1977,Leader Nimbus crashes with getClusterInfo when it doesn't have one or more replicated topology codes,"While investigating STORM-1976, I found that there're cases for nimbus to not having topology codes. 
Before BlobStore, only nimbuses which is having all topology codes can gain leadership, otherwise they give up leadership immediately. While introducing BlobStore, this logic is removed.

I don't know it's intended or not, but it incurs one of nimbus to gain leadership which doesn't have replicated topology code, and the nimbus will be crashed when getClusterInfo is requested.

Easiest way to reproduce is:

1. comment cleanup-corrupt-topologies! from nimbus.clj (It's a quick workaround for resolving STORM-1976), and patch Storm cluster
2. Launch Nimbus 1 (leader)
3. Run topology
4. Kill Nimbus 1
5. Launch Nimbus 2 from different node
6. Nimbus 2 gains leadership 
7. getClusterInfo is requested to Nimbus 2, and Nimbus 2 gets crashed

Log:

{code}
2016-07-17 08:47:48.378 o.a.s.b.FileBlobStoreImpl [INFO] Creating new blob store based in /grid/0/hadoop/storm/blobs
...
2016-07-17 08:47:48.619 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-07-17 08:47:48.651 o.a.s.zookeeper [INFO] <node1> gained leadership
...
2016-07-17 08:47:48.833 o.a.s.d.nimbus [INFO] Starting nimbus server for storm version '1.1.1-SNAPSHOT'
2016-07-17 08:47:49.295 o.a.s.t.ProcessFunction [ERROR] Internal error processing getClusterInfo
KeyNotFoundException(msg:production-topology-2-1468745167-stormcode.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:268)
...
        at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:498)
        at org.apache.storm.daemon.nimbus$get_cluster_info$iter__9520__9524$fn__9525.invoke(nimbus.clj:1427)
...
        at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1401)
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9612.getClusterInfo(nimbus.clj:1838)
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3724)
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3708)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
...
2016-07-17 08:47:49.397 o.a.s.b.BlobStoreUtils [ERROR] Could not download blob with keyproduction-topology-2-1468745167-stormconf.ser
2016-07-17 08:47:49.400 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyproduction-topology-2-1468745167-stormconf.ser
2016-07-17 08:47:49.402 o.a.s.d.nimbus [ERROR] Error when processing event
KeyNotFoundException(msg:production-topology-2-1468745167-stormconf.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:239)
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:271)
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:300)
...
       at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
        at org.apache.storm.daemon.nimbus$read_storm_conf_as_nimbus.invoke(nimbus.clj:548)
        at org.apache.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:555)
        at org.apache.storm.daemon.nimbus$mk_assignments$iter__9205__9209$fn__9210.invoke(nimbus.clj:912)
...
        at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:911)
        at clojure.lang.RestFn.invoke(RestFn.java:410)
        at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781$fn__9782.invoke(nimbus.clj:2216)
        at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781.invoke(nimbus.clj:2215)
        at org.apache.storm.timer$schedule_recurring$this__1732.invoke(timer.clj:105)
        at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:50)
        at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)
...
2016-07-17 08:47:49.408 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
        at clojure.lang.RestFn.invoke(RestFn.java:423)
        at org.apache.storm.daemon.nimbus$nimbus_data$fn__8727.invoke(nimbus.clj:205)
        at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:71)
        at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
2016-07-17 08:47:49.410 o.a.s.d.nimbus [INFO] Shutting down master
{code}
"
STORM-1976,Storm Nimbus H/A has issue on cleaning corrupted topologies,"In the following scenario storm-ha runs into issues:
1. Kill a non-leader nimbus
2. Submit a topology
3. Bring up the non-leader nimbus

After step-3 expectation is that the non-leader nimbus will download topology jar. Instead it cleans up the topology.

{code}
2016-07-12 07:11:09.511 o.a.s.c.zookeeper-state-factory [WARN] Received event ::none: with disconnected Reader Zookeeper.
2016-07-12 07:11:09.587 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-07-12 07:11:09.608 o.a.s.d.nimbus [INFO] Corrupt topology JoinedNonLeaderNimbusTriesToDownloadTopologyCode-2-1468307239 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2016-07-12 07:11:09.932 o.a.h.m.s.s.StormTimelineMetricsReporter [INFO] Preparing Storm Metrics Reporter
2016-07-12 07:11:09.946 o.a.s.d.m.MetricsUtils [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
{code}"
STORM-1974,Using System.lineSeparator to replacement write a new line,Using System.lineSeparator to replacement write a new line . It will write message in once and reduce writer synchroniz .
STORM-1972,Storm throws java.lang.ClassNotFoundException on Bolt class,"I'm trying to debug very simple topology (1 spout 2 bolts)

public class JoinerTopologyTest {

public static void main(String[] args) throws IOException {
    Config conf = new Config();
    conf.setNumWorkers(5);
    conf.setDebug(true);

    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout(""SPOUT-1"",new MySpout(),1);
    builder.setBolt(""BOLT-1"",new Bolt1(), 3)
            .shuffleGrouping(""SPOUT-1"");
    builder.setBolt(""JOINER"", new JoinerBolt(),1)
            .shuffleGrouping(""BOLT-1"")
            .shuffleGrouping(""SPOUT-1"",""str1"");

    final LocalCluster cluster = new LocalCluster();
    cluster.submitTopology(""TOPO1"",conf,builder.createTopology());


    System.in.read();

    cluster.shutdown();
}
}

But when i run it from InteliJ IDEA i get:

java.lang.RuntimeException: java.lang.ClassNotFoundException: com.pixonic.zephyr.compaction.tests.Bolt1 at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:181) ~[storm-core-1.0.1.jar:1.0.1] at org.apache.storm.utils.Utils.getSetComponentObject(Utils.java:430) ~[storm-core-1.0.1.jar:1.0.1]
and

[Thread-15] ERROR o.a.s.d.worker - Error on initialization of server mk-worker java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.storm.daemon.acker at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:181) ~[storm-core-1.0.1.jar:1.0.1] at org.apache.storm.utils.Utils.getSetComponentObject(Utils.java:430) ~[storm-core-1.0.1.jar:1.0.1]
but same topology runs well in Cluster mode. PS in my pom.xml in debug mode i have:

    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>storm-core</artifactId>
        <version>1.0.1</version>
    </dependency>

project to reproduce bug: https://github.com/holinov/storm-101-localcluster/tree/master
"
STORM-1967,Kafka 0.8 Incompatible with Storm 1.0.1,"Had a Storm Cluster deployed and functioning with storm 0.9.5. Updated the cluster and the topology to 1.0.1, leaving the same kafka version (0.8). Kafka Spout would not function, threw Kafka Buffer Underflow exceptions.

Updating to Kafka 0.9 fixed this issue, but I'm told I shouldn't have had to."
STORM-1966,Expand metric having Map type as value into multiple metrics based on entries,"We're introducing ""metrics filter"" (STORM-1700) into Storm 1.1.0, which can give a control of volume and kinds of metrics to users.

After playing with metrics, I found that most of built-in metrics in Storm (core and storm-kafka) are having Map as value which have been expected to be populated from Metrics Consumer. Since filter resides on metrics consumer bolt (not injected to metrics consumer) filter cannot know how metrics are populated, thus can't filter out some of populated metrics.

For example, let's say we have metric which name is 'A' and value is \{""B"": 1, ""C"": 2\}. For now we can't filter out 'A.C' and keep only 'A.B' since filter even doesn't know 'A' will be changed to 'A.B' and 'A.C'.

Since well-known metrics consumer (like storm-graphite) already supports populating metrics from one level map of value, I'd like to support this from Storm side and apply filter to populated metrics."
STORM-1964,Unexpected behavior when using count window together with timestamp extraction,"I launched a topology applying a tumbling count window of size 2 (watermark interval 200ms, lag 1s) with the following input (timestamp,value):

{noformat}
(10,10)
(10,20)
(11,30)
(12,40)
(12,50)
(12,60)
(12,70)
(13,80)
(14,90)
(15,100)
{noformat}

And I got these windows as output:

{noformat}
[(10,10), (10,20)]
[(12,60), (12,70)]
[(12,60), (12,70)]    // why (60, 70) twice?
[(13,80), (14,90)]
{noformat}


I would expect something like:

{noformat}
[(10,10), (10,20)]
[(11,30), (12,40)]
[(12,50), (12,60)]
[(12,70), (13,80)]
[(14,90), (15,100)]
{noformat}


It seems like that timestamp extraction and count windows does not fit each other."
STORM-1963,Replace Put add with addColumn,"HBase Put add() have deprecated , replace add() with addColumn()"
STORM-1962,python storm integration does not run on python 3,"This impacts other versions too (all of them), but fixing it is probably not that critical.

It is printing and exception handling that needs to be updated and can still maintain compatibility with 2.6 python."
STORM-1959,KafkaPartitionOffsetLag.java does not have license,"RAT is failing  external/storm-kafka-monitor/src/main/java/org/apache/storm/kafka/monitor/KafkaPartitionOffsetLag.java

Looks like this was introduced as a part of STORM-1950"
STORM-1958,storm-config.cmd doesn't handle spaces in JAVA_HOME,"We are currently upgrading the version of Storm we use in our environment to 1.0.1 (from 0.10.0). We have discovered that Storm does not start properly as JAVA_HOME has a space in it.

Investigating this, I have found that the main problem (in this case) seems to be in storm-config.cmd. In 0.10.0, line 128 contained:

{code}
set STORM_OPTS=-Dstorm.options= -Dstorm.home=%STORM_HOME% -Djava.library.path=%JAVA_LIBRARY_PATH%
{code}

The equivalent line in 1.0.1 (line 136) has:

{code}
set STORM_OPTS=%STORM_OPTS% -Dstorm.home=%STORM_HOME% -Djava.library.path=%JAVA_LIBRARY_PATH%;%JAVA_HOME%\bin;%JAVA_HOME%\lib;%JAVA_HOME%\jre\bin;%JAVA_HOME%\jre\lib
{code}

If JAVA_HOME has a space in it (as is frequently the case on Windows due to Java by default being installed under Program Files) this breaks the subsequent JVM command line.

This is an out of the box blocker to running Storm on Windows in commons configurations. I have not raised this as a blocker issue however, as there is a simple fix/workaround. We have changed storm-config.cmd in our local copy to add quotes around the java.library.path option:

{code}
set STORM_OPTS=%STORM_OPTS% -Dstorm.home=%STORM_HOME% ""-Djava.library.path=%JAVA_LIBRARY_PATH%;%JAVA_HOME%\bin;%JAVA_HOME%\lib;%JAVA_HOME%\jre\bin;%JAVA_HOME%\jre\lib""
{code}"
STORM-1957,Support Storm JDBC batch insert,"Batch insert support execute grouped SQL a batch and submit into one call . It can reduce the amount of communication , improving performance."
STORM-1956,Disable Backpressure by default,"Some of the context on this is captured in STORM-1949 
In short.. wait for BP mechanism to mature some more and be production ready before we enable by default."
STORM-1952,Keeping topology code for supervisor until topology got killed,"It's based on review comment from [~sriharsha].
https://github.com/apache/storm/pull/1528/files#r69152524
Please feel free to change reporter if you would like to.

In supervisor we're removing topology code when assignments for that supervisor has gone.
But there's valid scenario to need to keep the topology code though assignments for that supervisor is none, for example, rebalancing.

So it would be better for supervisor to keep topology code until topology has been killed (and all topology workers assigned to that supervisor are also killed)."
STORM-1950,"Change response json of ""Topology Lag"" REST API to keyed by spoutId, topic, partition","From code review for STORM-1945, there's an idea to change JSON response of ""Topology Lag"" API to keyed by topic, partition number.

https://github.com/apache/storm/pull/1541#issuecomment-230983140

I think also make result keyed by spout id would be good.
Here's sample JSON of output after this issue is resolved.

{code}
{
   ""spout1"":{
      ""spoutId"":""spout1"",
      ""spoutType"":""KAFKA"",
      ""spoutLagResult"":{
         ""topic"":{
            ""partition0"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            },
            ""partition2"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            }
         },
         ""topic2"":{
            ""partition0"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            },
            ""partition2"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            }
         }
      }
   }
}
{code}"
STORM-1945,Internal Server Error shown on topology page for topology using KafkaSpout,"When opening topology page which uses old storm-kafka Spout, page shows Internal Server Error on bottom side.
And REST API /api/v1/topology/:topology/lag also shows Internal Server Error.
Its errorMessage is describing NPE, but stack track gives less help since it's already within catch statement so we can't trace why origin exception is thrown by only looking at errorMessage. And there's no error message on ui log file."
STORM-1943,Support loading properties from a file,Support load properties from a file include config's argument . 
STORM-1942,Extra closing div tag in topology.html,Extra </div> in topology.html causing styling to be strage. Appears to have been introduced in STORM-1136.
STORM-1941,Nimbus discovery can fail when zookeeper reconnect happens.,"When zookeeper reconnect happens, nimbus registry can be deleted though nimbus is alive.

Below is zookeeper node for nimbus registry.

{code}
get /storm/nimbuses/<host>:6627
?f`d``??????M?-?-.?/??5??/H?+.IL???ON??``b`?|???^^???????
?'h?g?g?g?g
t-?,[??Q
cZxid = 0x4000005ae
ctime = Fri Jul 01 11:43:51 UTC 2016
mZxid = 0x4000005ae
mtime = Fri Jul 01 11:43:51 UTC 2016
pZxid = 0x4000005ae
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x255a62e310c0005
dataLength = 98
numChildren = 0
{code}

{code}
get /storm/nimbuses/<host>:6627
?f`d``??????M?-?-.?/??5??/H?+.IL???ON??``b`?|???^^???????
?'h?g?g?g?g
t-?,[??Q
cZxid = 0x4000005ae
ctime = Fri Jul 01 11:43:51 UTC 2016
mZxid = 0x50000000e
mtime = Fri Jul 01 11:46:08 UTC 2016
pZxid = 0x4000005ae
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x255a62e310c0005
dataLength = 98
numChildren = 0
{code}

Below is transaction log for that node.
{code}
7/1/16 11:43:51 AM UTC session 0x255a62e310c0005 cxid 0xd zxid 0x4000005ae create '/storm/nimbuses/<host>:6627,#1fffffff8b80000000ffffffe36660646060ffffff90ffffffcfffffffcaffffffc9ffffffccffffffd54dffffffcc2dffffffd62d2effffffc92fffffffcaffffffd535ffffffd2ffffffcb2f48ffffffcd2b2e494cffffffceffffffceffffffc94f4effffffccffffffe160606260ffffff907cffffffccffffffc1ffffffc01c5e165effffffceffffffc4ffffffc0ffffffc2ffffffc0ffffffcdffffffc0affffffd42768ffffffa867ffffffa067ffffffa867ffffffa467affffffa4d742dffffff8c2c1805b14ffffffc2ffffffaf51000,v{s{31,s{'world,'anyone}}},T,10

7/1/16 11:46:08 AM UTC session 0x355a647bd8c0000 cxid 0x3 zxid 0x50000000e setData '/storm/nimbuses/<host>:6627,#1fffffff8b80000000ffffffe36660646060ffffff90ffffffcfffffffcaffffffc9ffffffccffffffd54dffffffcc2dffffffd62d2effffffc92fffffffcaffffffd535ffffffd2ffffffcb2f48ffffffcd2b2e494cffffffceffffffceffffffc94f4effffffccffffffe160606260ffffff907cffffffccffffffc1ffffffc01c5e165effffffceffffffc4ffffffc0ffffffc2ffffffc0ffffffcdffffffc0affffffd42768ffffffa867ffffffa067ffffffa867ffffffa467affffffa4d742dffffff8c2c1805b14ffffffc2ffffffaf51000,1
{code}

Please take a look at ctime, mtime, and ephemeralOwner.
Ephemeral owner session was already closed from nimbus side but there's possible for node to be not deleted immediately, so new session doesn't create new node but set the value to ephemeral node for other session which is already closed.
*And eventually that node is deleted although session 0x355a647bd8c0000 is alive.*

{code}
2016-07-01 11:45:05.675 o.a.s.s.o.a.z.ClientCnxn [DEBUG] Disconnecting client for session: 0x255a62e310c0005
2016-07-01 11:45:05.675 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x255a62e310c0005 closed
{code}

We can delete the node first and set ephemeral node when reconnect event handler is called."
STORM-1940,Storm Topo is auto re-balance after ZK RECONNECTED,"I have a Topo with 2 workers at 2 Vm, while ZK RECONNECTED, Storm Topo will be auto-reblance. 
The log show NodeExists for /meta/712285. I guess it cause by: After reconnect successfully, TridentSpoutCoordinator create this node again, but this node is already created before the reconnect.
 Can we check if node exist first? Or not throw this exception to make whole Topo re-balance. 
{code}
06-29 05:54:37.515 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] shade.org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae1, negotiated timeout = 10000
06-29 05:54:37.515 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]-EventThread] apache.curator.framework.state.ConnectionStateManager [INFO] State change: RECONNECTED
06-29 05:54:37.519 [Thread-133-spout-DataKafkaSpout1466801942228-executor[154 154]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae5, negotiated timeout = 10000
06-29 05:54:37.519 [Thread-133-spout-DataKafkaSpout1466801942228-executor[154 154]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.524 [Thread-25-spout-DataKafkaSpout1466801942228-executor[156 156]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae4, negotiated timeout = 10000
06-29 05:54:37.524 [Thread-25-spout-DataKafkaSpout1466801942228-executor[156 156]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.528 [main-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] shade.org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7b556f0cc3a40896, negotiated timeout = 10000
06-29 05:54:37.528 [main-EventThread] apache.curator.framework.state.ConnectionStateManager [INFO] State change: RECONNECTED
06-29 05:54:37.528 [Thread-149-spout-DataKafkaSpout1466801942228-executor[160 160]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae3, negotiated timeout = 10000
06-29 05:54:37.528 [Thread-149-spout-DataKafkaSpout1466801942228-executor[160 160]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.536 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]] org.apache.storm.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80]
Caused by: java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:119) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.overrideState(RotatingTransactionalState.java:52) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:71) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more
Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:721) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:704) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:701) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:477) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:467) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.forPath(TransactionalState.java:83) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:95) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:115) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.overrideState(RotatingTransactionalState.java:52) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:71) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more
{code}"
STORM-1939,Frequent InterruptedException raised by ShellBoltMessageQueue.poll,"We've recently started testing out Storm 1.0.1 on a beta cluster we have setup, and we've noticed that one of our topologies frequently crashes with the following stack trace:

{code:java}
java.lang.InterruptedException 
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017) 
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2095) 
    at org.apache.storm.utils.ShellBoltMessageQueue.poll(ShellBoltMessageQueue.java:104) 
    at org.apache.storm.task.ShellBolt$BoltWriterRunnable.run(ShellBolt.java:383) 
    at java.lang.Thread.run(Thread.java:745)
{code}

We're using a lot of Python components with streamparse 3.0.0.dev3 and are using the [MessagePackSerializer that was originally from pyleus|https://github.com/YelpArchive/pyleus/blob/develop/topology_builder/src/main/java/com/yelp/pyleus/serializer/MessagePackSerializer.java] with all the instances of ""backtype"" replaced with ""org.apache"".

Aside from the frequent bolt deaths from these exceptions, things seem to work, so I'm not sure what's going on here."
STORM-1937,trident topologies WindowTridentProcessor cause NullPointerException when using windowing,"I'm working with trident and try to use windows support, under the local model is fine, but in distributed mode we got the following excepiton(I can reliably reproduce this issue):

java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:744) [?:1.7.0_51]
Caused by: java.lang.NullPointerException
	at org.apache.storm.trident.windowing.WindowTridentProcessor.finishBatch(WindowTridentProcessor.java:167) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.planner.SubtopologyBolt.finishBatch(SubtopologyBolt.java:151) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.TridentBoltExecutor.finishBatch(TridentBoltExecutor.java:266) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.TridentBoltExecutor.checkFinish(TridentBoltExecutor.java:299) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:378) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more"
STORM-1934,Race condition between sync-supervisor and sync-processes raises several strange issues,"There're some strange issues including STORM-1933 and others (which I will file an issue soon) which are related to race condition in supervisor.

As I mentioned to STORM-1933, basically sync-supervisor relies on zk assignment, and sync-processes relies on local assignment and local workers directory, but in fact sync-supervisor also access local state and take some actions which affects sync-processes. And also Satish left the comment to STORM-1933 describing other issue related to race condition and idea to fix this which is same page on me.

"
STORM-1933,Intermittent test failure on test-multiple-active-storms-multiple-supervisors for supervisor-test ,"test-multiple-active-storms-multiple-supervisors is failing with fairly high chance. I've run unit test of 1.x branch 3 times and met this issue, and users report FileNotFound issue on supervisor which seems to be related to this.

I have log file so I'll attach once issue is created."
STORM-1931,Share mapper and selector in Storm-Kafka,Storm Kafka's mapper and selector and Storm Kafka trident's mapper and selector are the same . I try to merge them into one .
STORM-1930,Kafka New Client API - Support for Topic Wildcards,
STORM-1929,Check when create topology,"Add some check when create topology .

1. Spout and Bolt id shouldn't conflict

2. createTopology's spout and bolt set shouldn't empty ."
STORM-1927,Upgrade Jetty and Ring,"Jetty 7 is EOL , upgrade to Jetty 9 & Ring could also support it."
STORM-1926,Upgrade Jetty and Ring,Jetty 7 is EOL so we should upgrade to Jetty 9
STORM-1925,Nimbus fails to start in secure mode ,"We are noticing a failure in secure cluster as nimbus failed to start
2016-06-23 06:43:48.874 o.a.s.d.nimbus [ERROR] Error when processing event
java.lang.NullPointerException
	at org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer.permit(SimpleACLAuthorizer.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:1047)
	at org.apache.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:1051)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__11183.getClusterInfo(nimbus.clj:1772)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313)
	at org.apache.storm.daemon.nimbus$send_cluster_metrics_to_executors.invoke(nimbus.clj:1393)
	at org.apache.storm.daemon.nimbus$fn__11394$exec_fn__3529__auto____11395$fn__11421.invoke(nimbus.clj:2254)
	at org.apache.storm.timer$schedule_recurring$this__2156.invoke(timer.clj:105)
	at org.apache.storm.timer$mk_timer$fn__2139$fn__2140.invoke(timer.clj:50)
	at org.apache.storm.timer$mk_timer$fn__2139.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
2016-06-23 06:43:48.877 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__10332.invoke(nimbus.clj:205)
	at org.apache.storm.timer$mk_timer$fn__2139$fn__2140.invoke(timer.clj:71)
	at org.apache.storm.timer$mk_timer$fn__2139.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)"
STORM-1923,Storm site page not found ,[DaemonMetrics/Monitoring|http://storm.apache.org/releases/1.0.1/storm-metrics-profiling-internal-actions.html]  Not found 
STORM-1921,Update parallelism_hint date type to integer,update TopologyBuilder's parallelism_hint date type from Number to int
STORM-1920,version of parent pom for storm-kafka-monitor is set 1.0.2-SNAPSHOT in master branch,"Recent Travis CI builds for pull requests are all failed due to this.

https://travis-ci.org/apache/storm/jobs/139371381

Unfortunately it should be hard to find with dev. environment since many of us ran ""mvn install"" on Storm 1.0.x-branch which eventually installed Storm 1.0.2 jar. That's why we shouldn't ignore CI bad sign."
STORM-1915,Supervisor keeps restarting forever,"While submitting a topology to a 20 node 40 worker strong cluster, the supervisor keeps throwing errors and keeps restarting the workers it is supervising.

For this reason the topology never starts, instead it keeps dancing by reassigning the bolts and spouts forever.

I'd love to attach the logs here but I can't find any upload button in the JIRA form.

The error basically says:
{code}
2016-06-18 12:04:26.589 o.a.s.config [WARN] Failed to get worker user for . #error {
 :cause /home/fogetti/downloads/apache-storm-1.0.1/storm-local/workers-users (Is a directory)
 :via
 [{:type java.io.FileNotFoundException
   :message /home/fogetti/downloads/apache-storm-1.0.1/storm-local/workers-users (Is a directory)
   :at [java.io.FileInputStream open0 FileInputStream.java -2]}]
 :trace
 [[java.io.FileInputStream open0 FileInputStream.java -2]
  [java.io.FileInputStream open FileInputStream.java 195]
  [java.io.FileInputStream <init> FileInputStream.java 138]
  [clojure.java.io$fn__9189 invoke io.clj 229]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9201 invoke io.clj 258]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9163 invoke io.clj 165]
  [clojure.java.io$fn__9115$G__9091__9122 invoke io.clj 69]
  [clojure.java.io$reader doInvoke io.clj 102]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [clojure.lang.AFn applyToHelper AFn.java 154]
  [clojure.lang.RestFn applyTo RestFn.java 132]
  [clojure.core$apply invoke core.clj 632]
  [clojure.core$slurp doInvoke core.clj 6653]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.config$get_worker_user invoke config.clj 239]
  [org.apache.storm.daemon.supervisor$shutdown_worker invoke supervisor.clj 281]
  [org.apache.storm.daemon.supervisor$kill_existing_workers_with_change_in_components invoke supervisor.clj 536]
  [org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078 invoke supervisor.clj 595]
  [org.apache.storm.event$event_manager$fn__8630 invoke event.clj 40]
  [clojure.lang.AFn run AFn.java 22]
  [java.lang.Thread run Thread.java 745]]}
{code}"
STORM-1914,Storm Kafka Field Topic Selector,Support field name and field index to select which kafka topic will used as a downstream .
STORM-1911,Inconsistency of timestamp between IMetricsConsumer and IClusterMetricsConsumer,"There's inconsistency of timestamp between IMetricsConsumer and IClusterMetricsConsumer: former is seconds and latter is milliseconds.

Since both of two are representing it to timestamp, and IMetricsConsumer is already being used, we need to change timestamp of IClusterMetricsConsumer to seconds."
STORM-1910,One topology can't use hdfs spout to read from two locations,"The hdfs uri is passed using config:
{code}
    conf.put(Configs.HDFS_URI, hdfsUri);
{code}
I see two problems with this approach:
1. If someone wants to used two hdfsUri in same or different spouts - then that does not seem feasible.
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/examples/storm-starter/src/jvm/storm/starter/HdfsSpoutTopology.java#L117-L117
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L331-L331
{code}
    if ( !conf.containsKey(Configs.SOURCE_DIR) ) {
      LOG.error(Configs.SOURCE_DIR + "" setting is required"");
      throw new RuntimeException(Configs.SOURCE_DIR + "" setting is required"");
    }
    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );
{code}
2. It does not fail fast i.e. at the time of topology submissing. We can fail fast if the hdfs path is invalid or credentials/permissions are not ok. Such errors at this time can only be detected at runtime by looking at the worker logs.
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L297-L297"
STORM-1909,Fix errors in HDFS Spout documentation,There is mistake in the example provided for the HDFS spout. The source/archive/bad directory settings should not have the 'hdfs://' portion.
STORM-1907,PartitionedTridentSpoutExecutor has incompatible types that cause ClassCastException,"This bug added during the refactor that occurred in this [pull request|https://github.com/apache/storm/pull/683]

This change causes a ClassCastException cannot cast ArrayList to Integer to occur when running org.apache.storm.starter.trident.TridentKafkaWordCount"
STORM-1906,Window count/length of zero should be disallowed,"This is related to: STORM-1841.
I see that we are still allowing zero for window size & window length. This should be disallowed."
STORM-1904,Storm shell display summary info,"display storm cluster summary info about cluster , nimbus ,supervisor and history . "
STORM-1900,Log configuration with logback,"I am trying to use logback in my project and am getting this message:
{quote}
2016-06-14 16:55:56.945 STDERR [INFO] SLF4J: Class path contains multiple SLF4J bindings.
2016-06-14 16:55:56.997 STDERR [INFO] SLF4J: Found binding in [jar:file:/opt/apache-storm-0.10.1/lib/log4j-slf4j-impl-2.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: Found binding in [jar:file:/srv/storm/supervisor/stormdist/MatchIdentifiers-675106f-1465923070-5-1465923118/stormjar.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2016-06-14 16:56:03.647 STDERR [INFO] 2016-06-14 16:56:03,641 ERROR Logger contains an invalid element or attribute ""appender""{quote}

The solution slf4j suggests is to exclude slf4j from the dependency which in this case would be storm. But since storm includes slf4j in the classpath when starting the topology this does not work.


Is there a known way to fix this?
"
STORM-1899,Release HBase connection when topology shutdown,Storm HBase Client release connection when topology shutdown.
STORM-1897,"re-pattern file-path-separator problem with windows, breaks logviewer","re-pattern file-path-separator will cause errors in windows...

more specifically 
----
java.util.regex.PatternSyntaxException: Unexpected internal error near index 1
\

----

the ""\"" character (windows separator) is a reserved one in regex.  

hence the logviewer will not work on a windows machine...

A potential fix could be to define:

(defn file-path-separator-regex []
    (if on-windows?
        (re-pattern ""\\\\"")
        (re-pattern file-path-separator)))

and use this instead of ""re-pattern file-path-separator"" in logviewer.clj and config.clj

"
STORM-1895,blobstore replication-factor argument,storm command line argument --repl-fctr have update to  replication-factor and update the document .
STORM-1894,storm-redis does not support a Redis cluster for state saving,"Working with Storm and stateful bolts we noticed that it is not possible to work with a Redis cluster at the moment. The problem is, that storm-redis requires that the configuration is of type {{JedisPoolConfig}} which only allows defining one host. If the given Redis instance is configured as a Redis cluster exceptions of type {{JedisMovedDataException}} might occur.
The configuration via {{JedisClusterConfig}} seems to provide support for a Redis cluster, but {{RedisKeyValueStateProvider}} does not handle it."
STORM-1892,class org.apache.storm.hdfs.spout.TextFileReader should be public,
STORM-1890,"Employ cache-busting method to ensure newly deployed UI forces browsers to refetch scripts, templates, and CSS","Currently we don't employ cache busting techniques in the Storm UI while fetching script.js, CSS and templates. Ring is providing the Last-Modified header, but browsers implement a heuristic to when they deem a resource stale (https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.2.4). This means that as the Last-Modified for a resource is further away in the past, the longer the browsers are going to wait until they refetch. It looks like 10% padding is common, so if script.js was last modified 100 days ago, the browser will not fetch it until 10 days after the time it was cached.

An easy approach is to add a url parameter to allow for cache busting whenever storm is packaged (mvn package). A more complicated method is versioning the files (we'd need to specify them in the pom.xml individually using the assembly plugin, unless we use some other plugin). The first method is (was?) considered less effective, since some CDNs/browsers can decide not to cache the query parameter.

I'd like to go with the simpler method, unless there are strong opinions to changing file names (this means we need to specify files in the assembly pom.xml). Also, going this route we don't need any new plugins, and the assembly build can just be changed to export a variable. We would modify calls to include a value that changes on mvn package:

{code}
<script src=""/js/script.js?_ts=${timestamp}"" type=""text/javascript""></script> 
{code}

instead of:

{code}
<script src=""/js/script.js"" type=""text/javascript""></script>
{code}

Where $\{timestamp\} will be replaced at assembly time by maven. This would be the time when the assembly build started.

The templates will also have the extra parameter. I think providing this to ajaxSetup will do the trick. For example:

{code}
$.ajaxSetup({ data: {""_ts"" : ""${timestamp}""}});
{code}"
STORM-1889,Datatables error message displayed when viewing UI,"Updating to storm 1.0.1, running on Windows 7, I receive error messages from Datatables.
This occurs on the Topology Summary as well as the Component Summary for a spout/bolt

Example error: DataTables warning: table id=executor-stats-table - Requested unknown parameter '9' for row 0. For more information about this error, please see http://datatables.net/tn/4

If I edit index.html to remove the type: num targets, the errors go away.

For example.

  $.getJSON(""/api/v1/topology/summary"",function(response,status,jqXHR) {
      $.get(""/templates/index-page-template.html"", function(template) {
          topologySummary.append(Mustache.render($(template).filter(""#topology-summary-template"").html(),response));
          //name, owner, status, uptime, num workers, num executors, num tasks, replication count, assigned total mem, assigned total cpu, scheduler info
          dtAutoPage(""#topology-summary-table"", {
            columnDefs: [
              //{type: ""num"", targets: [4, 5, 6, 7, 8, 9]},
			  {type: ""num"", targets: []},
              {type: ""time-str"", targets: [3]}
            ]
          });
          $('#topology-summary [data-toggle=""tooltip""]').tooltip();
      });




"
STORM-1887,Help message for the set_log_level command does not have the topology name parameter,"The help message for the set_log_level command does not have the topology-name parameter:

$ storm help set_log_level

    Dynamically change topology log levels

    Syntax: [storm set_log_level -l [logger name]=[log level][:optional timeout] -r [logger name]
    where log level is one of:
        ALL, TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF
    and timeout is integer seconds.
(...)

If you don't pass a `topology-name` as last parameter, you'll get the following error:

$ storm set_log_level -l com.myapp=WARN
Running: /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/usr/local/Cellar/storm/1.0.1/libexec -Dstorm.log.dir=/usr/local/Cellar/storm/1.0.1/libexec/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /usr/local/Cellar/storm/1.0.1/libexec/lib/asm-5.0.3.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/clojure-1.7.0.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/disruptor-3.3.2.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/kryo-3.0.3.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-api-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-core-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-over-slf4j-1.6.6.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-slf4j-impl-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/minlog-1.3.0.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/objenesis-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/reflectasm-1.10.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/servlet-api-2.5.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/slf4j-api-1.7.7.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/storm-core-1.0.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/storm-rename-hack-1.0.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/conf:/usr/local/Cellar/storm/1.0.1/libexec/bin org.apache.storm.command.set_log_level -l com.myapp=WARN
2670 [main] INFO  o.a.s.c.set-log-level - Sent log config LogConfig(named_logger_level:{com.myapp=LogLevel(action:UPDATE, target_log_level:WARN, reset_log_level_timeout_secs:0)}) for topology
Exception in thread ""main"" java.lang.IllegalArgumentException: No matching field found: IllegalArgumentException for class java.lang.String
    at clojure.lang.Reflector.getInstanceField(Reflector.java:271)
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:315)
    at org.apache.storm.command.set_log_level$get_storm_id.invoke(set_log_level.clj:31)
    at org.apache.storm.command.set_log_level$_main.doInvoke(set_log_level.clj:75)
    at clojure.lang.RestFn.applyTo(RestFn.java:137)
    at org.apache.storm.command.set_log_level.main(Unknown Source)
(...)

I opened a PR to fix this: https://github.com/apache/storm/pull/1463"
STORM-1883,FileReader extends Closeable Interface,use Closeable Interface to decorate FileReader to support close()
STORM-1882,Expose TextFileReader public,"[Storm HDFS Using|https://github.com/apache/storm/tree/master/external/storm-hdfs#usage-1]

TextFileReader is package-private .

Should make TextFileReader a public class to expose it to user and hdfs spout. "
STORM-1881,storm-redis is missing dependant libraries in distribution,"Despite the documentation on http://storm.apache.org/releases/1.0.1/State-checkpointing.html it is not enough to simply copy {{storm-redis-*.jar}} to {{extlib}} to get the {{RedisKeyValueStateProvider}} working. Depending jedis and apache-commons-pool2 jars are missing and must be copied by hand to get it working. Else one is greeted with exception stack traces like:

{code}
Caused by: java.lang.ClassNotFoundException: org.apache.commons.pool2.impl.GenericObjectPoolConfig
{code}

or

{code}
Caused by: java.lang.ClassNotFoundException: redis.clients.jedis.JedisPoolConfig
{code}

Copying {{commons-pool2-2.4.2.jar}} and {{jedis-2.8.1.jar}} from hand to {{extlib}} solves the issue.
It might be better to create a ""fat"" jar of {{storm-redis-*.jar}} or provide documentation, which libraries have to be made available."
STORM-1880,Support  EXISTS Command Storm-Redis,add exists command in storm-redis LookupBolt
STORM-1879,Supervisor may not shut down workers cleanly,"We've run into a strange issue with a zombie worker process. It looks like the worker pid file somehow got deleted without the worker process shutting down. This causes the supervisor to try repeatedly to kill the worker unsuccessfully, and means multiple workers may be assigned to the same port. The worker root folder sticks around because the worker is still heartbeating to it.

It may or may not be related that we've seen Nimbus occasionally enter an infinite loop of printing logs similar to the below.

{code}
2016-05-19 14:55:14.196 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormconf.ser
2016-05-19 14:55:14.210 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormcode.ser
2016-05-19 14:55:14.218 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormconf.ser
2016-05-19 14:55:14.256 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormcode.ser
2016-05-19 14:55:14.273 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormcode.ser
2016-05-19 14:55:14.316 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormconf.ser
{code}

Which continues until Nimbus is rebooted. We also see repeating blocks similar to the logs below.

{code}
2016-06-02 07:45:03.656 o.a.s.d.nimbus [INFO] Cleaning up ZendeskTicketTopology-127-1464780171
2016-06-02 07:45:04.132 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:ZendeskTicketTopology-127-1464780171-stormjar.jar)
2016-06-02 07:45:04.144 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:ZendeskTicketTopology-127-1464780171-stormconf.ser)
2016-06-02 07:45:04.155 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:ZendeskTicketTopology-127-1464780171-stormcode.ser)
{code}"
STORM-1878,Flux does not handle stateful bolts,"We noticed that it is not possible at the moment to create a topology with Flux which contains stateful bolts (based on IStatefulBolt). Those bolts will not be instantiated.

Pull request upcoming."
STORM-1875,Separate Jedis/JedisCluster Config,Separate Jedis / JedisCluster to provide full operations for each environment to users . 
STORM-1874,Update Logger access permissions,Update Log access permissions from public to private . 
STORM-1872,Storm Redis connection release ,Strom Redis connect should be release when topology shutdown .
STORM-1871,Storm Alluxio integrate,[alluxio|http://alluxio.org/] is a memory speed virtual distributed storage system.Alluxio’s memory-centric architecture enables data access orders of magnitude faster than existing solutions.
STORM-1867,Storm Topology Freezes,"My storm topology freezes after few hours. I have a KafkaSpout, a parser bolt and an aggregator bolt.

After few hours I see KafkaSpout is not emitting anything. I checked the STORM UI, there were no errors. I also checked the kafka topic via kafka-console-consumer, I was able to consume but kafkaspout was emitting nothing. 

I checked worker.log and there were no issues.

Following are the last few lines of worker.log. 


2016-05-26 03:45:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:45:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:45:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:47:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:47:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:47:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:49:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:49:16.259 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:49:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:51:16.254 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:51:16.259 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:51:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:53:16.254 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:53:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:53:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:55:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:55:16.265 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:55:16.265 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:57:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:57:16.261 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:57:16.261 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:59:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:59:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:59:16.261 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
"
STORM-1862,Flux ShellSpout and ShellBolt can't emit to named streams,See pull request: https://github.com/apache/storm/pull/1426
STORM-1854,Trident transactional spouts are broken in 1.0.x,"In the process of upgrading our Storm code from 0.10.0 to 1.0.0, I've run into an issue with TransactionalTridentKafkaSpout. When running one of our topologies I'm getting the following exception:

{code}
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.lang.Integer
	at org.apache.storm.trident.spout.PartitionedTridentSpoutExecutor$Coordinator.initializeTransaction(PartitionedTridentSpoutExecutor.java:55) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.trident.spout.PartitionedTridentSpoutExecutor$Coordinator.initializeTransaction(PartitionedTridentSpoutExecutor.java:43) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:70) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.0.jar:1.0.0]
{code}

The issue appears to be caused by a change in PartitionedTridentSpoutExecutor between the two versions, specifically this method:

1.0.0 - https://github.com/apache/storm/blob/v1.0.0/storm-core/src/jvm/org/apache/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L51

{code}
public Integer initializeTransaction(long txid, Integer prevMetadata, Integer currMetadata) {
    if(currMetadata!=null) {
        return currMetadata;
    } else {
        return _coordinator.getPartitionsForBatch();            
    }
}
{code}

0.10.0 - https://github.com/apache/storm/blob/v0.10.0/storm-core/src/jvm/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L51

{code}
public Object initializeTransaction(long txid, Object prevMetadata, Object currMetadata) {
    if(currMetadata!=null) {
        return currMetadata;
    } else {
        return _coordinator.getPartitionsForBatch();            
    }
}
{code}

This was introduced by: https://github.com/apache/storm/commit/9e4c3df17ffbc737210e606d3d8a9cdae8f86634

TransactionalTridentKafkaSpout uses List<GlobalPartitionInformation> for its metadata. Generally, transactional spouts should have metadata that is more complex than just an Integer. OpaquePartitionedTridentSpoutExecutor uses Object for its metadata and correctly handles the metadata used by OpaqueTridentKafkaSpout (List<GlobalPartitionInformation>).

It looks like reverting the metadata type for transactional spouts in PartitionedTridentSpoutExecutor should work, but I haven't tried this yet."
STORM-1851,Nimbus impersonation authorizer in defaults.yaml causes issues in secure mode,"  ""nimbus.impersonation.authorizer"" is set to ""ImpersonationAuthorizer"" by default and this causes issues when a user tries to submit topology as a different user in secure mode since the ""nimbus.impersonation.acl"" configuration is not set by default. Users need to set nimbus.impersonation.acl first before they can submit topology as a user other than ""storm"" in secure mode.

Removing this config allows users to submit topologies as any user in secure mode by default. Users can set up impersonation by providing both authorizer and the acls in storm.yaml."
STORM-1848,NotSerializableException when using storm-kafka spout with event logging,"We deployed a topology to multiple workers and set topology.eventlogger.executors to be identical to the number of workers. When the debug button in Storm UI is pressed, the spout will start throwing NotSerializableExceptions.

java.lang.RuntimeException: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.kafka.PartitionManager$KafkaMessageId at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) at (snip)

KafkaMessageId should be made serializable."
STORM-1845,use UTF-8 instead of default encoding,
STORM-1844,Some tests are flaky due to low timeout,"We saw a few test failures likely due to low timeouts https://github.com/apache/storm/pull/1417#issuecomment-219545865. The timeouts for these tests should be increased, and the test mentioned by [~kishorvpatil] should reflect STORM_TEST_TIMEOUT_MS."
STORM-1842,Forward references in storm.thrift cause tooling issues,"In recent versions of Storm (starting with version 1.0.0, I believe), storm.thrift uses several types before they are declared:

* HBPulse
* HBRecords
* HBNodes

These types are used in the definition of HBMessageData.

This causes issues with downstream tools. For example, generating Python wrappers for the Storm Thrift types creates a module that will not import successfully.

Would it be possible to reorder this code to define the types before using them? This appears to be a simple change."
STORM-1841,Address a few minor issues in windowing and doc,"1. Do not accept negative values for window length or sliding interval in BaseWindowedBolt
2. Added static factories for Count and Duration for ease of use.
3. Explicitly call out when the first window is evaluated for sliding windows in the windowing doc."
STORM-1837,"Running local clusters without simulating time breaks Testing.completeTopology, and may cause message loss","Since https://github.com/apache/storm/pull/810 it is no longer possible to call Testing.completeTopology when time is not simulating, because a call to advance-cluster-time is made from the function, which calls Time/advanceTime. advance-cluster-time should only be called if time is simulating.

Since https://github.com/apache/storm/pull/830 a local cluster run without time simulation may lose messages. When a worker emits messages for a worker that hasn't started yet, the message is lost. This can happen because spouts may start emitting before all workers have started, when time simulation is disabled. Local clusters usually run without message timeouts, so this will make tests relying on Testing.withLocalCluster flaky.

The problem is that there are no longer any queues to store messages for workers that haven't started yet. See https://github.com/apache/storm/pull/830/files#diff-c6ff4208ef84c7a5a1a6b8b6bd1f7d19R104. A queue should be added for messages for workers that haven't registered a receive callback yet."
STORM-1835,add lock info in thread dump,
STORM-1834,Documentation How to Generate Certificates For Local Testing SSL Setup,"This patch must be cherry picked in 0.10.x-branch, 1.x-branch, and master"
STORM-1832,Consistently slow metrics consumer triggers backpressure which will be never back to normal,"If metrics consumer is too slow to keep up processing received messages, eventually backpressure is triggered. Spout throttles, but metrics messages are not throttled so it could be chance for topology to be never back to normal.

While STORM-1698 can resolve this issue, I made this issue to clarify this behavior and mark this as 'bug'.

If you'd like to see its symptom, please refer here: https://github.com/apache/storm/pull/1324#issuecomment-218962460"
STORM-1775,Generate StormParserImpl before maven building instead of in packaging time,"Just like genthrift.sh genrates the generated thrift-about java source files. I think it is better generate StormParserImpl.java before maven execution.

It can reduce the complexity of storm-sql."
STORM-1774,Generate StormParserImpl before maven execution ,"Just like genthrift.sh genrates the generated thrift-about java source files. I think it is better to generate StormParserImpl.java before maven execution.

It can reduce the complexity of storm-sql."
STORM-1773,Utils.javaDeserialize() doesn't work with primitive types,"It's based on reporting from user@.
Please refer [here|http://mail-archives.apache.org/mod_mbox/storm-user/201605.mbox/%3CCAHObvqq81mVqPEi5e7C+i0r7u1hv4TqxT0Tn38dC1Exd6yUuxw@mail.gmail.com%3E] for details.

STORM-1040 (#919) replaces ObjectInputStream with ClassLoaderObjectInputStream while deserializing. But unfortunately ClassLoaderObjectInputStream has a bug which cannot handle primitive types. Please refer [IO-378|https://issues.apache.org/jira/browse/IO-378].

Fortunately IO-378 was included at latest release 2.5, so we would be OK to just upgrade the version of commons-io."
STORM-1767,metrics log entries are being appended to root log,"Current setup of metrics logger ( {{storm/log4j2/worker.xml}}) uses fully qualified name of the class where the logging is happening from i.e `org.apache.storm.metric.LoggingMetricsConsumer`, which is problematic and does not achieve the original intent as stated by the METRICS appender defined in {{storm/log4j2/worker.xml}}.

Currently the metrics logger created explicitly by using the name above:
{{LoggerFactory.getLogger(""org.apache.storm.metric.LoggingMetricsConsumer"")}} or implicitly from within the {{LoggingMetricsConsumer}} by calling {{LoggerFactory.getLogger(LoggingMetricsConsumer.class)}} will be logging to **root** logger.

This happens because logger names use Java namespaces and as such create hierarchies. 

The solution is to name metrics logger outside of {{org.apache.storm.*}} namespace which is what is happening for all other non-root loggers defined within the {{storm/log4j2/worker.xml}} file. 

This will also mean a code change to {{LoggingMetricsConsumer}} class itself for it to use the logger with an explicit name matching the name defined in the {{worker.xml}} file.

The fix is easy. 
"
STORM-1761,Storm-Solr Example Throws ArrayIndexOutOfBoundsException in Remote Cluster Mode,
STORM-1755,Revert the kafka client version upgrade in storm-kafka module,storm-kafka module does not use any feature of new API. The newer kafka client (0.9.x) is not backward compatible with 0.8.x brokers and upgrading storm-kafka version in topology could break the currently running topologies. 
STORM-1750,Report-error-and-die may not kill the worker,"The report-error-and-die function in executor.clj calls report-error, which can throw exceptions if Curator runs into any kind of trouble while registering the error. I suspect this may happen with network errors, but it can also happen if two executors for the same component throw exceptions at the same time and no errors have been registered for the component previously. This is because both calls to report-error-and-die update the lastErrorPath, and ZkStateStorage set_data doesn't catch the potential NodeExistsException that may be thrown from the create call.

If an exception is thrown from report-error, the suicide-fn is never called, and the worker keeps running sans the crashed executor."
STORM-1744,Missing javadoc in Trident code,"Some or most of the core Trident classes don't have javadoc. It makes it really difficult to use.

http://storm.apache.org/releases/2.0.0-SNAPSHOT/javadocs/index.html

Examples:
TridentTopologyBuilder
IBackingMap"
STORM-1741,storm-env.sh unconditionally sets JAVA_HOME,"STORM-1706 introduced storm-env.sh to the binary distribution. Before 1.0.1 we weren’t including `storm-env.sh`. That file does the following:

export JAVA_HOME=${JAVA_HOME}

Which, if JAVA_HOME is not set, will set it, but leave it empty. So the clojure code in supervisor.clj `if (nil? java-home)` will evaluate to false and we’ll end up with `/bin/java` as the java command."
STORM-1735,Nimbus logs that replication was not reached when min-replication-count was reached exactly,"When waiting for replication during topology submission, Nimbus logs whether replication succeeded within the timeout or not. The check for whether to log that it timed out or succeeded is off by one."
STORM-1723,Introduce ClusterMetricsConsumer,"NOTE: This issue is already discussed shortly. Please refer [here|http://mail-archives.apache.org/mod_mbox/storm-dev/201604.mbox/%3CCAF5108hDCcMKxLXKUYLReOoKkNNdgW2YudweR+mKr=1hLSL2Ew@mail.gmail.com%3E] for details.

This issue focuses to introduce ClusterMetricsConsumer and provide interface to let users plugin their consumers.

ClusterMetricsConsumers will be attached to Nimbus, and leader of Nimbus will push cluster related metrics to ClusterMetricsConsumer.

Requirements of ClusterMetricsConsumer are here:

- Only leader of Nimbus should publish cluster metrics to consumer.
- Nimbus shouldn't be affected by crashing or heavy latency on consumer.
- Consumer should have resilient when crashing or Nimbus should take care of."
STORM-1720,Support GEO in storm-redis,GEO is a new feature in redis 3.2 . It's useful in Geography calculate.
STORM-1719,Introduce REST API: Topology metric stats for stream,"Apache Storm has two REST APIs for showing topology stats, /api/v1/topology/:id and showing component stats, /api/v1/topology/:id/component/:component.

Both of APIs shows metrics aggregated by topology or each component. 
It helps determining traffics between component to component, which is good for many topologies. But if users use their own topology which utilizes lots of streams, they may want to see topology stats in detail - in point of stream's view.

We already have visualization API to show the traffics between stream to stream, but it's internal API, and contains only transferred, and it shows non-aggregated result so users need to aggregate theirselves."
STORM-1717,"Support Redis INRC , INRCBY and INRCBYFLOAT",to support redis incr incrby and incrbyfloat  
STORM-1716,Add some external Jedis pool config,add some jedis pool config 
STORM-1715,Jedis Default Host,Using Jedis Protocol.DEFAULT_HOST to replace DEFAULT_HOST
STORM-1714,StatefulBolts ends up as normal bolts while using TopologyBuilder.setBolt without parallelism,StatefulBolt inherits from IRichBolt which but the TopologyBuilder.setBolt overload is chosen based on the static type of the parameter causing issues. See if StatfulBolt can be refactored to not directly inherit from IRichBolt.
STORM-1711,Kerberos principals gets mixed up while using storm-hive,Storm-hive uses UserGroupInformation.loginUserFromKeytab which updates the static variable that stores current UGI.
STORM-1704,"When logviewer_search.html opens daemon file, next search always show no result","When searching keyword in /logviewer_search.html with daemon log file, is-daemon=yes parameter is gone so search shows no result."
STORM-1703,"In local mode, process is not shutting down clearly","Process is not shutting down clearly in local mode, but ‘Ctrl + C’ can terminate the process.

Will attach log file and jstack dump file."
STORM-1700,Introduce 'whitelist' / 'blacklist' option to MetricsConsumer,"Storm provides various metrics by default, and so on some external modules (storm-kafka).

When we register MetricsConsumer, MetricsConsumer should handle all of metrics. If MetricsConsumer cannot keep up with these metrics, only way to keep up is increasing parallelism, which seems limited. Furthermore, some users don't want to care about some metrics since unintended metrics will fill external storage.

Though MetricsConsumer itself can filter metrics by name, it would be better to support filter by Storm side. It will reduce the redundant works for Storm community.

If we provide filter options, it would be great."
STORM-1698,Asynchronous MetricsConsumerBolt,"Currently MetricsConsumerBolt is delegating MetricsConsumer to handle data points via synchronous manner.

When MetricsConsumer cannot keep up, it will trigger backpressure when (queue size + overflow buffer size) reaches high watermark, which incurs slowing down the topology in result. 

Slowing down Itself is not a problem because that’s what backpressure is for. The actual problem is that backpressure only throttles spout, not metrics. If MetricsConsumerBolt cannot keep up with incoming tuples, backpressure never ends and topology just hangs. If we turn off backpressure, we have unbounded queue and worker could throw OOME eventually.

Making MetricsConsumerBolt asynchronous can resolve this issue. One downside of making it async is that it's hard to see that MetricsConsumerBolt is keeping up now. (capacity will be always around 0)
I don't have an idea for now but I think it's still better than current.

Before making consensus about huge change of metrics, I'd love to improve current metrics without breaking backward compatible manner. It could be applied to 1.x-branch, and even 0.10.x-branch."
STORM-1697,artifacts symlink not created ,"No artifacts symlink generated under worker's current directory. Gc log, jstack and heapdump will not be working.

2016-04-07 17:43:19.909 STDERR [INFO] Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file artifacts/gc.log due to No such file or directory
2016-04-07 17:43:19.913 STDERR [INFO]"
STORM-1696,Backpressure flag not sync if zookeeper connection errors,"When there is a zk exception happens during worker-backpressure!,
there is a bad state which can block the topology from running normally any more.

The root cause: in worker/mk-backpressure-handler
if the worker-backpressure! fails once due to zk connection exception,
next time when this method gets called by WordBackpressureThread, because (when (not= prev-backpressure-flag curr-backpressure-flag) will never be true, the remote zk node can not be synced with local state.

This also explains why we will not see any problem when testing in a stable (zk never fail) environment.

Solution is quite straightforward: first change the zk status, if succeeds, change local status.

This fixes the hidden bug and removes redundant flags in executor-data and worker-data (since we can get the executor status directly from the ""_throttleOn"" boolean in the DisruptorQueue)
"
STORM-1688,provide ParallismKillWorkerManager to shutdown workers  in parallel,
STORM-1687,Divide by zero exception in stats,"Since uptime can be 0, this will cause ArithmeticException: Divide by zero in compute-agg-capacity.

This will happen for both stats.clj in 1.x and StatsUtil.java in master (2.0).

{noformat}
java.lang.ArithmeticException: Divide by zero
at clojure.lang.Numbers.divide(Numbers.java:156)
at clojure.core$SLASH.invoke(core.clj:986)
at clojure.lang.AFn.applyToHelper(AFn.java:156)
at clojure.lang.RestFn.applyTo(RestFn.java:132)
at clojure.core$apply.invoke(core.clj:626)
at backtype.storm.util$div.doInvoke(util.clj:355)
at clojure.lang.RestFn.invoke(RestFn.java:423)
at backtype.storm.stats$compute_agg_capacity$fn__2249.invoke(stats.clj:409)
at backtype.storm.stats$compute_agg_capacity.invoke(stats.clj:404)
at backtype.storm.stats$agg_pre_merge_topo_page_bolt.invoke(stats.clj:555)
at backtype.storm.stats$agg_topo_exec_stats_STAR_.invoke(stats.clj:724)
at backtype.storm.stats$fn__2319.invoke(stats.clj:772)
at clojure.lang.MultiFn.invoke(MultiFn.java:241)
at clojure.lang.AFn.applyToHelper(AFn.java:165)
at clojure.lang.AFn.applyTo(AFn.java:144)
at clojure.core$apply.invoke(core.clj:628)
at clojure.core$partial$fn__4230.doInvoke(core.clj:2470)
at clojure.lang.RestFn.invoke(RestFn.java:421)
at clojure.core.protocols$fn__6086.invoke(protocols.clj:143)
at clojure.core.protocols$fn_6057$G6052_6066.invoke(protocols.clj:19)
at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31)
at clojure.core.protocols$fn__6078.invoke(protocols.clj:54)
at clojure.core.protocols$fn_6031$G6026_6044.invoke(protocols.clj:13)
at clojure.core$reduce.invoke(core.clj:6289)
at backtype.storm.stats$aggregate_topo_stats.invoke(stats.clj:854)
at backtype.storm.stats$agg_topo_execs_stats.invoke(stats.clj:1008)
at backtype.storm.daemon.nimbus$fn_5838$exec_fn1478auto$reify_5862.getTopologyPageInfo(nimbus.clj:1729)
at backtype.storm.generated.Nimbus$Processor$getTopologyPageInfo.getResult(Nimbus.java:3651)
at backtype.storm.generated.Nimbus$Processor$getTopologyPageInfo.getResult(Nimbus.java:3635)
at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:39)
at backtype.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:143)
at org.apache.thrift7.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)


{noformat}"
STORM-1683,UI Topology Visualization should not check sys streams by default,"New system streams (beginning with two underscores {{__}}) have been added since the visualization code was added, such as  __ack_reset_timeout  and  __eventlog. The visualization code currently presents stream check boxes as unchecked if they are ack-related streams and checked otherwise.

It seems that it should present only non-system streams as checked, so that the graph shows more relevant streams by default."
STORM-1682,Kafka spout can lose partitions,"The KafkaSpout can lose partitions for a period, or hang because getBrokersInfo (https://github.com/apache/storm/blob/master/external/storm-kafka/src/jvm/org/apache/storm/kafka/DynamicBrokersReader.java#L77) may get a NoNodeException if there is no broker info in Zookeeper corresponding to the leader id in Zookeeper. When this error occurs, the spout ignores the partition until the next time getBrokersInfo is called, which isn't until the next time the spout gets an exception on fetch. If the timing is really bad, it might ignore all the partitions and never restart.

As far as I'm aware, Kafka doesn't update leader and brokerinfo atomically, so it's possible to get unlucky and hit the NoNodeException when a broker has just died.

I have a few suggestions for dealing with this. 

getBrokerInfo could simply retry the inner loop over partitions if it gets the NoNodeException (probably with a limit and a short sleep between attempts). If it fails repeatedly, the spout should be crashed.

Alternatively the DynamicBrokersReader could instead lookup all brokers in Zookeeper, create a consumer and send a TopicMetadataRequest on it. The response contains the leader for each partition and host/port for the relevant brokers.

Edit: I noticed that the spout periodically refreshes the brokers info, so the issue isn't as bad as I thought. I still think this change has value, since it avoids the spout temporarily dropping a partition."
STORM-1681,Bug in scheduling cyclic topologies when scheduling with RAS,There is a bug in the bfs algorithm in RAS that does not correctly account for components already visited during the breadth first traveral
STORM-1677,"Test resource files (.log) are excluded from source distribution, which makes logviewer-test failing","While building RC1 of Apache Storm 1.0.0 from source code distribution, I found that test for logviewer-test in storm-core is failing. It was not intermittent.

After seeking the reason, I found files on src/dev directory is different from repo, "".log"" files are all excluded.

https://github.com/apache/storm/blob/master/storm-dist/source/src/main/assembly/source.xml

We excludes .log files in all directories which normally makes sense but not for now."
STORM-1676,NullPointerException while serializing ClusterWorkerHearbeat,"`Map<ExecutorInfo,ExecutorStats> executor_stats` had null value in the key which was causing NPE during serialization. "
STORM-1674,Idle KafkaSpout consumes more bandwidth than needed,"Discovered 30 megabits of traffic flowing between a set of KafkaSpouts
and our kafka servers even though no Kafka messages were moving.
Using the wireshark kafka dissector, we were able to see that
each FetchRequest had maxWait set to 10000
and minBytes set to 0. When binBytes is set to 0 the kafka server
responds immediately when there are no messages. In turn the KafkaSpout
polls without any delay causing a constant stream of FetchRequest/
FetchResponse messages. Using a non-KafkaSpout client had a similar
traffic pattern with two key differences
1) minBytes was 1
2) maxWait was 100
With these FetchRequest parameters and no messages flowing,
the kafka server delays the FetchResponse by 100 ms. This reduces
the network traffic from megabits to the low kilobits. It also
reduced the CPU utilization of our kafka server from 140% to 2%."
STORM-1673,log4j2/worker.xml refers old package of LoggerMetricsConsumer,"We changed package path from 'backtype.storm' to 'org.apache.storm'. Source codes seem to moved properly, but missed log4j2 configuration, so metric log is logged into worker log file, not metrics file.

It should be simple patch so I'd like to include this as 1.0.0. If we don't want to include any bugfixes I'm OK to remove epic."
STORM-1672,Stats not get class cast exception,"Component page in UI
{code}
2016-03-31 14:21:44.576 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map
        at org.apache.storm.stats.StatsUtil.filterSysStreams(StatsUtil.java:1696)
        at org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt(StatsUtil.java:240)
        at org.apache.storm.stats.StatsUtil.aggCompExecStats(StatsUtil.java:1130)
        at org.apache.storm.stats.StatsUtil.aggregateCompStats(StatsUtil.java:1108)
        at org.apache.storm.stats.StatsUtil.aggCompExecsStats(StatsUtil.java:1236)
        at org.apache.storm.daemon.nimbus$fn__3490$exec_fn__789__auto__$reify__3519.getComponentPageInfo(nimbus.clj:2130)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3826)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3810)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
{code}"
STORM-1671,Enable logviewer to delete directory with no yaml file ,"For those old and dead worker directories, in some weird case, there is no yaml file in it. We should enable logviewer to delete them (any dir that has/ has no yaml file in it).."
STORM-1670,LocalState#get(String) can throw FileNotFoundException which results in not removing worker heartbeats and supervisor is kind of stuck and goes down after some time.,"LocalState#get(String) can throw FileNotFoundException which may result in supervisor.clj#sync-processes which stop assigning new workers/assignments etc and supervisor goes down later.

VersionedStore#mostRecentVersionPath() can return a file only with suffix of .version but the original file for a specific version may not have been there because .version suffix was not deleted but respective data file may have been deleted in earlier cleanups. "
STORM-1669,Fix SolrUpdateBolt flush bug,"SolrUpdateBolt is setting the default tick tuple interval in the prepare() method, which is not taking effect.
This issue is the same as https://issues.apache.org/jira/browse/STORM-1219 and https://issues.apache.org/jira/browse/STORM-1654."
STORM-1668,Flux silently fails while setting a non-existent property.,"Currently, if a yaml file has a property with a name that does not exist on the java topology component object then flux silently fails by logging a message and does not throw an exception. This needs to be changed so that flux throws the exception failing topology submission so that user can take corrective action."
STORM-1666,Kill from the UI fails silently.,"{code}
2016-03-30 14:02:21.613 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.NullPointerException
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:26)
        at org.apache.storm.daemon.nimbus$fn__3070$exec_fn__2175__auto__$reify__3099$iter__3210__3214$fn__3215.invoke(nimbus.clj:1888)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.RT.seq(RT.java:507)
        at clojure.core$seq__4128.invoke(core.clj:137)
        at clojure.core$dorun.invoke(core.clj:3009)
        at clojure.core$doall.invoke(core.clj:3025)
        at org.apache.storm.daemon.nimbus$fn__3070$exec_fn__2175__auto__$reify__3099.getTopologyInfoWithOpts(nimbus.clj:1885)
        at org.apache.storm.generated.Nimbus$Processor$getTopologyInfoWithOpts.getResult(Nimbus.java:3774)
        at org.apache.storm.generated.Nimbus$Processor$getTopologyInfoWithOpts.getResult(Nimbus.java:3758)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
{code}"
STORM-1663,Clicking on an active topology from storm ui home page and then refreshing the page throws exception,"The exception thrown is:

org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getTopologyPageInfo(Nimbus.java:1243)
	at org.apache.storm.generated.Nimbus$Client.getTopologyPageInfo(Nimbus.java:1228)
	at org.apache.storm.ui.core$topology_page.invoke(core.clj:638)
	at org.apache.storm.ui.core$fn__3662.invoke(core.clj:987)
	at org.apache.storm.shade.compojure.core$make_route$fn__302.invoke(core.clj:93)
	at org.apache.storm.shade.compojure.core$if_route$fn__290.invoke(core.clj:39)
	at org.apache.storm.shade.compojure.core$if_method$fn__283.invoke(core.clj:24)
	at org.apache.storm.shade.compojure.core$routing$fn__308.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__312.invoke(core.clj:111)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1204.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__724.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__3091.invoke(helpers.clj:50)
	at org.apache.storm.ui.core$catch_errors$fn__3837.invoke(core.clj:1250)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2852.invoke(keyword_params.clj:27)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2892.invoke(nested_params.clj:65)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2823.invoke(params.clj:55)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3075.invoke(flash.clj:14)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3063.invoke(session.clj:43)
	at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__2991.invoke(cookies.clj:160)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2729.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__2733.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)"
STORM-1655,Flux doesn't set return code to non-zero when there's any exception while deploying topology to remote cluster,"Flux.runCli() swallows any Exceptions when deploying topology remotely, while StormSubmitter.submitTopology() can throw exceptions.
(AlreadyAliveException, InvalidTopologyException, AuthorizationException, and so on)

It just prints warning log, and return code is 0, not non-zero.

You can easily reproduce via deploying same topology twice with Flux."
STORM-1654,HBaseBolt creates tick tuples with no interval when we don't set flushIntervalSecs  ,"As STORM-1219 addressed, we can't get value about topology's message timeout seconds at getComponentConfiguration(), so logic for applying flush interval to the half of message timeout is no effect.
Unless we set flushIntervalSeconds explicitly, tick tuple interval is set to 0 second, no interval.

Other bolts were fixed as STORM-1219, but seems missing HBaseBolt."
STORM-1636, Supervisor shutdown with worker id pass in being nil ,"In function kill-existing-workers-with-change-in-components in supervisor.clj:
The function tries to detect whether there is a change in assignment. The bug in this function is that the ordering of the assignment matters but it shouldn't. For example, if a worker assignment is [[1 1] [2 2]] and it changed to [[2 2] [1 1]] it will cause the supervisor to restart the worker"
STORM-1630,"""create symbolic link"" needs elevation or setting privilege about creating symbolic link","I already initiated discussion thread. Please refer here.
http://mail-archives.apache.org/mod_mbox/storm-dev/201603.mbox/%3CCAF5108hVJpeZbC+JngcQmvE9HmV8HejsZ=cO1gdK3JDirumNmg@mail.gmail.com%3E

This PR is for tracking and resolving symlink issue after discussion. 

While it's just initiated, but IMO, at least ""how to resolve privilege issue manually"" should be documented since privilege issue on Windows users is introduced to Storm 1.0.0 for the first time."
STORM-1629,Files/move doesn't work properly with non-empty directory in Windows,"Distributed version of download-storm-code uses Files#move().
It runs well on *Nix (including OSX) but fails on Windows.

Javadoc describes this behavior, please refer below link.
https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#move(java.nio.file.Path,%20java.nio.file.Path,%20java.nio.file.CopyOption...)

{quote}
When invoked to move a directory that is not empty then the directory is moved if it does not require moving the entries in the directory. For example, renaming a directory on the same FileStore will usually not require moving the entries in the directory. When moving a directory requires that its entries be moved then this method fails (by throwing an IOException). To move a file tree may involve copying rather than moving directories and this can be done using the copy method in conjunction with the Files.walkFileTree utility method.
{quote}

If directory is not empty, file system should treat ""move directory"" as ""rename"".
Unfortunately, file system on Windows 8 doesn't.

We should change the way to be compatible with both kinds of OS."
STORM-1628,Explore any optimizations to avoid storing replayed batches in trident windowing,Explore any optimizations to avoid storing replayed batches in trident windowing
STORM-1627,Handle emitting delayed triggered results in trident windowing operation.,
STORM-1608,Fix stateful topology acking behavior,"Right now the acking is automatically taken care of for the non-stateful bolts in a stateful topology. This leads to double acking if BaseRichBolts are part of the topology. For the non-stateful bolts, its better to let the bolt do the acking rather than automatically acking."
STORM-1605,storm shell script should use /bin/env python to check python version,"Currently storm script uses /usr/bin/python2.6 and /usr/bin/python to check python version, however, RHEL5 ships with python2.4 by default, so when users upgrade python to 2.7.5 without changing /usr/bin/python to 2.7.5 python binary, storm will fail to start since it thinks it's using python2.4.
It would be better to use /bin/env python to detect python version instead."
STORM-1601,Cluster-state must check if znode exists before getting children for storm backpressure,"You see below exception in the integration tests..

{panel}
15:46:23 java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /backpressure/topologytest-22ffcfa0-8992-4258-b8b6-52346a129b58-1-0
15:46:23 	at backtype.storm.util$wrap_in_runtime.invoke(util.clj:52) ~[classes/:?]
15:46:23 	at backtype.storm.zookeeper$get_children.invoke(zookeeper.clj:168) ~[classes/:?]
15:46:23 	at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4184.get_children(zookeeper_state_factory.clj:129) ~[classes/:?]
15:46:23 	at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source) ~[?:?]
15:46:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
15:46:23 	at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
15:46:23 	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
15:46:23 	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.cluster$mk_storm_cluster_state$reify__4091.topology_backpressure(cluster.clj:407) ~[classes/:?]
15:46:23 	at sun.reflect.GeneratedMethodAccessor210.invoke(Unknown Source) ~[?:?]
15:46:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
15:46:23 	at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
15:46:23 	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
15:46:23 	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.daemon.worker$fn__6837$exec_fn__1477__auto__$reify__6839$check_throttle_changed__6910$cb__6911.doInvoke(worker.clj:704) ~[classes/:?]
15:46:23 	at clojure.lang.RestFn.invoke(RestFn.java:408) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.cluster$issue_map_callback_BANG_.invoke(cluster.clj:183) ~[classes/:?]
15:46:23 	at backtype.storm.cluster$mk_storm_cluster_state$fn__4081.invoke(cluster.clj:239) ~[classes/:?]
15:46:23 	at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$fn__4166.invoke(zookeeper_state_factory.clj:45) ~[classes/:?]
15:46:23 	at backtype.storm.zookeeper$mk_client$reify__2993.eventReceived(zookeeper.clj:63) ~[classes/:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl$8.apply(CuratorFrameworkImpl.java:860) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl$8.apply(CuratorFrameworkImpl.java:853) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92) [curator-framework-2.5.0.jar:?]
15:46:23 	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:?]
15:46:23 	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl.processEvent(CuratorFrameworkImpl.java:850) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$000(CuratorFrameworkImpl.java:57) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl$1.process(CuratorFrameworkImpl.java:138) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.ConnectionState.process(ConnectionState.java:152) [curator-client-2.5.0.jar:?]
15:46:23 	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522) [zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /backpressure/topologytest-22ffcfa0-8992-4258-b8b6-52346a129b58-1-0
15:46:23 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1625) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:210) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at sun.reflect.GeneratedMethodAccessor93.invoke(Unknown Source) ~[?:?]
15:46:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
15:46:23 	at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
15:46:23 	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
15:46:23 	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.zookeeper$get_children.invoke(zookeeper.clj:166) ~[classes/:?]
15:46:23 	... 29 more
{panel}"
STORM-1599,Don't mark dependencies as provided unless they are in lib,"When we mark a dependency as provided it indicates the shade and assembly plugins to not include this particular dependency in the uber topology jar because it will be {{provided}} on the class path by the system.

We have been doing this for all of our kafka dependencies incorrectly, storm-cassandra does this for cassandra-driver-core, and storm-starter is doing it for storm-clojure as well.

This means that storm-starter does not have any version of kafka or storm-clojure packaged it the resulting jar and any example that uses kafka, TridentKafkaWordCount, will fail with missing class errors. 

storm-starter/pom.xml has should change its dependency on storm-kafka to be compile, and it should delete dependencies on kafka and kafka-clients as those should come from storm-kafka as transitive dependencies.

the main pom.xml should not have kafka-clients marked as provided in the dependency management section.

storm-kafka should remove its provided tag on kafka, and flux examples + storm-sql-kafka should remove dependencies on kafka and kafka-clients, and storm-kafka should not me marked as provided. 

the flux and sql code I am not as familiar with, but looking at them, and running `mvn dependecy:tree` and `mvn dependency:analyze` it looks like"
STORM-1596,Multiple Subject sharing Kerberos TGT - causes services to fail,"With multiple threads accessing same {{Subject}}, it can cause {{ServiceTicket}} in use be by one thread be destroyed by another thread.

Running BasicDRPCTopology with high parallelism in secure cluster would reproduce the issue.

Here is sample log from such a scenarios:
{code}
2016-01-20 15:52:26.904 o.a.t.t.TSaslTransport [ERROR] SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_40]
        at org.apache.thrift7.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:271) [storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:195) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:191) [storm-core-0.10.1.y.jar:0.10.1.y]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_40]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_40]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:190) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:54) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:109) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.DRPCInvocationsClient.reconnectClient(DRPCInvocationsClient.java:57) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.ReturnResults.reconnectClient(ReturnResults.java:113) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.ReturnResults.execute(ReturnResults.java:103) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$fn__6377$tuple_action_fn__6379.invoke(executor.clj:689) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__6301.invoke(executor.clj:448) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.disruptor$clojure_handler$reify__6018.onEvent(disruptor.clj:40) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:437) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:416) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$fn__6377$fn__6390$fn__6441.invoke(executor.clj:801) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.util$async_loop$fn__742.invoke(util.clj:482) [storm-core-0.10.1.y.jar:0.10.1.y]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]
Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: The ticket isn't for us (35) - BAD TGS SERVER NAME)
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:770) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more
Caused by: sun.security.krb5.KrbException: The ticket isn't for us (35) - BAD TGS SERVER NAME
        at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]
        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more
Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
        at sun.security.krb5.internal.KDCRep.init(KDCRep.java:140) ~[?:1.8.0_40]
        at sun.security.krb5.internal.TGSRep.init(TGSRep.java:65) ~[?:1.8.0_40]
        at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]
        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more


{code}"
STORM-1595,'Fail' messages get stuck somewhere ,"'Fail' acks seem to be getting stuck somewhere between the acker and the spout. 

After a long time - sometimes multiple minutes - the fails show up in the spout.
I tested this on master and 1.x-branch and it occurs in both places.
"
STORM-1592,clojure code calling into Utils.exitProcess throws ClassCastException,Our exception handling is not longer working on master to shut down the process when an error occurs.
STORM-1585,Add DDL support for UDFs in Storm-sql,
STORM-1579,Got NoSuchFileException when running tests in storm-core,"Stacktrace:
125277 [Thread-1736-__eventlogger-executor[4 4]] ERROR o.a.s.m.FileBasedEventLogger - Error setting up FileBasedEventLogger.
java.nio.file.NoSuchFileException: /logs/workers-artifacts/metrics-tester-1-0/1024/events.log
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[?:1.7.0_75]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.7.0_75]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.7.0_75]
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) ~[?:1.7.0_75]
    at java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:430) ~[?:1.7.0_75]
    at java.nio.file.Files.newOutputStream(Files.java:172) ~[?:1.7.0_75]
    at java.nio.file.Files.newBufferedWriter(Files.java:2722) ~[?:1.7.0_75]
    at org.apache.storm.metric.FileBasedEventLogger.initLogWriter(FileBasedEventLogger.java:51) [classes/:?]
    at org.apache.storm.metric.FileBasedEventLogger.prepare(FileBasedEventLogger.java:97) [classes/:?]
    at org.apache.storm.metric.EventLoggerBolt.prepare(EventLoggerBolt.java:48) [classes/:?]
    at org.apache.storm.daemon.executor$fn__6507$bolt_transfer_fn__6522.invoke(executor.clj:792) [classes/:?]
    at clojure.lang.AFn.call(AFn.java:18) [clojure-1.7.0.jar:?]
    at org.apache.storm.utils.Utils$6.run(Utils.java:2177) [classes/:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_75]"
STORM-1578,"ClassCastException from Integer to Long for ""port"" in cluster.clj translation","(:port worker) is passed as Integer to java,
in STORM-1273, the port is defined as Long, which will cause java.lang.ClassCastException.
Funtions might be afftected:
    public void workerBackpressure(String stormId, String node, Long port, boolean on)          (confirmed)



<code>
  8953 java.lang.ClassCastException: Cannot cast java.lang.Integer to java.lang.Long
  8954     at java.lang.Class.cast(Class.java:3369) ~[?:1.8.0_60]
  8955     at clojure.lang.Reflector.boxArg(Reflector.java:427) ~[clojure-1.7.0.jar:?]
  8956     at clojure.lang.Reflector.boxArgs(Reflector.java:460) ~[clojure-1.7.0.jar:?]
  8957     at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:58) ~[clojure-1.7.0.jar:?]
  8958     at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
  8959     at org.apache.storm.daemon.worker$mk_backpressure_handler$reify__7649.onEvent(worker.clj:160) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT
<code>"
STORM-1576,TopologyBuilder fails with ConcurrentModification in addCheckPointInputs for stateful topologies,"addCheckPointInputs adds to map while iterating over it, which needs to be fixed."
STORM-1575,TwitterSampleSpout throws NPE on close,"the global ""_twitterStream"" is not initialized in ""open"" but used in ""close"""
STORM-1574,"Better exception handling in backpressure thread, and remove backpressure dir during topology kill.","The current exception handling in WorkerBackpressureThread can cause the thread to die before we want, causing potential backpressure flag synchronizing problem. 
Also, we need to cleanup the topology backpressure directory during killing."
STORM-1567,in defaults.yaml  'topology.disable.loadaware' should be 'topology.disable.loadaware.messaging',"{code:title=defaults.yaml|borderStyle=solid}
diff --git a/conf/defaults.yaml b/conf/defaults.yaml
index 166b249..01821e1 100644
--- a/conf/defaults.yaml
+++ b/conf/defaults.yaml
@@ -256,7 +256,7 @@ topology.bolts.outgoing.overflow.buffer.enable: false
 topology.disruptor.wait.timeout.millis: 1000
 topology.disruptor.batch.size: 100
 topology.disruptor.batch.timeout.millis: 1
-topology.disable.loadaware: false
+topology.disable.loadaware.messaging: false
 topology.state.checkpoint.interval.ms: 1000
 
 # Configs for Resource Aware Scheduler
{code}"
STORM-1566,Worker exits with error o.a.s.d.worker [ERROR] Error on initialization of server mk-worker java.lang.ClassCastException: java.lang.String cannot be cast to java.io.File,
STORM-1561,Supervisor should relaunch worker if assignments have changed,"Currently, supervisor validates new assignments against existing assignments by port. It should also check on the same port - if executors have changed."
STORM-1558,Utils in java breaks component page due to illegal type cast,"Two methods in Utils.java:
logsFilename and eventLogsFilename, the 'port' argument was changed to String type in PR for #STORM-1538, but its caller event-log-link and worker-log-link in core.clj passes an int port, which results in illegal type cast.

Also this is possibly the cause to #STORM-1545"
STORM-1556,nimbus.clj/wait-for-desired-code-replication wrong reset for current-replication-count-jar in local mode,"https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L520-L521

{code}
(if (not (ConfigUtils/isLocalMode conf))
    (reset! current-replication-count-conf  (get-blob-replication-count (ConfigUtils/masterStormConfKey storm-id) nimbus)))
(reset! current-replication-count-code  (get-blob-replication-count (ConfigUtils/masterStormCodeKey storm-id) nimbus))
(reset! current-replication-count-jar  (get-blob-replication-count (ConfigUtils/masterStormJarKey storm-id) nimbus))))
{code}

We do not go to count the number of jar-replication in local mode, but will count the number of conf-replication. So is it a mistake that current-replication-count-conf and current-replication-count-jar in the wrong place?

If it is a bug, I will create a PR soon."
STORM-1555,Required field 'topology_id' is unset! seen failing integration-test in Travis CI Test,"A recent travis ci [error|https://travis-ci.org/apache/storm/jobs/109239992#L1570] was seen.  

Here is the stack trace:

{noformat}
107323 [main] ERROR i.o.a.s.t.integration-test - Error in cluster

java.lang.RuntimeException: org.apache.thrift.protocol.TProtocolException: Required field 'topology_id' is unset! Struct:LSTopoHistory(topology_id:null, time_stamp:1455491275, users:[], groups:[])

	at org.apache.storm.utils.LocalState.serialize(LocalState.java:186) ~[classes/:?]

	at org.apache.storm.utils.LocalState.put(LocalState.java:142) ~[classes/:?]

	at org.apache.storm.utils.LocalState.put(LocalState.java:136) ~[classes/:?]

	at org.apache.storm.local_state$ls_topo_hist_BANG_.invoke(local_state.clj:48) ~[classes/:?]

	at org.apache.storm.daemon.nimbus$add_topology_to_history_log.invoke(nimbus.clj:1279) ~[classes/:?]

	at org.apache.storm.daemon.nimbus$fn__4836$exec_fn__1827__auto__$reify__4865.killTopologyWithOpts(nimbus.clj:1587) ~[classes/:?]

	at sun.reflect.GeneratedMethodAccessor298.invoke(Unknown Source) ~[?:?]

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_76]

	at java.lang.reflect.Method.invoke(Method.java:606) ~[?:1.7.0_76]

	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]

	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]

	at org.apache.storm.trident.testing$with_topology_STAR_.invoke(testing.clj:62) ~[classes/:?]

	at integration.org.apache.storm.trident.integration_test$fn__5251.invoke(integration_test.clj:149) [?:?]

	at clojure.test$test_var$fn__7670.invoke(test.clj:704) [clojure-1.7.0.jar:?]

	at clojure.test$test_var.invoke(test.clj:704) [clojure-1.7.0.jar:?]

	at clojure.test$test_vars$fn__7692$fn__7697.invoke(test.clj:722) [clojure-1.7.0.jar:?]

	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]

	at clojure.test$test_vars$fn__7692.invoke(test.clj:722) [clojure-1.7.0.jar:?]

	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]

	at clojure.test$test_vars.invoke(test.clj:718) [clojure-1.7.0.jar:?]

	at clojure.test$test_all_vars.invoke(test.clj:728) [clojure-1.7.0.jar:?]

	at clojure.test$test_ns.invoke(test.clj:747) [clojure-1.7.0.jar:?]

	at clojure.core$map$fn__4553.invoke(core.clj:2624) [clojure-1.7.0.jar:?]

	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]

	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]

	at clojure.lang.Cons.next(Cons.java:39) [clojure-1.7.0.jar:?]

	at clojure.lang.RT.boundedLength(RT.java:1735) [clojure-1.7.0.jar:?]

	at clojure.lang.RestFn.applyTo(RestFn.java:130) [clojure-1.7.0.jar:?]

	at clojure.core$apply.invoke(core.clj:632) [clojure-1.7.0.jar:?]

	at clojure.test$run_tests.doInvoke(test.clj:762) [clojure-1.7.0.jar:?]

	at clojure.lang.RestFn.invoke(RestFn.java:408) [clojure-1.7.0.jar:?]

	at org.apache.storm.testrunner$eval10993$iter__10994__10998$fn__10999$fn__11000$fn__11001.invoke(test_runner.clj:107) [?:?]

	at org.apache.storm.testrunner$eval10993$iter__10994__10998$fn__10999$fn__11000.invoke(test_runner.clj:53) [?:?]

	at org.apache.storm.testrunner$eval10993$iter__10994__10998$fn__10999.invoke(test_runner.clj:52) [?:?]

	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]

	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]

	at clojure.lang.RT.seq(RT.java:507) [clojure-1.7.0.jar:?]

	at clojure.core$seq__4128.invoke(core.clj:137) [clojure-1.7.0.jar:?]

	at clojure.core$dorun.invoke(core.clj:3009) [clojure-1.7.0.jar:?]

	at org.apache.storm.testrunner$eval10993.invoke(test_runner.clj:52) [?:?]

	at clojure.lang.Compiler.eval(Compiler.java:6782) [clojure-1.7.0.jar:?]

	at clojure.lang.Compiler.load(Compiler.java:7227) [clojure-1.7.0.jar:?]

	at clojure.lang.Compiler.loadFile(Compiler.java:7165) [clojure-1.7.0.jar:?]

	at clojure.main$load_script.invoke(main.clj:275) [clojure-1.7.0.jar:?]

	at clojure.main$script_opt.invoke(main.clj:337) [clojure-1.7.0.jar:?]

	at clojure.main$main.doInvoke(main.clj:421) [clojure-1.7.0.jar:?]

	at clojure.lang.RestFn.invoke(RestFn.java:421) [clojure-1.7.0.jar:?]

	at clojure.lang.Var.invoke(Var.java:383) [clojure-1.7.0.jar:?]

	at clojure.lang.AFn.applyToHelper(AFn.java:156) [clojure-1.7.0.jar:?]

	at clojure.lang.Var.applyTo(Var.java:700) [clojure-1.7.0.jar:?]

	at clojure.main.main(main.java:37) [clojure-1.7.0.jar:?]

Caused by: org.apache.thrift.protocol.TProtocolException: Required field 'topology_id' is unset! Struct:LSTopoHistory(topology_id:null, time_stamp:1455491275, users:[], groups:[])

	at org.apache.storm.generated.LSTopoHistory.validate(LSTopoHistory.java:586) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistory$LSTopoHistoryStandardScheme.write(LSTopoHistory.java:702) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistory$LSTopoHistoryStandardScheme.write(LSTopoHistory.java:628) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistory.write(LSTopoHistory.java:544) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistoryList$LSTopoHistoryListStandardScheme.write(LSTopoHistoryList.java:409) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistoryList$LSTopoHistoryListStandardScheme.write(LSTopoHistoryList.java:359) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistoryList.write(LSTopoHistoryList.java:309) ~[classes/:?]

	at org.apache.thrift.TSerializer.serialize(TSerializer.java:79) ~[libthrift-0.9.3.jar:0.9.3]

	at org.apache.storm.utils.LocalState.serialize(LocalState.java:184) ~[classes/:?]

	... 50 more
{noformat}
"
STORM-1552,Fix topology event sampling log directory ,"Run a topology and enable event inspection by clicking ""Debug"" from UI. The events are logged under ""storm-local/workers-artifacts/{storm-id}/port/events.log"". In the spout/bolt details page, the ""events"" link does not display the log file.

The events.log should be kept under logs/workers-artifacts/{storm-id}/{port}/events.log so that its viewable via logviewer."
STORM-1543,DRPCSpout should always try to reconnect disconnected DRPCInvocationsClient,"It appears, DRPCSpout skips pull request from DRPC Server if its not connected - but does not request reconnects.."
STORM-1541,Change scope of 'hadoop-minicluster' to test,"STORM-969 added dependency 'hadoop-minicluster' but not set scope to 'test' though it's for unit test. (and normally hadoop-minicluster is)

It may come up with other unintended dependencies."
STORM-1516,Topology workers are not getting killed when a topology is killed.,"When topology with timebased windowing bolts are killed, respective workers are not shutdown properly and they remain running. When you want to deploy a new topology, it throws with the below Exception as the earlier worker is not shutdown. This issue is not specific with this topology though.

2016-02-02 10:07:42.845 o.a.s.d.worker [ERROR] Error on initialization of server mk-worker
org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6700
	at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$worker_data$fn__6329.invoke(worker.clj:265) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.util$assoc_apply_self.invoke(util.clj:934) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:262) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$fn__6627$exec_fn__2511__auto__$reify__6629.run(worker.clj:605) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_60]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_60]
	at org.apache.storm.daemon.worker$fn__6627$exec_fn__2511__auto____6628.invoke(worker.clj:603) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__6627$mk_worker__6722.doInvoke(worker.clj:577) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:764) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_60]
	at sun.nio.ch.Net.bind(Net.java:433) ~[?:1.8.0_60]
	at sun.nio.ch.Net.bind(Net.java:425) ~[?:1.8.0_60]
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:1.8.0_60]
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.8.0_60]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_60]
	at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_60]
2016-02-02 10:07:42.857 o.a.s.util [ERROR] Halting process: (""Error on initialization"")"
STORM-1501, launch worker process exception will cause supervisor process exited,"[util.clj/async-loop | https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/util.clj#L474] default kill-fn will kill current process  

when supervisor use [util.clj/launch-process | https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/util.clj#L546] to launch worker process , if exeception occurs , supervisor process will exit."
STORM-1489,Script for cleaner environment checking,"As a storm developer, I would like a common script that storm can execute to detect features of the environment (like OS), so that storm has cleaner code for enabling features at run-time.

See [original comment|https://github.com/apache/storm/pull/1012#discussion_r50280610]"
STORM-1488,UI Topology Page component last error timestamp is from 1970,"Seems to be something wrong with the parsing of the timestamp into a date string.

!screen-shot-error-time.png|thumbnail!"
STORM-1487,UI Topology Page tooltips misplaced,"!screen-shot-tooltips.png|thumbnail!

Seems the placement is off.

(Note, the error timestamp in the year 1970 will be handled in a separate issue.)"
STORM-1470,"org.apache.hadoop:hadoop-auth not shaded, breaks spnego authentication","{noformat}
2016-01-12 20:07:45.642 o.a.s.s.o.e.j.s.ServletHandler [WARN] Error for /favicon.ico
java.lang.NoClassDefFoundError: org/apache/commons/codec/binary/Base64
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:343)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:519)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
        at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
        at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
        at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
        at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

We already shade commons-codec:commons-codec, but we don't apply that shading to org.apache.hadoop:hadoop-auth.
"
STORM-1469,Unable to deploy large topologies on apache storm,"When deploying to a nimbus a topology which is larger in size >17MB, we get an exception. In storm 0.9.3 this could be mitigated by using the following config on the storm.yaml to increse the buffer size to handle the topology size. i.e. 50MB would be

nimbus.thrift.max_buffer_size: 50000000

This configuration does not resolve the issue in the master branch of storm and we cannot deploy topologies which are large in size.

Here is the log on the client side when attempting to deploy to the nimbus node:
java.lang.RuntimeException: org.apache.thrift7.transport.TTransportException
	at backtype.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:251) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:272) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:155) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at com.trustwave.siem.storm.topology.deployer.TopologyDeployer.deploy(TopologyDeployer.java:149) [siem-ng-storm-deployer-cloud.jar:]
	at com.trustwave.siem.storm.topology.deployer.TopologyDeployer.main(TopologyDeployer.java:87) [siem-ng-storm-deployer-cloud.jar:]
Caused by: org.apache.thrift7.transport.TTransportException
	at org.apache.thrift7.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TFramedTransport.read(TFramedTransport.java:101) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:77) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.generated.Nimbus$Client.recv_submitTopology(Nimbus.java:238) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.generated.Nimbus$Client.submitTopology(Nimbus.java:222) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:237) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	... 4 more

Here is the log on the server side (nimbus.log):

2016-01-13 10:48:07.206 o.a.s.d.nimbus [INFO] Cleaning inbox ... deleted: stormjar-c8666220-fa19-426b-a7e4-c62dfb57f1f0.jar
2016-01-13 10:55:09.823 o.a.s.d.nimbus [INFO] Uploading file from client to /var/storm-data/nimbus/inbox/stormjar-80ecdf05-6a25-4281-8c78-10062ac5e396.jar
2016-01-13 10:55:11.910 o.a.s.d.nimbus [INFO] Finished uploading file from client: /var/storm-data/nimbus/inbox/stormjar-80ecdf05-6a25-4281-8c78-10062ac5e396.jar
2016-01-13 10:55:12.084 o.a.t.s.AbstractNonblockingServer$FrameBuffer [WARN] Exception while invoking!
org.apache.thrift7.transport.TTransportException: Frame size (17435758) larger than max length (16384000)!
	at org.apache.thrift7.transport.TFramedTransport.readFrame(TFramedTransport.java:137)
	at org.apache.thrift7.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift7.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift7.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift7.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
	at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
	at org.apache.thrift7.server.Invocation.run(Invocation.java:18)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


"
STORM-1363,TridentKafkaState should handle null values from TridentTupleToKafkaMapper.getMessageFromTuple(),"If you look at the updateState API of storm.kafka.trident.TridentKafkaState. When producer is sending data its not handling if the null value is sent by mapper.getMessageFromTuple(tuple). Results into Kafka topic gets value as ""null"" string. There might be case in particular kind of exception user do not want to replay tuple and just report it and with that he needs to return null.

Also make the members as protected as I need to copy-paste the class to provide my implementation.

My updateState API looks like this

{code}
public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
      String topic = null;
		for (TridentTuple tuple : tuples) {
			if(tuple==null) {
				continue;
			}

			Object keyFromTuple = null;
			try {
				keyFromTuple = mapper.getKeyFromTuple(tuple);
				topic = topicSelector.getTopic(tuple);
				Object messageFromTuple = mapper.getMessageFromTuple(tuple);
				if (topic != null && messageFromTuple != null) {
					producer.send(new KeyedMessage(topic, keyFromTuple, messageFromTuple));
				} else {
					LOG.warn(""skipping key = "" + keyFromTuple + "", topic selector returned null."");
				}
			} catch (Exception ex) {
				String errorMsg = ""Could not send message with key = "" + keyFromTuple + "" to topic = "" + topic;
				LOG.warn(errorMsg, ex);
				throw new FailedException(errorMsg, ex);
			}
		}
	}
{code}"
STORM-1300,port  backtype.storm.scheduler.resource-aware-scheduler-test to java,Test RAS
STORM-1188,PartialKeyGrouping missing from storm.thrift (and can't use it via custom_object),"I'm working on a Python DSL for Storm to add to streamparse, and as part of it I realized that the new partial key grouping was never added to the Grouping struct in storm.thrift, so it's not usable outside of JVM-based topology definitions (at least not easily).  My initial thought was to just use Grouping.custom_object, but the PartialKeyGrouping constructor takes a Fields object, which isn't a type defined in storm.thrift, so I can't use it.

The fields grouping explicitly takes a list of strings in storm.thrift, so it would seem PartialKeyGrouping needs to be added in the same way."
STORM-1038,Upgrade netty transport from 3.x to 4.x,It will be nice to upgrade netty to 4.x to take advantage of its more efficient memory usage.
