Bug ID,Bug Summary,Bug Description
CASSANDRA-19911,Removed MemtableSizeTest test re-appeared after a merge,"As a part of CASSANDRA-17298 [https://github.com/apache/cassandra/blob/cassandra-4.1/test/unit/org/apache/cassandra/cql3/MemtableSizeTest.java] test was removed and replaced with other tests (commit. [https://github.com/apache/cassandra/commit/c56ba3b317e67f4530db1737169f5558969bd531#diff-e7b2385cd817d698d35bdeffea2d4cb765653e2e3eb13fa22abfb070fed05a8f]) but it is still in the repository for 4.1 (looks like a merge issue).

This old test needs to be deleted."
CASSANDRA-19909,[Analytics] Add writer option COORDINATED_WRITE_CONFIG to define coordinated write to multiple Cassandra clusters,"As the first step of implementing coordinated write to multiple Cassandra clusters, this patch introduces the new writer option, COORDINATED_WRITE_CONFIG and the optional clusterId to identify clusters. The COORDINATED_WRITE_CONFIG value is a json string that defines the target clusters for the bulk write.

The coordinated write feature requires the exact same table schema (not including table properties) across clusters."
CASSANDRA-19907,Add jeetkundoug's gig key to project's KEYS file,"This patch adds my gpg public key to the KEYS file.

My gpg public key has the fingerprint 9A648E3DEDA36EECCCC374C4277B602ED2C52277
"
CASSANDRA-19906,memtable configuration is order-dependent,https://lists.apache.org/thread/yp1fyqn8lzjrc7ppmwn4v4olojjw2s0m
CASSANDRA-19905,Fix incorrect nodetool suggestion when gossip mode is running,"In trunk, when data directories were present with older system tables starting up Cassandra produces the following error:

{code:java}
NoHostAvailable: ('Unable to complete the operation against any hosts', {<Host: 127.0.0.1:9042 datacenter1>: <Error from server: code=0000 [Server error] message=""java.lang.IllegalStateException: Can't commit transformations when running in gossip mode. Enable the ClusterMetadataService with `nodetool addtocms`."">})
{code}

The correct error message should state the the command to run is {{nodetool cms initialize}} instead."
CASSANDRA-19903,Add guardrail for enabling usage of VectorType,"Add a guardrail to allow operators to toggle support for the new Vector type. This is a precaution that operators may desire when working with clusters than have a diverse set of clients, and some clients may not yet support the new vector type. Those clients may fail when fetching metadata (""unexpected column type"") and prevent session establishment, or fail to query vector columns, or otherwise behave strangely.

 

Patch is ready here: https://github.com/apache/cassandra/compare/trunk...aratno:cassandra:CASSANDRA-X-guardrail-vector-enable"
CASSANDRA-19901,[Analytics] Refactor TokenRangeMapping to use proper types instead of Strings,"Proposing the refactoring of TokenRangeMapping and the related classes to use proper types instead of String to improve maintainability. As of now, String are used to represent IP, IP with port, node name, etc. It is difficult to distinguish the actual types."
CASSANDRA-19900,Make the compression cache configurable to reduce heap pressure for large SSTables,"Normally the CompressionInfo.db file is trivial in size (a few kb) and so can be easily cached to prevent wasteful requests for the same file, but for large SSTables (running into a few hundred GB, due to schema issues or large blobs) then the {color:#000000}CompressionInfo.db files can also grow to be a few hundred MB or GB making it problematic to cache. We should make the compression cache optional and configurable so we don’t cache large files.{color}"
CASSANDRA-19896,ant artifacts creates -src.tar.gz with .build/docker/*.sh scripts not having execution flag set,"Execution flags in that directory for shell scripts are set when cloned from the repository. We should have this aligned otherwise "".build/docker/build-debian.sh"" and similar do not work on source artifacts without having a user fixing that on her own. "
CASSANDRA-19895,Update OWASP dependency checker to version 10.0.4,"Version 10.0.0 we are at stopped to work.

https://github.com/jeremylong/DependencyCheck?tab=readme-ov-file#mandatory-upgrade-notice"
CASSANDRA-19880,"With enableTracing set to true, the unset() method of a BoundStatement for a map type field failed during execution","After creating bound statement, performing UNSET on collection type (e.g. map), and enabling tracing, request fails on C* side with:
{code:java}
java.lang.IndexOutOfBoundsException: null
	at java.base/java.nio.Buffer.checkIndex(Buffer.java:693)
	at java.base/java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:406)
	at org.apache.cassandra.utils.ByteBufferUtil.toInt(ByteBufferUtil.java:476)
	at org.apache.cassandra.db.marshal.ByteBufferAccessor.toInt(ByteBufferAccessor.java:208)
	at org.apache.cassandra.db.marshal.ByteBufferAccessor.toInt(ByteBufferAccessor.java:42)
	at org.apache.cassandra.serializers.CollectionSerializer.readCollectionSize(CollectionSerializer.java:147)
	at org.apache.cassandra.cql3.CQL3Type$Collection.toCQLLiteral(CQL3Type.java:222)
	at org.apache.cassandra.transport.messages.ExecuteMessage.traceQuery(ExecuteMessage.java:223)
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:155)
	at org.apache.cassandra.transport.Message$Request.execute(Message.java:259)
	at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:416)
	at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:435)
	at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:462)
	at org.apache.cassandra.transport.Dispatcher$RequestProcessor.run(Dispatcher.java:307)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:99)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}"
CASSANDRA-19878,Filtering on clustering columns in reversed order with BETWEEN return invalid results,"As noted by [~maedhroz], filtering on clustering columns in reverser order do not work properly.

The problem can be reproduced using the following test:
{code}
@Test
public void testReverseAndBetweenMemtableFiltering()
{
    createTable(""CREATE TABLE %s(p int, c int, c2 int, abbreviation ascii, PRIMARY KEY (p, c, c2)) WITH CLUSTERING ORDER BY (c DESC, c2 DESC)"");

    execute(""INSERT INTO %s(p, c, c2, abbreviation) VALUES (0, 1, 1, 'CA')"");
    execute(""INSERT INTO %s(p, c, c2, abbreviation) VALUES (0, 2, 2, 'MA')"");
    execute(""INSERT INTO %s(p, c, c2, abbreviation) VALUES (0, 3, 3, 'MA')"");
    execute(""INSERT INTO %s(p, c, c2, abbreviation) VALUES (0, 4, 4, 'TX')"");

    ResultSet betweenRowsNet = executeNet(""SELECT * FROM %s WHERE c2 BETWEEN 2 AND 3 ALLOW FILTERING"");
    assertRowsNet(betweenRowsNet, row (0, 3, 3, ""MA""), row (0, 2, 2, ""MA"")); // NO RESULTS?
}
{code}

The issue is due to the fact that the comparison performed in {{BETWEEN.isSatisfied}} ignore the {{ReversedType}}."
CASSANDRA-19873,[Analytics] Removes checks for blocked instances from bulk-write path,"The analytics bulk writer currently performs checks for blocked instances for consistency-level validations prior-to and after the bulk-write. It also takes all the blocked nodes into account for these validations instead of the nodes in the specific range being written (addressed separately under https://issues.apache.org/jira/browse/CASSANDRA-19842).

 

This change removes the notion of blocked instances for bulk-writes, treating such nodes as available, as the intended usage of ""blocked"" nodes is to operationally prevent client CQL connections going into the node, but not writes."
CASSANDRA-19872,Handle losing CMS status while committing a transformation,There is a small window where we might try to use the wrong processor when committing a transformation while the CMS changes - in this case we should just retry the commit and we'll use the RemoteProcessor
CASSANDRA-19871,Add size to the segment index for safer journal reads,
CASSANDRA-19867,Fix a problem with static segments being opened with an empty offset after switch unless active segments offset file was closed,Buffer backing the writer for segment offsets is not flushed to disk before being fsynced
CASSANDRA-19866,Fix Journal segment allocation/switch race condition,"Concurrent r/w workload is currently throwing:
{code}
java.lang.IllegalArgumentException: Can not reference segment 1724695101990
        at org.apache.cassandra.journal.Segments.isFlushed(Segments.java:189)
        at org.apache.cassandra.journal.Journal.isFlushed(Journal.java:199)
        at org.apache.cassandra.journal.Journal$FlusherCallbacks.submit(Journal.java:151)
        at org.apache.cassandra.journal.Journal.onFlush(Journal.java:204)
        at org.apache.cassandra.service.accord.AccordJournal.appendCommand(AccordJournal.java:264)
        at org.apache.cassandra.service.accord.AccordCommandStore.appendCommands(AccordCommandStore.java:579)
        at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:277)
        at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:303)
        at org.apache.cassandra.service.accord.async.AsyncOperation.onLoaded(AsyncOperation.java:169)
        at accord.utils.async.AsyncCallbacks.lambda$inExecutorService$0(AsyncCallbacks.java:36)
        at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}"
CASSANDRA-19865,"Fix CompacctionAccordIteratorTest, switch Journal to streaming serialization",
CASSANDRA-19864,Switch to infinite loop executor instead of a while-loop thread,DelayedRequestProcessor was causing simulation failures because it was running using raw threads rather than an executor.
CASSANDRA-19857,CommandsForRanges does not support slice which cause over returned data being sent,"This is split from CASSANDRA-19769

CommandsForRanges returns the data for the whole node but is processed per shard (which has a subset of data). Normally we use “.slice” to shrink the results to match the shard, but this is missing in CommandsForRanges, which cause it to return txn and ranges not present for the shard"
CASSANDRA-19856,Add a concept for retrying messages,"This is split from CASSANDRA-19769

Repair and TCM both have their own retry logic for messages, which makes it harder for new usages; we should refactor so there is a simple way to retry messages that covers both users."
CASSANDRA-19855,txns that update a static row when the desired row doesn't exist leads to an error,"This is split from CASSANDRA-19769

If the table has a static row and the txn does += on a non-static column and = on a static column, then accord fails"
CASSANDRA-19854,Make JIRA ticket names in commit messages to be links ,
CASSANDRA-19849,Test Failure: org.apache.cassandra.tcm.sequences.ProgressBarrierTest.testProgressBarrier,"Seen on current trunk here:
https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2726/workflows/2d5c888b-d5b8-4f72-9fcb-9e7ae0887940/jobs/61321/tests
{code:java}
junit.framework.AssertionFailedError: Should have collected at least 15 nodes but got 14.
RF: NtsReplicationFactor{map={datacenter1=5, datacenter2=5, datacenter3=5}}
Replicas: [/127.0.0.1:7012, /127.0.0.2:7012, /127.0.0.3:7012, /127.0.0.4:7012, /127.0.0.5:7012, /127.0.0.6:7012, /127.0.0.7:7012, /127.0.0.8:7012, /127.0.0.9:7012, /127.0.0.10:7012, /127.0.0.11:7012, /127.0.0.12:7012, /127.0.0.13:7012, /127.0.0.14:7012]
Nodes: [/127.0.0.1:7012, /127.0.0.2:7012, /127.0.0.3:7012, /127.0.0.4:7012, /127.0.0.5:7012, /127.0.0.6:7012, /127.0.0.7:7012, /127.0.0.8:7012, /127.0.0.9:7012, /127.0.0.10:7012, /127.0.0.11:7012, /127.0.0.12:7012, /127.0.0.13:7012, /127.0.0.14:7012]
	at org.apache.cassandra.tcm.sequences.ProgressBarrierTest.testProgressBarrier(ProgressBarrierTest.java:176)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
"
CASSANDRA-19847,"Create a fuzz test that randomizes topology changes, cluster actions, and CQL operations","This is a spinoff from CASSANDRA-19769 to lower the patch size

To help validate the cluster state it is good to put a cluster into random situations to make sure it still works correctly.  CASSANDRA-19769 Added such a test and found several issues with TCM (all fixed) and can be used to help find more issues as we expand the scope of what it can touch.

I propose we have a test that randomizes changes to the topology, performs cluster operations such as repair, and performs CQL operations (for this patch using Harry)"
CASSANDRA-19846,Allow looking up local tables in nodetool getendpoints,"In versions before 5.1 we allow looking up the endpoints for local tables using {{nodetool getendpoints}} - we only return the local node, but some scripts might depend on this so we should support it in 5.1"
CASSANDRA-19845,Handle existing tables with non-expected table ids,"On upgrade we merge the existing tables with the pre-defined ones, but we only check if the tableid exists, we should also check the name of the table since the id can be defined by the user."
CASSANDRA-19842,[Analytics] Consistency level check incorrectly passes when majority of the replica set is unavailable for write,"Consistency level check is performed before proceeding to bulk writing data. The check yields wrong results that when the majority of a replica set is unavailable, it still passes. Leading to writing data to replicas that cannot satisfy the desired consistency level. 

The following is the test to prove the bug. The test sets all 3 instances in the replica set as blocked (unavailable), so the validation is expected to throw. But it does not. 

{code:java}
@Test
void test()
{
    BulkWriterContext mockWriterContext = mock(BulkWriterContext.class);
    ClusterInfo mockClusterInfo = mock(ClusterInfo.class);
    when(mockWriterContext.cluster()).thenReturn(mockClusterInfo);

    CassandraContext mockCassandraContext = mock(CassandraContext.class);
    when(mockClusterInfo.getCassandraContext()).thenReturn(mockCassandraContext);
    Map<String, String> replicationOptions = new HashMap<>();
    replicationOptions.put(""class"", ""SimpleStrategy"");
    replicationOptions.put(""replication_factor"", ""3"");
    TokenRangeMapping<RingInstance> topology = CassandraClusterInfo.getTokenRangeReplicas(() -> mockSimpleTokenRangeReplicasResponse(10, 3),
                                                                                          () -> Partitioner.Murmur3Partitioner,
                                                                                          () -> new ReplicationFactor(replicationOptions),
                                                                                          ringInstance -> {
                                                                                              int nodeId = Integer.parseInt(ringInstance.ipAddress().replace(""localhost"", """"));
                                                                                              return nodeId <= 2; // block nodes 0, 1, 2
                                                                                          });
    when(mockClusterInfo.getTokenRangeMapping(anyBoolean())).thenReturn(topology);

    JobInfo mockJobInfo = mock(JobInfo.class);
    UUID jobId = UUID.randomUUID();
    when(mockJobInfo.getId()).thenReturn(jobId.toString());
    when(mockJobInfo.getRestoreJobId()).thenReturn(jobId);
    when(mockJobInfo.qualifiedTableName()).thenReturn(new QualifiedTableName(""testkeyspace"", ""testtable""));
    when(mockJobInfo.getConsistencyLevel()).thenReturn(ConsistencyLevel.CL.QUORUM);
    when(mockJobInfo.effectiveSidecarPort()).thenReturn(9043);
    when(mockJobInfo.jobKeepAliveMinutes()).thenReturn(-1);
    when(mockWriterContext.job()).thenReturn(mockJobInfo);

    BulkWriteValidator writerValidator = new BulkWriteValidator(mockWriterContext, new ReplicaAwareFailureHandler<>(Partitioner.Murmur3Partitioner));
    assertThatThrownBy(() -> writerValidator.validateClOrFail(topology))
    .isExactlyInstanceOf(RuntimeException.class)
    .hasMessageContaining(""Failed to load"");
}
{code}
"
CASSANDRA-19841,Memtable configs don't have defaults for each format supported,"We added support to change memtable and sstables via yaml but it seems we only added defaults for sstables and missed them for memtables.  We document recommended defaults in config/cassandra.yaml but that isn’t present in Config nor is it present in the resolution logic that translates the config to our maps, this means that anyone who defines their own yaml won’t have “skiplist”, “trie”, or “shardedskiplist” as default options to pick from and must know how to define them.

To make it easier for users we should provide defaults that could be overridden by users, just like we did for SSTables"
CASSANDRA-19839,Update Cassandra home page to include ASF events perma-link,"In a Slack conversation, Rich Bowen let us know that there is an ASF perma-link for ASF events. Adding this to the home page would let us promote the latest events without having to constantly update the web site.

 

[https://www.apachecon.com/event-images/]

 

Will also remove the Orbit link since that is no longer valid.

 

 "
CASSANDRA-19838,Add a table to inspect the current state of a txn,"When a txn is blocked its useful to figure out what is blocking it, so should expose a table to show this detail."
CASSANDRA-19836,[Analytics] Fix NPE when writing UDT values,"When UDT field values are set to null, the bulk writer throws NPE, e.g. the stacktrace below. Although it is on the boolean type, the NPE can be thrown on all other types whenever the value is null.

{code:java}
Caused by: java.lang.NullPointerException
  at org.apache.cassandra.spark.data.types.Boolean.setInnerValue(Boolean.java:91)
  at org.apache.cassandra.spark.data.complex.CqlUdt.setInnerValue(CqlUdt.java:534)
  at org.apache.cassandra.spark.data.complex.CqlUdt.toUserTypeValue(CqlUdt.java:522)
  at org.apache.cassandra.spark.data.complex.CqlUdt.convertForCqlWriter(CqlUdt.java:169)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.maybeConvertUdt(RecordWriter.java:450)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.getBindValuesForColumns(RecordWriter.java:432)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.writeRow(RecordWriter.java:415)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.write(RecordWriter.java:202)
{code}

"
CASSANDRA-19835,Memtable allocation type unslabbed_heap_buffers_logged will cause an assertion error for TrieMemtables and SegmentedTrieMemtables,"Config used

{code}
	---
	partitioner: Murmur3Partitioner
	commitlog_sync: periodic
	commitlog_sync_period: 9000ms
	commitlog_disk_access_mode: legacy
	memtable_allocation_type: unslabbed_heap_buffers_logged
	sstable:
	  selected_format: big
	disk_access_mode: standard
{code}

Error

{code}
Caused by: java.lang.AssertionError: null
	at org.apache.cassandra.config.Config$MemtableAllocationType.toBufferType(Config.java:1206)
	at org.apache.cassandra.index.sai.disk.v1.segment.SegmentTrieBuffer.<init>(SegmentTrieBuffer.java:48)
	at org.apache.cassandra.index.sai.disk.v1.segment.SegmentBuilder$TrieSegmentBuilder.<init>(SegmentBuilder.java:83)
	at org.apache.cassandra.index.sai.disk.v1.SSTableIndexWriter.newSegmentBuilder(SSTableIndexWriter.java:311)
	at org.apache.cassandra.index.sai.disk.v1.SSTableIndexWriter.addTerm(SSTableIndexWriter.java:195)
	at org.apache.cassandra.index.sai.disk.v1.SSTableIndexWriter.addRow(SSTableIndexWriter.java:99)
	at org.apache.cassandra.index.sai.disk.StorageAttachedIndexWriter.addRow(StorageAttachedIndexWriter.java:257)
	at org.apache.cassandra.index.sai.disk.StorageAttachedIndexWriter.nextUnfilteredCluster(StorageAttachedIndexWriter.java:131)
{code}

This was found by CASSANDRA-19833"
CASSANDRA-19833,"Improve CQLTester to make it trivial to run the tests with different configs, and to add randomness to the test","When creating a test many authors default to hard coding values as the barrier to make the test generic feels too high; to lower this barrier we should add an extension class to CQLTester that enables randomized testing without having to worry about which framework to use.

This should setup the single node cluster with different configs to improve coverage.

Here is a sample error message when a test fails

{code}
java.lang.AssertionError: Property error detected:
Config Seed: -3611771839852432544 -- To rerun do -Dcassandra.test.cqltester.fuzzed.seed.org.apache.cassandra.cql3.validation.operations.InsertTest=-3611771839852432544
Config:
	---
	partitioner: OrderPreservingPartitioner
	commitlog_sync: batch
	commitlog_disk_access_mode: mmap_index_only
	memtable_allocation_type: offheap_buffers
	sstable:
	  selected_format: bti
	disk_access_mode: mmap
	
Error:
	commitlog_disk_access_mode = mmap_index_only is not supported. Please use 'auto' when unsure.
{code}

And if the test fails

{code}
java.lang.AssertionError: Property error detected:
Config Seed: -5023694648830703272 -- To rerun do -Dcassandra.test.cqltester.fuzzed.seed.org.apache.cassandra.cql3.validation.operations.InsertTest=-5023694648830703272
Seed: -1088015559108807046 -- To rerun do -Dcassandra.test.cqltester.fuzzed.seed.org.apache.cassandra.cql3.validation.operations.InsertTest.testInsertZeroDuration=-1088015559108807046
Config:
	---
	partitioner: OrderPreservingPartitioner
	commitlog_sync: periodic
	commitlog_sync_period: 10000ms
	commitlog_disk_access_mode: direct
	memtable_allocation_type: offheap_objects
	sstable:
	  selected_format: bti
	disk_access_mode: mmap_index_only
Caused by: java.lang.NullPointerException
{code}"
CASSANDRA-19830,Use default commitlog settings in test YAMLs,"Since nearly the beginning of the project, we've used {{commitlog_sync: batch}} in our test configurations. However, the default is periodic/10s. At a very high level, it seems like we should test the default configuration, and that might be enough of a reason to change this. In addition, we might get a bit of a reduction in testing duration out of the change, especially on older hardware."
CASSANDRA-19827,[Analytics] Add job_timeout_seconds writer option,"Option to specify the timeout in seconds for bulk write jobs. By default, it is disabled.
When JOB_TIMEOUT_SECONDS is specified, a job exceeding the timeout is:
- successful when the desired consistency level is met
- a failure otherwise"
CASSANDRA-19823,Investigate timeouts in tests extending RandomIntersectionTester,
CASSANDRA-19822,Family of org.apache.cassandra.repair tests fail on accord.utils.Property$PropertyError: Property error detected,"I noticed this started to fail recently in trunk.

{code}
  ✕ j17_utests_oa                                   15m 15s
      org.apache.cassandra.repair.FailedAckTest failedAck
      org.apache.cassandra.repair.FailingRepairFuzzTest failingRepair
      org.apache.cassandra.repair.HappyPathFuzzTest happyPath
      org.apache.cassandra.repair.SlowMessageFuzzTest slowMessages
      org.apache.cassandra.repair.ConcurrentIrWithPreviewFuzzTest concurrentIrWithPreview
{code}

{code}
accord.utils.Property$PropertyError: Property error detected:
Seed = 3839006609844455027
Examples = 10
Pure = false
Error: Rejecting access
Values:
	0 = accord.utils.DefaultRandom@1b64dba7


	at accord.utils.Property$SingleBuilder.checkInternal(Property.java:242)
	at accord.utils.Property$SingleBuilder.check(Property.java:226)
	at accord.utils.Property$ForBuilder.check(Property.java:124)
	at org.apache.cassandra.repair.FailedAckTest.failedAck(FailedAckTest.java:55)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
Caused by: java.lang.IllegalStateException: Rejecting access
	at org.apache.cassandra.repair.FuzzTestBase$ClockAccess.checkAccess(FuzzTestBase.java:1465)
	at org.apache.cassandra.repair.FuzzTestBase$ClockAccess.nanoTime(FuzzTestBase.java:1404)
	at org.apache.cassandra.utils.Clock$Global.nanoTime(Clock.java:100)
	at org.apache.cassandra.repair.consistent.LocalSessions.start(LocalSessions.java:357)
	at org.apache.cassandra.service.ActiveRepairService.start(ActiveRepairService.java:274)
	at org.apache.cassandra.repair.FuzzTestBase$Cluster$Node.<init>(FuzzTestBase.java:1121)
	at org.apache.cassandra.repair.FuzzTestBase$Cluster.<init>(FuzzTestBase.java:741)
	at org.apache.cassandra.repair.FailedAckTest.lambda$failedAck$2(FailedAckTest.java:56)
	at accord.utils.Property$SingleBuilder.checkInternal(Property.java:238)
	... 33 more
{code}"
CASSANDRA-19821,Prevent double closing SSTable writer,"Analytics uses `org.apache.cassandra.io.sstable.SSTableSimpleWriter` to produce SSTables. Its implementation allows to be closed multiple times. However, the subsequent calls to ""close"" cause exception. For example,


{code:java}
java.lang.RuntimeException: Last written key DecoratedKey(-3078932293011064831, 000022fd) >= current key DecoratedKey(-3078932293011064831, 000022fd) writing into nb-1-big-Data.db
    	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.beforeAppend(BigTableWriter.java:169)
    	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:208)
    	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
    	at org.apache.cassandra.io.sstable.SSTableTxnWriter.append(SSTableTxnWriter.java:57)
    	at org.apache.cassandra.io.sstable.SSTableSimpleWriter.writePartition(SSTableSimpleWriter.java:152)
    	at org.apache.cassandra.io.sstable.SSTableSimpleWriter.writeLastPartitionUpdate(SSTableSimpleWriter.java:125)
    	at org.apache.cassandra.io.sstable.SSTableSimpleWriter.close(SSTableSimpleWriter.java:93)
    	at org.apache.cassandra.io.sstable.CQLSSTableWriter.close(CQLSSTableWriter.java:337)
{code}


Cassandra analytics should prevent double closing the underlying writer."
CASSANDRA-19820,Fix tests extending FuzzTestBase when running test-compression profile,"FuzzTestBase extends CQLTester.InMemory which uses jimfs filesystem instead of the real one. This does not play well with direct mode of commitlog_disk_access_mode. We fixed  this in CASSANDRA-19779 by always setting it to mmap. This does not work when we run tests in test-compression profile because then it expects ""standard"" mode and if fails on asserts.

The solution to fix test-compression profile is to set commitlog_disk_access_mode to mmap only in case it was indeed resolved to direct and not every time."
CASSANDRA-19819,Cassandra 4.1.5 on RedHat repo is no longer available and only version 4.1~alpha1-1 is available,"It looks like some configuration of the rpm repository for Cassandra 41x has changed and means only version 4.1~alpha1-1 is available.

We're trying to build a Docker image using Cassandra 4.1.5 version and a few days back the build was working as expected. 2-3 days ago the build is now failing and it no longer works. Here's the error message:
{code:java}
No package cassandra-4.1.5-1 available. {code}
Inspecting the repository [https://apache.jfrog.io/ui/native/cassandra-rpm/41x/] I can see that version 4.1.5 is listed, but it is not returned when executing yum search / yum list:
{code:java}
$ yum list cassandra --showduplicates
Available Packages
cassandra.noarch 4.1~alpha1-1
cassandra cassandra.src 4.1~alpha1-1 cassandra{code}
A series of files were modified on 2024-08-03 (five days ago) in the directory [https://apache.jfrog.io/ui/native/cassandra-rpm/41x/repodata] - could this be the change that caused this behaviour? We are now unable to install any version of Cassandra 4.1.x except the alpha release.
{code:java}
 $ cat /etc/yum.repos.d/cassandra.repo
[cassandra]
baseurl = https://redhat.cassandra.apache.org/41x/ 
enabled = 1 
gpgcheck = 1 
gpgkey = https://downloads.apache.org/cassandra/KEYS 
name = Apache Cassandra
repo_gpgcheck = 1
{code}"
CASSANDRA-19818,Minor improvements in Cassandra shutdown and startup logs,"To improve a DBA experience the following log messages would be nice to add/adjust:
 * on shutdown: an explicit message at the end of Cassandra shutdown
 * on startup:
 ** print the time spent to load prepared statements
 ** print the time spent to load repair session information and the number of loaded session records
 ** print the time spent to apply commit log

It would help with assessment of possible delays in startup/shutdown of Cassandra. For example, recently I observed a delay in Cassandra startup and from logs it was not clear was it caused by loading of prepared statements or repair service init."
CASSANDRA-19816,nodetoolResult failed in Cassandra Dtest before cluster upgrade but succeed after cluster upgrade,"*What happened*
Cassandra `nodetoolResult` behaves differently in dtest `runbeforeClusterUpgrade` and `runAfterClusterUpgrade`.
 

*How to reproduce:*
Put the following test under `cassandra/test/distributed/org/apache/cassandra/distributed/upgrade/`, and build dtest jars for version `5.0-beta1` and `5.1`.
{code:java}
package org.apache.cassandra.distributed.upgrade;

public class demoUpgradeTest extends UpgradeTestBase {
    @Test
    public void firstDemoTest() throws Throwable {
        new TestCase()
        .nodes(2)
        .nodesToUpgrade(1)
        .withConfig((cfg) -> cfg.with(Feature.NETWORK, Feature.GOSSIP))
        .upgradesToCurrentFrom(v3X)
        .setup((cluster) -> {
            cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + "".tbl (pk int, ck int, PRIMARY KEY (pk, ck))"");
        })
        .runbeforeClusterUpgrade((cluster) -> {
	        cluster.get(1).nodetoolResult(""cms"", ""initialize"").asserts().success();
        }).run();
    }

    @Test
    public void secondDemoTest() throws Throwable {
        new TestCase()
        .nodes(2)
        .nodesToUpgrade(1)
        .withConfig((cfg) -> cfg.with(Feature.NETWORK, Feature.GOSSIP))
        .upgradesToCurrentFrom(v3X)
        .setup((cluster) -> {
            cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + "".tbl (pk int, ck int, PRIMARY KEY (pk, ck))"");
        })
        .runafterClusterUpgrade((cluster) -> {
            cluster.get(1).nodetoolResult(""cms"", ""initialize"").asserts().success();
        }).run();
    }
}{code}

Run the test with:
{code:java}
$ ant test-jvm-dtest-some -Duse.jdk11=true -Dtest.name=org.apache.cassandra.distributed.upgrade.demoUpgradeTest{code}
 
`secondDemoTest` passes, but `firstDemoTest` fails with the following output:
 
{code:java}
java.lang.AssertionError: Error in test '5.0-beta2 -> [5.1]' while upgrading to '5.1'; successful upgrades []
 
at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:396)
at org.apache.cassandra.distributed.upgrade.demoUpgradeTest.secondDemoTest(demoUpgradeTest.java:56)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        ...
 
Caused by: java.lang.AssertionError: nodetool command [cms, initialize] was not successful
stdout:
nodetool: Found unexpected parameters: [cms, initialize]
See 'nodetool help' or 'nodetool help <command>'.
 
stderr:
 
Notifications:
Error:
io.airlift.airline.ParseArgumentsUnexpectedException: Found unexpected parameters: [cms, initialize]
at io.airlift.airline.Cli.validate(Cli.java:194)
at io.airlift.airline.Cli.parse(Cli.java:132)
        ...{code}
 
NodetoolResult command cannot recognize its parameters before the upgrade. When they executes on different versions of node before and after the upgrade, they call the same method on the delegate node with different versions. However, before the upgrade, the nodetoolResult command always fails to execute.
 
Here is the configuration property of the delegate nodes for the above two test cases. Both configurations are very similar except for `version` and `generation`:
{code:java}
this = {AbstractCluster$Wrapper@15093} ""node1""
 config = {InstanceConfig@15098} ""...""
 delegate = {Instance@15099} ""node1""
 version = {Versions$Version@15100} 
  version = {Semver@15118} ""5.0-beta1""
  classpath = {URL[1]@15119} 
 isShutdown = false
 broadcastAddress = {InetSocketAddress@15101} ""/127.0.0.1:7012""
 generation = 0
 this$0 = {UpgradeableCluster@15102} 
withNotifications = true
commandAndArgs = {String[2]@15094} [""cms"", ""initialize""]{code}
 
{code:java}
this = {AbstractCluster$Wrapper@28321} ""node1""
 config = {InstanceConfig@28326} ""...""
 delegate = {Instance@28327} ""node1""
 version = {Versions$Version@28328} 
  version = {Semver@28343} ""5.1""
  classpath = {URL[1]@28344} 
 isShutdown = false
 broadcastAddress = {InetSocketAddress@28329} ""/127.0.0.1:7012""
 generation = 1
 this$0 = {UpgradeableCluster@28330} 
withNotifications = true
commandAndArgs = {String[2]@28322} [""cms"", ""initialize""]{code}
 
And the code in `nodetoolResult` only have some minor difference in `SecurityManager` for version [`5.0-beta1`]([https://github.com/apache/cassandra/blob/87fd1fa88a0c859cc32d9f569ad09ad0b345e465/test/distributed/org/apache/cassandra/distributed/impl/Instance.java#L987]) and [`5.1`]([https://github.com/apache/cassandra/blob/9679206f7443328b8688e35f6f09ce284d4bfe21/test/distributed/org/apache/cassandra/distributed/impl/Instance.java#L1050):]

 
{code:java}
// install security manager to get informed about the exit-code
System.setSecurityManager(new SecurityManager()
{
    public void checkExit(int status)
    {
        throw new SystemExitException(status);
    }
    public void checkPermission(Permission perm)
    {
    }
    public void checkPermission(Permission perm, Object context)
    {
    }
});
{code}
 
{code:java}
SecurityManager before = System.getSecurityManager();
// install security manager to get informed about the exit-code
ClusterUtils.preventSystemExit();{code}
The expected behavior should be that `nodetoolResult` behaves consistently before and after the upgrade.
 "
CASSANDRA-19815,[Analytics] Decouple Cassandra types from Spark types so Cassandra types can be used independently from Spark,"The Cassandra types and Spark types are tightly coupled in the same classes, making it difficult to deserialize Cassandra types without pulling in Spark as a dependency, We can split out the Spark types into a separate module by introducing a new TypeConverter that maps Cassandra types to Spark types. This enables use of the Cassandra types without pulling in Spark and also opens the possibility of other TypeConverters in the future beyond Spark."
CASSANDRA-19813,timeout on  upgrade_tests/upgrade_through_versions_test.py::TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD::test_parallel_upgrade," The upgrade_tests/upgrade_through_versions_test.py::TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD::test_parallel_upgrade  test is taking longer than usual.

CI in 5.0 is timing out the dtest-upgrade-large 59/64 split always now.

Our 5.0-rc testing results are tainted because these time outs always abort the 5.0 pipeline runs.

It looks like that split is taking 16x times as long now… 

This appears to be caused from either/both
- https://github.com/apache/cassandra/commit/08e1fecf36507397cf3122d77f84aa23150da588
- https://github.com/apache/cassandra-dtest/commit/2b17c1293056068bb3e94c332d6fb99df6a0b0fa


Example of good run.
 [^test_rolling_upgrade.257.good.log] (ci-cassandra.a.o)

Examples of bad runs.
 [^test_rolling_upgrade.122.log],  [^test_rolling_upgrade.123-2.log],  [^test_rolling_upgrade.123-1.log],  [^test_rolling_upgrade.134.log]  (internal ci)
 [^test_rolling_upgrade.261-1.log] ,  [^test_rolling_upgrade.261-2.log]  (ci-cassandra.a.o)
"
CASSANDRA-19812,We should throw exception when commitlog 's DiskAccessMode is direct but direct io is not support,"Looking into the code below : 
{code:java}
private static DiskAccessMode resolveCommitLogWriteDiskAccessMode(DiskAccessMode providedDiskAccessMode)
    {
        boolean compressOrEncrypt = getCommitLogCompression() != null || (getEncryptionContext() != null && getEncryptionContext().isEnabled());
        boolean directIOSupported = false;
        try
        {
            directIOSupported = FileUtils.getBlockSize(new File(getCommitLogLocation())) > 0;
        }
        catch (RuntimeException e)
        {
            logger.warn(""Unable to determine block size for commit log directory: {}"", e.getMessage());
        }

        if (providedDiskAccessMode == DiskAccessMode.auto)
        {
            if (compressOrEncrypt)
                providedDiskAccessMode = DiskAccessMode.legacy;
            else
            {
                providedDiskAccessMode = directIOSupported && conf.disk_optimization_strategy == Config.DiskOptimizationStrategy.ssd ? DiskAccessMode.direct
                                                                                                                                     : DiskAccessMode.legacy;
            }
        }

        if (providedDiskAccessMode == DiskAccessMode.legacy)
        {
            providedDiskAccessMode = compressOrEncrypt ? DiskAccessMode.standard : DiskAccessMode.mmap;
        }

        return providedDiskAccessMode;
    }
{code}

We should throw exception when user set the DiskAccessMode to direct for commitlog but the directIOSupported return false after the judgement of ""FileUtils.getBlockSize(new File(getCommitLogLocation())) > 0;"" instead of waiting for the system to start and accepting reads and writes.
"
CASSANDRA-19809,Deprecate and ignore use_deterministic_table_id,"With transactional metadata finally in place in 5.x, we have no further need for {{{}use_deterministic_table_id{}}}. I propose we...

 
1.) Leave CQL {{WITH id}} intact.
2.) Deprecate and WARN on {{use_deterministic_table_id}} ** in 5.0.x.
3.) Ignore and WARN on {{use_deterministic_table_id}} ** in 5.1.
4.) In some future major release, remove {{use_deterministic_table_id}} from {{Config}}"
CASSANDRA-19808,Gossiper (micro) optmizations,"I am wondering about optimizations that can be done in the gossiper class, especially targeting performance at larger scales. 
 # getLiveMembers: Creating a hashmap every time this is called. Aside from creating some trash, its linear in terms of the number of peers.
 # getMaxEndpointStateVersion: I feel this can be calculated and cached rather than looking for it. It is also linear on the number of peers.
 # getGossipStatus: Linear in the number of peers, also source of garbage at larger scales.

It seems optmizing these methods can contribute to a cleaner/performant membership protocol. Is there any plans on having these types of optmizations?"
CASSANDRA-19807,[Analytics] Improve the core bulk reader test system to match actual and expected rows by concatenating the partition keys with the serialized hex string instead of utf-8 string,"The current test system for the bulk reader matches actual and expected rows by building a utf-8 string of the concatenated partition key(s), it would be better to match on the hex string of the serialized bytes to avoid the current custom string builder implementation."
CASSANDRA-19806,[Analytics] Stream sstable eagerly when bulk writing to allow reclaiming local disk space,"Currently, each bulk write executor only sends sstables after exhausting the input data (of the task). All produced sstables are staged locally, when executor local disk space is limited or the input data size is too large, there is a risk of running out of disk space.
The patch changes the streaming strategy to stream eagerly and remove the local files sooner."
CASSANDRA-19804,Flakey test upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD#test_rolling_upgrade,"{code}
==================================== ERRORS ====================================
_ ERROR at teardown of TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD.test_rolling_upgrade _
Unexpected error found in node logs (see stdout for full details). Errors: [[node3] 'ERROR [InternalResponseStage:3] 2024-07-26 04:35:12,345 MessagingService.java:509 - Cannot send the message (from:/127.0.0.3:7000, type:FETCH_LOG verb:TCM_FETCH_PEER_LOG_REQ) to /127.0.0.1:7000, as messaging service is shutting down', [node3] 'ERROR [InternalResponseStage:4] 2024-07-26 04:35:27,412 MessagingService.java:509 - Cannot send the message (from:/127.0.0.3:7000, type:FETCH_LOG verb:TCM_FETCH_PEER_LOG_REQ) to /127.0.0.1:7000, as messaging service is shutting down']
{code}"
CASSANDRA-19803,Flakey test org.apache.cassandra.distributed.test.TransientRangeMovement2Test#testMoveForward,"{code}
 junit.framework.AssertionFailedError: SHOULD NOT BE ON NODE: 11 -- [(16,30)]: [00, 02, 04, 06, 08, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 42, 44, 46, 48]
	at org.apache.cassandra.distributed.test.TransientRangeMovementTest.assertAllContained(TransientRangeMovementTest.java:231)
	at org.apache.cassandra.distributed.test.TransientRangeMovement2Test.testMoveForward(TransientRangeMovement2Test.java:143)
{code}"
CASSANDRA-19802,Harry Simulation test halts the JVM when errors are detected which loose all history in CI,"The Harry simulation test has a few issues, but the biggest is it calls System.exit when an error is found… this blocks the JVM from creating a test report and can loose all history needed to reproduce the issue..

1) when the test fails it doesn’t properly show the seed
2) you can’t rerun the test with the same seed without modifying the code
3) if uncaught exceptions are found the test still passes (this is masking TCM issues)"
CASSANDRA-19799,Add .asf.yml and GH to ML connections,Ensure .asf.yml is setup per project practices.
CASSANDRA-19798,Update Drivers subproject internal docs,"Add the driver to the Drivers subprojects wiki page.
"
CASSANDRA-19794,NPE on Directory access during Memtable flush fails ShortPaxosSimulationTest,"Run {{ShortPaxosSimulationTest}} w/ the following arguments on trunk:

{noformat}
PaxosSimulationRunner.main(new String[] { ""run"", ""-n"", ""3..6"", ""-t"", ""1000"", ""-c"", ""2"", ""--cluster-action-limit"", ""2"", ""-s"", ""30"", ""--seed"", ""0xe0247e19a75e3bba"" });
{noformat}

You should see a failure, starting with...

{noformat}
[junit-timeout] WARN  [OptionalTasks:1] node5 2024-07-22 15:46:00,210 LegacyStateListener.java:158 - Token -6148914691236517205 changing ownership from /127.0.0.1:7012 to /127.0.0.6:7012
[junit-timeout] WARN  [OptionalTasks:1] node6 2024-07-22 15:46:00,259 SystemKeyspace.java:1287 - Using stored Gossip Generation 1577894856 as it is greater than current system time 1577894855.  See CASSANDRA-3654 if you experience problems
[junit-timeout] WARN  [OptionalTasks:1] node6 2024-07-22 15:46:00,277 LegacyStateListener.java:158 - Token -6148914691236517205 changing ownership from /127.0.0.1:7012 to /127.0.0.6:7012
[junit-timeout] ERROR [isolatedExecutor:3] node6 2024-07-22 15:46:00,469 ReconfigureCMS.java:184 - Could not finish adding the node to the Cluster Metadata Service
[junit-timeout] java.lang.IllegalStateException: Can not commit transformation: ""SERVER_ERROR""(class java.lang.NullPointerException).
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.lambda$commit$6(ClusterMetadataService.java:491)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:535)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:488)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.ReconfigureCMS.executeNext(ReconfigureCMS.java:179)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.InProgressSequences.resume(InProgressSequences.java:200)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.InProgressSequences.finishInProgressSequences(InProgressSequences.java:72)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.reconfigureCMS(ClusterMetadataService.java:372)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadataService.ensureCMSPlacement(ClusterMetadataService.java:379)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.BootstrapAndReplace.executeNext(BootstrapAndReplace.java:274)
[junit-timeout] 	at org.apache.cassandra.simulator.cluster.OnClusterReplace$ExecuteNextStep.lambda$new$f5e64c00$1(OnClusterReplace.java:162)
[junit-timeout] 	at org.apache.cassandra.distributed.api.IInvokableInstance.unsafeRunOnThisThread(IInvokableInstance.java:85)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.SimulatedActionTask.lambda$asSafeRunnable$0(SimulatedActionTask.java:83)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.SimulatedActionTask$1.run(SimulatedActionTask.java:93)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:318)
[junit-timeout] 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout] 	at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}

...and underneath that...

{noformat}
[junit-timeout] Thread[ScheduledTasks:1,5,node3]
[junit-timeout] java.lang.NullPointerException
[junit-timeout] 	at org.apache.cassandra.utils.btree.AbstractBTreeMap.get(AbstractBTreeMap.java:92)
[junit-timeout] 	at org.apache.cassandra.tcm.membership.Directory.endpoint(Directory.java:312)
[junit-timeout] 	at org.apache.cassandra.tcm.transformations.cms.AdvanceCMSReconfiguration.executeRemove(AdvanceCMSReconfiguration.java:242)
[junit-timeout] 	at org.apache.cassandra.tcm.transformations.cms.AdvanceCMSReconfiguration.execute(AdvanceCMSReconfiguration.java:123)
[junit-timeout] 	at org.apache.cassandra.tcm.sequences.ReconfigureCMS.applyTo(ReconfigureCMS.java:149)
[junit-timeout] 	at org.apache.cassandra.tcm.ClusterMetadata.writePlacementAllSettled(ClusterMetadata.java:275)
[junit-timeout] 	at org.apache.cassandra.db.DiskBoundaryManager.getLocalRanges(DiskBoundaryManager.java:158)
[junit-timeout] 	at org.apache.cassandra.db.DiskBoundaryManager.getDiskBoundaryValue(DiskBoundaryManager.java:121)
[junit-timeout] 	at org.apache.cassandra.db.DiskBoundaryManager.getDiskBoundaries(DiskBoundaryManager.java:65)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.getDiskBoundaries(ColumnFamilyStore.java:3676)
[junit-timeout] 	at org.apache.cassandra.db.compaction.CompactionStrategyManager.maybeReloadDiskBoundaries(CompactionStrategyManager.java:587)
[junit-timeout] 	at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:899)
[junit-timeout] 	at org.apache.cassandra.db.lifecycle.Tracker.notify(Tracker.java:558)
[junit-timeout] 	at org.apache.cassandra.db.lifecycle.Tracker.notifySwitched(Tracker.java:547)
[junit-timeout] 	at org.apache.cassandra.db.lifecycle.Tracker.switchMemtable(Tracker.java:390)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore$Flush.<init>(ColumnFamilyStore.java:1248)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:1074)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtableIfCurrent(ColumnFamilyStore.java:1055)
[junit-timeout] 	at org.apache.cassandra.db.ColumnFamilyStore.signalFlushRequired(ColumnFamilyStore.java:1482)
[junit-timeout] 	at org.apache.cassandra.db.memtable.AbstractAllocatorMemtable.flushIfPeriodExpired(AbstractAllocatorMemtable.java:240)
[junit-timeout] 	at org.apache.cassandra.db.memtable.AbstractAllocatorMemtable$1.runMayThrow(AbstractAllocatorMemtable.java:221)
[junit-timeout] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.SimulatedExecution$1.call(SimulatedExecution.java:212)
[junit-timeout] 	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
[junit-timeout] 	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:585)
[junit-timeout] 	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout] 	at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}

Reverting the changes from CASSANDRA-19705 allows the test to complete successfully, which makes sense, as {{ensureCMSPlacement()}} shows up in the trace above."
CASSANDRA-19793,[Analytics] Split the Cassandra type logic out from CassandraBridge so it can be utilized without the Spark dependency.,The CassandraBridge is a monolithic class that bridges to Cassandra but for other use cases it is beneficial to access the Cassandra types independently to deserialize Cassandra data. By splitting out the Cassandra types into a separate object we can utilize Cassandra types for deserializing Cassandra raw ByteBuffers decoupled from the Spark dependency.
CASSANDRA-19792,Allow configuring log format for Audit Logs,"Enhance the configuration for audit loggers to take two new parameters. One is for defining the key-value separator character, and the second is for the field separator character.

The existing behavior will be preserved. However an operator can configure parameters to customize the separators. For example:


{code:java}
audit_logging_options:
  enabled: false
  logger:
    - class_name: FileAuditLogger
      parameters:
        - key_value_separator: ""=""
          field_separator: "" ""
{code}"
CASSANDRA-19791,"[Analytics] Remove other uses of Apache Commons Lang for hashcodes, equality and random string generation","Apache Commons Lang is pulled in transitively by Spark but the common module does not depend on it. This change removes other uses of Apache Commons Lang for hashcode, equality checks and random string generation so more code can be moved into common module."
CASSANDRA-19790,Add an ability to reconstruct arbitrary epoch state from the log to TCM,"Current Accord functionality requires TCM to be able to provide cluster metadata for an arbitrary epoch. Unfortunately, epochs are not always available locally especially on bootstrapping non-CMS nodes. For this, we need to reconstruct from the log."
CASSANDRA-19788,Simplify and deduplicate Harry ModelChecker,
CASSANDRA-19787,Remove centos7 (and use vault mirror for ant-junit rpm download),"centos7 is EOL, and its rpm repository is gone.

Use almalinux for the noboolean builds, and use the redhat vault repository to get the ant-junit rpm."
CASSANDRA-19782,Host replacements no longer fully populate system.peers table,"When running harry after a host replacement was done a failure happened due to peers having the new node, but not the tokens for it (leading to a NPE in harry).  I took the test org.apache.cassandra.distributed.test.hostreplacement.HostReplacementTest#replaceDownedHost and made one small change; log peers after the host replacement


4.1:
{code}
INFO  [main] <main> 2024-07-18 09:36:48,211 HostReplacementTest.java:107 - Peers table from node1:
[/127.0.0.3, datacenter0, 00000000-0000-4000-8000-000000000003, null, rack0, 4.1.5-SNAPSHOT, /127.0.0.3, 94a14fb6-2cd9-3d1d-af84-a30e257aa7b8, [9223372036854775805]]
{code}

Trunk:
{code}
INFO  [main] <main> 2024-07-18 09:38:59,568 HostReplacementTest.java:109 - Peers table from node1:
[/127.0.0.3, null, null, null, null, 5.1.0-SNAPSHOT, /127.0.0.3, 00000000-0000-0000-0000-00000000000a, null]
{code}

Several fields are missing"
CASSANDRA-19780,Illegal access warning logs visible on bin/tools invocations,"There is discrepancy between what opens we have for tools under Java 17 and what we open under Java 11.

For example this does not emit any warnings when we are on Java 17:
{code:java}
./tools/bin/auditlogviewer /tmp/diagnostics -f {code}
But it does emit these warnings when we are on Java 11
{code:java}
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access using Lookup on net.openhft.chronicle.core.Jvm (file:/tmp/apache-test/apache-cassandra-5.0-rc1-SNAPSHOT/lib/chronicle-core-2.23.36.jar) to class java.lang.reflect.AccessibleObject
WARNING: Please consider reporting this to the maintainers of net.openhft.chronicle.core.Jvm
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release{code}
When I compared what that tool runs with on Java 17 and Java 11, I see this:

 

Java 17
{code:java}
--add-exports java.base/jdk.internal.misc=ALL-UNNAMED 
--add-exports java.management.rmi/com.sun.jmx.remote.internal.rmi=ALL-UNNAMED 
--add-exports java.rmi/sun.rmi.registry=ALL-UNNAMED 
--add-exports java.rmi/sun.rmi.server=ALL-UNNAMED 
--add-exports java.sql/java.sql=ALL-UNNAMED 
--add-exports java.base/java.lang.ref=ALL-UNNAMED 
--add-exports jdk.unsupported/sun.misc=ALL-UNNAMED 
--add-opens java.base/java.lang.module=ALL-UNNAMED 
--add-opens java.base/jdk.internal.loader=ALL-UNNAMED 
--add-opens java.base/jdk.internal.ref=ALL-UNNAMED 
--add-opens java.base/jdk.internal.reflect=ALL-UNNAMED 
--add-opens java.base/jdk.internal.math=ALL-UNNAMED 
--add-opens java.base/jdk.internal.module=ALL-UNNAMED 
--add-opens java.base/jdk.internal.util.jar=ALL-UNNAMED 
--add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED 
--add-opens java.base/sun.nio.ch=ALL-UNNAMED 
--add-opens java.base/java.io=ALL-UNNAMED 
--add-opens java.base/java.lang=ALL-UNNAMED 
--add-opens java.base/java.lang.reflect=ALL-UNNAMED 
--add-opens java.base/java.util=ALL-UNNAMED 
--add-opens java.base/java.nio=ALL-UNNAMED 
--add-exports jdk.attach/sun.tools.attach=ALL-UNNAMED 
--add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED 
--add-opens jdk.compiler/com.sun.tools.javac=ALL-UNNAMED {code}
For Java 11
{code:java}
--add-exports java.base/jdk.internal.misc=ALL-UNNAMED 
--add-exports java.base/jdk.internal.ref=ALL-UNNAMED 
--add-exports java.base/sun.nio.ch=ALL-UNNAMED 
--add-exports java.management.rmi/com.sun.jmx.remote.internal.rmi=ALL-UNNAMED 
--add-exports java.rmi/sun.rmi.registry=ALL-UNNAMED 
--add-exports java.rmi/sun.rmi.server=ALL-UNNAMED 
--add-exports java.sql/java.sql=ALL-UNNAMED 
--add-opens java.base/java.lang.module=ALL-UNNAMED 
--add-opens java.base/jdk.internal.loader=ALL-UNNAMED 
--add-opens java.base/jdk.internal.ref=ALL-UNNAMED 
--add-opens java.base/jdk.internal.reflect=ALL-UNNAMED 
--add-opens java.base/jdk.internal.math=ALL-UNNAMED 
--add-opens java.base/jdk.internal.module=ALL-UNNAMED 
--add-opens java.base/jdk.internal.util.jar=ALL-UNNAMED 
--add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED {code}
The difference is that we are not exporting this for Java 11
{code:java}
--add-opens java.base/sun.nio.ch=ALL-UNNAMED 
--add-opens java.base/java.io=ALL-UNNAMED 
--add-opens java.base/java.lang=ALL-UNNAMED 
--add-opens java.base/java.lang.reflect=ALL-UNNAMED 
--add-opens java.base/java.util=ALL-UNNAMED 
--add-opens java.base/java.nio=ALL-UNNAMED 
--add-opens java.base/java.lang.reflect=ALL-UNNAMED {code}
For Java 17, we explicitly add only these which are not applicable for 11 (check the end of tools/bin/cassandra.in.sh)
{code:java}
--add-exports jdk.attach/sun.tools.attach=ALL-UNNAMED
--add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED
--add-opens jdk.compiler/com.sun.tools.javac=ALL-UNNAMED  {code}
 

So, what I propose is that we add the missing opens to cassandra.in.sh for Java 11.

Even better, I would add this to conf/jvm11-clients.options"
CASSANDRA-19779,direct IO support is always evaluated to false upon the very first start of a node,"When I extract the distribution tarball and I want to use tools in tools/bin, there is this warn log visible every time for tools when they are started (does not happen on ""help"" command, obviously)
{code:java}
WARN  14:25:11,835 Unable to determine block size for commit log directory: null {code}
This is because we introduced this (1) in CASSANDRA-18464

What that does is that it will go and try to create a temporary file in commit log directory to get ""block size"" for a ""file store"" that file is in.

The problem with that is that when we just extract a tarball and run the tools - Cassandra was never started - then such commit log directory does not exist yet, so it tries to create a temporary file in a non-existing directory, which fails, hence the log message.

The fix is to check if commitlog dir exists and return / skip the resolution of block size if it does not.

Another approach might be to check if this is executed in the context of a tool and skip it from resolution altogether. The problem with this is that not all tools we have in bin/log call DatabaseDescriptor.
toolInitialization() so we might combine these two.
(1) [https://github.com/apache/cassandra/blob/cassandra-5.0/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L1455-L1462]"
CASSANDRA-19778,[Analytics] Split out BufferingInputStream stats into separate interface,The class level generics in the org.apache.cassandra.spark.stats.Stats interface are clashing with other generic parameters in the method names. This can be improved by splitting out the stats methods used by the BufferingInputStream into a separate stats interface that is used only by the BufferingInputStream so that class level generics are not required for the Stats interface.
CASSANDRA-19774,[Analytics] Bump Cassandra Sidecar version to fix build issue,"Cassandra analytics build is failing due to sidecar build failure. It is fixed in the latest Cassandra Sidecar. This patch bumps the sidecar version in analytics.
"
CASSANDRA-19772,[Analytics] Deprecate option SIDECAR_INSTANCES and replace with SIDECAR_CONTACT_POINTS,"This patch introduces a new option SIDECAR_CONTACT_POINTS for both bulk writer and reader. The option name better describes the purpose, which is to specify the initial contact points to discover the cluster topology. The existing option SIDECAR_INSTANCES are used for the same purpose and it is not deprecated.
In addition, it allows including the port value in the addresses when defining SIDECAR_CONTACT_POINTS"
CASSANDRA-19768,nodetool assassinate of a CMS voting member should be allowed and should remove it from the CMS group,"nodetool assassinate is a dangerous command but is needed when the node to be removed really can’t be accessed anymore (such as host failures).  But when the node to be removed is a member of CMS, we block this action with the following error

{code}
Can not commit transformation: ""INVALID""(Rejecting this plan as the node NodeId{id=3} is still a part of CMS.).
{code}

If the node in question is not up, and we call nodetool assassinate, we should then try to remove that node from the CMS membership group

Steps to repo

* Start 4 node cluster
* nodetool cms reconfigure 3
* targetNode = <one of the nodes in CMS>
* stop targetNode
* nodetool assassinate targetNode"
CASSANDRA-19767,Fix storage_compatibility_mode and startup_checks documentation,"The documentation for storage_compatibility_mode ([https://cassandra.apache.org/doc/latest/cassandra/managing/configuration/cass_yaml_file.html#storage_compatibility_mode]) is very difficult to read. The below highlighted text seems incorrect.

!image-2024-07-12-09-38-16-284.png|width=505,height=487!

 

It appears that the entry for the YAML option above it is causing entries to get clobbered together (startup_checks)

 

This is actually a very useful and important feature for people upgrading to Cassandra 5 to understand how to use properly - It would be good for it to be easier to read, we should be encouraging use of the safest possible upgrade path, which from my understanding would be:

{{CASSANDRA_4 -> UPGRADING -> NONE}}

 

 

Update - seems like the startup_checks docs is also missing in the 4.1 pages, I'll fix that as well."
CASSANDRA-19762,Implement dictionary lookup for CassandraPasswordValidator,
CASSANDRA-19761,"When JVM dtest is shutting down, if a new epoch is being committed the node is unable to shut down","The following was seen in the accord branch, but the problem is found in trunk as well.

{code}
node1_isolatedExecutor:8:
	java.base@11.0.15/jdk.internal.misc.Unsafe.park(Native Method)
	java.base@11.0.15/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234)
	org.apache.cassandra.simulator.systems.InterceptorOfSystemMethods$None.parkNanos(InterceptorOfSystemMethods.java:373)
	org.apache.cassandra.simulator.systems.InterceptorOfSystemMethods$Global.parkNanos(InterceptorOfSystemMethods.java:166)
	java.base@11.0.15/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123)
	java.base@11.0.15/java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1454)
	org.apache.cassandra.utils.ExecutorUtils.awaitTerminationUntil(ExecutorUtils.java:110)
	org.apache.cassandra.utils.ExecutorUtils.awaitTermination(ExecutorUtils.java:100)
	org.apache.cassandra.concurrent.Stage.shutdownAndWait(Stage.java:195)
	org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$44(Instance.java:975)
{code}

{code}
node1_MiscStage:1:
	java.base@11.0.15/jdk.internal.misc.Unsafe.park(Native Method)
	java.base@11.0.15/java.util.concurrent.locks.LockSupport.park(LockSupport.java:323)
	org.apache.cassandra.utils.concurrent.WaitQueue$Standard$AbstractSignal.await(WaitQueue.java:290)
	org.apache.cassandra.utils.concurrent.WaitQueue$Standard$AbstractSignal.await(WaitQueue.java:283)
	org.apache.cassandra.utils.concurrent.Awaitable$AsyncAwaitable.await(Awaitable.java:306)
	org.apache.cassandra.utils.concurrent.Awaitable$AsyncAwaitable.await(Awaitable.java:338)
	org.apache.cassandra.utils.concurrent.Awaitable$Defaults.awaitUninterruptibly(Awaitable.java:186)
	org.apache.cassandra.utils.concurrent.Awaitable$AbstractAwaitable.awaitUninterruptibly(Awaitable.java:259)
	org.apache.cassandra.tcm.log.LocalLog$Async.runOnce(LocalLog.java:710)
	org.apache.cassandra.tcm.log.LocalLog.runOnce(LocalLog.java:404)
	org.apache.cassandra.tcm.log.LocalLog.waitForHighestConsecutive(LocalLog.java:346)
	org.apache.cassandra.tcm.PaxosBackedProcessor.fetchLogAndWait(PaxosBackedProcessor.java:163)
	org.apache.cassandra.tcm.AbstractLocalProcessor.commit(AbstractLocalProcessor.java:109)
	org.apache.cassandra.distributed.test.log.TestProcessor.commit(TestProcessor.java:61)
	org.apache.cassandra.tcm.ClusterMetadataService$SwitchableProcessor.commit(ClusterMetadataService.java:841)
	org.apache.cassandra.tcm.Processor.commit(Processor.java:45)
	org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:516)
	org.apache.cassandra.service.accord.AccordFastPathCoordinator$Impl.lambda$updateFastPath$2(AccordFastPathCoordinator.java:208)
	org.apache.cassandra.service.accord.AccordFastPathCoordinator$Impl$$Lambda$11211/0x0000000802441840.run(Unknown Source)
{code}

Accord is trying to commit a new epoch, but TCM uses “awaitUninterruptibly” which ignores the thread interrupt done while the cluster is shutting down.  When this is happening the instance is unable to make progress so loops endlessly, causing the test to fail to close."
CASSANDRA-19759,"CEP-15 (Accord): When starting a transaction in a table where Accord is not enabled, should fail fast rather than fail with lack of ranges","Reported in Slack: https://the-asf.slack.com/archives/C0459N9R5C6/p1712831271287869

The “accord_demo.txt” has the following

{code}
ccm create accord-cql-poc -n 3
ccm start

bin/cqlsh -e ""create keyspace ks with replication={'class':'SimpleStrategy', 'replication_factor':3};""
bin/cqlsh -e ""create table ks.tbl1 (k int primary key, v int);""
bin/cqlsh -e ""create table ks.tbl2 (k int primary key, v int);""

bin/nodetool -h 0000:0000:0000:0000:0000:ffff:7f00:0001 -p 7100 createepochunsafe
bin/nodetool -h 0000:0000:0000:0000:0000:ffff:7f00:0001 -p 7200 createepochunsafe
bin/nodetool -h 0000:0000:0000:0000:0000:ffff:7f00:0001 -p 7300 createepochunsafe

BEGIN TRANSACTION
  LET row1 = (SELECT * FROM ks.tbl1 WHERE k = 1);
  SELECT row1.v;
  IF row1 IS NULL THEN
    INSERT INTO ks.tbl1 (k, v) VALUES (1, 2);
  END IF
COMMIT TRANSACTION;
{code}

If you run that it fails in an unclear way

{code}
cqlsh> BEGIN TRANSACTION
   ...   LET row1 = (SELECT * FROM ks.tbl1 WHERE k = 1);
   ...   SELECT row1.v;
   ...   IF row1 IS NULL THEN
   ...     INSERT INTO ks.tbl1 (k, v) VALUES (1, 2);
   ...   END IF
   ... COMMIT TRANSACTION;
NoHostAvailable: ('Unable to complete the operation against any hosts', {<Host: 127.0.0.1:9042 datacenter1>: <Error from server: code=0000 [Server error] message=""java.lang.IllegalStateException: Unable to select a HomeKey as the topology does not have any ranges for epoch 17"">})
{code}

The issue is that the table was not marked as an Accord table; aka missing 

{code}
WITH transactional_mode='full'
{code}

The demo should be updated to show this, but the error message should also be improved.  We validate that the table exists but now that the metadata is required we should also enforce this in CQL validation"
CASSANDRA-19758,Accord: CommandsForKey should self-prune,"CommandsForKey should periodically self-prune, so as to continue functioning well in-between garbage collections. This is a bit complicated, as once we prune we are left with potentially incomplete information, and have to sometimes load per-command information from disk. But the payoff is ensuring CommandsForKey objects - which drive the majority of the state machine - are kept to a reasonable size.
"
CASSANDRA-19757, Accord Journal / Determinism: Load Command states from the log,"  * Persist intermediate Command inthe journal
  * Simplify AccordJournal by removing Framing
  * Save command outcomes to the log
  * Reconstruct latest command state from the log entries
  * Replace `SerializerSupport#reconstruct` with log reconstruction"
CASSANDRA-19756,Accord Journal / Determinism: Store intermediate Command states in the log,"Write side of the replay determinism: persist intermediate Command in the journal

      * Simplifies AccordJournal by removing Framing
      * Saves command outcomes to the log
      * Reconstructs latest command state from the log entries"
CASSANDRA-19755,Coordinator read latency metrics are inflated for some queries,"When a partition read is decomposed on the coordinator into multiple single partition read queries, the latency metric captured in StorageProxy can be artificially increased.
This primarily affects reads where paging and aggregates are used or where an IN clause selects multiple partition keys."
CASSANDRA-19753,Not getting responses with concurrent stream IDs in native protocol v5,"This is not gonna be an easy bug to report or to give a great set of repro steps for, so apologies in advance. I’m one of the authors and the maintainer of [Xandra|https://github.com/whatyouhide/xandra], the Cassandra client for Elixir.

We noticed an issue with request timeouts in a new version of our client. Just for reference, the issue is [this one|https://github.com/whatyouhide/xandra/issues/356].

After some debugging, we figured out that the issue was limited to *native protocol v5*. With native protocol v5, the issue shows up in C* 4.1 and 5.0. With native protocol v4, those versions (4.1 and 5.0) both work fine. I'm running C* in a Docker container, but I've had folks reproduce this with all sorts of C* setups.

h2. The Issue

The new version of our client in question uses concurrent requests. We assign each request a sequential stream ID ({{1}}, {{2}}, ...). We behave in a compliant way with [section 2.4.1.3. of the native protocol v5 spec|https://github.com/apache/cassandra/blob/e7cf38b5de6f804ce121e7a676576135db0c4bb1/doc/native_protocol_v5.spec#L316C1-L316C9]—to the best of my knowledge.

Now, it seems like C* does not respond do all requests this way. We have a [simple test|https://github.com/whatyouhide/xandra/pull/368] in our repo that reproduces this. It just issues two requests in parallel (with stream IDs {{1}} and {{2}}) and then keeps issuing requests as soon as there are responses. Almost 100% of the times, we don't get the response on at least one stream. I've also attached some debug logs that show this in case it can be helpful (from the client perspective). The {{<<56, 0, 2, 67, 161, ...>>}} syntax is Erlang's syntax for bytestrings, where each number is the decimal value for a single byte. You can see in the logs that we never get the response frame on stream ID 1. Sometimes it's stream ID 2, or 3, or whatever.

I’m pretty short on what to do next on our end. I’ve tried shuffling around the socket buffer size as well (from {{10}} bytes to {{1000000}} bytes) to get the packets to split up in all sorts of places, but everything works as expected _except_ for the requests that are not coming out of C*.

Any other help is appreciated here, but I've started to suspect this might be something with C*. It could totally not be, but I figured it was worth to post out here.

Thank you all in advance folks! 💟"
CASSANDRA-19752,Debian packaging fails after openjdk-8* and java8* removed from bullseye,"No candidates for {{`openjdk-8-jdk | java8-jdk`}}

Failure occurs at the {{mk-build-deps}} step…
{noformat}
Broken cassandra-build-deps:amd64 Depends on openjdk-8-jdk:amd64 < none @un H >
     Removing cassandra-build-deps:amd64 because I can't find openjdk-8-jdk:amd64
Broken cassandra-build-deps:amd64 Depends on java8-jdk:amd64 < none @un H >
     Removing cassandra-build-deps:amd64 because I can't find java8-jdk:amd64
     Or group remove for cassandra-build-deps:amd64
…
mk-build-deps: Unable to install cassandra-build-deps at /usr/bin/mk-build-deps line 457.
mk-build-deps: Unable to install all build-dep packages
{noformat}
ref: https://ci-cassandra.apache.org/job/Cassandra-3.0-artifacts/jdk=jdk_1.8_latest,label=cassandra/428/console "
CASSANDRA-19751,IllegalStateException when query on table having static columns during the Cassandra cluster upgrade from 3.11.4 to 4.0.11,"We are upgrading Cassandra cluster from 3.11.4 to 4.0.11. This cluster has SSL enabled.
While performing upgrade on 1st DC, we observed below WARN/ERROR messages on C* 3 and C* 4 nodes.

+C*3 nodes:+


{noformat}
WARN  [ReadStage-1] 2024-06-11 08:04:09,088 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IllegalStateException: [last_metadata_updt_ts, price_metadata] is not a subset of [price_metadata]

WARN  [ReadStage-1] 2024-06-19 05:10:31,226 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IllegalStateException: [default_price_json, last_metadata_updt_ts, price_metadata] is not a subset of [price_metadata]
{noformat}


+C*4 nodes:+

{noformat}
ERROR [ReadStage-1] 2024-06-19 05:48:47,388 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[ReadStage-1,5,main]
java.lang.IllegalStateException: [last_metadata_updt_ts, price_metadata] is not a subset of [price_metadata]
{noformat}


Table definition for which above columns are associated is as below:


{noformat}
CREATE TABLE omni_price_ks_v2.location_price_mstr (
    tcin text,
    location_id bigint,
    price_change_id text,
    default_price_json text static,
    end_ts bigint,
    last_metadata_updt_ts bigint static,
    last_update_ts bigint,
    price_json text,
    price_metadata text static,
    price_type text,
    start_ts bigint,
    status text,
    version text,
    PRIMARY KEY (tcin, location_id, price_change_id)
) WITH CLUSTERING ORDER BY (location_id ASC, price_change_id ASC)
    AND bloom_filter_fp_chance = 0.1
    AND caching = {'keys': 'ALL', 'rows_per_partition': '100'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{noformat}

App team also observed below error in their application logs when try to read from this table.

{noformat}
{ ""code"": ""ERR_GETPRICE_0034"", ""message"": ""Cassandra failure during read query at consistency LOCAL_QUORUM (2 responses were required but only 1 replica responded, 1 failed)"" }
{noformat}

Because of this error, the application is getting impacted during the upgrade.
Once the upgrade on all DCs is completed, this error stops.

I found below bug which matches our case.
https://issues.apache.org/jira/browse/CASSANDRA-17601

It seems like we are hitting some bug and hence raising this Jira.

Can you please have a look if this is still a bug and what would be the fix?

Let me know if you need any more details.
"
CASSANDRA-19749,ALTER USER | ROLE IF EXISTS creates a user / role if it does not exist,"Let's have:

{code}
authenticator:
  class_name : org.apache.cassandra.auth.PasswordAuthenticator
authorizer: CassandraAuthorizer
role_manager: CassandraRoleManager
{code}

and do this:

{code}
cassandra@cqlsh> select * from system_auth.roles;

 role      | can_login | is_superuser | member_of | salted_hash
-----------+-----------+--------------+-----------+--------------------------------------------------------------
 cassandra |      True |         True |      null | $2a$10$sFCKeluid5MlW/Z0CU1ygO1U5qpLW4Rgivmu8rZNmNNQ8WeC2y92S

{code}

Then 

{code}
cassandra@cqlsh> ALTER USER IF EXISTS this_does_not_exist SUPERUSER ;
cassandra@cqlsh> select * from system_auth.roles where role = 'this_does_not_exist';

 role                | can_login | is_superuser | member_of | salted_hash
---------------------+-----------+--------------+-----------+-------------
 this_does_not_exist |      null |         True |      null |        null

{code}

It seems to be same behaviour for ALTER ROLE too.

{code}
cassandra@cqlsh> ALTER ROLE IF EXISTS this_role_is_not_there WITH SUPERUSER = true ;
cassandra@cqlsh> select * from system_auth.roles where role = 'this_role_is_not_there';

 role                   | can_login | is_superuser | member_of | salted_hash
------------------------+-----------+--------------+-----------+-------------
 this_role_is_not_there |      null |         True |      null |        null

{code}"
CASSANDRA-19748,[Analytics] Refactor Analytics to move standalone code into common module with minimal dependencies,"The Analytics codebase is heavily tied to Spark. In an effort to re-use code across projects (like CDC) we should move standalone Pojos and util classes into an cassandra-analytics-common module that exists standalone without dependencies to Cassandra or Spark and with minimal standard dependencies (Kryo, Guava, Jackson, Apache Commons Lang etc)."
CASSANDRA-19747,Invalid schema.cql created by snapshot after dropping more than one field,"After dropping at least 2 fields the schema.cql produced by _nodetool snapshot_ is invalid (it is missing a comma)
{code:sql}
CREATE TABLE IF NOT EXISTS test.testtable (
    field1 text PRIMARY KEY,
    field2 text
    field3 text
) WITH ID ...{code}
expected outcome
{code:sql}
CREATE TABLE IF NOT EXISTS test.testtable (
    field1 text PRIMARY KEY,
    field2 text,
    field3 text
) WITH ID ...{code}
reproducing the isue is simple by running the following commands
{code:sh}
docker run -d --name cassandra cassandra:4.1.5

echo ""Wait for the container to start""
until docker exec -ti cassandra nodetool status | grep UN;do sleep 1;done;sleep 10

echo ""Create keyspace and table for test""
docker exec -ti cassandra cqlsh -e ""CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}; CREATE TABLE IF NOT EXISTS test.testtable (field1 text PRIMARY KEY,field2 text,field3 text);""

echo ""Drop 2 fields""
docker exec -ti cassandra cqlsh -e ""ALTER TABLE test.testtable DROP (field2, field3);""

echo ""Create snapshot and view schema.cql""
docker exec -ti cassandra /opt/cassandra/bin/nodetool snapshot -t my_snapshot
docker exec -ti cassandra find /var/lib/cassandra/data -name schema.cql  -exec cat {} +   {code}
the full output of the sql generated by the reproduce is below
{code:sql}
CREATE TABLE IF NOT EXISTS test.testtable (
    field1 text PRIMARY KEY,
    field2 text
    field3 text
) WITH ID = 0e9aa540-391f-11ef-945e-0be1221ff441
    AND additional_write_policy = '99p'
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND cdc = false
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND memtable = 'default'
    AND crc_check_chance = 1.0
    AND default_time_to_live = 0
    AND extensions = {}
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair = 'BLOCKING'
    AND speculative_retry = '99p';
ALTER TABLE test.testtable DROP field2 USING TIMESTAMP 1719999102807000;
ALTER TABLE test.testtable DROP field3 USING TIMESTAMP 1719999102807001;
{code}

Found this bug while trying to restore the schema from a backup  created by copying a snapshot from a running node."
CASSANDRA-19746,Update CQLSH website documentation to remove Python 2.7 reference,"The CQLSH [Compatibility (4.1)|https://cassandra.apache.org/doc/stable/cassandra/tools/cqlsh.html#compatibility] section mentions Python 2.7 but this should be Python 3 now.

Note: it seems to be only an issue only with 4.1.

The trunk link here is correct: [Compatibility (trunk)|https://cassandra.apache.org/doc/trunk/cassandra/managing/tools/cqlsh.html]"
CASSANDRA-19744,Accord migration and interop correctness,"There are several issues around splitting and retrying mutations, using the original timestamp for batchlog/hints, batchlog/hint support in general, running Accord barriers only against the ranges actually owned by Accord.

"
CASSANDRA-19739,Move bcprov-jdk18on-1.76.jar to build deps,This came up after I bumped dependency-check version to 10.0.0 as suggested in CASSANDRA-19738.
CASSANDRA-19738,Update dependency-check library to version 10.0.0,"Currently, we are at 9.0.5, which gives me locally basically this (1)

Version 10.0.0 was released today and check is passing again.

We should update it everywhere to 10.0.0

(1) https://github.com/jeremylong/DependencyCheck/issues/6515"
CASSANDRA-19737,Accord migration mode FULL always runs with interop,Whether we use interop is not done per transaction. Accord always seems to run with interop for every transaction when it is constructed with the factory that creates interop execution.
CASSANDRA-19736,Batchlog and hint replay have timestamps replaced by Accord,"The issue is that we might create the transaction at a much later time and then the operation would be written to Cassandra with a later timestamp. It should be fine to use the minimum of the two.

This also means that `USING TIMESTAMP` will also work as long as the provided timestamp is < the Accord timestamp."
CASSANDRA-19735,Cannot correctly create keyspace statement with replication during schemaChange,"h3. What happened

A specific schema change for creating keyspace with replications failed during Cassandra upgrade testing, but can pass under Cassandra distributed testing (non-upgrade).
h3. How to reproduce:

Put the following test under {{{}cassandra/test/distributed/org/apache/cassandra/distributed/upgrade/{}}}, and build dtest jars for any versions within [4.1.3, 5.0-alpha2].
{code:java}
package org.apache.cassandra.distributed.upgrade;
public class demoUpgradeTest extends UpgradeTestBase
    @Test
    public void demoTest() throws Throwable {
        new TestCase()
                .nodes(1)
                .nodesToUpgrade(1)
                .withConfig(config -> config.with(NETWORK, GOSSIP, NATIVE_PROTOCOL))
                .upgradesToCurrentFrom(v41)
                .setup((cluster) -> {
                    cluster.schemaChange(withKeyspace(""CREATE KEYSPACE %s WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2}""));
                }).runAfterNodeUpgrade((cluster, node) -> {
                    // let's do nothing here.
                }).run();
    }
} {code}
Run the test with
{code:java}
$ ant test-jvm-dtest-some-Duse.jdk11=true -Dtest.name=org.apache.cassandra.distributed.upgrade.demoUpgradeTest {code}
You will see the following failure:
{code:java}
[junit-timeout] Testcase: demoTest(org.apache.cassandra.distributed.upgrade.demoUpgradeTest)-_jdk11:    Caused an ERROR
[junit-timeout] Cannot add existing keyspace ""distributed_test_keyspace""
[junit-timeout] org.apache.cassandra.exceptions.AlreadyExistsException: Cannot add existing keyspace ""distributed_test_keyspace""
[junit-timeout]     at org.apache.cassandra.cql3.statements.schema.CreateKeyspaceStatement.apply(CreateKeyspaceStatement.java:78)
[junit-timeout]     at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.apply(DefaultSchemaUpdateHandler.java:230)
[junit-timeout]     at org.apache.cassandra.schema.Schema.transform(Schema.java:597)
[junit-timeout]     at org.apache.cassandra.cql3.statements.schema.AlterSchemaStatement.execute(AlterSchemaStatement.java:114)
[junit-timeout]     at org.apache.cassandra.cql3.statements.schema.AlterSchemaStatement.execute(AlterSchemaStatement.java:60)
[junit-timeout]     at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:122)
[junit-timeout]     at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:103)
[junit-timeout]     at org.apache.cassandra.distributed.impl.Coordinator.lambda$executeWithResult$0(Coordinator.java:66)
[junit-timeout]     at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
[junit-timeout]     at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
[junit-timeout]     at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[junit-timeout]     at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[junit-timeout]     at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout]     at java.base/java.lang.Thread.run(Thread.java:829) {code}
I have tested version pairs 4.1.3_4.1.4, 4.1.4_4.1.5, 4.1.5_5.0-alpha1, and 5.0-alpha1_5.0-alpha2. All of them have the same issue.

I wrote a very similar test with Cassandra distributed test framework (non-upgrade test) as below:
{code:java}
package org.apache.cassandra.distributed.test.streaming;public class LCSStreamingKeepLevelTest extends TestBaseImpl
{
    @Test
    public void demoTest() throws IOException
    {
        try (Cluster cluster = builder().withNodes(1)
                .withConfig(config -> config.with(NETWORK, GOSSIP, NATIVE_PROTOCOL))
                .start())
        {
            cluster.schemaChange(withKeyspace(""CREATE KEYSPACE %s WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2}""));
        }
    }
} {code}
This distributed test would pass successfully without any issues.

 

The expected behavior should be that the upgrade test above can also perform cluster.schemaChange() successfully."
CASSANDRA-19728,Improve debug around paused and disabled compaction,"Compactions can be paused (from other operations) or disabled (nodetool), and when compactions are not running it can be unclear what the reason is.

Add debug so this is clearer to catch for the operator."
CASSANDRA-19727,[Analytics] Bulk writer fails validation stage when writing to a cluster using RandomPartitioner,"In bulk writer after writing SSTables, written data to SSTables is read back again to perform validations before shipping the files to the Cassandra instances. The logic to validate the SSTables assumes {{{}Murmur3Partitioner{}}}. This validation fails however when a bulk writer job is running against a cluster using the {{RandomPartitioner}} with the following stacktrace:


{code}
java.lang.RuntimeException: java.lang.IllegalStateException: Partitioner in ValidationMetadata does not match TableMetaData: org.apache.cassandra.dht.RandomPartitioner vs. org.apache.cassandra.dht.Murmur3Partitioner
	at org.apache.cassandra.spark.utils.Throwing.lambda$function$2(Throwing.java:84)
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)
	at java.base/java.util.HashMap$KeySpliterator.forEachRemaining(HashMap.java:1707)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)
	at org.apache.cassandra.spark.data.BasicSupplier.openAll(BasicSupplier.java:44)
	at org.apache.cassandra.bridge.CassandraBridgeImplementation.getCompactionScanner(CassandraBridgeImplementation.java:248)
	at org.apache.cassandra.spark.data.DataLayer.openCompactionScanner(DataLayer.java:262)
	at org.apache.cassandra.spark.bulkwriter.SSTableWriter.validateSSTables(SSTableWriter.java:129)
	at org.apache.cassandra.spark.bulkwriter.SSTableWriter.close(SSTableWriter.java:113)
	at org.apache.cassandra.spark.bulkwriter.RecordWriter.finalizeSSTable(RecordWriter.java:224)
	at org.apache.cassandra.spark.bulkwriter.RecordWriter.write(RecordWriter.java:122)
	... 15 more
{code}
"
CASSANDRA-19720,Accord Journal: Periodic Commit Log Mode,Introduce periodic mode to Accord journal
CASSANDRA-19719,"CEP-15: (Accord) When nodes are removed from a cluster, need to update topology tracking to avoid being blocked","When doing a host replacement or decom nodes will leave the cluster and won't ever come back, this can put accord's topology sync logic into a bad state as its going to wait forever for those nodes to acknowledge the epoch or parts of the ranges."
CASSANDRA-19718,CEP-15: (Accord) SyncPoint timeouts become a Exhausted rather than a Timeout and doesn’t get retried,"In Cassandra we try to make sure coordinators return timeout if every call under it was also a timeout, this makes it easier to understand what is going on (coordination failure due to timeouts looks very different than us just timing out), but accord doesn't do this; leading to an Exhausted error (which we don't retry)"
CASSANDRA-19717,CEP-15 (C*): Don't run incremental repairs for accord only repairs,There's no reason to do anti-compaction for repairs that are only completing consensus operations and not moving data around
CASSANDRA-19716,[Analytics] Invalid mapping when timestamp is used as a partition key during bulk writes,"When Cassandra has a table with a schema that includes a timestamp PK, i.e {{CREATE TABLE (a timestamp, b text, c text, PRIMARY KEY (a))}}, bulk writer maps the timestamp to a {{LongType}}. This causes the bulk writer job to produce a class cast exception when building the composite key during repartition and sorting of data in the spark job"
CASSANDRA-19714,Use table-specific partitioners during Paxos repair,"Partition keys in the \{{system.paxos}} table are derived from the key involved in the paxos transaction. Initially, it was safe to assume that the paxos table itself used the same partitioner as the tables in the transactions as all distributed keyspaces and tables were configured with the global partitioner. This is no longer true as the \{{system_cluster_metadata.distributed_metadata_log}} has its own custom partitioner. 


Likewise, \{{PaxosRepairHistory}} and the \{{system.paxos_repair_history}} table which makes that history durable map token ranges in the transacted tables to ballots. Prior to CASSANDRA-19482 it was safe to assume that these ranges contained tokens from the global partitioner but as this is no longer the case, we must use the specific partitioner for the table in question when working with ranges during paxos repair. "
CASSANDRA-19713,Disallow denylisting keys in system_cluster_metadata,https://github.com/krummas/cassandra/commit/0435a9dbc382a428864b4b329e127882d9c18419
CASSANDRA-19712,Fix gossip status after replacement,"Make sure gossip status is correct for replacement node.

https://github.com/krummas/cassandra/commit/2ed38a6273def17e6decbb8e74826b1995800d59"
CASSANDRA-19711,Ignore repair requests for system_cluster_metadata,"Since system_cluster_metadata is not replicated like other keyspaces we might break existing repair automation if a {{nodetool repair}} is run against a node not in the CMS. Just ignore the request if so.

https://github.com/krummas/cassandra/commit/76437723acea35421ec5bf0412dcdee1411dcb6e"
CASSANDRA-19710,Avoid ClassCastException when verifying tables with reversed partitioner,"A few TCM tables use a custom partitioner, this causes class cast exception when running nodetool verify on them.

https://github.com/krummas/cassandra/commit/64897cb6382967f3e134752f5b9f223ff7daeb84"
CASSANDRA-19709,Always repair the full range when repairing system_cluster_metadata,"Since system_cluster_metadata uses a custom partitioner we need to repair the full range whenever an operator uses -st and -et tokens to avoid breaking existing repair automations. This is fine since the amount of data in this keyspace should be small.

https://github.com/krummas/cassandra/commit/6ab8fcd1126f67b0117e1ee3c1fd1d4b40ac2362"
CASSANDRA-19708,Remove sid from bullseye docker images,"sid is flakey, often broken and takes days for correct packages to be uploaded.

ref: https://ci-cassandra.apache.org/job/Cassandra-4.1-artifacts/jdk=jdk_1.8_latest,label=cassandra/611/ 

sid is only used for jdk8

looks like replacing it with temurin might be a safer/stable choice.

"
CASSANDRA-19705,Reconfigure CMS after move/bootstrap/replacement,"The CMS placement uses SimpleStrategy/NTS to decide where it is placed to make it easier to safely bounce a cluster using existing tools (with CMS placement {{dc1: 3, dc2: 3}} we will   use the placements for min_token in a NetworkTopologyStrategy with the same replication setting). 

We need to reconfigure this after move/bootstrap/replacement though, since the placements might have changed."
CASSANDRA-19704,UnsupportedOperationException is thrown when no space for LCS,"In {{CompactionTask#buildCompactionCandidatesForAvailableDiskSpace}} with LCS, if node has limited disk space and can't remove any sstable from L0 or L1 in {{{}LeveledCompactionTask#reduceScopeForLimitedSpace{}}}, {{LeveledCompactionTask#partialCompactionsAcceptable}} will throw {{UnsupportedOperationException}}.

We should handle {{LeveledCompactionTask#partialCompactionsAcceptable}} more gracefully with {{return level <= 1}} or simply {{true}} since {{reduceScopeForLimitedSpace}} only removes sstable from L0 or L1.

Related https://issues.apache.org/jira/browse/CASSANDRA-17272"
CASSANDRA-19697,Test failure: materialized_views_test.py::TestMaterializedViews::test_rename_column_atomicity,Breaking this out from CASSANDRA-19683.  The byteman script fails to execute in 5.0/trunk after CASSANDRA-19534.
CASSANDRA-19695,Accord Journal Simulation: Add instrumentation for Semaphore,
CASSANDRA-19694,Make Accord timestamps strictly monotonic,
CASSANDRA-19693,Relax slow_query_log_timeout for MultiNodeSAITest,"To stress the paging subsystem, we intentionally use a comically low fetch size in {{{}MultiNodeSAITest{}}}. This can lead to some very slow queries when we get matches into the hundreds of rows. It looks like CASSANDRA-19534 has gotten a little more aggressive about how the slow query timeout is triggered, and there’s a lot of noise around this in the logs, even in local runs. I think bumping the default slow query timeout and perhaps the native protocol timeout a bit should clear this up."
CASSANDRA-19692,ClassCastException on selection with where clause from system.local_metadata_log,"{code}
select * from system.local_metadata_log where epoch = 1;
NoHostAvailable: ('Unable to complete the operation against any hosts', {<Host: 172.19.0.12:9042 dc2>: <Error from server: code=0000 [Server error] message=""java.lang.ClassCastException: class org.apache.cassandra.dht.Murmur3Partitioner$LongToken cannot be cast to class org.apache.cassandra.dht.ReversedLongLocalPartitioner$ReversedLongLocalToken (org.apache.cassandra.dht.Murmur3Partitioner$LongToken and org.apache.cassandra.dht.ReversedLongLocalPartitioner$ReversedLongLocalToken are in unnamed module of loader 'app')"">})
{code}

same select but with ""limit"" works."
CASSANDRA-19688,SAI support for BETWEEN operator,The new CQL BETWEEN operator should map pretty neatly to SAI’s internal RANGE operator for numeric indexes. There’s no reason we shouldn’t support it.
CASSANDRA-19687,ApplyThenWaitUntilApplied supplies wrong epoch for executeAtEpoch,It's from the `txnId` not the `executeAt`
CASSANDRA-19685,Add auto_hints_cleanup_enabled to web documentation,"{{auto_hints_cleanup_enabled flag in cassandra.yaml is not documented in the [Cassandra web documentation|https://cassandra.apache.org/doc/stable/cassandra/operating/hints.html].}}

Let’s add it there for completion."
CASSANDRA-19683,Investigate dtest timeouts after CASSANDRA-19534,"We have seen increased dtest timeouts that don't appear to be environmental:

https://app.circleci.com/pipelines/github/driftx/cassandra/1651/workflows/738d1c92-0ffe-45e7-8ad4-f2646170ba76

https://ci-cassandra.apache.org/job/Cassandra-5.0/238/

I have confirmed these don't occur before CASSANDRA-19534"
CASSANDRA-19681,Debian repository is missing 3.11.17 package,"The Debian package for Cassandra 3.11.17 is missing from the JFrog artifactory. The [package index|https://apache.jfrog.io/artifactory/cassandra-deb/dists/311x/main/binary-amd64/Packages] is still referencing version 3.11.16:

{code}
Package: cassandra
Version: 3.11.16
...
Filename: pool/main/c/cassandra/cassandra_3.11.16_all.deb
...

Package: cassandra-tools
Source: cassandra
Version: 3.11.16
...
Filename: pool/main/c/cassandra/cassandra-tools_3.11.16_all.deb
...
{code}

When I tried to install the latest C* 3.11 version on Ubuntu, 3.11.16 got installed instead of 3.11.17.

Note that this was originally reported by [Roman on Stack Exchange|https://dba.stackexchange.com/questions/340007/]."
CASSANDRA-19680,Update URL in Java Driver README.md maven badge,The maven badge needs to be updated from com.datastax.oss to org.apache.cassandra.  Also add a license badge.
CASSANDRA-19679,Stream processing for SimpleRestriction::bindAndGetClusteringElements,"Part 2 (of 2) of low-hanging fruit Stream performance improvements.

The second main Stream contributor to allocations and CPU was SimpleRestriction::bindAndGetClusteringElements, which contributes to 5% of all allocations for a 50/50 workload. The image attached shows allocation profiling on _trunk_ (see purple highlighted sections for Stream-related allocs).

The 'after' profile for a 50/50 workload shows a reduction from 4.58% allocations down to 1.37%. For a 90/10 (w/r) workload we see 4.28% decrease to 1.10%."
CASSANDRA-19676,Stream processing for StorageProxy::updateCoordinatorWriteLatencyTableMetric,"On profiling a write-heavy workload (90% writes) using easy-cass-stress, it became very clear StorageProxy::updateCoordinatorWriteLatencyTableMetric was a hot path that ~15% of the CPU cycles of ModificationStatement::executeWithoutCondition were taken up by (see attached async-profiler image).

We should convert this stream to a simple for loop, as has been discussed recently on the mail list.

easy-cass-stress command:

$ bin/easy-cass-stress run KeyValue -n 10m --maxwlat 10 -r 0.1 --rate 20000 --compaction twcs

 "
CASSANDRA-19675,Avoid streams in the common case for UpdateTransaction creation,"Some recent Accord profiling highlighted some easily addressable inefficiency in the way we create new {{UpdateTransaction}} objects in {{SecondaryIndexManager}} that have existed since the introduction of index groups for SAI. We should be able to clean this up by avoiding stream creation or even iteration over the groups when there is a single index group, which is going to be the most common case with SAI anyway. If we do have to iterate, there should also be no reason to copy the collection of index groups via {{listIndexGroups()}}, although that copying can remain in the method itself for external callers.

 !new_update_txn_streams.png! "
CASSANDRA-19674,CEP-15: (Accord) Bootstrap's LocalOnly txn can not be recreated from SerializerSupport,Bootstrap creates a LocalOnly txn and executes it rather than following the normal txn flow.  This has a problem as these mutations are not stored in the journal so a cache evict of the LocalOnly txn causes us to fail as we don’t know how to reconstruct it.
CASSANDRA-19670,Add documentation about logging on the website,
CASSANDRA-19669,Audit Log entries are missing identity for mTLS connections,"Audit log entries are missing the {{IDENTITY}} when an mTLS connection is established. Currently, the client state is captured as part of the audit log entries, however the additional metadata for the authenticated user does not get propagated to the entry. For the mTLS connections, this means that the identity information is not included to the log entry details.

Additionally, when a TLS connection is terminated during handshake (say a client is using an expired certificate) the error is not propagated to the audit log failure attempts. "
CASSANDRA-19668,SIGSEGV originating in Paxos V2 Scheduled Task,"I haven't gotten to the root cause of this yet. Several 4.1 nodes have crashed in in production.  I'm not sure if this is related to Paxos v2 or not, but it is enabled.  offheap_objects also enabled. 

I'm not sure if this affects 5.0, yet.

Most of the crashes don't have a stacktrace - they only reference this

{noformat}
Stack: [0x00007fabf4c34000,0x00007fabf4d34000],  sp=0x00007fabf4d31f00,  free space=1015k
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
v  ~StubRoutines::jint_disjoint_arraycopy

{noformat}

They all are in the {{ScheduledTasks}} thread.

However, one node does have this in the crash log:

{noformat}
---------------  T H R E A D  ---------------

Current thread (0x000078b375eac800):  JavaThread ""ScheduledTasks:1"" daemon [_thread_in_Java, id=151791, stack(0x000078b34b780000,0x000078b34b880000)]

Stack: [0x000078b34b780000,0x000078b34b880000],  sp=0x000078b34b87c350,  free space=1008k
Native frames: (J=compiled Java code, A=aot compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 29467 c2 org.apache.cassandra.db.rows.AbstractCell.clone(Lorg/apache/cassandra/utils/memory/ByteBufferCloner;)Lorg/apache/cassandra/db/rows/Cell; (50 bytes) @ 0x000078b3dd40a42f [0x000078b3dd409de0+0x000000000000064f]
J 17669 c2 org.apache.cassandra.db.rows.Cell.clone(Lorg/apache/cassandra/utils/memory/Cloner;)Lorg/apache/cassandra/db/rows/ColumnData; (6 bytes) @ 0x000078b3dc54edc0 [0x000078b3dc54ed40+0x0000000000000080]
J 17816 c2 org.apache.cassandra.db.rows.BTreeRow$$Lambda$845.apply(Ljava/lang/Object;)Ljava/lang/Object; (12 bytes) @ 0x000078b3dbed01a4 [0x000078b3dbed0120+0x0000000000000084]
J 17828 c2 org.apache.cassandra.utils.btree.BTree.transform([Ljava/lang/Object;Ljava/util/function/Function;)[Ljava/lang/Object; (194 bytes) @ 0x000078b3dc5f35f0 [0x000078b3dc5f34a0+0x0000000000000150]
J 35096 c2 org.apache.cassandra.db.rows.BTreeRow.clone(Lorg/apache/cassandra/utils/memory/Cloner;)Lorg/apache/cassandra/db/rows/Row; (37 bytes) @ 0x000078b3dda9111c [0x000078b3dda90fe0+0x000000000000013c]
J 30500 c2 org.apache.cassandra.utils.memory.EnsureOnHeap$CloneToHeap.applyToRow(Lorg/apache/cassandra/db/rows/Row;)Lorg/apache/cassandra/db/rows/Row; (16 bytes) @ 0x000078b3dd59b91c [0x000078b3dd59b8c0+0x000000000000005c]
J 26498 c2 org.apache.cassandra.db.transform.BaseRows.hasNext()Z (215 bytes) @ 0x000078b3dcf1c454 [0x000078b3dcf1c180+0x00000000000002d4]
J 30775 c2 org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext()Ljava/lang/Object; (49 bytes) @ 0x000078b3dc789020 [0x000078b3dc788fc0+0x0000000000000060]
J 9082 c2 org.apache.cassandra.utils.AbstractIterator.hasNext()Z (80 bytes) @ 0x000078b3dbb3c544 [0x000078b3dbb3c440+0x0000000000000104]
J 35593 c2 org.apache.cassandra.service.paxos.uncommitted.PaxosRows$PaxosMemtableToKeyStateIterator.computeNext()Lorg/apache/cassandra/service/paxos/uncommitted/PaxosKeyState; (126 bytes) @ 0x000078b3dc7ceeec [0x000078b3dc7cee20+0x00000000000000cc]
J 35591 c2 org.apache.cassandra.service.paxos.uncommitted.PaxosRows$PaxosMemtableToKeyStateIterator.computeNext()Ljava/lang/Object; (5 bytes) @ 0x000078b3dc7d09e4 [0x000078b3dc7d09a0+0x0000000000000044]
J 9082 c2 org.apache.cassandra.utils.AbstractIterator.hasNext()Z (80 bytes) @ 0x000078b3dbb3c544 [0x000078b3dbb3c440+0x0000000000000104]
J 34146 c2 com.google.common.collect.Iterators.addAll(Ljava/util/Collection;Ljava/util/Iterator;)Z (41 bytes) @ 0x000078b3dd9197e8 [0x000078b3dd919680+0x0000000000000168]
J 38256 c1 org.apache.cassandra.service.paxos.uncommitted.PaxosRows.toIterator(Lorg/apache/cassandra/db/partitions/UnfilteredPartitionIterator;Lorg/apache/cassandra/schema/TableId;Z)Lorg/apache/cassandra/utils/CloseableIterator; (49 bytes) @ 0x000078b3d6b677ac [0x000078b3d6b672e0+0x00000000000004cc]
J 34823 c1 org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedIndex.repairIterator(Lorg/apache/cassandra/schema/TableId;Ljava/util/Collection;)Lorg/apache/cassandra/utils/CloseableIterator; (212 bytes) @ 0x000078b3d5675e0c [0x000078b3d5673be0+0x000000000000222c]
J 38259 c1 org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedTracker.uncommittedKeyIterator(Lorg/apache/cassandra/schema/TableId;Ljava/util/Collection;)Lorg/apache/cassandra/utils/CloseableIterator; (116 bytes) @ 0x000078b3d6b6bc54 [0x000078b3d6b6b7e0+0x0000000000000474]
J 38257 c1 org.apache.cassandra.service.StorageService.autoRepairPaxos(Lorg/apache/cassandra/schema/TableId;)Lorg/apache/cassandra/utils/concurrent/Future; (57 bytes) @ 0x000078b3d6b6902c [0x000078b3d6b68e00+0x000000000000022c]
j  org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedTracker.schedulePaxosAutoRepairs()V+146
j  org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedTracker$$Lambda$1773.run()V+4
J 39703 c1 org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedTracker.runAndLogException(Ljava/lang/String;Ljava/lang/Runnable;)V (39 bytes) @ 0x000078b3d435adfc [0x000078b3d435ad00+0x00000000000000fc]
j  org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedTracker.maintenance()V+19
j  org.apache.cassandra.service.paxos.uncommitted.PaxosUncommittedTracker$$Lambda$1534.run()V+4
J 30376 c2 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run()V java.base@11.0.22 (57 bytes) @ 0x000078b3dd56543c [0x000078b3dd565100+0x000000000000033c]
J 27255% c2 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V java.base@11.0.22 (187 bytes) @ 0x000078b3dd114d58 [0x000078b3dd114ac0+0x0000000000000298]
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5 java.base@11.0.22
j  io.netty.util.concurrent.FastThreadLocalRunnable.run()V+4
j  java.lang.Thread.run()V+11 java.base@11.0.22
v  ~StubRoutines::call_stub
V  [libjvm.so+0x877453]  JavaCalls::call_helper(JavaValue*, methodHandle const&, JavaCallArguments*, Thread*)+0x373
V  [libjvm.so+0x875a96]  JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, Thread*)+0x186
V  [libjvm.so+0x925653]  thread_entry(JavaThread*, Thread*)+0xa3
V  [libjvm.so+0xe41391]  JavaThread::thread_main_inner()+0x131
V  [libjvm.so+0xe3d790]  Thread::call_run()+0x140
V  [libjvm.so+0xbf97de]  thread_native_entry(Thread*)+0xee
{noformat}
"
CASSANDRA-19667,update deps in netbeans project file (2024-05),Dependencies have changed and the {{ide/nbproject/project.xml}} file needs to be updated accordingly.
CASSANDRA-19665,Add download link for Java drivers from download page,Seems like we should have a link to download Java drivers from the [download page|https://cassandra.apache.org/_/download.html] on the Website
CASSANDRA-19664,Accord Journal Determinism: PreAccept replay stability ,"Currently, some messages, such as PreAccept can have some of their context initialized on replay. This patch adds a concept of Context to Journal that can be used for arbitrary information necessary for replaying them just the way they were executed the first time."
CASSANDRA-19663,trunk fails to start,"On commit {{67bbbb01259bce91672a7c3ca9fb77ea7b040e9c}}, I get errors on startup.

Verified the build was successful:

{noformat}
    easy-cass-lab.amazon-ebs.ubuntu: BUILD SUCCESSFUL
    easy-cass-lab.amazon-ebs.ubuntu: Total time: 1 minute 41 seconds
{noformat}

Running on a new Ubuntu instance:

{noformat}
INFO  [main] 2024-05-24 18:31:16,397 YamlConfigurationLoader.java:103 - Configuration location: file:/usr/local/cassandra/trunk/conf/cassandra.yaml
ERROR [main] 2024-05-24 18:31:16,470 CassandraDaemon.java:900 - Exception encountered during startup
java.lang.NoSuchMethodError: 'void org.yaml.snakeyaml.LoaderOptions.setCodePointLimit(int)'
	at org.apache.cassandra.config.YamlConfigurationLoader.getDefaultLoaderOptions(YamlConfigurationLoader.java:433)
	at org.apache.cassandra.config.YamlConfigurationLoader$CustomConstructor.<init>(YamlConfigurationLoader.java:278)
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:135)
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:116)
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:403)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:265)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:250)
	at org.apache.cassandra.service.CassandraDaemon.applyConfig(CassandraDaemon.java:781)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:724)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:878)
{noformat}

Running on Java 17:

{noformat}
ubuntu@cassandra0:~$ java -version
openjdk version ""17.0.10"" 2024-01-16
OpenJDK Runtime Environment (build 17.0.10+7-Ubuntu-122.04.1)
OpenJDK 64-Bit Server VM (build 17.0.10+7-Ubuntu-122.04.1, mixed mode, sharing)
{noformat}

Built with 11.

The only configs I changed:

{noformat}
cluster_name: ""system_views""
num_tokens: 4
seed_provider:
  class_name: ""org.apache.cassandra.locator.SimpleSeedProvider""
  parameters:
    seeds: ""10.0.0.225""
hints_directory: ""/mnt/cassandra/hints""
data_file_directories:
- ""/mnt/cassandra/data""
commitlog_directory: ""/mnt/cassandra/commitlog""
concurrent_reads: 64
concurrent_writes: 64
trickle_fsync: true
endpoint_snitch: ""Ec2Snitch""
{noformat}
"
CASSANDRA-19661,Cannot restart Cassandra 5 after creating a vector table and index,"I'm using llama-index and llama3 to train a model. I'm using a very simple code that reads some *.txt files from local and uploads them to Cassandra and then creates the index:

 
{code:java}
# Create the index from documents
index = VectorStoreIndex.from_documents(
    documents,
    service_context=vector_store.service_context,
    storage_context=storage_context,
    show_progress=True,
    ) {code}
This works well and I'm able to use a Chat app to get responses from the Cassandra data. however, right after, I cannot restart Cassandra. It'll break with the following error:

 
{code:java}
INFO  [PerDiskMemtableFlushWriter_0:7] 2024-05-23 08:23:20,102 Flushing.java:179 - Completed flushing /data/cassandra/data/gpt/docs_20240523-10c8eaa018d811ef8dadf75182f3e2b4/da-6-bti-Data.db (124.236MiB) for commitlog position CommitLogPosition(segmentId=1716452305636, position=15336)
[...]
WARN  [MemtableFlushWriter:1] 2024-05-23 08:28:29,575 MemtableIndexWriter.java:92 - [gpt.docs.idx_vector_docs] Aborting index memtable flush for /data/cassandra/data/gpt/docs-aea77a80184b11ef8dadf75182f3e2b4/da-3-bti...{code}
{code:java}
java.lang.IllegalStateException: null
        at com.google.common.base.Preconditions.checkState(Preconditions.java:496)
        at org.apache.cassandra.index.sai.disk.v1.vector.VectorPostings.computeRowIds(VectorPostings.java:76)
        at org.apache.cassandra.index.sai.disk.v1.vector.OnHeapGraph.writeData(OnHeapGraph.java:313)
        at org.apache.cassandra.index.sai.memory.VectorMemoryIndex.writeDirect(VectorMemoryIndex.java:272)
        at org.apache.cassandra.index.sai.memory.MemtableIndex.writeDirect(MemtableIndex.java:110)
        at org.apache.cassandra.index.sai.disk.v1.MemtableIndexWriter.flushVectorIndex(MemtableIndexWriter.java:192)
        at org.apache.cassandra.index.sai.disk.v1.MemtableIndexWriter.complete(MemtableIndexWriter.java:117)
        at org.apache.cassandra.index.sai.disk.StorageAttachedIndexWriter.complete(StorageAttachedIndexWriter.java:185)
        at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
        at java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1085)
        at org.apache.cassandra.io.sstable.format.SSTableWriter.commit(SSTableWriter.java:289)
        at org.apache.cassandra.db.compaction.unified.ShardedMultiWriter.commit(ShardedMultiWriter.java:219)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1323)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1222)
        at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829) {code}
The table created by the script is as follows:

 
{noformat}
CREATE TABLE gpt.docs (
    partition_id text,
    row_id text,
    attributes_blob text,
    body_blob text,
    vector vector<float, 1024>,
    metadata_s map<text, text>,
    PRIMARY KEY (partition_id, row_id)
) WITH CLUSTERING ORDER BY (row_id ASC)
    AND additional_write_policy = '99p'
    AND allow_auto_snapshot = true
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND cdc = false
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.UnifiedCompactionStrategy', 'scaling_parameters': 'T4', 'target_sstable_size': '1GiB'}
    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND memtable = 'default'
    AND crc_check_chance = 1.0
    AND default_time_to_live = 0
    AND extensions = {}
    AND gc_grace_seconds = 864000
    AND incremental_backups = true
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair = 'BLOCKING'
    AND speculative_retry = '99p';

CREATE CUSTOM INDEX eidx_metadata_s_docs ON gpt.docs (entries(metadata_s)) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';

CREATE CUSTOM INDEX idx_vector_docs ON gpt.docs (vector) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex';{noformat}


Thank you

 "
CASSANDRA-19658,Test failure: replace_address_test.py::TestReplaceAddress::test_restart_failed_replace,"This can be seen failing in butler: https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-5.0/failure/replace_address_test/TestReplaceAddress/test_restart_failed_replace

{noformat}
ccmlib.node.TimeoutError: 14 May 2024 18:19:08 [node1] after 120.13/120 seconds Missing: ['FatClient /127.0.0.4:7000 has been silent for 30000ms, removing from gossip'] not found in system.log:
{noformat} "
CASSANDRA-19656,Revisit disabling chronicle analytics,"We first considered this in CASSANDRA-18538 but determined it wasn't a problem.  We have upgraded chronicle in CASSANDRA-18049 so we should reconfirm with packet analysis that nothing is phoning home, and perhaps consider taking further precautions by proactively disabling it."
CASSANDRA-19651,idealCLWriteLatency metric reports the worst response time instead of the time when ideal CL is satisfied,"org.apache.cassandra.service.AbstractWriteResponseHandler:
{code:java}
private final void decrementResponseOrExpired()
{
    int decrementedValue = responsesAndExpirations.decrementAndGet();
    if (decrementedValue == 0)
    {
        // The condition being signaled is a valid proxy for the CL being achieved
        // Only mark it as failed if the requested CL was achieved.
        if (!condition.isSignalled() && requestedCLAchieved)
        {
            replicaPlan.keyspace().metric.writeFailedIdealCL.inc();
        }
        else
        {
            replicaPlan.keyspace().metric.idealCLWriteLatency.addNano(nanoTime() - queryStartNanoTime);
        }
    }
} {code}
Actual result: responsesAndExpirations is a total number of replicas across all DCs which does not depend on the ideal CL, so the metric value for replicaPlan.keyspace().metric.idealCLWriteLatency is updated when we get the latest response/timeout for all replicas.
Expected result: replicaPlan.keyspace().metric.idealCLWriteLatency is updated when we get enough responses from replicas according to the ideal CL."
CASSANDRA-19650,CCM wrongly interprets CASSANDRA_USE_JDK11 for Cassandra 4.x,"CCM interprets {{CASSANDRA_USE_JDK11}} only by its existence in the environment rather than by its actual value (true/false). 

I can see two solutions:
- make it interpret {{CASSANDRA_USE_JDK11}} properly
- do not take into account {{CASSANDRA_USE_JDK11}} in the current env and set it or unset it automatically when starting a node basing on which Java version was selected
"
CASSANDRA-19649,add absurdfarce's gpg key to project's KEYS file,"The patch adds my gpg public key to the project's KEYS file found at [https://dist.apache.org/repos/dist/release/cassandra/KEYS]

My gpg public key here has the fingerprint 
 498AAC354AA5CB36FAAB7608B6E83A2D2E447E56

References:
 - [https://www.apache.org/dev/release-signing#keys-policy]
 - [http://www.apache.org/legal/release-policy.html]"
CASSANDRA-19648,Flaky test: StartupChecksTest#testKernelBug1057843Check() on Non-Linux OS,Flaky test: StartupChecksTest#testKernelBug1057843Check() cannot pass in my MacOs(maybe Windows OS). Just skip this test when tested on Non-Linux OS
CASSANDRA-19645,Mismatch of number of args of String.format() in three classes,"Affected classes:

GossipHelper lines 196-197
SchemaGenerators line 488
StorageService line 1087
I'm goind to provide a PR"
CASSANDRA-19644,deterministic token allocation combined with slow gossip propogation can lead to data loss,"I've seen several cases now where starting nodes within a somewhat short time window (about a minute) when using the default allocation tokens for RF leads to token conflicts.  Unfortunately this can easily go undetected with medium to large clusters.

When this happens, different nodes in the cluster will have different understandings of the topology of the cluster.  I've seen this go unnoticed in a production environment for several months, leading to data loss, data resurrection, and other odd behavior.

We should apply some randomness to the tokens to ensure that even in the case of 1 nodes starting at once, it's still unlikely that they will ever have a conflict.  Applying a random() value to the token value between - 2^8 and 2^8 makes this statistically very, very unlikely that we'll ever have a collision while also preserving the balance of token distribution in the ring.  In the case of 2 nodes starting at the same time, the operator will have weird token distribution instead of data loss.

 
{noformat}
INFO  [GossipStage:1] 2024-05-17 22:16:12,333 StorageService.java:3006 - Nodes /10.0.2.134:7000 and cassandra1/10.0.1.61:7000 have the same token -1938510198161598815. /10.0.2.134:7000 is the new owner
INFO  [GossipStage:1] 2024-05-17 22:16:12,333 StorageService.java:3006 - Nodes /10.0.2.134:7000 and cassandra1/10.0.1.61:7000 have the same token -3478858378222500629. /10.0.2.134:7000 is the new owner
INFO  [GossipStage:1] 2024-05-17 22:16:12,333 StorageService.java:3006 - Nodes /10.0.2.134:7000 and cassandra1/10.0.1.61:7000 have the same token 3562748272064835315. /10.0.2.134:7000 is the new owner
INFO  [GossipStage:1] 2024-05-17 22:16:12,333 StorageService.java:3006 - Nodes /10.0.2.134:7000 and cassandra1/10.0.1.61:7000 have the same token 8085185010613503278. /10.0.2.134:7000 is the new owner{noformat}"
CASSANDRA-19642,IndexOutOfBoundsException while serializing CommandsForKey,When serializing CommandsForKey we have a concept of “unmanaged” but tests didn’t cover this… when we have anything unmanaged we fail to serialize with a IndexOutOfBounds due to using the ValueAccessor API incorrectly
CASSANDRA-19641,Accord barriers/inclusive sync points cause failures in BurnTest,The burn test fails almost every run at the moment we found several things to fix.
CASSANDRA-19637,LWT conditions behavior on collections is inconsistent,"LWT conditions behaviour on collections is inconsistent in several ways around null values:

1)+Conditions comparing a collection column with a {{null}} value to a non-null have a different behaviour for frozen and non-frozen collection+.

 {code}UPDATE myTable SET l = ? WHERE k = 0 IF l < [1, 2]{code}
If {{l}} is null the previous query will return {{[false, null]}} for a frozen collection and {{[true]}} for a non-frozen collection. 

2) +Conditions on non-frozen collection treat empty differently from null+

Due to the way multi-cell collections are implemented, it is not possible to differentiate between {{null}} and empty collections like it is feasible for single cell (frozen) collections. Therefore an empty multi-cell collection will always be treated as {{null}}.
Unfortunately, the way LWT conditions handle that is not consistent with that.

For example for {{colA list<int>}} non null: {code}.. IF colA >= null{code} will throw an invalid request error whereas {code}..IF colA >= []{code} will returns {{true}}.
Moreover, if we insert an empty list through:
{code}INSERT INTO mytable (pk, colA) VALUES (1, []);{code}
and use {code}DELETE FROM mytable WHERE pk=1 IF colA = []{code} the returned results will be {code}{false, null}{code}. Which can be quite confusing.

The way to fix that behaviour to make it consistent with other operations is to consider empty multi-cell collection input as {{null}} and reject the {{null}} input for non {{=}} and {{!=}} operators.    

  "
CASSANDRA-19636,Fix CCM for Cassandra 5.0 and add arg to the command line which let the user explicitly select JVM,"CCM fails to select the right Java version for Cassandra 5 binary distribution.

There are also two additional changes proposed here:
 * add {{--jvm-version}} argument to let the user explicitly select Java version when starting a node from command line
 * fail if {{java}} command is available on the {{PATH}} and points to a different Java version than Java distribution defined in {{JAVA_HOME}} because there is no obvious way for the user to figure out which one is going to be used

 "
CASSANDRA-19635,"Update target Cassandra versions for integration tests, support new 5.0.x","{color:#172b4d}[CASSANDRA-19292|https://issues.apache.org/jira/browse/CASSANDRA-19292] added support for running integration tests against Cassandra 4.1.x but we still need the ability to run against Cassandra 5.0.x.  As of this writing we need [riptano/ccm|https://github.com/riptano/ccm] to manage Cassandra 5.0.x clusters.  The DataStax CI infrastructure, however, uses a private fork of ccm which adds the ability to manage DSE clusters (something riptano/ccm can't do right now).  So we presumably need to do one of the following:
{color}
 * Port Cassandra 5.0.x support to the private fork
 * Port DSE support to riptano/ccm
 * Change the build process to install both riptano/ccm and the private fork into distinct venvs and manage accordingly"
CASSANDRA-19634,[Analytics] Improve test coverage for downed instances,"This improvement adds test coverage for Cassandra Analytics. We test different scenarios during bulk writes. For example, bulk writes when downed Cassandra instances are encountered. We use different consistency levels, and we ensure the expected success or failure is achieved depending on the configured consistency level. Similarly, we test for when Sidecar instances are down under different consistency levels."
CASSANDRA-19632,wrap tracing logs in isTraceEnabled across the codebase,"Our usage of logger.isTraceEnabled across the codebase is inconsistent. This would also fix issues similar in e.g. CASSANDRA-19429 as [~rustyrazorblade] suggested.

We should fix this at least in trunk and 5.0 (not critical though) and probably come up with a checkstyle rule to prevent not calling isTraceEnabled while logging with TRACE level. "
CASSANDRA-19629,Upgrade from 4.1.4 to 5.0 crashes with CorruptSSTableException,"When migrating data from 4.1.4 to 5.0 (commit: ccdeb12), the upgrade crashed with the following exception
{code:java}
ERROR [SSTableBatchOpen:1] 2024-05-09 16:25:04,564 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""
org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /home/klay/system/cassandra/apache-cassandra-5.0/bin/../data/data/ks/tb-9f7e6da00e2011efa77a0bfeb6733ccc/nb-1-big
    at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)
    at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)
    at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)
    at org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)
    at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
    at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
    at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.AssertionError: null
    at org.apache.cassandra.db.RegularAndStaticColumns$Builder.add(RegularAndStaticColumns.java:166)
    at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:327)
    at org.apache.cassandra.io.sstable.format.StatsComponent.serializationHeader(StatsComponent.java:85)
    at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:78)
    at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)
    at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)
    ... 10 common frames omitted
{code}
h1. Reproduce

Start up one 4.1.4 node using default configuration and execute the following command
{code:java}
CREATE KEYSPACE ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
CREATE TABLE ks.tb (c1 INT, c2 INT, PRIMARY KEY (c1));
INSERT INTO ks.tb (c1, c2) VALUES (0,0);
ALTER TABLE ks.tb DROP c2 ;
ALTER TABLE ks.tb RENAME c1 TO c2;
{code}
Drain and upgrade to 5.0 (commit: ccdeb12)
{code:java}
bin/nodetool drain
bin/nodetool stopdaemon{code}
The upgrade would crash with the following exception
{code:java}
ERROR [SSTableBatchOpen:1] 2024-05-09 16:25:04,564 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""
org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /home/klay/system/cassandra/apache-cassandra-5.0/bin/../data/data/ks/tb-9f7e6da00e2011efa77a0bfeb6733ccc/nb-1-big
    at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)
    at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)
    at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)
    at org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)
    at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
    at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
    at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.AssertionError: null
    at org.apache.cassandra.db.RegularAndStaticColumns$Builder.add(RegularAndStaticColumns.java:166)
    at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:327)
    at org.apache.cassandra.io.sstable.format.StatsComponent.serializationHeader(StatsComponent.java:85)
    at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:78)
    at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)
    at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)
    ... 10 common frames omitted{code}
I attached a (1) data file. Use this file to start up 5.0 can easily reproduce it and (2)  system.log file."
CASSANDRA-19628,Correct testing instructions on the website,"At https://cassandra.apache.org/_/development/testing.html it says to issue these statements for cqlsh tests:

{noformat}
ccm updateconf ""enable_user_defined_functions: true""
ccm updateconf ""enable_scripted_user_defined_functions: true""
ccm updateconf ""cdc_enabled: true""
{noformat}

But these actually break the configuration so it won't start."
CASSANDRA-19626,[Analytics] NullPointerException when reading static column with null values,"The analytics library fails to read static columns that has null values depending on the type. For example, this problem is exhibited in the Timestamp type, where the deserialized object is null, and the type will attempt to get the time from the field. When that occurs it triggers the NullPointerException."
CASSANDRA-19622,Test failure: org.apache.cassandra.distributed.test.ClearSnapshotTest.clearSnapshotSlowTest-_jdk17,"https://app.circleci.com/pipelines/github/driftx/cassandra/1625/workflows/d3849d2a-987e-49f8-9f4b-916c7ce85279/jobs/88752/tests

{quote}
junit.framework.AssertionFailedError
	at org.apache.cassandra.distributed.test.ClearSnapshotTest.clearSnapshotSlowTest(ClearSnapshotTest.java:123)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{quote}"
CASSANDRA-19621,Test failure: gossip_test.TestGossip::test_2dc_parallel_startup,"As seen at https://app.circleci.com/pipelines/github/driftx/cassandra/1625/workflows/d3849d2a-987e-49f8-9f4b-916c7ce85279/jobs/88756/tests:

{quote}
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node4] 'ERROR [Messaging-EventLoop-3-3] 2024-05-07 20:10:54,261 NoSpamLogger.java:110 - /127.0.0.4:7004->/127.0.0.1:7001-URGENT_MESSAGES-[no-channel] failed to connect\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.OutboundConnectionInitiator$Handler.channelInactive(OutboundConnectionInitiator.java:322)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:411)\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:376)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:305)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:274)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:301)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:281)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:413)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)']""
{quote}"
CASSANDRA-19620,Refactor ColumnCondition post CASSANDRA-19341,"CASSANDRA-19341 added ColumnsExpression in restrictions. 
This ticket's target is to extend the kinds of ColumnsExpression and use them in conditions, too. 
Also, it should add a few other simplifications around ColumnCondition, which were suggested by [~blerer] "
CASSANDRA-19619,Enforce contract for internal metrics naming,"Metrics have their internal representation which uniquely identifies a particular metric name. The name is formed as {{<metric group>.<metric type>.<metric name>.<metric scope>}}. For some metrics, the scope includes a metric name that is not necessary for uniqueness, this can be simplified.

For example,

{code}
// AS IS
org.apache.cassandra.metrics.StorageAttachedIndex.BalancedTreeIntersectionEarlyExits.my_keyspace.my_table.ColumnQueryMetrics.BalancedTreeIntersectionEarlyExits

// TO BE
org.apache.cassandra.metrics.StorageAttachedIndex.BalancedTreeIntersectionEarlyExits.my_keyspace.my_table.ColumnQueryMetrics
{code}

The metrics are filtered based on knowledge of how they are formatted, so having a metric scope without a built-in metric name also simplifies the way we filter metrics."
CASSANDRA-19618,Accord: Need to simulate Cassandra Journal in Accord BurnTest to detect issues earlier before they are seen in Cassandra,"Right now Cassandra splits a txn data into 2 places: system table, and journal (commit log like structure), when we read we read from both places and zip together.  One issue we face is the logic to join them together is in Accord and kept failing as we would hit unexpected states.

We should simulate the journal logic in BurnTest and call this reconstruct logic so we improve test coverage before Cassandra sees it"
CASSANDRA-19617,Paxos may re-distribute stale commits that predate a collectable tombstone,"Note: this bug only affects {{paxos_state_purging: {gc_grace, repaired}}}, i.e. those introduced alongside Paxos v2.

There are two problems:
1) Purging is applied only on compaction, not on load, which can lead to very old commits being resurfaced in certain circumstances
2) PaxosPrepare does not filter commits based on paxos repair low bound

This permits surprising situations to arise, where some replicas purge a stale commit _and all newer commits_, but due to compaction peculiarities some other replica may purge only the newer commits, leaving a stale commit in some compaction ""purgatory""\[1] to be returned to reads indefinitely. 

So long as there are no newer commits, the paxos coordinator will see this commit is not universally known and redistribute it - no matter how old it is. This can permit an insert to be reapplied after GC grace has elapsed and the tombstone has been collected.

For proposals this is not a problem, as we correctly filter proposals based on the last paxos repair time. This also does not affect clusters with the legacy (and default) paxos state purging using TTL. Problem (1) only applies also to the new {{gc_grace}} compatibility mode for purging.

\[1] Compaction purgatory can arise for instance because paxos purging allows whole sstables to be erased quite effectively, and if this is able to ordinarily prevent sstables being promoted to L1, then if for some abnormal reason sstables reach L1 (e.g. repairs being disabled for some time), those that collect may remain uncompacted for an extended period without purging being applied."
CASSANDRA-19616,Integrate with the latest sidecar client,The patch updates the analytics code to consume the latest sidecar client after CASSANDRASC-127.
CASSANDRA-19615,Merge pre-existing schema with the system defined one during upgrade,"When upgrading we should merge the pre-existing schema with the system-defined schema. For example, if a table was defined in 5.0 in system_distributed, but then removed from SystemDistributedKeyspace.java in 5.1 we should still be able to read it (until manually dropped)."
CASSANDRA-19614,Add java version to prepare_release.sh human check step,"With the many versions of java one may need to float these days, it would be nice for prepare_release.sh to show the version you are using when it asks the ""Is this what you want?"" question showing the latest commit, that way there is a chance to correct things before it tags."
CASSANDRA-19613,Add ClusterMetadata.metadataIdentifier to GossipDigestSyn messages,"We should add \{{ClusterMetadata.instance().metadataIdentifier}} to \{{GossipDigestSyn}} messages and compare with the local one, rejecting anything that has the wrong identifier like we do with cluster name."
CASSANDRA-19612,flaky IntersectFilteringQueryTest,"I start to see flaky IntersectFilteringQueryTest both in 5.0 and trunk.

 
{code:java}
      org.apache.cassandra.distributed.test.guardrails.IntersectFilteringQueryTest shouldNotWarnOrFailOnIndexQuery
        com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency QUORUM (2 responses were required but only 1 replica responded, 1 failed)
                at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:180)
                at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:30)
                at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:35)
                at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:293)
                at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:58)
                at org.apache.cassandra.distributed.test.guardrails.GuardrailTester.executeViaDriver(GuardrailTester.java:92)
                at org.apache.cassandra.distributed.test.guardrails.IntersectFilteringQueryTest.shouldNotWarnOrFailOnIndexQuery(IntersectFilteringQueryTest.java:115)
                at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
                at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        Caused by: com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency QUORUM (2 responses were required but only 1 replica responded, 1 failed)
                at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:192)
                at com.datastax.driver.core.Responses$Error.asException(Responses.java:181)
 {code}
 

It most probably needs a little bit more love and run it through multiplexer maybe?"
CASSANDRA-19607,Compaction double reads every chunk,"I was taking a look at the I/O operations performed during compaction and noticed it looks like we're double reading every chunk of the filesystem, meaning we perform 2x the system calls that we need to.  The second read will come from page cache, but this isn't free, so we should try to avoid it.  Here's the steps to reproduce.

First, wrote a small amount of data to a table and flush.  I have a single SSTable that's 151KB here:

{noformat}
-rw-r--r-- 1 cassandra  127 May  1 21:00 da-8-bti-CompressionInfo.db
-rw-r--r-- 1 cassandra 151K May  1 21:00 da-8-bti-Data.db
-rw-r--r-- 1 cassandra   10 May  1 21:00 da-8-bti-Digest.crc32
-rw-r--r-- 1 cassandra  136 May  1 21:00 da-8-bti-Filter.db
-rw-r--r-- 1 cassandra  829 May  1 21:00 da-8-bti-Partitions.db
-rw-r--r-- 1 cassandra    0 May  1 21:00 da-8-bti-Rows.db
-rw-r--r-- 1 cassandra 5.0K May  1 21:00 da-8-bti-Statistics.db
-rw-r--r-- 1 cassandra   94 May  1 21:00 da-8-bti-TOC.txt
{noformat}

Then watch all reads by doing the following:

{noformat}
$ xfsslower -p $PID 0
{noformat}

Next, start a event profiling session on ChannelProxy.read by doing the following with async-profiler:

{noformat}
$ asprof -e org.apache.cassandra.io.util.ChannelProxy.read -d 10 $(cassandra-pid) -f /mnt/cassandra/artifacts/channelproxy-read.txt
{noformat}

Then I started a compaction on the table which lasted roughly a second.

I took the log of filesystem operations and pulled out all the accesses to `da-8-bti-Data.db` using ripgrep:

{noformat}
$ rg da-8-bti-Data.db artifacts/cassandra1/xfsslower-da8.txt
{noformat}

The headings are:

{noformat}
TIME     COMM           PID    T BYTES   OFF_KB   LAT(ms) FILENAME
{noformat}

The data:

{noformat}
27:21:07:05 CompactionExec 30550  R 16387   0           0.02 da-8-bti-Data.db
28:21:07:05 CompactionExec 30550  R 16387   0           0.00 da-8-bti-Data.db
40:21:07:05 CompactionExec 30550  R 16436   16          0.01 da-8-bti-Data.db
41:21:07:05 CompactionExec 30550  R 16436   16          0.01 da-8-bti-Data.db
44:21:07:05 CompactionExec 30550  R 16419   32          0.01 da-8-bti-Data.db
45:21:07:05 CompactionExec 30550  R 16419   32          0.01 da-8-bti-Data.db
48:21:07:05 CompactionExec 30550  R 16441   48          0.01 da-8-bti-Data.db
49:21:07:05 CompactionExec 30550  R 16441   48          0.00 da-8-bti-Data.db
52:21:07:05 CompactionExec 30550  R 16421   64          0.01 da-8-bti-Data.db
53:21:07:05 CompactionExec 30550  R 16421   64          0.01 da-8-bti-Data.db
56:21:07:05 CompactionExec 30550  R 16415   80          0.01 da-8-bti-Data.db
57:21:07:05 CompactionExec 30550  R 16415   80          0.01 da-8-bti-Data.db
60:21:07:05 CompactionExec 30550  R 16439   96          0.01 da-8-bti-Data.db
61:21:07:05 CompactionExec 30550  R 16439   96          0.00 da-8-bti-Data.db
64:21:07:05 CompactionExec 30550  R 16390   112         0.01 da-8-bti-Data.db
65:21:07:05 CompactionExec 30550  R 16390   112         0.00 da-8-bti-Data.db
68:21:07:05 CompactionExec 30550  R 16384   128         0.01 da-8-bti-Data.db
69:21:07:05 CompactionExec 30550  R 16384   128         0.01 da-8-bti-Data.db
72:21:07:05 CompactionExec 30550  R 6019    144         0.00 da-8-bti-Data.db
73:21:07:05 CompactionExec 30550  R 6019    144         0.00 da-8-bti-Data.db
{noformat}

There are 20 IO operations reported here, each of which occurs twice.  I was originally thinking this might be a bug with the reporting, but other files operations are listed only once.  For example:

{noformat}
21:07:05 CompactionExec 30550  W 16386   112         0.09 da-9-bti-Data.db
21:07:05 CompactionExec 30550  W 4       128         0.00 da-9-bti-Data.db
{noformat}

Looking at the output, the last read is approximately 6KB and starts at the 144KB offset which matches up to the total length of the file.

The profiler output indicates there's 2 code paths that arrive at the FS operation 9 times.

I've also attached the profiler output, which shows 22 calls to org.apache.cassandra.io.util.ChannelProxy.read.  Two of these originate in the Stats file, leaving 9 chunks in this file, read twice, plus 2 additional calls.  "
CASSANDRA-19606,Fix building debian packages,"Trying to run cassandra-deb-packaging.sh will result in the docker image looping:

{noformat}
Errors were encountered while processing:
 ed
 quilt
 cassandra-build-deps
E: Sub-process /usr/bin/dpkg returned an error code (1)
(Reading database ... 36721 files and directories currently installed.)
Removing cassandra-build-deps (5.0~beta2-20240501gitae9be29918) ...
mk-build-deps: Unable to install all build-dep packages
mk-build-deps failed… trying again after 10s… 
{noformat}"
CASSANDRA-19605,Accord: NPE in RangeDeps.forEach,"{code}
java.lang.NullPointerException
	accord.primitives.RangeDeps.visitTxnIdxsForRangeIndex(RangeDeps.java:249)
	accord.utils.CheckpointIntervalArray.forEach(CheckpointIntervalArray.java:219)
	accord.utils.CheckpointIntervalArray.forEach(CheckpointIntervalArray.java:127)
	accord.utils.CheckpointIntervalArray.forEach(CheckpointIntervalArray.java:97)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:178)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:202)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:281)
	accord.primitives.RangeDeps.forEach(RangeDeps.java:273)
	org.apache.cassandra.service.accord.AccordSafeCommandStore.registerHistoricalTransactions(AccordSafeCommandStore.java:190)
{code}"
CASSANDRA-19604,Add support for BETWEEN operator,"CQL support the {{>=}} and {{<=}} but does not support yet the {{BETWEEN}} operator. After CASSANDRA-19341 adding new operators should be much simpler and safer than it use to be.
For the scope of this ticket {{BETWEEN}} support should be added for {{WHERE}} clauses of {{SELECT}} and {{DELETE}} queries (for single column and multi-column restrictions). NOT BETWEEN should be added and should be supported everywhere BETWEEN is.

+Additional information for newcomers:+
Parts that will need to be modified:
 * {{Lexer.g}} and {{Parser.g}} to add support for the new keyword and syntax
 * The {{Operator.class}} to add the new {{BETWEEN}} operator
 * Unit tests in {{SelectSingleColumnRelationTest}} and {{SelectMultiColumnRelationTest}} classes for the different types of columns (partition key, clustering, static and regular).
 * CQLSH auto completion in {{cql3handling.py}} and test for it in {{test_cqlsh_completion.py}}
 * Update the documentation

Of course this is just an overview and some other parts might have to be changed as well."
CASSANDRA-19600,Resolve the oldest hints just from descriptors and current writer if available,"We should not resolve hints from buffers as these are just very short-lived / transient data structures and resolving ""the oldest hints"" from buffers too just does not make sense when we should just look into descriptors and current writer's descriptor at most."
CASSANDRA-19599,Remove unused config params for out of range token requests,"The fields {{log_out_of_token_range_requests}} and {{reject_out_of_token_range_requests}} in {{Config.java}} have never actually been used and are just vestiges from early development on CEP-21. 
We should remove them and the related accessors in {{DatabaseDescriptor}}.
"
CASSANDRA-19592,Expand CREATE TABLE CQL on a coordinating node before submitting to CMS,"This is done to unblock CASSANDRA-12937 and allow preserving defaults with which the table was created between node bounces and between nodes with different configurations. For now, we are preserving 5.0 behaviour."
CASSANDRA-19591,MarshalException when migrate data from 2.2.19 to 3.11.17,"When migrate data from 2.2.19 to 3.11.17, I encountered the following exception and the migration fails.
{code:java}
ERROR [main] 2024-04-25 19:41:22,996 JVMStabilityInspector.java:124 - Exiting due to error while processing commit log during initialization.
org.apache.cassandra.db.commitlog.CommitLogReadHandler$CommitLogReadException: Unexpected error deserializing mutation; saved to /tmp/mutation3085092904780349005dat.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception follows: org.apache.cassandra.serializers.MarshalException: Expected 4 or 0 byte int (2)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readMutation(CommitLogReader.java:471)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readSection(CommitLogReader.java:404)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:251)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readAllFiles(CommitLogReader.java:132)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommitLogReplayer.java:137)
        at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:189)
        at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:170)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:331)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:630)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:791) {code}
h1. Reproduce

1. Start up single node cassandra-2.2.19 with default configuration and execute the following commands
{code:java}
CREATE KEYSPACE ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
CREATE TABLE ks.tb (c0 INT,c2 TEXT, PRIMARY KEY (c0));
INSERT INTO ks.tb (c0, c2) VALUES (1,'BB');
ALTER TABLE ks.tb DROP c2 ;
ALTER TABLE ks.tb ADD c2 INT ; {code}
2. Stop the 2.2 node
{code:java}
bin/nodetool -h ::FFFF:127.0.0.1 flush
bin/nodetool -h ::FFFF:127.0.0.1 stopdaemon; {code}
3. Copy the data to 3.11.17 folder and start up, it will expose the following exception during the start up process. The node cannot start up.
{code:java}
ERROR [main] 2024-04-25 19:41:22,996 JVMStabilityInspector.java:124 - Exiting due to error while processing commit log during initialization.
org.apache.cassandra.db.commitlog.CommitLogReadHandler$CommitLogReadException: Unexpected error deserializing mutation; saved to /tmp/mutation3085092904780349005dat.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception follows: org.apache.cassandra.serializers.MarshalException: Expected 4 or 0 byte int (2)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readMutation(CommitLogReader.java:471)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readSection(CommitLogReader.java:404)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:251)
        at org.apache.cassandra.db.commitlog.CommitLogReader.readAllFiles(CommitLogReader.java:132)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommitLogReplayer.java:137)
        at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:189)
        at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:170)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:331)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:630)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:791) {code}
I have attached the system.log and data.tar.gz. (use 3.11.17 to start up with this data can directly expose the error).
h1. Drain before upgrade

If drain before upgrade, the upgrade will succeed but the read fails with the following exception
{code:java}
cqlsh> SELECT * FROM ks.tb;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'} {code}
System log
{code:java}
ERROR [ReadStage-2] 2024-04-27 02:18:17,105 AbstractLocalAwareExecutorService.java:166 - Uncaught exception on thread Thread[ReadStage-2,10,main]
java.lang.RuntimeException: org.apache.cassandra.serializers.MarshalException: Expected 4 or 0 byte int (2)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2777)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:113)
        at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.cassandra.serializers.MarshalException: Expected 4 or 0 byte int (2)
        at org.apache.cassandra.serializers.Int32Serializer.validate(Int32Serializer.java:42)
        at org.apache.cassandra.db.marshal.AbstractType.validate(AbstractType.java:164)
        at org.apache.cassandra.db.marshal.AbstractType.validateIfFixedSize(AbstractType.java:427)
        at org.apache.cassandra.db.LegacyLayout$CellGrouper.addCell(LegacyLayout.java:1488)
        at org.apache.cassandra.db.LegacyLayout$CellGrouper.addAtom(LegacyLayout.java:1407)
        at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.readRow(UnfilteredDeserializer.java:555)
        at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.hasNext(UnfilteredDeserializer.java:511)
        at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer.hasNext(UnfilteredDeserializer.java:336)
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.readStaticRow(AbstractSSTableIterator.java:177)
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:113)
        at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:49)
        at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:72)
        at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:392)
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48)
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.metadata(LazilyInitializedUnfilteredRowIterator.java:58)
        at org.apache.cassandra.db.transform.RTBoundValidator.applyToPartition(RTBoundValidator.java:60)
        at org.apache.cassandra.db.transform.RTBoundValidator.applyToPartition(RTBoundValidator.java:34)
        at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:374)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:186)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:155)
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$4.hasNext(UnfilteredPartitionIterators.java:233)
        at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92)
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:305)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:180)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:176)
        at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76)
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:360)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2007)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2773)
        ... 5 common frames omitted {code}"
CASSANDRA-19587,Remove leftover period column from system.metadata_snapshots,Seems we left a period column in metadata_snapshots in CASSANDRA-19189/CASSANDRA-19482 - it should be removed
CASSANDRA-19584,Glossary labeled as DataStax glossary,"Should be Cassandra glossary

https://cassandra.apache.org/_/glossary.html"
CASSANDRA-19582,[Analytics] Consume new Sidecar client API to stream SSTables,"A new client API was recently introduced in Sidecar to stream SSTables. Cassandra Analytics needs to start consuming the new API in order to take advantage of the fixes when streaming SSTables from a Cassandra installation with more than one data directory.
"
CASSANDRA-19581,Add nodetool command to unregister LEFT nodes,When decommissioning a node it still remains in ClusterMetadata with state = LEFT. We should provide a nodetool command to unregister such nodes completely.
CASSANDRA-19578,Concurrent equivalent schema updates lead to unresolved disagreement,"As part of CASSANDRA-17819 a check for empty schema changes was added to the updateSchema. This only looks at the _logical_ schema difference of the schemas, but the changes made to the system_schema keyspace are the ones that actually are involved in the digest.

If two nodes issue the same CREATE statement the difference from the keyspace.diff would be empty but the timestamps on the mutations would be different, leading to a pseudo schema disagreement which will never resolve until resetlocalschema or nodes being bounced.

Only impacts 4.1

test and fix : https://github.com/clohfink/cassandra/commit/ba915f839089006ac6d08494ef19dc010bcd6411"
CASSANDRA-19577,"Queries are not visible to the ""system_views.queries"" virtual table at the coordinator level","There appears to be a hole in the implementation of CASSANDRA-15241 where {{DebuggableTasks}} at the coordinator are not preserved through the creation of {{FutureTasks}} in {{TaskFactory}}. This means that {{QueriesTable}} can't see them when is asks {{SharedExecutorPool}} for running tasks. It should be possible to fix this in {{TaskFactory}} by making sure to propagate any {{RunnableDebuggableTask}} we encounter. We already do this in {{toExecute()}}, but it also needs to happen in the relevant {{toSubmit()}} method(s)."
CASSANDRA-19572,Test failure: org.apache.cassandra.db.ImportTest flakiness,"As discovered on CASSANDRA-19401, the tests in this class are flaky, at least the following:
 * testImportCorruptWithoutValidationWithCopying
 * testImportInvalidateCache
 * testImportCorruptWithCopying
 * testImportCacheEnabledWithoutSrcDir

[https://app.circleci.com/pipelines/github/instaclustr/cassandra/4199/workflows/a70b41d8-f848-4114-9349-9a01ac082281/jobs/223621/tests]"
CASSANDRA-19571,Avoid ConcurrentModificationException when removing metrics from CassandraMetricsRegistry,"Multiple threads can modify the lists in {{CassandraMetricsRegistry}} {{ALIASES}} map values. We have seen this CME when removing metrics, but could possibly affect other paths using those lists

Adding a small delay (10ms) when map:ing to {{getMetricName}} in {{CassandraMetricsRegistry.remove(name)}} makes this reproduce when running {{TransientRangeMovementTest.testLeave}}

Caused by CASSANDRA-14572"
CASSANDRA-19568,Jennkins pipeline's default Java version for Jabba has changed,"We need Java 8 to build the driver. In the past, `jabba use default` will use Java 8. After a jenkins runner update, it uses java 11 now. Therefore, we have to specify `jabba use 1.8` instead before we build the driver."
CASSANDRA-19567,Minimize the heap consumption when registering metrics,"The problem is only reproducible on the x86 machine, the problem is not reproducible on the arm64. A quick analysis showed a lot of MetricName objects stored in the heap, although the real cause could be related to something else, the MetricName object requires extra attention.

To reproduce run the command run locally:
{code}
ant test-jvm-dtest-some -Dtest.name=org.apache.cassandra.distributed.test.ReadRepairTest
{code}

The error:

{code:java}
[junit-timeout] Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
[junit-timeout]     at java.base/java.lang.StringLatin1.newString(StringLatin1.java:769)
[junit-timeout]     at java.base/java.lang.StringBuffer.toString(StringBuffer.java:716)
[junit-timeout]     at org.apache.cassandra.CassandraBriefJUnitResultFormatter.endTestSuite(CassandraBriefJUnitResultFormatter.java:191)
[junit-timeout]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.fireEndTestSuite(JUnitTestRunner.java:854)
[junit-timeout]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:578)
[junit-timeout]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1197)
[junit-timeout]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1042)
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.ReadRepairTest-cassandra.testtag_IS_UNDEFINED
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.ReadRepairTest-cassandra.testtag_IS_UNDEFINED Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.distributed.test.ReadRepairTest:readRepairRTRangeMovementTest-cassandra.testtag_IS_UNDEFINED:    Caused an ERROR
[junit-timeout] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout]     at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
[junit-timeout]     at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout]     at java.base/java.util.Vector.forEach(Vector.java:1365)
[junit-timeout]     at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
[junit-timeout]     at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout]     at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
[junit-timeout]     at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout]     at java.base/java.util.Vector.forEach(Vector.java:1365)
[junit-timeout]     at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
[junit-timeout]     at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout]     at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
[junit-timeout]     at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.test.ReadRepairTest FAILED (crashed)BUILD FAILED
 {code}"
CASSANDRA-19566,JSON encoded timestamp value does not always match non-JSON encoded value,"Description:

""SELECT JSON ..."" and ""toJson(...)"" on Cassandra 4.1.4 produces different date than ""SELECT ...""  for some timestamp type values.

 

Steps to reproduce:
{code:java}
$ sudo docker pull cassandra:4.1.4
$ sudo docker create --name cass cassandra:4.1.4
$ sudo docker start cass
$ # wait for the Cassandra instance becomes ready
$ sudo docker exec -ti cass cqlsh
Connected to Test Cluster at 127.0.0.1:9042
[cqlsh 6.1.0 | Cassandra 4.1.4 | CQL spec 3.4.6 | Native protocol v5]
Use HELP for help.
cqlsh> create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use test;
cqlsh:test> create table tbl (id int, ts timestamp, primary key (id));
cqlsh:test> insert into tbl (id, ts) values (1, -13767019200000);
cqlsh:test> select tounixtimestamp(ts), ts, tojson(ts) from tbl where id=1;
 system.tounixtimestamp(ts) | ts                              | system.tojson(ts)
----------------------------+---------------------------------+----------------------------
            -13767019200000 | 1533-09-28 12:00:00.000000+0000 | ""1533-09-18 12:00:00.000Z""
(1 rows)
cqlsh:test> select json * from tbl where id=1;
 [json]
---------------------------------------------
 {""id"": 1, ""ts"": ""1533-09-18 12:00:00.000Z""}
(1 rows)
{code}
 

Expected behaviour:

The ""select ts"", ""select tojson(ts)"" and ""select json *"" should all produce the same date.

 

Actual behaviour:

The ""select ts"" produced the ""1533-09-28"" date but the ""select tojson(ts)"" and ""select json *"" produced the ""1533-09-18"" date."
CASSANDRA-19565,SIGSEGV on Cassandra v4.1.4,"Hello,

Since upgrading to v4.1. we cannat run CAssandra any more. Each start immediately crashes:
{{Apr 17 08:58:34 SVALD108 cassandra[1116450]: # A fatal error has been detected by the Java Runtime Environment:
Apr 17 08:58:34 SVALD108 cassandra[1116450]: #  SIGSEGV (0xb) at pc=0x00007fccaab4d152, pid=1116450, tid=1116451}}

I have added the log from the coe dump.

This issue is perhaps related to https://davecturner.github.io/2021/08/30/seven-year-old-segfault.html ?
"
CASSANDRA-19564,MemtablePostFlush deadlock leads to stuck nodes and crashes,"I've run into an issue on a 4.1.4 cluster where an entire node has locked up due to what I believe is a deadlock in memtable flushing. Here's what I know so far.  I've stitched together what happened based on conversations, logs, and some flame graphs.

*Log reports memtable flushing*

The last successful flush happens at 12:19. 
{noformat}
INFO  [NativePoolCleaner] 2024-04-16 12:19:53,634 AbstractAllocatorMemtable.java:286 - Flushing largest CFS(Keyspace='ks', ColumnFamily='version') to free up room. Used total: 0.24/0.33, live: 0.16/0.20, flushing: 0.09/0.13, this: 0.13/0.15
INFO  [NativePoolCleaner] 2024-04-16 12:19:53,634 ColumnFamilyStore.java:1012 - Enqueuing flush of ks.version, Reason: MEMTABLE_LIMIT, Usage: 660.521MiB (13%) on-heap, 790.606MiB (15%) off-heap
{noformat}

*MemtablePostFlush appears to be blocked*

At this point, MemtablePostFlush completed tasks stops incrementing, active stays at 1 and pending starts to rise.
{noformat}
MemtablePostFlush   1    1   3446   0   0
{noformat}
 
The flame graph reveals that PostFlush.call is stuck.  I don't have the line number, but I know we're stuck in {{org.apache.cassandra.db.ColumnFamilyStore.PostFlush#call}} given the visual below:

*!image-2024-04-16-13-43-11-064.png!*


*Memtable flushing is now blocked.*

All MemtableFlushWriter threads are Parked waiting on {{{}OpOrder.Barrier.await{}}}. A wall clock profile of 30s reveals all time is spent here.  Presumably we're waiting on the single threaded Post Flush.

!image-2024-04-16-12-29-15-386.png!

*Memtable allocations start to block*

Eventually it looks like the NativeAllocator stops successfully allocating memory. I assume it's waiting on memory to be freed, but since memtable flushes are blocked, we wait indefinitely.

Looking at a wall clock flame graph, all writer threads have reached the allocation failure path of {{MemtableAllocator.allocate()}}.  I believe we're waiting on {{signal.awaitThrowUncheckedOnInterrupt()}}
{noformat}
 MutationStage    48    828425      980253369      0    0{noformat}
!image-2024-04-16-11-55-54-750.png!

 

*Compaction Stops*

Since we write to the compaction history table, and that requires memtables, compactions are now blocked as well.

 

!image-2024-04-16-13-53-24-455.png!

 

The node is now doing basically nothing and must be restarted."
CASSANDRA-19563,[Analytics] Support bulk write via S3,"I would like to propose a new write option in Cassandra Analytics to bulk write SSTables via S3, in addition to the previously-implemented ""direct upload to all sidecars"" (now known as the ""Direct"" transport). 
The new write option, now being implemented, is the ""S3_COMPAT"" transport, which allows the job to upload the generated SSTables to an S3-compatible storage system, and then inform the Cassandra Sidecar that those files are available for download & commit.

Additionally, a plug-in system was added to allow communications between custom transport hooks and the job, so the custom hook can provide updated credentials and out-of-band status updates on S3-related issues."
CASSANDRA-19562,DefaultLoadBalancingPolicy considers a TimeOut as a response,"At this line: [https://github.com/apache/cassandra-java-driver/blob/388a46b9c10b5653c71ac8840bcda0c91b59bce4/core/src/main/java/com/datastax/oss/driver/internal/core/loadbalancing/DefaultLoadBalancingPolicy.java#L255]



It considers a timeout as a response, when it is counting whether there are 2 responses in the past 200ms to decide whether a node is slow and should be excluded. This will result in the driver continue sending requests to a node that is not responding and giving timeout exception.

It should be fixed just by an if statement saying if the error is not DriverTimeoutException, then update response times."
CASSANDRA-19559,prepare_release.sh should check for mvn,"Part of the 'prepare' phase of releasing includes publishing Maven artifacts, which requires that it be installed.  The script should check for this since it's quite easy to miss."
CASSANDRA-19558,Standalone jenkinsfile first round bug fixes,"A few follow up improvements and bug fixes for the standalone jenkinsfile.

- add at top a list of test failures in ci_summary.html
- -docker scripts always try to login (as base images need to be pulled too)-
- move simulator-dtests to large containers (they need 8g just heap)
- in ubuntu2004_test.docker make sure /home/cassandra exists and has correct perms (from marcuse)
- persist the jenkinsfile parameters from run to run (important for the post-commit jobs to keep their non-default branch and profile values) (was CASSANDRA-19536)
- increase jvm-dtest splits from 8 to 12
- when on ci-cassandra, replace use of copyArtifacts in Jenkinsfile generateTestReports() with manual wget of test files, allowing the summary phase to be run on any agent (copyArtifact would take >4hrs otherwise) (was INFRA-25694)
- copy ci_summary.html and results_details.tar.xz to nightlies
"
CASSANDRA-19557,"ShallowIndexedEntry scenario: the same IndexInfo is read multiple times, per every read row","When we read rows from a large partition stored in an SSTable and ShallowIndexedEntry is used - the same IndexInfo entity is read from disk multiple times, it happens per every read row.
The following stacktrace shows the execution path:
{code:java}
at org.apache.cassandra.db.RowIndexEntry$ShallowInfoRetriever.fetchIndex(RowIndexEntry.java:742)
at org.apache.cassandra.db.RowIndexEntry$FileIndexInfoRetriever.columnsIndex(RowIndexEntry.java:792)
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$IndexState.index(AbstractSSTableIterator.java:528) 
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$IndexState.currentIndex(AbstractSSTableIterator.java:523)
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$IndexState.isPastCurrentBlock(AbstractSSTableIterator.java:513)
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$IndexState.updateBlock(AbstractSSTableIterator.java:487) <=== here we retrieve the current index entry
at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardIndexedReader.computeNext(SSTableIterator.java:290) <========== here we iterate over rows
at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.hasNextInternal(SSTableIterator.java:182)
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:342)
at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.hasNext(AbstractSSTableIterator.java:224)
at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:100)
at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:110)
at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:48)
at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:74)
at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:76)
at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:27)
at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:97)
at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:303)
at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:191)
at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:181)
at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:177)
at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:48)
at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:308)
at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1991)
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2277)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137)
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.lang.Thread.run(Thread.java:829)
{code}
This Cassandra logic was originally written for the case when there is a small number of index entries and all of them are fetched in memory, so it was cheap to retrieve the IndexInfo again and again for this case. Later a support of ShallowIndexedEntry case was added (CASSANDRA-11206) which shares the same IndexState logic but in this case the cost to fetch IndexInfo is much higher - we read it from disk every time.

I have ""disk_access_mode: standard"" enabled and for each IndexInfo retrieval there are two read syscalls:
1) to find an offset for the correspondent IndexInfo
2) to read the IndexInfo itself

If we trace pread syscall we will see the following disk read syscall pattern:
{code:java}
2722116<ReadStage-5> 14:20:00.769513 pread64(102<my_keyspace/mytable/nb-1-big-Index.db>, """"..., 65536, 338611) = 65536 <0.000021>
2722116<ReadStage-5> 14:20:00.769562 pread64(102<my_keyspace/mytable/nb-1-big-Index.db>, """"..., 65536, 164405) = 65536 <0.000020>
2722116<ReadStage-5> 14:20:00.769616 pread64(102<my_keyspace/mytable/nb-1-big-Index.db>, """"..., 65536, 338611) = 65536 <0.000021>
2722116<ReadStage-5> 14:20:00.769664 pread64(102<my_keyspace/mytable/nb-1-big-Index.db>, """"..., 65536, 164405) = 65536 <0.000020>
... same is repeated many times ...
{code}
when we do 2 reads for the same positions again and again, for every data row we read from Data file. 
Also because these 2 reads are not near each other - we reset index reader buffer each time, so it does not help to prevent the repeated disk reads.

This can be improved with a local simple change: we can cache the last read IndexInfo within IndexState, one IndexInfo is small and it doesn't create a lot of GC pressure.
The caching is efficient because when we iterate through rows we iterate in parallel through the IndexInfo items in a sequential way, so we fetch once an IndexInfo and then re-use it till the moment when we need the next one.

I have an example (pathological but still representative) when 350'000 rows are read from a partition (majority are expired tombstones). Before the change the read takes 1196 ms vs 150 ms after (361'262 read syscalls from Index file before vs 622 after).

The patch is attached.

There are other optimisations which can be done for ShallowIndexedEntry logic additionally but they are more complicated, once I will have some specific proposals ready I will create separate tickets."
CASSANDRA-19554,Website - Download section - Update / remove EOL dates,"Enterprise customers with on-prem Cassandra usage can be pretty nitpicking in terms of EOL, running unsupported Cassandra versions and they often refer to what is stated in https://cassandra.apache.org/_/download.html (as the only source available?) and don't really think about the dependency to 5.0 GA, but just reflecting EOL date information there.

As of April 11, 2024, the download section states the following information:
 !image-2024-04-11-13-15-52-317.png! 

According to that, 3.x is unmaintained, 4.0 soon to be EOL etc ...

Either remove these EOL estimates or keep them strongly maintained aligned with an updated 5.0 GA timeline.

Thanks!
"
CASSANDRA-19551,CCM nodes share the same environment variable map breaking upgrade tests,"In {{node.py}} {{__environment_variables}} is generally always set with a map that is passed in from {{cluster.py}} so it is [shared between nodes|https://github.com/riptano/ccm/blob/ac264706c8ca007cc584871ce907d48db334d36d/ccmlib/node.py#L151] and if nodes modify the map, such as in {{start}} when [updating the Java version|https://github.com/riptano/ccm/blob/ac264706c8ca007cc584871ce907d48db334d36d/ccmlib/node.py#L860] then when {{get_env}} runs it will [overwrite the Java version|https://github.com/riptano/ccm/blob/ac264706c8ca007cc584871ce907d48db334d36d/ccmlib/node.py#L244] that is selected by {{update_java_version}}.

This results in {{nodetool drain}} failing when upgrading from 3.11 to 4.0 in some of the upgrade tests because after the first node upgrades to 4.0 it's not longer possible for the subsequent nodes to select a Java version that isn't 11 because it's overridden by  {{__environment_variables}}.

I'm not even 100% clear on why the code in {{start}} should update {{__environment_variables}} at all if we calculate the correct java version on every invocation of other tools."
CASSANDRA-19550,Test failure: org.apache.cassandra.cql3.validation.operations.DropRecreateAndRestoreTest.testCreateWithIdRestore,"{noformat}
junit.framework.AssertionFailedError: Got less rows than expected. Expected 2 but got 1
	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1256)
	at org.apache.cassandra.cql3.validation.operations.DropRecreateAndRestoreTest.testCreateWithIdRestore(DropRecreateAndRestoreTest.java:80)
{noformat}

As seen at https://app.circleci.com/pipelines/github/driftx/cassandra/1579/workflows/03c0b2f6-d84f-440d-baad-bf323810a292/jobs/84088"
CASSANDRA-19549,Test failure: rebuild_test.TestRebuild.test_resumable_rebuild,"Interrupted exception thrown during shutdown and caught by {{JVMStabilityInspector}} - does not look serious but we may want to ignore interrupted exception during shutdown.

https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/1326/workflows/021d350a-4b62-44af-9650-f5a0eb105522/jobs/70413/tests

{noformat}
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). 
Errors: [[node2] 'ERROR [NettyStreaming-Outbound-/127.0.0.3.7000:3] 2024-04-09 08:32:19,662 JVMStabilityInspector.java:70 - Exception in thread Thread NettyStreaming-Outbound-/127.0.0.3.7000:3,5,NettyStreaming-Outbound-/127.0.0.3.7000]
org.apache.cassandra.utils.concurrent.UncheckedInterruptedException: java.lang.InterruptedException
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.acquirePermit(StreamingMultiplexedChannel.java:373)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.run(StreamingMultiplexedChannel.java:309)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:592)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.acquirePermit(StreamingMultiplexedChannel.java:356)
	... 8 common frames omitted', [node2] 'ERROR [NettyStreaming-Outbound-/127.0.0.3.7000:3] 2024-04-09 08:32:19,664 ExecutionFailure.java:72 - Unexpected error while handling unexpected error
org.apache.cassandra.utils.concurrent.UncheckedInterruptedException: java.lang.InterruptedException
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:142)
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:170)
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:89)
	at org.apache.cassandra.utils.JVMStabilityInspector.uncaughtException(JVMStabilityInspector.java:78)
	at org.apache.cassandra.concurrent.ExecutionFailure.handle(ExecutionFailure.java:67)
	at org.apache.cassandra.concurrent.FutureTask.tryFailure(FutureTask.java:86)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:75)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:592)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.acquirePermit(StreamingMultiplexedChannel.java:356)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.run(StreamingMultiplexedChannel.java:309)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	... 4 common frames omitted']""
Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [NettyStreaming-Outbound-/127.0.0.3.7000:3] 2024-04-09 08:32:19,662 JVMStabilityInspector.java:70 - Exception in thread Thread[NettyStreaming-Outbound-/127.0.0.3.7000:3,5,NettyStreaming-Outbound-/127.0.0.3.7000]
org.apache.cassandra.utils.concurrent.UncheckedInterruptedException: java.lang.InterruptedException
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.acquirePermit(StreamingMultiplexedChannel.java:373)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.run(StreamingMultiplexedChannel.java:309)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:592)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.acquirePermit(StreamingMultiplexedChannel.java:356)
	... 8 common frames omitted', [node2] 'ERROR [NettyStreaming-Outbound-/127.0.0.3.7000:3] 2024-04-09 08:32:19,664 ExecutionFailure.java:72 - Unexpected error while handling unexpected error
org.apache.cassandra.utils.concurrent.UncheckedInterruptedException: java.lang.InterruptedException
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:142)
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:170)
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:89)
	at org.apache.cassandra.utils.JVMStabilityInspector.uncaughtException(JVMStabilityInspector.java:78)
	at org.apache.cassandra.concurrent.ExecutionFailure.handle(ExecutionFailure.java:67)
	at org.apache.cassandra.concurrent.FutureTask.tryFailure(FutureTask.java:86)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:75)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.Semaphore.tryAcquire(Semaphore.java:592)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.acquirePermit(StreamingMultiplexedChannel.java:356)
	at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$FileStreamTask.run(StreamingMultiplexedChannel.java:309)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	... 4 common frames omitted']
{noformat}
"
CASSANDRA-19544,Vector search should be able to restrict on clustering keys when filtering isn't required,"With a table that has {{primary key((a,b),c,d)}}

a restriction on only the partition works,
e.g. {{where a=. and b=. order by . ann of .}}

but a restriction that also includes a forward sequence of clustering keys (i.e. a clustering key restriction that wouldn't require filtering) does not currently work.
e.g. {{where a=. and b=. and c=. order by . ann of .}}


It appears that StatementRestriction:321 is a little too greedy.

"
CASSANDRA-19538,Test Failure: test_assassinate_valid_node,"Failing consistently on trunk:

{code:java}
ccmlib.node.TimeoutError: 03 Apr 2024 19:39:32 [node1] after 120.11/120 seconds Missing: ['127.0.0.4:7000.* is now UP'] not found in system.log:
 Head: INFO  [Messaging-EventLoop-3-1] 2024-04-03 19:37:3
 Tail: ... some nodes were not ready
INFO  [OptionalTasks:1] 2024-04-03 19:39:30,454 CassandraRoleManager.java:484 - Setup task failed with error, rescheduling
self = <gossip_test.TestGossip object at 0x7f1e63fed400>

    def test_assassinate_valid_node(self):
        """"""
        @jira_ticket CASSANDRA-16588
        Test that after taking two non-seed nodes down and assassinating
        one of them, the other can come back up.
        """"""
        cluster = self.cluster
    
        cluster.populate(5).start()
        node1 = cluster.nodelist()[0]
        node3 = cluster.nodelist()[2]
    
        self.cluster.set_configuration_options({
            'seed_provider': [{'class_name': 'org.apache.cassandra.locator.SimpleSeedProvider',
                               'parameters': [{'seeds': node1.address()}]
                              }]
            })
    
        non_seed_nodes = cluster.nodelist()[-2:]
        for node in non_seed_nodes:
            node.stop()
    
        assassination_target = non_seed_nodes[0]
        logger.debug(""Assassinating non-seed node {}"".format(assassination_target.address()))
        out, err, _ = node1.nodetool(""assassinate {}"".format(assassination_target.address()))
        assert_stderr_clean(err)
    
        logger.debug(""Starting non-seed nodes"")
        for node in non_seed_nodes:
>           node.start()

gossip_test.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../env3.8/lib/python3.8/site-packages/ccmlib/node.py:915: in start
    node.watch_log_for_alive(self, from_mark=mark)
../env3.8/lib/python3.8/site-packages/ccmlib/node.py:684: in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
../env3.8/lib/python3.8/site-packages/ccmlib/node.py:608: in watch_log_for
    TimeoutError.raise_if_passed(start=start, timeout=timeout, node=self.name,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 1712173052.8186479, timeout = 120
msg = ""Missing: ['127.0.0.4:7000.* is now UP'] not found in system.log:\n Head: INFO  [Messaging-EventLoop-3-1] 2024-04-03 1...[OptionalTasks:1] 2024-04-03 19:39:30,454 CassandraRoleManager.java:484 - Setup task failed with error, rescheduling\n""
node = 'node1'

    @staticmethod
    def raise_if_passed(start, timeout, msg, node=None):
        if start + timeout < time.time():
>           raise TimeoutError.create(start, timeout, msg, node)
E           ccmlib.node.TimeoutError: 03 Apr 2024 19:39:32 [node1] after 120.11/120 seconds Missing: ['127.0.0.4:7000.* is now UP'] not found in system.log:
E            Head: INFO  [Messaging-EventLoop-3-1] 2024-04-03 19:37:3
E            Tail: ... some nodes were not ready
E           INFO  [OptionalTasks:1] 2024-04-03 19:39:30,454 CassandraRoleManager.java:484 - Setup task failed with error, rescheduling

../env3.8/lib/python3.8/site-packages/ccmlib/node.py:56: TimeoutError
{code}

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2680/workflows/8b1c0d0a-7458-4b43-9bba-ac96b9bfe64f/jobs/58929/tests#failed-test-0

https://ci-cassandra.apache.org/job/Cassandra-trunk/1859/#showFailuresLink

"
CASSANDRA-19536,Honour previous parameter defaults between builds in Jenkinsfile,"When the Jenkinsfile is being used in a job that was created by dsl and intended to be triggered by scm polling, e.g. post-commit, that dsl will want to set alternative defaults for the Jenkinsfile parameters.

 

By default each build of the Jenkinsfile will restore the job's default parameters (i.e. changing the defaults for the next build).

 

This is described in more detail here
[https://www.linkedin.com/pulse/build-jenkins-job-default-parameters-using-bipin-kumar-chaurasia/] 

and summarised in a quick answer here
[https://stackoverflow.com/questions/57745451/how-can-i-override-a-jenkinsfiles-default-parameters]

 

Patch for this is here:
[https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/jenkinsfile-persist-parameters]

 

I'm waiting on INFRA-25694 to see if there are any other problems/changes in the Jenkinsfile when it's running in ci-cassandra.a.o"
CASSANDRA-19535,Running with default configuration OOMs under light load,"Under very moderate load, Cassandra crashes with direct memory errors.  
{noformat}
ERROR [epollEventLoopGroup-5-16] 2024-04-04 19:16:50,096 JVMStabilityInspector.java:186 - Force heap space OutOfMemoryError in the presence of
java.lang.OutOfMemoryError: Cannot reserve 65536 bytes of direct buffer memory (allocated: 8589921806, limit: 8589934592)
	at java.base/java.nio.Bits.reserveMemory(Bits.java:178)
	at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:121)
	at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332)
	at org.apache.cassandra.utils.memory.BufferPool.allocate(BufferPool.java:238)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.get(BufferPool.java:923)
	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.getAtLeast(BufferPool.java:901)
	at org.apache.cassandra.utils.memory.BufferPool.getAtLeast(BufferPool.java:219)
	at org.apache.cassandra.net.BufferPoolAllocator.getAtLeast(BufferPoolAllocator.java:75)
	at org.apache.cassandra.net.BufferPoolAllocator.newDirectBuffer(BufferPoolAllocator.java:63)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:179)
	at io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53)
	at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:120)
	at io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:785)
	at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:425)
	at io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:413)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840){noformat}"
CASSANDRA-19534,Unbounded queues in native transport requests lead to node instability,"When a node is under pressure, hundreds of thousands of requests can show up in the native transport queue, and it looks like it can take way longer to timeout than is configured.  We should be shedding load much more aggressively and use a bounded queue for incoming work.  This is extremely evident when we combine a resource consuming workload with a smaller one:

Running 5.0 HEAD on a single node as of today:
{noformat}
# populate only
easy-cass-stress run RandomPartitionAccess -p 100  -r 1 --workload.rows=100000 --workload.select=partition --maxrlat 100 --populate 10m --rate 50k -n 1

# workload 1 - larger reads
easy-cass-stress run RandomPartitionAccess -p 100  -r 1 --workload.rows=100000 --workload.select=partition --rate 200 -d 1d

# second workload - small reads
easy-cass-stress run KeyValue -p 1m --rate 20k -r .5 -d 24h{noformat}
It appears our results don't time out at the requested server time either:

 
{noformat}
                 Writes                                  Reads                                  Deletes                       Errors
  Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  1min (errors/s)
 950286       70403.93        634.77 |  789524       70442.07        426.02 |       0              0             0 | 9580484         18980.45
 952304       70567.62         640.1 |  791072       70634.34        428.36 |       0              0             0 | 9636658         18969.54
 953146       70767.34         640.1 |  791400       70767.76        428.36 |       0              0             0 | 9695272         18969.54
 956833       71171.28        623.14 |  794009        71175.6        412.79 |       0              0             0 | 9749377         19002.44
 959627       71312.58        656.93 |  795703       71349.87        435.56 |       0              0             0 | 9804907         18943.11{noformat}
 

After stopping the load test altogether, it took nearly a minute before the requests were no longer queued."
CASSANDRA-19532,Allow operators to disable the execution of triggers,"Currently, triggers are discouraged but there's no explicit way to disable them. Similar configuration already exists to disable other features, such as ""conf.materialized_views_enabled"". There should be a means for operators to gracefully disable the creation and execution of triggers."
CASSANDRA-19529,Latency regression on 4.1 comparing to 4.0,"When upgrading from Cassandra 4.0.10 to 4.1.3, I noticed an increase from application point of view latency from ~8ms to ~15ms when upgrading to Cassandra. The latency includes 3 simple queries (INSERT + SELECT (PK+CK) + UPDATE) plus application overhead.
It has been investigated in CASSANDRA-18766 to realize it is not related.
I tested to downgrade to 4.1alpha1 and the latency regression is still there with same value.

The version 4.1.4  has the same issue.

In a graph how it looks like 
!screenshot-1.png!"
CASSANDRA-19528,[Analytics] Use a classloader to isolate in-jvm dtest classes in integration test,"We need to isolate the in-jvm dtest classes (coming from the shaded dtest jar) from the classes in Cassandra Analytics when running integration tests. The isolation will allow us to test the Analytics code in the {{AppClassloader}} without any class pollution from classes coming from the shaded dtest jar. In some cases, the pollution can spill to classes that are shaded in the analytics project (metrics, json serialization, guava, etc).

With the isolation in place we can be more confident that the code we are testing is the code that will be running in a Spark environment.
"
CASSANDRA-19527,Typo in Cassandra Testing Webpage,"In `Stress and FQLTool tests` Section:
{code:java}
To run the tests for those tools, first, build jar artifacts for them but calling: {code}
""but"" -> ""by""."
CASSANDRA-19526,Optionally enable TLS in the server and client for Analytics testing,"All integration tests today run without SSL, which is generally fine because they run locally. However, it would be helpful to be able to start up the sidecar with SSL enabled in the integration test framework so that third-party tests could connect via secure connections for testing purposes.

"
CASSANDRA-19525,Optionally avoid hint transfer during decommission - port from 5.0,"This ticket is to port the changes already made for https://issues.apache.org/jira/browse/CASSANDRA-17808 to 4.0 and 4.1

This will allow the option to turn off the transferring of hints during decommission (specifically unbootstrap) 

This also allows the hints to be transferred at a higher rate during decommission, as the hinted_handoff_throttle is not divided by the number of nodes in the cluster for the unbootstrap process. "
CASSANDRA-19519,[Analytics] Migrate remaining integration tests to the single dtest cluster per class model,"In https://issues.apache.org/jira/browse/CASSANDRA-19251, the majority of the tests were moved to the single in-jvm dtest cluster per class. There are a few remaining tests that are still using the {{CassandraIntegrationTest}} annotation. However, we'd like to consolidate in a single approach for testing for the following reasons:

# Consolidate testing code in a single place to reduce maintenance burden whenever we are touching the testing framework code
# Have a unique and consistent developer experience for integration tests

As part of this effort, we can remove the unused code left from the refactor."
CASSANDRA-19517,Raise priority of TCM internode messages during critical operations,"In a busy cluster, TCM messages may not get propagated throughout the cluster, since they will be ordered together with other P1 messages (for {{TCM_}} prefixed verbs), and with P2 with all Paxos operations.

To avoid this, and make sure we can continue cluster metadata changes, all {{TCM_}}-prefixed verbs should have {{P0}} priority, just like Gossip messages used to. All Paxos messages that involve distributed metadata keyspace should now get an {{URGENT}} flag, which will instruct internode messaging to schedule them on the {{URGENT_MESSAGES}} connection."
CASSANDRA-19516,Use Transformation.Kind.id in local and distributed log tables,We should store {{Kind.id}} added in CASSANDRA-19390 in the local and distributed log tables. Virtual table will still do the id -> string lookup for easier reading
CASSANDRA-19514,When jvm-dtest is shutting down an instance TCM retries block the shutdown causing the test to fail,"org.apache.cassandra.distributed.test.log.RequestCurrentEpochTest#testRequestingPeerWatermarks

{code}
java.lang.RuntimeException: java.util.concurrent.TimeoutException
	 org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
	 org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:540)
	 org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098)
	 org.apache.cassandra.distributed.test.log.RequestCurrentEpochTest.testRequestingPeerWatermarks(RequestCurrentEpochTest.java:77)
	 java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	 java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	 java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Caused by: java.util.concurrent.TimeoutException
	 org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:253)
	 org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:532) Suppressed: java.util.concurrent.TimeoutException
{code}

In debugger I found the blocked future and it was src/java/org/apache/cassandra/tcm/EpochAwareDebounce.java waiting on src/java/org/apache/cassandra/tcm/RemoteProcessor.java retries"
CASSANDRA-19513,[Analytics] Refactor Cassandra bridge,"I have a proposal to refactor the bridge implementation by splitting the actual implementation and the shaded {{cassandra-all}} library. This separation allows us for better integration of a different {{cassandra-all}} implementation. Additionally, it better separates the actual bridge code from the Cassandra code."
CASSANDRA-19508,"Getting tons of msgs ""Failed to get peer certificates for peer /x.x.x.x:45796"" when require_client_auth is set to false","We recently upgraded our production clusters from 3.11.15 to 4.1.4. We started seeing thousands of msgs ""Failed to get peer certificates for peer /x.x.x.x:45796"". SSL is enabled but require_client_auth is disabled.  This is causing a huge problem for us because cassandra log files are growing very fast as our connections are short live connections, we open more than 1K connections per second and they stay live for 1-2 seconds. 
{code:java}
DEBUG [Native-Transport-Requests-2] 2024-03-31 21:26:38,026 ServerConnection.java:140 - Failed to get peer certificates for peer /172.31.2.23:45796
javax.net.ssl.SSLPeerUnverifiedException: peer not verified
        at io.netty.handler.ssl.ReferenceCountedOpenSslEngine$DefaultOpenSslSession.getPeerCertificateChain(ReferenceCountedOpenSslEngine.java:2414)
        at io.netty.handler.ssl.ExtendedOpenSslSession.getPeerCertificateChain(ExtendedOpenSslSession.java:140)
        at org.apache.cassandra.transport.ServerConnection.certificates(ServerConnection.java:136)
        at org.apache.cassandra.transport.ServerConnection.getSaslNegotiator(ServerConnection.java:120)
        at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:76)
        at org.apache.cassandra.transport.Message$Request.execute(Message.java:255)
        at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:166)
        at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:185)
        at org.apache.cassandra.transport.Dispatcher.processRequest(Dispatcher.java:212)
        at org.apache.cassandra.transport.Dispatcher$RequestProcessor.run(Dispatcher.java:109)
        at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
        at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
        at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:142)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) {code}
*Our SSL config:*
{code:java}
client_encryption_options:
  enabled: true
  keystore: /path/to/keystore
  keystore_password: xxxxx
  optional: false
  require_client_auth: false {code}
 

We should stop throwing this msg when require_client_auth is set to false. Or at least it should be logged in TRACE not DEBUG. 

I'm working on preparing a PR. "
CASSANDRA-19507,[Analytics] Fix bulk reads of multiple tables that potentially have the same data file name,"When reading multiple data frames using bulk reader from different tables, it is possible to encounter a data file name being retrieved from the same Sidecar instance. Because the {{SSTable}}s are cached in the {{SSTableCache}}, it is possible that the {{org.apache.cassandra.spark.reader.SSTableReader}} is using the incorrect {{SSTable}} if it was cached with the same {{#hashCode}}."
CASSANDRA-19506,Add Jenkins support for testing against Cassandra 4.1,Currently hung up on the ccm invocation due to the recent cassandra.yaml property migration
CASSANDRA-19504,Jenkinsfile doesn't invoke tests with the correct Java versions,"The existing Jenkinsfile is intended to run tests against multiple Java versions (8, 11, 17). However, due to the concurrency in multiple matrix jobs and the global scope of `TEST_JAVA_VERSION` and `TEST_JAVA_HOME`, the jobs are actually only run against one version.


This can be fixed by defining these variables locally, using `def` keyword. "
CASSANDRA-19503,(Accord) Cassandra bootstrap no longer using the range txn and instead uses the sync point empty txn for reads,"Ephemeral reads made a change to ReadData which caused Bootstrap to no longer use the range txn it generates and instead uses the empty txn from the sync point.  This was not detected in Accord due to ListRead supporting ranges, and failed in Cassandra as we don’t support range reads.
"
CASSANDRA-19502,PropertyFileSnitchTest overwrites test/conf/cassandra-topology.properties,While this is not an issue for CI test or other items that replace the file on a development system the file is left in its modified state and under risk of being accidentally checked into the repository.
CASSANDRA-19498,support legacy [plain_text_auth] section in credentials file removed unintentionally,"The pylib/cqlshlib/cqlshmain.py code reads data from the credentials file, however, it is immediately ignored.

[https://github.com/apache/cassandra/blob/c9625e0102dab66f41d3ef2338c54d499e73a8c5/pylib/cqlshlib/cqlshmain.py#L2070]
{code:java}
    if not options.username:
        credentials = configparser.ConfigParser()
        if options.credentials is not None:
            credentials.read(options.credentials)        # use the username from credentials file but fallback to cqlshrc if username is absent from the command line parameters
        options.username = username_from_cqlshrc    if not options.password:
        rawcredentials = configparser.RawConfigParser()
        if options.credentials is not None:
            rawcredentials.read(options.credentials)        # handling password in the same way as username, priority cli > credentials > cqlshrc
        options.password = option_with_default(rawcredentials.get, 'plain_text_auth', 'password', password_from_cqlshrc)
        options.password = password_from_cqlshrc{code}
These corrections have been made in accordance with https://issues.apache.org/jira/browse/CASSANDRA-16983 and https://issues.apache.org/jira/browse/CASSANDRA-16456.

The documentation does not indicate that AuthProviders can be used in the cqlshrc and credentials files.

I propose to return the ability to use the legacy option of specifying the user and password in the credentials file in the [plain_text_auth] section.

It is also required to describe the rules for using the credentials file in the documentation.

I can make a corresponding pull request.

EDIT by Stefan Miklosovic:

specifying username and password in credentials file works, it is just that [plain_text_auth] section does not work in credentials file anymore. This was working with CASSANDRA-16983 but it stopped to work by CASSANDRA-16456. Both tickets were firstly introduced in 4.1.0 (for the public). I do not think that it was ever an intention to stop to support that when CASSANDRA-16456 was merged and it was most probably just overlooked."
CASSANDRA-19496,Add properties for redirecting build-resolve to mirrors,When running upgrade tests in CI it's not always possible to reach the public mirrors. Currently we have properties for configuring private mirrors in 4.0+ but we don't have this for 3.x.
CASSANDRA-19495,Hints not stored after node goes down for the second time,"I have a scenario where a node goes down, hints are recorded on the second node and replayed, as expected. If the first node goes down for a second time and time span between the first time it stopped and the second time it stopped is more than the max_hint_window then the hint is not recorded, no hint file is created, and the mutation never arrives at the node after it comes up again.

I have debugged this and it appears to due to the way hint window is persisted after https://issues.apache.org/jira/browse/CASSANDRA-14309

The code here: [https://github.com/apache/cassandra/blame/cassandra-4.1/src/java/org/apache/cassandra/service/StorageProxy.java#L2402] uses the time stored in the HintsBuffer.earliestHintByHost map.  This map is based on the UUID of the host, but this does not seem to be cleared when the node is back up, and I think this is what is causing the problem.

 

This is in cassandra 4.1.5"
CASSANDRA-19494,Optimize I/O during table scans,"The storage engine reads chunk by chunk during table scans.  We'd be much better off if we could perform larger I/O operations to an internal buffer, perform fewer I/O operations, and avoid making excessive system calls.

For example, doing a scan against this table:
{noformat}
CREATE TABLE easy_cass_stress.keyvalue (
    key text PRIMARY KEY,
    value text
) WITH additional_write_policy = '99p'
    AND allow_auto_snapshot = true
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND cdc = false
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND memtable = 'default'
    AND crc_check_chance = 1.0
    AND default_time_to_live = 0
    AND extensions = {}
    AND gc_grace_seconds = 864000
    AND incremental_backups = true
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair = 'BLOCKING'
    AND speculative_retry = '99p';{noformat}
I see the following I/O activity (sample only, see attachment for full accounting of all reads)

 
{noformat}
TIME     COMM           PID    T BYTES   OFF_KB   LAT(ms) FILENAME
16:59:23 ReadStage-2    2523   R 15051   0           0.02 nb-6-big-Data.db
16:59:23 ReadStage-2    2523   R 15049   0           0.01 nb-8-big-Data.db
16:59:23 ReadStage-2    2523   R 15025   0           0.01 nb-5-big-Data.db
16:59:23 ReadStage-2    2523   R 15064   0           0.01 nb-7-big-Data.db
16:59:25 ReadStage-2    2523   R 15051   0           0.01 nb-6-big-Data.db
16:59:25 ReadStage-2    2523   R 15049   0           0.01 nb-8-big-Data.db
16:59:25 ReadStage-2    2523   R 15025   0           0.01 nb-5-big-Data.db
16:59:25 ReadStage-2    2523   R 15064   0           0.00 nb-7-big-Data.db
16:59:25 ReadStage-2    2523   R 15064   14          0.01 nb-5-big-Data.db
16:59:25 ReadStage-2    2523   R 15051   0           0.01 nb-6-big-Data.db
16:59:25 ReadStage-2    2523   R 15049   0           0.00 nb-8-big-Data.db
16:59:25 ReadStage-2    2523   R 15064   14          0.00 nb-5-big-Data.db
16:59:25 ReadStage-2    2523   R 15064   0           0.00 nb-7-big-Data.db
16:59:25 ReadStage-2    2523   R 15012   29          0.01 nb-5-big-Data.db{noformat}
with a sample of our off-cpu time looking like this (after dropping caches)
{noformat}
cpudist -O -p $(cassandra-pid) -m 1 30

     msecs               : count     distribution
         0 -> 1          : 5259     |****************************************|
         2 -> 3          : 486      |***                                     |
         4 -> 7          : 0        |                                        |
         8 -> 15         : 1        |                                        |
        16 -> 31         : 0        |                                        |
        32 -> 63         : 29       |                                        |
        64 -> 127        : 77       |                                        |
       128 -> 255        : 4        |                                        |
       256 -> 511        : 6        |                                        |
       512 -> 1023       : 6        |                                        |{noformat}
We pay a pretty serious throughput penalty for excessive I/O.  

We should be able to leverage the work in CASSANDRA-15452 for this."
CASSANDRA-19493,Optionally fail writes when SAI refuses to index a term value exceeding a configured maximum size,"SAI currently emits a client warning when we try to index a text value larger than {{{}cassandra.sai.max_string_term_size{}}}. It might be nice to have a hard limit as well, above which we can reject the mutation entirely."
CASSANDRA-19491,decommissioned nodes show as UNREACHABLE in describecluster afterward,[~urandom] noticed this happening in his cluster and I have been able to reproduce with a modified dtest.
CASSANDRA-19489,Guardrail to warn clients about possible transient incorrect responses for filtering queries against multiple mutable columns,"Given we may not have time to fully resolve CASSANDRA-19007 before we release 5.0, it would still be helpful to have, at the very minimum, a client warning for cases where a user filters on two or more mutable (static or regular) columns at consistency levels that require coordinator reconciliation. We may also want the option to fail these queries outright, although that need not be the default.

The only art involved in this is deciding what we want to say in the warning/error message. It's probably reasonable to mention there that this only happens when we have unrepaired data. It's also worth noting that SAI queries are no longer vulnerable to this after the resolution of CASSANDRA-19018."
CASSANDRA-19486,enrich system_views.pending_hints with hints sizes,"I am happy to close this if somebody shows me how to get total size of all hints or all hints per particular node via JMX.

I could find StorageMetrics.totalHints but that is how many hints there are, not their sizes. We also have

org.apache.cassandra.metrics:type=HintedHandOffManager name=<MetricName>
org.apache.cassandra.metrics:type=HintsService name=<MetricName>

But that is again showing other metrics not sizes.

I would add two methods into HintsServiceMBean returning this. Seems to be very easy to do once we do CASSANDRA-19477."
CASSANDRA-19485,if max_hints_size_per_host < max_hints_file_size then it will write hints after max_hints_size_per_host is reached,"there is one problem in the current solution being that if we have this config

{noformat}
max_hints_size_per_host: 2MiB
max_hints_file_size: 128MiB
{noformat} 

basically, max size > size per host, then it will not stop it from writing hints after 2MiB for a particular node, because HintsDescriptor is added among dispatching ones after writer is closed, which happens after there is 128MiB written (all logic in HintsStore), so it will not be included into total sizes, it will be 0 until at least one hints file for that node is written to disk.

I consider this to be the flaw of CASSANDRA-17142, however it is questionable if this is serious enough to deal with in the first place, I don't think somebody would set it up in practice to these values, normally one puts there like max per host is few gigs so this problem is not so visible but it shows in tests almost instantly and it is technically just wrong, regardless of the probability this would happen in real ... 

cc [~yifanc]"
CASSANDRA-19484,Add support for providing nvdDatafeedUrl to OWASP,"This allows you to point to a mirror that is faster and doesn’t require an API key.

This is kind of painful to make work in {{ant}} because you can't specify the property at all if you want to use the API and I couldn't find a way to get {{ant}} to conditionally supply the property without having a dedicated invocation of the {{dependency-check}} task with/without the parameter {{nvdDataFeedUrl}} specified.

 "
CASSANDRA-19482,Simplify metadata log implementation using custom partitioner,"The distributed metadata log table can be simplified by leveraging the fact that replicas are all responsible for the entire token range. Given this assumption, we can then use {{ReversedLongLocalPartitioner}} introduced in CASSANDRA-19391 to make it much easier to append to/read from the tail of the log, effectively removing the need for the {{Period}} construct. This will also improve apply to the local metadata log used at startup.  "
CASSANDRA-19481,The repository 'https://debian.cassandra.apache.org 311x Release' does not have a Release file.,"The debian repo for 311 can't be used anymore; apt update fails with

{color:#000000}Ign:10 [https://debian.cassandra.apache.org|https://debian.cassandra.apache.org/] 311x InRelease {color}
Err:12 [https://debian.cassandra.apache.org|https://debian.cassandra.apache.org/] 311x Release 
 server certificate verification failed. CAfile: /etc/ssl/certs/ca-certificates.crt CRLfile: none 
Reading package lists... Done{color:#b26818} {color}{color:#000000} {color}
E: The repository 'https://debian.cassandra.apache.org 311x Release' does not have a Release file. 
N: Updating from such a repository can't be done securely, and is therefore disabled by default. 
N: See apt-secure(8) manpage for repository creation and user configuration details."
CASSANDRA-19479,Fix type issues and provide tests for type compatibility between 4.1 and 5.0,"This is a part of CASSANDRA-14476 - we should verify whether the type compatibility matrix is upgradable from 4.0 and 4.1 to 5.0, and if not, fix the remaining issues.

The implemented tests verify the following:
- assumed compatibility between primitive types
- equals method symmetricity
- freezing/unfreezing
- value compatibility by using a serializer of one type to deserialize a value serialized using a serializer of another type
- serialization compatibility by serializing a row with a column of one type as a column of another type for simple and complex cells (multicell types)
- (comparison) compatibility by comparing serialized values of one type using a comparator of another type; for multicell types - build rows and compare cell paths of a complex type using a cell path comparator of another complex type
- verify whether types that are (value/serialization/comparison) compatible in a previous release are still compatible with this release
- store the compatibility matrix in a compressed JSON file so that we can copy it to future releases to assert backward compatibility (similar approach to LegacySSTableTest)
- verify that type serializers are different for non-compatible type pairs which use custom comparisons

Additionally:
- the equals method in {{TupleType}} and {{UserType}} was fixed to be symmetric. Previously, comparing two values gave a different outcome when inverted.
- fixed a condition in comparison method of {{AbstractCompositeType}}
- ported a fix for composite and dynamic composite types which adds a distinct serializers for them so that the serializers for those types and for {{BytesType}} are considered different; similar thing was done for {{LexicalUUIDType}} to make its serializer different to {{UUIDType}} serializer (see https://the-asf.slack.com/archives/CK23JSY2K/p1712060572432959)
- fixed a problem with DCT builder - in 5.0+ the {{DynamicCompositeType}} generation has a problem with inverse alias-type mapping which makes it vulnerable to problems when the same type has two different aliases
"
CASSANDRA-19477,Do not go to disk to get HintsStore.getTotalFileSize,"When testing a cluster with more requests than it could handle, I noticed significant CPU time (25%) spent in HintsStore.getTotalFileSize.  Here's what I'm seeing from profiling:

10% of CPU time spent in HintsDescriptor.fileName which only does this:

 
{noformat}
return String.format(""%s-%s-%s.hints"", hostId, timestamp, version);{noformat}
At a bare minimum here we should create this string up front with the host and version and eliminate 2 of the 3 substitutions, but I think it's probably faster to use a StringBuilder and avoid the underlying regular expression altogether.

12% of the time is spent in org.apache.cassandra.io.util.File.length.  It looks like this is called once for each hint file on disk for each host we're hinting to.  In the case of an overloaded cluster, this is significant.  It would be better if we were to track the file size in memory for each hint file and reference that rather than go to the filesystem.

These fairly small changes should make Cassandra more reliable when under load spikes.

CPU Flame graph attached.

I only tested this in 4.1 but it looks like this is present up to trunk.

 "
CASSANDRA-19475,system_views.settings incorrectly handle array types,"4.1+ gives
{noformat}
cqlsh> select value from system_views.settings where name = 'data_file_directories'; 

 value
------------------------------
 [Ljava.lang.String;@21b4c4bb
 {noformat}
 

should be 

 
{noformat}
cqlsh> select value from system_views.settings where name = 'data_file_directories'; 

 value
----------------------------------------------------------------------------
 [/home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-4.1/data/data]
 {noformat}"
CASSANDRA-19474,Fix cqlsh_tests/test_cqlsh.py,https://app.circleci.com/pipelines/github/driftx/cassandra/1522/workflows/f1cd6321-9a48-4a43-92b3-aed0b610fe5e/jobs/77610/tests
CASSANDRA-19472,CEP-15 (C*) Integrate accord with repair,
CASSANDRA-19471,Commitlog with direct io fails test_change_durable_writes,"With the commitlog_disk_access_mode set to direct, and the improved configuration_test.py::TestConfiguration::test_change_durable_writes from CASSANDRA-19465, this fails with either:

{noformat}
 AssertionError: Commitlog was written with durable writes disabled
{noformat}

Or what appears to be the original exception reported in CASSANDRA-19465:

{noformat}
  node1: ERROR [PERIODIC-COMMIT-LOG-SYNCER] 2024-03-14 17:16:08,465 StorageService.java:631 - Stopping native transport
  node1: ERROR [MutationStage-5] 2024-03-14 17:16:08,465 StorageProxy.java:1670 - Failed to apply mutation locally :
  java.lang.IllegalArgumentException: newPosition > limit: (1048634 > 1048576)
        at java.base/java.nio.Buffer.createPositionException(Buffer.java:341)
        at java.base/java.nio.Buffer.position(Buffer.java:316)
        at java.base/java.nio.ByteBuffer.position(ByteBuffer.java:1516)
        at java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:321)
        at java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:73)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.allocate(CommitLogSegment.java:216)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.allocate(CommitLogSegmentManagerStandard.java:52)
        at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:307)
        at org.apache.cassandra.db.CassandraKeyspaceWriteHandler.addToCommitLog(CassandraKeyspaceWriteHandler.java:99)
        at org.apache.cassandra.db.CassandraKeyspaceWriteHandler.beginWrite(CassandraKeyspaceWriteHandler.java:53)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:612)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:497)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:244)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:264)
        at org.apache.cassandra.service.StorageProxy$4.runMayThrow(StorageProxy.java:1664)
        at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2624)
        at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:833)
  node1: ERROR [PERIODIC-COMMIT-LOG-SYNCER] 2024-03-14 17:16:08,470 StorageService.java:636 - Stopping gossiper
{noformat}"
CASSANDRA-19470,TestMultiDCWriteFailures doesn't test versions past 4.0.x,"I think this was disabled for future versions in CASSANDRA-17456, but there is no need to do this as I have a patch that will work for all versions."
CASSANDRA-19469,python dtest packages should pin versions,We have for example in CASSANDRA-19464 seen how allowing modules to automatically upgrade themselves causes us problems.  We should pin all of these to specific versions so that the environment is entirely reproducible.
CASSANDRA-19468,In some situations MetadataManager.SingleThreaded.startSchemaRequest could fail to set CompletableFuture arg,"Relevant code is:

 

 
{code:java}
private void startSchemaRequest(CompletableFuture<RefreshSchemaResult> refreshFuture) {
  assert adminExecutor.inEventLoop();
  if (closeWasCalled) {
    refreshFuture.complete(new RefreshSchemaResult(metadata));
    return;
  }
  if (currentSchemaRefresh == null) {
    currentSchemaRefresh = refreshFuture;
    LOG.debug(""[{}] Starting schema refresh"", logPrefix);
    initControlConnectionForSchema()
        .thenCompose(v -> context.getTopologyMonitor().checkSchemaAgreement())
        .whenComplete(
            (schemaInAgreement, agreementError) -> {
              if (agreementError != null) {
                refreshFuture.completeExceptionally(agreementError);
              } else {
                schemaQueriesFactory
                    .newInstance()
                    .execute()
                    .thenApplyAsync(this::parseAndApplySchemaRows, adminExecutor)
                    .whenComplete(
                        (newMetadata, metadataError) -> {
                          if (metadataError != null) {
                            refreshFuture.completeExceptionally(metadataError);
                          } else {
                            refreshFuture.complete(
                                new RefreshSchemaResult(newMetadata, schemaInAgreement));
                          }

...{code}
 

 

Problem is that the default impl of SchemaQueriesFactory (DefaultSchemaQueriesFactory) can chuck exceptions in a few cases, most notably if the control connection is in a bad way:

 

 
{code:java}
@Override
public SchemaQueries newInstance() {
  DriverChannel channel = context.getControlConnection().channel();
  if (channel == null || channel.closeFuture().isDone()) {
    throw new IllegalStateException(""Control channel not available, aborting schema refresh"");
  }
  Node node =
      context
          .getMetadataManager()
          .getMetadata()
          .findNode(channel.getEndPoint())
          .orElseThrow(
              () ->
                  new IllegalStateException(
                      ""Could not find control node metadata ""
                          + channel.getEndPoint()
                          + "", aborting schema refresh""));
  return newInstance(node, channel);
} {code}
 

 

 

In this case the MetadataManager code above will exit out before it ever sets a state on refreshFuture... meaning anything waiting on that future will just continue to wait.

 

 "
CASSANDRA-19467,"Deprecate Python 3.7 and earlier, but allow cqlsh to run with Python 3.6+","In 4.0, we introduced Python 3.6 support. In CASSANDRA-19245, for the 5.0 release, we dropped Python 3.6 and 3.7 support since Python 3.8 was the minimum supported version for version 3.29.0 of the Python driver, which we needed to support vectors. Unfortunately, that may break existing users of RHEL 7, which natively only supports Python 3.6.

We should deprecate Python 3.6-3.7 (which are all either EOL or will be soon) and mention this in NEWS.txt/appropriate warnings at {{cqlsh}} startup, but we don't need to have cqlsh refuse to start entirely with 3.6-3.7 until our 6.0 release."
CASSANDRA-19465,Test Failure: configuration_test.TestConfiguration.test_change_durable_writes,"https://app.circleci.com/pipelines/github/bereng/cassandra/1181/workflows/fe2ac859-f6ba-4f1e-b0b1-e6923b16e874/jobs/39449/tests

{noformat}
self = <configuration_test.TestConfiguration object at 0x7ff4ecd33460>

    @pytest.mark.timeout(60*30)
    def test_change_durable_writes(self):
        """"""
        @jira_ticket CASSANDRA-9560
    
        Test that changes to the DURABLE_WRITES option on keyspaces is
        respected in subsequent writes.
    
        This test starts by writing a dataset to a cluster and asserting that
        the commitlogs have been written to. The subsequent test depends on
        the assumption that this dataset triggers an fsync.
    
        After checking this assumption, the test destroys the cluster and
        creates a fresh one. Then it tests that DURABLE_WRITES is respected by:
    
        - creating a keyspace with DURABLE_WRITES set to false,
        - using ALTER KEYSPACE to set its DURABLE_WRITES option to true,
        - writing a dataset to this keyspace that is known to trigger a commitlog fsync,
        - asserting that the commitlog has grown in size since the data was written.
        """"""
        def new_commitlog_cluster_node():
            # writes should block on commitlog fsync
            self.fixture_dtest_setup.cluster.populate(1)
            node = self.fixture_dtest_setup.cluster.nodelist()[0]
            self.fixture_dtest_setup.cluster.set_batch_commitlog(enabled=True, use_batch_window = self.fixture_dtest_setup.cluster.version() < '5.0')
    
            self.fixture_dtest_setup.cluster.start()
            return node
    
        durable_node = new_commitlog_cluster_node()
        durable_init_size = commitlog_size(durable_node)
        durable_session = self.patient_exclusive_cql_connection(durable_node)
    
        # test assumption that write_to_trigger_fsync actually triggers a commitlog fsync
        durable_session.execute(""CREATE KEYSPACE ks WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1} ""
                                ""AND DURABLE_WRITES = true"")
        durable_session.execute('CREATE TABLE ks.tab (key int PRIMARY KEY, a int, b int, c int)')
        logger.debug('commitlog size diff = ' + str(commitlog_size(durable_node) - durable_init_size))
        write_to_trigger_fsync(durable_session, 'ks', 'tab')
    
        assert commitlog_size(durable_node) > durable_init_size, \
            ""This test will not work in this environment; write_to_trigger_fsync does not trigger fsync.""
    
        # get a fresh cluster to work on
        durable_session.shutdown()
        self.fixture_dtest_setup.cleanup_and_replace_cluster()
    
        node = new_commitlog_cluster_node()
        init_size = commitlog_size(node)
        session = self.patient_exclusive_cql_connection(node)
    
        # set up a keyspace without durable writes, then alter it to use them
        session.execute(""CREATE KEYSPACE ks WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1} ""
                        ""AND DURABLE_WRITES = false"")
        session.execute('CREATE TABLE ks.tab (key int PRIMARY KEY, a int, b int, c int)')
        session.execute('ALTER KEYSPACE ks WITH DURABLE_WRITES=true')
>       write_to_trigger_fsync(session, 'ks', 'tab')

configuration_test.py:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
configuration_test.py:186: in write_to_trigger_fsync
    execute_concurrent_with_args(session,
../env3.8/src/cassandra-driver/cassandra/concurrent.py:238: in execute_concurrent_with_args
    return execute_concurrent(session, zip(cycle((statement,)), parameters), *args, **kwargs)
../env3.8/src/cassandra-driver/cassandra/concurrent.py:94: in execute_concurrent
    return executor.execute(concurrency, raise_on_first_error)
../env3.8/src/cassandra-driver/cassandra/concurrent.py:201: in execute
    return super(ConcurrentExecutorListResults, self).execute(concurrency, fail_fast)
../env3.8/src/cassandra-driver/cassandra/concurrent.py:120: in execute
    return self._results()
../env3.8/src/cassandra-driver/cassandra/concurrent.py:219: in _results
    self._raise(self._exception)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

exc = NoHostAvailable('Unable to complete the operation against any hosts', {<Host: 127.0.0.1:9042 datacenter1>: ConnectionShutdown('Connection to 127.0.0.1:9042 was closed')})

    @staticmethod
    def _raise(exc):
        if six.PY2 and isinstance(exc, tuple):
            (exc_type, value, traceback) = exc
            six.reraise(exc_type, value, traceback)
        else:
>           raise exc
E           cassandra.cluster.NoHostAvailable: ('Unable to complete the operation against any hosts', {<Host: 127.0.0.1:9042 datacenter1>: ConnectionShutdown('Connection to 127.0.0.1:9042 was closed')})

../env3.8/src/cassandra-driver/cassandra/concurrent.py:167: NoHostAvailable
{noformat}



{noformat}
failed on teardown with ""Failed: Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [MutationStage-3] 2024-03-08 09:36:11,386 StorageProxy.java:1670 - Failed to apply mutation locally : \njava.lang.IllegalArgumentException: newPosition > limit: (1048644 > 1048576)\n\tat java.base/java.nio.Buffer.createPositionException(Buffer.java:341)\n\tat java.base/java.nio.Buffer.position(Buffer.java:316)\n\tat java.base/java.nio.ByteBuffer.position(ByteBuffer.java:1516)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:321)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:73)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegment.allocate(CommitLogSegment.java:216)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.allocate(CommitLogSegmentManagerStandard.java:52)\n\tat org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:307)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.addToCommitLog(CassandraKeyspaceWriteHandler.java:99)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.beginWrite(CassandraKeyspaceWriteHandler.java:53)\n\tat org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:612)\n\tat org.apache.cassandra.db.Keyspace.apply(Keyspace.java:497)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:244)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:264)\n\tat org.apache.cassandra.service.StorageProxy$4.runMayThrow(StorageProxy.java:1664)\n\tat org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2624)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)\n\tat org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)', [node1] 'ERROR [MutationStage-4] 2024-03-08 09:36:11,386 StorageProxy.java:1670 - Failed to apply mutation locally : \njava.lang.IllegalArgumentException: newPosition > limit: (1048722 > 1048576)\n\tat java.base/java.nio.Buffer.createPositionException(Buffer.java:341)\n\tat java.base/java.nio.Buffer.position(Buffer.java:316)\n\tat java.base/java.nio.ByteBuffer.position(ByteBuffer.java:1516)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:321)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:73)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegment.allocate(CommitLogSegment.java:216)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.allocate(CommitLogSegmentManagerStandard.java:52)\n\tat org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:307)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.addToCommitLog(CassandraKeyspaceWriteHandler.java:99)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.beginWrite(CassandraKeyspaceWriteHandler.java:53)\n\tat org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:612)\n\tat org.apache.cassandra.db.Keyspace.apply(Keyspace.java:497)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:244)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:264)\n\tat org.apache.cassandra.service.StorageProxy$4.runMayThrow(StorageProxy.java:1664)\n\tat org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2624)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)\n\tat org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)', [node1] 'ERROR [COMMIT-LOG-WRITER] 2024-03-08 09:36:11,387 StorageService.java:631 - Stopping native transport', [node1] 'ERROR [COMMIT-LOG-WRITER] 2024-03-08 09:36:11,392 StorageService.java:636 - Stopping gossiper']""
Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [MutationStage-3] 2024-03-08 09:36:11,386 StorageProxy.java:1670 - Failed to apply mutation locally : \njava.lang.IllegalArgumentException: newPosition > limit: (1048644 > 1048576)\n\tat java.base/java.nio.Buffer.createPositionException(Buffer.java:341)\n\tat java.base/java.nio.Buffer.position(Buffer.java:316)\n\tat java.base/java.nio.ByteBuffer.position(ByteBuffer.java:1516)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:321)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:73)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegment.allocate(CommitLogSegment.java:216)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.allocate(CommitLogSegmentManagerStandard.java:52)\n\tat org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:307)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.addToCommitLog(CassandraKeyspaceWriteHandler.java:99)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.beginWrite(CassandraKeyspaceWriteHandler.java:53)\n\tat org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:612)\n\tat org.apache.cassandra.db.Keyspace.apply(Keyspace.java:497)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:244)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:264)\n\tat org.apache.cassandra.service.StorageProxy$4.runMayThrow(StorageProxy.java:1664)\n\tat org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2624)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)\n\tat org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)', [node1] 'ERROR [MutationStage-4] 2024-03-08 09:36:11,386 StorageProxy.java:1670 - Failed to apply mutation locally : \njava.lang.IllegalArgumentException: newPosition > limit: (1048722 > 1048576)\n\tat java.base/java.nio.Buffer.createPositionException(Buffer.java:341)\n\tat java.base/java.nio.Buffer.position(Buffer.java:316)\n\tat java.base/java.nio.ByteBuffer.position(ByteBuffer.java:1516)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:321)\n\tat java.base/java.nio.MappedByteBuffer.position(MappedByteBuffer.java:73)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegment.allocate(CommitLogSegment.java:216)\n\tat org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.allocate(CommitLogSegmentManagerStandard.java:52)\n\tat org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:307)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.addToCommitLog(CassandraKeyspaceWriteHandler.java:99)\n\tat org.apache.cassandra.db.CassandraKeyspaceWriteHandler.beginWrite(CassandraKeyspaceWriteHandler.java:53)\n\tat org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:612)\n\tat org.apache.cassandra.db.Keyspace.apply(Keyspace.java:497)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:244)\n\tat org.apache.cassandra.db.Mutation.apply(Mutation.java:264)\n\tat org.apache.cassandra.service.StorageProxy$4.runMayThrow(StorageProxy.java:1664)\n\tat org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2624)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)\n\tat org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)', [node1] 'ERROR [COMMIT-LOG-WRITER] 2024-03-08 09:36:11,387 StorageService.java:631 - Stopping native transport', [node1] 'ERROR [COMMIT-LOG-WRITER] 2024-03-08 09:36:11,392 StorageService.java:636 - Stopping gossiper']
{noformat}
"
CASSANDRA-19463,Test failure: org.apache.cassandra.fuzz.ring.ConsistentBootstrapTest.coordinatorIsBehindTest,"Looks like this was broken by CASSANDRA-18275

Timeouts in the log and then:
{code}
java.lang.IllegalStateException: Can't use shutdown instances, delegate is null

	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegate(AbstractCluster.java:283)
	at org.apache.cassandra.distributed.impl.DelegatingInvokableInstance.transfer(DelegatingInvokableInstance.java:49)
	at org.apache.cassandra.distributed.api.IInvokableInstance.runsOnInstance(IInvokableInstance.java:45)
	at org.apache.cassandra.distributed.api.IInvokableInstance.runOnInstance(IInvokableInstance.java:46)
	at org.apache.cassandra.distributed.shared.ClusterUtils.unpauseCommits(ClusterUtils.java:548)
	at org.apache.cassandra.fuzz.ring.ConsistentBootstrapTest.coordinatorIsBehindTest(ConsistentBootstrapTest.java:227)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
{code}"
CASSANDRA-19461,SAI does not index empty bytes even for types that allow empty bytes as a valid input,"This is easy to reproduce with a test that looks something like this:

{noformat}
@Test
public void testEmptyString()
{
    createTable(""CREATE TABLE %s (k TEXT PRIMARY KEY, v text)"");
    createIndex(String.format(CREATE_INDEX_TEMPLATE, 'v'));

    execute(""INSERT INTO %s (k, v) VALUES ('0', '')"");
    execute(""INSERT INTO %s (k) VALUES ('1')"");
    
    // flush(); <---- there is not always a memtable index involved, a fix will have to pay attention to this

    List<Row> rows = executeNet(""SELECT * FROM %s WHERE v = ''"").all();
    assertEquals(1, rows.size()); <— FAILS! No matches...
}
{noformat}"
CASSANDRA-19460,Fix tests to work with ULID SSTable identifiers to enable uuid_sstable_identifiers_enabled in cassandra-latest.yaml,"In CASSANDRA-18753 we identified that we want to also set uuid_sstable_identifiers_enabled to true, while running a CI with it turned on, it failed (1).

Errors do not seem to be serious, it is just the test suite we have is not prepared for the case when uuid_sstable_identifiers_enabled is set to true by default.

We need to fix all these tests so we can have cassandra-latest.yaml containing that property.

https://app.circleci.com/pipelines/github/blambov/cassandra/609/workflows/aef2b936-0551-4f3b-9d86-a49451c83947

"
CASSANDRA-19459,test_complementary_deletion_with_limit_on_partition_key_column_with_empty_partitions fails with SAI,"The dtest {{replica_side_filtering_test::TestSecondaryIndexes::test_complementary_deletion_with_limit_on_partition_key_column_with_empty_partitions}} fails when the default secondary index is switched to SAI with
{code}
test_complementary_deletion_with_limit_on_partition_key_column_with_empty_partitions failed; it passed 0 out of the required 1 times.
	<class 'ccmlib.node.ToolError'>
	Subprocess ['nodetool', '-h', 'localhost', '-p', '7200', 'flush'] exited with non-zero status; exit status: 2; 
stderr: error: null
-- StackTrace --
java.lang.NullPointerException
	at java.base/java.util.Objects.requireNonNull(Objects.java:209)
	at org.apache.cassandra.index.sai.disk.v1.segment.SegmentMetadata.<init>(SegmentMetadata.java:102)
	at org.apache.cassandra.index.sai.disk.v1.MemtableIndexWriter.flush(MemtableIndexWriter.java:166)
	at org.apache.cassandra.index.sai.disk.v1.MemtableIndexWriter.complete(MemtableIndexWriter.java:125)
	at org.apache.cassandra.index.sai.disk.StorageAttachedIndexWriter.complete(StorageAttachedIndexWriter.java:185)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
	at java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1092)
	at org.apache.cassandra.io.sstable.format.SSTableWriter.commit(SSTableWriter.java:289)
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.commit(SimpleSSTableMultiWriter.java:90)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1354)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1253)
	at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
{code}

Discovered while testing CASSANDRA-18753."
CASSANDRA-19458,Minor bugs in generate.sh -d,Option -d does not work if you don't skip repetition of tests. It should error out.
CASSANDRA-19457,Object reference in Micrometer metrics prevent GC from reclaiming Session instances,"There is a memory leak of previous closed {{{}DefaultSession{}}}s. It can be reproduced by this:
{code:java}
    public static void main(String[] args) throws InterruptedException {
        Semaphore sema = new Semaphore(20);
        for (int i = 0; i < 10000; i++) {
            new Thread(() -> {
                try {
                    sema.acquire();
                    try(CqlSession session = CqlSession.builder()
                            .withCloudSecureConnectBundle(Paths.get(""bundle.zip""))
                            .withAuthCredentials(""token"", ""<some token here>"")
                            .build()) {
                        // Do stuff
                    }
                } catch (Exception e) {
                    System.out.println(e);
                } finally {
                    sema.release();
                }
            }).start();
        }
    }{code}
On initial investigation, it seems like {{MicrometerMetricUpdater.initializeGauge()}} uses {{{}Gauge.{}}}{{{}_builder()_{}}} _using_ {{_Supplier_}} _._ This creates a strong reference that is causing the issue."
CASSANDRA-19456,invalid,
CASSANDRA-19455,Create Trigger is not working on 4.x version cassandra,"Current version is 3.11.16  , try to upgrade 4.0.0 , 4.0.5 ,4.0.12 and both of 4.x version gives below error.

 

Reload triggers is executed , custom trigger jar put in correct path ""../conf/triggers"".

 

cassandra@cqlsh:radius> CREATE TRIGGER radacct_trigger ON radius.radacct USING 'com.XXXX.cassandra.trigger.RadAcctTrigger';

 

InvalidRequest: Error from server: code=2200 [Invalid query] message=""Trigger class 'com.XXXX.cassandra.trigger.RadAcctTrigger' couldn't be loaded""

 

 "
CASSANDRA-19454,Revert switch to approximate time in Dispatcher to avoid mixing with nanoTime() in downstream timeout calculations,"CASSANDRA-15241 changed {{Dispatcher}} to use the {{approxTime}} implementation of {{MonotonicClock}} rather than {{nanoTime()}}, but clock drift between the two, can potentially cause queries to time out more quickly. We should be able to revert the {{Dispatcher}} to use {{nanoTime()}} again and similarly change {{QueriesTable} to {{nanoTime()}} as well for consistency."
CASSANDRA-19453,Enabling remote JMX fails to start,"If you set LOCAL_JMX to something other than 'yes' in conf/cassandra-env.sh, you receive:

{noformat}
Exception (java.lang.ExceptionInInitializerError) encountered during startup: null
java.lang.ExceptionInInitializerError
        at org.apache.cassandra.utils.JMXServerUtils.configureJmxAuthentication(JMXServerUtils.java:188)
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:106)
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:154)
        at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:172)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:240)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:721)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:855)
Caused by: java.lang.RuntimeException: java.lang.IllegalAccessException: access to public member failed: com.sun.jmx.remote.security.JMXPluggableAuthenticator.<init>[Ljava.lang.Object;@afb5821/invokeSpecial, from class org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper (unnamed module @51dcb805)
        at org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper.<clinit>(JMXServerUtils.java:306)
        ... 7 more
Caused by: java.lang.IllegalAccessException: access to public member failed: com.sun.jmx.remote.security.JMXPluggableAuthenticator.<init>[Ljava.lang.Object;@afb5821/invokeSpecial, from class org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper (unnamed module @51dcb805)
        at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)
        at java.base/java.lang.invoke.MethodHandles$Lookup.checkAccess(MethodHandles.java:3882)
        at java.base/java.lang.invoke.MethodHandles$Lookup.getDirectConstructorCommon(MethodHandles.java:4117)
        at java.base/java.lang.invoke.MethodHandles$Lookup.getDirectConstructorNoSecurityManager(MethodHandles.java:4111)
        at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectConstructor(MethodHandles.java:3433)
        at org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper.<clinit>(JMXServerUtils.java:302)
        ... 7 more
ERROR [main] 2024-03-01 06:16:00,028 CassandraDaemon.java:877 - Exception encountered during startup
java.lang.ExceptionInInitializerError: null
        at org.apache.cassandra.utils.JMXServerUtils.configureJmxAuthentication(JMXServerUtils.java:188)
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:106)
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:154)
        at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:172)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:240)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:721)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:855)
Caused by: java.lang.RuntimeException: java.lang.IllegalAccessException: access to public member failed: com.sun.jmx.remote.security.JMXPluggableAuthenticator.<init>[Ljava.lang.Object;@afb5821/invokeSpecial, from class org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper (unnamed module @51dcb805)
        at org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper.<clinit>(JMXServerUtils.java:306)
        ... 7 common frames omitted
Caused by: java.lang.IllegalAccessException: access to public member failed: com.sun.jmx.remote.security.JMXPluggableAuthenticator.<init>[Ljava.lang.Object;@afb5821/invokeSpecial, from class org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper (unnamed module @51dcb805)
        at java.base/java.lang.invoke.MemberName.makeAccessException(MemberName.java:955)
        at java.base/java.lang.invoke.MethodHandles$Lookup.checkAccess(MethodHandles.java:3882)
        at java.base/java.lang.invoke.MethodHandles$Lookup.getDirectConstructorCommon(MethodHandles.java:4117)
        at java.base/java.lang.invoke.MethodHandles$Lookup.getDirectConstructorNoSecurityManager(MethodHandles.java:4111)
        at java.base/java.lang.invoke.MethodHandles$Lookup.unreflectConstructor(MethodHandles.java:3433)
        at org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper.<clinit>(JMXServerUtils.java:302)
        ... 7 common frames omitted
{noformat}"
CASSANDRA-19452,[Analytics] Use constant reference time during bulk read process,"Bulk reader leverages a time provider that returns the current time during read to guide compaction and validation.

As the current time value varies in spark executors, there is a chance that rows/cells get expired inconsistently. Another issue is the validation on no-expired rows/cells after compaction might fail, since they could expire during read. The read can take minutes or even hours.
It could lead to false data omission and job failure.

The fix is to use constant reference time that is decided by Spark driver and distribute to all executors. The reference time is used for compaction and validation later."
CASSANDRA-19451,An Option of Latency Sensitive Load Balancing Policy,"We have received concerns from the community about the 4.x java driver default load balancing policy's performance in terms of busy node avoidance. In some circumstances, when some nodes are busy, the 3.x java driver's `LatencyAwarenessLoadBalancingPolicy` almost does not send requests to the busy nodes, while the 4.x default one will still send a lot of requests to them. This is because the 4.x default one uses in-flight count and responses in the past 200ms to determine a node's health instead of latency. 

Therefore, we want to create another class called `LatencySensitiveLoadBalancingPolicy` in the 4.x java driver, which will combine the in-flight count and the latency. A user can use it by specifying it in `application.conf`.  "
CASSANDRA-19450,Hygiene updates for warnings and pytests," 
 * -Update 'Warning' message to write to stderr-
 * -Replace TimeoutError Exception with builtin (since Python 3.3)-
 * -Remove re.pattern_type (removed since Python 3.7)-
 * Fix mutable arg [] in test/run_cqlsh.py read_until()
 * Remove redirect of stderr to stdout in pytest fixture with tty=false; Deprecation warnings can otherwise fail unit tests when stdout & stderr output is combined.
 * Fix several pycodestyle issues"
CASSANDRA-19449,Support building with JDK17,"We're saying that C* has experimental JDK 17 support yet the 5.0 branch can't be built with it.  

 
{noformat}
ubuntu@ip-172-31-36-36:/usr/local/cassandra/current$ ant
Buildfile: /usr/local/cassandra/trunk/build.xml
     [echo] Non default JDK version used: 17
init:
BUILD FAILED
/usr/local/cassandra/trunk/build.xml:388: Directory /usr/local/cassandra/trunk/build/classes/main creation was not successful for an unknown reason
Total time: 1 second
ubuntu@ip-172-31-36-36:/usr/local/cassandra/current$ java -version
openjdk version ""17.0.10"" 2024-01-16
OpenJDK Runtime Environment (build 17.0.10+7-Ubuntu-122.04.1)
OpenJDK 64-Bit Server VM (build 17.0.10+7-Ubuntu-122.04.1, mixed mode, sharing)
{noformat}
Trying to use a similar flag to -Duse.jdk11=true fails as well:

 
{noformat}
ubuntu@ip-172-31-36-36:/usr/local/cassandra/current$ ant -Duse.jdk17=true
Buildfile: /usr/local/cassandra/trunk/build.xml
     [echo] Non default JDK version used: 17init:BUILD FAILED
/usr/local/cassandra/trunk/build.xml:388: Directory /usr/local/cassandra/trunk/build/classes/main creation was not successful for an unknown reasonTotal time: 0 seconds{noformat}
I see this in build.xml:
{noformat}
<property name=""java.supported"" value=""11,17"" />{noformat}
 

I think a bigger picture question is, why do we require special flags to build different JVM versions?  Is this a limitation of ant, or the way we've configured it?"
CASSANDRA-19447,Register the measurements of the bootstrap as Dropwizard metrics,"Currently, we can view the node's bootstrap state in the following ways:
- via the nodetool cli tool, e.g. by running the ""resume"" command;
- querying the bootstrapped column of the system_veis.local virtual table;

In addition, we can also expose the status and state of the node's bootstrap via JMX. This is used by third-party tools that rely entirely on the JMX API and don't have access to the CQL interface. The operator will be able to get all the information they need from the dashboards without having to use CLIs."
CASSANDRA-19446,Test Failure: dtest-offheap.snapshot_test.TestSnapshot.test_basic_snapshot_and_restore,"Seen here:
[https://ci-cassandra.apache.org/job/Cassandra-5.0/173/testReport/junit/dtest-offheap.snapshot_test/TestSnapshot/test_basic_snapshot_and_restore/]

 
{code:java}
Error Message

failed on teardown with ""TypeError: not all arguments converted during string formatting""
Stacktrace

request = <SubRequest 'fixture_dtest_setup' for <Function test_basic_snapshot_and_restore>>
dtest_config = <dtest_config.DTestConfig object at 0x7f27a8053520>
fixture_dtest_setup_overrides = <dtest_setup_overrides.DTestSetupOverrides object at 0x7f27a43a6550>
fixture_logging_setup = None, fixture_dtest_cluster_name = 'test'
fixture_dtest_create_cluster_func = <function DTestSetup.create_ccm_cluster at 0x7f27a81a2790>

    @pytest.fixture(scope='function', autouse=False)
    def fixture_dtest_setup(request,
                            dtest_config,
                            fixture_dtest_setup_overrides,
                            fixture_logging_setup,
                            fixture_dtest_cluster_name,
                            fixture_dtest_create_cluster_func):
        if running_in_docker():
            cleanup_docker_environment_before_test_execution()
    
        # do all of our setup operations to get the enviornment ready for the actual test
        # to run (e.g. bring up a cluster with the necessary config, populate variables, etc)
        initial_environment = copy.deepcopy(os.environ)
        dtest_setup = DTestSetup(dtest_config=dtest_config,
                                 setup_overrides=fixture_dtest_setup_overrides,
                                 cluster_name=fixture_dtest_cluster_name)
        dtest_setup.initialize_cluster(fixture_dtest_create_cluster_func)
    
        if not dtest_config.disable_active_log_watching:
            dtest_setup.begin_active_log_watch()
    
        # at this point we're done with our setup operations in this fixture
        # yield to allow the actual test to run
        yield dtest_setup
    
        # phew! we're back after executing the test, now we need to do
        # all of our teardown and cleanup operations
    
        reset_environment_vars(initial_environment)
        dtest_setup.jvm_args = []
    
        for con in dtest_setup.connections:
            con.cluster.shutdown()
        dtest_setup.connections = []
    
        failed = False
        try:
            if not dtest_setup.allow_log_errors:
                errors = check_logs_for_errors(dtest_setup)
                if len(errors) > 0:
                    failed = True
                    pytest.fail('Unexpected error found in node logs (see stdout for full details). Errors: [{errors}]'
                                .format(errors=str.join("", "", errors)), pytrace=False)
        finally:
            try:
                # save the logs for inspection
                if failed or not dtest_config.delete_logs:
>                   copy_logs(request, dtest_setup.cluster)

conftest.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
conftest.py:291: in copy_logs
    shutil.copyfile(file, os.path.join(logdir, target_name))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

src = '/home/cassandra/cassandra/build/run-python-dtest.PRhg7u/dtest-pqe8_k2h/test/node1/logs/gc.log'
dst = 'logs/1708958581606_test_basic_snapshot_and_restore/node1_gc.log'

    def copyfile(src, dst, *, follow_symlinks=True):
        """"""Copy data from src to dst in the most efficient way possible.
    
        If follow_symlinks is not set and src is a symbolic link, a new
        symlink will be created instead of copying the file it points to.
    
        """"""
        sys.audit(""shutil.copyfile"", src, dst)
    
        if _samefile(src, dst):
            raise SameFileError(""{!r} and {!r} are the same file"".format(src, dst))
    
        file_size = 0
        for i, fn in enumerate([src, dst]):
            try:
                st = _stat(fn)
            except OSError:
                # File most likely does not exist
                pass
            else:
                # XXX What about other special files? (sockets, devices...)
                if stat.S_ISFIFO(st.st_mode):
                    fn = fn.path if isinstance(fn, os.DirEntry) else fn
                    raise SpecialFileError(""`%s` is a named pipe"" % fn)
                if _WINDOWS and i == 0:
                    file_size = st.st_size
    
        if not follow_symlinks and _islink(src):
            os.symlink(os.readlink(src), dst)
        else:
>           with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:
E           FileNotFoundError: [Errno 2] No such file or directory: '/home/cassandra/cassandra/build/run-python-dtest.PRhg7u/dtest-pqe8_k2h/test/node1/logs/gc.log'

/usr/lib/python3.8/shutil.py:264: FileNotFoundError

During handling of the above exception, another exception occurred:

request = <SubRequest 'fixture_dtest_setup' for <Function test_basic_snapshot_and_restore>>
dtest_config = <dtest_config.DTestConfig object at 0x7f27a8053520>
fixture_dtest_setup_overrides = <dtest_setup_overrides.DTestSetupOverrides object at 0x7f27a43a6550>
fixture_logging_setup = None, fixture_dtest_cluster_name = 'test'
fixture_dtest_create_cluster_func = <function DTestSetup.create_ccm_cluster at 0x7f27a81a2790>

    @pytest.fixture(scope='function', autouse=False)
    def fixture_dtest_setup(request,
                            dtest_config,
                            fixture_dtest_setup_overrides,
                            fixture_logging_setup,
                            fixture_dtest_cluster_name,
                            fixture_dtest_create_cluster_func):
        if running_in_docker():
            cleanup_docker_environment_before_test_execution()
    
        # do all of our setup operations to get the enviornment ready for the actual test
        # to run (e.g. bring up a cluster with the necessary config, populate variables, etc)
        initial_environment = copy.deepcopy(os.environ)
        dtest_setup = DTestSetup(dtest_config=dtest_config,
                                 setup_overrides=fixture_dtest_setup_overrides,
                                 cluster_name=fixture_dtest_cluster_name)
        dtest_setup.initialize_cluster(fixture_dtest_create_cluster_func)
    
        if not dtest_config.disable_active_log_watching:
            dtest_setup.begin_active_log_watch()
    
        # at this point we're done with our setup operations in this fixture
        # yield to allow the actual test to run
        yield dtest_setup
    
        # phew! we're back after executing the test, now we need to do
        # all of our teardown and cleanup operations
    
        reset_environment_vars(initial_environment)
        dtest_setup.jvm_args = []
    
        for con in dtest_setup.connections:
            con.cluster.shutdown()
        dtest_setup.connections = []
    
        failed = False
        try:
            if not dtest_setup.allow_log_errors:
                errors = check_logs_for_errors(dtest_setup)
                if len(errors) > 0:
                    failed = True
                    pytest.fail('Unexpected error found in node logs (see stdout for full details). Errors: [{errors}]'
                                .format(errors=str.join("", "", errors)), pytrace=False)
        finally:
            try:
                # save the logs for inspection
                if failed or not dtest_config.delete_logs:
                    copy_logs(request, dtest_setup.cluster)
            except Exception as e:
>               logger.error(""Error saving log:"", str(e))

conftest.py:373: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/usr/lib/python3.8/logging/__init__.py:1475: in error
    self._log(ERROR, msg, args, **kwargs)
/usr/lib/python3.8/logging/__init__.py:1589: in _log
    self.handle(record)
/usr/lib/python3.8/logging/__init__.py:1599: in handle
    self.callHandlers(record)
/usr/lib/python3.8/logging/__init__.py:1661: in callHandlers
    hdlr.handle(record)
/usr/lib/python3.8/logging/__init__.py:954: in handle
    self.emit(record)
/usr/lib/python3.8/logging/__init__.py:1093: in emit
    self.handleError(record)
/usr/lib/python3.8/logging/__init__.py:1085: in emit
    msg = self.format(record)
/usr/lib/python3.8/logging/__init__.py:929: in format
    return fmt.format(record)
/usr/lib/python3.8/logging/__init__.py:668: in format
    record.message = record.getMessage()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <LogRecord: conftest, 40, /home/cassandra/cassandra-dtest/conftest.py, 373, ""Error saving log:"">

    def getMessage(self):
        """"""
        Return the message for this LogRecord.
    
        Return the message for this LogRecord after merging any user-supplied
        arguments with the message.
        """"""
        msg = str(self.msg)
        if self.args:
>           msg = msg % self.args
E           TypeError: not all arguments converted during string formatting

/usr/lib/python3.8/logging/__init__.py:373: TypeError
{code}
"
CASSANDRA-19445,"Cassandra 4.1.4 floods logs with ""Completed 0 uncommitted paxos instances for""","Hello,

On our cluster logs are flooded with: 
{code:java}
INFO  [OptionalTasks:1] 2024-02-27 14:27:51,213 PaxosCleanupLocalCoordinator.java:185 - Completed 0 uncommitted paxos instances for XXXXXXXXX on ranges [(9210458530128018597,-9222146739399525061], (-9222146739399525061,-9174246180597321488], (-9174246180597321488,-9155837684527496840], (-9155837684527496840,-9148981328078890812], (-9148981328078890812,-9141853035919151700], (-9141853035919151700,-9138872620588476741], {code}
I cannot find anything in doc regarding this longline. Also this are huge log payloads that heavy flood system.log. "
CASSANDRA-19444,AccordRepairJob should be async like CassandraRepairJob,The thread that manages repairs needs to be available and not block.
CASSANDRA-19442,Update access of ClearSnapshotStrategy,"Want to update access of ClearSnapshotStrategy added to Cassandra Analytics library to allow setting TTL for snapshots created by bulk reader. Currently the access of the class is package private, for plugging in custom implementation, we need access outside the package."
CASSANDRA-19440,Non-serial writes can race with Accord topology changes,"Accord and Paxos handle these, but non-SERIAL writes don't check for this condition and can't retry the portions of the write that failed on the correct system until the entire thing succeeds."
CASSANDRA-19438,Accord barriers need to handle racing with topology changes,Topology changes can result in the ranges sent to Accord including things not managed by Accord. It might be sufficient to have the range barriers automatically remove the unsupported subranges since that might be sufficient for the caller.
CASSANDRA-19435,Hint delivery doesn't write through Accord,Hint delivery doesn't write through Accord which would make txn recovery non-deterministic.
CASSANDRA-19434,Batch log doesn't write through Accord during Accord migration,This can result in writes not through Accord occurring which makes txn recovery non-deterministic
CASSANDRA-19430,Read repair through Accord needs to only route the read repair through Accord if the range is actually migrated/running on Accord,This is because the read repair will simply fail if Accord doesn't manage that range. Not only does it need to be routed through Accord but if it races with topology change it needs to retry and not surface an error.
CASSANDRA-19428,Clean up KeyRangeIterator classes,Remove KeyRangeIterator.current and simplify
CASSANDRA-19427,Fix concurrent access of ClientWarn causing AIOBE for SELECT WHERE IN queries with multiple coordinator-local partitions,"On one of our clusters, we noticed rare but periodic ArrayIndexOutOfBoundsExceptions:

 
{code:java}
message=""Uncaught exception on thread Thread[ReadStage-3,5,main]""
exception=""java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2579)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ArrayIndexOutOfBoundsException""{code}
 

 

The error was in a Runnable, so the stacktrace didn't directly indicate where the error was coming from. We enabled JFR to log the underlying exception that was thrown:
 
{code:java}
message=""Uncaught exception on thread Thread[ReadStage-2,5,main]"" exception=""java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 0
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2579)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 0
at java.base/java.util.ArrayList.add(ArrayList.java:487)
at java.base/java.util.ArrayList.add(ArrayList.java:499)
at org.apache.cassandra.service.ClientWarn$State.add(ClientWarn.java:84)
at org.apache.cassandra.service.ClientWarn$State.access$000(ClientWarn.java:77)
at org.apache.cassandra.service.ClientWarn.warn(ClientWarn.java:51)
at org.apache.cassandra.db.ReadCommand$1MetricRecording.onClose(ReadCommand.java:596)
at org.apache.cassandra.db.transform.BasePartitions.runOnClose(BasePartitions.java:70)
at org.apache.cassandra.db.transform.BaseIterator.close(BaseIterator.java:95)
at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2260)
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2575)
... 6 more""{code}
 
 

An AIOBE on ArrayList.add(E) should only be possible when multiple threads attempt to call the method at the same time.

 

This was seen while executing a SELECT WHERE IN query with multiple partition keys. This exception could happen when multiple local reads are dispatched by the coordinator in org.apache.cassandra.service.reads.AbstractReadExecutor#makeRequests. In this case, multiple local reads exceed the tombstone warning threshold, so multiple tombstone warnings are added to the same ClientWarn.State reference.  Currently, org.apache.cassandra.service.ClientWarn.State#warnings is an ArrayList, which isn't safe for concurrent modification, causing the AIOBE to be thrown.

 

I have a patch available for this, and I'm preparing it now. The patch is simple - it just changes org.apache.cassandra.service.ClientWarn.State#warnings to a thread-safe CopyOnWriteArrayList. I also have a jvm-dtest that demonstrates the issue but doesn't need to be merged - it shows how a SELECT WHERE IN query with local reads that add client warnings can add to the same ClientWarn.State from different threads. I'll push that in a separate branch just for demonstration purposes.

 

Demonstration branch: [https://github.com/apache/cassandra/compare/trunk...aratno:cassandra:CASSANDRA-19427-aiobe-clientwarn-demo]

Fix branch: [https://github.com/apache/cassandra/compare/trunk...aratno:cassandra:CASSANDRA-19427-aiobe-clientwarn-fix] (PR linked below)

 

This appears to have been an issue since at least 3.11, that was the earliest release I checked."
CASSANDRA-19424,Add certificate expiry check to start up validations done in Cassandra Analytics library,Want to add certificate expiry check to startup Keystore validations done in Cassandra Analytics library. This is to fail fast if user certificates are expired. 
CASSANDRA-19422,Fix Git repository links,"I am creating an issue based on this PR (1)

(1) https://github.com/apache/cassandra/pull/3120"
CASSANDRA-19421,expired data in sstable not remove,"Version: cassandra:3.11.3

 

My table is using a TTL of 90 days. but sometimes expired data not deleted, The file size will increase to several hundred gigabytes, occupying a large amount of disk space.

Liked this sstable file:

-rw-r--r-- 1 cassandra cassandra 396G Sep 10 00:16 mc-22009-big-Data.db
-rw-r--r-- 1 cassandra cassandra    9 Sep 10 00:16 mc-22009-big-Digest.crc32
-rw-r--r-- 1 cassandra cassandra  37K Sep 10 00:16 mc-22009-big-Filter.db
-rw-r--r-- 1 cassandra cassandra 533M Sep 10 00:16 mc-22009-big-Index.db
-rw-r--r-- 1 cassandra cassandra  12K Sep 10 00:16 mc-22009-big-Statistics.db
-rw-r--r-- 1 cassandra cassandra  15K Sep 10 00:16 mc-22009-big-Summary.db
-rw-r--r-- 1 cassandra cassandra   92 Sep 10 00:16 mc-22009-big-TOC.txt

By using sstabledump tool, i found all rows expired

    17        {
    18          ""type"" : ""row"",
    19          ""position"" : 104,
    20          ""clustering"" : [ 1689491660981 ],
    21          ""liveness_info"" : \{ ""tstamp"" : ""2023-07-16T07:14:24.273004Z"", ""ttl"" : 7776000, ""expires_at"" : ""2023-10-14T07:14:24Z"", ""expired"" : true },
    22          ""cells"" : [
    23            \{ ""name"" : ""long_v"", ""value"" : 1 }
    24          ]
    25        }

 

This issue makes my disk space estimation unpredictable, so i want get some help.

The attachment contains information on sstablemetadata."
CASSANDRA-19420,WEBSITE - Link fix for mathematical functions on 5.0 landing page,link fix for mathematical functions on 5.0 landing page
CASSANDRA-19418,[Analytics] Report additional bulk analytics job stats for instrumentation,"Currently, the Cassandra bulk analytics library supports a ""dialHome"" API to publish some initial job metadata, which in its current form, is redirected to a log. The intention behind this is to allow custom implementations that can utilize these summarized stats for instrumentation or reporting of client behavior.

This task is meant to enhance this API to allow for additional job metadata to be published both at the Spark executor level and at the task levels to gather stats such as ""success/failure"", ""number of rows written/read"", ""failure reason"" etc."
CASSANDRA-19417,LIST SUPERUSERS cql command,"Developing a new CQL command LIST SUPERUSERS to return list of roles with superuser privilege. This includes roles who acquired superuser privilege in the hierarchy. 

Context: LIST ROLES cql command lists roles, their membership details and displays super=true for immediate superusers. But there can be roles who acquired superuser privilege due to a grant. LIST ROLES command won't display super=true for such roles and the only way to recognize such roles is to look for atleast one row with super=true in the output of LIST ROLES OF <role name> command. While this works to check is a given role has superuser privilege, there may be services (for example, Sidecar) working with C* and may need to maintain list of roles with superuser privilege. There is no existing command/tool to retrieve such roles details. Hence developing this command which returns all roles having superuser privilege."
CASSANDRA-19416,"fix ""if"" condition for mx4j tool in cassandra-env.sh","There is this in cassandra-env.sh

{code}
if [[ ""$MX4J_ADDRESS"" == \-Dmx4jaddress* ]]; then
{code}

(similar for port)

This is wrong for /bin/sh shell (our shebang in bin/cassandra) and this does not work, probably in bash only, because /bin/sh does not understand what ""[["" is nor it understand what ""=="" is.

The reason this was never detected so far is that the logic will never come there when MX4J_ADDRESS and / or MX4J_PORT is commented out couple lines above."
CASSANDRA-19414,Skinny dev circle workflow,"CircleCi CI runs are getting pretty heavy. During dev iterations we trigger many CI pre-commit jobs which are just an overkill.

This ticket has the purpose to purge from the pre-commit workflow all variations of the test matrix but the vanilla one. That should enable us for a quick and cheap to iterate *during dev*, this is not a substitute for pre-commit . This ticket's work will serve as the basis for the upcoming changes being discussed [atm|https://lists.apache.org/thread/qf5c3hhz6qkpyqvbd3sppzlmftlc0bw0]"
CASSANDRA-19412,delete useless collection:backPressureHosts in the sendToHintedReplicas to improve write performance,"Every normal write request will go through this method({_}*sendToHintedReplicas*{_}). However, the list:backPressureHosts in the method has never been used functionally.

The _*backpressure*_ was introduced by:
{code:java}
Support optional backpressure strategies at the coordinator
patch by Sergio Bossa; reviewed by Stefania Alborghetti for CASSANDRA-9318

d43b9ce5 Sergio Bossa <sergio.bossa@gmail.com> on 2016/9/19 at 10:42 AM {code}
{code:java}
public static void sendToHintedEndpoints(final Mutation mutation,
                                             Iterable<InetAddress> targets,
                                             AbstractWriteResponseHandler<IMutation> responseHandler,
                                             String localDataCenter,
                                             Stage stage)
    throws OverloadedException
    {
        int targetsSize = Iterables.size(targets);
        // this dc replicas:
        Collection<InetAddress> localDc = null;
        // extra-datacenter replicas, grouped by dc
        Map<String, Collection<InetAddress>> dcGroups = null;
        // only need to create a Message for non-local writes
        MessageOut<Mutation> message = null;
        boolean insertLocal = false;
        ArrayList<InetAddress> endpointsToHint = null;
        List<InetAddress> backPressureHosts = null;
        for (InetAddress destination : targets)
        {
            checkHintOverload(destination);
            if (FailureDetector.instance.isAlive(destination))
            {
                if (canDoLocalRequest(destination))
                {
                    insertLocal = true;
                }
                else
                {
                    // belongs on a different server
                    if (message == null)
                        message = mutation.createMessage();
                    String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(destination);
                    // direct writes to local DC or old Cassandra versions
                    // (1.1 knows how to forward old-style String message IDs; updated to int in 2.0)
                    if (localDataCenter.equals(dc))
                    {
                        if (localDc == null)
                            localDc = new ArrayList<>(targetsSize);
                        localDc.add(destination);
                    }
                    else
                    {
                        Collection<InetAddress> messages = (dcGroups != null) ? dcGroups.get(dc) : null;
                        if (messages == null)
                        {
                            messages = new ArrayList<>(3); // most DCs will have <= 3 replicas
                            if (dcGroups == null)
                                dcGroups = new HashMap<>();
                            dcGroups.put(dc, messages);
                        }
                        messages.add(destination);
                    }
                    if (backPressureHosts == null)
                        backPressureHosts = new ArrayList<>(targetsSize);
                    backPressureHosts.add(destination);
                }
            }
            else
            {
                if (shouldHint(destination))
                {
                    if (endpointsToHint == null)
                        endpointsToHint = new ArrayList<>(targetsSize);
                    endpointsToHint.add(destination);
                }
            }
        }
        if (backPressureHosts != null)
            MessagingService.instance().applyBackPressure(backPressureHosts, responseHandler.currentTimeout());
        if (endpointsToHint != null)
            submitHint(mutation, endpointsToHint, responseHandler);
        if (insertLocal)
            performLocally(stage, Optional.of(mutation), mutation::apply, responseHandler);
        if (localDc != null)
        {
            for (InetAddress destination : localDc)
                MessagingService.instance().sendRR(message, destination, responseHandler, true);
        }
        if (dcGroups != null)
        {
            // for each datacenter, send the message to one node to relay the write to other replicas
            for (Collection<InetAddress> dcTargets : dcGroups.values())
                sendMessagesToNonlocalDC(message, dcTargets, responseHandler);
        }
    } {code}
Now the backPressure related codes had been deleted in the codebase, and here maybe someone forgot to remove the collection: backPressureHosts. Removing it will save every write request to add a few items to a list to reduce the memory footprint"
CASSANDRA-19411,[Analytics] Bulk reader fails to produce a row when regular column values are null,"Bulk Reader won't emit a row when the regular column values are all null. For example,
a schema {{PK}} = {{a,b}} ; {{CK}} = {{c,d}} ; and columns = {{e,f}}

|| a    || b     || c    || d     || e     || f      ||
| {{pk1}} |  {{pk2}} | {{ck1}} |  {{ck2}} |  {{null}} |  {{null}} |

when queried from bulk reader, it won't produce a row."
CASSANDRA-19410,Bug in generate.sh removal of jobs,Currently the removal of jobs in generate.sh for circle CI has regexp problem which can match unwanted jobs
CASSANDRA-19409,Test Failure: dtest-upgrade.upgrade_tests.upgrade_through_versions_test.*,"Failing in Jenkins:
 * [dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD/test_parallel_upgrade_with_internode_ssl/]
 * [dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_parallel_upgrade_with_internode_ssl/]
 * [dtest-upgrade-novnode.upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD.test_parallel_upgrade|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade-novnode.upgrade_tests.upgrade_through_versions_test/TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD/test_parallel_upgrade/]
 * [dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_parallel_upgrade/]
 * [dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_parallel_upgrade_with_internode_ssl/]"
CASSANDRA-19405,"(Accord) AsyncChain.flatMap does not begin the result of the flatMap, which causes AsyncLoader to hang in some cases","While a Node is in the Saving state, but linked, we can call org.apache.cassandra.service.accord.AccordStateCache#maybeEvictSomeNodes which will promote the value to Loaded.  Now that the node is Loaded we are allowed to evict it!"
CASSANDRA-19404,Unexpected NullPointerException in ANN+WHERE when adding rows in another partition,"* *Bug observed on the Docker image 5.0-beta1*
 * *Bug also observed on latest head of Cassandra repo (as of 2024-02-15)*
 * _*(working fine on vsearch branch of datastax/cassandra, commit hash 80c2f8b9ad5b89efee0645977a5ca53943717c0d)*_

Summary: A query with _ann + where clause on a map + where clause on the partition key_ starts erroring once there are other partitions in the table.

There are three SELECT statements in the repro minimal code below - the third is where the error is triggered.
{code:java}
// reproduced with Dockerized Cassandra 5.0-beta1 on 2024-02-15

/////////
// SCHEMA
/////////

CREATE TABLE ks.v_table (
    pk int,
    row_v vector<float, 2>,
    metadata map<text, text>,
    PRIMARY KEY (pk)
);
CREATE CUSTOM INDEX v_md
    ON ks.v_table (entries(metadata))
    USING 'StorageAttachedIndex';
CREATE CUSTOM INDEX v_idx
    ON ks.v_table (row_v)
    USING 'StorageAttachedIndex';


/////////////////////////////
// SELECT WORKS (empty table)
/////////////////////////////

SELECT * FROM ks.v_table
    WHERE metadata['map_k'] = 'map_v'
        AND pk = 0
    ORDER BY row_v ANN OF [0.1, 0.2]
    LIMIT 4;


//////////////
// ADD ONE ROW
//////////////

INSERT INTO ks.v_table (pk, metadata, row_v)
VALUES
    (0, {'map_k': 'map_v'}, [0.11, 0.19]);



/////////////////////////////////////////////
// SELECT WORKS (table has queried partition)
/////////////////////////////////////////////

SELECT * FROM ks.v_table
    WHERE metadata['map_k'] = 'map_v'
        AND pk = 0
    ORDER BY row_v ANN OF [0.1, 0.2]
    LIMIT 4;


//////////////////////////////////
// ADD ONE ROW (another partition)
//////////////////////////////////

INSERT INTO ks.v_table (pk, metadata, row_v)
VALUES
    (10, {'map_k': 'map_v'}, [0.11, 0.19]);


/////////////////////////////////////////////////
// SELECT BREAKS (table gained another partition)
/////////////////////////////////////////////////

SELECT * FROM ks.v_table
    WHERE metadata['map_k'] = 'map_v'
        AND pk = 0
    ORDER BY row_v ANN OF [0.1, 0.2]
    LIMIT 4; {code}
The error has this appearance in CQL Console:
{code:java}
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures: UNKNOWN from /172.17.0.2:7000"" info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'failures': 1, 'error_code_map': {'172.17.0.2': '0x0000'}} {code}
And the Cassandra logs have this to say:
{code:java}
java.lang.NullPointerException: Cannot invoke ""org.apache.cassandra.index.sai.iterators.KeyRangeIterator.skipTo(org.apache.cassandra.index.sai.utils.PrimaryKey)"" because ""this.nextIterator"" is null {code}
 "
CASSANDRA-19401,Nodetool import expects directory structure,"According to the [documentation|https://cassandra.apache.org/doc/4.1/cassandra/operating/bulk_loading.html] the nodetool import should not rely on the folder structure of the imported sst files:

{quote}
Because the keyspace and table are specified on the command line for nodetool import, there is not the same requirement as with sstableloader, to have the SSTables in a specific directory path. When importing snapshots or incremental backups with nodetool import, the SSTables don’t need to be copied to another directory.
{quote}

However when importing old cassandra snapshots, we figured out, that sstables still need to be in a directory called like $KEYSPACE/$TABLENAME files, even when keyspace and table name are already present as parameters for the nodetool import call.

Call we used:

{code}
nodetool import --copy-data mykeyspace mytable /full_path_to/test1
{code}

Log:

{code}
INFO  [RMI TCP Connection(21)-127.0.0.1] 2024-02-15 10:41:06,565 SSTableImporter.java:72 - Loading new SSTables for mykeyspace/mytable: Options{srcPaths='[/full_path_to/test1]', resetLevel=true, clearRepaired=true, verifySSTables=true, verifyTokens=true, invalidateCaches=true, extendedVerify=false, copyData= true}
INFO  [RMI TCP Connection(21)-127.0.0.1] 2024-02-15 10:41:06,566 SSTableImporter.java:173 - No new SSTables were found for mykeyspace/mytable
{code}

However, when we move the sstables (.db-Files) to {{alternative/mykeyspace/mytable}}

and import with

{code}
nodetool import --copy-data mykeyspace mytable /fullpath/alternative/mykeyspace/mytable
{code}

the import works

{code}
INFO  [RMI TCP Connection(23)-127.0.0.1] 2024-02-15 10:43:36,093 SSTableImporter.java:177 - Loading new SSTables and building secondary indexes for mykeyspace/mytable: [BigTableReader(path='/mnt/ramdisk/cassandra4/data/mykeyspace/mytable-561a12d0cbe611eead78fbfd293cee40/me-2-big-Data.db'), BigTableReader(path='/mnt/ramdisk/cassandra4/data/mykeyspace/mytable-561a12d0cbe611eead78fbfd293cee40/me-1-big-Data.db')]
INFO  [RMI TCP Connection(23)-127.0.0.1] 2024-02-15 10:43:36,093 SSTableImporter.java:190 - Done loading load new SSTables for mykeyspace/mytable
{code}


We experienced this in Cassandra 4.1.3 on Java 11 (Linux)"
CASSANDRA-19400,IndexStatusManager needs to prioritize SUCCESS over UNKNOWN states to maximize availability,"{{IndexStatusManager}} is responsible for knowing what SAI indexes are queryable across the ring, endpoint by endpoint. There are two statuses that SAI treats as queryable, but it should not treat them equally. {{BUILD_SUCCEEDED}} means the index is definitely available and should be able to serve queries without issue. {{UNKNOWN}} indicates that the status of the index hasn’t propagated yet to this coordinator. It may be just fine, or it may not be. If it isn’t a query will not return incorrect results, but it will fail. If there are enough {{BUILD_SUCCEEDED}} replicas, we should ignore {{UNKNOWN}} replicas and maximize availability. If the UNKNOWN replica is going to become {{BUILD_SUCCEEDED}} shortly, it will happily start taking requests at that point and spread the load. If not, we’ll avoid futile attempts to query it too early."
CASSANDRA-19398,Test Failure: org.apache.cassandra.distributed.test.UpgradeSSTablesTest.truncateWhileUpgrading,"[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2646/workflows/bc2bba74-9e56-4bea-8de7-4ff840c4f450/jobs/56028/tests#failed-test-0]


{code:java}
junit.framework.AssertionFailedError at org.apache.cassandra.distributed.test.UpgradeSSTablesTest.truncateWhileUpgrading(UpgradeSSTablesTest.java:220) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){code}"
CASSANDRA-19397,Remove all code around native_transport_port_ssl,We deprecated native_transport_port_ssl in CASSANDRA-19392 and we told we go to remove it next. This ticket is about that removal. 
CASSANDRA-19394,Rethink dumping of cluster metadata via CMSOperationsMBean,"I think there are two problems in the implementation of dumping ClusterMetadata in CMSOperationsMBean

1) A dump is saved in a file and dumpClusterMetadata methods will return just a file name where that dump is. However, nodetool / JMX call to MBean (or any place this method is invoked from, we would like to offer a command in nodetool which returns the dump) is meant to be used from anywhere, remotely, so what happens when we execute nodetool or call these methods on a machine different from a machine a node runs on? E.g. admins can just have some jumpbox to a cluster they manage, they do not necessarily have access to nodes themselves. So they would not be able to read it.

2) It creates temp file which is not deleted so /tmp will be populated with these dumps until node is turned off which might take a lot of time and can consume a lot of disk space if dumps are done frequently and they are big. An adversary might just dump cluster metadata until no disk space is left.

What I propose is that we would return all dump string, not just a filename where we save it. We can also format the output on the client or we can tell server what format we want the dump to be returned in. 

If there is a concern about size of data to be returned, we might optionally allow dumps to be returned as compressed by simple zipping on server and unzipping on client where ""zipper"" is a standard java.util.zip so it basically doesn't matter what jvm runs on client and server."
CASSANDRA-19393,nodetool: group CMS-related commands into one command,"The purpose of this ticket is to group all CMS-related commands under one ""nodetool cms"" command where existing command would be subcommands of it."
CASSANDRA-19392,deprecate dual ports support (native_transport_port_ssl) ,"We decided (1) to deprecate dual ports support in 5.0 (and eventually remove it in trunk). This ticket will track the work towards the deprecation for 5.0.

(1) https://lists.apache.org/thread/dow196gspwgp2og576zh3lotvt6mc3lv"
CASSANDRA-19391,Flush metadata snapshot table on every write,"We depend on the latest snapshot when starting up, flushing avoids gaps between latest snapshot and the most recent local log entry"
CASSANDRA-19390,Transformation.Kind should contain an explicit integer id,
CASSANDRA-19387,invalid,"invalid

 "
CASSANDRA-19384,Avoid exposing intermediate node state during startup,"During startup we replay the local log, during this time we might expose intermediate node states (via JMX for example)."
CASSANDRA-19380,Enhance CqlSession to refresh the auth credentials dynamically,"Our team is working on a feature to automatically deliver the credentials to application when the credentials are rotated without application teams restarting the application. Is this currently supported with ""com.datastax.oss.driver.api.core.CqlSession""?  If yes, please provide a sample code for the same.

 

If this feature isn't supported, is it feasible to enhance? Below is our request,
 # To dynamically refresh the credentials at runtime within existing CqlSession instance.
 # Internally drain the old connections gracefully in resilient manner and establish the new connections with new credentials without doubling the overall number of connections.

 

This is how we currently set the auth credentials at application startup.

 
{code:java}
cqlSessionBuilder.withConfigLoader(driverConfigLoader)
                    .withAuthCredentials(userName, password); {code}
 

 "
CASSANDRA-19377,Startup Validation Failures when Checking Sidecar Connectivity (Analytics),"We have experienced repeated startup validation failures caused by Sidecar health checks for some jobs with a large number of Spark executors.
It is worth increasing the overall timeout for Sidecar health checks from current 30 seconds to 5 minutes and making it configurable."
CASSANDRA-19375,Link in docs to Achilles Java Driver links to malicious site,"https://cassandra.apache.org/doc/4.1/cassandra/getting_started/drivers.html#java

The Achilles link looks dangerous. I tried it and it looked like the link has been taken over by a malicious user."
CASSANDRA-19373,"5.0 updates FQL format, but doesn't update version or handle reading old version files",
CASSANDRA-19372,WESBITE - Adding blog post,"Adding blog post to website.

Apache Cassandra 5.0 Features: Mathematical CQL Functions"
CASSANDRA-19369,[Analytics] Use XXHash32 for digest calculation of SSTables,"During bulk writes, Cassandra Analytics calculates the MD5 checksum of every SSTable it produces. During SSTable upload to Cassandra Sidecar, Cassandra Analytics includes the {{content-md5}} header as part of the upload request. This information is used by Cassandra Sidecar to validate the integrity of the uploaded SSTable and prevent issues with bit flips and corrupted SSTables.

Recently, Cassandra Sidecar introduced [support for additional checksum validations|https://issues.apache.org/jira/browse/CASSANDRASC-97] during SSTable upload. Notably the XXHash32 digest support was added which offers for more performant checksum calculations. This support now allows Cassandra Analytics to use a more efficient digest algorithm that is friendlier on the CPU usage of Sidecar and spark resources."
CASSANDRA-19368,Add way for SAI to disable row to token index so internal tables may leverage SAI,"Internal tables tend to use LocalPartitioner and may not actually have murmur tokens but rather LocalPartitioner, which is variable length bytes tokens!  For internal use cases we don’t always care about paging so don’t really need this index to function.

The use case motivating this work is for Accord, we wish to add a custom SAI index on the system_accord.commands#routes column.  Since this logic is purely internal we don’t care about paging, but can not leverage SAI at this moment as it hard codes murmur tokens, and fails during memtable flush"
CASSANDRA-19367,Refactor SAI so the selection of the index type is not scattered to multiple places,For Accord we want to write an internal index and finding plugging into SAI is a bit more channeling than it could be… we need to find multiple places where the SAI code “infer” the index type so it can delegate… this logic should be done once and made pluggable so custom SAI indexes can be defined
CASSANDRA-19366,"Expose mode of authentication in system_views.clients, nodetool clientstats, and ClientMetrics","CASSANDRA-18554 added support for mTLS-authenticated clients. Part of this contribution introduced {{{}MutualTlsWithPasswordFallbackAuthenticator{}}}, which enables Cassandra to support either password and mTLS-authenticated connections.

As an operator, it would be useful to know which connections are mTLS authenticated, and which are password authenticated, as a possible mode of operation is migrating users from one from of authentication to another. It would also be useful to know if that if authentication attempts are failing which mode of authentication is unsuccessful.

Proposing to add the following:
 * Add a {{mode: string}} and {{metadata: map<string, string>}} to {{{}AuthenticatedUser{}}}. Update existing {{IAuthenticator}} implementations to pass {{mode}} (e.g. {{password}} , {{{}mtls{}}}), and optionally pass a {{metadata}} map (e.g. this can include the extracted {{identity}} from a client certificate for {{mtls}} authentication).
 * Update nodetool clientstats to add a new option flag {{{}--metadata{}}}, which when passed exposes these new fields on {{{}AuthenticatedUser{}}}. (Not added to existing output to maintain compatibility, much like {{-client-options}} did.
 * Update {{system_views.clients}} to include columns for these new fields.
 * Add new metrics to {{{}ClientMetrics{}}}:
 ** Track authentication success and failures by mode. (Note: The metrics present by authentication mode scope are contextual based on the Authenticator used (e.g. only {{scope=Password}} will be present for {{{}PasswordAuthenticator{}}})

{noformat}
Existing:

org.apache.cassandra.metrics:name=AuthSuccess,type=Client
org.apache.cassandra.metrics:name=AuthFailure,type=Client

New:

org.apache.cassandra.metrics:name=AuthSuccess,scope=MutualTls,type=Client
org.apache.cassandra.metrics:name=AuthSuccess,scope=Password,type=Client

org.apache.cassandra.metrics:name=AuthFailure,scope=MutualTls,type=Client
org.apache.cassandra.metrics:name=AuthFailure,scope=Password,type=Client
{noformat}
 * 
 ** Track connection counts by mode:

{noformat}
Existing:
org.apache.cassandra.metrics:name=ConnectedNativeClients,type=Client
org.apache.cassandra.metrics:name=connectedNativeClients,type=Client (previously deprecated but still maintained)

New:
org.apache.cassandra.metrics:name=ConnectedNativeClients,scope=MutualTls,type=Client
org.apache.cassandra.metrics:name=ConnectedNativeClients,scope=Password,type=Client
{noformat}
 * 
 ** A metric to track encrypted vs. non-encrypted connections:

{noformat}
org.apache.cassandra.metrics:name=ConnectedNativeClients,scope=Encrypted,type=Client
org.apache.cassandra.metrics:name=ConnectedNativeClients,scope=Unencrypted,type=Client
{noformat}"
CASSANDRA-19361,fix node info NPE when ClusterMetadata is null,"h3. How

 
I create an ensemble with 3 nodes(It works well), then I add the fourth node to join the party. 
when executing nodetool info, get the following exception:
{code:java}
➜  bin ./nodetool info

java.lang.NullPointerException at org.apache.cassandra.service.StorageService.operationMode(StorageService.java:3744) at org.apache.cassandra.service.StorageService.isBootstrapFailed(StorageService.java:3810) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)   

➜  bin ./nodetool info 

WARN  [InternalResponseStage:152] 2024-02-02 11:45:15,731 RemoteProcessor.java:213 - Got error from /127.0.0.4:7000: TIMEOUT when sending TCM_COMMIT_REQ, retrying on CandidateIterator{candidates=[/127.0.0.4:7000], checkLive=true} error: null -- StackTrace -- java.lang.NullPointerException at org.apache.cassandra.service.StorageService.getLocalHostId(StorageService.java:1904) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71) at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:260){code}
server 1 cannot execute node info and cql shell, server 2 and 3 can do it. Try to query the system prefix tables, I attach stack error log for the further debugging. Cannot find a way to recover. After deleting data(losing all data), restart and everything became OK
{code:java}
➜  bin ./nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load  Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.2  ?     16      51.2%             6d194555-f6eb-41d0-c000-000000000002  rack1
DN  127.0.0.4  ?     16      48.8%             6d194555-f6eb-41d0-c000-000000000001  rack1{code}
h3. When

 
It was introduced by the Patch: CEP-21. Anyway, the NPE check is needed to protect its propagation anywhere
{code:java}
Implementation of Transactional Cluster Metadata as described in CEP-21
Hash: ae084237
 
code diff:
 
    public String getLocalHostId()
     {
-        UUID id = getLocalHostUUID();
-        return id != null ? id.toString() : null;
+        return getLocalHostUUID().toString();
     }
 
     public UUID getLocalHostUUID()
     {
-        UUID id = getTokenMetadata().getHostId(FBUtilities.getBroadcastAddressAndPort());
-        if (id != null)
-            return id;
-        // this condition is to prevent accessing the tables when the node is not started yet, and in particular,
-        // when it is not going to be started at all (e.g. when running some unit tests or client tools).
-        else if ((DatabaseDescriptor.isDaemonInitialized() || DatabaseDescriptor.isToolInitialized()) && CommitLog.instance.isStarted())
-            return SystemKeyspace.getLocalHostId();
-
-        return null;
+        // Metadata collector requires using local host id, and flush of IndexInfo may race with
+        // creation and initialization of cluster metadata service. Metadata collector does accept
+        // null localhost ID values, it's just that TokenMetadata was created earlier.
+        ClusterMetadata metadata = ClusterMetadata.currentNullable();
+        if (metadata == null || metadata.directory.peerId(getBroadcastAddressAndPort()) == null)
+            return null;
+        return metadata.directory.peerId(getBroadcastAddressAndPort()).toUUID();
     } {code}"
CASSANDRA-19355,Accord: PreLoadContext must properly and consistently support ranges,"There are some mechanisms for ensuring range transactions are loaded for range transactions, but they do not currently work properly (having several race conditions), are potentially costly in terms of memory consumption, and are inconsistent with how they work for key transactions."
CASSANDRA-19353,Cancel Signal when unused in Local Log,
CASSANDRA-19352,4.x Java driver support for native_port_ssl and native_transport_port_ssl,"DSE 6.8 added a ""native_transport_port_ssl"" column to peers_v2 to indicate when peers were making use of ""native_transport_port_ssl"" in cassandra.yaml.  Similar functionality (with slightly different column names) was brought to OSS Cassandra with CASSANDRA-16999.  3.x Java driver support for these columns has been added (or is in the process of being added) in [JAVA-2967|https://datastax-oss.atlassian.net/browse/JAVA-2967].  This ticket represents the work to implement similar functionality for the 4.x Java driver."
CASSANDRA-19351,[Analytics] No longer need to synchronize on Schema.instance after Cassandra 4.0.12,"We no longer need to synchronize on the {{Schema.instance}} in Analytics after the release of Cassandra 4.0.12, that includes a synchronization fix in https://issues.apache.org/jira/browse/CASSANDRA-18317. We need to clean up the TODOs pending on that code being released"
CASSANDRA-19350,Add cli log level to circle config,"We try to set the [log cli level|https://github.com/apache/cassandra/blob/trunk/.circleci/config_template.yml#L3538] so that dtest logging is captured to stdout, but we don't do it [everywhere|https://github.com/apache/cassandra/blob/trunk/.circleci/config_template.yml#L3063] so we miss some logs."
CASSANDRA-19349,Timeuuid compare is broken,"{{I have stumbled over a wired problem on my pc.}}

{{When i turn on my wifi interface, then some of my integration test are failing.}}

{{The mac part(lsb) of the timeuuids become changed in our Uuid implementation.}}

{{These uuids are used for the cassandra insertions and queries.}}

 

{{TestSetup with ""broken"" Uuids:}}
{code:java}
CREATE TABLE object_comment (
    object timeuuid,
    comment timeuuid,
    value blob,
    PRIMARY KEY (object, comment)
)

INSERT INTO object_comment (object, comment , value) VALUES (95278adc-c03f-11ee-ab43-bb35e932d536, cf9e6440-c01e-11ee-847b-34cff6b1be80, 0x01);
INSERT INTO object_comment (object, comment , value) VALUES (95278adc-c03f-11ee-ab43-bb35e932d536, cf9f75b0-c01e-11ee-847b-34cff6b1be80, 0x02);

// cf9f75b0-c01e-11ee-847b-34cff6b1be7f is lsb-1 and the same timestamp
SELECT * FROM object_comment where object = 95278adc-c03f-11ee-ab43-bb35e932d536 AND comment <= cf9f75b0-c01e-11ee-847b-34cff6b1be7f; object                               | comment                              | value
--------------------------------------+--------------------------------------+-------
 95278adc-c03f-11ee-ab43-bb35e932d536 | cf9e6440-c01e-11ee-847b-34cff6b1be80 |  0x01
 95278adc-c03f-11ee-ab43-bb35e932d536 | cf9f75b0-c01e-11ee-847b-34cff6b1be80 |  0x02(2 rows)
 {code}
 

 

The second row must not be present. The Only row expected is : 
{code:java}
95278adc-c03f-11ee-ab43-bb35e932d536 | cf9e6440-c01e-11ee-847b-34cff6b1be80 |  0x01{code}
 

I think i have found the cause of the issue.

The Methods `org.apache.cassandra.utils.TimeUUID#compareTo` and `org.apache.cassandra.db.marshal.TimeUUIDType#compareCustom` return different results.

Test pseudocode:
{code:java}
var id = UUID.fromString(""cf9f75b0-c01e-11ee-847b-34cff6b1be80"");
var idDecrementInLsb = UUID.fromString(""cf9f75b0-c01e-11ee-847b-34cff6b1be7f"");

// java.util.UUID#compareTo
assertThat(idDecrementInLsb.compareTo(id)).isEqualTo(-1);

var timeUuidDec = org.apache.cassandra.utils.TimeUUID.fromUuid(idDecrementInLsb);
var timeUuidId = org.apache.cassandra.utils.TimeUUID.fromUuid(id);

// org.apache.cassandra.utils.TimeUUID#compareTo
assertThat(timeUuidDec.compareTo(timeUuidId)).isEqualTo(-1);

// org.apache.cassandra.db.marshal.TimeUUIDType.compareCustom
assertThat(org.apache.cassandra.db.marshal.TimeUUIDType.compareCustom(idDecrementInLsb, id1)).isEqualTo(-1); // This fails
 {code}
 "
CASSANDRA-19348,Fix serialization version check in InProgressSequences,
CASSANDRA-19347,Improve Could not perform commit after 4089/10 tries log message,"Improve retry logging for the local processor when using deadline and indefinite strategies:

Currently, it is sometimes possible to get into the situation where we get the following log message:

{code}
java.lang.IllegalStateException: Can not commit transformation: ""SERVER_ERROR""(Could not perform commit after 4089/10 tries. Time remaining: 0ms). 
at org.apache.cassandra.tcm.ClusterMetadataService.lambda$commit$6(ClusterMetadataService.java:470) 
at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:514) 
at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:467) 
at org.apache.cassandra.tcm.Startup.initializeAsFirstCMSNode(Startup.java:139) 
at org.apache.cassandra.tcm.migration.Election.finish(Election.java:141) 
at org.apache.cassandra.tcm.migration.Election.nominateSelf(Election.java:94) 
at org.apache.cassandra.tcm.ClusterMetadataService.upgradeFromGossip(ClusterMetadataService.java:345) 
at org.apache.cassandra.tcm.CMSOperations.initializeCMS(CMSOperations.java:65) 
at org.apache.cassandra.tools.nodetool.InitializeCMS.execute(InitializeCMS.java:37) 
{code}"
CASSANDRA-19344,Range movements involving transient replicas must safely enact changes to read and write replica sets,"(edit) This was originally opened due to a flaky test {{org.apache.cassandra.distributed.test.TransientRangeMovementTest.testRemoveNode-_jdk17}}

The test can fail in two different ways:
{code:java}
junit.framework.AssertionFailedError: NOT IN CURRENT: 31 -- [(00,20), (31,50)] at org.apache.cassandra.distributed.test.TransientRangeMovementTest.assertAllContained(TransientRangeMovementTest.java:203) at org.apache.cassandra.distributed.test.TransientRangeMovementTest.testRemoveNode(TransientRangeMovementTest.java:183) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){code}
as in here - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2639/workflows/32b92ce7-5e9d-4efb-8362-d200d2414597/jobs/55139/tests#failed-test-0]
and
{code:java}
junit.framework.AssertionFailedError: nodetool command [removenode, 6d194555-f6eb-41d0-c000-000000000003, --force] was not successful stdout: stderr: error: Node /127.0.0.4:7012 is alive and owns this ID. Use decommission command to remove it from the ring -- StackTrace -- java.lang.UnsupportedOperationException: Node /127.0.0.4:7012 is alive and owns this ID. Use decommission command to remove it from the ring at org.apache.cassandra.tcm.sequences.SingleNodeSequences.removeNode(SingleNodeSequences.java:110) at org.apache.cassandra.service.StorageService.removeNode(StorageService.java:3682) at org.apache.cassandra.tools.NodeProbe.removeNode(NodeProbe.java:1020) at org.apache.cassandra.tools.nodetool.RemoveNode.execute(RemoveNode.java:51) at org.apache.cassandra.tools.NodeTool$NodeToolCmd.runInternal(NodeTool.java:388) at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:373) at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:272) at org.apache.cassandra.distributed.impl.Instance$DTestNodeTool.execute(Instance.java:1129) at org.apache.cassandra.distributed.impl.Instance.lambda$nodetoolResult$51(Instance.java:1038) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:833) Notifications: Error: java.lang.UnsupportedOperationException: Node /127.0.0.4:7012 is alive and owns this ID. Use decommission command to remove it from the ring at org.apache.cassandra.tcm.sequences.SingleNodeSequences.removeNode(SingleNodeSequences.java:110) at org.apache.cassandra.service.StorageService.removeNode(StorageService.java:3682) at org.apache.cassandra.tools.NodeProbe.removeNode(NodeProbe.java:1020) at org.apache.cassandra.tools.nodetool.RemoveNode.execute(RemoveNode.java:51) at org.apache.cassandra.tools.NodeTool$NodeToolCmd.runInternal(NodeTool.java:388) at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:373) at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:272) at org.apache.cassandra.distributed.impl.Instance$DTestNodeTool.execute(Instance.java:1129) at org.apache.cassandra.distributed.impl.Instance.lambda$nodetoolResult$51(Instance.java:1038) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:833) at org.apache.cassandra.distributed.api.NodeToolResult$Asserts.fail(NodeToolResult.java:214) at org.apache.cassandra.distributed.api.NodeToolResult$Asserts.success(NodeToolResult.java:97) at org.apache.cassandra.distributed.test.TransientRangeMovementTest.testRemoveNode(TransientRangeMovementTest.java:173) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){code}
as in here - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2634/workflows/24617d26-e297-4857-bc43-b6a04e64a6ea/jobs/54534/tests#failed-test-0]"
CASSANDRA-19343,Test Failure: org.apache.cassandra.fuzz.ring.ConsistentBootstrapTest.coordinatorIsBehindTest,"{code:java}
java.lang.IllegalStateException: Can't use shutdown instances, delegate is null at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegate(AbstractCluster.java:283) at org.apache.cassandra.distributed.impl.DelegatingInvokableInstance.transfer(DelegatingInvokableInstance.java:49) at org.apache.cassandra.distributed.api.IInvokableInstance.runsOnInstance(IInvokableInstance.java:45) at org.apache.cassandra.distributed.api.IInvokableInstance.runOnInstance(IInvokableInstance.java:46) at org.apache.cassandra.distributed.shared.ClusterUtils.unpauseCommits(ClusterUtils.java:548) at org.apache.cassandra.fuzz.ring.ConsistentBootstrapTest.coordinatorIsBehindTest(ConsistentBootstrapTest.java:227) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){code}

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2636/workflows/93adbf3e-acf8-4a62-a1f6-baf4f4689347/jobs/54912/tests"
CASSANDRA-19342,Test Failure: org.apache.cassandra.distributed.test.RemoveNodeTest.testAbort,"{code:java}
junit.framework.AssertionFailedError: ""RemovalStatus: No removals in progress. "" does not contain MID_LEAVE at org.apache.cassandra.distributed.test.RemoveNodeTest.testAbort(RemoveNodeTest.java:54) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43){code}

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra?branch=testAbort-trunk"
CASSANDRA-19341,Relation and Restriction hierarchies are too complex and error prone,"The {{Relation}} and {{Restriction}} hierarchy have been designed when C* was only supporting a limited amount of operators and columns expressions (single column, multi-column and token expressions). Over time they have grown in complexity making the code harder to understand and modify and error prone. Their design is also resulting in unnecessary limitations that could be easily lifted, like the ability to accept different predicates on the same column.

Today adding a new operator requires the addition of a lot of glue code and surgical changes accross the CQL layer. Making patch for features such as CASSANDRA-18584 much complex than it should be.

The goal of this ticket is to simplify the {{Relation}} and {{Restriction}} hierarchies and modify operator  class so that adding new operators requires only changes to the {{Operator}} class and ANTLR file.           "
CASSANDRA-19340,Analytics writer should support writing UDTs,"{{CQLSSTableWriter}} supports writing UDTs. We should support converting from Map-type fields or nested Spark structs to UDTs in the Analytics Bulk Writer, similar to the way Bulk Reader converts UDTs to maps."
CASSANDRA-19337,Fix a few leftovers from previous versions on the java17.adoc page for 5.0+,"- Java17 is experimental for now for both JDK11 and JDK17 builds
 - the text refers to the 4.0 source code when it should be 5.0
 - The JDK 11 build section points to output that says we build with non-default JDK version 11. This is probably from the before JDK8 drop times.  (we probably need to check and update other example screens, too)"
CASSANDRA-19336,Repair causes out of memory,"CASSANDRA-14096 introduced {{repair_session_space}} as a limit for the memory usage for Merkle tree calculations during repairs. This limit is applied to the set of Merkle trees built for a received validation request ({{{}VALIDATION_REQ{}}}), divided by the replication factor so as not to overwhelm the repair coordinator, who will have requested RF sets of Merkle trees. That way the repair coordinator should only use {{repair_session_space}} for the RF Merkle trees.

However, a repair session without {{{}-pr-{}}}/{{{}-partitioner-range{}}} will send RF*RF validation requests, because the repair coordinator node has RF-1 replicas and is also the replica of RF-1 nodes. Since all the requests are sent at the same time, at some point the repair coordinator can have up to RF*{{{}repair_session_space{}}} worth of Merkle trees if none of the validation responses is fully processed before the last response arrives.

Even worse, if the cluster uses virtual nodes, many nodes can be replicas of the repair coordinator, and some nodes can be replicas of multiple token ranges. It would mean that the repair coordinator can send more than RF or RF*RF simultaneous validation requests.

For example, in an 11-node cluster with RF=3 and 256 tokens, we have seen a repair session involving 44 groups of ranges to be repaired. This produces 44*3=132 validation requests contacting all the nodes in the cluster. When the responses for all these requests start to arrive to the coordinator, each containing up to {{repair_session_space}}/3 of Merkle trees, they accumulate quicker than they are consumed, greatly exceeding {{repair_session_space}} and OOMing the node."
CASSANDRA-19334,[Analytics] Upgrade to Cassandra 4.0.12 and remove RowBufferMode and BatchSize options,"In cassandra-all:4.0.12, improvements were made for the CQLSSTableWriter. The sorted writer now can produce size-capped SSTables. It replaces the need for the unsorted sstable writer, which has to buffer and sort data on flushing. The dataset to write in the spark application is already sorted. By avoiding using the unsorted writer, it prevents wasting CPU time on sorting the sorted data. Since the sorted sstable writer does not need to buffer data, its size estimation is more accurate than the unsorted one, meaning the produced sstables files are closer to the expectation.

By removing the unsorted sstable writer, it no longer requires the RowBufferMode option.
By supporting size-capping in sorted writer, it no longer requires the BatchSize option."
CASSANDRA-19333,Fix decode in VectorCodec,"We made a server-side fix in CASSANDRA-19167 for VectorCodec.
This needs to be checked also in the driver [here |https://github.com/apache/cassandra-java-driver/blob/8d5849cb38995b312f29314d18256c0c3e94cf07/core/src/main/java/com/datastax/oss/driver/internal/core/type/codec/VectorCodec.java#L130-L139]"
CASSANDRA-19331,[Analytics] Improve logging for bulk writes and on task failures,"Builds on top of the resiliency changes to add additional logging around token-range metadata fetched from C* during bulk-writes, and also specifically for case when the spark task fails when the cluster is resized.  "
CASSANDRA-19328,Incorrect return value of getMaxTasksQueued in ThreadPoolExecutorPlus,"While working on CASSANDRA-19289, I got wrong results there. I was attempting to add core_pool_size, max_pool_size and max_tasks_queued to the output of system_views.thread_pools but initially I got this:

 
{noformat}
cqlsh> select name, core_pool_size, max_pool_size, max_tasks_queued,pending_tasks from system_views.thread_pools;
name                            | core_pool_size | max_pool_size | max_tasks_queued | pending_tasks
--------------------------------+----------------+---------------+------------------+---------------
           CacheCleanupExecutor |              1 |             1 |                0 |             0
             CompactionExecutor |              2 |             2 |                0 |             0
                    GossipStage |              1 |             1 |                0 |             0
                HintsDispatcher |              2 |             2 |                0 |             0
            MemtableFlushWriter |              2 |             2 |                0 |             0
              MemtablePostFlush |              1 |             1 |                0 |             0
          MemtableReclaimMemory |              1 |             1 |                0 |             0
 Native-Transport-Auth-Requests |              0 |             4 |       2147483647 |             0
      Native-Transport-Requests |              0 |           128 |       2147483647 |             0
   PerDiskMemtableFlushWriter_0 |              2 |             2 |                0 |             0
                      ReadStage |              0 |            32 |       2147483647 |             0
                        Sampler |              1 |             1 |                0 |             0
         SecondaryIndexExecutor |              2 |             2 |                0 |             0
       SecondaryIndexManagement |              1 |             1 |                0 |             0
      StatusPropagationExecutor |              1 |             1 |                0 |             0
             ValidationExecutor |              2 |             2 |                0 |             0
              ViewBuildExecutor |              1 |             1 |                0 |             0
 {noformat}
This is wrong on max_tasks_queued column. That ""0"" there is misleading. That number is fetched from here (1) but getQueue().size() is 0 when that queue does not have any tasks queued. It should be ""getQueue().remainingCapacity()"" which reflects the number of tasks that queue can hold until tasks will be rejected. That is what ""max_tasks_queued"" means in my books.

getQueue().size() is used in ThreadPoolExecutorBase here
{noformat}
    @Override
    public int getPendingTaskCount()
    {
        return getQueue().size();
    }
{noformat}
We are using getQueue().size() for two independent things.

When I change it to ""remainingCapacity()"" it will report it like this:
{noformat}
cqlsh> select name, core_pool_size, max_pool_size, max_tasks_queued,pending_tasks from system_views.thread_pools;

 name                           | core_pool_size | max_pool_size | max_tasks_queued | pending_tasks
--------------------------------+----------------+---------------+------------------+---------------
           CacheCleanupExecutor |              1 |             1 |       2147483647 |             0
             CompactionExecutor |              2 |             2 |       2147483647 |             0
                    GossipStage |              1 |             1 |       2147483647 |             0
                HintsDispatcher |              2 |             2 |       2147483647 |             0
            MemtableFlushWriter |              2 |             2 |       2147483647 |             0
              MemtablePostFlush |              1 |             1 |       2147483647 |             0
          MemtableReclaimMemory |              1 |             1 |       2147483647 |             0
 Native-Transport-Auth-Requests |              0 |             4 |       2147483647 |             0
      Native-Transport-Requests |              0 |           128 |       2147483647 |             0
   PerDiskMemtableFlushWriter_0 |              2 |             2 |       2147483647 |             0
                      ReadStage |              0 |            32 |       2147483647 |             0
                        Sampler |              1 |             1 |             1000 |             0
         SecondaryIndexExecutor |              2 |             2 |       2147483647 |             0
       SecondaryIndexManagement |              1 |             1 |       2147483647 |             0
      StatusPropagationExecutor |              1 |             1 |       2147483647 |             0
             ValidationExecutor |              2 |             2 |       2147483647 |             0
              ViewBuildExecutor |              1 |             1 |       2147483647 |             0
{noformat}
So what happens in practice is that if there are some pending tasks, lets say, 100 of them, then max_tasks_queued will be Integer.MAX minus 100.

cc [~benedict]

(1) [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/concurrent/ThreadPoolExecutorPlus.java#L123]"
CASSANDRA-19327,Test Failure: org.apache.cassandra.index.sai.cql.RandomIntersectionTest.randomIntersectionTest[Small partition restricted high high]-system_keyspace_directory_jdk17,"Seen here: 
[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2629/workflows/75f57272-f299-40e1-8e4f-fdb75bca2f7c/jobs/53821/tests]

The tests were run with _memtable_allocation_type: heap_buffers_ (By default we run them now with offheap_objects, to be changed in CASSANDRA-19326)
{code:java}
junit.framework.AssertionFailedError: Got less rows than expected. Expected 16 but got 0 at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1880) at org.apache.cassandra.index.sai.cql.RandomIntersectionTest.lambda$runRestrictedQueries$3(RandomIntersectionTest.java:118) at org.apache.cassandra.cql3.CQLTester.beforeAndAfterFlush(CQLTester.java:2269) at org.apache.cassandra.index.sai.cql.RandomIntersectionTest.runRestrictedQueries(RandomIntersectionTest.java:104) at org.apache.cassandra.index.sai.cql.RandomIntersectionTest.randomIntersectionTest(RandomIntersectionTest.java:95) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36){code}
CC [~maedhroz] "
CASSANDRA-19326,Switch memtable_allocation_type from offheap_objects to heap_buffers in test/conf/cassandra.yaml,"By default we use heap_buffers in cassandra.yaml, but the unit tests were switched for testing to offheap_objects. This needs to be reverted. We should be testing with heap_buffers on all branches, we do it only in cassandra-3.0 at the moment. "
CASSANDRA-19325,[Analytics] Fix range split and use open-closed range notation consistently,"The ranges used in the analytics library do not have the consistent notation with Cassandra. The analytics library, as in the cassandra ecosystem, should use the open-closed range notation consistently, to avoid potential bugs in implementation.

Besides that, during write process, the split sub-ranges are unordered. It does not seem to affect correctness, but can be confusing."
CASSANDRA-19321,Accord: Command to Exclude Replicas from Durability Status Coordination,"So that other replicas may continue to cleanup their state, we must have an operator command for marking replicas as stale so that the remaining replicas do not wait for them to coordinate their durability status."
CASSANDRA-19310,Accord: More efficient CommandsForKey with transitive dependency elision,"We currently depend on state GC for dependency pruning, but we can prune dependencies directly."
CASSANDRA-19305,Accord: Fast single-partition reads,Introduce guaranteed 1RT single-partition reads with no transaction metadata
CASSANDRA-19297,Accord: RejectBefore must be up-to-date on joining nodes before ready to coordinate,"The exclusive sync point used to join the shard will be known by a majority of the existing replicas, but in the event the quorum changes and the new replica has not recorded the exclusive sync point this might in principle lead to failing to reject a TxnId that should be rejected.

Simple fix, but introduce tests to corroborate this issue, and see if can reproduce in burn test."
CASSANDRA-19292,"Update target Cassandra versions for integration tests, support new 4.0.x and 4.1.x","Currently, apache/cassandra-java-driver runs against 4.0.0 but not newer 4.0.x or 4.1.x releases: https://github.com/apache/cassandra-java-driver/blob/4.x/core/src/main/java/com/datastax/oss/driver/api/core/Version.java#L54C1-L55C1

4.1 introduces changes to config as well, so there are failures to start CCM clusters if we do a naive version bump, like: ""org.apache.cassandra.exceptions.ConfigurationException: Config contains both old and new keys for the same configuration parameters, migrate old -> new: [enable_user_defined_functions -> user_defined_functions_enabled]""

I have a patch ready for this, working on preparing it."
CASSANDRA-19291,Fix NEWS.txt Compact Storage section,"In CASSANDRA-16733 we added a note that Compact Storage will no longer be supported in 5.0. The idea was that drop_compact_storage would be pulled out of the experimental version. 
This did not happen, and compact storage is still around. 
I think this will not be handled at least until 6.0 (major breaking changes) and it is good to be corrected. More and more people are upgrading to 4.0+ and they are confused. "
CASSANDRA-19289,"Expose core_pool_size, max_pool_size and max_tasks_queued in nodetool tpstats / system_views.thread_pools","It would be nice to expose this information so it is visible from cql(sh) / nodetool tpstats. I briefly checked the complexity of this and it should be quite easy, just add few columns in both and put the sizes to metrics so we can read that.

Without this, I think the only way to know the sizes is to read it from JMX. I just prefer some more handy and faster way to check the sizes so I can adjust them accordingly when necessary."
CASSANDRA-19288,Accord: Asynchronous reads may be unsafe,"In principle we should invalidate asynchronous reads before they complete if the data they read may be invalid, but this anyway causes faults when we permit them to occur in accord-core. We can and perhaps should simply ensure the reads are issued against an sstable/memtable snapshot taken by the command store, as this is lower cost and more robust. Otherwise we should investigate what issue asynchronous reads cause."
CASSANDRA-19285,Flaky Host replacement tests and shrink tests (Instance class loader is already closed),"During Circle CI runs there are some flaky integration tests, some noticed are
 * HostReplacementMultiDCTest
 * HostReplacementMultiDCFailureTest
 * HostReplacementFailureTest
 * LeavingSingleFailureTest

Some of the error message I see in these tests are e.g.

{code:java}
java.lang.RuntimeException: java.lang.IllegalStateException: Can't load org.apache.cassandra.utils.concurrent.Ref$OnLeak. Instance class loader is already closed.
{code}


On repeated run, these tests pass."
CASSANDRA-19284,Harry overrides model,Harry model to allow providing specific values for the test. 
CASSANDRA-19283,Update rpm and debian shell includes,"While working on CASSANDRA-19001, it was identified that there are differences between bin/cassandra.in.sh and redhat/cassandra.in.sh, and it seems the debian diff on 5.0 was updated once in 2020 since it was created in 2019.

CC [~brandon.williams]"
CASSANDRA-19279,Test Failure: org.apache.cassandra.simulator.test.HarrySimulatorTest,"Failing 421 times out of 468 runs as visible in this repeated run:
[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2619/workflows/5a5c0104-16a8-493b-99a2-092e2aba2799/jobs/53164/tests]
{code:java}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:547)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098)
	at org.apache.cassandra.simulator.ClusterSimulation.close(ClusterSimulation.java:854)
	at org.apache.cassandra.simulator.test.HarrySimulatorTest.simulate(HarrySimulatorTest.java:543)
	at org.apache.cassandra.simulator.test.HarrySimulatorTest.harryTest(HarrySimulatorTest.java:228)
	at org.apache.cassandra.simulator.test.HarrySimulatorTest.test(HarrySimulatorTest.java:204)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:254)
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:539)
	Suppressed: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
		at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
		at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:246)
		at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$47(Instance.java:975)
		at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
		at org.apache.cassandra.simulator.systems.SimulatedExecution$NoIntercept$1Run.run(SimulatedExecution.java:82)
		at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:318)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:829)
	Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
		at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
		at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$46(Instance.java:969)
		at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$async$10(IsolatedExecutor.java:156)
		at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
		at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
		at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:246)
		at org.apache.cassandra.hints.HintsService.shutdownBlocking(HintsService.java:282)
		at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$17(Instance.java:898)
		at org.apache.cassandra.distributed.impl.Instance.lambda$parallelRun$52(Instance.java:1193)
	Caused by: java.lang.OutOfMemoryError: Direct buffer memory
		at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
		at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
		at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)
		at org.apache.cassandra.hints.HintsBuffer.create(HintsBuffer.java:77)
		at org.apache.cassandra.hints.HintsBufferPool.createBuffer(HintsBufferPool.java:150)
		at org.apache.cassandra.hints.HintsBufferPool.initializeCurrentBuffer(HintsBufferPool.java:121)
		at org.apache.cassandra.hints.HintsBufferPool.currentBuffer(HintsBufferPool.java:113)
		at org.apache.cassandra.hints.HintsWriteExecutor$FlushBufferPoolTask.run(HintsWriteExecutor.java:168)
		at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
		at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
		at org.apache.cassandra.simulator.systems.SimulatedExecution$NoIntercept$1Run.run(SimulatedExecution.java:82)
		at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:585)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:246)
	at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$47(Instance.java:975)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.SimulatedExecution$NoIntercept$1Run.run(SimulatedExecution.java:82)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:318)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
	at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$46(Instance.java:969)
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$async$10(IsolatedExecutor.java:156)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:246)
	at org.apache.cassandra.hints.HintsService.shutdownBlocking(HintsService.java:282)
	at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$17(Instance.java:898)
	at org.apache.cassandra.distributed.impl.Instance.lambda$parallelRun$52(Instance.java:1193)
Caused by: java.lang.OutOfMemoryError: Direct buffer memory
	at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
	at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
	at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)
	at org.apache.cassandra.hints.HintsBuffer.create(HintsBuffer.java:77)
	at org.apache.cassandra.hints.HintsBufferPool.createBuffer(HintsBufferPool.java:150)
	at org.apache.cassandra.hints.HintsBufferPool.initializeCurrentBuffer(HintsBufferPool.java:121)
	at org.apache.cassandra.hints.HintsBufferPool.currentBuffer(HintsBufferPool.java:113)
	at org.apache.cassandra.hints.HintsWriteExecutor$FlushBufferPoolTask.run(HintsWriteExecutor.java:168)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.SimulatedExecution$NoIntercept$1Run.run(SimulatedExecution.java:82)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:585)
test-_jdk11
{code}
 
{code:java}
junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}"
CASSANDRA-19278,AND queries involving both static and regular columns can hide matches from Memtable-attached indexes,"As part of our ongoing work on CASSANDRA-18275, Harry started surfacing errors shortly after we added static columns to the schema of {{SingleNodeSAITest}}. After a bit of debugging, I was able to come up with the following concrete repro:

{noformat}
import org.junit.Test;

import org.apache.cassandra.cql3.UntypedResultSet;
import org.apache.cassandra.index.sai.utils.SAIRandomizedTester;

public class InMemoryIntersectionsTest extends SAIRandomizedTester
{
    @Test
    public void staticAndRegularIntersection()
    {
        createTable(""CREATE TABLE %s (pk int, ck int, v1 int, s1 int static, PRIMARY KEY(pk, ck))"");
        createIndex(""CREATE INDEX ON %s(v1) USING 'sai'"");
        createIndex(""CREATE INDEX ON %s(s1) USING 'sai'"");

        execute(""INSERT INTO %s (pk, ck, v1) VALUES (?, ?, ?)"", 0, 1, 0);
        execute(""INSERT INTO %s (pk, ck, v1) VALUES (?, ?, ?)"", 0, 2, 1);
        execute(""INSERT INTO %s (pk, ck, v1) VALUES (?, ?, ?)"", 0, 3, 2);
        execute(""INSERT INTO %s (pk, ck, v1) VALUES (?, ?, ?)"", 0, 4, 3);
        execute(""INSERT INTO %s (pk, ck, v1) VALUES (?, ?, ?)"", 0, 5, 4);
        execute(""INSERT INTO %s (pk, ck, v1) VALUES (?, ?, ?)"", 0, 6, 5);

        execute(""INSERT INTO %s (pk, s1) VALUES (?, ?)"", 0, 100);

	// Flushing here passes test

        UntypedResultSet result1 = execute(""SELECT * FROM %s WHERE pk = ? AND v1 > ?"", 0, 2);
        assertRowCount(result1, 3);
        UntypedResultSet result2 = execute(""SELECT * FROM %s WHERE pk = ? AND v1 > ? AND s1 = ?"", 0, 2, 100);        
        assertRowCount(result2, 3); // Only returns one result!
    }
}
{noformat}

Flushing memtables immediately before the queries passes the test, but it seems we have an issue with how the {{PrimaryKey}} iterators produced by static and regular column indexes are intersected when the iterators come from Memtable-attached indexes. Once the root cause of this is determined, it might make some sense to enhance {{RandomIntersectionTest}} to cover this interaction as well, just in case it turns up further problems."
CASSANDRA-19275,Flaky Host replacement tests and shrink tests,"During Circle CI runs there are some flaky integration tests, some noticed are
 * HostReplacementMultiDCTest
 * HostReplacementMultiDCFailureTest
 * HostReplacementFailureTest
 * LeavingSingleFailureTest

Some of the error message I see in these tests are e.g.

`java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Failed to bind port 42611 on 127.0.0.2.`

On repeated run, these tests pass."
CASSANDRA-19274,Fix version in run-external.sh,"_run-external.sh_ has _0.0.1_ version for the {_}harry-integration-external-0.0.1-SNAPSHOT.jar{_}; however, pom.xml has 0.0.2. So in this ticket, fix the version in _run-external.sh_"
CASSANDRA-19273,Allow setting TTL for snapshots created by Analytics bulk reader,"Analytics user can add an existing snapshot's name or create a new snapshot through reader options, from which data is bulk read from. Incase of creating new snapshot, we want to allow users to set TTL option and have a default value for the TTL. This is to make sure, in case of job failures, the snapshots are cleared, to release space. "
CASSANDRA-19272,[Analytics] Add integration tests using in-jvm-dtest to cover blocklisted instances,"Scope:
- Add default mechanism to load blocklist from configuration
- Relevant in-jvm-dtest based integration tests to validate the CL validations for different CL combinations"
CASSANDRA-19271,Improve setup and initialisation of LocalLog/LogSpec,
CASSANDRA-19269,NPE in MetaStrategy.java when collecting metrics,"Now that the metrics exporter has been removed in CASSANDRA-18743, I thought I'd take a stab at setting up a metrics collector through DropWizard.  I have a POC of an agent that exposes a Prometheus endpoint using the Prometheus simpleclient here: https://github.com/rustyrazorblade/cassandra-prometheus-exporter

There's build instructions in the README - it's just a simple agent on a background thread.

Unfortunately when trying to view the metrics, Cassandra throws an NPE exception, stack trace is here:

{code:java}
WARN  [qtp104447770-28] 2024-01-12 14:26:50,102 ServletHandler.java:522 - /metrics
java.lang.NullPointerException: null
	at org.apache.cassandra.locator.MetaStrategy.getReplicationFactor(MetaStrategy.java:64)
	at org.apache.cassandra.metrics.KeyspaceMetrics.lambda$new$12(KeyspaceMetrics.java:224)
	at org.apache.cassandra.metrics.KeyspaceMetrics$1.getValue(KeyspaceMetrics.java:344)
	at org.apache.cassandra.metrics.KeyspaceMetrics$1.getValue(KeyspaceMetrics.java:338)
	at org.apache.cassandra.metrics.StorageMetrics.lambda$static$0(StorageMetrics.java:40)
	at org.apache.cassandra.metrics.StorageMetrics.lambda$createSummingGauge$2(StorageMetrics.java:55)
	at java.base/java.util.stream.ReferencePipeline$5$1.accept(ReferencePipeline.java:229)
	at com.google.common.collect.CollectSpliterators$1.lambda$forEachRemaining$1(CollectSpliterators.java:128)
	at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)
	at com.google.common.collect.CollectSpliterators$1.forEachRemaining(CollectSpliterators.java:128)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)
	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.LongPipeline.reduce(LongPipeline.java:474)
	at java.base/java.util.stream.LongPipeline.sum(LongPipeline.java:432)
	at org.apache.cassandra.metrics.StorageMetrics.lambda$createSummingGauge$3(StorageMetrics.java:56)
	at io.prometheus.client.dropwizard.DropwizardExports.fromGauge(DropwizardExports.java:47)
	at io.prometheus.client.dropwizard.DropwizardExports.collect(DropwizardExports.java:133)
	at io.prometheus.client.Collector.collect(Collector.java:45)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:204)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.<init>(CollectorRegistry.java:162)
	at io.prometheus.client.CollectorRegistry.filteredMetricFamilySamples(CollectorRegistry.java:140)
	at io.prometheus.client.exporter.MetricsServlet.doGet(MetricsServlet.java:43)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:648)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:455)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1072)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:382)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1006)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:365)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:485)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:926)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:988)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:627)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:51)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}

Related [discussion thread|https://lists.apache.org/thread/lppjlxrd91qn0r0dkz80r5y843jwl1qo]."
CASSANDRA-19266,Make concurrent_index_builders configurable at runtime,"CASSANDRA-17781 introduced the YAML option \{{concurrent_index_builders}} to allow us to configure index building independently of compaction. We need to be able to adjust this after index building starts without a restart. (In some cases, we might do it in conjunction w/ \{{nodetool stop}}, but it depends on the level of ongoing disruption.)"
CASSANDRA-19265,Mutation.PartitionUpdateCollector#add asserts Partitioner pointers are equal but this may fail during CommitLogReplay for LocalPartitioner,"This was found on the cep-15-accord branch.  We have a system table all_commands_for_key and it seems that the LocalPartitioner pointer gets changed, causing CommitLogReplyer to fail

{code}
java.lang.AssertionError: Update to key DecoratedKey(11:01c93b4893674349ae:2b74bbca-eae8-3217-a675-115cf5642ef3\:3030312e302e393134, 00040000000b00000901c93b4893674349ae000021000000102b74bbcaeae83217a675115cf5642ef3000000093030312e302e39313400) with partitioner org.apache.cassandra.dht.LocalPartitioner@1fc4de9e (class org.apache.cassandra.dht.LocalPartitioner) had an update ([system_accord.all_commands_for_key]...) with a different partitioner! org.apache.cassandra.dht.LocalPartitioner@c7d8be7 (class org.apache.cassandra.dht.LocalPartitioner)
	at org.apache.cassandra.utils.Throwables.unchecked(Throwables.java:308)
	at org.apache.cassandra.utils.Throwables.cleaned(Throwables.java:327)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:544)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.handleMutation(CommitLogReplayer.java:521)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readMutation(CommitLogReader.java:478)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readSection(CommitLogReader.java:397)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:244)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:147)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommitLogReplayer.java:195)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:225)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:206)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:332)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:726)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:876)
{code}"
CASSANDRA-19263,Fix tcm startup ExceptionInInitializerError when upgrade from 5.0-beta1 to trunk,"5.0-beta1 had set the storage_compatibility_mode: CASSANDRA_4 explicitly,

However, the trunk has set storage_compatibility_mode: NONE recently.

I just upgrade a test cluster from 5.0-beta1 to trunk only with source codes(not with conf)

The trunk source code is incompatible with storage_compatibility_mode: CASSANDRA_4. we will get the following exceptions at the start up:
{code:java}
INFO  [main] 2024-01-03 17:40:54,341 Startup.java:98 - Initializing as first CMS node in a new cluster
Exception (java.lang.NoClassDefFoundError) encountered during startup: Could not initialize class org.apache.cassandra.net.MessagingService
java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.net.MessagingService
 at org.apache.cassandra.tcm.Commit$Handler.<init>(Commit.java:304)
 at org.apache.cassandra.tcm.ClusterMetadataService.<init>(ClusterMetadataService.java:181)
 at org.apache.cassandra.tcm.Startup.initializeAsNonCmsNode(Startup.java:146)
 at org.apache.cassandra.tcm.Startup.initialize(Startup.java:99)
 at org.apache.cassandra.tcm.Startup.initialize(Startup.java:86)
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:268)
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:726)
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:876)
ERROR [main] 2024-01-03 17:40:54,378 CassandraDaemon.java:898 - Exception encountered during startup
java.lang.NoClassDefFoundError: Could not initialize class org.apache.cassandra.net.MessagingService
 at org.apache.cassandra.tcm.Commit$Handler.<init>(Commit.java:304)
 at org.apache.cassandra.tcm.ClusterMetadataService.<init>(ClusterMetadataService.java:181)
 at org.apache.cassandra.tcm.Startup.initializeAsNonCmsNode(Startup.java:146)
 at org.apache.cassandra.tcm.Startup.initialize(Startup.java:99)
 at org.apache.cassandra.tcm.Startup.initialize(Startup.java:86)
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:268)
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:726)
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:876) {code}
 

After debugging, I found it was caused by the {*}_ExceptionInInitializerError_{*}, in the {_}*LINE-WITH-ISSUE*{_}(logger here is null). A circular dependency is here：

_*(init)MessagingService -> Version(init its static block) -> logger.warn(requires the init of MessagingService)*_

 
{code:java}
public class MessagingService
{
 
    private static final Logger logger = LoggerFactory.getLogger(MessagingService.class);
    public enum Version
    {
        static
        {
            if (DatabaseDescriptor.getStorageCompatibilityMode().isBefore(5))
             {
                 // LINE-WITH-ISSUE
                 logger.warn(""Starting in storage compatibility mode "" + DatabaseDescriptor.getStorageCompatibilityMode());
                 CURRENT = VERSION_40;
             }
             else
             {
                 CURRENT = VERSION_51;
             }
        }
    }
    public static final int current_version = Version.CURRENT.value;
} {code}
 "
CASSANDRA-19262,Handle MIN_TOKEN placement correctly,"During concurrent range movements, it is possible to end up in a situation when from the perspective of the node that owns the {{MIN}} token, there are two independent ranges {{(MIN, A]}} and {{{}(Z, MIN]{}}}. And from the perspective of the joining node, it does not know that these are two _separate_ ranges, and thinks they are just one wraparound range, so it tries to just add some placements there."
CASSANDRA-19260,org.apache.cassandra.tcm.ClusterMetadataService#commit does not catch up when rejected,"This was found in the cep-15-accord branch (CASSANDRA-18804).  The test that found this was a simple benchmark test.

1) deploy a 6 node cluster
2) create a table
3) in parallel launch many accord transactions

When accord gets a transaction it needs to make sure the table is “managed” by accord which uses TCM for this bookkeeping, this is just a List<TableId> in ClusterMetadata.  We found that we detect that the table isn’t managed so we try to add it, we get a reject and the TCM epoch has not moved forward!

Debugging this it looks like org.apache.cassandra.tcm.RemoteProcessor#commit is the root cause as it only seems to try to catch up if there is a messaging error and not a TCM rejection!  Given that the caller to TCM is not able to find the epoch to “wait” on I feel that this is a TCM issue as TCM normally tries to make sure success/rejects are blocking, but in this one case it appears not to be so"
CASSANDRA-19258,Release Cassandra 4.1.4,"I wanted to raise a question but as this is not possible I raised a task.

Can we have the release of Cassandra 4.1.4 be done? I see there is no open issue targeted for this release and I think it can be done. 

We need this release done in order to migrate to version 4.1.4 and have the fix for https://issues.apache.org/jira/browse/CASSANDRA-18878

We cannot move the version 4 of Cassandra as is still a Beta and is not allowed by our management to use Beta Versions while 4.1.3 still have this Vulnerability Issue inside it. So we are stuck => either we have a new Cassandra 4.1.4 released or you Release Cassandra 5.0 but make it official. 

Thanks, 

Nicolae"
CASSANDRA-19257,[Analytics] Fix bulk writer consistency level validations for blocked instances,"The analytics library bulk writer spark job performs consistency level validations around write operations.

Currently, these validations incorrectly double count the number of blocked instances for these validations resulting in valid scenarios failing CL checks."
CASSANDRA-19255,StorageService.getRangeToEndpointMap() MBean operation is running into NPE for LocalStrategy keysapces,"When the StorageService's MBean operation getRangeToEndpointMap is called for LocalStrategy keyspaces, then it is running into NPE. It is working in earlier major version, but failing in trunk. It can be reproduced in local using JConsole or using a tool like `jmxterm` (unfortunately these tools are not giving full stacktrace). Observed the same behavior with getRangeToEndpointWithPortMap operation too."
CASSANDRA-19253,(Accord) NPE while trying to serialize FoundKnownMap as value is null half the time but unexpected while serializing,"{code}
java.lang.NullPointerException
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serializedSize(CommandSerializers.java:277)
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serializedSize(CommandSerializers.java:253)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serializedSize(CheckStatusSerializers.java:75)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serializedSize(CheckStatusSerializers.java:54)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$2.serializedSize(CheckStatusSerializers.java:115)
{code}

{code}
java.lang.NullPointerException
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serialize(CommandSerializers.java:257)
	org.apache.cassandra.service.accord.serializers.CommandSerializers$3.serialize(CommandSerializers.java:253)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serialize(CheckStatusSerializers.java:58)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$1.serialize(CheckStatusSerializers.java:54)
	org.apache.cassandra.service.accord.serializers.CheckStatusSerializers$2.serialize(CheckStatusSerializers.java:91)
{code}
"
CASSANDRA-19251,[Analytics] Speed up integration tests,"We can speed up the integration tests by reusing the in-jvm dtest cluster in a test class when possible. The proposal has the following pros and cons.

Pros: 
# Ability to iterate faster by running tests faster
# Reduce the resources needed to run tests
# Reduce flakiness due to resource allocation issues
# Remove necessity to spin up a new process for bulk writes
# Adds the ability to use parameterized tests (for example different CLs for the same test)

Cons:
# Potentially having to add new test classes
# No longer be able to use the {{CassandraIntegrationTest}} annotation
# Can only test a single Cassandra version at a time

In terms of resources, the proposed approach brings huge improvements by reducing the current runtime of about 1 hour down to 20 minutes. Additionally, we reduce the resource requirements for the integration tests, and add the ability to iterate faster. Reduces flakiness which helps surface real issues in the tests."
CASSANDRA-19250,Test failure: org.apache.cassandra.fuzz.ring.ClusterMetadataUpgradeHarryTest#simpleUpgradeTest,"This fails consistently in trunk. It seems to be introduced since Harry is in-tree.
{code:java}
java.lang.NullPointerException: Cannot invoke ""java.util.List.stream()"" because the return value of ""java.util.Map.get(Object)"" is null
	at org.apache.cassandra.distributed.shared.Versions.getLatest(Versions.java:127)
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.upgradesTo(UpgradeTestBase.java:219)
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.upgradesToCurrentFrom(UpgradeTestBase.java:204)
	at org.apache.cassandra.fuzz.ring.ClusterMetadataUpgradeHarryTest.simpleUpgradeTest(ClusterMetadataUpgradeHarryTest.java:77)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) {code}"
CASSANDRA-19249,Fix some errors in Vector Search docs,"Fix vector type description: Vector types in 5.0  support all data types. But vector indexing only support floats.  Subtle diff.  Astra only supports float. 

Add some additional embeddings info."
CASSANDRA-19247,Minor corrections to TCM Implementation documentation,"In the ""[Bootstrap: InProgressSequence, PrepareJoin, BootstrapAndJoin|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tcm/TCM_implementation.md#bootstrap-inprogresssequence-preparejoin-bootstrapandjoin]” section of TCM Implementation doc, InProgressSequence transformations are mentioned as ""{_}InProgressSequence`, holding the three transformations (`PrepareJoin`, `MinJoin` `FinishJoin`){_}”. As per [PrepareJoin|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tcm/transformations/PrepareJoin.java#L154], these are {*}StartJoin{*}, Mi{*}d{*}Join & FinishJoin. Doc needs to be updated.
 "
CASSANDRA-19246,BLOG - Contributor and townhall replay posts,Adding the Town Hall and Contributor November meetup replays.
CASSANDRA-19245,Upgrade Python driver to 3.29.0,"The version of the Python driver that is used by cqlsh (3.28.0) doesn't correctly support vectors of variable-width types. Wrong deserialization can either fail with an exception or silently provide wrong results. For example:
{code:java}
cqlsh:k> CREATE TABLE t (k int PRIMARY KEY, v vector<varint, 2>);
cqlsh:k> INSERT INTO t(k, v) VALUES (1, [23452352356235654634567437463767365783768, 3]);
cqlsh:k> SELECT * FROM t;
 k | v
---+--------------------------
 1 | [289729430, -1001073214]}}
{code}
The most recent driver at the moment (3.29.0) still doesn't support this kind of vector, but it always fails with a proper message instead of dangerously providing wrong results, thanks to [PYTHON-1371|https://datastax-oss.atlassian.net/browse/PYTHON-1371]."
CASSANDRA-19242,Update cqlsh version in 5.x,"The cqlsh in version 5.0 is 6.2.0, so the version should be incremented on trunk (5.1) to 6.3.0."
CASSANDRA-19225,Async Query Cancellation Not Propagated To RequestThrottler in Java Driver,"Executing a CQL statement asynchronously and then subsequently cancelling it can result in the RequestThrottler becoming ""full"", as its never informed of the cancellation. 
CqlRequestHandler does have a callback that handles cancellation, however it doesn't inform the throttler. This is particularly bad when using a throttler such as ConcurrencyLimitingRequestThrottler which tracks the number of in-flight requests as well as queues requests beyond a certain threshold, neither of which gets properly updated when a request is cancelled.

 

We experienced this issue with java-driver-core 4.17.x, but looking at the code it appears to impact all versions of the Java driver.
 
To reproduce, one can simply do the following:
{code:java}
AsyncCqlSession session;
Statement statement;
CompletableFuture<AsyncResultSet> future = session.executeAsync(statement).toCompletableFuture();
future.cancel(true);
{code}
 
Doing this repeatedly with a throttler such as ConcurrencyLimitingRequestThrottler you can observe that the throttler is never informed of the cancellation, and eventually it will become saturated with cancelled requests and reject the request. The issue likely applies to any of the methods in AsyncCqlSession, as they all use CqlRequestHandler under the hood.
 
We fixed the issue in our application by installing a custom CqlRequestHandler that calls signalTimeout on the throttler, and verified that it resolves the problem; adding that logic to the existing callback seems like the most straight forward solution."
CASSANDRA-19223,[Analytics] Column type mapping error for timestamp type during bulk writes,"When doing bulk reads with the analytics library, a user can specify the last modified column as an option. Bulk reader will add a column with the last modified column to the data frame. If a user wants to use the bulk-read data frame to persist data, and using the {{WriterOptions.TIMESTAMP}} feature from the last modified column from the bulk-read data frame, the bulk write will fail with a data type mapping error.

{code:java}
Caused by: java.lang.RuntimeException: Unsupported conversion for LONG from java.sql.Timestamp
	at org.apache.cassandra.spark.bulkwriter.SqlToCqlTypeConverter$LongConverter.convertInternal(SqlToCqlTypeConverter.java:245)
	at org.apache.cassandra.spark.bulkwriter.SqlToCqlTypeConverter$LongConverter.convertInternal(SqlToCqlTypeConverter.java:231)
	at org.apache.cassandra.spark.bulkwriter.SqlToCqlTypeConverter$Converter.convert(SqlToCqlTypeConverter.java:203)
	at org.apache.cassandra.spark.bulkwriter.SqlToCqlTypeConverter$NullableConverter.convert(SqlToCqlTypeConverter.java:212)
	at org.apache.cassandra.spark.bulkwriter.TableSchema.normalize(TableSchema.java:91)
	at org.apache.spark.api.java.JavaPairRDD$.$anonfun$toScalaFunction$1(JavaPairRDD.scala:1070)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
{code}
"
CASSANDRA-19221,CMS: Nodes can restart with new ipaddress already defined in the cluster,"I am simulating running a cluster in Kubernetes and testing what happens when several pods go down and  ip addresses are swapped between nodes. In 4.0 this is blocked and the node cannot be restarted.



To simulate this I create a 3 node cluster on a local machine using 3 loopback addresses

{code}
127.0.0.1
127.0.0.2
127.0.0.3
{code}

The nodes are created correctly and the first node is assigned as a CMS node as shown:

{code}
bin/nodetool -p 7199 describecms
{code}

Cluster Metadata Service:
{code}
Members: /127.0.0.1:7000
Is Member: true
Service State: LOCAL
{code}

At this point I bring down the nodes 127.0.0.2 and 127.0.0.3 and swap the ip addresses for the rpc_address and listen_address 
 
The nodes come back as normal, but the nodeid has now been swapped against the ip address:

Before:
{code}
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.3  75.2 KiB   16      76.0%             6d194555-f6eb-41d0-c000-000000000003  rack1
UN  127.0.0.2  86.77 KiB  16      59.3%             6d194555-f6eb-41d0-c000-000000000002  rack1
UN  127.0.0.1  80.88 KiB  16      64.7%             6d194555-f6eb-41d0-c000-000000000001  rack1
{code}

After:
{code}
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load        Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.3  149.62 KiB  16      76.0%             6d194555-f6eb-41d0-c000-000000000003  rack1
UN  127.0.0.2  155.48 KiB  16      59.3%             6d194555-f6eb-41d0-c000-000000000002  rack1
UN  127.0.0.1  75.74 KiB   16      64.7%             6d194555-f6eb-41d0-c000-000000000001  rack1
{code}

On previous tests of this I have created a table with a replication factor of 1, inserted some data before the swap.   After the swap the data on nodes 2 and 3 is now missing. 


One theory I have is that I am using different port numbers for the different nodes, and I am only swapping the ip addresses and not the port numbers, so the ip:port still looks unique

i.e. 127.0.0.2:9043 becomes 127.0.0.2:9044
and 127.0.0.3:9044 becomes 127.0.0.3:9043

 "
CASSANDRA-19219,CMS: restarting a CMS node with different ip address,"I am simulating running a cluster in Kubernetes and testing what happens when a pod goes down and is re created with a new ip address, the data is all stored on a detached volume so when the new pod is created all the old data for the node is reattached. In 4.0 this is handled correctly the node will come back up with the same hostid, tokens etc, just a new ip address and the cluster is healthy throughout.

 

To simulate this I create a 3 node cluster on a local machine using 3 loopback addresses

127.0.0.1
127.0.0.2
127.0.0.3

I then run nodetool -p 7199 reconfigurecms datacenter1:3 --sync to create 3 CMS nodes


I then bring down 127.0.0.1 and replace the rpc_address and listen_address with 127.0.0.4 and re start the node. The node then hangs with this as the last error message:

(8821185654333640868,9200867415893016118]=ForRange\{lastModified=Epoch{epoch=12}, endpointsForRange=[Full(/127.0.0.1:7000,(8821185654333640868,9200867415893016118]), Full(/127.0.0.2:7000,(8821185654333640868,9200867415893016118]), Full(/127.0.0.3:7000,(8821185654333640868,9200867415893016118])]},
}}}, lockedRanges=LockedRanges\{lastModified=Epoch{epoch=14}, locked={}}}. This can mean that this node is configured differently from CMS.
java.lang.AssertionError: not aware of any cluster members
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalReplicas(NetworkTopologyStrategy.java:233)
        at org.apache.cassandra.locator.CMSPlacementStrategy$DatacenterAware.reconfigure(CMSPlacementStrategy.java:119)
        at org.apache.cassandra.tcm.transformations.cms.PrepareCMSReconfiguration$Complex.execute(PrepareCMSReconfiguration.java:164)
        at org.apache.cassandra.tcm.log.LocalLog.processPendingInternal(LocalLog.java:429)
        at org.apache.cassandra.tcm.log.LocalLog$Async$AsyncRunnable.run(LocalLog.java:682)
        at org.apache.cassandra.concurrent.InfiniteLoopExecutor.loop(InfiniteLoopExecutor.java:121)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
WARN  [GlobalLogFollower] 2023-12-21 11:11:34,408 LocalLog.java:693 - Stopping log processing on the node... All subsequent epochs will be ignored.
org.apache.cassandra.tcm.log.LocalLog$StopProcessingException: java.lang.AssertionError: not aware of any cluster members
        at org.apache.cassandra.tcm.log.LocalLog.processPendingInternal(LocalLog.java:434)
        at org.apache.cassandra.tcm.log.LocalLog$Async$AsyncRunnable.run(LocalLog.java:682)
        at org.apache.cassandra.concurrent.InfiniteLoopExecutor.loop(InfiniteLoopExecutor.java:121)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.AssertionError: not aware of any cluster members
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalReplicas(NetworkTopologyStrategy.java:233)
        at org.apache.cassandra.locator.CMSPlacementStrategy$DatacenterAware.reconfigure(CMSPlacementStrategy.java:119)
        at org.apache.cassandra.tcm.transformations.cms.PrepareCMSReconfiguration$Complex.execute(PrepareCMSReconfiguration.java:164)
        at org.apache.cassandra.tcm.log.LocalLog.processPendingInternal(LocalLog.java:429)
        ... 4 common frames omitted"
CASSANDRA-19217,Test failure: auth_test.TestAuthUnavailable,https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/1233/workflows/bb617340-f1da-4550-9c87-5541469972c4/jobs/62551/tests
CASSANDRA-19216,CMS: Additional nodes are not added to CMS,"When creating a 3 node cluster on a local machine using 3 loopback addresses

127.0.0.1
127.0.0.2
127.0.0.3

The nodes are created correctly and the first node is assigned as a CMS node as shown:

{{bin/nodetool p 7199 status{-}{-}}}
{{{}Datacenter: datacenter1{}}}{{{}======================={}}}{{{}Status=Up/Down{}}}{{{}|/ State=Normal/Leaving/Joining/Moving{}}}{{{}-  Address    Load       Tokens  Owns (effective)  Host ID                               Rack{}}}

{{UN  127.0.0.3  75.55 KiB  16      76.0%             6d194555-f6eb-41d0-c000-000000000003  rack1}}

{{UN  127.0.0.2  67.97 KiB  16      59.3%             6d194555-f6eb-41d0-c000-000000000002  rack1}}

{{UN  127.0.0.1  81 KiB     16      64.7%             6d194555-f6eb-41d0-c000-000000000001  rack1}}

{{bin/nodetool -p 7199 describecms}}
{{Cluster Metadata Service:}}
{{Members: /127.0.0.1:7000}}
{{Is Member: true}}
{{Service State: LOCAL}}
{{Is Migrating: false}}
{{Epoch: 14}}
{{Local Pending Count: 0}}
{{Commits Paused: false}}
{{{}Replication factor: ReplicationParams{class=org.apache.cassandra.locator.MetaStrategy, datacenter1=1{}}}}



{{However after doing a reconfigurecms to create a replication factor of 3, it seems that there is still only one member of cms.}}



{{bin/nodetool -p 7199 reconfigurecms datacenter1:3}}
{{bin/nodetool -p 7199 describecms}}
{{Cluster Metadata Service:}}
{{Members: /127.0.0.1:7000}}
{{Is Member: true}}
{{Service State: LOCAL}}

Is Migrating: false

Epoch: 16

Local Pending Count: 0

Commits Paused: false

Replication factor: ReplicationParams\{class=org.apache.cassandra.locator.MetaStrategy, datacenter1=3}

Is this correct, should all 3 nodes be shown in the Members section ?"
CASSANDRA-19215,"""Query start time"" in native transport request threads should be the task enqueue time","Recently, our Cassandra 4.0.6 cluster experienced an outage due to a surge in expensive traffic from the application side. This surge involved a large volume of costly read queries, which took a considerable amount of time to process on the server side. The client had timeout settings; if a request timed out, it might trigger the sending of new requests. Since the server nodes were overloaded, numerous nodes had hundreds of thousands of tasks queued in the Native-Transport-Request pending queue. I expected that once the application ceased sending requests, the server node would quickly return to normal, as most requests in the queue were over half an hour old and should have timed out rapidly, clearing the queue. However, it actually took an hour to clear the native transport's pending queue, even with native transport disabled. Upon examining the code, I noticed that for read/write requests, the [queryStartNanoTime|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/transport/Dispatcher.java#L78], which determines if a request has timed out, only begins when the task starts processing. This means that no matter how long a request has been pending, it doesn't contribute to the timeout. I believe this is incorrect. The timer should start when the Cassandra server receives the request or when it enqueues the task, not when the request/task begins processing. This way, an overloaded node with many pending tasks can quickly discard timed-out requests and recover from an outage once new requests stop."
CASSANDRA-19212,Better handle Spark job timeouts/process killing in Analytics tests,"The Process#destroyForcibly() call in ResiliencyTestBase#bulkWriteData can complete without the process actually exiting (and is documented to do so). We should wait on the process to exit before attempting to read the exit code, which throws if the process hasn’t yet exited.  Otherwise, we can lose the Spark output when the test determines that the job is taking too long."
CASSANDRA-19211,Paxos repair failed when upgrading from 3.11.16 to 4.1.3,"I am upgrading from 3.11.16 to 4.1.3 and come up with following error messages: 
{code:java}
INFO  [OptionalTasks:1] 2023-12-20 14:44:53,811 PaxosRepair.java:687 - PaxosRepair isn't supported by Full(/192.168.117.175:7000,(-9100045633799031437,-9090844491356996692]) on version 3.11.16
INFO  [OptionalTasks:1] 2023-12-20 14:44:53,811 PaxosCleanupLocalCoordinator.java:179 - Failing paxos cleanup session 00000000-0000-0000-0000-000000000000 for... {code}
The upgrade path is to upgrade one DC to new version while leaving second DC in old version. I have read upgrade doc and seems there is no info on how to proceed with Paxos in cluster mixed mode. "
CASSANDRA-19210,Bring Harry into OSS Cassandra tree,"This patch introduces Harry into tree. 

_No_ modifications were made to Harry code, so technically our tree has already relied on it. The intention to introduce Harry to tree to lower the entry barrier and allow quicker developer turnarounds.

CI results attached; all tests are green. Python tests are skipped because this patch does not make any changes to production code, it's all testonly."
CASSANDRA-19209,Merkle Tree repair errors,"On Cassandra 4.0.7 we are seeing Merkle tree repair errors , we are using cassandra-reaper to do continuous repairs, below is the exception

 


ERROR] [RequestResponseStage-9] 2023-11-05 13:49:58,131 RepairMessage.java:78 - 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 VALIDATION_REQ failed on /<IP>:7000: UNKNOWN
[ERROR] [RequestResponseStage-9] 2023-11-05 13:49:58,132 RepairMessage.java:78 - 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 VALIDATION_REQ failed on /<IP>:7000: UNKNOWN
[ERROR] [RequestResponseStage-9] 2023-11-05 13:49:58,132 RepairMessage.java:78 - 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 VALIDATION_REQ failed on /<IP>:7000: UNKNOWN
[ERROR] [RequestResponseStage-9] 2023-11-05 13:49:58,132 RepairMessage.java:78 - 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 VALIDATION_REQ failed on /<IP>:7000: UNKNOWN
[ERROR] [RequestResponseStage-9] 2023-11-05 13:49:58,132 RepairMessage.java:78 - 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 VALIDATION_REQ failed on /<IP>:7000: UNKNOWN
[WARN] [RepairJobTask:3] 2023-11-05 13:49:58,132 RepairJob.java:177 - repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 replication.payloads_by_bucket sync failed
[WARN] [RepairJobTask:8] 2023-11-05 13:49:58,132 RepairJob.java:177 - repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 replication.segments sync failed
[WARN] [RepairJobTask:7] 2023-11-05 13:49:58,133 RepairJob.java:177 - repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 replication.replication_queue sync failed
[WARN] [RepairJobTask:11] 2023-11-05 13:49:58,133 RepairJob.java:177 - repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 replication.payload_register sync failed
[ERROR] [Repair#240941:1] 2023-11-05 13:49:58,134 RepairSession.java:321 - repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 Session completed with the following error
org.apache.cassandra.exceptions.RepairException: [repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/replication_queue, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276|#7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/replication_queue, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276], (7303356382144251816,7303733610091843383], (8313290647942866939,8331919689226408479]]] Got VALIDATION_REQ failure from /<IP>:7000: UNKNOWN
    at org.apache.cassandra.repair.messages.RepairMessage$1.onFailure(RepairMessage.java:81)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53)
    at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:45)
    at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:432)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:522)
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165)
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137)
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:826)
[WARN] [RepairJobTask:3] 2023-11-05 13:49:58,134 RepairJob.java:177 - repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 replication.replication_queue_ptr sync failed
[ERROR] [Repair#240941:1] 2023-11-05 13:49:58,134 RepairRunnable.java:178 - Repair 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 failed:
java.lang.RuntimeException: Repair session 7fc76c30-7c14-11ee-a6b1-09b8491c94a6 for range [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276], (7303356382144251816,7303733610091843383], (8313290647942866939,8331919689226408479]] failed with error [repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/replication_queue, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276|#7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/replication_queue, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276], (7303356382144251816,7303733610091843383], (8313290647942866939,8331919689226408479]]] Got VALIDATION_REQ failure from /<IP>:7000: UNKNOWN
    at org.apache.cassandra.repair.RepairRunnable$RepairSessionCallback.onFailure(RepairRunnable.java:698)
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1056)
    at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30)
    at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1138)
    at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:958)
    at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:748)
    at org.apache.cassandra.repair.RepairSession.forceShutdown(RepairSession.java:342)
    at org.apache.cassandra.repair.RepairSession$1.onFailure(RepairSession.java:323)
    at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1056)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:826)
Caused by: org.apache.cassandra.exceptions.RepairException: [repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/replication_queue, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276|#7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/replication_queue, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276], (7303356382144251816,7303733610091843383], (8313290647942866939,8331919689226408479]]] Got VALIDATION_REQ failure from /<IP>:7000: UNKNOWN
    at org.apache.cassandra.repair.messages.RepairMessage$1.onFailure(RepairMessage.java:81)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53)
    at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:45)
    at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:432)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:522)
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165)
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137)
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
    ... 2 common frames omitted
[ERROR] [ValidationExecutor:66] 2023-11-05 13:49:58,136 Validator.java:237 - Failed creating a merkle tree for [repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/dc_1cae277e_6e2e_46d2_bffa_983557f6b155, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276|#7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/dc_1cae277e_6e2e_46d2_bffa_983557f6b155, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276], (7303356382144251816,7303733610091843383], (8313290647942866939,8331919689226408479]]], /<IP>:7000 (see log for details)
[ERROR] [ValidationExecutor:66] 2023-11-05 13:49:58,137 ValidationManager.java:173 - Validation failed.
java.lang.RuntimeException: Parent repair session with id = 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 has failed.
    at org.apache.cassandra.service.ActiveRepairService.getParentRepairSession(ActiveRepairService.java:690)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.getSSTablesToValidate(CassandraValidationIterator.java:116)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.<init>(CassandraValidationIterator.java:203)
    at org.apache.cassandra.db.repair.CassandraTableRepairManager.getValidationIterator(CassandraTableRepairManager.java:51)
    at org.apache.cassandra.repair.ValidationManager.getValidationIterator(ValidationManager.java:89)
    at org.apache.cassandra.repair.ValidationManager.doValidation(ValidationManager.java:112)
    at org.apache.cassandra.repair.ValidationManager.access$000(ValidationManager.java:41)
    at org.apache.cassandra.repair.ValidationManager$1.call(ValidationManager.java:162)
    at java.util.concurrent.FutureTask.run(FutureTask.java:277)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:826)
[ERROR] [ValidationExecutor:66] 2023-11-05 13:49:58,137 CassandraDaemon.java:581 - Exception in thread Thread[ValidationExecutor:66,1,main]
java.lang.RuntimeException: Parent repair session with id = 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 has failed.
    at org.apache.cassandra.service.ActiveRepairService.getParentRepairSession(ActiveRepairService.java:690)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.getSSTablesToValidate(CassandraValidationIterator.java:116)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.<init>(CassandraValidationIterator.java:203)
    at org.apache.cassandra.db.repair.CassandraTableRepairManager.getValidationIterator(CassandraTableRepairManager.java:51)
    at org.apache.cassandra.repair.ValidationManager.getValidationIterator(ValidationManager.java:89)
    at org.apache.cassandra.repair.ValidationManager.doValidation(ValidationManager.java:112)
    at org.apache.cassandra.repair.ValidationManager.access$000(ValidationManager.java:41)
    at org.apache.cassandra.repair.ValidationManager$1.call(ValidationManager.java:162)
    at java.util.concurrent.FutureTask.run(FutureTask.java:277)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:826)
[ERROR] [ValidationExecutor:66] 2023-11-05 13:49:58,137 Validator.java:237 - Failed creating a merkle tree for [repair #7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/dc_1a116929_41ec_4be6_b561_0d42dad8caf7, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276|#7fc76c30-7c14-11ee-a6b1-09b8491c94a6 on replication/dc_1a116929_41ec_4be6_b561_0d42dad8caf7, [(-892119161849290738,-881535601694419919], (-3078145018521241272,-3075823683795731276], (7303356382144251816,7303733610091843383], (8313290647942866939,8331919689226408479]]], /<IP>:7000 (see log for details)
[ERROR] [ValidationExecutor:66] 2023-11-05 13:49:58,137 ValidationManager.java:173 - Validation failed.
java.lang.RuntimeException: Parent repair session with id = 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 has failed.
    at org.apache.cassandra.service.ActiveRepairService.getParentRepairSession(ActiveRepairService.java:690)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.getSSTablesToValidate(CassandraValidationIterator.java:116)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.<init>(CassandraValidationIterator.java:203)
    at org.apache.cassandra.db.repair.CassandraTableRepairManager.getValidationIterator(CassandraTableRepairManager.java:51)
    at org.apache.cassandra.repair.ValidationManager.getValidationIterator(ValidationManager.java:89)
    at org.apache.cassandra.repair.ValidationManager.doValidation(ValidationManager.java:112)
    at org.apache.cassandra.repair.ValidationManager.access$000(ValidationManager.java:41)
    at org.apache.cassandra.repair.ValidationManager$1.call(ValidationManager.java:162)
    at java.util.concurrent.FutureTask.run(FutureTask.java:277)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:826)
[ERROR] [ValidationExecutor:66] 2023-11-05 13:49:58,137 CassandraDaemon.java:581 - Exception in thread Thread[ValidationExecutor:66,1,main]
java.lang.RuntimeException: Parent repair session with id = 7fbb5e40-7c14-11ee-a6b1-09b8491c94a6 has failed.
    at org.apache.cassandra.service.ActiveRepairService.getParentRepairSession(ActiveRepairService.java:690)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.getSSTablesToValidate(CassandraValidationIterator.java:116)
    at org.apache.cassandra.db.repair.CassandraValidationIterator.<init>(CassandraValidationIterator.java:203)
    at org.apache.cassandra.db.repair.CassandraTableRepairManager.getValidationIterator(CassandraTableRepairManager.java:51)
    at org.apache.cassandra.repair.ValidationManager.getValidationIterator(ValidationManager.java:89)
    at org.apache.cassandra.repair.ValidationManager.doValidation(ValidationManager.java:112)
    at org.apache.cassandra.repair.ValidationManager.access$000(ValidationManager.java:41)
    at org.apache.cassandra.repair.ValidationManager$1.call(ValidationManager.java:162)
    at java.util.concurrent.FutureTask.run(FutureTask.java:277)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1160)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.lang.Thread.run(Thread.java:826)"
CASSANDRA-19208,Fix ClusterMetadataUpgradeHarryTest,"Unfortunately I do not have a stack trace handy, but the test was essentially timing out after CMS was falling into an infinite loop during initialisation. A problem was caused by the fact that if there is a paxos timeout during propose phase, CMS would get stuck in an infinite loop trying to catch up, and not finding anything to catch up from."
CASSANDRA-19205,Fix flaky test: org.apache.cassandra.tools.BulkLoaderTest.testBulkLoader_WithArgs1-_jdk17,"Seen here: https://app.circleci.com/pipelines/github/driftx/cassandra/1418/workflows/5d108993-69f8-4bb8-a616-93a1d904ff1a/jobs/67177/tests

{quote}
junit.framework.AssertionFailedError: Wrong thread status, active threads unaccounted for: [cluster2-nio-worker-2]
	at org.apache.cassandra.tools.OfflineToolUtils.assertNoUnexpectedThreadsStarted(OfflineToolUtils.java:120)
	at org.apache.cassandra.tools.BulkLoaderTest.testBulkLoader_WithArgs1(BulkLoaderTest.java:97)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{quote}

Looks like either a thread is hanging around too long or we need to except it."
CASSANDRA-19202,A multi-line comment containing semi colon as the end of line throws error,"{{Tested this with C* 4.0.latest.}}

Let's say I've a comment section as follows, it fails today,

 
{quote}{{/*}}
{{ Licensed under the Apache License, Version 2.0 (the ""License"");}}
{{ you may not use this file except in compliance with the License.}}
{{ You may obtain a copy of the License at}}
{{ [http://www.apache.org/licenses/LICENSE-2.0]}}
{{ Unless required by applicable law or agreed to in writing, software}}
{{ distributed under the License is distributed on an ""AS IS"" BASIS,}}
{{ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.}}
{{ See the License for the specific language governing permissions and}}
{{ limitations under the License.}}
{{*/}}

{{SELECT * FROM origin.table_a;}}
{quote}
which throws an exception as follows when execute it as,

{quote}
cqlsh -f test.cql
{quote}
 
error,
{quote}/tmp/test.cql:3:SyntaxException: line 2:64 mismatched character '<EOF>' expecting '*'
/tmp/test.cql:14:SyntaxException: line 1:0 no viable alternative at input 'you' ([you]...)
{quote}
This is because of having a semi-colon at the end of the [1st] line. If that same line ended as follows,
{quote}Version 2.0 (the ""License""); you
{quote}
This error will not occur."
CASSANDRA-19201,Refactor cqlshmain global constants,"Refactor global constants
 * move globals CASSANDRA_CQL_HTML and cqlruleset
 * move module level defaults (DEFAULT_) into Shell class
 * move setup_cqlruleset into cqlshhandling

Remove unused exceptions 
 * FunctionNotFound(Exception)
 * AggregateNotFound(Exception)"
CASSANDRA-19199,Remove write option VALIDATE_SSTABLES to enforce validation,We should not allow the end-user to bypass the non-extended verify. Remove VALIDATE_SSTABLES in writer options in Bulk Writer.
CASSANDRA-19198,[Analytics] Flaky test: SSTableInputStreamTests#testTimeoutShouldAccountForActivityTime,"The test case uses relative time value for assertion. It is flaky since test execution could be delayed depending on the test environment.

Error Message
{code:java}
org.opentest4j.AssertionFailedError: Timeout didn't account for activity time. Took 1219ms should have taken at most 1100ms ==> expected: <true> but was: <false> {code}
Stacktrace
{code:java}
org.opentest4j.AssertionFailedError: Timeout didn't account for activity time. Took 1219ms should have taken at most 1100ms ==> expected: <true> but was: <false>
at app//org.junit.jupiter.api.AssertionFailureBuilder.build(AssertionFailureBuilder.java:151)
at app//org.junit.jupiter.api.AssertionFailureBuilder.buildAndThrow(AssertionFailureBuilder.java:132)
at app//org.junit.jupiter.api.AssertTrue.failNotTrue(AssertTrue.java:63)
at app//org.junit.jupiter.api.AssertTrue.assertTrue(AssertTrue.java:36)
at app//org.junit.jupiter.api.Assertions.assertTrue(Assertions.java:211)
at app//org.apache.cassandra.spark.utils.SSTableInputStreamTests.testTimeoutShouldAccountForActivityTime(SSTableInputStreamTests.java:234)
... {code}"
CASSANDRA-19197,OA format serialization/deserialization problem,"There is a problem with either serialization or deserialization of the new TTL
"
CASSANDRA-19196,Don't allow to enable direct i/o with broken kernels,"https://lwn.net/Articles/954285/, found by [~rustyrazorblade]
"
CASSANDRA-19193,Reimplement ClusterMetadata::writePlacementAllSettled,"This should step through InProgressSequences to determine state when finished, rather than emulating the logic inline."
CASSANDRA-19191,"Optimisations to PlacementForRange, improve lookup on r/w path",The lookup used when selecting the appropriate replica group for a range or token while peforming reads and writes is extremely simplistic and inefficient. There is plenty of scope to improve {{PlacementsForRange}} to by replacing the current naive iteration with use a more efficient lookup.
CASSANDRA-19190,ForceSnapshot transformations should not be persisted in the local log table,"Per its inline comments, ForceSnapshot is a synthetic transformation whose purpose it to enable the local log to jump missing epochs. A common use for this is when replaying persisted events from the metadata log at startup. The log is initialised with {{Epoch.EMPTY}} but rather that replaying every single entry since the beginning of history, we select the most recent snapshot held locally and start the replay from that point. Likewise, when catching up from a peer, a node may receive a snapshot plus subsequent log entries. In order to bring local metadata to the same state as the snapshot, a {{ForceSnapshot}} with the same epoch as the snapshot is inserted into the {{LocalLog}} and enacted like any other other transformation. These synthetic transformations should not be persisted in the `system.local_metadata_log`, as they do not exist in the distributed metadata log. We _should_ persist the snapshot itself in {{system.metadata_snapshots}} so that we can avoid having to re-fetch remote snapshots (i.e. if a node were to restart shortly after receiving a catchup from a peer)."
CASSANDRA-19189,Revisit use of sealed period lookup tables,"Metadata snapshots are stored locally in the {{system.metadata_snapshots}} table, which is keyed by epoch. Snapshots are retrieved from this table for three purposes:
* to replay locally during startup
* to provide log state for a peer requesting catchup
* to create point-in-time ClusterMetadata, for disaster recovery

In the majority of cases, we always want to replay from the most recent snapshot so we can usually select the appropriate snapshot by simply scanning the snapshots table in reverse, which allows us to considerably simplify the process of looking up the desired snapshot. We will continue to persist historical snapshots, at least for now, so that we are able to select arbitrary snapshots should we want to reconstruct metadata state for arbitrary epochs."
CASSANDRA-19187,nodetool assassinate may cause thread serialization for that node,"When assassinate an ip address that is not in the gossip map, a ""corrupted"" entry will be inserted into the gossip map. [(1)|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/gms/Gossiper.java#L810]

For example, if we do ""nodetool assassinate 10.1.1.1""

we will get an entry like below by running ""nodetool gossipinfo"":

 
{code:java}
/10.1.1.1
  generation:1702006511
  heartbeat:9999
  STATUS:209516:LEFT,-8393921141401589197,1702265651923
  STATUS_WITH_PORT:209515:LEFT,-8393921141401589197,1702265651923
  TOKENS: not present {code}
 

This entry in endpointStateMap will cause issue for [isUpgradingFromVersionLowerThan|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/gms/Gossiper.java#L2284] function. Because the [upgradeFromVersionSupplier|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/gms/Gossiper.java#L191] supplier will always set the [allHostsHaveKnownVersion|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/gms/Gossiper.java#L216] flag to false so no memoized value will be returned. The ""get"" function will always require a lock from this [line|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/ExpiringMemoizingSupplier.java#L66].

If application is using ""fetchAll"", the native-transport-requests thread will hit this [line|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/db/filter/ColumnFilter.java#L574]. This means all the native-transport-requests thread is serialized, also, the lock is shared by GossipStage threads. It means if a node in a cluster with the corrupted gossip map is restart, the node will run into this problem.

To fix the issue,
 # Why we want to add a dummy entry for nodetool assassinate if the endpoint is not in the map anymore. Should we do nothing or throw exception if the node is not in the gossip map anymore?
 # Before checking if a version is null, we should make sure the node is not a dead node. A decommissioned node, a left node should not be considered part of the cluster anymore when calculating ""upgradeInProgressPossible""

 "
CASSANDRA-19185,Vector search tests are failing on recall accuracy,"Vector tests are failing randomly because they do not meet recall assertion values. Currently, the following tests have been reported as failing:

VectorSegmentationTest.testMultipleSegmentsForCompaction
VectorDistributedTest.rangeRestrictedTest
VectorDistributedTest.testPartitionRestrictedVectorSearch

Since the vector searches are approximate and the vectors used in the tests are random, it is unlikely that they will always meet a high recall. The recall assertions are looking for recall values of 0.9 and above. Part of this issue is related to the use of random values in the vectors being tested. We have seen, with other tests, that the vector search performs better with non-random generated datasets like the Glove datasets. As such, there are the following available to fix these tests.
 # Downgrade the assertions to a value that is likely to always pass. The problem is that there is no guarantee that a test will always pass any recall value we give it.
 # Use generated datasets for these tests to see if that improves the recall results.
 # Remove the recall assertions unless they are specifically asked for. We could use a system property to enable recall testing for targeted vector testing.

I don't think option 1 is a viable long-term solution as we can never be certain that it will always work. Option 2 has more promise but it could still result in failures because of the approximate nature of the vector searches. As such, option 3 seems the only viable solution here but means that, in most cases, we are only really testing that we are returning results from the search, not how accurate those results are.
 "
CASSANDRA-19184,logback-core-1.2.12.jar vulnerability: CVE-2023-6481,"https://nvd.nist.gov/vuln/detail/CVE-2023-6481

{quote}
A serialization vulnerability in logback receiver component part of logback version 1.4.13, 1.3.13 and 1.2.12 allows an attacker to mount a Denial-Of-Service attack by sending poisoned data. 
{quote}"
CASSANDRA-19183,Fix cqlshlib cython test,"This is failing on [trunk|https://app.circleci.com/pipelines/github/driftx/cassandra/1405/workflows/ff7b5121-b6df-4290-8d63-bcc6d6385724/jobs/65554] and [5.0|https://app.circleci.com/pipelines/github/driftx/cassandra/1406/workflows/f7f2b35c-1233-43d7-a942-218f1712add4/jobs/65649]:

{quote}
PermissionError: [Errno 13] Permission denied: '/.ccm'
{quote}"
CASSANDRA-19182,IR may leak SSTables with pending repair when coming from streaming,"There is a race condition where SSTables from streaming may race with pending repair cleanup in compaction causing us to cleanup the pending repair state in compaction while the SSTables are being added to it; this leads to IR failing in the future when those files get selected for repair.

This problem was hard to track down as the in-memory state was wiped, so we don’t have any details.  To better aid these types of investigation we should make sure the repair vtables get updated when IR session failures are submitted"
CASSANDRA-19180,Support reloading certificate stores in cassandra-java-driver,"Currently, apache/cassandra-java-driver does not reload SSLContext when the underlying certificate store files change. When the DefaultSslEngineFactory (and the other factories) are set up, they build a fixed instance of javax.net.ssl.SSLContext that doesn't change: https://github.com/apache/cassandra-java-driver/blob/12e3e3ea027c51c5807e5e46ba542f894edfa4e7/core/src/main/java/com/datastax/oss/driver/internal/core/ssl/DefaultSslEngineFactory.java#L74

This fixed SSLContext is used to negotiate SSL with the cluster, and if a keystore is reloaded on disk it isn't picked up by the driver, and future reconnections will fail if the keystore certificates have expired by the time they're used to handshake a new connection.

We should reload client certificates so that applications that provide them can use short-lived certificates and not require a bounce to pick up new certificates. This is especially relevant in a world with CASSANDRA-18554 and broad use of mTLS.

I have a patch for this that is nearly ready. Now that the project has moved under apache/ - who can I work with to understand how CI works now?"
CASSANDRA-19177,SAI query timeouts can cause resource leaks,There are several places in the SAI query path where a query timeout can result in a resource not being closed correctly. We need to make sure that wherever QueryContext.checkpoint is called we catch the resulting exception and close any open resources.
CASSANDRA-19171,Test Failure: org.apache.cassandra.locator.PropertyFileSnitchTest.configContainsRemoteConfig-cdc_jdk17_x86_64,"h3.  
{code:java}
Error Message
Multiple entries with same key: 127.0.0.1:7012=OTHER_DC1:OTHER_RAC1 and 127.0.0.1:7012=DC1:RAC1

Stacktrace
java.lang.IllegalArgumentException: Multiple entries with same key: 127.0.0.1:7012=OTHER_DC1:OTHER_RAC1 and 127.0.0.1:7012=DC1:RAC1 at com.google.common.collect.ImmutableMap.conflictException(ImmutableMap.java:378) at com.google.common.collect.ImmutableMap.checkNoConflict(ImmutableMap.java:372) at com.google.common.collect.RegularImmutableMap.checkNoConflictInKeyBucket(RegularImmutableMap.java:246) at com.google.common.collect.RegularImmutableMap.fromEntryArrayCheckingBucketOverflow(RegularImmutableMap.java:133) at com.google.common.collect.RegularImmutableMap.fromEntryArray(RegularImmutableMap.java:95) at com.google.common.collect.RegularImmutableMap.fromEntries(RegularImmutableMap.java:78) at com.google.common.collect.ImmutableMap.of(ImmutableMap.java:139) at org.apache.cassandra.locator.PropertyFileSnitchTest.configContainsRemoteConfig(PropertyFileSnitchTest.java:121) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-19169,Don't NPE when initializing CFSs for local system keyspaces with UCS,"When UnifiedCompactionStrategy is used as the default, NPEs are thrown when
flushing the system keyspace tables early during startup. The system keyspace is
initialised before the cluster metadata, but UCS currently tries to access the
current epoch when initialising the shard manager, to determine whether the
local ranges are out of date. This isn't necessary for the system keyspaces as
they use LocalStrategy and cover the whole token space."
CASSANDRA-19168,Test Failure: VectorUpdateDeleteTest fails with heap_buffers,"When {{memtable_allocation_type}} is set to {{heap_buffers}}, {{updateTest}} fails with
{code}
junit.framework.AssertionFailedError: Result set does not contain a row with pk = 0
	at org.apache.cassandra.index.sai.cql.VectorTypeTest.assertContainsInt(VectorTypeTest.java:133)
	at org.apache.cassandra.index.sai.cql.VectorUpdateDeleteTest.updateTest(VectorUpdateDeleteTest.java:308)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
{code}"
CASSANDRA-19167,Test failure: CQLVectorTest fails with heap_buffers,"When {{memtable_allocation_type}} is set to {{heap_buffers}}, the {{udf}} test fails with
{code}
org.apache.cassandra.cql3.functions.types.exceptions.InvalidTypeException: Invalid 32-bits integer value, expecting 4 bytes but got 6
	at org.apache.cassandra.cql3.functions.types.TypeCodec$IntCodec.deserializeNoBoxing(TypeCodec.java:1695)
	at org.apache.cassandra.cql3.functions.types.TypeCodec$PrimitiveIntCodec.deserialize(TypeCodec.java:842)
	at org.apache.cassandra.cql3.functions.types.TypeCodec$PrimitiveIntCodec.deserialize(TypeCodec.java:819)
	at org.apache.cassandra.cql3.functions.types.VectorCodec$FixedLength.deserialize(VectorCodec.java:135)
	at org.apache.cassandra.cql3.functions.types.VectorCodec$FixedLength.deserialize(VectorCodec.java:83)
	at org.apache.cassandra.cql3.functions.types.TypeCodec$AbstractCollectionCodec.deserialize(TypeCodec.java:2141)
	at org.apache.cassandra.cql3.functions.types.TypeCodec$AbstractCollectionCodec.deserialize(TypeCodec.java:2082)
	at org.apache.cassandra.cql3.functions.UDFDataType.compose(UDFDataType.java:180)
	at org.apache.cassandra.cql3.functions.FunctionArguments.set(FunctionArguments.java:142)
	at org.apache.cassandra.cql3.selection.AbstractFunctionSelector.setArg(AbstractFunctionSelector.java:277)
	at org.apache.cassandra.cql3.selection.ScalarFunctionSelector.getOutput(ScalarFunctionSelector.java:58)
	at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:605)
	at org.apache.cassandra.cql3.selection.ResultSetBuilder.getOutputRow(ResultSetBuilder.java:175)
	at org.apache.cassandra.cql3.selection.ResultSetBuilder.build(ResultSetBuilder.java:162)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:999)
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:564)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:600)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeLocally(SelectStatement.java:570)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeLocally(SelectStatement.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:445)
	at org.apache.cassandra.cql3.CQLTester.executeFormattedQuery(CQLTester.java:1597)
	at org.apache.cassandra.cql3.CQLTester.execute(CQLTester.java:1576)
	at org.apache.cassandra.cql3.validation.operations.CQLVectorTest.udf(CQLVectorTest.java:427)
{code}"
CASSANDRA-19166,StackOverflowError on ALTER after many previous schema changes,"Since 4.1, TableMetadataRefCache re-wraps its fields in Collections.unmodifiableMap on every local schema update. This causes TableMetadataRefCache's Map fields to reference chains of nested UnmodifiableMaps. Eventually, this leads to a StackOverflowError on get(), which has to traverse lots of these maps to fetch the actual value.

https://github.com/apache/cassandra/blob/4059faf5b948c5a285c25fb0f2e4c4288ee7c305/src/java/org/apache/cassandra/schema/TableMetadataRefCache.java#L53

The issue goes away on restart, since TableMetadataRefCache is reloaded from disk.

See CASSANDRA-17044, when TableMetadataRefCache was introduced. This issue was discovered on a real test cluster where schema changes were failing, via a heap dump."
CASSANDRA-19163,Test Failure: org.apache.cassandra.db.DiskBoundaryManagerTest.alterKeyspaceTest-cdc_jdk17_x86_64 ,"[https://ci-cassandra.apache.org/job/Cassandra-trunk/1801/testReport/org.apache.cassandra.db/DiskBoundaryManagerTest/alterKeyspaceTest_cdc_jdk17_x86_64/]
{code:java}

Error Message
expected not same

Stacktrace
junit.framework.AssertionFailedError: expected not same at org.apache.cassandra.db.DiskBoundaryManagerTest.alterKeyspaceTest(DiskBoundaryManagerTest.java:140) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-19162,Test Failure: 	SSTableCorruptionDetectionTest.testSinglePartitionIterator,"[https://ci-cassandra.apache.org/job/Cassandra-trunk/1785/testReport/org.apache.cassandra.io.sstable/SSTableCorruptionDetectionTest/testSinglePartitionIterator_cassandra_testtag_IS_UNDEFINED/]
h3.  
{code:java}
Error Message
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

Stacktrace
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout. at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.util.Vector.forEach(Vector.java:1365) at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.util.Vector.forEach(Vector.java:1365) at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at jdk.internal.reflect.GeneratedMethodAccessor49.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.util.Vector.forEach(Vector.java:1365) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at org.apache.cassandra.anttasks.TestHelper.execute(TestHelper.java:53) at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.util.Vector.forEach(Vector.java:1365) at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
This test has timedout many times on trunk these days (5 out of 7), I found the same failure only once on 5.0 in Jenkins
[https://ci-cassandra.apache.org/job/Cassandra-5.0/116/testReport/org.apache.cassandra.io.sstable/SSTableCorruptionDetectionTest/testSinglePartitionIterator_cassandra_testtag_IS_UNDEFINED/]



 "
CASSANDRA-19158,Reuse native transport-driven futures in Debounce,"Currently, we create a future in Debounce, then create one more future in RemoteProcessor#sendWithCallback. This is further exacerbated by chaining calls, when we first attempt to catch up from peer, and then from CMS.


First of all, we should always only use a native transport timeout driven futures returned from sendWithCallback, since they implement reasonable retries under the hood, and are easy to bulk-configure (ie you can simply change timeout in yaml and have all futures change their behaviour).


Second, we should _chain_ futures and use map or andThen for fallback operations such as trying to catch up from CMS after an unsuccesful attemp to catch up from peer.


This should significantly simplify the code and number of blocked/waiting threads."
CASSANDRA-19153,Preclude irrecoverable log corruption in case split-brain situation during leader election with absent seeds,"It should be possible to detect a scenario where two partitioned nodes independently elect themselves as the first CMS nodes in a brand new cluster. In such a case, metadata changes should not be applied so that the conflict can be resolved.
"
CASSANDRA-19149,Fix SAI statement,"Hi docs. There appears to be an error in the UDT section on the page for SAI queries.
[!https://www.datastax.com/favicon.ico!Query with SAI :: Cassandra Query Language (CQL)|https://docs.datastax.com/en/cql/astra/docs/developing/indexing/sai/sai-query.html#user-defined-type] The doco states ""{_}SAI can index either a single field of a user-defined type (UDT) or a list of UDTs. This example shows how to index a list of UDTs.{_}""
I think the doco is trying to say that you can index a UDT or a list of UDTs, but it reads as though I can index a field within the UDT.Could we improve the doco to read:
""{_}SAI can index a user-defined type (UDT) or a list of UDTs. This example shows how to index a list of UDTs.{_}"""
CASSANDRA-19148,Remove unused dead code in Cassandra Analytics,There are a large chunk of code in Cassandra Analytics are unused. We should remove those code for the sake of maintenance.
CASSANDRA-19146,Upgrade owasp to 9.0.x,"From https://github.com/jeremylong/DependencyCheck :

{quote}
Upgrading to 9.0.0 or later is mandatory; previous versions of dependency-check utilize the NVD data feeds which will be deprecated on Dec 15th, 2023. 
{quote}"
CASSANDRA-19142,logback-core-1.2.12.jar vulnerability: CVE-2023-6378,"https://nvd.nist.gov/vuln/detail/CVE-2023-6378

{quote}
A serialization vulnerability in logback receiver component part of logback version 1.4.11 allows an attacker to mount a Denial-Of-Service attack by sending poisoned data. 
{quote}"
CASSANDRA-19141,Security vulnerabilities in SnakeYAML In Cassandra (CVE-2022-1471),"Component name - SnakeYAML
Component version name - 1.26
CVE - CVE-2022-1471 (BDSA-2022-3447)
CVSS - 9.8 (Critical)

Archive Context and Path
apache-cassandra/lib/snakeyaml-1.26.jar"
CASSANDRA-19140,Security vulnerabilities in google-guava In Cassandra (CVE-2023-2976),"Security vulnerabilities in google-guava In Cassandra (CVE-2023-2976)

Component name - google-guava
Component version name - v27.0
CVE - CVE-2023-2976 (BDSA-2016-1748)
CVSS - 7.1 (High)

Archive Context and Path
apache-cassandra/lib/guava-27.0-jre.jar"
CASSANDRA-19139,Security vulnerabilities in Netty in cassandra netty-all-4.1.58.Final.jar ,"CVE - 
|CVE-2021-21290 (BDSA-2021-0311)|
|CVE-2021-21295 (BDSA-2021-0589)|
|CVE-2021-21409 (BDSA-2021-0828)|
|CVE-2021-37136 (BDSA-2021-2832)|
|CVE-2021-37137 (BDSA-2021-2831)|
|CVE-2021-43797 (BDSA-2021-3741)|
|CVE-2022-24823|
|CVE-2022-41881 (BDSA-2022-3559)|
|CVE-2023-34462 (BDSA-2023-1556)|"
CASSANDRA-19138,Flush sealed period/snapshot tables on every write,"These tables are low throughput, so flushing on every write would not have a significant perf impact, but would improve availability of snapshots for recovery and replay at startup"
CASSANDRA-19132,Update use of transition plan in PrepareReplace,"When PlacementTransitionPlan was reworked to make its use more consistent across join and leave operations, PrepareReplace was not updated. This could now be simplified in line with the other operations."
CASSANDRA-19128,The result of applying a metadata snapshot via ForceSnapshot should return the correct set of modified keys,It should use the same logic as Transformer::build to compare the updated CM with the previous to derive the modified keys
CASSANDRA-19127,Test Failure: org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest,"The test is not flaky on 5.0 - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2590/workflows/47dedf52-87fd-4178-bc89-d179e58b6562
But it is significantly flaky on trunk - https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2591/workflows/1150aab2-4961-4fe3-a126-b96356fdb939/jobs/49867/tests
{code:java}
org.apache.cassandra.simulator.SimulationException: Failed on seed 0xf2b8eff98afd45dd
	Suppressed: java.lang.RuntimeException: java.util.concurrent.TimeoutException
		at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
		at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:537)
		at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098)
		at org.apache.cassandra.simulator.ClusterSimulation.close(ClusterSimulation.java:854)
		at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:361)
		at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:346)
		at org.apache.cassandra.simulator.paxos.PaxosSimulationRunner$Run.run(PaxosSimulationRunner.java:34)
		at org.apache.cassandra.simulator.paxos.PaxosSimulationRunner.main(PaxosSimulationRunner.java:148)
		at org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest(ShortPaxosSimulationTest.java:101)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	Caused by: java.util.concurrent.TimeoutException
		at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:253)
		at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:529)
		Suppressed: java.util.concurrent.TimeoutException
		Suppressed: java.util.concurrent.TimeoutException
		Suppressed: java.util.concurrent.TimeoutException
		Suppressed: java.util.concurrent.TimeoutException
		Suppressed: java.util.concurrent.TimeoutException
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.simulator.cluster.KeyspaceActions.scheduleAndUpdateTopologyOnCompletion(KeyspaceActions.java:352)
	at org.apache.cassandra.simulator.cluster.KeyspaceActions.next(KeyspaceActions.java:291)
	at org.apache.cassandra.simulator.Actions.next(Actions.java:147)
	at org.apache.cassandra.simulator.Actions.lambda$streamNextSupplier$3(Actions.java:156)
	at org.apache.cassandra.simulator.Actions$LambdaAction.performSimple(Actions.java:63)
	at org.apache.cassandra.simulator.Action.performAndRegister(Action.java:468)
	at org.apache.cassandra.simulator.Action.perform(Action.java:486)
	at org.apache.cassandra.simulator.ActionSchedule.next(ActionSchedule.java:379)
	at org.apache.cassandra.simulator.paxos.PaxosSimulation$2.next(PaxosSimulation.java:217)
	at org.apache.cassandra.simulator.paxos.PaxosSimulation.run(PaxosSimulation.java:189)
	at org.apache.cassandra.simulator.paxos.PairOfSequencesPaxosSimulation.run(PairOfSequencesPaxosSimulation.java:351)
	at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:365)
	at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:346)
	at org.apache.cassandra.simulator.paxos.PaxosSimulationRunner$Run.run(PaxosSimulationRunner.java:34)
	at org.apache.cassandra.simulator.paxos.PaxosSimulationRunner.main(PaxosSimulationRunner.java:148)
	at org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest(ShortPaxosSimulationTest.java:101)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
"
CASSANDRA-19126,Streaming appears to be incompatible with different storage_compatibility_mode settings,"In particular, SSTableLoader appears to be incompatible with storage_compatibility_mode: NONE, which manifests as a failure of {{org.apache.cassandra.distributed.test.SSTableLoaderEncryptionOptionsTest}} when the flag is turned on (found during CASSANDRA-18753 testing). Setting {{storage_compatibility_mode: NONE}} in the tool configuration yaml does not help (according to the docs, this setting is not picked up).

This is likely a bigger problem as the acceptable streaming version for C* 5 is 12 only in legacy mode and 13 only in none, i.e. two C* 5 nodes do not appear to be able to stream with each other if their setting for the compatibility mode is different."
CASSANDRA-19125,Investigate increased memory usage in tests with TCM,A few tests started failing with OOM after we committed TCM - we should investigate why/if we use more memory
CASSANDRA-19124,Test failure:  ConsistentBootstrapTest#coordinatorIsBehindTest,"{code}
Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit. 

junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit. 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at org.apache.cassandra.anttasks.TestHelper.execute(TestHelper.java:53) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
{code}"
CASSANDRA-19123,Test failure: ConsistentMoveTest#moveTest-cassandra.testtag_IS_UNDEFINED,"{code}
Timeout occurred. Please note the time in the report does not reflect the time until the timeout. 

junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout. 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at org.apache.cassandra.anttasks.TestHelper.execute(TestHelper.java:53) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at java.base/java.util.Vector.forEach(Vector.java:1394) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
{code}"
CASSANDRA-19122,Test failure: org.apache.cassandra.distributed.test.ReadRepairEmptyRangeTombstonesTest-_jdk11,"{code}
java.util.concurrent.TimeoutException 

java.lang.RuntimeException: java.util.concurrent.TimeoutException 
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79) 
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:537) 
at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098) 
at org.apache.cassandra.distributed.test.ReadRepairEmptyRangeTombstonesTest.teardownCluster(ReadRepairEmptyRangeTombstonesTest.java:108) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
Caused by: java.util.concurrent.TimeoutException 
at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:253) 
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:529) 
Suppressed: java.util.concurrent.TimeoutException 
{code}"
CASSANDRA-19121,Remove unused suppressions,"After CASSANDRA-18875 we have some unused suppressions that can be removed:

{quote}
[dependency-check] Suppression Rule had zero matches: SuppressionRule{packageUrl=PropertyType{value=^pkg:maven/org\.yaml/snakeyaml@.*$, regex=true, caseSensitive=false},cve={CVE-2023-2251,CVE-2022-25857,CVE-2022-38749,CVE-2022-38750,CVE-2022-38751,CVE-2022-38752,CVE-2022-41854,CVE-2022-1471,CVE-2022-3064,CVE-2021-4235,CVE-2017-18640,}}
[dependency-check] Suppression Rule had zero matches: SuppressionRule{packageUrl=PropertyType{value=^pkg:maven/com\.fasterxml\.jackson\.core/jackson\-databind@.*$, regex=true, caseSensitive=false},cve={CVE-2022-42003,CVE-2022-42004,CVE-2023-35116,}}
{quote}"
CASSANDRA-19120,local consistencies may get timeout if blocking read repair is sending the read repair mutation to other DC ,"For a two DCs cluster setup. When a new node is being added to DC1, for blocking read repair triggered by local_quorum in DC1, it will require to send read repair mutation to an extra node(1)(2). The selector for read repair may select *ANY* node that has not been contacted before(3) instead of selecting the DC1 nodes. If a node from DC2 is selected, this will cause 100% timeout because of the bug described below:

When we initialized the latch(4) for blocking read repair, the shouldBlockOn function will only return true for local nodes(5), the blockFor value will be reduced if a local node doesn't require repair(6). The blockFor is same as the number of read repair mutation sent out. But when the coordinator node receives the response from the target nodes, the latch only count down for nodes in same DC(7). The latch will wait till timeout and the read request will timeout.

This can be reproduced if you have a constant load on a 3 + 3 cluster when adding a node. If you have someway to trigger blocking read repair(maybe by adding load using stress tool). If you use local_quorum consistency with a constant read after write load in the same DC that you are adding node. You will see read timeout issue from time to time because of the bug described above

 

I think for read repair when selecting the extra node to do repair, we should prefer local nodes than the nodes from other region. Also, we need to fix the latch part so even if we send mutation to the nodes in other DC, we don't get a timeout.

(1)[https://github.com/apache/cassandra/blob/cassandra-4.0.11/src/java/org/apache/cassandra/locator/ReplicaPlans.java#L455]

(2)[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/ConsistencyLevel.java#L183]

(3)[https://github.com/apache/cassandra/blob/cassandra-4.0.11/src/java/org/apache/cassandra/locator/ReplicaPlans.java#L458]

(4)[https://github.com/apache/cassandra/blob/cassandra-4.0.11/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java#L96]

(5)[https://github.com/apache/cassandra/blob/cassandra-4.0.11/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java#L71]

(6)[https://github.com/apache/cassandra/blob/cassandra-4.0.11/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java#L88]

(7)[https://github.com/apache/cassandra/blob/cassandra-4.0.11/src/java/org/apache/cassandra/service/reads/repair/BlockingPartitionRepair.java#L113]

 "
CASSANDRA-19119,Remove relocated prop,"When relocating, relocator matches strings, so properties do not have to be added with {{relocated}}, otherwise we have duplicates"
CASSANDRA-19118,Add support of vector type to COPY command,"Currently it's not possible to import rows with vector literals via {{COPY}} command.

STR:
 * Create a table
{code:sql}
CREATE TABLE testcopyfrom (id text PRIMARY KEY, embedding_vector VECTOR<FLOAT, 6>
{code}

 * Prepare csv file with sample data, for instance:
{code:sql}
1,""[0.1, 0.2, 0.3, 0.4, 0.5, 0.6]""
2,""[-0.1, -0.2, -0.3, -0.4, -0.5, -0.6]"" {code}

 * in cqlsh run
{code:sql}
COPY ks.testcopyfrom FROM data.csv
{code}

It will result in getting:
{code:sql}
TypeError: Received an argument of invalid type for column ""embedding_vector"". Expected: <class 'cassandra.cqltypes.VectorType(6)'>, Got: <class 'str'>; (required argument is not a float){code}"
CASSANDRA-19117,Harry: Remove notion of Modification,
CASSANDRA-19116,History Builder API 2.0,"Harry history Builder 2.0

      * New History Builder API
      * Add an ability to track LTS visiteb by partition in visited_lts static column
      * Add a model checker that checks against a different Cluster instance (for example, flush vs no flush, local vs nonlocal, etc)
      * Add an ability to issue LTSs out-of-order"
CASSANDRA-19115,Avoid bad serializedSize when sending GOSSIP_SHUTDOWN message,In CASSANDRA-18913 we started sending EndpointState with the GOSSIP_SHUTDOWN message - but there is no guard that EndpointState doesn't change between calls to {{serializedSize}} and {{serialize}}
CASSANDRA-19114,Fix flaky test accord.utils.RandomSourceTest.testBiasedInts,This test fails from time to time as binary search finds the first match it finds and not the first in the array.  This class is also problematic as it hand rolls property test framework but does so in a way where seeds may be lost or attributed to the wrong test.
CASSANDRA-19112,UCS min_sstable_size should not be larger than target_sstable_size lower bound,
CASSANDRA-19111,Add j17|j11_dtests_repeat and j17|j11_dtests_vnode_repeat among circle jobs,"j17_dtests_repeat and j17_dtests_vnode_repeat do exist in circleci but they are not referenced in workflows. I basically can not run multiplexer on dtests in circle.

Same happens for their j11 counterparts.

Same applies for 3.0 where j8_dtests_repeat is, again, not referenced in workflows. I guess it will be same for all branches we have."
CASSANDRA-19110,"apt-key deprecation, replace with gpg --dearmor in the docs.","the command `apt-key` is deprecated and soon to be removed, especially on Ubuntu.
the directory `/usr/share/keyrings` for shared keys are also being removed.

I suggest to convert the docs from
{code:java}
curl https://downloads.apache.org/cassandra/KEYS | sudo apt-key add - {code}
to a simpler command:
{code:java}
curl https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor -o /etc/apt/keyrings/cassandra-archive-keyring.gpg {code}
The path `/etc/apt/keyrings` doesn't exists by default on Ubuntu 20.04 but it does on 22.04.



I also suggest to add the source.list.d text from 
{code:java}
$ echo ""deb https://debian.cassandra.apache.org 42x main"" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

deb https://debian.cassandra.apache.org 42x main{code}
to 
{code:java}
$ echo ""deb [signed-by=/etc/apt/keyrings/cassandra-archive-keyring.gpg] https://debian.cassandra.apache.org 42x main"" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

deb [signed-by=/etc/apt/keyrings/cassandra-archive-keyring.gpg] https://debian.cassandra.apache.org 42x main {code}
I have made a [PR|https://github.com/apache/cassandra/pull/2936]

I have tested the gpg --dearmor on a VM with Ubuntu 22.04 myself recently and it works just fine."
CASSANDRA-19109,Couldn't find table with id on deserialization,"I hit this when I was testing CASSANDRA-19103, 19103 is a fairly simple patch (PR in that ticket) so I do not think the test itself is to blame. I run two multiplexers with 500 runs each, 1k in total, and it failed like this just 1 time.

{code}
java.lang.RuntimeException: Can not deserialize message MessageImpl{verb=24, bytes=2c09131949ea93881801005c011b255f4def2540a6000000000000000809040000000010fceadfeced28a000000105746f74616c01200000000000000024000100000000000000000000c00000000000000000000000000000010000000000000001010003414c4c, id=44, version=13, from=/127.0.0.3:7012}
	at org.apache.cassandra.distributed.impl.Instance.deserializeMessage(Instance.java:481)
	at org.apache.cassandra.distributed.impl.Instance.lambda$receiveMessageRunnable$43231af8$1(Instance.java:516)
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$async$10(IsolatedExecutor.java:156)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: org.apache.cassandra.exceptions.UnknownTableException: Couldn't find table with id 1b255f4d-ef25-40a6-0000-000000000008. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
	at org.apache.cassandra.schema.SchemaProvider.getExistingTableMetadata(SchemaProvider.java:160)
	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:747)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:493)
	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:522)
	at org.apache.cassandra.db.CounterMutation$CounterMutationSerializer.deserialize(CounterMutation.java:374)
	at org.apache.cassandra.db.CounterMutation$CounterMutationSerializer.deserialize(CounterMutation.java:364)
	at org.apache.cassandra.net.Message$Serializer.deserialize(Message.java:791)
	at org.apache.cassandra.distributed.impl.Instance.deserializeMessage(Instance.java:477)
{code}

Basically, as it goes to deserialize a counter mutation, it will not find schema of that table.

https://app.circleci.com/pipelines/github/instaclustr/cassandra/3562/workflows/3aeab43a-1b13-4b93-a6a9-c6c3cca0e9ca/jobs/157655/tests#failed-test-0"
CASSANDRA-19107,Revert unnecessary read lock acquisition when reading ring version in TokenMetadata introduced in CASSANDRA-16286,"CASSANDRA-16286 achieved its goal of making sure that concurrent increments to the ring version would independently increment the version (i.e. not ""merge"" multiple invalidations into single versions), but it also unnecessarily replaced the volatile read on {{ringVersion}} w/ making {{readVersion}} non-volatile and acquiring the read lock on the fair {{ReadWriteLock}} in {{TokenMetadata}}. This can result in unnecessary queueing w/ high CPU usage/read volume. For example, you might see this on a 4.0 cluster...

{noformat}
""Native-Transport-Requests-99"" #271 daemon prio=5 os_prio=0 cpu=5822566.56ms elapsed=19477779.40s tid=0x00007fcc96c31b00 nid=0xb7bd waiting on condition  [0x00007fcb7f144000]
  java.lang.Thread.State: WAITING (parking)
       at jdk.internal.misc.Unsafe.park(java.base@11.0.16/Native Method)
       - parking to wait for  <0x00000005c0ab92a0> (a java.util.concurrent.locks.ReentrantReadWriteLock$FairSync)
       at java.util.concurrent.locks.LockSupport.park(java.base@11.0.16/LockSupport.java:194)
       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(java.base@11.0.16/AbstractQueuedSynchronizer.java:885)
       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(java.base@11.0.16/AbstractQueuedSynchronizer.java:1009)
       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(java.base@11.0.16/AbstractQueuedSynchronizer.java:1324)
       at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(java.base@11.0.16/ReentrantReadWriteLock.java:738)
       at org.apache.cassandra.locator.TokenMetadata.getRingVersion(TokenMetadata.java:1389)
       at org.apache.cassandra.locator.AbstractReplicationStrategy.getCachedReplicas(AbstractReplicationStrategy.java:82)
       at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalReplicas(AbstractReplicationStrategy.java:116)
       at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalReplicasForToken(AbstractReplicationStrategy.java:109)
       at org.apache.cassandra.locator.ReplicaLayout.forTokenWriteLiveAndDown(ReplicaLayout.java:209)
       at org.apache.cassandra.locator.ReplicaPlans.forWrite(ReplicaPlans.java:328)
       at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:1426)
       at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:937)
{noformat}

Reverting to a volatile read makes this no longer possible, but keeps the fix from CASSANDRA-16286 intact."
CASSANDRA-19106,harry-core-0.0.2 vulnerability: CVE-2020-13946,"https://nvd.nist.gov/vuln/detail/CVE-2020-13946

This is our own old CVE so we can probably suppress."
CASSANDRA-19104,Standardize tablestats formatting and data units,"Tablestats reports output in plaintext, JSON or YAML. The human readable output currently has a mix of KiB, bytes with inconsistent spacing

Simplify and default output to 'human readable'. Machine readable output is available as an option and the current mixed output formatting is neither friendly for human or machine reading and can be replaced.

!image-2023-11-27-13-49-14-247.png!

*Not a goal now (consider a follow up Jira):*

Fix inconsistencies with KiB/MiB/GiB and KB/MB/GB formatting:
 * gcstats - uses MB
 * getcompactionthroughput - uses MB/s
 * getstreamthroughput - uses MB/s
 * info - uses MiB/GiB"
CASSANDRA-19102,Test Failure: org.apache.cassandra.distributed.test.ReadRepairTest#readRepairRTRangeMovementTest,"{noformat}
java.lang.AssertionError: Expected a different error message, but got Operation failed - received 2 responses and 1 failures: INVALID_ROUTING from /127.0.0.2:7012

	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.cassandra.distributed.test.ReadRepairTest.readRepairRTRangeMovementTest(ReadRepairTest.java:424)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)
	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
{noformat}

Manual testing in IntelliJ / trunk. Detected during investigation of test failures of CASSANDRA-18464
"
CASSANDRA-19100,commitlog_test.py::TestCommitLog::test_stop_failure_policy failed,"https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/1121/workflows/fce907b1-0526-4d4d-beb5-b6620737a5f3/jobs/50905/tests

{noformat}
AssertionError: Cannot find the commitlog failure message in logs
assert []
self = <commitlog_test.TestCommitLog object at 0x7f1bf1cb3f50>

    def test_stop_failure_policy(self):
        """"""
        Test the stop commitlog failure policy (default one)
        """"""
        self.prepare()
    
        self._provoke_commitlog_failure()
        failure = self.node1.grep_log(""Failed .+ commit log segments. Commit disk failure policy is stop; terminating thread"")
        logger.debug(failure)
>       assert failure, ""Cannot find the commitlog failure message in logs""
E       AssertionError: Cannot find the commitlog failure message in logs
E       assert []

commitlog_test.py:325: AssertionError
{noformat}
"
CASSANDRA-19099,Test Failure: 5.0 dtest-upgrade failing bc nodetool initiatlizecms,"This commit 
[cassandra-dtest@c0082c9|https://github.com/apache/cassandra-dtest/commit/c0082c9d0b2ded7da93942dfbfc7c87c896d53e0] is not entirely working.  Python upgrade dtests are still failing, see this run on clean  cassandra-5.0

https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/261/workflows/6a15688e-b110-4ede-977c-550b85306867/jobs/21417/tests 


EDIT: also now visible on post-commit ci: https://ci-cassandra.apache.org/job/Cassandra-5.0/119/ "
CASSANDRA-19098,Test Failure: org.apache.cassandra.db.SystemKeyspaceMigrator41Test.testMigrateCompactionHistory,"Seen in j17_utests_compression in CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/259/workflows/4ba92f17-e131-4a5c-8e97-e2f82c7c1a35/jobs/21078/tests

{noformat}
junit.framework.AssertionFailedError: expected:<1> but was:<2>
	at org.apache.cassandra.db.SystemKeyspaceMigrator41Test.testMigrateCompactionHistory(SystemKeyspaceMigrator41Test.java:321)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}"
CASSANDRA-19097,Test Failure: bootstrap_test.TestBootstrap.*,"test_killed_wiped_node_cannot_join
test_read_from_bootstrapped_node
test_shutdown_wiped_node_cannot_join

Seen in dtests_offheap in CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/258/workflows/cea7d697-ca33-40bb-8914-fb9fc662820a/jobs/21162/parallel-runs/38

{noformat}
self = <bootstrap_test.TestBootstrap object at 0x7fc471171d50>

    def test_killed_wiped_node_cannot_join(self):
>       self._wiped_node_cannot_join_test(gently=False)

bootstrap_test.py:608: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <bootstrap_test.TestBootstrap object at 0x7fc471171d50>, gently = False

    def _wiped_node_cannot_join_test(self, gently):
        """"""
        @jira_ticket CASSANDRA-9765
        Test that if we stop a node and wipe its data then the node cannot join
        when it is not a seed. Test both a nice shutdown or a forced shutdown, via
        the gently parameter.
        """"""
        cluster = self.cluster
        cluster.set_environment_variable('CASSANDRA_TOKEN_PREGENERATION_DISABLED', 'True')
        cluster.populate(3)
        cluster.start()
    
        stress_table = 'keyspace1.standard1'
    
        # write some data
        node1 = cluster.nodelist()[0]
        node1.stress(['write', 'n=10K', 'no-warmup', '-rate', 'threads=8'])
    
        session = self.patient_cql_connection(node1)
        original_rows = list(session.execute(""SELECT * FROM {}"".format(stress_table,)))
    
        # Add a new node, bootstrap=True ensures that it is not a seed
        node4 = new_node(cluster, bootstrap=True)
        node4.start(wait_for_binary_proto=True)
    
        session = self.patient_cql_connection(node4)
>       assert original_rows == list(session.execute(""SELECT * FROM {}"".format(stress_table,)))
E       assert [Row(key=b'PP...e9\xbb'), ...] == [Row(key=b'PP...e9\xbb'), ...]
E         At index 587 diff: Row(key=b'OP2656L630', C0=b""E02\xd2\x8clBv\tr\n\xe3\x01\xdd\xf2\x8a\x91\x7f-\x9dm'\xa5\xe7PH\xef\xc1xlO\xab+d"", C1=b""\xb2\xc0j\xff\xcb'\xe3\xcc\x0b\x93?\x18@\xc4\xc7tV\xb7q\xeeF\x82\xa4\xd3\xdcFl\xd9\x87 \x9a\xde\xdc\xa3"", C2=b'\xed\xf8\x8d%\xa4\xa6LPs;\x98f\xdb\xca\x913\xba{M\x8d6XW\x01\xea-\xb5<J\x1eo\xa0F\xbe', C3=b'\x9ec\xcf\xc7\xec\xa5\x85Z]\xa6\x19\xeb\xc4W\x1d%lyZj\xb9\x94I\x90\xebZ\xdba\xdd\xdc\x9e\x82\x95\x1c', C4=b'\xab\x9e\x13\x8b\xc6\x15D\x9b\xccl\xdcX\xb23\xd0\x8b\xa3\xba7\xc1c\xf7F\x1d\xf8e\xbd\x89\xcb\xd8\xd1)f\xdd') != Row(key=b'4LN78NONP0', C0=b""\xdf\x90\xb3/u\xc9/C\xcdOYG3\x070@#\xc3k\xaa$M'\x19\xfb\xab\xc0\x10]\xa6\xac\x1d\x81\xad"", C1=b'\x8a\xb7j\x95\xf9\xbd?&\x11\xaaH\xcd\x87\xaa\xd2\x85\x08X\xea9\x94\xae8U\x92\xad\xb0\x1b9\xff\x87Z\xe81', C2=b'6\x1d\xa1-\xf77\xc7\xde+`\xb7\x89\xaa\xcd\xb5_\xe5\xb3\x04\xc7\xb1\x95e\x81s\t1\x8b\x16sc\x0eMm', C3=b'\xfbi\x08;\xc9\x94\x15}r\xfe\x1b\xae5\xf6v\x83\xae\xff\x82\x9b`J\xc2D\xa6k+\xf3\xd3\xff{C\xd0;', C4=b'\x8f\x87\x18\x0f\xfa\xadK""\x9e\x96\x87:tiu\xa5\x99\xe1_Ax\xa3\x12\xb4Z\xc9v\xa5\xad\xb8{\xc0\xa3\x93')
E         Left contains 2830 more items, first extra item: Row(key=b'5N7N172K30', C0=b'Y\x81\xa6\x02\x89\xa0hyp\x00O\xe9kFp$\x86u\xea\n\x7fK\x99\xe1\xf6G\xf77\xf7\xd7\xe1\xc7L\x...0\x87a\x03\xee', C4=b'\xe8\xd8\x17\xf3\x14\x16Q\x9d\\jb\xde=\x81\xc1B\x9c;T\xb1\xa2O-\x87zF=\x04`\x04\xbd\xc9\x95\xad')
E         Full diff:
E           [
…
{noformat}
"
CASSANDRA-19096,Test Failure: org.apache.cassandra.db.lifecycle.TrackerTest-trie_jdk11,"Seen in j11_utests_trie in CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/259/workflows/f343d3e3-00cf-4e13-bb4d-bbfff1d3658c/jobs/21098/tests

{noformat}
FSWriteError in build/test/cassandra/data/system
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:862)
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:845)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursiveUsingNixCommand(PathUtils.java:384)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:402)
	at org.apache.cassandra.io.util.File.deleteRecursive(File.java:225)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:678)
	at org.apache.cassandra.schema.MockSchema.cleanup(MockSchema.java:377)
	at org.apache.cassandra.db.lifecycle.TrackerTest.setUp(TrackerTest.java:92)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.io.IOException: [rm, -rd, /tmp/cassandra/build/test/cassandra/data/system] returned non-zero exit code: 1
stdout:


stderr:
rm: cannot remove '/tmp/cassandra/build/test/cassandra/data/system': Directory not empty
	at org.apache.cassandra.io.util.PathUtils.deleteRecursiveUsingNixCommand(PathUtils.java:377)
{noformat}"
CASSANDRA-19094,Test Failure: org.apache.cassandra.simulator.test.HarrySimulatorTest.harryTest,"Seen in j11_simulator_dtests in CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/259/workflows/f343d3e3-00cf-4e13-bb4d-bbfff1d3658c/jobs/21101/tests

{noformat}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:537)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098)
	at org.apache.cassandra.simulator.ClusterSimulation.close(ClusterSimulation.java:854)
	at org.apache.cassandra.simulator.test.SimulationTestBase.simulate(SimulationTestBase.java:206)
	at org.apache.cassandra.simulator.test.HarrySimulatorTest.simulate(HarrySimulatorTest.java:361)
	at org.apache.cassandra.simulator.test.HarrySimulatorTest.harryTest(HarrySimulatorTest.java:135)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:254)
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:529)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:246)
	at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$47(Instance.java:978)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.SimulatedExecution$NoIntercept$1Run.run(SimulatedExecution.java:82)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:318)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
	at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$46(Instance.java:972)
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$async$10(IsolatedExecutor.java:156)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Direct buffer memory
	at org.apache.cassandra.utils.concurrent.AbstractFuture.getWhenDone(AbstractFuture.java:239)
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:246)
	at org.apache.cassandra.hints.HintsService.shutdownBlocking(HintsService.java:282)
	at org.apache.cassandra.distributed.impl.Instance.lambda$shutdown$17(Instance.java:901)
	at org.apache.cassandra.distributed.impl.Instance.lambda$parallelRun$52(Instance.java:1196)
Caused by: java.lang.OutOfMemoryError: Direct buffer memory
	at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
	at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
	at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317)
	at org.apache.cassandra.hints.HintsBuffer.create(HintsBuffer.java:77)
	at org.apache.cassandra.hints.HintsBufferPool.createBuffer(HintsBufferPool.java:150)
	at org.apache.cassandra.hints.HintsBufferPool.initializeCurrentBuffer(HintsBufferPool.java:121)
	at org.apache.cassandra.hints.HintsBufferPool.currentBuffer(HintsBufferPool.java:113)
	at org.apache.cassandra.hints.HintsWriteExecutor$FlushBufferPoolTask.run(HintsWriteExecutor.java:168)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.SimulatedExecution$NoIntercept$1Run.run(SimulatedExecution.java:82)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:585)
{noformat}"
CASSANDRA-19093,Test Failure: materialized_views_test.TestMaterializedViews.test_interrupt_build_process,"Seen in j11_dtests in CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/259/workflows/f343d3e3-00cf-4e13-bb4d-bbfff1d3658c/jobs/21100/tests

{noformat}
AssertionError: assert not [('DEBUG [ViewBuildExecutor:2] 2023-11-25 10:20:56,917 ViewBuilderTask.java:128 - Resuming view build for range (-3458...token -5761824694134994220 with 1 covered keys\n', <re.Match object; span=(79, 98), match='Resuming view build'>), ...]
 +  where [('DEBUG [ViewBuildExecutor:2] 2023-11-25 10:20:56,917 ViewBuilderTask.java:128 - Resuming view build for range (-3458...token -5761824694134994220 with 1 covered keys\n', <re.Match object; span=(79, 98), match='Resuming view build'>), ...] = <bound method Node.grep_log of <ccmlib.node.Node object at 0x7f09f960c390>>('Resuming view build', filename='debug.log')
 +    where <bound method Node.grep_log of <ccmlib.node.Node object at 0x7f09f960c390>> = <ccmlib.node.Node object at 0x7f09f960c390>.grep_log
self = <materialized_views_test.TestMaterializedViews object at 0x7f09fa5f0250>

    def test_interrupt_build_process(self):
        """"""Test that an interrupted MV build process is resumed as it should""""""
    
        options = {'hinted_handoff_enabled': False}
        if self.cluster.version() >= '4':
            options['concurrent_materialized_view_builders'] = 4
    
        session = self.prepare(options=options, install_byteman=True)
        node1, node2, node3 = self.cluster.nodelist()
    
        logger.debug(""Avoid premature MV build finalization with byteman"")
        for node in self.cluster.nodelist():
            if self.cluster.version() >= '4':
                node.byteman_submit([mk_bman_path('4.0/skip_view_build_finalization.btm')])
                node.byteman_submit([mk_bman_path('4.0/skip_view_build_task_finalization.btm')])
            else:
                node.byteman_submit([mk_bman_path('pre4.0/skip_finish_view_build_status.btm')])
                node.byteman_submit([mk_bman_path('pre4.0/skip_view_build_update_distributed.btm')])
    
        session.execute(""CREATE TABLE t (id int PRIMARY KEY, v int, v2 text, v3 decimal)"")
    
        logger.debug(""Inserting initial data"")
        for i in range(10000):
            session.execute(
                ""INSERT INTO t (id, v, v2, v3) VALUES ({v}, {v}, 'a', 3.0) IF NOT EXISTS"".format(v=i)
            )
    
        logger.debug(""Create a MV"")
        session.execute((""CREATE MATERIALIZED VIEW t_by_v AS SELECT * FROM t ""
                         ""WHERE v IS NOT NULL AND id IS NOT NULL PRIMARY KEY (v, id)""))
    
        logger.debug(""Wait and ensure the MV build has started. Waiting up to 2 minutes."")
        self._wait_for_view_build_start(session, 'ks', 't_by_v', wait_minutes=2)
    
        logger.debug(""Stop the cluster. Interrupt the MV build process."")
        self.cluster.stop()
    
        logger.debug(""Checking logs to verify that the view build tasks have been created"")
        for node in self.cluster.nodelist():
            assert node.grep_log('Starting new view build', filename='debug.log')
>           assert not node.grep_log('Resuming view build', filename='debug.log')
E           AssertionError: assert not [('DEBUG [ViewBuildExecutor:2] 2023-11-25 10:20:56,917 ViewBuilderTask.java:128 - Resuming view build for range (-3458...token -5761824694134994220 with 1 covered keys\n', <re.Match object; span=(79, 98), match='Resuming view build'>), ...]
E            +  where [('DEBUG [ViewBuildExecutor:2] 2023-11-25 10:20:56,917 ViewBuilderTask.java:128 - Resuming view build for range (-3458...token -5761824694134994220 with 1 covered keys\n', <re.Match object; span=(79, 98), match='Resuming view build'>), ...] = <bound method Node.grep_log of <ccmlib.node.Node object at 0x7f09f960c390>>('Resuming view build', filename='debug.log')
E            +    where <bound method Node.grep_log of <ccmlib.node.Node object at 0x7f09f960c390>> = <ccmlib.node.Node object at 0x7f09f960c390>.grep_log

materialized_views_test.py:1129: AssertionError
{noformat}"
CASSANDRA-19092,Test Failure: cql_tracing_test.TestCqlTracing.test_tracing_simple,"Seen in cqlsh_dtests in CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/257/workflows/ddcb5f4e-e2c5-430b-b922-f7064a244971/jobs/20648/tests

{noformat}failed on teardown with ""Failed: Unexpected error found in node logs (see stdout for full details). Errors: [[node3] 'ERROR [main] 2023-11-24 19:02:04,201 FailureDetector.java:309 - Unknown endpoint: /127.0.0.1:7000
java.lang.IllegalArgumentException: Unknown endpoint: /127.0.0.1:7000
    at org.apache.cassandra.gms.FailureDetector.isAlive(FailureDetector.java:309)
    at org.apache.cassandra.tcm.RemoteProcessor$CandidateIterator.computeNext(RemoteProcessor.java:326)
    at org.apache.cassandra.tcm.RemoteProcessor$CandidateIterator.computeNext(RemoteProcessor.java:256)
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
    at org.apache.cassandra.tcm.RemoteProcessor$1Request.retry(RemoteProcessor.java:187)
    at org.apache.cassandra.tcm.RemoteProcessor.sendWithCallbackAsync(RemoteProcessor.java:223)
    at org.apache.cassandra.tcm.RemoteProcessor.sendWithCallback(RemoteProcessor.java:170)
    at org.apache.cassandra.tcm.RemoteProcessor.commit(RemoteProcessor.java:76)
    at org.apache.cassandra.tcm.ClusterMetadataService$SwitchableProcessor.commit(ClusterMetadataService.java:837)
    at org.apache.cassandra.tcm.Processor.commit(Processor.java:45)
    at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:502)
    at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:467)
    at org.apache.cassandra.tcm.transformations.Register.register(Register.java:112)
    at org.apache.cassandra.tcm.transformations.Register.register(Register.java:95)
    at org.apache.cassandra.tcm.transformations.Register.register(Register.java:132)
    at org.apache.cassandra.tcm.transformations.Register.maybeRegister(Register.java:89)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:807)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:367)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:728)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:879)']""
{noformat}"
CASSANDRA-19091,Test Failure: org.apache.cassandra.db.compaction.writers.CompactionAwareWriterTest.test*CompactionWriter-trie,"Broken on unit_tries, seen in CASSANDRA-19034

- https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/257/workflows/95cdc05c-56fd-43bf-95ac-1122cf01535b/jobs/20685/tests
- https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/257/workflows/ddcb5f4e-e2c5-430b-b922-f7064a244971/jobs/20670/tests

Variants of…
{noformat}
junit.framework.AssertionFailedError: [BtiTableReader:bti(path='/tmp/cassandra/build/test/cassandra/data/cawt_keyspace/cawt_table-1b255f4def2540a60000000000000003/da-8-bti-Data.db'), BtiTableReader:bti(path='/tmp/cassandra/build/test/cassandra/data/cawt_keyspace/cawt_table-1b255f4def2540a60000000000000003/da-7-bti-Data.db')]
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriterTest.populate(CompactionAwareWriterTest.java:276)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriterTest.testSplittingSizeTieredCompactionWriter(CompactionAwareWriterTest.java:139)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}"
CASSANDRA-19090,Test Failure: org.apache.cassandra.distributed.test.topology.DecommissionAvoidWriteTimeoutsTest,"Seen in j17_jvm_dtests  CASSANDRA-19034
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/257/workflows/ddcb5f4e-e2c5-430b-b922-f7064a244971/jobs/20667/tests
{noformat}
java.lang.IllegalStateException: Live member count did not converge across all instances
	at org.apache.cassandra.distributed.impl.AbstractCluster$ChangeMonitor.waitForCompletion(AbstractCluster.java:939)
	at org.apache.cassandra.distributed.impl.AbstractCluster.startup(AbstractCluster.java:1055)
	at org.apache.cassandra.distributed.shared.AbstractBuilder.start(AbstractBuilder.java:166)
	at org.apache.cassandra.distributed.test.topology.DecommissionAvoidTimeouts.test(DecommissionAvoidTimeouts.java:84)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}
"
CASSANDRA-19089,Test Failure: DiskBoundaryManagerTest.updateTokensTest-_jdk17,"Seen in j17_unit_tests from CASSANDRA-18166: https://app.circleci.com/pipelines/github/adelapena/cassandra/3331/workflows/dbbe8281-266d-4bd6-afa3-7fb6c5522087/jobs/93419/tests
"
CASSANDRA-19088,Test Failure: pushed_notifications_test.TestPushedNotifications.test_move_single_node,"In j11_dtests from CASSANDRA-19034
https://app.circleci.com/pipelines/github/mike-tr-adamson/cassandra/402/workflows/92aacb84-fd3a-48e0-9fb2-d1e2fe6fc71a/jobs/35345/tests

{noformat}
AssertionError: assert 'MOVED_NODE' == 'NEW_NODE'
  - NEW_NODE
  + MOVED_NODE
self = <pushed_notifications_test.TestPushedNotifications object at 0x7fd28ee838d0>

    @pytest.mark.no_vnodes
    def test_move_single_node(self):
        """"""
        @jira_ticket CASSANDRA-8516
        Moving a token should result in MOVED_NODE notifications.
        """"""
        self.cluster.populate(3).start()
    
        waiters = [NotificationWaiter(self, node, [""TOPOLOGY_CHANGE""])
                   for node in list(self.cluster.nodes.values())]
    
        # The first node sends NEW_NODE for the other 2 nodes during startup, in case they are
        # late due to network delays let's block a bit longer
        logger.debug(""Waiting for unwanted notifications...."")
        waiters[0].wait_for_notifications(timeout=30, num_notifications=2)
        waiters[0].clear_notifications()
    
        logger.debug(""Issuing move command...."")
        node1 = list(self.cluster.nodes.values())[0]
        node1.move(""123"")
    
        for waiter in waiters:
            logger.debug(""Waiting for notification from {}"".format(waiter.address,))
            notifications = waiter.wait_for_notifications(60.0)
            assert 1 == len(notifications), notifications
            notification = notifications[0]
            change_type = notification[""change_type""]
            address, port = notification[""address""]
>           assert ""MOVED_NODE"" == change_type
E           AssertionError: assert 'MOVED_NODE' == 'NEW_NODE'
E             - NEW_NODE
E             + MOVED_NODE

pushed_notifications_test.py:118: AssertionError
{noformat}
"
CASSANDRA-19087,Test Failure: org.apache.cassandra.distributed.test.log.FetchLogFromPeersTest.testSchema-_jdk11,"In j11_jvm_dtests_vnode, ref CASSANDRA-19034

https://app.circleci.com/pipelines/github/mike-tr-adamson/cassandra/402/workflows/92aacb84-fd3a-48e0-9fb2-d1e2fe6fc71a/jobs/35324/tests

{noformat}
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1124)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1110)
	at org.apache.cassandra.distributed.test.log.FetchLogFromPeersTest.testSchema(FetchLogFromPeersTest.java:75)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	Suppressed: java.lang.IllegalStateException: Could not find range for token 356242581507269238 in PlacementForRange: {}
		at org.apache.cassandra.tcm.ownership.PlacementForRange.forToken(PlacementForRange.java:119)
		at org.apache.cassandra.db.ReadCommandVerbHandler.getLocalReplica(ReadCommandVerbHandler.java:218)
		at org.apache.cassandra.db.ReadCommandVerbHandler.checkTokenOwnership(ReadCommandVerbHandler.java:159)
		at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:57)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:102)
		at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:122)
		at org.apache.cassandra.distributed.impl.Instance.lambda$receiveMessageRunnable$6(Instance.java:538)
		at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
		at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}
"
CASSANDRA-19086,Repeated tests (CircleCI) errors on moving logs,"https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/1114/workflows/a1d02a59-31ee-48e2-9426-a4eca52ed3ff/jobs/50107

{noformat}
BUILD SUCCESSFUL
Total time: 5 seconds
+ dest=/tmp/results/repeated_utests/stdout/passes/01
+ mkdir -p /tmp/results/repeated_utests/stdout/passes/01
+ mv stdout.txt /tmp/results/repeated_utests/stdout/passes/01/org.apache.cassandra.db.commitlog.CommitLogSegmentBackpressureTest.txt
+ source=build/test/output/
+ dest=/tmp/results/repeated_utests/output/passes/01
+ mkdir -p /tmp/results/repeated_utests/output/passes/01
+ [[ -d build/test/output/ ]]
++ ls build/test/output/
+ [[ -n TEST-org.apache.cassandra.db.commitlog.CommitLogSegmentBackpressureTest.xml ]]
+ mv build/test/output//TEST-org.apache.cassandra.db.commitlog.CommitLogSegmentBackpressureTest.xml /tmp/results/repeated_utests/output/passes/01/
+ source=build/test/logs/
+ dest=/tmp/results/repeated_utests/logs/passes/01
+ mkdir -p /tmp/results/repeated_utests/logs/passes/01
+ [[ -d build/test/logs/ ]]
++ ls build/test/logs/
+ [[ -n _jdk11 ]]
+ mv build/test/logs//_jdk11 /tmp/results/repeated_utests/logs/passes/01/
mv: cannot move 'build/test/logs//_jdk11' to '/tmp/results/repeated_utests/logs/passes/01/_jdk11': File exists

Exited with code exit status 1
{noformat}
"
CASSANDRA-19085,In-jvm dtest RepairTest fails with storage_compatibility_mode: NONE,"More precisely, when the {{MessagingService}} version to {{{}VERSION_50{}}}, the test fails with an exception that appears to be a genuine problem:
{code:java}
junit.framework.AssertionFailedError: Exception found expected null, but was:<java.lang.RuntimeException: Did not get replies from all endpoints.
	at org.apache.cassandra.service.ActiveRepairService.lambda$prepareForRepair$2(ActiveRepairService.java:678)
	at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)
>
	at org.apache.cassandra.distributed.test.DistributedRepairUtils.lambda$assertParentRepairSuccess$4(DistributedRepairUtils.java:129)
	at org.apache.cassandra.distributed.test.DistributedRepairUtils.validateExistingParentRepair(DistributedRepairUtils.java:164)
	at org.apache.cassandra.distributed.test.DistributedRepairUtils.assertParentRepairSuccess(DistributedRepairUtils.java:124)
	at org.apache.cassandra.distributed.test.RepairTest.testForcedNormalRepairWithOneNodeDown(RepairTest.java:211)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)


org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1117)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1103)
	at org.apache.cassandra.distributed.test.RepairTest.closeCluster(RepairTest.java:160)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	Suppressed: java.lang.IllegalStateException: complete already: (failure: java.lang.RuntimeException: Did not get replies from all endpoints.)
		at org.apache.cassandra.utils.concurrent.AsyncPromise.setSuccess(AsyncPromise.java:106)
		at org.apache.cassandra.service.ActiveRepairService$2.ack(ActiveRepairService.java:721)
		at org.apache.cassandra.service.ActiveRepairService$2.onResponse(ActiveRepairService.java:697)
		at org.apache.cassandra.repair.messages.RepairMessage$2.onResponse(RepairMessage.java:187)
		at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:58)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)
		at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:64)
		at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:50)
		at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97)
		at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:45)
		at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:430)
		at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
		at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:833){code}
The updates to {{pending}} in ActiveRepairService are not concurrency-safe, but fixing them by doing e.g.
{code:java}
Index: src/java/org/apache/cassandra/service/ActiveRepairService.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/java/org/apache/cassandra/service/ActiveRepairService.java b/src/java/org/apache/cassandra/service/ActiveRepairService.java
--- a/src/java/org/apache/cassandra/service/ActiveRepairService.java    (revision 04552046f74f596e69e2d98c3f3e522fb5888c99)
+++ b/src/java/org/apache/cassandra/service/ActiveRepairService.java    (date 1700839874092)
@@ -675,7 +675,7 @@
             if (promise.isDone())
                 return;
             String errorMsg = ""Did not get replies from all endpoints."";
-            if (promise.tryFailure(new RuntimeException(errorMsg)))
+            if (pending.getAndSet(-1) > 0 && promise.tryFailure(new RuntimeException(errorMsg)))
                 participateFailed(parentRepairSession, errorMsg);
         }, timeoutMillis, MILLISECONDS);
 
@@ -703,8 +703,8 @@
                 failedNodes.add(from.toString());
                 if (failureReason == RequestFailureReason.TIMEOUT)
                 {
-                    pending.set(-1);
-                    promise.setFailure(failRepairException(parentRepairSession, ""Did not get replies from all endpoints.""));
+                    if (pending.getAndSet(-1) > 0)
+                        promise.setFailure(failRepairException(parentRepairSession, ""Did not get replies from all endpoints.""));
                 }
                 else
                 {
 {code}
still results in a test failure:
{code:java}
java.lang.AssertionError: Exception found expected null, but was:<java.lang.RuntimeException: Did not get replies from all endpoints.	at org.apache.cassandra.service.ActiveRepairService.lambda$prepareForRepair$2(ActiveRepairService.java:678)	at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)	at java.base/java.lang.Thread.run(Thread.java:829)>
	at org.junit.Assert.fail(Assert.java:88)	at org.junit.Assert.failNotNull(Assert.java:755)	at org.junit.Assert.assertNull(Assert.java:737)	at org.apache.cassandra.distributed.test.DistributedRepairUtils.lambda$assertParentRepairSuccess$4(DistributedRepairUtils.java:129)	at org.apache.cassandra.distributed.test.DistributedRepairUtils.validateExistingParentRepair(DistributedRepairUtils.java:164)	at org.apache.cassandra.distributed.test.DistributedRepairUtils.assertParentRepairSuccess(DistributedRepairUtils.java:124)	at org.apache.cassandra.distributed.test.RepairTest.testForcedNormalRepairWithOneNodeDown(RepairTest.java:211)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.base/java.lang.reflect.Method.invoke(Method.java:566)	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)	at com.intellij.rt.junit.IdeaTestRunner$Repeater$1.execute(IdeaTestRunner.java:38)	at com.intellij.rt.execution.junit.TestsRepeater.repeat(TestsRepeater.java:11)	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:35)	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:232)	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:55)
 {code}"
CASSANDRA-19084,Test Failure: IndexStreamingFailureTest.testAvailabilityAfterFailed*EntireFileStreaming,"Flakies 

https://app.circleci.com/pipelines/github/adelapena/cassandra/3329/workflows/f2124edd-fa0e-4bc5-ab03-ddfb886bf015/jobs/93097/tests

{noformat}
java.lang.NullPointerException
	at java.base/sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:133)
	at java.base/sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:155)
	at java.base/java.net.URL.openStream(URL.java:1165)
	at java.base/java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1739)
	at net.bytebuddy.dynamic.ClassFileLocator$ForClassLoader.locate(ClassFileLocator.java:453)
	at net.bytebuddy.dynamic.ClassFileLocator$ForClassLoader.locate(ClassFileLocator.java:434)
	at net.bytebuddy.dynamic.scaffold.TypeWriter$Default$ForInlining.create(TypeWriter.java:4009)
	at net.bytebuddy.dynamic.scaffold.TypeWriter$Default.make(TypeWriter.java:2224)
	at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase$UsingTypeWriter.make(DynamicType.java:4050)
	at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase.make(DynamicType.java:3734)
	at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase$Delegator.make(DynamicType.java:3986)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest$ByteBuddyHelper.installErrors(IndexStreamingFailureTest.java:154)
	at org.apache.cassandra.distributed.shared.AbstractBuilder$1.initialise(AbstractBuilder.java:360)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.newInstance(AbstractCluster.java:312)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegateForStartup(AbstractCluster.java:292)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:410)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:383)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest.testAvailabilityAfterStreaming(IndexStreamingFailureTest.java:123)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest.testAvailabilityAfterFailedNonEntireFileStreaming(IndexStreamingFailureTest.java:79)
{noformat}	


{noformat}
java.lang.IllegalStateException: Can't use shutdown instances, delegate is null
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegate(AbstractCluster.java:285)
	at org.apache.cassandra.distributed.impl.DelegatingInvokableInstance.transfer(DelegatingInvokableInstance.java:49)
	at org.apache.cassandra.distributed.api.IInvokableInstance.runsOnInstance(IInvokableInstance.java:45)
	at org.apache.cassandra.distributed.api.IInvokableInstance.runOnInstance(IInvokableInstance.java:46)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest.testAvailabilityAfterFailedEntireFileStreaming(IndexStreamingFailureTest.java:85)

{noformat}

https://ci-cassandra.apache.org/job/Cassandra-5.0/106/testReport/org.apache.cassandra.distributed.test.sai/IndexStreamingFailureTest/testAvailabilityAfterFailedNonEntireFileStreaming__jdk11_x86_64_novnode/

{noformat}
java.lang.RuntimeException: The class file could not be written
	at net.bytebuddy.dynamic.scaffold.TypeWriter$Default$ForInlining.create(TypeWriter.java:4021)
	at net.bytebuddy.dynamic.scaffold.TypeWriter$Default.make(TypeWriter.java:2224)
	at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase$UsingTypeWriter.make(DynamicType.java:4050)
	at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase.make(DynamicType.java:3734)
	at net.bytebuddy.dynamic.DynamicType$Builder$AbstractBase$Delegator.make(DynamicType.java:3986)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest$ByteBuddyHelper.installErrors(IndexStreamingFailureTest.java:155)
	at org.apache.cassandra.distributed.shared.AbstractBuilder$1.initialise(AbstractBuilder.java:360)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.newInstance(AbstractCluster.java:312)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegateForStartup(AbstractCluster.java:292)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:410)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:383)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest.testAvailabilityAfterStreaming(IndexStreamingFailureTest.java:124)
	at org.apache.cassandra.distributed.test.sai.IndexStreamingFailureTest.testAvailabilityAfterFailedNonEntireFileStreaming(IndexStreamingFailureTest.java:80)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.io.IOException: Stream closed
	at java.base/java.util.zip.InflaterInputStream.ensureOpen(InflaterInputStream.java:68)
	at java.base/java.util.zip.InflaterInputStream.read(InflaterInputStream.java:143)
	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
	at net.bytebuddy.utility.StreamDrainer.drain(StreamDrainer.java:85)
	at net.bytebuddy.dynamic.ClassFileLocator$ForClassLoader.locate(ClassFileLocator.java:456)
	at net.bytebuddy.dynamic.ClassFileLocator$ForClassLoader.locate(ClassFileLocator.java:434)
	at net.bytebuddy.dynamic.scaffold.TypeWriter$Default$ForInlining.create(TypeWriter.java:4009)
{noformat}"
CASSANDRA-19083,Remove dependency on bundled Harry jar,"For expediency, we temporarily added a snapshot jar to the source tree, {{lib/harry-core-0.0.2-CASSANDRA-18768.jar}}. We should remove this as soon as the next Harry release is published."
CASSANDRA-19082,Histogram overflow causes client timeouts and message drops,"Hi,

We have recently noticed that sometimes this exception happens on our Cassandra cluster: 
{code:java}
ERROR [ScheduledTasks:1] 2023-11-24 06:24:12,680 CassandraDaemon.java:244 - Exception in thread Thread[ScheduledTasks:1,5,main]
java.lang.IllegalStateException: Unable to compute when histogram overflowed
        at org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir$EstimatedHistogramReservoirSnapshot.getMean(DecayingEstimatedHistogramReservoir.java:472)
        at org.apache.cassandra.net.MessagingService.getDroppedMessagesLogs(MessagingService.java:1272)
        at org.apache.cassandra.net.MessagingService.logDroppedMessages(MessagingService.java:1244)
        at org.apache.cassandra.net.MessagingService.access$200(MessagingService.java:84)
        at org.apache.cassandra.net.MessagingService$4.run(MessagingService.java:512)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84)
        at java.lang.Thread.run(Thread.java:750)
 {code}
It happens on all 6 nodes at the same time. Also we see increased client timeouts and dropped READ and READ_RESPONSE messages. Our Cassandra is 3.11.16, 2 DC setup, 6 node in each DC. RF is 3. I have searched issues but could not find exactly same issue causing messages to be dropped. Any suggestion would be appreciated. "
CASSANDRA-19081,Remove harry-example.conf from conf/,Configure harry programmatically instead
CASSANDRA-19080,User documentation for CEP-21,"The initial implementation contains some minimal docs in-tree (TransactionalClusterMetadata.md & TCM_implementation.md).

The CEP doc can be found at [https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-21%3A+Transactional+Cluster+Metadata]"
CASSANDRA-19079,Test failure: org.apache.cassandra.distributed.test.log.FetchLogFromPeersTest.catchupCoordinatorAheadPlacementsReadTest-_jdk17,"Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests

```
 junit.framework.AssertionFailedError
 at org.apache.cassandra.distributed.test.log.FetchLogFromPeersTest.catchupCoordinatorAheadPlacementsReadTest(FetchLogFromPeersTest.java:217)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
```"
CASSANDRA-19078,Test failure: org.apache.cassandra.distributed.test.ring.DecommissionTest.test*Version*,"Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests
Also: Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20502/tests
Also: CircleCI Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20503/tests


```
 org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
 at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1124)
 at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1110)
 at org.apache.cassandra.distributed.test.ring.DecommissionTest.testMixedVersionBlockDecom(DecommissionTest.java:171)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 Suppressed: java.util.concurrent.RejectedExecutionException: GossipStage has shut down
 at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49)
 at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238)
 at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)
 at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)
 at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50)
 at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57)
 at org.apache.cassandra.concurrent.Stage.execute(Stage.java:127)
 at org.apache.cassandra.gms.Gossiper.runInGossipStageBlocking(Gossiper.java:467)
 at org.apache.cassandra.tcm.compatibility.GossipHelper.removeFromGossip(GossipHelper.java:100)
 at org.apache.cassandra.tcm.listeners.LegacyStateListener.notifyPostCommit(LegacyStateListener.java:122)
 at org.apache.cassandra.tcm.log.LocalLog.lambda$notifyPostCommit$3(LocalLog.java:561)
 at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
 at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
 at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
 at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.base/java.lang.Thread.run(Thread.java:833)
 Suppressed: java.util.concurrent.RejectedExecutionException: GossipStage has shut down
 at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49)
 at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238)
 at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)
 at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)
 at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50)
 at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57)
 at org.apache.cassandra.concurrent.Stage.execute(Stage.java:127)
 at org.apache.cassandra.gms.Gossiper.runInGossipStageBlocking(Gossiper.java:467)
 at org.apache.cassandra.tcm.compatibility.GossipHelper.removeFromGossip(GossipHelper.java:100)
 at org.apache.cassandra.tcm.listeners.LegacyStateListener.notifyPostCommit(LegacyStateListener.java:122)
 at org.apache.cassandra.tcm.log.LocalLog.lambda$notifyPostCommit$3(LocalLog.java:561)
 at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
 at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
 at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
 at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
 at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
 at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.base/java.lang.Thread.run(Thread.java:833)
```"
CASSANDRA-19077,Test failure: org.apache.cassandra.distributed.test.log.RegisterTest.serializationVersionDisagreementTest-_jdk17,"Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests

```
 java.lang.RuntimeException: java.lang.NoSuchFieldException: modifiers
 at org.apache.cassandra.distributed.test.log.RegisterTest.lambda$serializationVersionDisagreementTest$81c80a4a$2(RegisterTest.java:132)
 at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
 at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
 at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.base/java.lang.Thread.run(Thread.java:833)
 Caused by: java.lang.NoSuchFieldException: modifiers
 at java.base/java.lang.Class.getDeclaredField(Class.java:2610)
 at org.apache.cassandra.distributed.test.log.RegisterTest.lambda$serializationVersionDisagreementTest$81c80a4a$2(RegisterTest.java:120)
```
"
CASSANDRA-19076,Test failure: org.apache.cassandra.distributed.test.log.InProgressSequenceCoordinationTest:rejectSubsequentInProgressSequence,"Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests

Also fais on jdk11: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20464/tests
{code}
 junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base/java.util.Vector.forEach(Vector.java:1365)
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base/java.util.Vector.forEach(Vector.java:1365)
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base/java.util.Vector.forEach(Vector.java:1365)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at org.apache.cassandra.anttasks.TestHelper.execute(TestHelper.java:53)
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.base/java.util.Vector.forEach(Vector.java:1365)
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
 at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}"
CASSANDRA-19075,Test failure: org.apache.cassandra.distributed.test.ClearSnapshotTest.clearSnapshotSlowTest-_jdk17,"Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests

Also on jdk11: CircleCI failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20464/tests

 ```
 java.lang.RuntimeException: java.util.concurrent.TimeoutException
  at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
  at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:537)
  at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098)
  at org.apache.cassandra.distributed.test.ClearSnapshotTest.clearSnapshotSlowTest(ClearSnapshotTest.java:124)
  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  Caused by: java.util.concurrent.TimeoutException
  at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:253)
  at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:529)
  Suppressed: java.util.concurrent.TimeoutException
```
"
CASSANDRA-19074,Test Failure: org.apache.cassandra.distributed.test.ring.DecommissionTest#testMixedVersionBlockDecom-_jdk17,"Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20502/tests


```
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1124)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1110)
	at org.apache.cassandra.distributed.test.ring.DecommissionTest.testMixedVersionBlockDecom(DecommissionTest.java:171)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	Suppressed: java.util.concurrent.RejectedExecutionException: GossipStage has shut down
		at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238)
		at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)
		at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57)
		at org.apache.cassandra.concurrent.Stage.execute(Stage.java:127)
		at org.apache.cassandra.gms.Gossiper.runInGossipStageBlocking(Gossiper.java:467)
		at org.apache.cassandra.tcm.compatibility.GossipHelper.removeFromGossip(GossipHelper.java:100)
		at org.apache.cassandra.tcm.listeners.LegacyStateListener.notifyPostCommit(LegacyStateListener.java:122)
		at org.apache.cassandra.tcm.log.LocalLog.lambda$notifyPostCommit$3(LocalLog.java:561)
		at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
		at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
		at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
		at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
		at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:833)
```
"
CASSANDRA-19073,Test failure: org.apache.cassandra.distributed.test.log.RegisterTest#serializationVersionDisagreementTest-_jdk17,"Circleci Failure: [https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20502/tests]

{code}
org.apache.cassandra.distributed.test.log.RegisterTest.lambda$serializationVersionDisagreementTest$81c80a4a$2(RegisterTest.java:132)
at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:833)
Caused by: java.lang.NoSuchFieldException: modifiers
at java.base/java.lang.Class.getDeclaredField(Class.java:2610)
at org.apache.cassandra.distributed.test.log.RegisterTest.lambda$serializationVersionDisagreementTest$81c80a4a$2(RegisterTest.java:120)
{code}"
CASSANDRA-19072,Test failure: org.apache.cassandra.distributed.test.log.FetchLogFromPeersTest.catchupCoordinatorAheadPlacementsReadTest-_jdk11,"CircleCI failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20464/tests

Also failing on 17: Circleci Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests

{code}
junit.framework.AssertionFailedError
	at org.apache.cassandra.distributed.test.log.FetchLogFromPeersTest.catchupCoordinatorAheadPlacementsReadTest(FetchLogFromPeersTest.java:217)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
"
CASSANDRA-19071,Test failure: org.apache.cassandra.distributed.test.log.InProgressSequenceCoordinationTest.rejectSubsequentInProgressSequence,"CircleCI failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20464/tests

```
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.apache.cassandra.anttasks.TestHelper.execute(TestHelper.java:53)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
```
"
CASSANDRA-19070,Test failure: org.apache.cassandra.distributed.test.ClearSnapshotTest#clearSnapshotSlowTest-_jdk11,"CircleCI failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20464/tests

Also on jdk17: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20500/tests

```
java.lang.RuntimeException: java.util.concurrent.TimeoutException
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:79)
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:537)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1098)
	at org.apache.cassandra.distributed.test.ClearSnapshotTest.clearSnapshotSlowTest(ClearSnapshotTest.java:124)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.util.concurrent.TimeoutException
	at org.apache.cassandra.utils.concurrent.AbstractFuture.get(AbstractFuture.java:253)
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:529)
	Suppressed: java.util.concurrent.TimeoutException
```
"
CASSANDRA-19069,Test failure: j11_jvm_dtests org.apache.cassandra.distributed.test.ring.DecommissionTest#testMixedVersionBlockDecom-_jdk11,"CircleCI Failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20503/tests

```org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1124)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1110)
	at org.apache.cassandra.distributed.test.ring.DecommissionTest.testMixedVersionBlockDecom(DecommissionTest.java:171)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	Suppressed: java.util.concurrent.RejectedExecutionException: GossipStage has shut down
		at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238)
		at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)
		at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57)
		at org.apache.cassandra.concurrent.Stage.execute(Stage.java:127)
		at org.apache.cassandra.gms.Gossiper.runInGossipStageBlocking(Gossiper.java:467)
		at org.apache.cassandra.tcm.compatibility.GossipHelper.removeFromGossip(GossipHelper.java:100)
		at org.apache.cassandra.tcm.listeners.LegacyStateListener.notifyPostCommit(LegacyStateListener.java:122)
		at org.apache.cassandra.tcm.log.LocalLog.lambda$notifyPostCommit$3(LocalLog.java:561)
		at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
		at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
		at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
		at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
		at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:829)
	Suppressed: java.util.concurrent.RejectedExecutionException: GossipStage has shut down
		at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238)
		at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)
		at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50)
		at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57)
		at org.apache.cassandra.concurrent.Stage.execute(Stage.java:127)
		at org.apache.cassandra.gms.Gossiper.runInGossipStageBlocking(Gossiper.java:467)
		at org.apache.cassandra.tcm.compatibility.GossipHelper.removeFromGossip(GossipHelper.java:100)
		at org.apache.cassandra.tcm.listeners.LegacyStateListener.notifyPostCommit(LegacyStateListener.java:122)
		at org.apache.cassandra.tcm.log.LocalLog.lambda$notifyPostCommit$3(LocalLog.java:561)
		at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
		at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
		at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
		at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
		at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
		at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
		at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
		at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:829)
```
"
CASSANDRA-19068,Test failure: j11_dtests_large.pending_range_test.TestPendingRangeMovements,"CircleCI failure: [https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20466/tests]

 {code}
ccmlib.node.TimeoutError: 23 Nov 2023 21:15:48 [node1] after 10.01/10 seconds Missing: ['Moving .* to -634023222112864484'] not found in system.log:
Head: INFO [RMI TCP Connection(2)-127.0.0.1] 2023-11-23
Tail: ...1-23 21:15:39,483 BigFormat.java:324 - Deleting sstable: /tmp/dtest-ovho0emz/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/oa-11-big
self = <pending_range_test.TestPendingRangeMovements object at 0x7f0f2c335d50>

@pytest.mark.resource_intensive
def test_pending_range(self):
""""""
@jira_ticket CASSANDRA-10887
""""""
cluster = self.cluster
 # If we are on 2.1, we need to set the log level to debug or higher, as debug.log does not exist.
if cluster.version() < '2.2':
cluster.set_log_level('DEBUG')

 # Create 5 node cluster
ring_delay_ms = 3_600_000 # 1 hour
cluster.populate(5).start(jvm_args=['-Dcassandra.ring_delay_ms={}'.format(ring_delay_ms)])
node1, node2 = cluster.nodelist()[0:2]

 # Set up RF=3 keyspace
session = self.patient_cql_connection(node1)
create_ks(session, 'ks', 3)

session.execute(""CREATE TABLE users (login text PRIMARY KEY, email text, name text, login_count int)"")
 # We use the partition key 'jdoe3' because it belongs to node1.
 # The key MUST belong to node1 to repro the bug.
session.execute(""INSERT INTO users (login, email, name, login_count) VALUES ('jdoe3', 'jdoe@abc.com', 'Jane Doe', 1) IF NOT EXISTS;"")

lwt_query = SimpleStatement(""UPDATE users SET email = 'janedoe@abc.com' WHERE login = 'jdoe3' IF email = 'jdoe@abc.com'"")
 # Show we can execute LWT no problem
for i in range(1000):
session.execute(lwt_query)

token = '-634023222112864484'

mark = node1.mark_log()
 # Move a node without waiting for the response of nodetool, so we don't have to wait for ring_delay
threading.Thread(target=(lambda: node1.nodetool('move {}'.format(token)))).start()

 # Watch the log so we know when the node is moving
> node1.watch_log_for('Moving .* to {}'.format(token), timeout=10, from_mark=mark)

pending_range_test.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../env3.7/lib/python3.7/site-packages/ccmlib/node.py:605: in watch_log_for
head=reads[:50], tail=""...""+reads[len(reads)-150:]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

start = 1700774138.162831, timeout = 10
msg = ""Missing: ['Moving .* to -634023222112864484'] not found in system.log:\n Head: INFO [RMI TCP Connection(2)-127.0.0.1...324 - Deleting sstable: /tmp/dtest-ovho0emz/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/oa-11-big\n""
node = 'node1'

@staticmethod
def raise_if_passed(start, timeout, msg, node=None):
if start + timeout < time.time():
> raise TimeoutError.create(start, timeout, msg, node)
E ccmlib.node.TimeoutError: 23 Nov 2023 21:15:48 [node1] after 10.01/10 seconds Missing: ['Moving .* to -634023222112864484'] not found in system.log:
E Head: INFO [RMI TCP Connection(2)-127.0.0.1] 2023-11-23
E Tail: ...1-23 21:15:39,483 BigFormat.java:324 - Deleting sstable: /tmp/dtest-ovho0emz/test/node1/data0/system/local-7ad54392bcdd35a684174e047860b377/oa-11-big

../env3.7/lib/python3.7/site-packages/ccmlib/node.py:56: TimeoutError
{code}"
CASSANDRA-19067,Test failure: j11_dtests_large.replace_address_test.TestReplaceAddress,"CircleCI failure: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20466/tests


{code}
assert 0 == 1
 +  where 0 = len(set())
self = <replace_address_test.TestReplaceAddress object at 0x7f390285a8d0>

    @pytest.mark.resource_intensive
    def test_replace_first_boot(self):
>       self._test_replace_node(jvm_option='replace_address_first_boot')

replace_address_test.py:281: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
replace_address_test.py:300: in _test_replace_node
    previous_log_size = self._verify_tokens_migrated_successfully()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <replace_address_test.TestReplaceAddress object at 0x7f390285a8d0>
previous_log_size = None

    def _verify_tokens_migrated_successfully(self, previous_log_size=None):
        if not self.dtest_config.use_vnodes:
            num_tokens = 1
        else:
            # a little hacky but grep_log returns the whole line...
            num_tokens = int(self.replacement_node.get_conf_option('num_tokens'))
    
        logger.debug(""Verifying {} tokens migrated successfully"".format(num_tokens))
        replmnt_address = self.replacement_node.address_for_current_version_slashy()
        repled_address = self.replaced_node.address_for_current_version_slashy()
        token_ownership_log = r""Token (.*?) changing ownership from {} to {}"".format(repled_address,
                                                                                     replmnt_address)
        logs = self.replacement_node.grep_log(token_ownership_log)
    
        if (previous_log_size is not None):
            assert len(logs) == previous_log_size
    
        moved_tokens = set([l[1].group(1) for l in logs])
        logger.debug(""number of moved tokens: {}"".format(len(moved_tokens)))
>       assert len(moved_tokens) == num_tokens
E       assert 0 == 1
E        +  where 0 = len(set())

replace_address_test.py:207: AssertionError
{code}"
CASSANDRA-19066,Test Failure: org.apache.cassandra.distributed.upgrade.MixedModeFrom3LoggedBatchTest.testSimpleStrategy-_jdk11,"Failed in Circle:

[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20534/tests]"
CASSANDRA-19065,Test Failure: org.apache.cassandra.distributed.upgrade.MixedModeTTLOverflowUpgradeTest.testTTLOverflowDuringUpgrade-_jdk11,"Failed in Circle:

[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20534/tests]

{noformat}
junit.framework.AssertionFailedError: Error in test '4.1.4 -> [5.1]' while upgrading to '5.1'; successful upgrades [4.0.12 -> [5.1]]
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:397)
	at org.apache.cassandra.distributed.upgrade.MixedModeTTLOverflowUpgradeTest.testTTLOverflow(MixedModeTTLOverflowUpgradeTest.java:134)
	at org.apache.cassandra.distributed.upgrade.MixedModeTTLOverflowUpgradeTest.testTTLOverflowDuringUpgrade(MixedModeTTLOverflowUpgradeTest.java:49)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.utils.Throwables.unchecked(Throwables.java:308)
	at org.apache.cassandra.utils.Throwables.cleaned(Throwables.java:327)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:550)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.handleMutation(CommitLogReplayer.java:521)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readMutation(CommitLogReader.java:478)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readSection(CommitLogReader.java:397)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:244)
	at org.apache.cassandra.db.commitlog.CommitLogReader.readCommitLogSegment(CommitLogReader.java:147)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayFiles(CommitLogReplayer.java:195)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:223)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:204)
	at org.apache.cassandra.tcm.Startup.initializeFromGossip(Startup.java:259)
	at org.apache.cassandra.tcm.Startup.initialize(Startup.java:115)
	at org.apache.cassandra.distributed.impl.Instance.partialStartup(Instance.java:711)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$7(Instance.java:615)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)
	at java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)
	at org.apache.cassandra.utils.memory.SlabAllocator.getRegion(SlabAllocator.java:139)
	at org.apache.cassandra.utils.memory.SlabAllocator.allocate(SlabAllocator.java:104)
	at org.apache.cassandra.utils.memory.MemtableBufferAllocator$1.allocate(MemtableBufferAllocator.java:40)
	at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:77)
	at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:63)
	at org.apache.cassandra.utils.memory.ByteBufferCloner.clone(ByteBufferCloner.java:46)
	at org.apache.cassandra.db.memtable.SkipListMemtable.put(SkipListMemtable.java:115)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1451)
	at org.apache.cassandra.db.CassandraTableWriteHandler.write(CassandraTableWriteHandler.java:38)
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:626)
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:483)
	at org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator$1.runMayThrow(CommitLogReplayer.java:316)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:26)
	at org.apache.cassandra.concurrent.FutureTask$3.call(FutureTask.java:130)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
{noformat}"
CASSANDRA-19064,Test Failure: org.apache.cassandra.distributed.upgrade.ClusterMetadataUpgradeTest.simpleUpgradeTest-_jdk11,"Failed in Circle with OOM

[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20534/tests]"
CASSANDRA-19063,Test Failure: org.apache.cassandra.distributed.upgrade.ClusterMetadataUpgradeTest.upgradeWithHintsTest-_jdk11,"Fails in circle with OOM

[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20534/tests]"
CASSANDRA-19062,Test failure: org.apache.cassandra.db.lifecycle.ViewTest,"Fails on both j17_unit_tests and j17_utests_cdc, same error as CASSANDRA-19059
{code}
FSWriteError in build/test/cassandra/data/system
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:862)
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:845)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursiveUsingNixCommand(PathUtils.java:384)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:402)
	at org.apache.cassandra.io.util.File.deleteRecursive(File.java:225)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:678)
	at org.apache.cassandra.schema.MockSchema.cleanup(MockSchema.java:377)
	at org.apache.cassandra.db.lifecycle.ViewTest.setUp(ViewTest.java:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.io.IOException: [rm, -rd, /tmp/cassandra/build/test/cassandra/data/system] returned non-zero exit code: 1
stdout:


stderr:
rm: cannot remove '/tmp/cassandra/build/test/cassandra/data/system/IndexInfo-9f5c6374d48532299a0a5094af9ad1e3': Directory not empty
	at org.apache.cassandra.io.util.PathUtils.deleteRecursiveUsingNixCommand(PathUtils.java:377)
{code}

https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20505/tests"
CASSANDRA-19061,Test failure: org.apache.cassandra.db.CorruptPrimaryIndexTest.bigPrimaryIndexDoesNotDetectDiskCorruption,"Timeout, fails on both j17_utests_trie and j11_utests

https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20465/tests"
CASSANDRA-19060,Test failure: org.apache.cassandra.tools.JMXCompatabilityTest,"diff30, diff311, diff40 and diff41 failed on both j11_utests_trie and j17_utests_trie. This does not repeat locally
{code}
junit.framework.AssertionFailedError: 
Expecting empty but was: ""Objects not in right:
org.apache.cassandra.db:type=Caches
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=Capacity
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=Entries
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=FifteenMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=FiveMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=HitRate
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=Hits
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=OneMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=Requests
org.apache.cassandra.metrics:type=Cache,scope=CounterCache,name=Size
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=Capacity
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=Entries
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=FifteenMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=FiveMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=HitRate
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=Hits
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=OneMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=Requests
org.apache.cassandra.metrics:type=Cache,scope=KeyCache,name=Size
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=Capacity
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=Entries
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=FifteenMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=FiveMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=HitRate
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=Hits
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=OneMinuteHitRate
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=Requests
org.apache.cassandra.metrics:type=Cache,scope=RowCache,name=Size
""
	at org.apache.cassandra.tools.JMXCompatabilityTest.diff(JMXCompatabilityTest.java:273)
	at org.apache.cassandra.tools.JMXCompatabilityTest.diff30(JMXCompatabilityTest.java:139)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)

{code}
https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20465/tests"
CASSANDRA-19059,Test failure: org.apache.cassandra.db.RepairedDataInfoTest,"{code}
FSWriteError in build/test/cassandra/data/system
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:862)
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:845)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursiveUsingNixCommand(PathUtils.java:384)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:402)
	at org.apache.cassandra.io.util.File.deleteRecursive(File.java:225)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:678)
	at org.apache.cassandra.schema.MockSchema.cleanup(MockSchema.java:377)
	at org.apache.cassandra.db.RepairedDataInfoTest.setUp(RepairedDataInfoTest.java:72)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.io.IOException: [rm, -rd, /tmp/cassandra/build/test/cassandra/data/system] returned non-zero exit code: 1
stdout:
{code}

https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20450/tests"
CASSANDRA-19058,Test Failure: org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest-_jdk11,"butler shows this as failing on J17 but here we see it fail on J11 

[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20463/tests]"
CASSANDRA-19057,Test Failure: pending_range_test.TestPendingRangeMovements.test_pending_range,"Failed on circle in [https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20474/tests]

 

(Circle CI marks this as flaky but nothing in butler)"
CASSANDRA-19056,Test failure: materialized_views_test.TestMaterializedViewsConsistency.test_multi_partition_consistent_reads_after_write,"Fails or is flaky on both JDK 11 and 17 


[https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/256/workflows/c4fda8f1-a8d6-4523-be83-5e30b9de39fe/jobs/20462/parallel-runs/14]
 
{noformat}

[node3] 'ERROR [MutationStage-1] 2023-11-23 21:18:31,953 JVMStabilityInspector.java:70 - Exception in thread Thread[MutationStage-1,10,SharedPool]
java.lang.NullPointerException: Cannot invoke ""org.apache.cassandra.schema.TableMetadata.partitionKeyColumns()"" because ""this.viewMetadata"" is null
    at org.apache.cassandra.db.view.ViewUpdateGenerator.<init>(ViewUpdateGenerator.java:99)
    at org.apache.cassandra.db.view.TableViews.generateViewUpdates(TableViews.java:227)
    at org.apache.cassandra.db.view.TableViews.pushViewReplicaUpdates(TableViews.java:193)
    at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:615)
    at org.apache.cassandra.db.Keyspace.applyFuture(Keyspace.java:447)
    at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:239)
    at org.apache.cassandra.db.MutationVerbHandler.applyMutation(MutationVerbHandler.java:64)
    at org.apache.cassandra.db.AbstractMutationVerbHandler.processMessage(AbstractMutationVerbHandler.java:60)
    at org.apache.cassandra.db.MutationVerbHandler.doVerb(MutationVerbHandler.java:54)
    at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:102)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:122)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:51)
    at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:432)
    at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:833)', [node3] 'ERROR [MutationStage-2] 2023-11-23 21:18:31,953 JVMStabilityInspector.java:70 - Exception in thread Thread[MutationStage-2,5,SharedPool]
java.lang.NullPointerException: Cannot invoke ""org.apache.cassandra.schema.TableMetadata.partitionKeyColumns()"" because ""this.viewMetadata"" is null
    at org.apache.cassandra.db.view.ViewUpdateGenerator.<init>(ViewUpdateGenerator.java:99)
    at org.apache.cassandra.db.view.TableViews.generateViewUpdates(TableViews.java:227)
    at org.apache.cassandra.db.view.TableViews.pushViewReplicaUpdates(TableViews.java:193)
    at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:615)
    at org.apache.cassandra.db.Keyspace.applyFuture(Keyspace.java:447)
    at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:239)
    at org.apache.cassandra.db.MutationVerbHandler.applyMutation(MutationVerbHandler.java:64)
    at org.apache.cassandra.db.AbstractMutationVerbHandler.processMessage(AbstractMutationVerbHandler.java:60)
    at org.apache.cassandra.db.MutationVerbHandler.doVerb(MutationVerbHandler.java:54)
    at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:102)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:122)
    at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:51)
    at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:432)
    at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:833)']
{noformat}"
CASSANDRA-19054,WEBSITE - Add Cassandra Catalyst page and blog,"This ticket is to capture the work associated with creating a new page for the Cassandra Catalyst program and it's associated blog.

 

Preferably, the blog and page are live as of *November 27.* Please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-19053,Refactor LocalLog to make intialisation more straightforward and consistent,"It should be possible to rationalise the multiple ways we currently have for obtaining an instance of {{LocalLog}}. We also want to ensure that the necessary steps for initialising the log, which may vary depending on context (i.e. starting up a node normally, starting up immediately after upgrade, forcing recovery from a metadata export, running unit & dtests), are performed exactly once and in the correct sequence. 
"
CASSANDRA-19050,Enhanced usage of test method names in CQLTester for better test debugging,"All views, tables, ks,... created in CQLTester are of the form {{table_00}}:
- While debugging tests and flakies this makes it really hard to match logs to test methods.
- Some async operations can spill log lines from a previous test method into the next's log lines which is incredibly confusing
- It's hard on the eyes and easy to mix up
- When comparing logs from 2 different branches, envs,... it's really hard to match logs

The proposed solution is for {{CQLTester}} to decorate these with the test name."
CASSANDRA-19048,Audit table properties passed through Analytics CqlUtils,"In mixed version clusters, table properties (`WITH` clauses in the table definition) have historically been moved or renamed without much thought to backward-compatibility of external tools. This has caused breaking changes to the Analytics library in the past. Therefore, the Analytics library removes most table properties from the table schema it uses in order to avoid these issues. However, it is currently too aggressive in this removal process.  For example, the `default_time_to_live` parameter is currently removed from the table options, but the Bulk Writer should use this to set the appropriate TTL when the user doesn’t specify it in the bulk job.

Similarly, compression options passed to the Bulk Writer should be honored so that the appropriate compression algorithm is used when generating sstables.

The allowlist of options needs to be audited and improved to add additional options where appropriate."
CASSANDRA-19045,Various Accord protocol fixes and improvements to validation,"Improve validation, and address various faults discovered by the improved validation."
CASSANDRA-19042,Repair fuzz tests fail with paxos_variant: v2,"Adding {{paxos_variant: v2}} to the test yaml causes all fuzz repair tests to fail with
{code}
java.lang.NullPointerException: null
	at org.apache.cassandra.gms.EndpointStateSerializer.serializedSize(EndpointState.java:337)
	at org.apache.cassandra.gms.EndpointStateSerializer.serializedSize(EndpointState.java:300)
	at org.apache.cassandra.service.paxos.cleanup.PaxosStartPrepareCleanup$RequestSerializer.serializedSize(PaxosStartPrepareCleanup.java:176)
	at org.apache.cassandra.service.paxos.cleanup.PaxosStartPrepareCleanup$RequestSerializer.serializedSize(PaxosStartPrepareCleanup.java:147)
	at org.apache.cassandra.net.Message$Serializer.payloadSize(Message.java:1067)
	at org.apache.cassandra.net.Message.payloadSize(Message.java:1114)
	at org.apache.cassandra.net.Message$Serializer.serializedSize(Message.java:750)
	at org.apache.cassandra.net.Message.serializedSize(Message.java:1094)
...
{code}

This happens for all three options of {{paxos_state_purging}} and both with and without {{storage_compatibility_mode: NONE}}.

Tests still fail if {{PaxosStartPrepareCleanup}} is changed to use {{EndpointState.nullableSerializer}}."
CASSANDRA-19040,Consider CONTAINS KEY in SAI unindexed index contexts,"As reported by CASSANDRA-19034, {{SelectTest#testContainsKeyAndContainsWithIndexOnMapValue}} fails when the default config uses SAI:
{code:java}
[junit-timeout] Testcase: testContainsKeyAndContainsWithIndexOnMapValue(org.apache.cassandra.cql3.validation.operations.SelectTest)-_jdk11:	FAILED
[junit-timeout] Got less rows than expected. Expected 1 but got 0
[junit-timeout] junit.framework.AssertionFailedError: Got less rows than expected. Expected 1 but got 0
[junit-timeout] 	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1849)
[junit-timeout] 	at org.apache.cassandra.cql3.validation.operations.SelectTest.lambda$testContainsKeyAndContainsWithIndexOnMapValue$9(SelectTest.java:625)
[junit-timeout] 	at org.apache.cassandra.cql3.CQLTester.beforeAndAfterFlush(CQLTester.java:2238)
[junit-timeout] 	at org.apache.cassandra.cql3.validation.operations.SelectTest.testContainsKeyAndContainsWithIndexOnMapValue(SelectTest.java:618)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout] 
[junit-timeout] 
{code}
This can also be reproduced with this simple test:
{code:java}
createTable(""CREATE TABLE %s (k int PRIMARY KEY, v int, m map<int,int>)"");
createIndex(""CREATE INDEX ON %s(v) USING 'SAI'"");
execute(""INSERT INTO %s (k, v, m) VALUES (?, ?, ?)"", 0, 1, map(2, 3));
assertRows(execute(""SELECT k, v, m FROM %s WHERE v = 1 AND m CONTAINS KEY 2 ALLOW FILTERING""),
           row(0, 1, map(2, 3)));
{code}
I think the problem is that the index context created by the query planner for an unindexed column always has a {{VALUES}} index target type, [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/index/sai/plan/QueryController.java#L149]. This is problematic when there is a {{CONTAINS KEY}} expression that isn't backed by an index, since we'll be comparing the searched map key to scanned map values."
CASSANDRA-19034,SelectTest fails when run with SAI index,"When run with SAI index, the following two tests error out:

{code}
[junit-timeout] Testcase: testContainsKeyAndContainsWithIndexOnMapValue(org.apache.cassandra.cql3.validation.operations.SelectTest)-_jdk11:	FAILED
[junit-timeout] Got less rows than expected. Expected 1 but got 0
[junit-timeout] junit.framework.AssertionFailedError: Got less rows than expected. Expected 1 but got 0
[junit-timeout] 	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1849)
[junit-timeout] 	at org.apache.cassandra.cql3.validation.operations.SelectTest.lambda$testContainsKeyAndContainsWithIndexOnMapValue$9(SelectTest.java:625)
[junit-timeout] 	at org.apache.cassandra.cql3.CQLTester.beforeAndAfterFlush(CQLTester.java:2238)
[junit-timeout] 	at org.apache.cassandra.cql3.validation.operations.SelectTest.testContainsKeyAndContainsWithIndexOnMapValue(SelectTest.java:618)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Testcase: testFilterWithIndexForContains(org.apache.cassandra.cql3.validation.operations.SelectTest)-_jdk11:	FAILED
[junit-timeout] Invalid value for row 1 column 0 (k1 of type int), expected <1> but got <0>
[junit-timeout] Invalid value for row 1 column 2 (v of type set<int>), expected <{4, 5, 6}> but got <{2, 3, 4}>
[junit-timeout] 
[junit-timeout] junit.framework.AssertionFailedError: Invalid value for row 1 column 0 (k1 of type int), expected <1> but got <0>
[junit-timeout] Invalid value for row 1 column 2 (v of type set<int>), expected <{4, 5, 6}> but got <{2, 3, 4}>
[junit-timeout] 
[junit-timeout] 	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1826)
[junit-timeout] 	at org.apache.cassandra.cql3.validation.operations.SelectTest.lambda$testFilterWithIndexForContains$6(SelectTest.java:543)
[junit-timeout] 	at org.apache.cassandra.cql3.CQLTester.beforeAndAfterFlush(CQLTester.java:2240)
[junit-timeout] 	at org.apache.cassandra.cql3.validation.operations.SelectTest.testFilterWithIndexForContains(SelectTest.java:542)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}

The latter seems to be giving the results in the wrong order, and the order flips when the data is flushed.

Caught during preparation of _latest config that would switch default to SAI (CASSANDRA-18753)."
CASSANDRA-19032,StorageAttachedIndexDDLTest.concurrentTruncateWithIndexBuilding is flakey,"The StorageAttachedIndexDDLTest.concurrentTruncateWithIndexBuilding has been showing flakiness in test runs and need investigating and fixing. Looking at the failure is seems that there is an edge condition where the index will fail to complete its build if a table truncate is triggered at the right moment.

We need to try and provoke this edge condition using injection and then see what can be done to fix the build failure.

"
CASSANDRA-19031,[Analytics] Fix bulk writing when using identifiers that need quotes,"In the Cassandra Analytics library, when writing tables that need quoted identifiers, writes to Spark Cassandra Bulk Analytics fails. We need to make sure that mixed-case and reserved words are properly quoted when writing keyspaces / tables as well as column names that need quotes.
"
CASSANDRA-19030,Vector Quickstart Documentation does not work,"The Documentation here [https://cassandra.apache.org/doc/latest/cassandra/getting-started/vector-search-quickstart.html]

doesn't work.

Some example errors, when creating the comments_vs table
{code:java}
instaclustr@cqlsh:cycling> CREATE TABLE IF NOT EXISTS cycling.comments_vs (
           ...   record_id timeuuid,
           ...   id uuid,
           ...   commenter text,
           ...   comment text,
           ...   comment_vector VECTOR <FLOAT, 5>;
SyntaxException: line 6:34 mismatched input ';' expecting ')' (...comment_vector VECTOR <FLOAT, 5>[;])
instaclustr@cqlsh:cycling>   created_at timestamp,
           ...   PRIMARY KEY (id, created_at)
           ... )
           ... WITH CLUSTERING ORDER BY (created_at DESC);
SyntaxException: line 1:0 no viable alternative at input 'created_at' ([created_at]...)
instaclustr@cqlsh:cycling> {code}
Which then breaks all the subsequent commands, some of the later inserts and SELECTS need work even after repairing.

There's a few errors in the CQL commands and table definitions, I managed to get it working in the below CQL.
{code:java}
CREATE KEYSPACE IF NOT EXISTS demo
   WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
   
CREATE TABLE IF NOT EXISTS demo.comments_vs (
  record_id timeuuid,
  id uuid,
  commenter text,
  comment text,
  comment_vector VECTOR <FLOAT, 5>,
  created_at timestamp,
  PRIMARY KEY (id, created_at)
);
WITH CLUSTERING ORDER BY (created_at DESC);CREATE INDEX IF NOT EXISTS ann_index
  ON demo.comments_vs(comment_vector) USING 'sai';
  
  
INSERT INTO demo.comments_vs (record_id, id, created_at, comment, commenter, comment_vector)
   VALUES (
      now(),
      e7ae5cf3-d358-4d99-b900-85902fda9bb0,
      '2017-03-21 13:11:09.999-0800',
      'Second rest stop was out of water',
      'Alex',
      [0.99, 0.5, 0.99, 0.1, 0.34]
);

INSERT INTO demo.comments_vs (record_id, id, created_at, comment, commenter, comment_vector)
   VALUES (
      now(),
      e7ae5cf3-d358-4d99-b900-85902fda9bb0,
      '2017-04-01 06:33:02.16-0800',
      'LATE RIDERS SHOULD NOT DELAY THE START',
      'Alex',
      [0.9, 0.54, 0.12, 0.1, 0.95]
);

INSERT INTO demo.comments_vs (record_id, id, created_at, comment, commenter, comment_vector)
   VALUES (
      now(),
      c7fceba0-c141-4207-9494-a29f9809de6f,
      totimestamp(now()),
      'The gift certificate for winning was the best',
      'Amy',
      [0.13, 0.8, 0.35, 0.17, 0.03]
);

INSERT INTO demo.comments_vs (record_id, id, created_at, comment, commenter, comment_vector)
   VALUES (
      now(),
      c7fceba0-c141-4207-9494-a29f9809de6f,
      '2017-02-17 12:43:20.234+0400',
      'Glad you ran the race in the rain',
      'Amy',
      [0.3, 0.34, 0.2, 0.78, 0.25]
);

INSERT INTO demo.comments_vs (record_id, id, created_at, comment, commenter, comment_vector)
   VALUES (
      now(),
      c7fceba0-c141-4207-9494-a29f9809de6f,
      '2017-03-22 5:16:59.001+0400',
      'Great snacks at all reststops',
      'Amy',
      [0.1, 0.4, 0.1, 0.52, 0.09]
);

INSERT INTO demo.comments_vs (record_id, id, created_at, comment, commenter, comment_vector)
   VALUES (
      now(),
      c7fceba0-c141-4207-9494-a29f9809de6f,
      '2017-04-01 17:43:08.030+0400',
      'Last climb was a killer',
      'Amy',
      [0.3, 0.75, 0.2, 0.2, 0.5]
);

SELECT * FROM demo.comments_vs
    ORDER BY comment_vector ANN OF [0.15, 0.1, 0.1, 0.35, 0.55]
    LIMIT 3;
    
SELECT comment, similarity_cosine(comment_vector, [0.2, 0.15, 0.3, 0.2, 0.05])
    FROM demo.comments_vs
    ORDER BY comment_vector ANN OF [0.1, 0.15, 0.3, 0.12, 0.05]
    LIMIT 1; {code}
Just raising a ticket to link for a website PR."
CASSANDRA-19028,"Python DTest does not release CQL connections, which can cause containers to hang for 2 hours trying to connect to a server thats already gone","While debugging why upgrade tests were taking 2 hours we noticed that the test completed 2 hours early… yet the process did not exit and was logging cassandra.pool trying to connect to a cluster (but python dtest already cleaned it up)…

After investigating the code I saw that we create connections and store them in an array, but only 2 tests actually clean up!"
CASSANDRA-19027,"Python DTest does not release CQL connections, which can cause containers to hang for 2 hours trying to connect to a server thats already gone","While debugging why upgrade tests were taking 2 hours we noticed that the test completed 2 hours early… yet the process did not exit and was logging cassandra.pool trying to connect to a cluster (but python dtest already cleaned it up)…

After investigating the code I saw that we create connections and store them in an array, but only 2 tests actually clean up!"
CASSANDRA-19024,[Analytics] Fix bulk reading when using identifiers that need quotes,"In the Cassandra Analytics library, when reading tables that need quoted identifiers, reading from Spark Cassandra Bulk Analytics fails. We need to make sure that mixed-case and reserved words are properly quoted when reading keyspaces / tables that need quotes."
CASSANDRA-19021,Update default disk_access_mode to mmap_index_only,https://lists.apache.org/thread/nhp6vftc4kc3dxskngxy5rpo1lp19drw
CASSANDRA-19020,cqlsh should allow failure to import cqlshlib.serverversion,"cqlshlib.serverversion is created by ant, recording the server's version so that python can see if it matches cqlsh later.  This can make work for other things that need to be aware of it like CASSANDRA-18594, so we should relax it a bit since this really has no value outside of warning humans they have a mismatch."
CASSANDRA-19019,DESC TYPE forgets to quote UDT's field names,"If I create a type with

*CREATE TYPE ""Quoted_KS"".""udt_@@@"" (a int, ""field_!!!"" text)*

and then run DESC TYPE ""Quoted_KS"".""udt_@@@"" I get:

*CREATE TYPE ""Quoted_KS"".""udt_@@@"" (a int, field_!!! text)*

Note the missing quotes around the non-alphanumeric field name, which does need quoting. If I'll try to run this command, it won't work.

Tested on Cassandra 4.1."
CASSANDRA-19018,An SAI-specific mechanism to ensure consistency isn't violated for multi-column (i.e. AND) queries at CL > ONE,"CASSANDRA-19007 is going to be where we add a guardrail around filtering/index queries that use intersection/AND over partially updated non-key columns. (ex. Restricting one clustering column and one normal column does not cause a consistency problem, as primary keys cannot be partially updated.) This issue exists to attempt to fix this specifically for SAI in 5.0.x, as Accord will (last I checked) not be available until the 5.1 release.

The SAI-specific version of the originally reported issue is this:

{noformat}
try (Cluster cluster = init(Cluster.build(2).withConfig(config -> config.with(GOSSIP).with(NETWORK)).start()))
        {
            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.t (k int PRIMARY KEY, a int, b int)""));
            cluster.schemaChange(withKeyspace(""CREATE INDEX ON %s.t(a) USING 'sai'""));
            cluster.schemaChange(withKeyspace(""CREATE INDEX ON %s.t(b) USING 'sai'""));

            // insert a split row
            cluster.get(1).executeInternal(withKeyspace(""INSERT INTO %s.t(k, a) VALUES (0, 1)""));
            cluster.get(2).executeInternal(withKeyspace(""INSERT INTO %s.t(k, b) VALUES (0, 2)""));

        // Uncomment this line and test succeeds w/ partial writes completed...
        //cluster.get(1).nodetoolResult(""repair"", KEYSPACE).asserts().success();

            String select = withKeyspace(""SELECT * FROM %s.t WHERE a = 1 AND b = 2"");
            Object[][] initialRows = cluster.coordinator(1).execute(select, ConsistencyLevel.ALL);
            assertRows(initialRows, row(0, 1, 2)); // not found!!
        }
{noformat}

To make a long story short, the local SAI indexes are hiding local partial matches from the coordinator that would combine there to form full matches. Simple non-index filtering queries also suffer from this problem, but they hide the partial matches in a different way. I'll outline a possible solution for this in the comments that takes advantage of replica filtering protection and the repaired/unrepaired datasets...and attempts to minimize the amount of extra row data sent to the coordinator."
CASSANDRA-19017,Ensure that empty SAI column indexes do not fail on validation after full-SSTable streaming,"I actually discovered this in some exploratory testing for CASSANDRA-19007...

{noformat}
@Test
public void testRepairWithEmptyColumnIndex() throws IOException
{
   try (Cluster cluster = init(Cluster.build(2).withConfig(config -> config.with(GOSSIP).with(NETWORK)).start()))
   {
       cluster.schemaChange(withKeyspace(""CREATE TABLE %s.t (k int PRIMARY KEY, a int, b int)""));
       cluster.schemaChange(withKeyspace(""CREATE INDEX ON %s.t(a) USING 'sai'""));
       cluster.schemaChange(withKeyspace(""CREATE INDEX ON %s.t(b) USING 'sai'""));

       // insert a split row
       cluster.get(1).executeInternal(withKeyspace(""INSERT INTO %s.t(k, a) VALUES (0, 1)""));
       cluster.get(2).executeInternal(withKeyspace(""INSERT INTO %s.t(k, b) VALUES (0, 2)""));
       cluster.get(1).flush(KEYSPACE);
       cluster.get(2).flush(KEYSPACE);
       cluster.get(1).nodetoolResult(""repair"", KEYSPACE).asserts().success(); // fails w/ no data for the ""a"" index on node 2!
   }
}
{noformat}

{noformat}
java.lang.RuntimeException: Repair job has failed with the error message: Repair command #1 failed with error Repair session 55a34e20-7f24-11ee-b345-d158c708a34e for range [(-1,9223372036854775805], (9223372036854775805,-1]] failed with error Stream failed: 
Session peer /127.0.0.2:7012 Failed because of an unknown exception
java.io.UncheckedIOException: java.nio.file.NoSuchFileException: /private/var/folders/4d/zfjs7m7s6x5_l93k33r5k6680000gn/T/dtests2185335212676803469/node1/data0/distributed_test_keyspace/t-3459c6aa21753083afcc5b0c9f77c3f7/distributed_test_keyspace-t-nc-4-big-SAI+aa+t_a_idx+Meta.db
	org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat.rethrowIOException(V1OnDiskFormat.java:241)
	org.apache.cassandra.index.sai.disk.v1.V1OnDiskFormat.validatePerColumnIndexComponents(V1OnDiskFormat.java:230)
java.nio.file.NoSuchFileException: /private/var/folders/4d/zfjs7m7s6x5_l93k33r5k6680000gn/T/dtests2185335212676803469/node1/data0/distributed_test_keyspace/t-3459c6aa21753083afcc5b0c9f77c3f7/distributed_test_keyspace-t-nc-4-big-SAI+aa+t_a_idx+Meta.db
	java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
{noformat}

This is an untested side-effect of a change in CASSANDRA-18670. The fix should be fairly simple, essentially making sure we don't assume the column index components are present during validation when the index completion marker is present."
CASSANDRA-19016,CEP-15: (C*) per-table transactional configuration,"Accord configuration options should be rolled into as few options as possible, with as few touch points as possible and, with support for per-table configuration"
CASSANDRA-19015,Nodetool 'tablestats' formatting uses inconsistent significant digits,"Nodetool reports milliseconds (ms) with anywhere from 3 to 15 significant digits.  Ratios use five or sixteen decimal places.  Averages use 1 or 13 decimal places.
 * milliseconds should use 3 decimal places 
 * ratios should use 3 decimal places (tenths of a percent)
 * averages should use 1 or 2

For readability, it would be helpful if large integers had comma separators.  I.e., space used: as 1,463,210,998,523 and/or in GiB/MiB/KiB.  It's unclear if the exact disk size is somehow useful, as it may change minute-by-minute, if not, rounding would be best, or displaying both   Space used (live): 1,463,210,998,523 (1,463GiB)

Total number of tables: 83
----------------
Keyspace : X
    Read Count: 1007337271
    Read Latency: 8.485891803649942 ms
    Write Count: 67550181
    Write Latency: 0.02556443163342523 ms
    Pending Flushes: 0
        Table: Y
        SSTable count: 7183
        Old SSTable count: 0
        SSTables in each level: [0, 9, 92, 754, 6328, 0, 0, 0, 0]
        Space used (live): 1463210998523
        Space used (total): 1463210998523
        Space used by snapshots (total): 0
        Off heap memory used (total): 607419608
        SSTable Compression Ratio: 0.3146620992793412
        Number of partitions (estimate): 24784137
        Memtable cell count: 106067
        Memtable data size: 248539982
        Memtable off heap memory used: 0
        Memtable switch count: 256
        Local read count: 865440924
        Local read latency: 6.857 ms
        Local write count: 13881409
        Local write latency: 0.037 ms
        Pending flushes: 0
        Percent repaired: 0.0
        Bytes repaired: 0.000KiB
        Bytes unrepaired: 4315.386GiB
        Bytes pending repair: 0.000KiB
        Bloom filter false positives: 11027855
        Bloom filter false ratio: 0.01099
        Bloom filter space used: 33590024
        Bloom filter off heap memory used: 33532560
        Index summary off heap memory used: 8174024
        Compression metadata off heap memory used: 565713024
        Compacted partition minimum bytes: 36
        Compacted partition maximum bytes: 17797419593
        Compacted partition mean bytes: 189740
        Average live cells per slice (last five minutes): 1443.2146104466253
        Maximum live cells per slice (last five minutes): 105778
        Average tombstones per slice (last five minutes): 1.0
        Maximum tombstones per slice (last five minutes): 1
        Dropped Mutations: 0
        Droppable tombstone ratio: 0.00000"
CASSANDRA-19014,repair_admin summarize-pending command is throwing OpenDataException,"Execution of repair_admin summarize-pending is throwing the following exception:
{code:java}
error: Item names do not match CompositeType: names in items but not in CompositeType: []; names in CompositeType but not in items: [failed]
-- StackTrace --
javax.management.openmbean.OpenDataException: Item names do not match CompositeType: names in items but not in CompositeType: []; names in CompositeType but not in items: [failed]
        at javax.management.openmbean.CompositeDataSupport.<init>(CompositeDataSupport.java:220)
        at javax.management.openmbean.CompositeDataSupport.<init>(CompositeDataSupport.java:176)
        at org.apache.cassandra.repair.consistent.admin.PendingStats.toComposite(PendingStats.java:86)
        at org.apache.cassandra.service.ActiveRepairService.getPendingStats(ActiveRepairService.java:296)
        at org.apache.cassandra.tools.nodetool.RepairAdmin$SummarizePendingCmd.execute(RepairAdmin.java:112)
        at org.apache.cassandra.tools.NodeTool$NodeToolCmd.runInternal(NodeTool.java:358)
        at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:343)
        at org.apache.cassandra.tools.NodeTool.execute(NodeTool.java:246)
        at org.apache.cassandra.distributed.impl.Instance$DTestNodeTool.execute(Instance.java:996)
        at org.apache.cassandra.distributed.impl.Instance.lambda$nodetoolResult$40(Instance.java:906)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:750) {code}
The bug is caused by the fact that the list of fields at [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/consistent/admin/PendingStats.java#L78-L82] does not align with [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/repair/consistent/admin/PendingStats.java#L35.]"
CASSANDRA-19013,Performance issues after Cassandra upgrade,"I would like to ask for your help in the following issue.
 
We have upgraded our Cassandra 3.11.4 to 4.1.2 in production environment and since that we're facing with many performance issues:
 * The cluster operations lowered to 17-20k/s which was between 21-25k/s with 3.11.4
 * CPU usage significantly increased -> previously it was nearly 50% at peak times, after the update it's between 65-70%
 * Pending tasks increased -> from nearly 0 to 30 at peak times
 * Native Transport Requests significantly increased -> from 100-200 to 2K-3K at peak times
 * Chunck cache hit rate metric stopped working
 * Read latency increased from microsecs to milisecs
 * sstables per read histogram also shows increased values

We have already done with:
 * file_cache_enabled has been enabled
 * sstableupgrade
 * Java has been updated from 8 to 11
 * Made a truncate of affected keyspace
 * Affected keyspace's indexes has been rebuilt

Our infrastructure contains 2 DCs with 24 nodes/dc. Every node has 8 CPUs and 64GBs of RAM and running on RHEL 8.6.

We have already compared the two versions but didn't notice any new feature or option which could cause this performance degradation. If do you have any idea or do you need any kind of logs don't hesitate to contact me."
CASSANDRA-19011,Primary key -> row ID lookups are broken for skipping and intersections during SAI queries,"Schema:
{code:java}
CREATE TABLE IF NOT EXISTS distributed_test_keyspace.tbl1 (pk1 bigint,ck1 bigint,v1 ascii,v2 bigint, PRIMARY KEY (pk1, ck1)) WITH  CLUSTERING ORDER BY (ck1 ASC);
CREATE CUSTOM INDEX v1_sai_idx ON distributed_test_keyspace.tbl1 (v1) USING 'StorageAttachedIndex' WITH OPTIONS = {'case_sensitive': 'false', 'normalize': 'true', 'ascii': 'true'}; ;
CREATE CUSTOM INDEX v2_sai_idx ON distributed_test_keyspace.tbl1 (v2) USING 'StorageAttachedIndex';
 {code}
{code:java}
java.lang.AssertionError: skipped to an item smaller than the target; iterator: org.apache.cassandra.index.sai.disk.IndexSearchResultIterator@f399f79, target key: PrimaryKey: { token: 8384965201802291970, partition: DecoratedKey(8384965201802291970, c4bc1c50f9e76a50), clustering: CLUSTERING:8b4b4c5991a4ea10 } , returned key: PrimaryKey: { token: 8384965201802291970, partition: DecoratedKey(8384965201802291970, c4bc1c50f9e76a50), clustering: CLUSTERING:89f1cf92658cb668 } 
	at org.apache.cassandra.index.sai.iterators.KeyRangeIntersectionIterator.computeNext(KeyRangeIntersectionIterator.java:95)
	at org.apache.cassandra.index.sai.iterators.KeyRangeIntersectionIterator.computeNext(KeyRangeIntersectionIterator.java:39)
	at org.apache.cassandra.utils.AbstractGuavaIterator.tryToComputeNext(AbstractGuavaIterator.java:122)
	at org.apache.cassandra.index.sai.iterators.KeyRangeIterator.tryToComputeNext(KeyRangeIterator.java:129)
	at org.apache.cassandra.utils.AbstractGuavaIterator.hasNext(AbstractGuavaIterator.java:116)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.nextKey(StorageAttachedIndexSearcher.java:274)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.nextKeyInRange(StorageAttachedIndexSearcher.java:203)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.nextSelectedKeyInRange(StorageAttachedIndexSearcher.java:234)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.nextRowIterator(StorageAttachedIndexSearcher.java:188)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.computeNext(StorageAttachedIndexSearcher.java:169)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.computeNext(StorageAttachedIndexSearcher.java:111)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:91)
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:338)
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:201)
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:186)
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:48)
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:346)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2186)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2581)
	at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
	at relocated.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829) {code}
 

Unfortunately, there's no tooling for shrinking around SAI just yet, but I have a programmatic repro using INSERT and DELETE statements. I will do my best to post it asap, but thought this can already be useful for visibility."
CASSANDRA-19010,Remove sstable format 'nc' and make 5.0 'nb' and 'oa',CASSANDRA-18933 found out 4.1 sstables are 'nb' and not 'nc' as incorrectly noted in the code. Hence 5.0 should use 'nb' in compatibility mode and 'oa' out of it. This leaves 'nc' only to be removed.
CASSANDRA-19009,CEP-15: (C*/Accord)  Schema based fast path reconfiguration,"This adds availability aware accord fast path reconfiguration, as well as user configurable fast path settings, which are set at the keyspace level and (optionally) at the table level for increased granularity.

The major parts are:

*Add availability information to cluster metadata*

Accord topology in C* is not stored in cluster metadata, but is meant to calculated deterministically from cluster metadata state at a given epoch. This adds the availability data, as well as the failure detector / gossip listener and state change deduplication to CMS.

*Move C* accord keys/topology from keyspace prefixes to tableid prefixes*

To support per-table fast path settings, topologies and keys need to include the table id. Since accord topologies could begin to consume a lot of memory in clusters with a lot of nodes and tables, topology generation has been updated to reuse previously allocated shards / shard parts where possible, which will only increase heap sizes when things actually change.

*Make fast path settings configurable via schema*

There are 2.5 strategies: Simple, Parameterized, and InheritKeyspaceSettings. Simple will use as many available nodes as possible for the fast path electorate, this is the default for the keyspace fast path strategy. Parameterized allows you to set a target size, and preferred datacenters for the FP electorate. InheritKeyspace tells topology generation to just use the keyspace fast path settings, and is the default for the table fast path strategy."
CASSANDRA-19003,Get Sidecar port through CassandraContext for more flexibility ,We get sidecar port from `BulkSparkConf` but it will be better if we get it from `CassandraContext` provides more flexibility 
CASSANDRA-19002,Create / update tests to ensure commit logs and hints for all versions in MS are ingestible in 5.0,This is follow up of CASSANDRA-18314
CASSANDRA-19001,Check whether the startup warnings for unknown modules represent a legit problem or cosmetic issue,"During the 5.0 alpha 2 release [vote|https://lists.apache.org/thread/lt3x0obr5cpbcydf5490pj6b2q0mz5zr], [~paulo] raised the following concerns:
{code:java}
Launched a tarball-based 5.0-alpha2 container on top of
""eclipse-temurin:17-jre-focal"" and the server starts up fine, can run
nodetool and cqlsh.
I got these seemingly harmless JDK17 warnings during startup and when
running nodetool (no warnings on JDK11):
WARNING: Unknown module: jdk.attach specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-exports
WARNING: Unknown module: jdk.compiler specified to --add-opens
WARNING: A terminally deprecated method in java.lang.System has been called
WARNING: System::setSecurityManager has been called by
org.apache.cassandra.security.ThreadAwareSecurityManager
(file:/opt/cassandra/lib/apache-cassandra-5.0-alpha2-SNAPSHOT.jar)
WARNING: Please consider reporting this to the maintainers of
org.apache.cassandra.security.ThreadAwareSecurityManager
WARNING: System::setSecurityManager will be removed in a future release
Anybody knows if these warnings are legit/expected ? We can create
follow-up tickets if needed.
$ java --version
openjdk 17.0.9 2023-10-17
OpenJDK Runtime Environment Temurin-17.0.9+9 (build 17.0.9+9)
OpenJDK 64-Bit Server VM Temurin-17.0.9+9 (build 17.0.9+9, mixed mode,
sharing)
{code}
{code:java}
Clarification: - When running nodetool only the ""Unknown module"" warnings show up. All warnings show up during startup.{code}
We need to verify whether this presents a real problem in the features where those modules are expected to be used, or if it is a false alarm. 

 "
CASSANDRA-19000,Test Failure: org.apache.cassandra.tools.BulkLoaderTest.testBulkLoader_WithArgs2,"h3.  

https://ci-cassandra.apache.org/job/Cassandra-trunk/1766/testReport/org.apache.cassandra.tools/BulkLoaderTest/testBulkLoader_WithArgs2_cdc_jdk17_arch_x86_64_python2_7/
{code:java}
Error Message
Wrong thread status, active threads unaccounted for: [cluster3-nio-worker-0]

Stacktrace
junit.framework.AssertionFailedError: Wrong thread status, active threads unaccounted for: [cluster3-nio-worker-0] at org.apache.cassandra.tools.OfflineToolUtils.assertNoUnexpectedThreadsStarted(OfflineToolUtils.java:120) at org.apache.cassandra.tools.BulkLoaderTest.testBulkLoader_WithArgs2(BulkLoaderTest.java:129) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-18999,Gossiper::hasMajorVersion3Nodes returns true when a cluster is upgrading patch version without Cassandra 3 nodes.,"When working on https://issues.apache.org/jira/browse/CASSANDRA-18968 we found that {{Gossiper::hasMajorVersion3Nodes}} will return true when the cluster is undergoing an upgrade from a patch version even if the cluster has no Cassandra 3 nodes in it.

This can be reproduced by running this Gossiper test:
{code:java}
    @Test
    public void testHasVersion3NodesShouldReturnFalseWhenNoVersion3NodesDetectedAndCassandra4UpgradeInProgress() throws Exception
    {
        Gossiper.instance.start(0);
        Gossiper.instance.expireUpgradeFromVersion();

        VersionedValue.VersionedValueFactory factory = new VersionedValue.VersionedValueFactory(null);
        EndpointState es = new EndpointState((HeartBeatState) null);
        es.addApplicationState(ApplicationState.RELEASE_VERSION, factory.releaseVersion(CURRENT_VERSION.toString()));
        Gossiper.instance.endpointStateMap.put(InetAddressAndPort.getByName(""127.0.0.1""), es);
        Gossiper.instance.liveEndpoints.add(InetAddressAndPort.getByName(""127.0.0.1""));

        es = new EndpointState((HeartBeatState) null);
        String previousPatchVersion = String.valueOf(CURRENT_VERSION.major) + '.' + (CURRENT_VERSION.minor) + '.' + (CURRENT_VERSION.patch - 1);
        es.addApplicationState(ApplicationState.RELEASE_VERSION, factory.releaseVersion(previousPatchVersion));
        Gossiper.instance.endpointStateMap.put(InetAddressAndPort.getByName(""127.0.0.2""), es);
        Gossiper.instance.liveEndpoints.add(InetAddressAndPort.getByName(""127.0.0.2""));
        assertFalse(Gossiper.instance.hasMajorVersion3Nodes());
    }
{code}
This seems to be because of [https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/gms/Gossiper.java#L2360], where an upgrade in progress is possible but we are not upgrading from a lower family version (i.e from 4.1.1 to 4.1.2).

From the comment in this function, it seems instead of the existing check, we would want to iterate over all known endpoints in gossip and return true if any of them do not have a version (similar to [https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/gms/Gossiper.java#L227-L236) |https://github.com/apache/cassandra/blob/cassandra-4.1/src/java/org/apache/cassandra/gms/Gossiper.java#L227-L236).]"
CASSANDRA-18997,Unified Compaction Strategy is missing documentation,"UCS is missing from [the CQL documentation for 5.0|https://cassandra.apache.org/doc/5.0/cassandra/developing/cql/ddl.html#cql-compaction-options] and [the compaction page|https://cassandra.apache.org/doc/5.0/cassandra/managing/operating/compaction/index.html#compaction-options].

We need to create a documentation page for UCS and link it from both."
CASSANDRA-18996,Add documentation about crypto providers,We should document what we did in CASSANDRA-18624 to notify users about the ability to set a custom crypto provider and instruct them how to use it.
CASSANDRA-18995,Improve documentation about cloud-based snitches,"There were various improvements and refactorings of cloud-based snitches and snitches in general in CASSANDRA-18646 CASSANDRA-18438 or CASSANDRA-16555.

We should reflect new features and add new snitches in the documentation."
CASSANDRA-18994,"SAI range query does not play together with ""IN""","I am using schema from the website's quickstart.

{code}
cqlsh> DESCRIBE KEYSPACE cycling ;

CREATE KEYSPACE cycling WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE cycling.cyclist_semi_pro (
    id int PRIMARY KEY,
    affiliation text,
    age int,
    country text,
    firstname text,
    lastname text,
    registration date
) WITH additional_write_policy = '99p'
    AND allow_auto_snapshot = true
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND cdc = false
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND memtable = 'default'
    AND crc_check_chance = 1.0
    AND default_time_to_live = 0
    AND extensions = {}
    AND gc_grace_seconds = 864000
    AND incremental_backups = true
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair = 'BLOCKING'
    AND speculative_retry = '99p';

CREATE CUSTOM INDEX age_sai_idx ON cycling.cyclist_semi_pro (age) USING 'StorageAttachedIndex';

CREATE CUSTOM INDEX country_sai_idx ON cycling.cyclist_semi_pro (country) USING 'StorageAttachedIndex' WITH OPTIONS = {'ascii': 'true', 'case_sensitive': 'false', 'normalize': 'true'};

CREATE CUSTOM INDEX lastname_sai_idx ON cycling.cyclist_semi_pro (lastname) USING 'StorageAttachedIndex' WITH OPTIONS = {'ascii': 'true', 'case_sensitive': 'false', 'normalize': 'true'};

CREATE CUSTOM INDEX registration_sai_idx ON cycling.cyclist_semi_pro (registration) USING 'StorageAttachedIndex';

{code}

Then I do:

{code}
cqlsh> SELECT * FROM cycling.cyclist_semi_pro WHERE lastname in ('Cantona', 'Boyd');
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING""

cqlsh> SELECT * FROM cycling.cyclist_semi_pro WHERE lastname in ('Cantona', 'Boyd') ALLOW FILTERING;

 id | affiliation     | age | country | firstname | lastname | registration
----+-----------------+-----+---------+-----------+----------+--------------
  5 |   Como Velocità |  24 |     ITA |     Irene |  Cantona |   2012-07-22
 20 | London Cyclists |  18 |     GBR |    Leslie |     Boyd |   2012-12-15

{code}

But check this:

{code}
cqlsh> SELECT * FROM cycling.cyclist_semi_pro WHERE registration > '2010-01-01' AND registration < '2015-12-31' and lastname in ('Cantona', 'Boyd') allow filtering;

ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures: UNKNOWN from localhost/127.0.0.1:7000"" info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'failures': 1, 'error_code_map': {'127.0.0.1': '0x0000'}}
{code}

and in the logs:

{code}
java.lang.AssertionError: null
	at org.apache.cassandra.index.sai.plan.Expression.add(Expression.java:171)
	at org.apache.cassandra.index.sai.plan.Operation.buildIndexExpressions(Operation.java:136)
	at org.apache.cassandra.index.sai.plan.Operation$AndNode.analyze(Operation.java:303)
	at org.apache.cassandra.index.sai.plan.Operation$Node.doTreeAnalysis(Operation.java:266)
	at org.apache.cassandra.index.sai.plan.Operation$Node.analyzeTree(Operation.java:251)
	at org.apache.cassandra.index.sai.plan.Operation.buildIterator(Operation.java:185)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher$ResultRetriever.<init>(StorageAttachedIndexSearcher.java:151)
	at org.apache.cassandra.index.sai.plan.StorageAttachedIndexSearcher.search(StorageAttachedIndexSearcher.java:107)
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:431)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2184)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2581)
	at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
{code}

If it is not supported it should not throw and it should print appropriate message.

If it is supported I guess this is a bug?"
CASSANDRA-18993,Harry-found silent data loss issue,"Harry has discovered a silent data loss bug in trunk, but it goes all the way back to early 5.0.

Some rows are not visble after flush. Compared Harry repro running a memtable instead of a regular Harry model, and it still reproduces (in other words, it is almost certainly not a Harry issue).

Simple schema, and only one flush is required for repro, so not an unlikely bug. Max partition size is 1k rows, so not an unlikely setup here, either. No concurrency involved; reproduces stably. Good chance this is related to the fact the schema has DESC clusterings.

I am working on posting a Harry branch that stably reproduces it.

 "
CASSANDRA-18992,Exclude jna transitive dependecy from the ohc library,"The concern is related to the JNA dependency on the ohc library. We are now using version 0.5.1 of the ohc library in Cassandra. The ohc 0.5.1 depends on version 4.1.0 of the net.java.dev.jna . For the Cassandra project, JNA has been updated since the 4.0 release version up to JNA 5.6.0 in CASSANDRA-16212 to support running nodes on the arm64 architecture, so I assume that JNA 4.1.0 is no longer in Casssandra's classpath. 

The common practice in the Cassandra project is to exclude these kinds of transitive dependencies when they are explicitly mentioned in the main parent pom, since Ant may resolve them incorrectly. See comment:
https://issues.apache.org/jira/browse/CASSANDRA-14667?focusedCommentId=17765091&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17765091"
CASSANDRA-18991,Test failure: sstableutil_test.TestSSTableUtil.test_abortedcompaction,"Seen here:

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2544/workflows/f221b339-c336-475c-b094-daab486756d3/jobs/44832/tests
{code:java}
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [SSTableBatchOpen:1] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:1,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-4-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)', [node1] 'ERROR [SSTableBatchOpen:3] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:3,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']"" Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [SSTableBatchOpen:1] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:1,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-4-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)', [node1] 'ERROR [SSTableBatchOpen:3] 2023-11-01 00:49:29,910 JVMStabilityInspector.java:68 - Exception in thread Thread[SSTableBatchOpen:3,5,SSTableBatchOpen]\njava.lang.AssertionError: Stats component is missing for sstable /tmp/dtest-14cqh6up/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nb-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:470)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:381)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader$2.run(SSTableReader.java:551)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']{code}"
CASSANDRA-18990,Fix SAI docs issues,"# Change CREATE CUSTOM INDEX syntax to CREATE INDEX in SAI quickstart.
 # Consolidate SAI code examples."
CASSANDRA-18988,Updating the column of a non-existent row in an Accord transaction results in Atomicity violation,"*System configuration and information:*

Single node Cassandra with Accord transactions enabled running on docker

Built from commit: [a7cd114435704b988c81f47ef53d0bfd6441f38b|https://github.com/apache/cassandra/commit/a7cd114435704b988c81f47ef53d0bfd6441f38b]

CQLSH: [cqlsh 6.2.0 | Cassandra 5.0-alpha2-SNAPSHOT | CQL spec 3.4.7 | Native protocol v5]

 

*Steps to reproduce in CQLSH:*
{code:java}
CREATE KEYSPACE accord WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;{code}
{code:java}
CREATE TABLE accord.accounts (
    partition text,
    account_id int,
    balance int,
    PRIMARY KEY (partition, account_id)
) WITH CLUSTERING ORDER BY (account_id ASC);
{code}
{code:java}
BEGIN TRANSACTION
    INSERT INTO accord.accounts (partition, account_id, balance) VALUES ('default', 0, 100);
    INSERT INTO accord.accounts (partition, account_id, balance) VALUES ('default', 1, 100);
COMMIT TRANSACTION;{code}
atomicity bug happens after executing the following statement:

Based on [Cassandra documentation|https://cassandra.apache.org/doc/4.1/cassandra/cql/dml.html#update-statement] regarding the use of UPDATE statements, I expect the result of this transaction to be the insertion of a new account (\{ account_id: 3, balance: 10 }). The total balance across the three (3) accounts should be maintained (200). After executing the below transaction, the total number of accounts remains at two (2) and the total balance drops to 190. Basically, it appears as if only one half of the transaction proceeds.
{code:java}
BEGIN TRANSACTION
    UPDATE accord.accounts
    SET balance -= 10
    WHERE
      partition = 'default'
      AND account_id = 1;
    UPDATE accord.accounts
    SET balance += 10
    WHERE
      partition = 'default'
      AND account_id = 3;
COMMIT TRANSACTION;{code}
Bug / Error:
======================================================================

The result of performing a table read after executing the buggy transaction is:
{code:java}
 partition | account_id | balance
-----------+------------+---------
   default |          0 |     100
   default |          1 |      90
{code}
{color:#172b4d}Note that the above transactions are not possible without a transaction block because only counter type columns can be updated with += or -= syntax in normal (non-transactional) cql statements. Using counter type columns also results in a separate, related bug: [CASSANDRA-18987|https://issues.apache.org/jira/browse/CASSANDRA-18987]{color}

{color:#172b4d}This was found while testing Accord transactions with [~henrik.ingo] and team.{color}"
CASSANDRA-18987,Using counter column type in Accord transactions leads to Atomicity / Consistency violations,"*System configuration and information:*

Single node Cassandra with Accord transactions enabled running on docker

Built from commit: [a7cd114435704b988c81f47ef53d0bfd6441f38b|https://github.com/apache/cassandra/commit/a7cd114435704b988c81f47ef53d0bfd6441f38b]

CQLSH: [cqlsh 6.2.0 | Cassandra 5.0-alpha2-SNAPSHOT | CQL spec 3.4.7 | Native protocol v5]

 

*Steps to reproduce in CQLSH:*
{code:java}
CREATE KEYSPACE accord WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;{code}
{code:java}
CREATE TABLE accord.accounts (
    partition text,
    account_id int,
    balance counter,
    PRIMARY KEY (partition, account_id)
) WITH CLUSTERING ORDER BY (account_id ASC);
{code}
{code:java}
BEGIN TRANSACTION
  UPDATE accord.accounts
    SET balance += 100
  WHERE
    partition = 'default'
    AND account_id = 0;
  UPDATE accord.accounts
    SET balance += 100
  WHERE
    partition = 'default'
    AND account_id =1;
COMMIT TRANSACTION;{code}
bug happens after executing the following statement:

Based on [Cassandra documentation|https://cassandra.apache.org/doc/trunk/cassandra/developing/cql/types.html#counters] regarding the use of counters, I expect the following results:

Transaction A: subtract 10 from the balance of account 1 (total ending balance of 90) and add 10 to the balance of account 0 (total ending balance of 110)

{*}Bug A{*}: Neither account's balance is updated and the state of the rows is left unchanged
{code:java}
/* Transaction A */
BEGIN TRANSACTION
    UPDATE accord.accounts
    SET balance -= 10
    WHERE
      partition = 'default'
      AND account_id = 1;
    UPDATE accord.accounts
    SET balance += 10
    WHERE
      partition = 'default'
      AND account_id = 0;
COMMIT TRANSACTION;{code}
Transaction B: subtract 10 from the balance of account 1 (total ending balance of 90) and add 10 to the balance of a new account 2 (total ending balance of 10)

{*}Bug B{*}: Only the new account 2 is created. The balance of account 1 is left unchanged
{code:java}
/* Transaction B */
BEGIN TRANSACTION
    UPDATE accord.accounts
    SET balance -= 10
    WHERE
      partition = 'default'
      AND account_id = 1;
    UPDATE accord.accounts
    SET balance += 10
    WHERE
      partition = 'default'
      AND account_id = 2;
COMMIT TRANSACTION;{code}
Bug / Error:
======================================================================

The result of performing a table read after executing each buggy transaction is:
{code:java}
/* Transaction / Bug A */
 partition | account_id | balance
-----------+------------+---------
   default |          0 |     100
   default |          1 |     100{code}
{code:java}
/* Transaction / Bug B */
 partition | account_id | balance
-----------+------------+---------
   default |          0 |     100
   default |          1 |     100
   default |          2 |      10 {code}
Note that performing the above statements without transaction blocks works as expected.

{color:#172b4d}This was found while testing Accord transactions with [~henrik.ingo] and team.{color}"
CASSANDRA-18986,SHA1 keys prevent installation on RHEL 9,"Due to the presence of SHA1 keys they have to be explicitly allowed before C* can be installed on RHEL 9-based systems: 

{quote}
Importing GPG key 0xF2833C93:
 Userid     : ""Eric Evans <eevans@sym-link.com>""
 Fingerprint: CEC8 6BB4 A0BA 9D0F 9039 7CAE F835 8FA2 F283 3C93
 From       : https://downloads.apache.org/cassandra/KEYS
Is this ok [y/N]: y
Key imported successfully
Importing GPG key 0x8D77295D:
 Userid     : ""Eric Evans <eevans@sym-link.com>""
 Fingerprint: C496 5EE9 E301 5D19 2CCC F2B6 F758 CE31 8D77 295D
 From       : https://downloads.apache.org/cassandra/KEYS
Is this ok [y/N]: y
Key imported successfully
Importing GPG key 0x2B5C1B00:
 Userid     : ""Sylvain Lebresne (pcmanus) <sylvain@datastax.com>""
 Fingerprint: 5AED 1BF3 78E9 A19D ADE1 BCB3 4BD7 36A8 2B5C 1B00
 From       : https://downloads.apache.org/cassandra/KEYS
Is this ok [y/N]: y
warning: Signature not supported. Hash algorithm SHA1 not available.
Key import failed (code 2). Failing package is: cassandra-4.0.11-1.noarch
 GPG Keys are configured as: https://downloads.apache.org/cassandra/KEYS
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
Error: GPG check FAILED
{quote}

This can be worked around by allowing SHA1:

{quote}
update-crypto-policies --set DEFAULT:SHA1
{quote}

https://www.redhat.com/en/blog/rhel-security-sha-1-package-signatures-distrusted-rhel-9
"
CASSANDRA-18985,Remove unused GlobalPlacementDelta class,"This is a leftover from early development and has been unused for some time.

Implementation can be found in:
https://github.com/beobal/cassandra/commit/0ee6c6299c3ae391bea1c1efc4f33e60070f2f80

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18984,Simplify scope-local retries using Entry.Id,"If a node submits a commit request to the CMS which succeeds but doesn't receive the response, it will automatically retry. However, the CMS node handling the resubmission most likely reject it as transformations are generally not idempotent. The submitter will need to then handle that rejection of the resubmission, which can currently mean inspecting the ClusterMetadata to see if the expected effects of the transformation are present. By using the entry id, the CMS node can identify the resubmission as such and return the same (success) response it sent (or should have sent) the first time.

Implementation can be found in:
https://github.com/beobal/cassandra/commit/c3c33ea1430e1bd14552f68de75bb9c4166fc986

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18983,Use epoch based table ids by default,"The initial implementation of CEP-21 depends on the use of deterministic table identifiers, as the table creation itself is performed by each node locally. This is suboptimal as recreating a previously dropped table can lead to the old data directory and files on disk, as well as CFIDs in the commit log being incorrectly associated with the new table.
As the epoch gives us a unique identifier for each creation event, we can base the table id on that to ensure uniqueness.

Implementation can be found in:
https://github.com/beobal/cassandra/commit/89a0a2bfb0d1646e8057d9bb69f8ce558ef3b483

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18982,Refactor multistep operations,"Make it easier to identify the current position in a sequence of steps.
Improve documentation and readability.

Implementation can be found in:
https://github.com/beobal/cassandra/commit/91a5cdde0c5e79063e2e90e8d5839736de20e076
https://github.com/beobal/cassandra/commit/791af4b59103b09e47aa2b13a285d8970a02ab53
https://github.com/beobal/cassandra/commit/2ff794db348f11196f857dafdef74561627d3453
https://github.com/beobal/cassandra/commit/cd8b4e29fe53e176874ab61e848c0512454806a9
https://github.com/beobal/cassandra/commit/c6254c161524ea364ff10ff68494f125f07161d9

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18981,Add a mechanism to signal that a rejected Transformation should not be retried,"In certain circumstances on the CMS node, it is useful to be able to cause a Transformation being applied to return a Rejected response without triggering catchup and retry. Adding an unchecked exception type for this enables it to be triggered from anywhere within the call stack, without polluting the interfaces.

Implementation can be found in:
https://github.com/beobal/cassandra/commit/8a84c0b562d59487a4c5fe7c8fb233cba3036216

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18980,Bring PropertyFileSnitch & RackInferringSnitch into line with TCM,"With TCM, all snitch implementations should only be responsible for the initial the topology configuration of the local node. For example, PFS should behave more like {{GossipingPropertyFileSnitch}} in that its configuration file specifies the DC & rack for the local node only. Each node in the cluster is responsible for updating its own topology info in {{ClusterMetadata}} when it is registered and the location of peers can be easily looked up there.

Modifying the DC & rack of a live node via GPFS/PFS config is desabled by default and strongly discouraged (CASSANDRA-10242/CASSANDRA-10243/CASSANDRA-9474) and there are {{StartupChecks}} in place to prevent changing location at startup. 

Given that modifying location of a joined node is a fundamentally unsafe operation, we should remove the ability to do so. The argument for allowing it has previously been that if a DC or cluster has no data, then the overhead of decommissioning and re-joining nodes to fix location is prohibitive. CEP-21 fixes that by making those operations more reliable, so the override becomes less valuable. 


Implementation can be found in:
https://github.com/beobal/cassandra/commit/8ec1ea705210b7e2e38d888768ba83faa53754d8
https://github.com/beobal/cassandra/commit/7b41f063e04f25b8215318f51d908732b1f21f5f
https://github.com/beobal/cassandra/commit/09cdc5351ad049d97c7eeb53c4e68701c088a711

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18979,Refactor StorageService to relocate functionality to more appropriate classes,"Historically, {{StorageService}} contains much of the logic related to bootstrap, decommission, token movement and so on. Although much of this has already been refactored in the {{cep-21-tcm}} branch, there remains plenty of room for improvement.

Implementation can be found in:
https://github.com/beobal/cassandra/commit/525f46d7961e7ffaa126792d5d003e9a42ef6c63

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|
"
CASSANDRA-18978,Test assigning the same token to multiple nodes,"Add a test to ensure a node cannot join the cluster with a token already assigned to another peer

Implementation can be found in:
https://github.com/beobal/cassandra/commit/b9e70ffa12455119396a1bcdc60e3dfc64b4f795


CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|

"
CASSANDRA-18977,Improve CMS handoff and configuration,"The intial mechanism for configuring membership of the CMS is rather simplistic. It requires the operator to select which nodes are members and to execute nodetool commands to perform every individual change to the membership. Handoff when a member is decommissioned or replaced also uses a naive strategy for selecting a replacement member.

This ticket enables the CMS membership to be configured more declaratively, by specifying the desired number of CMS members per-DC, much like replication using {{{}NetworkTopologyStrategy{}}}. 
Cassandra itself is responsible for selecting exactly which nodes should be CMS members based on this configuration and will attempt to maintain rack diversity if possible, with handoff of CMS membership handled in the same way.

When making changes to membership Cassandra will manage the entire transition from the starting state to end state using an in-progress sequence to compose the specific additions and removal operations.

Implementation can be found in:
[https://github.com/beobal/cassandra/commit/bcbb3f441d9df10207be38066c9752a08d62545b]

CI summary results attached. For right now, we've only been foccussing on unit tests, in-jvm & python dtests, so the failures in other suites are to be expected. Unfortunately, the archive containing the detailed results is too large to attach as a single file, so will look into the best way to work around that.
The intention is to fix all suites before merging the feature branch.

 
||Suite||Total||Passed||Skipped||Failed||Errors||
|jvm dtest|1508|1465|24|8|11|
|python dtest|1083|790|262|28|3|
|unit|12082|12006|61|6|9|"
CASSANDRA-18976,Repository repodata file does not contain all versions.,"The repomd.xml files inside the noboolean/repodata/ folders are only pointing to the latest version rpms, this way we are unable to install using yum from previous versions for Centos7.

The repomd.xml of the main repository is the only one that has all the RPM packages, but in such a way that it is impossible to differentiate versions for CentOS8 and CentOS7.
{code:java}

vi /etc/yum.repos.d/cassandra.repo
[cassandra]
name=Apache Cassandra
baseurl=https://redhat.cassandra.apache.org/40x/noboolean/
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://downloads.apache.org/cassandra/KEYS

 {code}
{code:java}
# yum list --showduplicate cassandra\*
Loaded plugins: fastestmirror, ovl
Loading mirror speeds from cached hostfile
 * base: mirrors.up.pt
 * extras: mirrors.up.pt
 * updates: mirrors.up.pt
Available Packages
cassandra.noarch                                          4.0.11-1                       cassandra
cassandra-tools.noarch                                    4.0.11-1                       cassandra
 {code}
If I try one of the old versions:
{code:java}
# yum install cassandra-4.0.5-1
Loaded plugins: fastestmirror, ovl
Loading mirror speeds from cached hostfile
 * base: mirrors.up.pt
 * extras: mirrors.up.pt
 * updates: mirrors.up.pt
No package cassandra-4.0.5-1 available.
Error: Nothing to do
 {code}
 
Changing to main 40x repo folder:
{code:java}
vi /etc/yum.repos.d/cassandra.repo

[cassandra]
name=Apache Cassandra
baseurl=https://redhat.cassandra.apache.org/40x/
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://downloads.apache.org/cassandra/KEYS {code}
Same results:
{code:java}
# yum list --showduplicate cassandra\*
Loaded plugins: fastestmirror, ovl
Loading mirror speeds from cached hostfile
 * base: mirror.cloudhosting.lv
 * extras: mirror.cloudhosting.lv
 * updates: mirrors.up.pt
Available Packages
cassandra.noarch                                                                                                                                                 4.0.11-1                                                                                                                                            cassandracassandra-tools.noarch                                                                                                                                           4.0.11-1                                                                                                                                            cassandra
 {code}
 
[https://apache.jfrog.io/artifactory/cassandra-rpm/noboolean/|http://example.com] Has no repodata files.
 
And
 
https://apache.jfrog.io/artifactory/cassandra-rpm/repodata/repomd.xml.asc: [Errno 14] HTTPS Error 404 - Not Found


 
 
 "
CASSANDRA-18968,StartupClusterConnectivityChecker fails on upgrade from 3.X,"Starting up a new 4.X node on a 3.x cluster throws the following warning:

{noformat}
WARN  [main] 2023-10-27 15:58:22,234 StartupClusterConnectivityChecker.java:183 - Timed out after 10002 milliseconds, was waiting for remaining peers to connect: {dc1=[X.Y.Z.W, A.B.C.D]}
{noformat}

I think this is because the PING messages used by the startup check are not available on 3.X.

To provide a smoother upgrade experience we should probably disable this check on a mixed version clusters, or skip peers on versions < 4.x when doing the connectivity check."
CASSANDRA-18964,Vector type in UDTs can break schema loading ,"{{RawVector}} doesn’t override {{referencesUserType(String name)}}, which causes {{Types.RawBuilder.build()}} to not populate UDT dependency DAG correctly. Then depending on natural iteration order or the vertices map,  a UDT with a vector referencing another UDT can fail to resolve, if that referenced UDT has not been resolved."
CASSANDRA-18963,Test Failure: sstableutil_test.TestSSTableUtil,"Seems flaky [https://app.circleci.com/pipelines/github/driftx/cassandra/1342/workflows/9554c7b5-dd60-4f08-8db5-c954febc8ad6/jobs/58957/tests]
 * 
h4. test_abortedcompaction

sstableutil_test.TestSSTableUtil 
 
{code:java}
self = <sstableutil_test.TestSSTableUtil object at 0x7fc099ecea58> def test_abortedcompaction(self): """""" @jira_ticket CASSANDRA-7066 @jira_ticket CASSANDRA-11497 Check that we can cleanup temporary files after a compaction is aborted. """""" log_file_name = 'debug.log' cluster = self.cluster cluster.populate(1).start() node = cluster.nodelist()[0] numrecords = 250000 self._create_data(node, KeyspaceName, TableName, numrecords) finalfiles, tmpfiles = self._check_files(node, KeyspaceName, TableName) assert len(finalfiles) > 0, ""Expected to find some final files"" assert 0 == len(tmpfiles), ""Expected no tmp files"" t = InterruptCompaction(node, TableName, filename=log_file_name, delay=2) t.start() try: logger.debug(""Compacting..."") node.compact() except ToolError: pass # expected to fail t.join() finalfiles = _normcase_all(self._invoke_sstableutil(KeyspaceName, TableName, type='final')) tmpfiles = _normcase_all(self._invoke_sstableutil(KeyspaceName, TableName, type='tmp')) # In most cases we should end up with some temporary files to clean up, but it may happen # that no temporary files are created if compaction finishes too early or starts too late # see CASSANDRA-11497 logger.debug(""Got {} final files and {} tmp files after compaction was interrupted"" .format(len(finalfiles), len(tmpfiles))) self._invoke_sstableutil(KeyspaceName, TableName, cleanup=True) self._check_files(node, KeyspaceName, TableName, finalfiles, []) # restart to make sure not data is lost logger.debug(""Restarting node..."") > node.start(wait_for_binary_proto=True) sstableutil_test.py:97: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ../env3.6/lib/python3.6/site-packages/ccmlib/node.py:914: in start self.wait_for_binary_interface(from_mark=self.mark) ../env3.6/lib/python3.6/site-packages/ccmlib/node.py:702: in wait_for_binary_interface self.watch_log_for(""Starting listening for CQL clients"", **kwargs) ../env3.6/lib/python3.6/site-packages/ccmlib/node.py:599: in watch_log_for self.raise_node_error_if_cassandra_process_is_terminated() _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = <ccmlib.node.Node object at 0x7fc08a583be0> def raise_node_error_if_cassandra_process_is_terminated(self): if not self._is_pid_running(): msg = ""C* process with {pid} is terminated"".format(pid=self.pid) common.debug(msg) > raise NodeError(msg) E ccmlib.node.NodeError: C* process with 19530 is terminated ../env3.6/lib/python3.6/site-packages/ccmlib/node.py:683: NodeError{code}
{code:java}
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [SSTableBatchOpen:1] 2023-10-19 22:25:07,449 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""\norg.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-4-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:129)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:79)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)\n\t... 10 common frames omitted', [node1] 'ERROR [SSTableBatchOpen:4] 2023-10-19 22:25:07,449 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""\norg.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:129)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:79)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)\n\t... 10 common frames omitted', [node1] 'ERROR [SSTableBatchOpen:4] 2023-10-19 22:25:07,450 SSTableReader.java:419 - Corrupt sstable /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-5-big=[Index.db, Data.db]; skipping table\norg.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:129)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:79)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)\n\t... 10 common frames omitted']"" Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [SSTableBatchOpen:1] 2023-10-19 22:25:07,449 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""\norg.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-4-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:129)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:79)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)\n\t... 10 common frames omitted', [node1] 'ERROR [SSTableBatchOpen:4] 2023-10-19 22:25:07,449 DefaultFSErrorHandler.java:129 - Exiting forcefully due to file system exception on startup, disk failure policy ""stop""\norg.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:129)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:79)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)\n\t... 10 common frames omitted', [node1] 'ERROR [SSTableBatchOpen:4] 2023-10-19 22:25:07,450 SSTableReader.java:419 - Corrupt sstable /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-5-big=[Index.db, Data.db]; skipping table\norg.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/dtest-ahz16wrh/test/node1/data0/system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/nc-5-big\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:353)\n\tat org.apache.cassandra.io.sstable.format.SSTableReader.lambda$openAll$4(SSTableReader.java:414)\n\tat org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)\n\tat org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)\n\tat org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: null\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:129)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:79)\n\tat org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)\n\tat org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)\n\t... 10 common frames omitted']{code}"
CASSANDRA-18962,Add retries to IR messages,"With CASSANDRA-18816 we now have repair message retries for full and preview repair, but IR was left out as message processing was handled differently, this patch is to extend that work to include IR messages!

Out of scope of this patch was making the status checks deal with retries as we retry anyways every hour or so… "
CASSANDRA-18960,Upgrade Python driver to 3.28.0,"The version of the Python driver that is used by cqlsh (3.25.0) doesn't entirely support the new vector data type introduced by CASSANDRA-18504. While we can perfectly write data, read vectors are presented as blobs:
{code}
> CREATE TABLE t (k int PRIMARY KEY, v vector<int, 2>);
> INSERT INTO t(k, v) VALUES (0, [1, 2]);
> SELECT * FROM t;

 k | v
---+-------------------------------------
 0 | b'\x00\x00\x00\x01\x00\x00\x00\x02'
{code}
I think that would we fixed if we update the driver to (at least) 3.28.0, which includes support for the new vector data type."
CASSANDRA-18959,Remove deprecated code in Cassandra 1.x and 2.x for 5.0,This ticket tracks removal of deprecations which were added in Cassandra 1.x and 2.x for Cassandra 5 branch.
CASSANDRA-18957,BLOG - Apache Cassandra 5.0 Features: Unified Compaction Strategy,"This ticket is to capture the work associated with publishing the blog ""Apache Cassandra 5.0 Features: Unified Compaction Strategy""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(October 24)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18955,TableHistograms output a lot of repeated information,"This patch aims to fix two issues:

1. the original output of nodetool tablehistograms with no tables specified is wrong , the output will contains many repeated informations. See the picture below.

The reason is that for [tablesList.keys()|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java#L80] will give a Multiset output as tablesList is a Multimap.  

2. as we have DISCUSS for ""Change the useage of nodetool tablehistograms"" , for arguments more than two, the output of tablehistograms will be all of the tables, it's a bug that we should fix. We can see that the [description|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java#L42] of tablehistograms is ,out put the statistic histograms for the given table. and the format of the argument is like [tthis|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java#L45]

"
CASSANDRA-18952,Add metrics and logging to repair retries,In CASSANDRA-18816 we added repair message retries but forgot to add logging and metrics to actually let operators know repairs happened and how often… we should add such visibility to help operators know how best to tune it for their environment.
CASSANDRA-18951,Add option for MutualTlsAuthenticator to restrict the certificate validity period,"In {{org.apache.cassandra.auth.MutualTlsAuthenticator}}, we validate that a certificate is valid by looking at the identities inside the
certificate and making sure the identity exists in the identity to role table.

In some situations we may want to restrict the certificates
we accept by rejecting certificates older than x amount of days. Some certificates can be generated with long expiration dates,
and this might be undesired when you want to protect against potential certificates being compromised. For that reason, it is
important to add an option, that when configured, we can limit the age of the certificate we accept for mTLS authentication.

When enabled, this will force clients to have to renew certificates more frequently, reducing the exposure of a Cassandra cluster
to leaked certificates."
CASSANDRA-18949,Test failure: org.apache.cassandra.tools.nodetool.ClearSnapshotTest.testClearSnapshot_RemoveByName-.jdk11.arch=x86_64.python2.7,"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1747/testReport/org.apache.cassandra.tools.nodetool/ClearSnapshotTest/testClearSnapshot_RemoveByName__jdk11_arch_x86_64_python2_7/]
h3.  
{code:java}
Error Message
[bin/nodetool, -p, 36753, -h, 127.0.0.1, snapshot, -t, some-name] exited with code 2 stderr: error: Unknown keyspace keyspace_04 -- StackTrace -- java.lang.AssertionError: Unknown keyspace keyspace_04 at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324) at org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162) at org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105) at org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251) at org.apache.cassandra.db.Keyspace.open(Keyspace.java:162) at org.apache.cassandra.db.Keyspace.open(Keyspace.java:151) at com.google.common.collect.Iterators$6.transform(Iterators.java:828) at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:52) at org.apache.cassandra.service.StorageService.takeSnapshot(StorageService.java:4356) at org.apache.cassandra.service.StorageService.takeSnapshot(StorageService.java:4221) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71) at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:260) at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112) at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46) at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237) at java.management/com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138) at java.management/com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:809) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1466) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1399) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:827) at java.base/jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:359) at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200) at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196) at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:562) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:796) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:677) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:676) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:829) stdout: Requested creating snapshot(s) for [all keyspaces] with snapshot name [some-name] and options {skipFlush=false}

Stacktrace
junit.framework.AssertionFailedError: [bin/nodetool, -p, 36753, -h, 127.0.0.1, snapshot, -t, some-name] exited with code 2 stderr: error: Unknown keyspace keyspace_04 -- StackTrace -- java.lang.AssertionError: Unknown keyspace keyspace_04 at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324) at org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162) at org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105) at org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251) at org.apache.cassandra.db.Keyspace.open(Keyspace.java:162) at org.apache.cassandra.db.Keyspace.open(Keyspace.java:151) at com.google.common.collect.Iterators$6.transform(Iterators.java:828) at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:52) at org.apache.cassandra.service.StorageService.takeSnapshot(StorageService.java:4356) at org.apache.cassandra.service.StorageService.takeSnapshot(StorageService.java:4221) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112) at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46) at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237) at java.management/com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138) at java.management/com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:809) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1466) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1399) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:827) at java.base/jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:359) at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200) at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196) at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:562) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:796) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:677) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:676) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at java.base/java.lang.Thread.run(Thread.java:829) stdout: Requested creating snapshot(s) for [all keyspaces] with snapshot name [some-name] and options {skipFlush=false} at org.apache.cassandra.tools.ToolRunner$ToolResult.assertExitCode(ToolRunner.java:445) at org.apache.cassandra.tools.ToolRunner$ToolResult.assertOnExitCode(ToolRunner.java:439) at org.apache.cassandra.tools.ToolRunner$ToolResult.assertOnCleanExit(ToolRunner.java:492) at org.apache.cassandra.tools.ToolRunner$ToolResult.assertOnCleanExit(ToolRunner.java:487) at org.apache.cassandra.tools.nodetool.ClearSnapshotTest.testClearSnapshot_RemoveByName(ClearSnapshotTest.java:78) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-18948,Test failure: org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testReplayLogic-compression.jdk17.arch=x86_64.python2.7 ,"Seen here:

https://ci-cassandra.apache.org/job/Cassandra-5.0/71/testReport/org.apache.cassandra.db.commitlog/CommitLogSegmentManagerCDCTest/testReplayLogic_compression_jdk17_arch_x86_64_python2_7/
h3.  
{code:java}
Error Message
Missing old CDCIndexData in new set after replay: CommitLog-7-1697673230997_cdc.idx,1747785 List of CDCIndexData in new set of indexes after replay: CommitLog-7-1697673230996_cdc.idx,3510561 CommitLog-7-1697673230998_cdc.idx,3509214

Stacktrace
junit.framework.AssertionFailedError: Missing old CDCIndexData in new set after replay: CommitLog-7-1697673230997_cdc.idx,1747785 List of CDCIndexData in new set of indexes after replay: CommitLog-7-1697673230996_cdc.idx,3510561 CommitLog-7-1697673230998_cdc.idx,3509214 at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerCDCTest.testReplayLogic(CommitLogSegmentManagerCDCTest.java:319) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-18947,Test failure: dtest-novnode.disk_balance_test.TestDiskBalance.test_disk_balance_stress,"Seen here:

https://ci-cassandra.apache.org/job/Cassandra-5.0/72/testReport/dtest-novnode.disk_balance_test/TestDiskBalance/test_disk_balance_stress/
h3.  
{code:java}
Error Message
AssertionError: values not within 10.00% of the max: (2534183, 2762123, 2423706) (node1)

Stacktrace
self = <disk_balance_test.TestDiskBalance object at 0x7f8fae9b7590> def test_disk_balance_stress(self): cluster = self.cluster if self.dtest_config.use_vnodes: cluster.set_configuration_options(values={'num_tokens': 256}) cluster.populate(4).start() node1 = cluster.nodes['node1'] node1.stress(['write', 'n=50k', 'no-warmup', '-rate', 'threads=100', '-schema', 'replication(factor=3)', 'compaction(strategy=SizeTieredCompactionStrategy,enabled=false)']) cluster.flush() # make sure the data directories are balanced: for node in cluster.nodelist(): > self.assert_balanced(node) disk_balance_test.py:48: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ disk_balance_test.py:186: in assert_balanced assert_almost_equal(*new_sums, error=0.1, error_message=node.name) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ args = (2534183, 2762123, 2423706) kwargs = {'error': 0.1, 'error_message': 'node1'}, error = 0.1, vmax = 2762123 vmin = 2423706, error_message = 'node1' def assert_almost_equal(*args, **kwargs): """""" Assert variable number of arguments all fall within a margin of error. @params *args variable number of numerical arguments to check @params error Optional margin of error. Default 0.16 @params error_message Optional error message to print. Default '' Examples: assert_almost_equal(sizes[2], init_size) assert_almost_equal(ttl_session1, ttl_session2[0][0], error=0.005) """""" error = kwargs['error'] if 'error' in kwargs else 0.16 vmax = max(args) vmin = min(args) error_message = '' if 'error_message' not in kwargs else kwargs['error_message'] assert vmin > vmax * (1.0 - error) or vmin == vmax, \ > ""values not within {:.2f}% of the max: {} ({})"".format(error * 100, args, error_message) E AssertionError: values not within 10.00% of the max: (2534183, 2762123, 2423706) (node1) tools/assertions.py:206: AssertionError
{code}
 "
CASSANDRA-18946,Add cqlsh autocompletion for the vector data type,"Add cqlsh autocompletion for the vector data type, so we have completions such as:
{code:java}
cqlsh> CREATE TABLE t (v
ascii     boolean   decimal   float     int       set       time      tinyint   varint
bigint    counter   double    frozen    list      smallint  timestamp uuid      vector
blob      date      duration  inet      map       text      timeuuid  varchar

cqlsh> CREATE TABLE t (v vector<
ascii     boolean   decimal   float     int       set       time      tinyint   varint
bigint    counter   double    frozen    list      smallint  timestamp uuid      vector
blob      date      duration  inet      map       text      timeuuid  varchar

cqlsh> CREATE TABLE t (v vector<float,
              <wholenumber>

cqlsh> INSERT INTO t(k, v) VALUES ( 0,
                                 <value for v (vector<float, 3>)>
{code}"
CASSANDRA-18945,Unified Compaction Strategy is creating too many sstables,"The unified compaction strategy currently aims to create sstables with close to the same size, defaulting to 1 GiB. Unfortunately tests show that Cassandra starts to have performance problems when the number of sstables grows to the order of a thousand, and in particular that even 1 TiB of data with the default configuration is creating too many sstables for efficient processing. This matters even more for SAI, where the number of sstables in the system can have a proportional effect on the complexity of operations.

It is quite easy to create a configuration option that allows sstables to take some part of the data growth by adding a multiplier to [the shard count calculation|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/UnifiedCompactionStrategy.md#sharding] formula, replacing 
{{2 ^ round(log2(d / (t * b))) * b}} 
with 
{{2 ^ round((1 - 𝜆) * log2(d / (t * b))) * b}}, 
where 𝜆 is a parameter whose value is between 0 and 1.

With this, a 𝜆 of 0.5 would mean that shard count and sstable size grow in parallel at the square root of the data size growth. 0 would result in no growth, and 1 in always using the same number of shards.

It may also be valuable to introduce a threshold for engaging the base shard count to avoid splitting lowest-level sstables into fragments that are too small.

Once both of these are in place, we can set defaults that better suit all node densities, including 10 TiB and beyond, for example:
 - target size of 1 GiB
 - 𝜆 of 1/3
 - base shard count of 4
 - minimum size 100 MiB"
CASSANDRA-18944,Test failure: org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest,"The simulator test {{org.apache.cassandra.simulator.test.ShortPaxosSimulationTest#simulationTest}} is flaky on 5.0 and trunk:
* https://app.circleci.com/pipelines/github/mike-tr-adamson/cassandra/332/workflows/946e28f4-2dec-4384-ac38-de011093f6c6/jobs/25735/tests
* https://app.circleci.com/pipelines/github/adelapena/cassandra/3253/workflows/e48b49e9-cf36-412a-a811-d813031e6f01/jobs/83735/tests
* https://app.circleci.com/pipelines/github/adelapena/cassandra/3254/workflows/69f451ef-fb39-48e4-b1d1-40ee4141b0c1/jobs/83739/tests

{code}
org.apache.cassandra.simulator.SimulationException: Failed on seed 0xb01206bb3b021127
Caused by: java.lang.ClassCastException: class org.apache.cassandra.net.NoPayload cannot be cast to class org.apache.cassandra.gms.GossipShutdown (org.apache.cassandra.net.NoPayload and org.apache.cassandra.gms.GossipShutdown are in unnamed module of loader org.apache.cassandra.distributed.shared.InstanceClassLoader @68801feb)
	at org.apache.cassandra.gms.GossipShutdown$Serializer.serialize(GossipShutdown.java:41)
	at org.apache.cassandra.net.Message$Serializer.serialize(Message.java:722)
	at org.apache.cassandra.distributed.impl.Instance.serializeMessage(Instance.java:427)
	at org.apache.cassandra.distributed.impl.Instance.lambda$registerOutboundFilter$5(Instance.java:370)
	at org.apache.cassandra.net.OutboundSink$Filtered.accept(OutboundSink.java:54)
	at org.apache.cassandra.net.OutboundSink.accept(OutboundSink.java:70)
	at org.apache.cassandra.net.MessagingService.send(MessagingService.java:449)
	at org.apache.cassandra.net.MessagingService.send(MessagingService.java:419)
	at org.apache.cassandra.gms.Gossiper.unsafeSendShutdown(Gossiper.java:2634)
	at org.apache.cassandra.simulator.cluster.OnInstanceSendShutdown.lambda$invokableSendShutdown$1(OnInstanceSendShutdown.java:48)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.SyncFutureTask$1.call(SyncFutureTask.java:46)
	at org.apache.cassandra.concurrent.SyncFutureTask.run(SyncFutureTask.java:68)
	at org.apache.cassandra.simulator.systems.InterceptingExecutor$AbstractSingleThreadedExecutorPlus.lambda$new$0(InterceptingExecutor.java:584)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}
The test failure can easily be reproduced on CircleCI with:
{code}
.circleci/generate.sh -sp \
  -e REPEATED_SIMULATOR_DTESTS=org.apache.cassandra.simulator.test.ShortPaxosSimulationTest \
  -e REPEATED_SIMULATOR_DTESTS_COUNT=100
{code}
Flakiness seems ~18%.

The test failure is not reported on Butler because simulator tests are not run by Jenkins."
CASSANDRA-18943,http/2 vulnerability: CVE-2023-44487,"https://nvd.nist.gov/vuln/detail/CVE-2023-44487

Basically anything using http/2 is covered by this, but we don't use it."
CASSANDRA-18941,Support max SSTable size in sorted CQLSSTableWriter,"The CQLSSTableWriter can produce a series of SSTables with a bounded size in the unsorted mode. The functionality is missing in the sorted CQLSSTableWriter.

It will be great to bringing the parity to the sorted mode, so that it is able to produce a series of size-bounded SSTable, instead of a single but giant one.

Unlike the unsorted CQLSSTableWriter, the max SSTable size in the sorted writer does not require buffering."
CASSANDRA-18940,SAI post-filtering reads don't update local table latency metrics,"Once an SAI index finds matches (primary keys), it reads the associated rows and post-filters them to incorporate partial writes, tombstones, etc. However, those reads are not currently updating the local table latency metrics. It should be simple enough to attach a metrics recording transformation to the iterator produced by querying local storage."
CASSANDRA-18939,Creating a SASI index after creating an SAI index breaks secondary index queries,"In the 5.0 branch, HEAD is e45c1092f91edd63591f562b2120ea6a5fd3edd5, I was able to break secondary indexes by doing the following:

{code}
cqlsh -e ""create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};""
cqlsh -e ""create table test.blah (id int primary key, val text);""
cqlsh -e ""create INDEX on test.blah (val) using 'sai';""

bin/nodetool flush

❯ cqlsh -e ""SELECT * from test.blah WHERE val = 'something';""

 id | val
----+-----

(0 rows)

cqlsh -e ""create custom INDEX on test.blah (val) using 'org.apache.cassandra.index.sasi.SASIIndex';""

Warnings :
SASI indexes are experimental and are not recommended for production use.

cqlsh -e ""SELECT * from test.blah WHERE val = 'something';""
<stdin>:1:ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures: UNKNOWN from localhost/127.0.0.1:7000"" info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'failures': 1, 'error_code_map': {'127.0.0.1': '0x0000'}}
{code}

Server throws an exception:
{code}
ERROR [ReadStage-2] 2023-10-18 12:09:42,391 JVMStabilityInspector.java:70 - Exception in thread Thread[ReadStage-2,10,SharedPool]
java.lang.RuntimeException: java.lang.ClassCastException: class org.apache.cassandra.index.sai.StorageAttachedIndex cannot be cast to class org.apache.cassandra.index.sasi.SASIIndex (org.apache.cassandra.index.sai.StorageAttachedIndex and org.apache.cassandra.index.sasi.SASIIndex are in unnamed module of loader 'app')
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2585)
	at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ClassCastException: class org.apache.cassandra.index.sai.StorageAttachedIndex cannot be cast to class org.apache.cassandra.index.sasi.SASIIndex (org.apache.cassandra.index.sai.StorageAttachedIndex and org.apache.cassandra.index.sasi.SASIIndex are in unnamed module of loader 'app')
	at org.apache.cassandra.index.sasi.plan.QueryController.getIndex(QueryController.java:96)
	at org.apache.cassandra.index.sasi.plan.Operation.analyzeGroup(Operation.java:282)
	at org.apache.cassandra.index.sasi.plan.Operation$Builder.complete(Operation.java:433)
	at org.apache.cassandra.index.sasi.plan.SASIIndexSearcher.analyze(SASIIndexSearcher.java:65)
	at org.apache.cassandra.index.sasi.plan.SASIIndexSearcher.search(SASIIndexSearcher.java:77)
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:425)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2184)
	at org.apache.cassandra.service
{code}

Dropping the SASI index restores correct behavior.
"
CASSANDRA-18937,Two accord transactions have the exact same transaction id,"When testing solutions for CASSANDRA-18798 I noticed that two independent transactions running at the same time in two parallel threads ended up having the exact same transaction id:


{code}
    public void testListAddition() throws Exception
    {
        SHARED_CLUSTER.schemaChange(""CREATE TABLE "" + currentTable + "" (k int PRIMARY KEY, l list<int>)"");
        SHARED_CLUSTER.forEach(node -> node.runOnInstance(() -> AccordService.instance().setCacheSize(0)));

        CountDownLatch latch = CountDownLatch.newCountDownLatch(1);

        Vector<Integer> completionOrder = new Vector<>();
        try
        {
            for (int i=0; i<100; i++)
            {

                ForkJoinTask<?> add1 = ForkJoinPool.commonPool().submit(() -> {
                    latch.awaitThrowUncheckedOnInterrupt();
                    SHARED_CLUSTER.get(1).executeInternal(""BEGIN TRANSACTION "" +
                            ""UPDATE "" + currentTable + "" SET l = l + [1] WHERE k = 1; "" +
                            ""COMMIT TRANSACTION"");
                    completionOrder.add(1);
                });

                ForkJoinTask<?> add2 = ForkJoinPool.commonPool().submit(() -> {
                    try {
                        Thread.sleep(0);
{code}

Adding some logging in TxnWrite.java reveals the two threads ave identical executeAt and unix timestamps:

{noformat}
lastmicros 0
DEBUG [node2_Messaging-EventLoop-3-4] node2 2023-10-18 18:26:08,954 AccordVerbHandler.java:54 - Receiving Apply{kind:Minimal, txnId:[10,1697642767659000,10,1], deps:[distributed_test_keyspace:[(-Inf,-1], (-1,9223372036854775805], (9223372036854775805,+Inf]]]:{}, {}, executeAt:[10,1697642767659000,10,1], writes:TxnWrites{executeAt:[10,1697642767659000,10,1], keys:[distributed_test_keyspace:DecoratedKey(-4069959284402364209, 00000001)], write:TxnWrite{}}, result:accord.api.Result$1@253c102e} from /127.0.0.1:7012
raw 0  (NO_LAST_EXECUTED_HLC=-9223372036854775808
lastExecutedTimestamp [0,0,0,0]
lastmicros 1697642767659000
raw -9223372036854775808  (NO_LAST_EXECUTED_HLC=-9223372036854775808
lastExecutedTimestamp [10,1697642767659000,10,1]
DEBUG [node2_CommandStore[1]:1] node2 2023-10-18 18:26:09,023 AccordMessageSink.java:167 - Replying ACCORD_APPLY_RSP ApplyApplied to /127.0.0.1:7012
lastmicros 0
raw 0  (NO_LAST_EXECUTED_HLC=-9223372036854775808
lastExecutedTimestamp [0,0,0,0]
lastmicros 1697642767659000
raw -9223372036854775808  (NO_LAST_EXECUTED_HLC=-9223372036854775808
lastExecutedTimestamp [10,1697642767659000,10,1]
timestamp 1697642767659000    executeAt[10,1697642767659000,10,1]
timestamp 1697642767659000    executeAt[10,1697642767659000,10,1]
{noformat}


Increasing the Thread.sleep() to9 or 10 helps so that the transactions have different IDs."
CASSANDRA-18936,CI jvm-dtest-upgrade breakage,"git clean failing when reusing the dtest sub clone.
ref: https://the-asf.slack.com/archives/CK23JSY2K/p1697465832946299 "
CASSANDRA-18935,Fix nodetool enable/disablebinary to correctly set rpc," 
{code:java}
        if ((nativeFlag != null && Boolean.parseBoolean(nativeFlag)) || (nativeFlag == null && DatabaseDescriptor.startNativeTransport()))
        {
            startNativeTransport();
            StorageService.instance.setRpcReady(true);
        } {code}
The startup code here only sets RpcReady if native transport is enabled. If you call 
{code:java}
nodetool enablebinary{code}
then this flag doesn't get set.

But with the change from CASSANDRA-13043 it requires RpcReady set to true in order to get a leader for the counter update.

Not sure what the correct fix is here, seems to only really use this flag for counters. So thinking perhaps the fix is to just move this outside the if condition.

 "
CASSANDRA-18933,Correct comment for nc SSTable format,"In [CASSANDRA-18134 |https://issues.apache.org/jira/browse/CASSANDRA-18134], the {{nc}} SSTable format was introduced. The patch was merged into {{5.0+}}, however the comment in source incorrectly mentions that the format was added to version {{4.1}}."
CASSANDRA-18932,Harry-found CorruptSSTableException / RT Closer issue when reading entire partition,"While testing some new machinery for Harry, I have encountered a new RT closer / SSTable Corruption issue. I have grounds to believe this was introduced during the last year.

Issue seems to happen because of intricate interleaving of flushes with writes and deletes.
{code:java}
ERROR [ReadStage-2] 2023-10-16 18:47:06,696 JVMStabilityInspector.java:76 - Exception in thread Thread[ReadStage-2,5,SharedPool]
org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: RandomAccessReader:BufferManagingRebufferer.Aligned:CompressedChunkReader.Mmap(/Users/ifesdjeen/foss/java/apache-cassandra-4.0/data/data1/harry/table_1-07c35a606c0a11eeae7a4f6ca489eb0c/nc-5-big-Data.db - LZ4Compressor, chunk length 16384, data length 232569)
        at org.apache.cassandra.io.sstable.AbstractSSTableIterator$AbstractReader.hasNext(AbstractSSTableIterator.java:381)
        at org.apache.cassandra.io.sstable.AbstractSSTableIterator.hasNext(AbstractSSTableIterator.java:242)
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95)
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:376)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:188)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:157)
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:534)
        at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:402)
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95)
        at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:151)
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:101)
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:86)
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:343)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:201)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:186)
        at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:48)
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:346)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2186)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2581)
        at org.apache.cassandra.concurrent.ExecutionFailure$2.run(ExecutionFailure.java:163)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
        Suppressed: java.lang.IllegalStateException: PROCESSED UnfilteredRowIterator for harry.table_1 (key: ZinzDdUuABgDknItABgDknItABgDknItXEFrgBnOmPmPylWrwXHqjBHgeQrGfnZd1124124583:ZinzDdUuABgDknItABgDknItABgDknItABgDknItABgDknItzHqchghqCXLhVYKM22215251:3.2758E-41 omdt: [deletedAt=564416, localDeletion=1697450085]) has an illegal RT bounds sequence: expected all RTs to be closed, but the last one is open
                at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:117)
                at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.onPartitionClose(RTBoundValidator.java:112)
                at org.apache.cassandra.db.transform.BaseRows.runOnClose(BaseRows.java:91)
                at org.apache.cassandra.db.transform.BaseIterator.close(BaseIterator.java:95)
                at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:341)
                ... 10 common frames omitted
Caused by: java.io.IOException: Invalid Columns subset bytes; too many bits set:10
        at org.apache.cassandra.db.Columns$Serializer.deserializeSubset(Columns.java:578)
        at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeRowBody(UnfilteredSerializer.java:604)
        at org.apache.cassandra.db.UnfilteredDeserializer.readNext(UnfilteredDeserializer.java:143)
        at org.apache.cassandra.io.sstable.format.big.SSTableIterator$ForwardIndexedReader.computeNext(SSTableIterator.java:175)
        at org.apache.cassandra.io.sstable.AbstractSSTableIterator$ForwardReader.hasNextInternal(AbstractSSTableIterator.java:533)
        at org.apache.cassandra.io.sstable.AbstractSSTableIterator$AbstractReader.hasNext(AbstractSSTableIterator.java:368)
        ... 31 common frames omitted {code}
 

Unfortunately, harry branch is not ready for release yet. That said, I have a snapshot pinned for quick repro and can share SSTables that will easily repro the issue if there are any takers.

To reproduce:
{code:java}
paging 1; # make sure to set page size to 1 (it breaks with other page sizes, too)
select * from harry.table_1;
{code}
Please also make sure to modify snitch to set rack/dc:
{code:java}
+    public static final String DATA_CENTER_NAME = ""datacenter0"";
+    public static final String RACK_NAME = ""rack0"";
{code}
And set directories in cassandra.yaml: 
{code:java}
+data_file_directories:
+ - /your/path/data/data0
+ - /your/path/data/data1
+ -/your/path/data/data2{code}
SHA for repro: 865d7c30e4755e74c4e4d26205a7aed4cfb55710"
CASSANDRA-18929,CEP-15: (C*) Implement TopologySorter to prioritise hosts based on DynamicSnitch and/or topology layout,Implement TopologySorter to prioritise hosts based on DynamicSnitch and/or topology layout
CASSANDRA-18928,Simplify handling of Insufficient replies from Commit and Apply,"Remove the use of Defer for Commit, and reply with Maximal Apply to Insufficient Apply responses"
CASSANDRA-18927,Fix potential race condition in IndexViewManager during invalidation,"There is a potential race condition in the {{IndexViewManager.invalidate}} method:
{code:java}
public void invalidate()
{
    View currentView = view.get();

    for (SSTableIndex index : currentView)
    {
        index.markObsolete();
    }

    view.set(new View(context, Collections.emptyList()));
} {code}
We should {{getAndSet}} the view before marking the indexes as obsolete. This would avoid indexes potentially being made obsolete when being accessed. "
CASSANDRA-18926,SAI in-memory index does not include maximum term size check when adding terms,"The {{SSTableIndexWriter}} rejects terms that exceed a maximum term size with a no-spam warning, but the {{TrieMemoryIndex}} does not do this. 

We should check term sizes when rows are added and issue client warnings when this happens. This still needs to happen in the {{SSTableIndexWriter}} to handle terms during an initial index build. "
CASSANDRA-18924,TCM: Allow unknown nodes during discovery,"      * avoid discovered.addAll(DatabaseDescriptor.getSeeds()) when starting discovery to exclude them from the final result
      * add responded node to discovered set, even if it responds with an empty set
      * Implement a simple simulation for discovery that does not involve setting up entire clusters
      * Allow _any_ seed to start up first"
CASSANDRA-18923,BLOG - Apache Cassandra 5.0 Features: Dynamic Data Masking,"This ticket is to capture the work associated with publishing the blog ""Apache Cassandra 5.0 Features: Dynamic Data Masking""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(October 11)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18922,cassandra-driver-core-3.11.5 vulnerability: CVE-2023-4586,"This is failing OWASP: https://nvd.nist.gov/vuln/detail/CVE-2023-4586

but appears to be a false positive."
CASSANDRA-18921,Describe statement may include inconsistent schema and schema version for paging,"When the {{DescribeStatement}} is executed, it initially gets the current schema snapshot and the schema version with two separate unsynchronized calls. When the schema is being modified around that time, the schema snapshot and schema version may diverge which will result in failing or inconsistent paging. This is not super important but it is easy to fix.
"
CASSANDRA-18920,Add jsonl support to sstabledump,"Add a new option to sstabledump to allow the generation of jsonl format.

The -l option for sstabledump does not produce valid jsonl.

[https://jsonlines.org/validator/]

Valid jsonl removes whitespace, including newlines, to make the format compact.  

By adding an option to generate valid jsonl, jsonl can be generated that is smaller.

Smaller output is better for execution time, storage, and transport. 

jsonl also allows the splitting of a file for processing at a known good line for processing without parsing the file, as each row is a line.

 "
CASSANDRA-18918,Test failure: secondary_indexes_test.TestPreJoinCallback.test_resume,"As seen here: https://ci-cassandra.apache.org/job/Cassandra-5.0/60/testReport/dtest-novnode.secondary_indexes_test/TestPreJoinCallback/test_resume/

{quote}
failed on teardown with ""Failed: Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [Stream-Deserializer-/127.0.0.1:7000-ffd39504] 2023-10-09 16:12:59,153 StreamSession.java:702 - [Stream #b69e6f80-66be-11ee-b813-3bca425f16a7] Socket closed before session completion, peer 127.0.0.1:7000 is probably down.\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119)\n\tat org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:178)\n\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:49)\n\tat org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:59)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)']""
{quote}
"
CASSANDRA-18916,Start-Up Validation Should Log a Single Report,"In order to improve readability of the log, start-up validation should produce a complete report and emit it as a single event upon completion"
CASSANDRA-18913,Gossip NPE due to shutdown event corrupting empty statuses,"When an instance either disables gossip or shuts down we send a gossip shutdown message, peers ignore it if the endpoint isn’t known, else it mutates its local copy of the state to mark shutdown…
When an instance restarts it populates gossip with the endpoints found in peers, but the state is empty (not null)

So, there is a fun timing bug…

* stop node1
* start node1; at this point all known endpoints before exist in gossip but are empty
* node2 shutdown (gossip shutdown or node, doesn’t matter)
* node1 sees the shutdown before gossip messages, and gets corruptted
* node3 tries to join the cluster, fails due to node1 being corrupted

There are 2 different patterns the NPE can happen with, in this example node1 and node3 will have different stack traces

{code}
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test
	Suppressed: java.lang.NullPointerException: Unable to get HOST_ID; HOST_ID is not defined, given EndpointState: HeartBeatState = HeartBeat: generation = 0, version = 2147483647, AppStateMap = {STATUS=Value(shutdown,true,37), RPC_READY=Value(false,38), STATUS_WITH_PORT=Value(shutdown,true,36)}
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1218)
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1208)
		at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:3279)
		at org.apache.cassandra.service.StorageService.onChange(StorageService.java:2756)
		at org.apache.cassandra.gms.Gossiper.markAsShutdown(Gossiper.java:611)
		at org.apache.cassandra.gms.GossipShutdownVerbHandler.doVerb(GossipShutdownVerbHandler.java:39)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)
	Suppressed: java.lang.NullPointerException: Unable to get HOST_ID; HOST_ID is not defined, given EndpointState: HeartBeatState = HeartBeat: generation = 0, version = 2147483647, AppStateMap = {STATUS=Value(shutdown,true,37), RPC_READY=Value(false,38), STATUS_WITH_PORT=Value(shutdown,true,36)}
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1218)
		at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:1208)
		at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:3279)
		at org.apache.cassandra.service.StorageService.onChange(StorageService.java:2756)
		at org.apache.cassandra.gms.Gossiper.doOnChangeNotifications(Gossiper.java:1762)
		at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:3793)
		at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1465)
		at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1678)
		at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
		at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78)

{code}"
CASSANDRA-18912,"Specify ""since"" in all Deprecated annotations","It would be great if we introduced in 5.0 a change in Deprecated annotations like this:

{code}
@Deprecated(since = ""4.0"")
{code}

or 

{code}
@Deprecated(since = ""3.11"")
{code}

The reasoning behind this is that as of now, it is pretty cumbersome to figure out what can be removed on the next major version. It has to be, basically, done manually every time.

There is also this parameter available:

{code}
@Deprecated(forRemoval = true / false)
{code}

which indicates whether the annotated element is subject to removal in a future version so we do not need to think about this every time if it is eligible for deletion in a next major or not.

We could then have a check which would ensure that we are not releasing a next major with some deprecations introduced two majors before."
CASSANDRA-18911,KeyCacheTest is failing with sstable_preemptive_open_interval < 0,Discovered by [~maedhroz] 
CASSANDRA-18910,Debian packaging broken by quilt?,"Something has changed in the docker image that is breaking the debian packaging in all versions, similar to this:

{quote}
dpkg-buildpackage: info: source package cassandra
dpkg-buildpackage: info: source version 4.1.4-20231004git486acc68f1
dpkg-buildpackage: info: source distribution unstable
dpkg-buildpackage: info: source changed by build <build@1518e06a5507>
dpkg-buildpackage: info: host architecture amd64
 dpkg-source --tar-ignore=.git --before-build .
 fakeroot debian/rules clean
QUILT_PATCHES=debian/patches \
        quilt --quiltrc /dev/null pop -a -R || test $? = 1
No patch removed
make: *** [/usr/share/quilt/quilt.make:23: unpatch] Error 1
dpkg-buildpackage: error: fakeroot debian/rules clean subprocess returned exit status 2
{quote}"
CASSANDRA-18909,[WEBSITE] Update the Metric Reporters documentation page ,"After CASSANDRA-18743 and CASSANDRA-14667 we need to update the website (documentation) information accordingly. The links to dropwizard metrics must be updated. The references to the metrics reporters should be added to the following page:
[https://cassandra.apache.org/doc/latest/cassandra/operating/metrics.html#metric-reporters]"
CASSANDRA-18907,Remove unnecessary reporter-config3 dependency,"After the CASSANDRA-14667 the dwopwizard metrics library was updated to the latest version, metrics reporters e.g. ConsoleReporter or CsvReporter that were shipped with the {{reporter-config3}} are no longer needed as they are part of the dropwizard itself. 

The project has been archived since the 2020 year and is no longer supported.
https://github.com/addthis/metrics-reporter-config

I think we can remove the following:
{code}
      <dependency>
        <groupId>com.addthis.metrics</groupId>
        <artifactId>reporter-config3</artifactId>
        <version>3.0.3</version>
        <exclusions>
          <exclusion>
            <artifactId>hibernate-validator</artifactId>
            <groupId>org.hibernate</groupId>
          </exclusion>
        </exclusions>
      </dependency>
{code}"
CASSANDRA-18906,Exclude unnecessary dependencies introduced by caffeine update to 2.9.2,"When I checked out the latest code with CASSANDRA-18805 in, I noticed that it pulls these dependencies:

{code}
[resolver:resolve] Downloading https://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/3.1.8/caffeine-3.1.8.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/3.1.8/caffeine-3.1.8.pom (3 KB at 8.7 KB/sec)
[resolver:resolve] Downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.37.0/checker-qual-3.37.0.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.37.0/checker-qual-3.37.0.pom (3 KB at 127.9 KB/sec)
[resolver:resolve] Downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.21.1/error_prone_annotations-2.21.1.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.21.1/error_prone_annotations-2.21.1.pom (2 KB at 119.3 KB/sec)
[resolver:resolve] Downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_parent/2.21.1/error_prone_parent-2.21.1.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/com/google/errorprone/error_prone_parent/2.21.1/error_prone_parent-2.21.1.pom (13 KB at 658.5 KB/sec)
[resolver:resolve] Downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.pom
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.pom (0 B at 0.0 KB/sec)
[resolver:resolve] Resolving artifacts
[resolver:resolve] Downloading https://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/3.1.8/caffeine-3.1.8.jar
[resolver:resolve] Downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.37.0/checker-qual-3.37.0.jar
[resolver:resolve] Downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.21.1/error_prone_annotations-2.21.1.jar
[resolver:resolve] Downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar (0 B at 0.0 KB/sec)
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.21.1/error_prone_annotations-2.21.1.jar (17 KB at 219.1 KB/sec)
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.37.0/checker-qual-3.37.0.jar (220 KB at 2490.9 KB/sec)
[resolver:resolve] Downloaded https://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/3.1.8/caffeine-3.1.8.jar (869 KB at 6679.8 KB/sec)
{code}

Notice there is checker-qual and error_prone_annotations

If you do this on the current cassandra-5.0:
{code}
find . -type f -name '*.java' | xargs grep 'import .*\.Nullable;' | cut -d ':' -f2 | sort | uniq
{code}

you will get

{code}
import org.checkerframework.checker.nullness.qual.Nullable;
import javax.annotation.Nullable;
{code}

The first import is wrong. We should not use that, we should use just javax.annotation.Nullable.

The reason why it is possible to use that import is that it is on the classpath. Even worse, it is also added into libs which is not necessary at all.

error_prone_annotations are compile-time annotations and checker-qual are too. We do not need them in Cassandra at all. They are dependencies which are brought there from caffeine. 

"
CASSANDRA-18905,Index.Group is incorrectly unregistered from the SecondaryIndexManager,"An Index.Group is removed from the SecondaryIndexManager during unregisterIndex if it contains no indexes after the index is unregistered.

The code for removing the group uses the wrong key to remove the group from the indexGroups map. It is using the group object rather than the group name that is used as the key in the map.

This means that the group is not added again if a new index is registered using that group. The knock on from this is that the StorageAttachedIndexGroup unregisters itself from the Tracker when it has no indexes after an index is removed. The same group with no tracker is then used for new indexes. This group then receives no notifications about sstable or memtable updates. The ultimate side effect of this is that, memtables are not released, resulting in memory leaks and indexes are not updated with new sstables and their associated index files."
CASSANDRA-18903,Incremental repair never cleans up completed sessions from CoordinatorSessions.sessions,"Currently, there is nothing cleaning up repaired sessions from org.apache.cassandra.repair.consistent.CoordinatorSessions#sessions. This causes memory to leak for cluster members with long uptimes and lots of incremental repair."
CASSANDRA-18902,Test failure: org.apache.cassandra.distributed.test.MigrationCoordinatorTest.explicitEndpointIgnore,"Repeated run from `cassandra-4.1` [https://app.circleci.com/pipelines/github/jacek-lewandowski/cassandra/941/workflows/46fc6cb7-135e-4862-b9d3-6996c0993de8]

 "
CASSANDRA-18899,WEBSITE - Add Meetup Organizer Handbook and Events Approval Checklist pages,This ticket is to capture the work associated with adding the Meetup Organizer Handbook and Events Approval Checklist pages to the website.
CASSANDRA-18898,WEBSITE - Ecosystem page update for AxonOps and link fix,This ticket is to capture the work associated with updating the Apache Cassandra website's ecosystem page.
CASSANDRA-18896,ClientRequestSize metrics should not treat CONTAINS restrictions as being equality-based,"The following behavior needs to be changed to consider the column restricted by {{CONTAINS}} or {{CONTAINS KEY}} as ""read"", rather than ""provided by the client"". We already do this for things like range restrictions, and the current behavior is inconsistent.

{noformat}
@Test
public void shouldRecordReadMetricsForContainsQuery() throws Throwable
{
    createTable(""CREATE TABLE %s (pk int, ck int, v set<int>, PRIMARY KEY (pk, ck))"");

    executeNet(CURRENT, ""INSERT INTO %s (pk, ck, v) VALUES (1, 1, {1, 2, 3} )"");
    executeNet(CURRENT, ""INSERT INTO %s (pk, ck, v) VALUES (2, 2, {4, 5, 6})"");
    executeNet(CURRENT, ""SELECT * FROM %s WHERE v CONTAINS 1 ALLOW FILTERING"");

    assertEquals(1, ClientRequestSizeMetrics.totalRowsRead.getCount());
    // The filtering term is provided by the client in the request, so we don't consider that column read.
    assertEquals(2, ClientRequestSizeMetrics.totalColumnsRead.getCount());
}
{noformat}

The fix should be literally two lines, one in {{SingleRestriction}} and one in the test above."
CASSANDRA-18895,Test failure: MessageFiltersTest.hintSerializationTest (jdk17),"h3.  
{code:java}
https://ci-cassandra.apache.org/job/Cassandra-5.0/47/testReport/org.apache.cassandra.distributed.test/MessageFiltersTest/hintSerializationTest__jdk17/{code}
{code:java}
Error Message
Uncaught exceptions were thrown during test

Stacktrace
org.apache.cassandra.distributed.shared.ShutdownException: Uncaught exceptions were thrown during test 
at org.apache.cassandra.distributed.impl.AbstractCluster.checkAndResetUncaughtExceptions(AbstractCluster.java:1104) 
at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1090) 
at org.apache.cassandra.distributed.test.MessageFiltersTest.hintSerializationTest(MessageFiltersTest.java:297) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) 
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 

Suppressed: java.lang.IllegalStateException: Can't use shutdown instances, delegate is null 
at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegate(AbstractCluster.java:285) 
at org.apache.cassandra.distributed.impl.DelegatingInvokableInstance.transfer(DelegatingInvokableInstance.java:49) 
at org.apache.cassandra.distributed.api.IInvokableInstance.acceptsOnInstance(IInvokableInstance.java:49) 
at org.apache.cassandra.distributed.test.MessageFiltersTest.lambda$hintSerializationTest$11(MessageFiltersTest.java:287) 
at org.apache.cassandra.distributed.impl.MessageFilters$Filter.matches(MessageFilters.java:137) 
at org.apache.cassandra.distributed.impl.MessageFilters.permit(MessageFilters.java:61) 
at org.apache.cassandra.distributed.impl.MessageFilters.permitInbound(MessageFilters.java:37) 
at org.apache.cassandra.distributed.impl.Instance.lambda$registerInboundFilter$4(Instance.java:361) 
at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:63) 
at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:50) 
at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97) 
at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:45) 
at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:430) 
at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133) 
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:143) 
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.base/java.lang.Thread.run(Thread.java:833)
{code}
 
{code:java}
ERROR 12:39:56 uncaught exception in thread Thread[node1_MemtablePostFlush:1,5,MemtablePostFlush] java.util.concurrent.RejectedExecutionException: MemtableReclaimMemory has shut down 
at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238) 
at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833) 
at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57) 
at org.apache.cassandra.utils.concurrent.ListenerList.safeExecute(ListenerList.java:166) 
at org.apache.cassandra.utils.concurrent.ListenerList.notifyListener(ListenerList.java:157) 
at org.apache.cassandra.utils.concurrent.ListenerList$RunnableWithExecutor.notifySelf(ListenerList.java:345) 
at org.apache.cassandra.utils.concurrent.ListenerList.lambda$notifyExclusive$0(ListenerList.java:124) 
at org.apache.cassandra.utils.concurrent.IntrusiveStack.forEach(IntrusiveStack.java:195) 
at org.apache.cassandra.utils.concurrent.ListenerList.notifyExclusive(ListenerList.java:124) 
at org.apache.cassandra.utils.concurrent.ListenerList.notify(ListenerList.java:96) 
at org.apache.cassandra.utils.concurrent.AsyncFuture.trySet(AsyncFuture.java:104) 
at org.apache.cassandra.utils.concurrent.AbstractFuture.trySuccess(AbstractFuture.java:143) 
at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) 
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.base/java.lang.Thread.run(Thread.java:833) 

ERROR [node1_MemtablePostFlush:1] <main> 2023-09-27 12:39:56,474 AbstractCluster.java:536 - uncaught exception in thread Thread[node1_MemtablePostFlush:1,5,MemtablePostFlush] java.util.concurrent.RejectedExecutionException: MemtableReclaimMemory has shut down 
at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238) at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833) 
at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.execute(ThreadPoolExecutorPlus.java:57) 
at org.apache.cassandra.utils.concurrent.ListenerList.safeExecute(ListenerList.java:166) 
at org.apache.cassandra.utils.concurrent.ListenerList.notifyListener(ListenerList.java:157) 
at org.apache.cassandra.utils.concurrent.ListenerList$RunnableWithExecutor.notifySelf(ListenerList.java:345) 
at org.apache.cassandra.utils.concurrent.ListenerList.lambda$notifyExclusive$0(ListenerList.java:124) 
at org.apache.cassandra.utils.concurrent.IntrusiveStack.forEach(IntrusiveStack.java:195) 
at org.apache.cassandra.utils.concurrent.ListenerList.notifyExclusive(ListenerList.java:124) 
at org.apache.cassandra.utils.concurrent.ListenerList.notify(ListenerList.java:96) 
at org.apache.cassandra.utils.concurrent.AsyncFuture.trySet(AsyncFuture.java:104) 
at org.apache.cassandra.utils.concurrent.AbstractFuture.trySuccess(AbstractFuture.java:143) 
at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) 
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.base/java.lang.Thread.run(Thread.java:833) 

ERROR 12:39:56 uncaught exception in thread Thread[node1_MigrationStage:1,5,MigrationStage] java.util.concurrent.RejectedExecutionException: MemtablePostFlush has shut down 
at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238) 
at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833) 
at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.submit(ThreadPoolExecutorPlus.java:81) 
at org.apache.cassandra.db.ColumnFamilyStore.waitForFlushes(ColumnFamilyStore.java:1073) 
at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:1035) 
at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$19(SchemaKeyspace.java:361) 
at com.google.common.collect.ImmutableList.forEach(ImmutableList.java:422) 
at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:361) 
at org.apache.cassandra.schema.SchemaKeyspace.applyChanges(SchemaKeyspace.java:1400) 
at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.applyMutations(DefaultSchemaUpdateHandler.java:206) 
at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.lambda$new$0(DefaultSchemaUpdateHandler.java:97) 
at org.apache.cassandra.schema.SchemaPushVerbHandler.lambda$doVerb$0(SchemaPushVerbHandler.java:60) 
at java.base/java.util.concurrent.CopyOnWriteArrayList.forEach(CopyOnWriteArrayList.java:807) 
at org.apache.cassandra.schema.SchemaPushVerbHandler.doVerb(SchemaPushVerbHandler.java:60) 
at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78) 
at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:64) 
at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:50) 
at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97) 
at org.apache.cassandra.distributed.impl.Instance.lambda$receiveMessageRunnable$6(Instance.java:527) 
at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) 
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.base/java.lang.Thread.run(Thread.java:833) 

ERROR [node1_MigrationStage:1] <main> 2023-09-27 12:39:56,475 AbstractCluster.java:536 - uncaught exception in thread Thread[node1_MigrationStage:1,5,MigrationStage] java.util.concurrent.RejectedExecutionException: MemtablePostFlush has shut down 
at org.apache.cassandra.concurrent.ThreadPoolExecutorBase.lambda$static$0(ThreadPoolExecutorBase.java:49) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorJMXAdapter.lambda$rejectedExecutionHandler$0(ThreadPoolExecutorJMXAdapter.java:238) 
at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833) 
at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.addTask(ThreadPoolExecutorPlus.java:50) 
at org.apache.cassandra.concurrent.ThreadPoolExecutorPlus.submit(ThreadPoolExecutorPlus.java:81) 
at org.apache.cassandra.db.ColumnFamilyStore.waitForFlushes(ColumnFamilyStore.java:1073) 
at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:1035) 
at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$19(SchemaKeyspace.java:361) 
at com.google.common.collect.ImmutableList.forEach(ImmutableList.java:422) 
at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:361) 
at org.apache.cassandra.schema.SchemaKeyspace.applyChanges(SchemaKeyspace.java:1400) 
at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.applyMutations(DefaultSchemaUpdateHandler.java:206) 
at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.lambda$new$0(DefaultSchemaUpdateHandler.java:97) 
at org.apache.cassandra.schema.SchemaPushVerbHandler.lambda$doVerb$0(SchemaPushVerbHandler.java:60) 
at java.base/java.util.concurrent.CopyOnWriteArrayList.forEach(CopyOnWriteArrayList.java:807) 
at org.apache.cassandra.schema.SchemaPushVerbHandler.doVerb(SchemaPushVerbHandler.java:60) 
at org.apache.cassandra.net.InboundSink.lambda$new$0(InboundSink.java:78) 
at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:64) 
at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:50) 
at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:97) 
at org.apache.cassandra.distributed.impl.Instance.lambda$receiveMessageRunnable$6(Instance.java:527) 
at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) 
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.base/java.lang.Thread.run(Thread.java:833){code}
 "
CASSANDRA-18893,Upgrade hdrhistogram,"We are on 2.1.9, but some breaking changes for J9+ were introduced only in 2.1.10 (we don't hit them in our CI but a user reported they saw NoClassFound in some testing)
https://github.com/HdrHistogram/HdrHistogram/commit/c410c5c2699b69ce0eb026ff954fefc4045ec363

"
CASSANDRA-18892,Distributed tests can return ordering columns that have not been selected,"The following test fails
{code:java}
@Test
public void incorrectClusteringColumnTest() throws IOException
{
    try (Cluster cluster = init(Cluster.build(1).start()))
    {
        cluster.schemaChange(withKeyspace(""CREATE TABLE %s.t (k int, c int, v int, primary key(k, c))""));

        cluster.coordinator(1).execute(withKeyspace(""INSERT INTO %s.t (k, c, v) VALUES (0, 1, 2)""), ConsistencyLevel.QUORUM);

        String query = withKeyspace(""SELECT v FROM %s.t WHERE k IN (0, 1) ORDER BY c LIMIT 10"");
        assertRows(cluster.coordinator(1).execute(query, ConsistencyLevel.ONE), row(2));
    }
}
 {code}
The query is returning the clustering column c as well as the regular column v.

The reason for the extra column being returned is that the RowUtil is using ResultMessage.Rows.result.metadata.names instead on ResultMessage.Rows.result.metadata.requestNames(). This last method removes columns that have not been requested."
CASSANDRA-18890,Test failure: org.apache.cassandra.repair.ConcurrentIrWithPreviewFuzzTest,"The unit test {{org.apache.cassandra.repair.ConcurrentIrWithPreviewFuzzTest}} is flaky in both 5.0 and trunk:
* https://app.circleci.com/pipelines/github/adelapena/cassandra/3222/workflows/ecfca708-f183-429e-80e5-b2bfea8d25a0/jobs/80292/tests
* https://app.circleci.com/pipelines/github/adelapena/cassandra/3221/workflows/bb777ac0-6263-4d6e-aa54-35d6928e1e9b/jobs/80294

{code}
junit.framework.AssertionFailedError: Property error detected:
Seed = 3695691971125975155
Examples = 2
Pure = false
Error: property test did not complete within PT1M
Values:

	at accord.utils.Property$Common.checkWithTimeout(Property.java:115)
	at accord.utils.Property$SingleBuilder.check(Property.java:223)
	at accord.utils.Property$ForBuilder.check(Property.java:124)
	at org.apache.cassandra.repair.ConcurrentIrWithPreviewFuzzTest.concurrentIrWithPreview(ConcurrentIrWithPreviewFuzzTest.java:46)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
Flakiness is around 6%. The CircleCI runs above have been generated with:
{code}
.circleci/generate.sh -p -s \
  -e REPEATED_UTESTS=org.apache.cassandra.repair.ConcurrentIrWithPreviewFuzzTest \
  -e REPEATED_UTESTS_COUNT=500
{code}
"
CASSANDRA-18889,DDM's mask_default function should support vectors,"DDM uses the CQL function {{mask_default}} as the default masking function. This function supports all CQL data types except for {{vector}}, which didn't exist when DDM was added. 

We should make {{mask_default}} (and thereof default masking) work with vector columns."
CASSANDRA-18887,CEP-21 - Implement missing features and various other fixes,"[CEP-21] CASSANDRA-18816 rebase fixes
https://github.com/krummas/cassandra/commit/4d06d73c7e
make the 18816 tests pass with TCM

[CEP-21] fix cqlshlib tests
https://github.com/krummas/cassandra/commit/a90cc1b98a
Add cluster_metadata keyspace where needed

[CEP-21] remove authsetup
https://github.com/krummas/cassandra/commit/e5a8ac2607
Avoid calling doAuthSetup twice

[CEP-21] serialize MemtableParams when writing TableParams
https://github.com/krummas/cassandra/commit/e81ddb4e31
Missed in earlier rebase, need to serialize MemtableParams when saving a snapshot

[CEP-21] Upgrading a one node cluster to TCM fails attempting Gossip shadow round
https://github.com/krummas/cassandra/commit/8bcd581fc4
Upgrading fails due to first upgraded node waiting for gossip state from peers.

[CEP-21] Implement replacement with same address
https://github.com/krummas/cassandra/commit/c1e94aa21a
Add back missing replace with same address feature

[CEP-21] fix nodetool bootstrap resume
https://github.com/krummas/cassandra/commit/2265c4b615
Add back missing nodetool bootstrap resume feature

[CEP-21] Remove LEFT peers from system tables and exclude them during startup
https://github.com/krummas/cassandra/commit/2aef6f6a96
Avoid trying to gossip with LEFT peers

[CEP-21] Fix flaky distributed log test. While it fails very infrequently on CI, it consistently fails locally.
https://github.com/krummas/cassandra/commit/6ea5042618
Test fix

[CEP-21] Retry indefinitely for STARTUP messages.
https://github.com/krummas/cassandra/commit/5924404bd6
If the CMS is down and we try to upgrade an instance we should block until the CMS comes back

[CEP-21] Implement versioning for ranges
https://github.com/krummas/cassandra/commit/ef1ad17669
To be able to know if the coordinator has the correct ring view we keep track of when a range was last updated. If coordinator has not seen this update we fail the request.

[CEP-21] Handle case where removenode requires no streaming to restore RF
https://github.com/krummas/cassandra/commit/47769f0ba2

[CEP-21] Remove redundant Keyspaces arg from SchemaTransformation::apply
https://github.com/krummas/cassandra/commit/d0367189da
Code cleanup

[CEP-21] Don't trigger client warnings or guardrails when executing AlterSchema transformations
https://github.com/krummas/cassandra/commit/ce7511f2ec
Avoid getting multiple client warnings

[CEP-21] Validate schema alterations on coordinator
https://github.com/krummas/cassandra/commit/bf9169683b

[CEP-21] Add a script for simulation running
https://github.com/krummas/cassandra/commit/599d6f70a3

[CEP-21] Add nextId to Directory serialization and make it possible to bump metadata serialization version
https://github.com/krummas/cassandra/commit/6dd4c92797
Previously, on bounce, each instance calculated which nodeId we should give a new node by counting the number of instances in the cluster, this can diverge so that different nodes have different ids. Fixed by serializing the current nextId when storing snapshot.

[CEP-21] Add metric for CMS membership
https://github.com/krummas/cassandra/commit/5dfb270d92

[CEP-21] Reenable starting without joining (-Dcassandra.join_ring=false)
https://github.com/krummas/cassandra/commit/bd1e9da2f8
Add back missing join_ring=false feature.

[CEP-21] Preparing DDL statements should capture the CQL string
https://github.com/krummas/cassandra/commit/c312fd5273
Avoid NPE
"
CASSANDRA-18886,snappy-java-1.1.10.1 vulnerability: CVE-2023-43642,"https://nvd.nist.gov/vuln/detail/CVE-2023-43642

Another DoS which is probably not a huge deal, but we can upgrade."
CASSANDRA-18884,Test failure: org.apache.cassandra.db.compaction.LongCompactionsTest ,"org.apache.cassandra.db.compaction.LongCompactionsTest in ""long"" tests is broken. This seems to be a regression after CASSANDRA-18737

If I checkout the code one merge before (CASSANDRA-18871), these tests do not fail there."
CASSANDRA-18882,Update the slf4j-api library in order to avoid potential leaks,"{color:red}>{color}The proposed update is 1.7.25 (current) > 1.7.36 (latest)

The following commits are included in the newer slf4j-api and that currently look very useful for us:
 - [SLF4J-469|https://www.mail-archive.com/slf4j-dev@qos.ch/msg02569.html] Potential memory leaks if there is no underlying implementation
 - [SLF4j-466|https://www.mail-archive.com/slf4j-dev@qos.ch/msg02499.html] Add test for all happy flow cases
 - [SLF4J-460|https://www.mail-archive.com/search?l=slf4j-dev@qos.ch&q=subject:%22%5C%5Bslf4j%5C-dev%5C%5D+%5C%5BJIRA%5C%5D+%5C%28SLF4J%5C-460%5C%29+EventRecodingLogger+debug+logs+as%09TRACE%22&o=newest&f=1)] EventRecodingLogger debug logs as TRACE"
CASSANDRA-18881,Test failure: org.apache.cassandra.tools.nodetool.ForceCompactionTest.forceCompactMultipleRowsTombstoneTest-.jdk11 ,"[https://ci-cassandra.apache.org/job/Cassandra-trunk/1718/testReport/org.apache.cassandra.tools.nodetool/ForceCompactionTest/forceCompactMultipleRowsTombstoneTest__jdk11/]
h3.  
{code:java}
Stacktrace
junit.framework.AssertionFailedError at org.apache.cassandra.tools.nodetool.ForceCompactionTest.verifyNotContainsTombstones(ForceCompactionTest.java:251) at org.apache.cassandra.tools.nodetool.ForceCompactionTest.forceCompactMultipleRowsTombstoneTest(ForceCompactionTest.java:129) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-18880,Test failure: dtest-novnode.transient_replication_test.TestTransientReplicationRepairStreamEntireSSTable.test_optimized_primary_range_repair_with_lcs,"https://ci-cassandra.apache.org/job/Cassandra-trunk/1718/testReport/dtest-novnode.transient_replication_test/TestTransientReplicationRepairStreamEntireSSTable/test_optimized_primary_range_repair_with_lcs/
{code:java}
Error Message
failed on setup with ""TypeError: 'list' object is not callable""

Stacktrace
self = <transient_replication_test.TestTransientReplicationRepairStreamEntireSSTable object at 0x7f4b3a4e4ac0> fixture_dtest_setup = <dtest_setup.DTestSetup object at 0x7f4b3a583250> @pytest.fixture(scope='function', autouse=True) def setup_cluster(self, fixture_dtest_setup): > self.tokens = self.tokens() E TypeError: 'list' object is not callable transient_replication_test.py:206: TypeError
{code}
 "
CASSANDRA-18879,Modernize CQLSH datetime conversions,"Python 3.x introduced many updates to datetime conversion which allows simplified conversions.

1. For example, tracing.py defines a function datetime_from_utc_to_local() but datetime now has a native function astimezone() which will convert UTC to local time.

Review the following users of datetime which apply conversions:
 * cqlshmain.py
 * formatting.py 
 * tracing.py

Example: 
{code:java}
>>> from dateutil import tz
>>> import datetime
>>> a = datetime.datetime.now().astimezone(tz.tzutc())
>>> a
datetime.datetime(2023, 9, 25, 11, 22, 36, 251705, tzinfo=tzutc())
>>> b = a.astimezone()
>>> b
datetime.datetime(2023, 9, 25, 14, 22, 36, 251705, tzinfo=datetime.timezone(datetime.timedelta(seconds=10800), 'EST')) {code}
See [[PEP 495|http://example.com]]]

 "
CASSANDRA-18878,Upgrade snappy java library,"The snappy java library needs to be updated to fix the latest CVEs.
[https://github.com/xerial/snappy-java/security/advisories/GHSA-55g7-9cwv-5qfv]"
CASSANDRA-18877,remove bytebuddy / byteman from production classpath and remove compress-lzf dependency from build deps,"I was digging in the project deps and if you compare all libs in ""libs"" dir and all libs in ""build/lib/jars"", there are indeed some differences which are OK however in build/lib/jars there are also libraries for byteman and byte-buddy. This is clearly wrong as these dependecies should not be accessible from the production code, only from tests.

The reason they are accessible in prod code is that there is the class TestRateLimiter (1). I do not have a clue why that class is in the prod code in the first place. The only place it is referenced in is here (2) but that byteman script is not loaded anywhere in tests. I was also checking Python dtests.

I think this is some leftover or something like ""I will keep it here when I need it"", but as nobody seems to do, I strongly advocate for removing it and making bytebuddy and byteman only test scoped dependencies as it should be.

A reader who pays attention notices that these dependencies are of provided scope which is a trick to have it compilable but not among the libraries in the production runtime and it does not do any harm as it is never invoked from the production code (if it was, it would fail on missing imports) neverthless this is still an issue which should be addressed. We were doing something similar with assertj dependency recently.

The second issue is that there is a dependency on compress-lzf in build dependencies. This is not necessary either as that library was removed from the repository in (3) but it still somehow leaked to the build process again. 

(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/TestRateLimiter.java
(2) https://github.com/apache/cassandra/blob/trunk/test/resources/byteman/mutation_limiter.btm
(3) https://github.com/apache/cassandra/commit/fc92db2b9b56c143516026ba29cecdec37e286bb"
CASSANDRA-18876,The vector data type shouldn't support empty value,"As discussed on [the mail list|https://lists.apache.org/thread/qq0jkm6rlkd2slfmhgz7om14tbpys891], the vector data type shouldn't allow empty values."
CASSANDRA-18875,Upgrade the snakeyaml library version,"Apache cassandra uses 1.26 version of snakeyaml dependency and there are several [vulnerabilities|https://mvnrepository.com/artifact/org.yaml/snakeyaml/1.26#] in this version that can be fixed by upgrading to 2.x version. I understand that this is not security issue as cassandra already uses SafeConstructor and is not a vulnerability under OWASP, so there are no plans to fix it as per  CASSANDRA-18122

 

Cassandra as a open source used and distributed by many enterprise customers and also when downloading cassandra as tar and using it external scanners are not aware of the implementation of SafeConstructor have no idea if it's vulnerable or not. 

Can we consider upgrading the version to 2.x in the next releases as snakeyaml is not something that has a large dependency between the major and minor versions. I am happy to open a PR for this. Please let me know your thoughts on this."
CASSANDRA-18874,Add Jepsen's Elle to Accord and Paxos validation,"Paxos and Accord both use a custom validator to make sure we offer Strict Serializability, but to increase confidence we should use external validators as well, such as Jepson’s Elle."
CASSANDRA-18872,Remove deprecated crc_check_chance in compression params,crc_check_chance was moved from compression parameters and it is a standalone table parameter. This was done in times of 3.0 so it is now time to get rid of that in 5.0.
CASSANDRA-18871,JMH benchmark improvements,"1. CASSANDRA-12586  introduced {{build-jmh}} task which builds uber jar for JMH benchmarks which is then not used with {{ant microbench}} task. It is used though by the {{test/bin/jmh}} script. 

In fact, I have no idea why we should use uber jar if JMH can perfectly run with a regular classpath. Maybe that had something to do with older JMH version which was used that time. Building uber jars takes time and is annoying. Since it seems to be redundant anyway, I'm going to remove it and fix {{test/bin/jmh}} to use a regular classpath. 

2. I'll add support for async profiler in benchmarks. That is, the {{microbench}} target automatically fetches the async profiler binaries and adds the necessary args for JMH ({{-prof asyc...}} in particular) whenever we run {{microbench-with-profiler}} task. If no additional properties are provided some default options will be applied (defined in the script, can be negotiated). Otherwise, whatever is passed to the {{profiler.opts}} property will be added as profiler options after library path and target directory definition.

3. If someone wants to see any additional improvements, please comment on the ticket.
"
CASSANDRA-18870,Invalid unit test check for CreateTableValidationTest,"https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/schema/CreateTableValidationTest.java#L40
This test case testInvalidBloomFilterFPRatio()  makes no sense and cannot achieve the desired purpose."
CASSANDRA-18865,Enable unnecessary import from the same package check in IDEs,"Currently, the unnecessary imports are disabled by the code style and are not displayed by the IDE with the configuration stored in the project root. See the screenshot below. 

It seems the following needs to be done:
- enable this check in the IDE configuration for all supported IDEs;
- update the documentation;


"
CASSANDRA-18864,CIDR permission cache probably does not work,"CIDR permission cache code:

{code:java}
    /**
     * Invalidate a role from CIDR permissions cache
     * @param roleName role for which to invalidate the cache
     * @return boolean returns true if given role found in the cache and invalidated, otherwise returns false
     */
    public boolean invalidateCidrPermissions(String roleName)
    {
        if (cache.getIfPresent(roleName) == null)
            return false;

        invalidate(RoleResource.role(roleName));
        return true;
    }
{code}

passes {{String}} role name to the cache and if it returns {{null}} it just returns {{false}}. It will always return {{null}} because cache expects objects of type {{RoleResource}}.
"
CASSANDRA-18862,Fix logging / exception output on mismatched cache and schema version,"Sometimes I see this in the logs and Cassandra just continues to start as nothing happened:

{code}
INFO  [loadSavedCache:1] 2023-09-18 08:53:05,395 AutoSavingCache.java:214 - Reading saved cache: /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra/data/saved_caches/KeyCache-g.db, /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra/data/saved_caches/KeyCache-g.crc, /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra/data/saved_caches/KeyCache-g.metadata
INFO  [loadSavedCache:1] 2023-09-18 08:53:05,403 AutoSavingCache.java:277 - Harmless error reading saved cache /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra/data/saved_caches/KeyCache-g.db
java.lang.RuntimeException: Cache schema version e40b993e-88c0-314c-8a5e-c76871f0a144 does not match current schema version f1884cc9-7d66-3b5f-85e5-4bc0d647c885
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:228)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
{code}

Well if the log says it is harmless, why do we need to log the exception? There is no reason to print that stacktrace, it is just enough to print the mismatching versions. It should be also changed from INFO to WARN, or maybe better to DEBUG? I mean ... it is harmless, right? Why should I see, by default, that something harmless has happened?"
CASSANDRA-18861,add time elapsed for simple CQL statement in the cql shell,
CASSANDRA-18860,add time elapsed for simple CQL statement in the cql shell,
CASSANDRA-18858,AssertionError: Unknown keyspace after dropping a keyspace combined with drain operation,"After dropping the keyspace and then perform the drain, I met the following ERROR message
{code:java}
rzayqyaovyytmgo],droppedColumns={},triggers=[],indexes=[]]
INFO  [MigrationStage:1] 2023-09-09 02:06:40,500 ColumnFamilyStore.java:432 - Initializing uuid00bd53ebd0c84be39baf96c086b9f8d0.hlknh
INFO  [Native-Transport-Requests-6] 2023-09-09 02:06:40,549 MigrationManager.java:362 - Drop Keyspace 'uuid00bd53ebd0c84be39baf96c086b9f8d0'
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:13,353 StorageService.java:1679 - DRAINING: starting drain process
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:13,358 HintsService.java:210 - Paused hints dispatch
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:13,364 Server.java:179 - Stop listening for CQL clients
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:13,365 Gossiper.java:1747 - Announcing shutdown
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:13,369 StorageService.java:2604 - Node /192.168.7.2 state jump to shutdown
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:15,373 MessagingService.java:985 - Waiting for messaging service to quiesce
INFO  [ACCEPT-/192.168.7.2] 2023-09-09 02:07:15,375 MessagingService.java:1346 - MessagingService has terminated the accept() thread
ERROR [RMI TCP Connection(2)-127.0.0.1] 2023-09-09 02:07:15,384 StorageService.java:4889 - Caught an exception while draining 
java.lang.AssertionError: Unknown keyspace uuid00bd53ebd0c84be39baf96c086b9f8d0
    at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:316)
    at org.apache.cassandra.db.Keyspace.open(Keyspace.java:129)
    at org.apache.cassandra.db.Keyspace.open(Keyspace.java:106)
    at org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:92)
    at org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:89)
    at com.google.common.collect.Iterators$8.transform(Iterators.java:799)
    at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
    at org.apache.cassandra.service.StorageService.drain(StorageService.java:4824)
    at org.apache.cassandra.service.StorageService.drain(StorageService.java:4749)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:276)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
    at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
    at sun.rmi.transport.Transport$1.run(Transport.java:200)
    at sun.rmi.transport.Transport$1.run(Transport.java:197)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:834)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:750)
INFO  [RMI TCP Connection(4)-127.0.0.1] 2023-09-09 02:07:17,953 CassandraDaemon.java:576 - Cassandra shutting down...
INFO  [main] 2023-09-09 02:07:26,619 YamlConfigurationLoader.java:104 - Configuration location: file:/etc/apache-cassandra-4.1.3/cassandra.yaml
INFO  [main] 2023-09-09 02:07:27,094 Config.java:1171 - Node configuration:[allocate_tokens_for_keyspace=null;  {code}
This is a concurrency problem related to the metadata update.

There are two threads interleaving:
 * Thread1 (meta-data update related thread): update the keyspace metadata
 * Thread2 (drain): flush all in-memory data to disk.

Here's how it happened in detail

Step1: The user issues a drop keyspace command. However, the update of the metadata gets delayed.

Step2: Thread2 (Drain) tries to iterate all the keyspace for the flushing operation
{code:java}
protected synchronized void drain(boolean isFinalShutdown) throws IOException, InterruptedException, ExecutionException
{
    for (Keyspace keyspace : Keyspace.nonSystem())
    {
        for (ColumnFamilyStore cfs : keyspace.getColumnFamilyStores())
            flushes.add(cfs.forceFlush());
    }{code}
keyspace.getColumnFamilyStores() returns a copy of the keyspaces
{code:java}
// Schema.java
public List<String> getNonSystemKeyspaces()
{
    return ImmutableList.copyOf(getNonSystemKeyspacesSet());
}

private Set<String> getNonSystemKeyspacesSet()
{
    return Sets.difference(keyspaces.keySet(), SchemaConstants.LOCAL_SYSTEM_KEYSPACE_NAMES);
}{code}
Step3: Thread1 (Metadata update related threads) updates the metadata and removes the keyspace
Step4: Thread2: it keeps iterating the old copy of the keyspaces and runs into exceptions since the keyspace has been dropped.

 

The symptom is similar to CASSANDRA-18636, but the required triggering thread interleaving is different => different stack traces. For CASSANDRA-18636, it's the interleaving between _thread updating metadata_ and {_}thread estimating the size{_}. For this case, it's interleaving between the _thread updating the metadata_ and _thread performing the drain._

They might be fixed together with the [patch|https://github.com/apache/cassandra/pull/2457] by [Stefan Miklosovic|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stefan.miklosovic] if exhausting all inferences to the old keyspace copies. (The previous patch only targets at one interleaving situation, this ticket shows that there could be other cases).

I have attached the system.log file."
CASSANDRA-18857,Allow CQL client certificate authentication to work without sending an AUTHENTICATE request,"Currently when using {{MutualTlsAuthenticator}} or {{MutualTlsWithPasswordFallbackAuthenticator}} a client is prompted with an {{AUTHENTICATE}} message to which they must respond with an {{AUTH_RESPONSE}} (e.g. a user name and password).  This shouldn't be needed as the role can be identified using only the certificate.

To address this, we could add the capability to authenticate early in processing of a {{STARTUP}} message if we can determine that both the configured authenticator supports certificate authentication and a client certificate was provided.  If the certificate can be authenticated, a {{READY}} response is returned, otherwise an {{ERROR}} is returned.

This change can be done done in a fully backwards compatible way and requires no protocol or driver changes;  I will supply a patch shortly!"
CASSANDRA-18856,Test failure: rebuild_test.TestRebuild.test_resumable_rebuild,"https://app.circleci.com/pipelines/github/driftx/cassandra/1294/workflows/04464235-3bcf-433e-ae81-206aa2c9c874/jobs/54042/tests

{quote}
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node3] 'ERROR [Stream-Deserializer-/127.0.0.2:7000-d94b6b54] 2023-09-15 16:04:30,685 CassandraEntireSSTableStreamReader.java:146 - [Stream 8d7c61b0-53e1-11ee-a721-a91a3065a930] Error while reading sstable from stream for table = ks.cf\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119)\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.consume(AsyncStreamingInputPlus.java:139)\n\tat org.apache.cassandra.io.sstable.SSTableZeroCopyWriter.write(SSTableZeroCopyWriter.java:218)\n\tat org.apache.cassandra.io.sstable.SSTableZeroCopyWriter.writeComponent(SSTableZeroCopyWriter.java:207)\n\tat org.apache.cassandra.db.streaming.CassandraEntireSSTableStreamReader.read(CassandraEntireSSTableStreamReader.java:124)\n\tat org.apache.cassandra.db.streaming.CassandraIncomingFile.read(CassandraIncomingFile.java:87)\n\tat org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:50)\n\tat org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:36)\n\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:50)\n\tat org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:59)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)', [node3] 'ERROR [Stream-Deserializer-/127.0.0.2:7000-6f7e3946] 2023-09-15 16:04:30,687 CassandraEntireSSTableStreamReader.java:146 - [Stream 8d7c61b0-53e1-11ee-a721-a91a3065a930] Error while reading sstable from stream for table = ks.cf\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119)\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.consume(AsyncStreamingInputPlus.java:139)\n\tat org.apache.cassandra.io.sstable.SSTableZeroCopyWriter.write(SSTableZeroCopyWriter.java:218)\n\tat org.apache.cassandra.io.sstable.SSTableZeroCopyWriter.writeComponent(SSTableZeroCopyWriter.java:207)\n\tat org.apache.cassandra.db.streaming.CassandraEntireSSTableStreamReader.read(CassandraEntireSSTableStreamReader.java:124)\n\tat org.apache.cassandra.db.streaming.CassandraIncomingFile.read(CassandraIncomingFile.java:87)\n\tat org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:50)\n\tat org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:36)\n\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:50)\n\tat org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:59)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)']""
{quote}

This is probably similar to CASSANDRA-18815"
CASSANDRA-18854,Gossip never recovers from a single failed echo,"As discovered on CASSANDRA-18792, if an initial echo request is lost, the node will never be marked up.  This appears to be a regression caused by CASSANDRA-18543."
CASSANDRA-18853,IDEA to mark unused imports as error,"During dev it's quite easy to miss unused imports which will fail CI. That is very annoying, slows dev down and CI is expensive. It's easy to avoid by making Idea mark them as errors"
CASSANDRA-18852,[Analytics] Make bulk writer resilient to cluster resize events,"Currently, the analytics bulk writer fails-fast if there are any ongoing cluster resizing events such as expansion, shrink, host-replacement or node movements. 

This effort is make the bulk writer resilient to these events by having the writes account for the transient nodes and their corresponding pending token ranges.

Related: https://issues.apache.org/jira/browse/CASSANDRASC-60"
CASSANDRA-18850,Building a Cassandra website fails when running on arm64,"It seems to me that building a Cassandra website is not supported when running on arm64 (tested on Mac M1). Running the build command throws the following exceptions below. 
Docker Version 4.23.0 (120376), the experimental feature ""use Rosetta for x86/amd64 emulation on Apple Silicon"" is enabled.


{code:java}
m@m1 cassandra-website % ./run.sh website build
Server Docker Engine version: 20.10.21

Executing docker command:
docker build -f ./site-content/Dockerfile -t apache/cassandra-website:latest  ./site-content/
[+] Building 7.2s (12/23)                                                                                                                                                                                                                                                                                               
 => [internal] load build definition from Dockerfile                                                                                                                                                                                                                                                               0.0s
 => => transferring dockerfile: 5.03kB                                                                                                                                                                                                                                                                             0.0s
 => [internal] load .dockerignore                                                                                                                                                                                                                                                                                  0.0s
 => => transferring context: 2B                                                                                                                                                                                                                                                                                    0.0s
 => [internal] load metadata for docker.io/library/ubuntu:18.04                                                                                                                                                                                                                                                    0.6s
 => [ 1/19] FROM docker.io/library/ubuntu:18.04@sha256:152dc042452c496007f07ca9127571cb9c29697f42acbfad72324b2bb2e43c98                                                                                                                                                                                            0.0s
 => [internal] load build context                                                                                                                                                                                                                                                                                  0.0s
 => => transferring context: 42B                                                                                                                                                                                                                                                                                   0.0s
 => CACHED [ 2/19] RUN echo ""Building with arguments:""     && echo "" - BUILD_USER_ARG=build""     && echo "" - UID_ARG=1000""     && echo "" - GID_ARG=1000""     && echo "" - NODE_VERSION_ARG=v12.16.2""     && echo "" - ENTR_VERSION_ARG=4.6""                                                                          0.0s
 => CACHED [ 3/19] RUN apt update &&     apt install -y         openjdk-8-jdk         openjdk-11-jdk         python3         python3-pip         ant         ant-optional         make         git         gpg         wget         sudo                                                                           0.0s
 => CACHED [ 4/19] RUN apt-get install -y python-markupsafe                                                                                                                                                                                                                                                        0.0s
 => [ 5/19] RUN apt-get install -y python-jinja2                                                                                                                                                                                                                                                                   2.0s
 => [ 6/19] RUN pip3 install requests                                                                                                                                                                                                                                                                              1.9s
 => [ 7/19] RUN wget https://nodejs.org/download/release/v12.16.2/node-v12.16.2-linux-x64.tar.gz &&     tar -C /usr/local --strip-components 1 -xzf node-v12.16.2-linux-x64.tar.gz &&     rm node-v12.16.2-linux-x64.tar.gz                                                                                        2.4s 
 => ERROR [ 8/19] RUN npm i -g @antora/cli@2.3 @antora/site-generator-default@2.3 @djencks/asciidoctor-openblock                                                                                                                                                                                                   0.3s 
------                                                                                                                                                                                                                                                                                                                  
 > [ 8/19] RUN npm i -g @antora/cli@2.3 @antora/site-generator-default@2.3 @djencks/asciidoctor-openblock:                                                                                                                                                                                                              
#11 0.304 qemu-x86_64: Could not open '/lib64/ld-linux-x86-64.so.2': No such file or directory                                                                                                                                                                                                                          
------                                                                                                                                                                                                                                                                                                                  
executor failed running [/bin/sh -c npm i -g @antora/cli@2.3 @antora/site-generator-default@2.3 @djencks/asciidoctor-openblock]: exit code: 255                                                                                                                                                                         
Error: No such object: apache/cassandra-website:latest
{code}
{code:java}
entr: cannot create kqueue: Function not implemented

https://docs.docker.com/desktop/troubleshoot/known-issues/
{code}"
CASSANDRA-18849,Test failure: dtest.secondary_indexes_test.TestPreJoinCallback.test_resume,"Since recently, it fails regularly on 4.1, 5.0, and trunk: 
{code:java}
Error Message
failed on teardown with ""Failed: Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [Stream-Deserializer-/127.0.0.1:7000-6e2ae2f5] 2023-09-13 12:41:02,461 StreamSession.java:700 - [Stream #cc2010c0-5232-11ee-bc8b-4945091d03ae] Socket closed before session completion, peer 127.0.0.1:7000 is probably down.\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119)\n\tat org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:178)\n\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:49)\n\tat org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:59)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']""

Stacktrace
Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [Stream-Deserializer-/127.0.0.1:7000-6e2ae2f5] 2023-09-13 12:41:02,461 StreamSession.java:700 - [Stream #cc2010c0-5232-11ee-bc8b-4945091d03ae] Socket closed before session completion, peer 127.0.0.1:7000 is probably down.\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119)\n\tat org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:178)\n\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:49)\n\tat org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:59)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']
{code}
 "
CASSANDRA-18848,"GCInspector ""Error accessing field of java.nio.Bits"" under java17","Running under java17, {{GCInspector}} throws the following exception:
{noformat}
DEBUG [main] 2023-09-13 09:35:28,031 GCInspector.java:85 - Error accessing field of java.nio.Bits
java.lang.reflect.InaccessibleObjectException: Unable to make field private static final java.util.concurrent.atomic.AtomicLong java.nio.Bits.TOTAL_CAPACITY accessible: module java.base does not ""opens java.nio"" to unnamed module @503d687a
        at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
        at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
        at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
        at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
        at org.apache.cassandra.service.GCInspector.<clinit>(GCInspector.java:80)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:328)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:729)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:853)
INFO  [main] 2023-09-13 09:35:28,039 PaxosUncommittedTracker.java:236 - enabling PaxosUncommittedTracker {noformat}
This is because {{GCInspector}} uses reflection to read the {{TOTAL_CAPACITY}} from {{{}java.nio.Bits{}}}. Access was restricted somewhere between 11 and 17.

Note: this is a rather harmless error, as we only look at {{Bits.totalCapacity}} for metrics collection on how much direct memory is being used by \{{ByteBuffer}}s. If we fail to read the field, we simply return -1 for the metric value."
CASSANDRA-18846,CancellationException when shutting down the 3.11.16 node,"I encountered the following exception when I tried to shutdown the cassandra node after drain operation.
{code:java}
INFO  [RMI TCP Connection(2)-127.0.0.1] 2023-09-07 12:59:43,440 StorageService.java:1679 - DRAINED
ERROR [MigrationStage:1] 2023-09-07 12:59:43,443 CassandraDaemon.java:244 - Exception in thread Thread[MigrationStage:1,5,main]
java.util.concurrent.CancellationException: null
        at java.util.concurrent.FutureTask.report(FutureTask.java:121)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:438)
        at org.apache.cassandra.db.lifecycle.LogTransaction.waitForDeletions(LogTransaction.java:430)
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.waitForDeletions(LifecycleTransaction.java:623)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:587)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:557)
        at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:378)
        at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:371)
        at org.apache.cassandra.config.Schema.dropTable(Schema.java:715)
        at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$18(SchemaKeyspace.java:1426)
        at java.lang.Iterable.forEach(Iterable.java:75)
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1426)
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1413)
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1390)
        at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:464)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84)
        at java.lang.Thread.run(Thread.java:750)
ERROR [Native-Transport-Requests-6] 2023-09-07 12:59:43,444 QueryMessage.java:129 - Unexpected error during query
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.util.concurrent.CancellationException
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:442)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:429)
        at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:363)
        at org.apache.cassandra.cql3.statements.DropKeyspaceStatement.announceMigration(DropKeyspaceStatement.java:63)
        at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:123)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:243)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:274)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:259)
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116)
        at org.apache.cassandra.transport.Message$Dispatcher.processRequest(Message.java:688)
        at org.apache.cassandra.transport.Message$Dispatcher.lambda$channelRead0$0(Message.java:594)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:113)
        at java.lang.Thread.run(Thread.java:750)
Caused by: java.util.concurrent.ExecutionException: java.util.concurrent.CancellationException
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:438)
        ... 14 common frames omitted
Caused by: java.util.concurrent.CancellationException: null
        at java.util.concurrent.FutureTask.report(FutureTask.java:121)
        at java.util.concurrent.FutureTask.get(FutureTask.java:192)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:438)
        at org.apache.cassandra.db.lifecycle.LogTransaction.waitForDeletions(LogTransaction.java:430)
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.waitForDeletions(LifecycleTransaction.java:623)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:587)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:557)
        at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:378)
        at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:371)
        at org.apache.cassandra.config.Schema.dropTable(Schema.java:715)
        at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$18(SchemaKeyspace.java:1426)
        at java.lang.Iterable.forEach(Iterable.java:75)
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1426)
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1413)
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1390)
        at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:464)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84)
        ... 1 common frames omitted
 {code}
The CancellationException might be safely handled. I haven't observed data integrity problem, but I don't know whether it will cause any other effects.

I executed a long list of commands before shutting down the node. It mainly contains cqlsh related commands. This error message might not be reproduced deterministically since it's more likely related to interleaving between the (1) shutdown process and (2) threads working on requests.

I have attached the log file for the complete error message. "
CASSANDRA-18844,Cassandra 5 Debian package only runs on OpenJDK,"A recent change to the Cassandra 5 branch has caused the Cassandra deb package to no longer install on systems that don't run OpenJDK

 

The following line is too restrictive

[https://github.com/apache/cassandra/blob/trunk/debian/control#L14]

It should be modified to use the java11-runtime and java17-runtime virtual packages."
CASSANDRA-18842,Jenkins: Add python upgrade dtests running with vnodes,"Python upgrade dtests are currently only running with non-vnodes.

A number of tests are broken if using vnodes, mostly missing `@pytest.mark.no_vnodes` annotations."
CASSANDRA-18841,InstanceClassLoader leak in 5.0/trunk,"Something in the 5.0/trunk branches has caused an in-jvm dtest InstanceClassLoader leak - it appears to have something to do with the Mutual TLS Authenticator (f078c02cb58bddd735490b07548f7352f0eb09aa) but nothing in that commit, so far, has stood out as causing issues.

The culprit class appears to be {{io.netty.util.internal.InternalThreadLocalMap}}, which seems to no be removed when the threads stops for some reason."
CASSANDRA-18840,Leakage of references to SSTable on unsuccessful operations,"This is a little bit tricky to describe correctly as I can talk about the symptoms only. I hit this issue when testing CASSANDRA-18781.

In a nutshell, when we go to bulkload an SSTable, it opens it in SSTableLoader. If bulkloading fails on server side and exception is propagated to the client, on releasing of references, it fails on this assert (1). This practically means that we are leaking resources as something still references that SSTable but it was not tidied up (on failure). On a happy path, it is all de-referenced correctly.

I think that this might have implications beyond SSTable loading, e.g. this could happen upon streaming too.

(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java#L245"
CASSANDRA-18839,Catch SSLHandshakeExceptions exceptions,"When SSL connection errors occur, they tend to flood the log with stack traces and lack the identity of the remote client IP.  Instead, PreV5Handlers.decode() could catch SSLHandshakeException and provide a brief, more informative WARN level message instead of the verbose and mostly unhelpful stack trace.

I.e., 
{code:java}
[WARN ] [epollEventLoopGroup-5-5] cluster_id=3 ip_address=10.0.0.1  PreV5Handlers.java:261 - SSLHandshakeException in client networking with peer 10.0.0.10:9042 error:100000d7:SSL routines:OPENSSL_internal:SSL_HANDSHAKE_FAILURE {code}
instead of the current ones which flood the logs:
{code:java}
2023-09-12 00:00:25,368 [WARN ] [epollEventLoopGroup-5-5] cluster_id=3 ip_address=10.0.0.1  PreV5Handlers.java:261 - Unknown exception in client networking
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: error:100000d7:SSL routines:OPENSSL_internal:SSL_HANDSHAKE_FAILURE
    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:478)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
    at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795)
    at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480)
    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: javax.net.ssl.SSLHandshakeException: error:100000d7:SSL routines:OPENSSL_internal:SSL_HANDSHAKE_FAILURE
    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.shutdownWithError(ReferenceCountedOpenSslEngine.java:1031)
    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.sslReadErrorResult(ReferenceCountedOpenSslEngine.java:1321)
    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1270)
    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1346)
    at io.netty.handler.ssl.ReferenceCountedOpenSslEngine.unwrap(ReferenceCountedOpenSslEngine.java:1389)
    at io.netty.handler.ssl.SslHandler$SslEngineType$1.unwrap(SslHandler.java:206)
    at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1387)
    at io.netty.handler.ssl.SslHandler.decodeNonJdkCompatible(SslHandler.java:1294)
    at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1331)
    at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:508)
    at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:447)
    ... 15 common frames omitted {code}"
CASSANDRA-18838,make stop-server shell out of the box,
CASSANDRA-18836,Replace CRC32 w/ CRC32C in IndexFileUtils.ChecksummingWriter,"It seems that now we're on Java 11 for 5.0, there isn't much reason not to use CRC32C as a drop-in replacement for CRC32. SAI isn't even released, so has no binary compatibility entanglements, and this should be pretty straightforward.

See https://github.com/apache/bookkeeper/pull/3309"
CASSANDRA-18834,Testing Pubsub/QBot,creating a ticket of type Bug for QBot testing
CASSANDRA-18833,Testing Pubsub/QBot,Testing workflow for an INFRA ticket
CASSANDRA-18832,Add 5.0-alpha1 to cassandra-dtest upgrade_manifest.py,"With the release of 5.0-alpha1, that version can be added to the dtest-upgrades.

Unfortunately ccm has a bug in it where it cannot determine the supported jdks for binary installs of 5+

Patches:
 - https://github.com/riptano/ccm/compare/master...thelastpickle:ccm:mck/5.0-alpha1
 - https://github.com/apache/cassandra-dtest/compare/trunk...thelastpickle:cassandra-dtest:mck/5.0-alpha1-bump

CI demonstrating the failure
- 5.0 https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/242/workflows/8eee6b06-46d5-4d42-b137-b696b63a190c/jobs/18955
- trunk https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/243/workflows/d5a12c3d-04d2-4de2-a88f-3c8cae09c341/jobs/18956

CI demonstrating the fix
- 5.0 https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/244/workflows/0d0484c0-82c1-49aa-9656-8d0eb6d6648b/jobs/19051
- trunk https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/245/workflows/c6d2963b-6b2a-4b33-b721-b56e84627c48/jobs/19146


Note this does not fix DSE versions, which was already broken."
CASSANDRA-18830,Set io.netty.transport.noNative to false for in-jvm dtests,"This ticket was created as a reaction to (1).

(1) https://lists.apache.org/thread/p42yksvo6t0dy67wmnl2bzpnggo9gpp9"
CASSANDRA-18829,Fix flaky test: org.apache.cassandra.index.sai.disk.SelectiveIntersectionTest.tracingIsCorrectlyReported,"https://ci-cassandra.apache.org/job/Cassandra-5.0/27/testReport/org.apache.cassandra.index.sai.disk/SelectiveIntersectionTest/tracingIsCorrectlyReported_cdc_jdk17/

{quote}
Error Message

Failed to bind port 34357 on 127.0.0.1.

Stacktrace

java.lang.IllegalStateException: Failed to bind port 34357 on 127.0.0.1.
	at org.apache.cassandra.transport.Server.start(Server.java:132)
	at org.apache.cassandra.cql3.CQLTester.startServer(CQLTester.java:649)
	at org.apache.cassandra.cql3.CQLTester.requireNetwork(CQLTester.java:606)
	at org.apache.cassandra.cql3.CQLTester.requireNetwork(CQLTester.java:591)
	at org.apache.cassandra.index.sai.disk.SelectiveIntersectionTest.setup(SelectiveIntersectionTest.java:52)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use
{quote}"
CASSANDRA-18828,Fix dtest: TestTopology.test_resumable_decommission,"Strangely this wasn't failing when CASSANDRA-18815 was done, but it is now for the same cause.

https://ci-cassandra.apache.org/job/Cassandra-5.0/27/testReport/dtest.topology_test/TestTopology/test_resumable_decommission/"
CASSANDRA-18827,Fix pytest deprecations in cqlsh tests,"Pytest reports several warnings regarding deprecated packages or methods:

 
{noformat}
 
../../../../../usr/local/lib/python3.10/site-packages/eventlet/support/greenlets.py:6
  /usr/local/lib/python3.10/site-packages/eventlet/support/greenlets.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    preserves_excinfo = (distutils.version.LooseVersion(greenlet.__version__)
 
../../../../../usr/local/lib/python3.10/site-packages/eventlet/support/greenlets.py:7
  /usr/local/lib/python3.10/site-packages/eventlet/support/greenlets.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    >= distutils.version.LooseVersion('0.3.2'))
 
../../../../../usr/local/lib/python3.10/site-packages/cassandra/io/asyncorereactor.py:34
  /usr/local/lib/python3.10/site-packages/cassandra/io/asyncorereactor.py:34: DeprecationWarning: The asyncore module is deprecated and will be removed in Python 3.12. The recommended replacement is asyncio
    import asyncore
 
cqlshlib/test/test_copyutil.py: 42 warnings
  /Users/brad/Cassandra/cassandra/pylib/cqlshlib/copyutil.py:147: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead
    feeding_thread.setDaemon(True)
 
cqlshlib/test/test_cqlsh_completion.py::testrun_cqlsh
  /usr/local/lib/python3.10/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but cqlshlib/test/test_cqlsh_completion.py::testrun_cqlsh returned <contextlib.closing object at 0x126dbc460>, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(
 
cqlshlib/test/test_cqlsh_output.py::testcall_cqlsh
  /usr/local/lib/python3.10/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but cqlshlib/test/test_cqlsh_output.py::testcall_cqlsh returned ('\nNotice: Credentials in the cqlshrc file is deprecated and will be ignored in the future.\nPlease use a credentials file to specify the username and password.\n\nWARNING: cqlsh was built against 5.1, but this server is 4.1..  All features may not work!\n', 0), which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(
 
cqlshlib/test/test_cqlsh_output.py::testrun_cqlsh
  /usr/local/lib/python3.10/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but cqlshlib/test/test_cqlsh_output.py::testrun_cqlsh returned <contextlib.closing object at 0x12710c520>, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(
 
cqlshlib/test/test_unicode.py::testrun_cqlsh
  /usr/local/lib/python3.10/site-packages/_pytest/python.py:198: PytestReturnNotNoneWarning: Expected None, but cqlshlib/test/test_unicode.py::testrun_cqlsh returned <contextlib.closing object at 0x126c93700>, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(
{noformat}
 

Resolution:
 * distutils Version class is used in cassandra-cql-tests.sh and was introduced in Cassandra [3.11.4 title|[https://github.com/apache/cassandra/blob/cassandra-3.11.4/pylib/cassandra-cqlsh-tests.sh].] It appears to be checking that the version used by ccm is > 3.8 to enable cdc.  Thus the code block can be dropped now in > 4.x.
 * Renamed testrun_cqlsh() to cqlsh_testrun() to avoid being picked up as a pytest

Out of scope:
 * DeprecationWarning: distutils Version from greenlets.py appears to be eventlet issue *[#763|https://github.com/eventlet/eventlet/issues/763]*
 * DeprecationWarning: The asyncore module from asyncorereactor.py appears to come from the eventlet issue [#804|https://github.com/eventlet/eventlet/issues/804]"
CASSANDRA-18826,BLOG - Cassandra Contributor July Meeting Replay,"This ticket is to capture the work associated with publishing the September 2023 blog ""Cassandra Contributor July Meeting Replay""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(September 6)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18824,Backport CASSANDRA-16418: Cleanup behaviour during node decommission caused missing replica,"Node decommission triggers data transfer to other nodes. While this transfer is in progress,
receiving nodes temporarily hold token ranges in a pending state. However, the cleanup process currently doesn't consider these pending ranges when calculating token ownership.
As a consequence, data that is already stored in sstables gets inadvertently cleaned up.

STR:
 * Create two node cluster
 * Create keyspace with RF=1
 * Insert sample data (assert data is available when querying both nodes)
 * Start decommission process of node 1
 * Start running cleanup in a loop on node 2 until decommission on node 1 finishes
 * Verify of all rows are in the cluster - it will fail as the previous step removed some of the rows

It seems that the cleanup process does not take into account the pending ranges, it uses only the local ranges - [https://github.com/apache/cassandra/blob/caad2f24f95b494d05c6b5d86a8d25fbee58d7c2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L466].

There are two solutions to the problem.

One would be to change the cleanup process in a way that it start taking pending ranges into account. Even thought it might sound tempting at first it will require involving changes and a lot of testing effort.

Alternatively we could interrupt/prevent the cleanup process from running when any pending range on a node is detected. That sounds like a reasonable alternative to the problem and something that is relatively easy to implement.

The bug has been already fixed in 4.x with CASSANDRA-16418, the goal of this ticket is to backport it to 3.x."
CASSANDRA-18823,Cleanup behaviour during node decommission caused missing replica,"Node decommission triggers data transfer to other nodes. While this transfer is in progress,
receiving nodes temporarily hold token ranges in a pending state. However, the cleanup process currently doesn't consider these pending ranges when calculating token ownership.
As a consequence, data that is already stored in sstables gets inadvertently cleaned up.

STR:
 * Create two node cluster
 * Create keyspace with RF=1
 * Insert sample data (assert data is available when querying both nodes)
 * Start decommission process of node 1
 * Start running cleanup in a loop on node 2 until decommission on node 1 finishes
 * Verify of all rows are in the cluster - it will fail as the previous step removed some of the rows

It seems that the cleanup process does not take into account the pending ranges, it uses only the local ranges - https://github.com/apache/cassandra/blob/caad2f24f95b494d05c6b5d86a8d25fbee58d7c2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L466.

There are two solutions to the problem.

One would be to change the cleanup process in a way that it start taking pending ranges into account. Even thought it might sound tempting at first it will require involving changes and a lot of testing effort.

Alternatively we could interrupt/prevent the cleanup process from running when any pending range on a node is detected. That sounds like a reasonable alternative to the problem and something that is relatively easy to implement."
CASSANDRA-18821,org.apache.cassandra.distributed.test.guardrails.GuardrailDiskUsageTest failed with Authentication error on host /127.0.0.1:9042: Provided username cassandra and/or password are incorrect,
CASSANDRA-18818,Add cqlshrc.sample and credentials.sample into Debian package,RPM package contains these two files buy DEB does not. Debian users suffer from not having these files available so they need to craft them completely from scratch. 
CASSANDRA-18817,RPM install does not work with jdk17,"The cassandra.in.sh file was updated as part of CASSANDRA-18255 but the corresponding redhat/cassandra.in.sh was not.

This means when cassandra starts on a rpm install using jdk17 the jvm11-server.options file is still used. And so it fails.

Initial patch that works: https://github.com/apache/cassandra/compare/cassandra-5.0...thelastpickle:cassandra:mck/redhat-jdk17-fix/5.0 

Maybe it is possible to find a superior solution, like wrapping this file instead of duplicating it."
CASSANDRA-18816,Add support for repair coordinator to retry messages that timeout,"Now that CASSANDRA-15399 is in, most of the repair messages have a state that they can check against to make message delivery idempotent, allowing the coordinator to retry such messages; a few of the most critical messages to retry are: PREPARE_MSG, VALIDATION_REQ, VALIDATION_RSP, SYNC_REQ, and SYNC_RSP.

With this I propose making the coordinator able to retry these key messages to try and make repair more resilient to ephemeral issues."
CASSANDRA-18815,Fix dtests: replace_address_test.TestReplaceAddress.test_restart_failed_replace and others,"https://ci-cassandra.apache.org/job/Cassandra-5.0/18/testReport/dtest-large.replace_address_test/TestReplaceAddress/test_restart_failed_replace/

This and other similar tests recently started failing:

{noformat}
Unexpected error found in node logs (see stdout for full details). Errors: [[replacement] 'ERROR [Stream-Deserializer-/127.0.0.1:7000-91782e47] 2023-08-29 23:05:51,677 StreamSession.java:700 - [Stream #990152d0-46c0-11ee-9290-158c46e94542] Socket closed before session completion, peer 127.0.0.1:7000 is probably down.\njava.nio.channels.ClosedChannelException: null\n\tat org.apache.cassandra.net.AsyncStreamingInputPlus.reBuffer(AsyncStreamingInputPlus.java:119)\n\tat org.apache.cassandra.io.util.RebufferingInputStream.readByte(RebufferingInputStream.java:178)\n\tat org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:49)\n\tat org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:59)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)']
{noformat}"
CASSANDRA-18814,Repair hangs on Cassandra 4.0.11,"When we run a full repair on Cassandra 4.0.11, it hangs and doesn't evolve. What we noticed was this message:

_{*}""{*}WARN [Messaging-OUT-/214.5.143.5:7001->/214.5.143.4:7001-LARGE_MESSAGES] 2023-08-18 07:02:54,862 OutboundConnection.java:488 - /214.5. 143.5:7001->/214.5.143.4:7001-LARGE_MESSAGES-[no-channel] *dropping message of type VALIDATION_RSP due to error*_
_{*}java.nio.channels.ClosedChannelException{*}: null""_ 

in one of the nodes, in the validate phase the merkle tree.

Besides that, I found some connection reset , but we do not know if there is a relation it.

_18 07:02:54,860 OutboundConnection.java:1056 - /214.5.143.5:7001->/214.5.143.4:7001-LARGE_MESSAGES-94900b4d channel closed by provider
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer_

 

I have uploaded logs from all nodes "
CASSANDRA-18813,Simplify the bind marker and Term logic,"The current logic around {{Term}} and {{Terms}} classes is confusing specially with {{MultiItemTerminal}} and {{MultiColumnRaw}} that are used to handle different use cases that could be handled simply with the {{Term}} interface.

On top of that IN marker add to the confusion because the are represented as single Term where in practice they are a set of terms. Representing them as a {{Terms}} could simplify  the way we handle IN restrictions.

The goal of this ticket is:
*  to refactor the {{Term}} and {{Terms}} interfaces to simplify the logic
* Represents IN bind marker as {{Terms}} instead of having 2 different representations (a list of terms and a single {{MultiItemTerminal}}.
* Simplify the {{AbstractMarker}} hierachy     "
CASSANDRA-18812,Investigate and fix CVE-2023-4586,"CVE-2023-4586 was found by dependency checker on 5.0 / trunk for Netty 4.1.96.

 

HTML report is attached."
CASSANDRA-18811,Set right client auth for creating SSL context in mTLS optional mode,"Adding a new value `optional` for require_client_auth in Encryption options. when require_client_auth is optional, the SSL context that is created will allow client connections that provide a client certificate along with the client connections that do not provide certificates."
CASSANDRA-18810,Cassandra Analytics Start-Up Validation,"Cassandra Analytics should perform a start-up validation of network connectivity to Cassandra and Cassandra Sidecar, as well as of authentication materials"
CASSANDRA-18809,Fix Depends and Build-Depends for Java for Debian packages,"Same needs to happen for 5.0 and trunk as was done in CASSANDRA-18751.

There is still Java 8 and Java 11 combo. We need to throw away Java 8 and add Java 17.

FYI Debian 12 does not have openjdk Java 11 anymore."
CASSANDRA-18808,netty-handler vulnerability: CVE-2023-4586,"This is failing OWASP:

{noformat}
Dependency-Check Failure:
One or more dependencies were identified with vulnerabilities that have a CVSS score greater than or equal to '1.0': 
netty-handler-4.1.96.Final.jar: CVE-2023-4586
{noformat}"
CASSANDRA-18807,Missing license info and headers identified in 5.0-alpha1 candidate,"LICENSE.txt is missing
python-smhasher
OrderedDict
MagnetoDB
java-driver

LICENSE.txt has mistakes around
Chronicle-Bytes
Guava
PATRICIA Trie

pylib docker files needed license headers

ref: https://lists.apache.org/thread/3db8n220qdnpx3jd40pwbtlf14w1b6m7"
CASSANDRA-18805,Upgrade caffeine to 3.1.8 and fix CIDR permissions cache invalidation,"3.1.8 is based on Java 11. This version is testing with newer JDK versions, while 2.x versions are based on JDK8, and as I understand, only bug-fix releases are expected."
CASSANDRA-18804,(Accord): Bug fixes from CASSANDRA-18675 to better support adding keyspaces,"This work is forking CASSANDRA-18675 and pulling out the initial work and bug fixes, but does not actually include drop keyspace support, but does include the testing for add keyspaces.
"
CASSANDRA-18803,Refactor validation logic in StorageService.rebuild,This is a follow-up ticket of CASSANDRA-14319
CASSANDRA-18797,CircleCI: Fix BASE_BRANCH in generate.sh for cassandra-5.0,"We have hardcoded BASE_BRANCH in the CircleCI script generate.sh. That one is used for the diff comparison when we detect new/changed tests.

After adding cassandra-5.0, BASE_BRANCH needs to be updated from trunk to cassandra-5.0 in the newly created branch."
CASSANDRA-18796,Optionally fail when a non-partition-restricted query is issued against a storage-attached index,"With LCS, we will have potentially thousands of SSTables for a given user table. Storage-attached also means SSTable-attached, and searching thousands of attached indexes is not going to scale well at all locally, due to the sheer number of searches and amount of postings list merging involved. We should have a guardrail to prohibit this by default.

Partition-restricted queries, the use-case SAI is broadly designed for, should be very efficient.

UPDATE: The consensus from the discussion below is that we'll add two guardrails here. One is simply for whether we'll allow non-partition-restricted 2i queries. The other is a warn/fail threshold pair for the number of SSTable indexes read locally on a query."
CASSANDRA-18794,Add missing features to 5.0 new features page,"The [page|https://cassandra.apache.org/doc/trunk/cassandra/new/index.html] listing the 5.0 new features is not mentioning multiple new features like trie sstables, unified compaction strategy, dynamic data masking, etc."
CASSANDRA-18792,Test failure: transient_replication_ring_test.py::TestTransientReplicationRing::test_move_forwards_between_and_cleanup,"The Python dtest {{Test failure: transient_replication_ring_test.py::TestTransientReplicationRing::test_move_forwards_between_and_cleanup}} seems to be flaky at least in {{trunk}}:
* https://app.circleci.com/pipelines/github/instaclustr/cassandra/2993/workflows/80ac4db3-fc3d-4908-bc39-dfff6ab88871/jobs/105464/tests
* https://app.circleci.com/pipelines/github/adelapena/cassandra/3128/workflows/b0cf2754-81fd-491e-bac4-cc7fe8b0ac1b/jobs/70390/tests

{code}
ccmlib.node.ToolError: Subprocess ['nodetool', '-h', 'localhost', '-p', '7200', 'cleanup'] exited with non-zero status; exit status: 2; 
stderr: error: Node is involved in cluster membership changes. Not safe to run cleanup.
-- StackTrace --
java.lang.RuntimeException: Node is involved in cluster membership changes. Not safe to run cleanup.
	at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:4037)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)
	at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:262)
	at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at java.management/com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at java.management/com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:814)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:802)
	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1472)
	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1310)
	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1405)
	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:360)
	at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200)
	at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:587)
	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)
	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:705)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:399)
	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:704)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:833)
self = <transient_replication_ring_test.TestTransientReplicationRing object at 0x7f6d38295710>

    @flaky(max_runs=1)
    @pytest.mark.no_vnodes
    def test_move_forwards_between_and_cleanup(self):
        """"""Test moving a node forwards past a neighbor token""""""
        move_token = '00025'
        expected_after_move = [gen_expected(range(0, 26), range(31, 40, 2)),
                               gen_expected(range(0, 21, 2), range(31, 40)),
                               gen_expected(range(1, 11, 2), range(11, 21, 2), range(21, 31)),
                               gen_expected(range(21, 26, 2), range(26, 40))]
        expected_after_repair = [gen_expected(range(0, 26)),
                                 gen_expected(range(0, 21), range(31, 40)),
                                 gen_expected(range(21, 31),),
                                 gen_expected(range(26, 40))]
>       self.move_test(move_token, expected_after_move, expected_after_repair)

transient_replication_ring_test.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transient_replication_ring_test.py:268: in move_test
    cleanup_nodes(nodes)
transient_replication_ring_test.py:43: in cleanup_nodes
    node.nodetool('cleanup')
../env3.6/lib/python3.6/site-packages/ccmlib/node.py:1018: in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port)] + shlex.split(cmd))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

process = <subprocess.Popen object at 0x7f6d376055c0>
cmd_args = ['nodetool', '-h', 'localhost', '-p', '7200', 'cleanup']

    def handle_external_tool_process(process, cmd_args):
        out, err = process.communicate()
        if (out is not None) and isinstance(out, bytes):
            out = out.decode()
        if (err is not None) and isinstance(err, bytes):
            err = err.decode()
        rc = process.returncode
    
        if rc != 0:
>           raise ToolError(cmd_args, rc, out, err)
E           ccmlib.node.ToolError: Subprocess ['nodetool', '-h', 'localhost', '-p', '7200', 'cleanup'] exited with non-zero status; exit status: 2; 
E           stderr: error: Node is involved in cluster membership changes. Not safe to run cleanup.
E           -- StackTrace --
E           java.lang.RuntimeException: Node is involved in cluster membership changes. Not safe to run cleanup.
E           	at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:4037)
E           	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
E           	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
E           	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)
E           	at jdk.internal.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
E           	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
E           	at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:262)
E           	at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
E           	at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
E           	at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
E           	at java.management/com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
E           	at java.management/com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
E           	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:814)
E           	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:802)
E           	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1472)
E           	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1310)
E           	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1405)
E           	at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
E           	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
E           	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
E           	at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:360)
E           	at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200)
E           	at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197)
E           	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
E           	at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196)
E           	at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:587)
E           	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)
E           	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:705)
E           	at java.base/java.security.AccessController.doPrivileged(AccessController.java:399)
E           	at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:704)
E           	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
E           	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
E           	at java.base/java.lang.Thread.run(Thread.java:833)

../env3.6/lib/python3.6/site-packages/ccmlib/node.py:2318: ToolError
{code}
This hasn't been seen yet on Butler but on ad-hoc PRs. It can also be reproduced with the multiplexer:
{code}
.circleci/generate.sh -p \
  -e REPEATED_DTESTS_COUNT=500 \
  -e REPEATED_DTESTS=transient_replication_ring_test.py::TestTransientReplicationRing::test_move_forwards_between_and_cleanup
{code}"
CASSANDRA-18791,"CEP-21 - Multiple TCM fixes for issues discovered by unit, integration and simulation testing","Full branch: [https://github.com/krummas/cassandra/commits/marcuse/cep-21-tcm]
Tests: [cci|https://app.circleci.com/pipelines/github/krummas/cassandra/885/workflows/7cc3e1a1-45b9-4069-bb02-2a46855b4dbe] - current status is: 
unit tests: 35/12052 failures
jvm dtests: 23/1459 failures
python dtests: 110/1018 failures

We will spend the next few weeks getting all the test targets down to 0

Summary of changes;

[CEP-21] Python dtest fixes * maybe fix hintedhandoff test
 - [https://github.com/krummas/cassandra/commit/3da91c26fb]
 - [https://github.com/krummas/cassandra/commit/98e444adbb]

[CEP-21] In-JVM DTest fixes
 - [https://github.com/krummas/cassandra/compare/6491a70041...c9126a8024]

[CEP-21] Unit test fixes
 - [https://github.com/krummas/cassandra/compare/02be7aa71c...6491a70041]

[CEP-21] Escape infinite local log loop on replica mis-configuration
 - [https://github.com/krummas/cassandra/commit/02be7aa71c]
Currently different replicas can have different configurations (guardrails for example) If a transformation is not applied on a replica, this node got stuck in an infinite loop. For now escape that loop until we have a better solution.

[CEP-21] Fix batchlog consistency errors during epoch bumps
 - [https://github.com/krummas/cassandra/commit/c853bf864e]

[CEP-21] Avoid using batches in distributed metadata log keyspace
 - [https://github.com/krummas/cassandra/commit/d4b4766e0b]

[CEP-21] Fix table metadata serialization
 - [https://github.com/krummas/cassandra/commit/4056bab669]

[CEP-21] add more metrics
 - [https://github.com/krummas/cassandra/commit/b6ccb559f5]

[CEP-21] getHostIdForEndpoint return null if unknown endpoint
 - [https://github.com/krummas/cassandra/commit/b9243df05b]

[CEP-21] CMS handling
 - [https://github.com/krummas/cassandra/commit/15eea30d43]
 - [https://github.com/krummas/cassandra/commit/d64da5f5e4]
 - [https://github.com/krummas/cassandra/commit/61deb52811]

[CEP-21] Upgrade fixes
 - [https://github.com/krummas/cassandra/commit/33d186b4ce]
Properly set system.local host id on upgrade.
 - [https://github.com/krummas/cassandra/commit/b96bdc83e1]
If replica misses migration message, set migration as successfull when it sees the first epoch bump.
 - [https://github.com/krummas/cassandra/commit/712828bc82]
Handle hints on upgrade - we change the hostid when enabling CMS, hints should be delivered before that.

[CEP-21] Catchup/log fetching improvements
 - [https://github.com/krummas/cassandra/commit/31a183e236]
When an instance sees a message from a peer with a newer epoch, try to catch up from that peer instead of the CMS to reduce load on the CMS nodes and to allow for cluster to quiesce in the case of the CMS being down.
 - [https://github.com/krummas/cassandra/commit/8c6a4b35db]
We can get a snapshot when catching up, in this case the pending log should first apply the snapshot and skip any previous entries.
 - [https://github.com/krummas/cassandra/commit/387853487f]
When deserializing partition update, allow if current epoch >= serialized epoch
 - [https://github.com/krummas/cassandra/commit/626d224716]
When we replay from a snapshot we might see a node as LEFT for the first time (it was bootstrapped and left while we were down)

[CEP-21] Require Paxos V2 for cluster metadata log operations
 - [https://github.com/krummas/cassandra/commit/2217f551a6]
TCM is required to use Paxos V2 to because of the way the legacy paxos path uses a keyspace’s RF to assert whether there are enough available replicas to perform the read before a CAS. It doesn’t work properly with meta strategy when adding CMS members

[CEP-21] Disaster recovery
 - [https://github.com/krummas/cassandra/commit/9011233604]
Allow an instance to dump its current cluster metadata, and force-boot from it. Basically we need a way to force an instance to become the CMS, in case the original CMS goes down.

[CEP-21] Switch nodeId from uuid to int
 - [https://github.com/krummas/cassandra/commit/aea5500ae0]

[CEP-21] Make CQLSSTableWriter exclusively a client utility
 - [https://github.com/krummas/cassandra/commit/0693b22297]

[CEP-21] Support nodetool assasinate
 - [https://github.com/krummas/cassandra/commit/312a1c1b0e]

[CEP-21] In progress sequence updates
 - [https://github.com/krummas/cassandra/commit/7f56e0e5b3]
Protection against out-of-order and repeated execution, sequence rediscovery and reliability improvements.
 - [https://github.com/krummas/cassandra/commit/7ddb941d80]
DC and RF aware acks for multistep operations. Make progress barrier consistency level configurable.

[CEP-21] Enforce data ownership checks
 - [https://github.com/krummas/cassandra/commit/5c42fd098c]
Never accept operations for ranges we don't own.

[CEP-21] Gossip fixes
 - [https://github.com/krummas/cassandra/commit/8572735e28]
Several gossip issues found during upgrade and load testing
 - [https://github.com/krummas/cassandra/commit/ed785cb414]
Avoid gossip deadlock when merging CM nodes to gossip
 - [https://github.com/krummas/cassandra/commit/e40c3a4ea]
Replaced endpoints should be evicted from gossip like in previous versions.

[CEP-21] Re-enable startup checks on non-test initialization
 - [https://github.com/krummas/cassandra/commit/63013ad366]

[CEP-21] Unify streaming: make all operations use explicit ranges for streaming
 - [https://github.com/krummas/cassandra/commit/ccba2e84de]
All streaming operations now use a movement map describing what should be streamed where.

[CEP-21] Add vtable for metadata log
 - [https://github.com/krummas/cassandra/commit/9fa4d61e5a]

[CEP-21] Add exception code to commit result if rejected
 - [https://github.com/krummas/cassandra/commit/7331e0842b]

[CEP-21] Make cleanup safe to run during range movements
 - [https://github.com/krummas/cassandra/commit/6f990c118f]

[CEP-21] ReplicaPlan recomputation and stillAppliesTo implementation for Paxos
 - [https://github.com/krummas/cassandra/commit/0e5cc6a4fd]

[CEP-21] Update index status fixes post-rebase
 - [https://github.com/krummas/cassandra/commit/06fba6bbc0]

[CEP-21] Create new auth tables, remove cidr constants for column names
 - [https://github.com/krummas/cassandra/commit/fef280dda6]

[CEP-21] Schema fixes
 - [https://github.com/krummas/cassandra/commit/dd9a7e9752]
Schema cleanups, remove old schema pulling
 - [https://github.com/krummas/cassandra/commit/9f0538c4b3]
Don't include system_distributed in initial schema.
 - [https://github.com/krummas/cassandra/commit/4d5fce6884]
Simplify check for whether DROP COMPACT STORAGE is permitted
 - [https://github.com/krummas/cassandra/commit/0bb8efb8f0]
Don't invalidate prepared stmt cache on every schema change
 - [https://github.com/krummas/cassandra/commit/93517d9ee4]
Allow Schema.instance to be initialized empty for client apps
 - [https://github.com/krummas/cassandra/commit/f612d2cd3d]
Simplistic schema metadata diff
 - [https://github.com/krummas/cassandra/commit/42bc2dd5ee]
Don't warn about new system tables in StartupCheck
 - [https://github.com/krummas/cassandra/commit/b570e74bf3]
Exclude meta keyspace from TableMetrics::totalNonSystemTablesSize

[CEP-21] Simulator updates
 - [https://github.com/krummas/cassandra/commit/7e368cfc3e]
Simulate NTS
 - [https://github.com/krummas/cassandra/commit/00a34d88c3]
Multi cms simulation, Deadlines for local processor, reworked retries for local and remote processor
 - [https://github.com/krummas/cassandra/commit/e6dce927da]
Simulator harry integration
 - [https://github.com/krummas/cassandra/commit/f30bf25060]
Eclipse warn

[CEP-21] Bootstrap fixes
 - [https://github.com/krummas/cassandra/commit/6eea8aad69]
ClusterMetadata::writePlacementAllSettled handles bootstrapping nodes correctly
 - [https://github.com/krummas/cassandra/commit/ef9e9c6074]
Reenable write survey mode

[CEP-21] Minor cleanups
 - [https://github.com/krummas/cassandra/commit/335d10c9d6]"
CASSANDRA-18790,Fix spelling errors in the documentation for DDM,[~polandll] has pointed out multiple spelling errors on the documentation for dynamic data masking. This ticket aims to fix them.
CASSANDRA-18789,Update commons-lang3 to 3.13.0,"It seems we are on a version that was testing up to Java 11 - 3.11.0
In the latest one the community goes up to JDK21 so I think it is good to update, hopefully also on cassandra-5.0
Below are the release notes:
https://commons.apache.org/proper/commons-lang/changes-report.html#a3.12.0"
CASSANDRA-18788,BLOG - Town Hall Replay: Time Series Data Modeling for Massive Scale,"This ticket is to capture the work associated with publishing the blog ""Town Hall Replay: Time Series Data Modeling for Massive Scale""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(August 23)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18787,Replace obsolete Python functions,"# Python's stdlib datetime.timezone.utc can replace the custom UTC timezone in pylib/cqlshlib/util.py.
 # Removed unused attributes and functions:
 ## field_size _limit 
 ## trim_if_present()
 ## maybe_ensure_text()
 # Simplify cql_version handling into a settor

Functionality should remain the same while simplifying the code by eliminating ~60 lines."
CASSANDRA-18786,Javadoc BigFormat,This ticket intends to go through the current sstables code and javadoc the format at high-level.
CASSANDRA-18785,Add optional task to run Sonar analysis,"Add the ability to run the Sonar analysis and submit results to the Sonar Server instance.

Additional Ant tasks are available to set up the Sonar server locally in a docker container. The scripts can configure the local server instance using the REST API, which includes setting up the project, getting the analysis token, and installing the quality profile and quality gate (stored in {{.build/sonar}} directory).

The usage description is available in {{.build/README.md}}
"
CASSANDRA-18784,CEP-15: Accord - reduce command deps,"Commands do not need to list every known command as a dependency, by taking deps on the most recently committed commands, and the chain of deps between them and any uncommitted commands."
CASSANDRA-18783,"CEP-15 (C*): when loading commands that have empty waiting_on, make sure not to loose the partial deps","This was found in benchmark clusters

{code}
java.lang.IllegalStateException: Deps do not match; expected {}, {} == [tlp_stress:[(-1182483594468561006,-472993437787424406]]]:{tlp_stress:DecoratedKey(-1000036068852656106, 3030312e302e35383730313035):[[17,1692599889087003,0,1]]}, {}
	accord.utils.Invariants.illegalState(Invariants.java:44)
	accord.utils.Invariants.checkState(Invariants.java:133)
	accord.local.Command$Committed.<init>(Command.java:826)
	accord.local.Command$Committed.<init>(Command.java:817)
	accord.local.Command$Executed.<init>(Command.java:905)
	accord.local.Command$Executed.executed(Command.java:951)
	accord.local.Command$SerializerSupport.executed(Command.java:165)
	org.apache.cassandra.service.accord.AccordKeyspace.loadCommand(AccordKeyspace.java:1197)
{code}

The root cause is that “empty” waiting_on drops the partial deps when we write/load but this becomes an issue for Command.Committed as it double checks partial_deps == waiting_on.deps
"
CASSANDRA-18782,Forbid analyzed SAI indexes on primary key columns,"Queries using SAI indexes don't find any results when the index is on a primary key column, the indexing uses analysis, and the queried value is different to the exact value of the column. For example:
{code:java}
CREATE TABLE t(k int, c text, PRIMARY KEY (k, c));
CREATE INDEX ON t(c) USING 'sai' WITH OPTIONS = { 'case_sensitive' : false };
INSERT INTO t(k, c) VALUES (1, 'A');
SELECT * FROM t WHERE c = 'a'; -- no results found!!!
{code}
This happens because the {{ClusteringIndexFilter}} for the query doesn't take analysis into account. Thus, when that filter is applied by [{{QueryController#doesNotSelect(PrimaryKey)}}|https://github.com/apache/cassandra/blob/655a2455ac29395b0a303e6ad7fc4d458b18932d/src/java/org/apache/cassandra/index/sai/plan/QueryController.java#L194-L210] it rejects the results that have been correctly found by the index.

An initial approach to solve this problem could be making {{ClusteringIndexFilter}} aware of the index analysis options. However, this would be problematic for paging. The first page of the query contains a restriction in the clustering that requires analysis. But subsequent queries will contain the last seen clustering, and we don’t want analysis in that case.

Another approach would be not adding a {{ClusteringIndexFilter}} to the query restrictions when it contains this type of restriction on columns. However, this approach would create a weird situation where adding an index might make {{ALLOW FILTERING}} necessary in queries that wouldn’t need it without the index. This is the opposite of the natural way of things, where more indexes mean less AF needed. For example:
{code:java}
CREATE TABLE t(k int, c1 text, c2 int, PRIMARY KEY (k, c1, c2));
CREATE INDEX idx ON t(c1) USING 'sai' WITH OPTIONS = { 'case_sensitive' : false };
SELECT * FROM t WHERE k = 0 AND c1 = 'a' AND c2 = 0 ALLOW FILTERING;
{code}
The query would need AF because it has been translated into an index query without a clustering filter, and {{c2}} is not indexed.

I think there is an ambiguity in the query, and it's not clear if it should use the secondary index filter and use analysis, or it should be a primary index query and not use analysis. Although we can default to one or another interpretation, both can serve different use cases. We will probably need some new CQL syntax to allow users to specify whether they want to use the secondary index or not.

We can work on those CQL improvements during the second phase of SAI. In the meantime, I think we should simply forbid the creation of analyzed indexes on primary key columns."
CASSANDRA-18781,Add the ability to disable bulk loading of SSTables on a node,"Currently, Cassandra database users can use sstableloader to bulk load data into Cassandra. However, for a Cassandra operator, there is no way to forcibly block this behavior. Additionally, there is no metric indicating whether the bulk load is being used on the server side. If a client is using sstableloader, they will also need to upgrade the sstableloader code to the new major version. This lack of control and visibility can become a blocker during a major version upgrade.

 

1. Can we add a config to disable bulk load feature? Or it falls into https://issues.apache.org/jira/browse/CASSANDRA-8303

2. Can we add metrics for bulk load used on server end?"
CASSANDRA-18778,Empty keystore_password no longer allowed on encryption_options,"After CASSANDRA-18124 (introduced in 4.1.2 and 5.0) it is no longer possible to set an empty {{keystore_password}} under {{client_encryption_options}} or {{server_encryption_options}} using the default implementation {{{}DefaultSslContextFactory{}}}.

While keytool does not allow generating keystores with empty passwords, it does support reading them. It is not uncommon to use PKCS12 certificates generated by other tools (eg. openssl) that do not enforce passwords.

The fix for this should be pretty straightforward, which should involve changing [FileBasedSslContextFactory.validatePassword|https://github.com/apache/cassandra/blob/cassandra-4.1.2/src/java/org/apache/cassandra/security/FileBasedSslContextFactory.java#L128-L135] to only disallow null passwords (which would be consistent with previous versions). I will create pull requests against the relevant branches shortly.
{noformat}
Exception (org.apache.cassandra.exceptions.ConfigurationException) encountered during startup: Failed to initialize SSL
org.apache.cassandra.exceptions.ConfigurationException: Failed to initialize SSL
	at org.apache.cassandra.config.DatabaseDescriptor.applySslContext(DatabaseDescriptor.java:1155)
	at org.apache.cassandra.config.DatabaseDescriptor.applyAll(DatabaseDescriptor.java:390)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:204)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:188)
	at org.apache.cassandra.service.CassandraDaemon.applyConfig(CassandraDaemon.java:804)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:747)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:875)
Caused by: java.io.IOException: Failed to create SSL context using Native transport
	at org.apache.cassandra.security.SSLFactory.validateSslContext(SSLFactory.java:405)
	at org.apache.cassandra.config.DatabaseDescriptor.applySslContext(DatabaseDescriptor.java:1150)
	... 6 more
Caused by: java.lang.IllegalArgumentException: 'keystore_password' must be specified
	at org.apache.cassandra.security.FileBasedSslContextFactory.validatePassword(FileBasedSslContextFactory.java:133)
	at org.apache.cassandra.security.FileBasedSslContextFactory.buildKeyManagerFactory(FileBasedSslContextFactory.java:151)
	at org.apache.cassandra.security.AbstractSslContextFactory.createNettySslContext(AbstractSslContextFactory.java:181)
	at org.apache.cassandra.security.SSLFactory.createNettySslContext(SSLFactory.java:168)
	at org.apache.cassandra.security.SSLFactory.validateSslContext(SSLFactory.java:355)
	... 7 more
{noformat}"
CASSANDRA-18776,Restore accidentally deleted documentation for DDM,CASSANDRA-18228 seems to have deleted [the section of documentation about dynamic data masking (CEP-20)|https://github.com/apache/cassandra/blob/63ab8e09286e4b2076695f62e08a7f39cb6860d1/doc/modules/cassandra/pages/cql/dynamic_data_masking.adoc]. The code examples and the (broken) index entry are still present. This ticket should add it back.
CASSANDRA-18775,Remove libraries in lib/sigar-bin for unsupported architectures,"{code}
✔ ~/dev/cassandra/cassandra-instaclustr/cassandra [trunk L|⚑ 49] 
15:41 $ ls -la lib/sigar-bin/
total 6376
drwxrwxr-x 2 fermat fermat   4096 aug 17 11:26 .
drwxrwxr-x 5 fermat fermat  12288 aug 17 11:28 ..
-rw-rw-r-- 1 fermat fermat 210641 aug 17 11:26 libsigar-amd64-freebsd-6.so
-rw-rw-r-- 1 fermat fermat 246605 aug 17 11:26 libsigar-amd64-linux.so
-rw-rw-r-- 1 fermat fermat 251360 aug 17 11:26 libsigar-amd64-solaris.so
-rw-rw-r-- 1 fermat fermat 577452 aug 17 11:26 libsigar-ia64-hpux-11.sl
-rw-rw-r-- 1 fermat fermat 494929 aug 17 11:26 libsigar-ia64-linux.so
-rw-rw-r-- 1 fermat fermat 516096 aug 17 11:26 libsigar-pa-hpux-11.sl
-rw-rw-r-- 1 fermat fermat 425077 aug 17 11:26 libsigar-ppc64-aix-5.so
-rw-rw-r-- 1 fermat fermat 310792 aug 17 11:26 libsigar-ppc64le-linux.so
-rw-rw-r-- 1 fermat fermat 330767 aug 17 11:26 libsigar-ppc64-linux.so
-rw-rw-r-- 1 fermat fermat 400925 aug 17 11:26 libsigar-ppc-aix-5.so
-rw-rw-r-- 1 fermat fermat 258547 aug 17 11:26 libsigar-ppc-linux.so
-rw-rw-r-- 1 fermat fermat 269932 aug 17 11:26 libsigar-s390x-linux.so
-rw-rw-r-- 1 fermat fermat 261896 aug 17 11:26 libsigar-sparc64-solaris.so
-rw-rw-r-- 1 fermat fermat 285004 aug 17 11:26 libsigar-sparc-solaris.so
-rw-rw-r-- 1 fermat fermat 397440 aug 17 11:26 libsigar-universal64-macosx.dylib
-rw-rw-r-- 1 fermat fermat 377668 aug 17 11:26 libsigar-universal-macosx.dylib
-rw-rw-r-- 1 fermat fermat 179751 aug 17 11:26 libsigar-x86-freebsd-5.so
-rw-rw-r-- 1 fermat fermat 179379 aug 17 11:26 libsigar-x86-freebsd-6.so
-rw-rw-r-- 1 fermat fermat 233385 aug 17 11:26 libsigar-x86-linux.so
-rw-rw-r-- 1 fermat fermat 242880 aug 17 11:26 libsigar-x86-solaris.so
✔ ~/dev/cassandra/cassandra-instaclustr/cassandra [trunk L|⚑ 49] 
15:43 $ du -shc lib/sigar-bin/
6,3M    lib/sigar-bin/
6,3M    total
{code}

I think we could definitely improve this. We basically need just x86-linux, amd64-linux and two libs for macosx.

We could save like 5M from the final tarball size. From 75M down to 70M is not bad!

Or maybe I am not getting something and we need this?"
CASSANDRA-18774,"pre-commit check for submodule does not consider remotes other than ""origin""","So when having a feature branch in a private fork, I always get an error"
CASSANDRA-18773,Compactions are slow,"I have noticed that compactions involving a lot of sstables are very slow (for example major compactions). I have attached a cassandra stress profile that can generate such a dataset under ccm. In my local test I have 2567 sstables at 4Mb each.

I added code to track wall clock time of various parts of the code. One problematic part is ManyToOne constructor. Tracing through the code for every partition creating a ManyToOne for all the sstable iterators for each partition. In my local test get a measy 60Kb/sec read speed, and bottlenecked on single core CPU (since this code is single threaded) with it spending 85% of the wall clock time in ManyToOne constructor.

As another datapoint to show its the merge iterator part of the code using the cfstats from [https://github.com/instaclustr/cassandra-sstable-tools/] which reads all the sstables but does no merging gets 26Mb/sec read speed.

Tracking back from ManyToOne call I see this in UnfilteredPartitionIterators::merge
{code:java}
                for (int i = 0; i < toMerge.size(); i++)
                {
                    if (toMerge.get(i) == null)
                    {
                        if (null == empty)
                            empty = EmptyIterators.unfilteredRow(metadata, partitionKey, isReverseOrder);
                        toMerge.set(i, empty);
                    }
                }
 {code}
Not sure what purpose of creating these empty rows are. But on a whim I removed all these empty iterators before passing to ManyToOne and then all the wall clock time shifted to CompactionIterator::hasNext() and read speed increased to 1.5Mb/s.

So there are further bottlenecks in this code path it seems, but the first is this ManyToOne and having to build it for every partition read."
CASSANDRA-18772,Remove dependency on commons-codec,"The dependency doesn't seem to have any direct usage in our codebase.
The only reference to how it might be used was found in CASSANDRA-12790. Considering we also deprecate in 5.0 and remove metrics-reporter-config as part of CASSANDRA-18743, I do not see a reason not to remove this one too. 

We should probably take the same approach as CASSANDRA-18743 - deprecate in 5.0 and remove in 5.1. ML thread is to be opened too. "
CASSANDRA-18769,Fix flaky test: org.apache.cassandra.cql3.MemtableSizeTest.testTruncationReleasesLogSpace,"{noformat}
java.lang.reflect.InaccessibleObjectException: Unable to make field private final jdk.management.jfr.StreamManager jdk.management.jfr.FlightRecorderMXBeanImpl.streamHandler accessible: module jdk.management.jfr does not ""opens jdk.management.jfr"" to unnamed module @6db9f5a4
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:340)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:280)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:176)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:170)
	at org.github.jamm.MemoryMeter.addFieldChildren(MemoryMeter.java:330)
	at org.github.jamm.MemoryMeter.measureDeep(MemoryMeter.java:269)
	at org.apache.cassandra.utils.ObjectSizes.measureDeep(ObjectSizes.java:216)
	at org.apache.cassandra.cql3.MemtableSizeTest.testSize(MemtableSizeTest.java:121)
	at org.apache.cassandra.Util.runCatchingAssertionError(Util.java:644)
	at org.apache.cassandra.Util.flakyTest(Util.java:669)
	at org.apache.cassandra.cql3.MemtableSizeTest.testTruncationReleasesLogSpace(MemtableSizeTest.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}

Seen here: https://app.circleci.com/pipelines/github/driftx/cassandra/1203/workflows/087cb08a-d035-491f-bb90-208b25a5fd4b/jobs/44699/tests"
CASSANDRA-18768,"Harry changes required for CEP-21 testing (NTS support, concurrent r/w improvements, and more)",
CASSANDRA-18767,tablestats should show speculative retries,"NodeProbe makes [provisions|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/NodeProbe.java#L1921] for it, but it is never used later."
CASSANDRA-18766,high speculative retries on v4.1.3,"There are up to 10+ times higher speculative retries for reads on 4.1.3 comparing to 4.0.7 and 4.1.2 when using QUORUM and default setting of 99p.

On 4.1.3 after upgrade I see speculative retries for up to 35% of all reads for specific table. Latency for reads is stable around 500 microseconds.

java 1.8.0_382 is used"
CASSANDRA-18765,Add CASSANDRA-14227 to 5.0 new features page,The [page|https://cassandra.apache.org/doc/trunk/cassandra/new/index.html] listing the 5.0 new features is not mentioning the extended TTL implemented in CASSANDRA-14227
CASSANDRA-18764,CEP-15 (C*) When a host replacement happens don't loose the peer mapping right away,"When a host replacement happens the latest epoch will no longer have the removed peer, but we may have in-flight messages that still reference it.  When C* restarts we need to also filter out these peers so we don’t wait for an epoch ack."
CASSANDRA-18763,BLOG - Town Hall Replay: Bad Partition Handling & Large Language Models,"This ticket is to capture the work associated with publishing the blog ""Town Hall Replay: Bad Partition Handling & Large Language Models""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(August 16)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18762,Repair triggers OOM with direct buffer memory,"We are seeing repeated failures of nodes with 16GB of heap on a VM with 32GB of physical RAM due to direct memory.  This seems to be related to CASSANDRA-15202 which moved Merkel trees off-heap in 4.0.   Using Cassandra 4.0.6 with Java 11.
{noformat}
2023-08-09 04:30:57,470 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e55a3b0-366d-11ee-a644-d91df26add5e] Received merkle tree for table_a from /169.102.200.241:7000
2023-08-09 04:30:57,567 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e0d2900-366d-11ee-a644-d91df26add5e] Received merkle tree for table_b from /169.93.192.29:7000
2023-08-09 04:30:57,568 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e1dcad0-366d-11ee-a644-d91df26add5e] Received merkle tree for table_c from /169.104.171.134:7000
2023-08-09 04:30:57,591 [INFO ] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 RepairSession.java:202 - [repair #5e69a0e0-366d-11ee-a644-d91df26add5e] Received merkle tree for table_b from /169.79.232.67:7000
2023-08-09 04:30:57,876 [INFO ] [Service Thread] cluster_id=101 ip_address=169.0.0.1 GCInspector.java:294 - G1 Old Generation GC in 282ms. Compressed Class Space: 8444560 -> 8372152; G1 Eden Space: 7809794048 -> 0; G1 Old Gen: 1453478400 -> 820942800; G1 Survivor Space: 419430400 -> 0; Metaspace: 80411136 -> 80176528
2023-08-09 04:30:58,387 [ERROR] [AntiEntropyStage:1] cluster_id=101 ip_address=169.0.0.1 JVMStabilityInspector.java:102 - OutOfMemory error letting the JVM handle the error:
java.lang.OutOfMemoryError: Direct buffer memory
at java.base/java.nio.Bits.reserveMemory(Bits.java:175)
at java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118)
at java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:318)
at org.apache.cassandra.utils.MerkleTree.allocate(MerkleTree.java:742)
at org.apache.cassandra.utils.MerkleTree.deserializeOffHeap(MerkleTree.java:780)
at org.apache.cassandra.utils.MerkleTree.deserializeTree(MerkleTree.java:751)
at org.apache.cassandra.utils.MerkleTree.deserialize(MerkleTree.java:720)
at org.apache.cassandra.utils.MerkleTree.deserialize(MerkleTree.java:698)
at org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer.deserialize(MerkleTrees.java:416)
at org.apache.cassandra.repair.messages.ValidationResponse$1.deserialize(ValidationResponse.java:100)
at org.apache.cassandra.repair.messages.ValidationResponse$1.deserialize(ValidationResponse.java:84)
at org.apache.cassandra.net.Message$Serializer.deserializePost40(Message.java:782)
at org.apache.cassandra.net.Message$Serializer.deserialize(Message.java:642)
at org.apache.cassandra.net.InboundMessageHandler$LargeMessage.deserialize(InboundMessageHandler.java:364)
at org.apache.cassandra.net.InboundMessageHandler$LargeMessage.access$1100(InboundMessageHandler.java:317)
at org.apache.cassandra.net.InboundMessageHandler$ProcessLargeMessage.provideMessage(InboundMessageHandler.java:504)
at org.apache.cassandra.net.InboundMessageHandler$ProcessMessage.run(InboundMessageHandler.java:429)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:834)no* further _formatting_ is done here{noformat}
 
-XX:+AlwaysPreTouch
-XX:+CrashOnOutOfMemoryError
-XX:+ExitOnOutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:+ParallelRefProcEnabled
-XX:+PerfDisableSharedMem
-XX:+ResizeTLAB
-XX:+UseG1GC
-XX:+UseNUMA
-XX:+UseTLAB
-XX:+UseThreadPriorities
-XX:-UseBiasedLocking
-XX:CompileCommandFile=/opt/nosql/clusters/cassandra-101/conf/hotspot_compiler
-XX:G1RSetUpdatingPauseTimePercent=5
-XX:G1ReservePercent=20
-XX:HeapDumpPath=/opt/nosql/data/cluster_101/cassandra-1691623098-pid2804737.hprof
-XX:InitiatingHeapOccupancyPercent=70
-XX:MaxGCPauseMillis=200
-XX:StringTableSize=60013
-Xlog:gc*:file=/opt/nosql/clusters/cassandra-101/logs/gc.log:time,uptime:filecount=10,filesize=10485760
-Xms16G
-Xmx16G
-Xss256k
 
From our Prometheus metrics, the behavior shows the direct buffer memory ramping up until it reaches the max and then causes an OOM.  It would appear that direct memory is never being released by the JVM until its exhausted.
 
!Cluster-dm-metrics.PNG!

An Eclipse Memory Analyzer

Class Histogram:
||Class Name||Objects||Shallow Heap||Retained Heap||
|java.lang.Object[]|445,014|42,478,160|>= 4,603,280,344| |
|io.netty.util.concurrent.FastThreadLocalThread|167|21,376|>= 4,467,294,736|

Leaks: Problem Suspect 1
The thread *io.netty.util.concurrent.FastThreadLocalThread @ 0x501dd5930 AntiEntropyStage:1* keeps local variables with total size *4,295,042,472 (84.00%)* bytes."
CASSANDRA-18761,"Change the logging level from WARN to INFO when invoked from ""insights"" thread (Spinning trying to capture readers)","The metric collector for Apache Cassandra (MCAC) is trying to get stats about the sstables to report metrics while compaction has them locked.

Since the warning message is from the ""{*}insights{*}"" thread rather than the ""{*}compaction{*}"" thread, the message is considered harmless.

Since the message is harmless and doesn't require action when reported from the “{*}insights”{*} thread{*}.{*} The message logging level should be moved from WARN to INFO.

Sample WARN Message:
WARN [insights-6-1] 2022-09-21 12:00:53,416 NoSpamLogger.java:95 - Spinning trying to capture readers [BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-17753-big-Data.db'), BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-19976-big-Data.db'), BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-18476-big-Data.db'), BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-21870-big-Data.db'), BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-20638-big-Data.db'), BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-22723-big-Data.db'), BigTableReader(path='/cassandra/data/tokens/events-d111f320f8e011ec8ae317b93c6db81b/nb-21223-big-Data.db'),"
CASSANDRA-18760,Backport CASSANDRA-16905 to older branches,"Recently hit an un-recoverable situation in Cassandra 4.0.10 after dropping a 'map' column and adding it back as a 'blob', which caused corruption that neither offline nor online scrub could fix.
When dropping a 'blob' column and attempting to add it back as a 'map' type, the operation is blocked with:

{code:java}
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot re-add previously dropped column 'col1' of type map<int, tinyint>, incompatible with previous type blob""
{code}

We need to do the same going from 'map' to 'blob' to avoid this potentially very bad scenario that can cause data loss.
"
CASSANDRA-18759,Introduce in-jvm dtest for testing in Analytics project,"Now that the in-jvm dtest framework supports JMX, we’ve updated the Cassandra sidecar to use that as its base test framework. We should use this support in the Analytics project so we can run tests more quickly and allow for more advanced testing (startup/shutdown/move/decommission/expand)

Work to complete this ticket would be to incorporate the sidecar’s test framework into the analytics project and get the `SampleCassandraJob` to run successfully on an in-jvm 3-node cluster with the Sidecar running. We can then leverage this framework in future tests."
CASSANDRA-18757,UnifiedCompactionTask is incorrectly setting keepOriginals,"{code:java}
super(cfs, txn, gcBefore, strategy.getController().getIgnoreOverlapsInExpirationCheck());{code}
in {{UnifiedCompactionTask}} is calling the base constructor
{code:java}
 public CompactionTask(ColumnFamilyStore cfs, LifecycleTransaction txn, long gcBefore, boolean keepOriginals)
{code}
which can set {{keepOriginals}} to true when it should not be."
CASSANDRA-18756,TimeWindowCompactionStrategy with unsafe_aggressive_sstable_expiration keeps overlaping SSTable references,"When {{unsafe_aggressive_sstable_expiration}} is turned on, TWCS should not create or maintain an iterator of overlapping sstables. However, because {{TimeWindowCompactionController}} inherits from {{CompactionController}} and only sets {{ignoreOverlaps}} after the base class has constructed the overlap iterator, it ends up making an overlap iterator and then never updating it.

The end result is that such a compaction keeps references to lots of and likely _all_ other SSTables on the node and thus delays the deletion of obsolete ones by hours or even days."
CASSANDRA-18755,Optimized default configuration,"We currently offer only one sample configuration file with Cassandra, and that file is deliberately configured to disable all new functionality and incompatible improvements. This works well for legacy users that want to have a painless upgrade, but is a very bad choice for new users, or anyone wanting to make comparisons between Cassandra versions or between Cassandra and other databases.

We offer very little indication, in the database packaging itself, that there are well-tested configuration choices that can solve known problems and dramatically improve performance. This is guaranteed to paint the database in a worse light than it deserves, and will very likely hurt adoption.

We should find a way to offer a very easy way of choosing between ""optimized"" and ""compatible"" defaults. At minimal, we could provide alternate yaml files. Alternatively, we could build on the {{storage_compatibility_mode}} concept to grow it into a setting that not only enables/disables certain settings, but also changes their default values."
CASSANDRA-18754,We should offer an option for optimized default configuration,"We currently offer only one sample configuration file with Cassandra, and that file is deliberately configured to disable all new functionality and incompatible improvements. This works well for legacy users that want to have a painless upgrade, but is a very bad choice for new users, or anyone wanting to make comparisons between Cassandra versions or between Cassandra and other databases.

We offer very little indication, in the database packaging itself, that there are well-tested configuration choices that can solve known problems and dramatically improve performance. This is guaranteed to paint the database in a worse light than it deserves, and will very likely hurt adoption.

We should find a way to offer a very easy way of choosing between ""optimized"" and ""compatible"" defaults. At minimal, we could provide alternate yaml files. Alternatively, we could build on the {{storage_compatibility_mode}} concept to grow it into a setting that not only enables/disables certain settings, but also changes their default values."
CASSANDRA-18753,Add an optimized default configuration to tests and make it available for new users,"We currently offer only one sample configuration file with Cassandra, and that file is deliberately configured to disable all new functionality and incompatible improvements. This works well for legacy users that want to have a painless upgrade, but is a very bad choice for new users, or anyone wanting to make comparisons between Cassandra versions or between Cassandra and other databases.

We offer very little indication, in the database packaging itself, that there are well-tested configuration choices that can solve known problems and dramatically improve performance. This is guaranteed to paint the database in a worse light than it deserves, and will very likely hurt adoption.

We should find a way to offer a very easy way of choosing between ""optimized"" and ""compatible"" defaults. At minimal, we could provide alternate yaml files. Alternatively, we could build on the {{storage_compatibility_mode}} concept to grow it into a setting that not only enables/disables certain settings, but also changes their default values."
CASSANDRA-18752,Deal with commented assertion in UFTest#testSchemaChange,"[~maedhroz] reported that there is an assertion left commented out in UFTest#testSchemaChange in CASSANDRA-18252.

It should be either removed or fixed if needed. 

[https://github.com/apache/cassandra/pull/2154/files#diff-d22803d878bbcebc2bfb5e73ec39cea75ae0800f3cfbeb81d0bce358ab2b3c00R118-R126]"
CASSANDRA-18751,Fix Requires for Java for RPM packages,"We have to change Requires in redhat/cassandra.spec to

java-1.8.0 for 3.0 and 3.11
(java-1.8.0 or java-11) for 4.0. and 4.1
(java-11 or java-17) for 5.0 and trunk

noboolean would contain only java-1.8.0 and java-11 respectively where applicable."
CASSANDRA-18750,Make unit tests compatible with the tmp.dir property,"Several unit tests hard-code file paths under the ""/tmp"" directory, which means they do not honor the {{tmp.dir}} ant build property. These should be updated so that when the user specifies {{tmp.dir}}, they can be certain that any files or directories created by the unit tests will be in their specified location."
CASSANDRA-18749,Expose bootstrap failure state via JMX,"Similar to how we have exposed a node's decommissioning via JMX as part of CASSANDRA-18555.

This ticket will address the same when a new node joins (i.e., bootstrap). 

When a node is bootstrapped, an error is thrown back to the caller if any failure happens.

But Cassandra's bootstrap takes considerable time ranging from minutes to hours to days. There are various scenarios in that the caller may need to probe the status again:
 * The caller times out
 * It is not possible to keep the caller hanging for such a long time

And If the caller does not know what happened internally, then it cannot retry, etc., leading to other issues.

So, in this ticket, a new nodetool/JMX option will be exposed that can be invoked by the caller anytime, and it will return the correct status."
CASSANDRA-18747,Test failure: Fix assertion error AssertionError: Unknown keyspace system_auth\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162),"I've been seeing this assertion error in different tests lately.

Full error message:
{code:java}
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [PendingRangeCalculator:1] 2023-08-11 16:35:14,445 JVMStabilityInspector.java:70 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]\njava.lang.AssertionError: Unknown keyspace system_auth\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162)\n\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\n\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:162)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:151)\n\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\n\tat org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']"" Unexpected error found in node logs (see stdout for full details). Errors: [[node2] 'ERROR [PendingRangeCalculator:1] 2023-08-11 16:35:14,445 JVMStabilityInspector.java:70 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]\njava.lang.AssertionError: Unknown keyspace system_auth\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:324)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:162)\n\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\n\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:162)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:151)\n\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\n\tat org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)']{code}
Example failures:
test_failed_snitch_update_property_file_snitch - [https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2475/workflows/2086619e-0f21-464b-a866-84aca516b5e5/jobs/36716/tests]
test_gcgs_validation - [https://ci-cassandra.apache.org/job/Cassandra-trunk/1666/testReport/junit/dtest.materialized_views_test/TestMaterializedViews/test_gcgs_validation/]"
CASSANDRA-18745,cqlsh should warn on server version mismatch,"When cqlsh is used against a different version of the server than the one that it shipped with, it should emit a warning so that users aren't confused when some things don't work correctly."
CASSANDRA-18744,cassandra-stress in simplenative mode and prepared fails with exceptions,"% cassandra-stress write n=10 -mode simplenative cql3 prepared

...

ap'; org.apache.cassandra.transport.messages.ResultMessage$Prepared is in unnamed module of loader 'app')

java.lang.ClassCastException: class [B cannot be cast to class org.apache.cassandra.transport.messages.ResultMessage$Prepared ([B is in module java.base of loader 'bootstrap'; org.apache.cassandra.transport.messages.ResultMessage$Prepared is in unnamed module of loader 'app')

java.io.IOException: Operation x10 on key(s) [4e334f364c4c4b373530]: Error executing: (ClassCastException): class [B cannot be cast to class org.apache.cassandra.transport.messages.ResultMessage$Prepared ([B is in module java.base of loader 'bootstrap'; org.apache.cassandra.transport.messages.ResultMessage$Prepared is in unnamed module of loader 'app')

 

 at org.apache.cassandra.stress.Operation.error(Operation.java:127)

 at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:105)

 at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:91)

 at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:99)

 at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:242)

 at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:471)

java.io.IOException: Operation x10 on key(s) [373038504b3436363830]: Error executing: (ClassCastException): class [B cannot be cast to class org.apache.cassandra.transport.messages.ResultMessage$Prepared ([B is in module java.base of loader 'bootstrap'; org.apache.cassandra.transport.messages.ResultMessage$Prepared is in unnamed module of loader 'app')

 

 at org.apache.cassandra.stress.Operation.error(Operation.java:127)

 at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:105)

 at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:91)

 at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:99)

 at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:242)

 at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:471)

java.lang.RuntimeException: Failed to execute warmup

 at org.apache.cassandra.stress.StressAction.warmup(StressAction.java:128)

 at org.apache.cassandra.stress.StressAction.run(StressAction.java:71)

 at org.apache.cassandra.stress.Stress.run(Stress.java:101)

 at org.apache.cassandra.stress.Stress.main(Stress.java:54)"
CASSANDRA-18743,Remove dependency on archived project metrics-reporter-config,"metrics-reporter-config has been archived for over 3 years, and has received no updates in nearly 6 years:
https://github.com/addthis/metrics-reporter-config

metrics-reporter-config processes YAML unsafely: https://github.com/addthis/metrics-reporter-config/blob/master/reporter-config-base/src/main/java/com/addthis/metrics/reporter/config/AbstractReporterConfig.java#L39C34-L39C45

This is related to CASSANDRA-18614, which is focused on migrating to safe YAML processing for the main configuration files."
CASSANDRA-18742,Remove deprecated options in CompressionParams,"These should go away (1). They were deprecated like 8 years ago in CASSANDRA-9712 and CASSANDRA-9839.

We should also remove this (2). That is a little bit more tricky but nothing special I would say ...

(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/schema/CompressionParams.java#L86-L88
(2) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/schema/CompressionParams.java#L96-L97"
CASSANDRA-18741,Some unit tests create a heap dump each time,"I noticed this by running out of space while working on CASSANDRA-18706.  Something must be calling HeaupUtils.maybeCreateHeapDump because the build/test directory is littered with pidxxxx-epochxxxxxxxx.hprof files despite the tests not crashing.  I checked a couple other random tests, RandomSchemaTest also generates a dump but KeywordSplitTest does not.

This behavior does not occur on 4.1."
CASSANDRA-18740,Fix compilation warning in RandomSchemaTest,"There are compilation warning for RandomSchemaTest and it just pollutes the ant build log unnecessarily. We should just fix it. The fix is about casting the objects to execute method to (Object[]).

{code}
_build-test:
    [javac] Compiling 1874 source files to /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/build/test/classes
    [javac] Note: Processing compiler hints annotations
    [javac] Note: Processing compiler hints annotations
    [javac] Note: Processing compiler hints annotations
    [javac] Note: Writing compiler command file at META-INF/hotspot_compiler
    [javac] Note: Done processing compiler hints annotations
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:131: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     execute(insertStmt, expected);
    [javac]                                         ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:133: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     assertRows(execute(selectStmt, rowKey), expected);
    [javac]                                                    ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:134: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     assertRows(execute(tokenStmt, partitionKeys), partitionKeys);
    [javac]                                                   ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:135: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     assertRowsNet(executeNet(selectStmt, rowKey), expected);
    [javac]                                                          ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:140: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     assertRows(execute(selectStmt, rowKey), expected);
    [javac]                                                    ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:141: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     assertRows(execute(tokenStmt, partitionKeys), partitionKeys);
    [javac]                                                   ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning
    [javac] /home/fermat/dev/cassandra/cassandra-instaclustr/cassandra-5.0/test/unit/org/apache/cassandra/cql3/RandomSchemaTest.java:142: warning: non-varargs call of varargs method with inexact argument type for last parameter;
    [javac]                     assertRowsNet(executeNet(selectStmt, rowKey), expected);
    [javac]                                                          ^
    [javac]   cast to Object for a varargs call
    [javac]   cast to Object[] for a non-varargs call and to suppress this warning

{code}"
CASSANDRA-18739,UDF functions fail to load on rolling restart,"UDFs fail to reload properly after a rolling restart.
h3. *Symptom:*

NPE thrown when used after restart.
h3. *Steps to recreate:*
 # Create a cluster as per cql file
 # Populate the cluster with data.cql.
 # Execute SELECT city_measurements(city, measurement, 16.5) AS m FROM current
 # expect min and max values for cities.
 # Performing a rolling restart on one server.
 # When the server is back up
 # Execute SELECT city_measurements(city, measurement, 16.5) AS m FROM current
 # expect: error result with NPE message.


{*}Analysis{*}:

During system restart the SchemaKeyspace.fetchNonSystemKeyspaces() is called, when a keyspace with a UDF is loaded the SchemaKeyspace method createUDFFromRow() is called, this in turn calls UDFunction.create() which eventually calls back to UDFunction constructor where the Schema.instance.getKeyspaceMetadata() is called with the keyspace for the UDF name as the argument. However, the keyspace for the UDF name is being constructed and is not yet in the instance so the method returns null for the KeyspaceMetadata. That null KeyspaceMetadata is then used in the udfContext.

Later when the UDF method is called, if there is a need to call a method on the keyspaceMetadata, such as udfContext.newUDTValue() where the implementation uses keyspaceMetadata.types, a null pointer is thrown.

I have verified this affects version 4.0, 4.1 and trunk. I have not verified 3.x but I suspect it is the same there.

I modified UDFunction constructor to assert that the metadata was not null and received the following stack trace

ERROR [main] 2023-08-09 11:44:46,408 CassandraDaemon.java:911 - Exception encountered during startup
java.lang.AssertionError: No metadata for temperatures.city_measurements_sfunc
    at org.apache.cassandra.cql3.functions.UDFunction.<init>(UDFunction.java:240)
    at org.apache.cassandra.cql3.functions.JavaBasedUDFunction.<init>(JavaBasedUDFunction.java:195)
    at org.apache.cassandra.cql3.functions.UDFunction.create(UDFunction.java:276)
    at org.apache.cassandra.schema.SchemaKeyspace.createUDFFromRow(SchemaKeyspace.java:1182)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchUDFs(SchemaKeyspace.java:1131)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchFunctions(SchemaKeyspace.java:1119)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:859)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:848)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:836)
    at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:132)
    at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:121)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:287)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:765)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:889)

 

{{*Possible solution:*}}

*Version 4.x*

Create a KeyspaceMetadata.Builder class that uses accepts the types, tables and views but uses a builder for the functions.

Add a KeyspaceMetadata constructor to accept the KeyspaceMetadata.Builder so that the function builder keyspaceMetadata value can be set correctly during construction of the KeyspaceMetadata.

Modify SchemaKeyspace.fetchKeyspace(string) so that it uses the KeyspaceMetadata.Builder.

 

*Version 5.x*

Similar to 4.x except that the KeyspaceMetadata.Builder will have to have builders for Views and Tables because the functions necessary to construct those objects will not be available until the KeyspaceMetadata.Builder constructs it.

 "
CASSANDRA-18738,Update the How to Commit page after CASSANDRA-18618,"In CASSANDRA-18618, checks like rat and checkstyle were moved to a separate ant task - ""check"".

We need to update our How to commit page that on branches Cassandra-5.0 + we need to run now ant realclean && ant jar build-test check pre-commit. While on that, we already have the cassandra-5.0 branch, so that we can add that too."
CASSANDRA-18737,Test failure: org.apache.cassandra.io.sstable.SSTableLoaderTest (testLoadingIncompleteSSTable-.jdk17),"{noformat}
java.lang.RuntimeException: Failed to list files in /tmp/1409486429862512729/SSTableLoaderTest/Standard2-57877ac036d311eea01f83fcb8f6fee5
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:77)
	at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getFiles(LifecycleTransaction.java:626)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:103)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:202)
	at org.apache.cassandra.io.sstable.SSTableLoaderTest.testLoadingIncompleteSSTable(SSTableLoaderTest.java:213)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /tmp/1409486429862512729/SSTableLoaderTest/Standard2-57877ac036d311eea01f83fcb8f6fee5/nc-17-big
	at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:111)
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:397)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openForBatch(SSTableReader.java:373)
	at org.apache.cassandra.io.sstable.SSTableLoader.lambda$openSSTables$0(SSTableLoader.java:152)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.lambda$innerList$2(LogAwareFileLister.java:99)
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:178)
	at java.base/java.util.TreeMap$EntrySpliterator.forEachRemaining(TreeMap.java:3287)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)
	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.innerList(LogAwareFileLister.java:101)
	at org.apache.cassandra.db.lifecycle.LogAwareFileLister.list(LogAwareFileLister.java:73)
Caused by: java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:903)
	at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.buildSummaryAndBloomFilter(BigSSTableReaderLoadingBuilder.java:193)
	at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:116)
	at org.apache.cassandra.io.sstable.format.big.BigSSTableReaderLoadingBuilder.openComponents(BigSSTableReaderLoadingBuilder.java:58)
	at org.apache.cassandra.io.sstable.format.SSTableReaderLoadingBuilder.build(SSTableReaderLoadingBuilder.java:92)
{noformat}

Seen here: https://app.circleci.com/pipelines/github/driftx/cassandra/1174/workflows/263f1e22-e4d0-48b8-b3e2-496edb30a068/jobs/41924/tests"
CASSANDRA-18736,Streaming exception race creates corrupt transaction log files that prevent restart,"On restart, Cassandra logs this message and terminates.
{code:java}
ERROR 2023-07-17T17:17:22,931 [main] org.apache.cassandra.db.lifecycle.LogTransaction:561 - Unexpected disk state: failed to read transaction log [nb_txn_stream_39d5f6b0-fb81-11ed-8f46-e97b3f61511e.log in /datadir1/keyspace/table-c9527530a0d611e8813f034699fc9043]
Files and contents follow:
/datadir1/keyspace/table-c9527530a0d611e8813f034699fc9043/nb_txn_stream_39d5f6b0-fb81-11ed-8f46-e97b3f61511e.log
        ABORT:[,0,0][737437348]
                ***This record should have been the last one in all replicas
        ADD:[/datadir1/keyspace/table-c9527530a0d611e8813f034699fc9043/nb-284490-big-,0,8][2493503833]
{code}
The root cause is a race during streaming exception handling.

Although concurrent modification of to the {{LogTransaction}} was added for CASSANDRA-16225, there is nothing to prevent usage after the transaction is completed (committed/aborted) once it has been processed by {{TransactionTidier}} (after the last reference is released). Before the transaction is tidied, the {{LogFile}} keeps a list of records that are checked for completion before adding new entries. In {{TransactionTidier}} {{LogFile.records}} are cleared as no longer needed, however the LogTransaction/LogFile is still accessible to the stream.

The changes in CASSANDRA-17273 added a parallel set of {{onDiskRecords}} that could be used to reliably recreate the transaction log at any new datadirs the same as the existing
datadirs - regardless of the effect of {{LogTransaction.untrackNew/LogFile.remove}}

If a streaming exception causes the LogTransaction to be aborted and tidied just before {{SimpleSSTableMultiWriter}} calls trackNew to add a new sstable. At the time of the call, the {{LogFile}} will not contain any {{LogReplicas}},
{{LogFile.records}} will be empty, and {{LogFile.onDiskRecords}} will contain an {{ABORT}}.

When {{LogTransaction.trackNew/LogFile.add}} is called, the check for completed transaction fails as records is empty, there are no replicas on the datadir, so {{maybeCreateReplicas}} creates a new txnlog file replica containing ABORT, then
appends an ADD record.

The LogFile has already been tidied after the abort so the txnlog file is not removed and sits on disk until a restart, causing the faiulre.

There is a related exception caused with a different interleaving of aborts, after an sstable is added, however this is just a nuisance in the logs as the LogRelica is already created with an {{ADD}} record first.
{code:java}
java.lang.AssertionError: [ADD:[/datadir1/keyspace/table/nb-23314378-big-,0,8][1869379820]] is not tracked by 55be35b0-35d1-11ee-865d-8b1e3c48ca06
        at org.apache.cassandra.db.lifecycle.LogFile.remove(LogFile.java:388)
        at org.apache.cassandra.db.lifecycle.LogTransaction.untrackNew(LogTransaction.java:158)
        at org.apache.cassandra.db.lifecycle.LifecycleTransaction.untrackNew(LifecycleTransaction.java:577)
        at org.apache.cassandra.db.streaming.CassandraStreamReceiver$1.untrackNew(CassandraStreamReceiver.java:149)
        at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.abort(SimpleSSTableMultiWriter.java:95)
        at org.apache.cassandra.io.sstable.format.RangeAwareSSTableWriter.abort(RangeAwareSSTableWriter.java:191)
        at org.apache.cassandra.db.streaming.CassandraCompressedStreamReader.read(CassandraCompressedStreamReader.java:115)
        at org.apache.cassandra.db.streaming.CassandraIncomingFile.read(CassandraIncomingFile.java:85)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:53)
        at org.apache.cassandra.streaming.messages.IncomingStreamMessage$1.deserialize(IncomingStreamMessage.java:38)
        at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:53)
        at org.apache.cassandra.streaming.async.StreamingInboundHandler$StreamDeserializingTask.run(StreamingInboundHandler.java:172)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}"
CASSANDRA-18735,Test failure: Fix Python DTests post CASSANDRA-18676,"From Butler:
|failling|DTEST|u.cql_tests |[TestCQLNodes3RF3_Upgrade_current_4_1_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_4_1_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes2RF1_Upgrade_indev_4_0_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_indev_4_0_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[cls.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/cls/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes3RF3_Upgrade_indev_4_1_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_indev_4_1_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes3RF3_Upgrade_indev_4_0_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_indev_4_0_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes3RF3_Upgrade_current_4_0_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_4_0_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes2RF1_Upgrade_current_4_0_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_4_0_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes2RF1_Upgrade_current_4_1_x_To_indev_trunk.test_negative_timestamp (upgrade) |https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_4_1_x_To_indev_trunk/test_negative_timestamp]|
|failing|DTEST|u.cql_tests |[TestCQLNodes2RF1_Upgrade_indev_4_1_x_To_indev_trunk.test_negative_timestamp (upgrade)|https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_indev_4_1_x_To_indev_trunk/test_negative_timestamp]|"
CASSANDRA-18734,SAI result retriever is filtering too many rows,"Performance tests on SAI have shown that the number of rows being filtered for wide row partitions is more than expected. For a 10k row/partition test - limit 10, the following has been observed.

!image-2023-08-08-13-49-19-576.png|width=952,height=368!

This is not the expected outcome of row-awareness and needs investigating. The number of rows read should, roughly, match the number of partitions read.

 "
CASSANDRA-18733,Waiting indefinitely on ReceivedMessage response in StreamSession#receive() can cause deadlock,"I've observed in a recent stack trace from a node running 4.1 what looks like a deadlock around the {{StreamSession}} monitor lock when {{StreamSession#receive()}} waits via {{syncUninteruptibly()}} for a response to a control message.

{noformat}
""Messaging-EventLoop-3-10"" #320 daemon prio=5 os_prio=0 cpu=57979617.98ms elapsed=5587916.03s tid=0x00007f056e88ae00 nid=0x80ec waiting for monitor entry  [0x00007f056d277000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:524)
        - waiting to lock <0x00000006816fae70> (a org.apache.cassandra.streaming.StreamSession)
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:690)
        at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel.onMessageComplete(StreamingMultiplexedChannel.java:264)
        at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel.lambda$sendMessage$1(StreamingMultiplexedChannel.java:233)
        at org.apache.cassandra.streaming.async.StreamingMultiplexedChannel$$Lambda$2029/0x00000008007a0c40.operationComplete(Unknown Source)
        at org.apache.cassandra.utils.concurrent.ListenerList.notifyListener(ListenerList.java:134)
        at org.apache.cassandra.utils.concurrent.ListenerList.notifyListener(ListenerList.java:148)
        at org.apache.cassandra.utils.concurrent.ListenerList$GenericFutureListenerList.notifySelf(ListenerList.java:190)
        at org.apache.cassandra.utils.concurrent.ListenerList.lambda$notifyExclusive$0(ListenerList.java:124)
        at org.apache.cassandra.utils.concurrent.ListenerList$$Lambda$950/0x0000000800666040.accept(Unknown Source)
        at org.apache.cassandra.utils.concurrent.IntrusiveStack.forEach(IntrusiveStack.java:195)
        at org.apache.cassandra.utils.concurrent.ListenerList.notifyExclusive(ListenerList.java:124)
        at org.apache.cassandra.utils.concurrent.ListenerList.notify(ListenerList.java:96)
        at org.apache.cassandra.utils.concurrent.AsyncFuture.trySet(AsyncFuture.java:104)
        at org.apache.cassandra.utils.concurrent.AbstractFuture.tryFailure(AbstractFuture.java:148)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.tryFailure(AsyncPromise.java:139)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:1009)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:870)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
        at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(java.base@11.0.16/Thread.java:829)
{noformat}

It seems that while {{receive()} is holding the monitor lock on {{StreamSession}}, the callback that executes on a different thread for the control message it sends carries an error. This error, when handled in {{onError()}}, then calls {{closeSession()}}, which tries to acquire the monitor lock already held in {{receive()}}.

{noformat}
""Stream-Deserializer-/100.70.229.6:7000-de724029"" #1919000 daemon prio=5 os_prio=0 cpu=224.66ms elapsed=1604976.92s tid=0x00007f0561c66500 nid=0xe2a2 waiting on condition  [0x00007f0830947000]
   java.lang.Thread.State: WAITING (parking)
        at jdk.internal.misc.Unsafe.park(java.base@11.0.16/Native Method)
        at java.util.concurrent.locks.LockSupport.park(java.base@11.0.16/LockSupport.java:323)
        at org.apache.cassandra.utils.concurrent.WaitQueue$Standard$AbstractSignal.await(WaitQueue.java:289)
        at org.apache.cassandra.utils.concurrent.WaitQueue$Standard$AbstractSignal.await(WaitQueue.java:282)
        at org.apache.cassandra.utils.concurrent.Awaitable$AsyncAwaitable.await(Awaitable.java:306)
        at org.apache.cassandra.utils.concurrent.AsyncFuture.await(AsyncFuture.java:154)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.await(AsyncPromise.java:244)
        at org.apache.cassandra.net.AsyncChannelPromise.await(AsyncChannelPromise.java:127)
        at org.apache.cassandra.net.AsyncChannelPromise.await(AsyncChannelPromise.java:34)
        at org.apache.cassandra.utils.concurrent.Awaitable$Defaults.awaitUninterruptibly(Awaitable.java:186)
        at org.apache.cassandra.utils.concurrent.AbstractFuture.awaitUninterruptibly(AbstractFuture.java:482)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.awaitUninterruptibly(AsyncPromise.java:254)
        at org.apache.cassandra.net.AsyncChannelPromise.awaitUninterruptibly(AsyncChannelPromise.java:133)
        at org.apache.cassandra.net.AsyncChannelPromise.awaitUninterruptibly(AsyncChannelPromise.java:34)
        at org.apache.cassandra.utils.concurrent.Future.syncUninterruptibly(Future.java:94)
        at org.apache.cassandra.utils.concurrent.AsyncPromise.syncUninterruptibly(AsyncPromise.java:186)
        at org.apache.cassandra.net.AsyncChannelPromise.syncUninterruptibly(AsyncChannelPromise.java:121)
        at org.apache.cassandra.net.AsyncChannelPromise.syncUninterruptibly(AsyncChannelPromise.java:34)
        at org.apache.cassandra.streaming.StreamSession.receive(StreamSession.java:1054)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:628)
        - locked <0x00000006816fae70> (a org.apache.cassandra.streaming.StreamSession)
        at org.apache.cassandra.streaming.StreamDeserializingTask.run(StreamDeserializingTask.java:76)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(java.base@11.0.16/Thread.java:829)
{noformat}

The most straightforward way to fix this might be to put an upper bound on the wait in {{receive()}}, along w/ some logging at WARN if it times out. This would at least allow us to make progress eventually and close the session properly. The syncUninterruptibly() was intended to avoid a race during shutdown, IIRC, and unless the timeout is comically low, it shouldn't compromise that.

The problem can usually be fixed by bouncing affected nodes, who will usually present w/ an increasing backlog of unrepaired data and a log message that looks something like this, in addition to a number of streaming errors:

{noformat}
INFO  2023-08-04T10:45:54,845 [NettyStreaming-Outbound-/<ip:port>] org.apache.cassandra.streaming.async.StreamingMultiplexedChannel:359 - [Stream #10465c10-3108-11ee-af8a-3f74fd21ad9d] waiting to acquire a permit to begin streaming <data directory>-nb-101920-big-Data.db. This message logs every 3 minutes
{noformat}"
CASSANDRA-18732,Baseline Diagnostic vtables for Accord,"In addition to JMX-based metrics, there are bits of diagnostic information for Accord that we should consider exposing through vtables:

1.) We should ensure that coordinator-level CQL transactions and the local reads and writes they spawn are visible to the existing {{QueriesTable}} vtable.

The first may already just work. We may need to make some tweaks to {{TxnNamedRead}} and {{TxnWrite}} for the local operations though. ({{CommandStore}} tasks are out of scope here, as they would probably be more confusing than useful in {{QueriesTable}}?)

2.) A new vtable for pending commands for a key.

- Disable SELECT */require a partition key
- Might require partial back-port of stringifying table/partition key from Accord to be correct
- ex. {{SELECT timestamps FROM myawesometable where ks=? and table=? and partition_key=?}}
- Clustering can be the Accord timestamp elements, no further normal columns.

3.) A new vtable for command store-specific cache stats

- Gather via {{Store.execute()}} for correctness.
- Store id should be partition key (see {{AccordCommandStore}})
- hits, misses, total (maybe just throw out the keyspaces and coalesce ranges?)

4.) (Requires [~aweisberg]'s outstanding work) A new vtable for live migration state

- {{TableMigrationState}} could be flattened into a row
- Is this already persisted? If so, why a new vtable?

5.) A vtable to expose {{accord.local.Node#coordinating()}} as a map

- ex. {{SELECT txn_id, type}}"
CASSANDRA-18730,Add guardrail for vector dimensions,Add a guardrail limiting the number of dimensions of vector columns. It would be analogous to the current guardrails limiting the number of items per collection and the number of fields per UDT.
CASSANDRA-18729,Remove unnecessary Netty dependencies after upgrade to 4.1.96,"We introduced a lot of unnecessary dependencies in CASSANDRA-17992 which should not be there. This ticket is about their removal.

The original patch also got some dependencies wrong. They are 4.1.94 but we are on 4.1.96."
CASSANDRA-18727,JMXUtil.getJmxConnector should retry connection attempts ,"We previously added a JMXUtil class that makes it easy for dtests to get a JMX connection. It ends up that occasionally the JMX server side needs more time to start up (especially when stopping and restarting instances, which we’re now doing more frequently, or when stopping and restarting the whole cluster). In these cases, JMXUtil.getJmxConnector can fail, when it would be possible to connect if a retry mechanism was added. We should add this capability to the in-jvm dtest framework.
 "
CASSANDRA-18726,Simplify helptopics.py using associative array,"The ~200 lines of code in helptopics.py can be greatly simplified using an associative array to map topics to HTML anchors instead of class reflection. This will halve the lines of code and improve readability.

Functionality will not be changed."
CASSANDRA-18725,IsolatedJMX should not release all TCPEndpoints on instance shutdown,"In the original implementation of the JMX feature, we fixed some memory leaks by clearing some internal state in Java’s TCPEndpoint. However, that implementation was overly aggressive and cleared the whole map, vs. just removing the endpoints created by the individual instances. This causes issues when you remove a node from the cluster (as all of the endpoints are cleared, not just the ones in use by that instance).
 
In stead, we should check if the endpoint was created by the instance in question and only remove it if it was."
CASSANDRA-18724,Investigate unmatched suppressions,"Since we upgraded OWASP, it now reports unmatched suppressions (""Suppression Rule had zero matches"") we should investigate and possibly remove."
CASSANDRA-18723,bcprov-jdk15on-1.70.jar vulnerability: CVE-2023-33201,"https://nvd.nist.gov/vuln/detail/CVE-2023-33201

{quote}
Bouncy Castle For Java before 1.74 is affected by an LDAP injection vulnerability. The vulnerability only affects applications that use an LDAP CertStore from Bouncy Castle to validate X.509 certificates. During the certificate validation process, Bouncy Castle inserts the certificate's Subject Name into an LDAP search filter without any escaping, which leads to an LDAP injection vulnerability.
{quote}"
CASSANDRA-18722,Support Dynamic Port Allocation for in-jvm dtest framework,"Currently, {{INodeProvisionStrategy}} supports two strategies {{OneNetworkInterface}} and {{MultipleNetworkInterfaces}}. However the {{seedPort}}, {{storagePorts}}, {{nativeTransportPorts}}, and {{jmxPorts}} are always fixed or a function of the node number.

In order to better support parallel test runs, we need to support dynamic port allocation for the {{seedPort}}, {{storagePorts}}, {{nativeTransportPorts}}, and {{jmxPorts}}. This would enable us to more easily write tests that can run in parallel. This effort is only a stepping stone in what's required to run more tests in parallel, but it allows us to begin somewhere with the in-jvm dtest framework."
CASSANDRA-18721,Wildcard rat checks for files named by version,"Instead of adding a rat exclude every time we add another file with the version in the name, we can use a wildcard."
CASSANDRA-18720,BLOG - Town Hall Replay: Monitoring Apache Cassandra Without Implementation,"This ticket is to capture the work associated with publishing the blog ""Town Hall Replay: Monitoring Apache Cassandra Without Implementation""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(Aug 2)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18718,Fix gen-doc task,"As discussed on CASSANDRA-17687, we see below errors when ant gen-doc is run:
{code:java}
gen-asciidoc: [exec] python3 ./scripts/gen-nodetool-docs.py [exec] python3 ./scripts/convert_yaml_to_adoc.py ../conf/cassandra.yaml ./modules/cassandra/pages/configuration/cass_yaml_file.adoc [exec] Traceback (most recent call last): [exec] File ""/home/jlewandowski/dev/cassandra/trunk/doc/./scripts/convert_yaml_to_adoc.py"", line 144, in <module> [exec] convert(yaml_file, dest_file) [exec] File ""/home/jlewandowski/dev/cassandra/trunk/doc/./scripts/convert_yaml_to_adoc.py"", line 61, in convert [exec] with open(dest_file, 'w') as outfile: [exec] FileNotFoundError: [Errno 2] No such file or directory: './modules/cassandra/pages/configuration/cass_yaml_file.adoc' [exec] make: *** [Makefile:26: gen-asciidoc] Error 1 [exec] Result: 2{code}
-This seems to have been broken during the migration from rst to asciidoc.- "
CASSANDRA-18716,In-Memory write support for a vector search index,This ticket will introduce the write side in-memory components of the vector index.
CASSANDRA-18715,Add support for vector search in SAI,"The patch associated with this ticket adds a new vector index to SAI. This introduces the following new elements and changes to SAI:
 * VectorMemtableIndex - the in-memory representation of the vector indexes that writes data to a DiskANN instance
 * VectorSegmentBuilder - that writes a DiskANN graph to the following on-disk components:
 ** VECTOR - contains the floating point vectors associated with the graph
 ** TERMS - contains the HNSW graph on-disk representation written by a HnswGraphWriter
 ** POSTINGS - contains the index postings as written by a VectorPostingsWriter
 * VectorIndexSegmentSearcher - used to search the on-disk DiskANN graph"
CASSANDRA-18714,Expand CQLSSTableWriter to write SSTable-attached secondary indexes,"{{CQLSSTableWriter}} currently has no way of writing any secondary indexes inline as it writes the core SSTable components. With SAI, this has become tractable problem, and we should be able to enhance both it and {{SSTableImporter}} to handle cases where we might want to write SSTables somewhere in bulk (and in parallel) and then import them without waiting for index building on import. It would require the following changes:

1.) {{CQLSSTableWriter}} must accept 2i definitions on top of its current table schema definition. Once added to the schema, any {{ColumnFamilyStore}} instances opened will have those 2i defined in their index managers.

2.) All {{AbstractSSTableSimpleWriter}} instances must register index groups, allowing the proper {{SSTableFlushObservers}} to be attached to {{SSTableWriter}}. Once this is done, SAI (and any other SSTable-attached indexes) components will be built incrementally along w/ the SSTable data file, and will be finalized when the newly written SSTable is finalized.

3.) Provide an example (in a unit test?) of how a third-party tool might, assuming access to the right C* JAR, validate/checksum SAI components outside C* proper.

4.) {{SSTableImporter}} should have two new options:
    a.) an option that fails import if any SSTable-attached 2i must be built (i.e. has not already been built and brought along w/ the other new SSTable components)
    b.) an option that allows us to bypass full checksum validation on imported/already-built SSTable-attached indexes (assuming they have just been written by {{CQLSSTableWriter}})"
CASSANDRA-18713,Fix / align ConfigCompatabilityTest,
CASSANDRA-18710,Test failure: org.apache.cassandra.io.DiskSpaceMetricsTest.testFlushSize-.jdk17 (from org.apache.cassandra.io.DiskSpaceMetricsTest-.jdk17),"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1644/testReport/org.apache.cassandra.io/DiskSpaceMetricsTest/testFlushSize__jdk17/]
h3.  
{code:java}
Error Message
expected:<7200.0> but was:<1367.83970468544>

Stacktrace
junit.framework.AssertionFailedError: expected:<7200.0> but was:<1367.83970468544> at org.apache.cassandra.io.DiskSpaceMetricsTest.testFlushSize(DiskSpaceMetricsTest.java:119) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 "
CASSANDRA-18709,Test failure: dtest.jmx_test.TestJMX.test_compactionstats,"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1646/testReport/dtest.jmx_test/TestJMX/test_compactionstats/]
h3.  
{code:java}
Error Message
AttributeError: 'NoneType' object has no attribute 'named'

Stacktrace
self = <jmx_test.TestJMX object at 0x7fe78cfc2940> def test_compactionstats(self): """""" @jira_ticket CASSANDRA-10504 @jira_ticket CASSANDRA-10427 Test that jmx MBean used by nodetool compactionstats properly updates the progress of a compaction """""" cluster = self.cluster cluster.populate(1) node = cluster.nodelist()[0] cluster.start() # Run a quick stress command to create the keyspace and table node.stress(['write', 'n=1', 'no-warmup']) # Disable compaction on the table node.nodetool('disableautocompaction keyspace1 standard1') node.nodetool('setcompactionthroughput 1') node.stress(['write', 'n=150K', 'no-warmup']) node.flush() # Run a major compaction. This will be the compaction whose # progress we track. node.nodetool_process('compact') # We need to sleep here to give compaction time to start # Why not do something smarter? Because if the bug regresses, # we can't rely on jmx to tell us that compaction started. time.sleep(5) compaction_manager = make_mbean('db', type='CompactionManager') with JolokiaAgent(node) as jmx: progress_string = jmx.read_attribute(compaction_manager, 'CompactionSummary')[0] # Pause in between reads # to allow compaction to move forward time.sleep(2) updated_progress_string = jmx.read_attribute(compaction_manager, 'CompactionSummary')[0] var = 'Compaction@{uuid}(keyspace1, standard1, {progress}/{total})bytes' if self.cluster.version() >= LooseVersion('4.0'): # CASSANDRA-15954 var = 'Compaction({taskUuid}, {progress} / {total} bytes)@{uuid}(keyspace1, standard1)' > progress = int(parse.search(var, progress_string).named['progress']) E AttributeError: 'NoneType' object has no attribute 'named' jmx_test.py:218: AttributeError
{code}
 "
CASSANDRA-18708,Test failure: org.apache.cassandra.tools.StandaloneSplitterWithCQLTesterTest.testNoSnapshotOption-.jdk17,"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1650/testReport/junit/org.apache.cassandra.tools/StandaloneSplitterWithCQLTesterTest/testNoSnapshotOption__jdk17/]
h3.  
{code:java}
Error Message
java.lang.reflect.InaccessibleObjectException: Unable to make field private final sun.nio.fs.UnixFileSystem sun.nio.fs.UnixPath.fs accessible: module java.base does not ""opens sun.nio.fs"" to unnamed module @1f28c152 at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354) at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297) at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178) at java.base/java.lang.reflect.Field.setAccessible(Field.java:172) at org.apache.cassandra.utils.concurrent.Ref.getFields(Ref.java:656) at org.apache.cassandra.utils.concurrent.Ref$Visitor.traverse(Ref.java:613) at org.apache.cassandra.utils.concurrent.Ref$Visitor.run(Ref.java:568) at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133) at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:833)
{code}
 "
CASSANDRA-18707,Test failure: junit.framework.TestSuite.org.apache.cassandra.distributed.test.CASMultiDCTest-.jdk11 ,"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1650/testReport/junit.framework/TestSuite/org_apache_cassandra_distributed_test_CASMultiDCTest__jdk11/]
h3.  
{code:java}
Error Message
Schema agreement not reached. Schema versions of the instances: [ef1c8e05-a06d-388d-a46d-53cc22a94762, 6c386108-1805-3985-b48e-8016012a0207, 6c386108-1805-3985-b48e-8016012a0207, ef1c8e05-a06d-388d-a46d-53cc22a94762]

Stacktrace
java.lang.IllegalStateException: Schema agreement not reached. Schema versions of the instances: [ef1c8e05-a06d-388d-a46d-53cc22a94762, 6c386108-1805-3985-b48e-8016012a0207, 6c386108-1805-3985-b48e-8016012a0207, ef1c8e05-a06d-388d-a46d-53cc22a94762] at org.apache.cassandra.distributed.impl.AbstractCluster$ChangeMonitor.waitForCompletion(AbstractCluster.java:907) at org.apache.cassandra.distributed.impl.AbstractCluster.lambda$schemaChange$8(AbstractCluster.java:836) at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829)
{code}
 "
CASSANDRA-18706,Test failure: org.apache.cassandra.cql3.ViewTest.testIgnoreUpdate-compression.jdk11 (from org.apache.cassandra.cql3.ViewTest-compression.jdk11),"Seen here - [https://ci-cassandra.apache.org/job/Cassandra-trunk/1650/testReport/junit/org.apache.cassandra.cql3/ViewTest/testIgnoreUpdate_compression_jdk11/]
h3.  
{code:java}
Error Message
expected:<1> but was:<2>

Stacktrace
junit.framework.AssertionFailedError: expected:<1> but was:<2> at org.apache.cassandra.cql3.ViewTest.testIgnoreUpdate(ViewTest.java:328) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at org.jboss.byteman.contrib.bmunit.BMUnitRunner$6.evaluate(BMUnitRunner.java:263) at org.jboss.byteman.contrib.bmunit.BMUnitRunner$1.evaluate(BMUnitRunner.java:97)
{code}
 

 "
CASSANDRA-18705,Create release branch cassandra-5.0,"Figure out what needs to be done beyond the following…

1. Create release branch
{code:java}
git switch -c cassandra-5.0 trunk
git push --set-upstream origin cassandra-5.0
{code}
2. Bump trunk's version
{code:java}
git switch trunk
# increment version to 5.1
edit build.xml debian/changelog CHANGES.txt NEWS.txt
{code}

2a. Update jvm-dtest supported upgrade paths, and dtest jar generation (both circleci and in-tree scripts), in both cassandra-5.0 and trunk branches:
 - [https://github.com/apache/cassandra/blob/trunk/test/distributed/org/apache/cassandra/distributed/upgrade/UpgradeTestBase.java#L85-L96]
 - [https://github.com/apache/cassandra/blob/trunk/.circleci/config.yml#L2374]
 - [https://github.com/apache/cassandra/blob/trunk/.build/run-tests.sh#L85]

2b. Bump versions in cassandra-5.0 branch
 - doc/antora.yml 
 - build.xml, append ""-alpha1"" to `base.version`

3. Add `5.0-alpha`, `5.0-alpha1`, `5.0-beta`, `5.0-rc`, `5.0.x` and `5.1` to jira versions
(no existing tickets will be changed - assignees need to change appropriate bugs from 5.x to what is appropriate…)

4. Update docker images to include cassandra-5.0
(Docker images also need to be deployed)

5. Add pipeline to ci-cassandra
[https://github.com/apache/cassandra-builds/blob/trunk/jenkins-dsl/cassandra_job_dsl_seed.groovy#L51]

6. Add dtest version and upgrade paths
 - [https://github.com/apache/cassandra-dtest/blob/trunk/upgrade_tests/upgrade_manifest.py]
 - [https://github.com/apache/cassandra-builds/blob/trunk/build-scripts/cassandra-test.sh#L41]

7. Update how_to_commit documentation
[https://github.com/apache/cassandra-website/blob/trunk/site-content/source/modules/ROOT/pages/development/how_to_commit.adoc]

8. Update website CI to trigger off cassandra-5.0 builds
 - [https://github.com/apache/cassandra-builds/commit/1fc9b5ee71dc37e1145f276ead5c680c6b3fe3db]

9. Update website to build new branches
 - https://github.com/apache/cassandra-website/blob/trunk/site-content/Dockerfile#L93"
CASSANDRA-18704,Test Failure: HandshakeTest.testOutboundConnectionfFallbackDuringUpgrades ClassCastException: Established ->  Disconnected,"The following has been witnesses a number of times in repeated runs, in different jdks and different test configurations.
{noformat}
java.lang.ClassCastException: class org.apache.cassandra.net.OutboundConnection$Established cannot be cast to class org.apache.cassandra.net.OutboundConnection$Disconnected (org.apache.cassandra.net.OutboundConnection$Established and org.apache.cassandra.net.OutboundConnection$Disconnected are in unnamed module of loader 'app')
	at org.apache.cassandra.net.OutboundConnection$State.disconnected(OutboundConnection.java:201)
	at org.apache.cassandra.net.OutboundConnection$1Initiate.initiate(OutboundConnection.java:1248)
	at org.apache.cassandra.net.OutboundConnection.initiate(OutboundConnection.java:1254)
	at org.apache.cassandra.net.HandshakeTest.initiateOutbound(HandshakeTest.java:365)
	at org.apache.cassandra.net.HandshakeTest.testOutboundFallbackOnSSLHandshakeFailure(HandshakeTest.java:380)
	at org.apache.cassandra.net.HandshakeTest.testOutboundConnectionfFallbackDuringUpgrades(HandshakeTest.java:255)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}

ref: https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/217/workflows/ce7392fa-756a-4392-b3a5-206bb2940553/jobs/15965/tests "
CASSANDRA-18703,Test failure: SSTablesSystemViewTest#testVirtualTableThroughIndexLifeCycle,"The utest {{org.apache.cassandra.index.sai.virtual.SSTablesSystemViewTest#testVirtualTableThroughIndexLifeCycle}} is consistently broken with tries at least on trunk:

* https://app.circleci.com/pipelines/github/adelapena/cassandra/3068/workflows/17760f03-9807-41cc-9cb3-a1fdf3e60f77/jobs/64069/tests
* https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/215/workflows/efcd6699-c1d2-48c0-99de-b35544106264/jobs/15684/tests
* https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/215/workflows/efcd6699-c1d2-48c0-99de-b35544106264/jobs/15692/tests

{code}
junit.framework.AssertionFailedError: Got 4 extra row(s) and 2 missing row(s) in result.  Extra rows:
    row(index_name=table_00_v1_idx, sstable_name=/tmp/cassandra/build/test/cassandra/data/cql_test_keyspace/table_00-12e640202d2a11eea926d15115276651/da-5-bti-Data.db, table_name=table_00, column_name=v1, format_version=aa, cell_count=1, min_row_id=0, max_row_id=0, start_token=8213365047359667313, end_token=8213365047359667313, per_table_disk_size=335, per_column_disk_size=270)
    row(index_name=table_00_v2_idx, sstable_name=/tmp/cassandra/build/test/cassandra/data/cql_test_keyspace/table_00-12e640202d2a11eea926d15115276651/da-5-bti-Data.db, table_name=table_00, column_name=v2, format_version=aa, cell_count=1, min_row_id=0, max_row_id=0, start_token=8213365047359667313, end_token=8213365047359667313, per_table_disk_size=335, per_column_disk_size=274)
    row(index_name=table_00_v2_idx, sstable_name=/tmp/cassandra/build/test/cassandra/data/cql_test_keyspace/table_00-12e640202d2a11eea926d15115276651/da-6-bti-Data.db, table_name=table_00, column_name=v2, format_version=aa, cell_count=4, min_row_id=0, max_row_id=4, start_token=-8982230457741691068, end_token=5293579765126103566, per_table_disk_size=610, per_column_disk_size=339)
    row(index_name=table_00_v1_idx, sstable_name=/tmp/cassandra/build/test/cassandra/data/cql_test_keyspace/table_00-12e640202d2a11eea926d15115276651/da-6-bti-Data.db, table_name=table_00, column_name=v1, format_version=aa, cell_count=3, min_row_id=2, max_row_id=4, start_token=-8982230457741691068, end_token=5293579765126103566, per_table_disk_size=610, per_column_disk_size=310)
Missing Rows:
    row(index_name=table_00_v1_idx, sstable_name=/tmp/cassandra/build/test/cassandra/data/cql_test_keyspace/table_00-12e640202d2a11eea926d15115276651/da-5-bti-Data.db, table_name=table_00, column_name=v1, format_version=aa, cell_count=4, min_row_id=2, max_row_id=5, start_token=8213365047359667313, end_token=8213365047359667313, per_table_disk_size=335, per_column_disk_size=270)
    row(index_name=table_00_v2_idx, sstable_name=/tmp/cassandra/build/test/cassandra/data/cql_test_keyspace/table_00-12e640202d2a11eea926d15115276651/da-5-bti-Data.db, table_name=table_00, column_name=v2, format_version=aa, cell_count=5, min_row_id=0, max_row_id=5, start_token=8213365047359667313, end_token=8213365047359667313, per_table_disk_size=335, per_column_disk_size=274)
	at org.apache.cassandra.cql3.CQLTester.assertRowsIgnoringOrderInternal(CQLTester.java:1900)
	at org.apache.cassandra.cql3.CQLTester.assertRowsIgnoringOrder(CQLTester.java:1834)
	at org.apache.cassandra.index.sai.virtual.SSTablesSystemViewTest.testVirtualTableThroughIndexLifeCycle(SSTablesSystemViewTest.java:133)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
{code}
The failure seems exclusive to {{test-trie}}."
CASSANDRA-18702,Add jmh microbenchmarks to eclipse IDE,Currently jmh tests are not being picked up by the Eclipse IDE.
CASSANDRA-18701,Switch Jenkins dev on trunk from J8+J11 to J11+j17,
CASSANDRA-18700,Remove AssertJ annotations from the production code,"We are using VisibleForTesting annotation from AssertJ in the production code. At the same time, we are not shipping assertj library in the tarball. 

The way it works is that we depend on assertj with ""provided"" scope and then we will not ship the lib. It is possible, because the default RetentionPolicy for that annotation is CLASS which means that while we compile with the annotation, it does not need to be there in runtime ...

There is VisibleForTesting annotation from Guava which is used extensively. Usage of VisibleForTesting from AssertJ was used by mistake as that probably popped up as the first import to add when doing code completion in IDEA."
CASSANDRA-18699,Python dtests upgrade tests mixed SHA runs,"Hi adding a bug I noticed in jenkins:
A- Commit SHA1
B- Jenkins starts a test run and eventually starts python dtests
C- Before B completes commit SHA2 comes in

Python dtests in B will run mixed SHA1 and SHA2 versions of trunk. An example [https://ci-cassandra.apache.org/job/Cassandra-trunk-dtest-upgrade/1213/jdk=jdk_11_latest,label=cassandra-dtest,split=11/consoleFull|http://example.com] where both 2a458ba01dae3b35b62ad2ef361e1acb567c97f0 and the later a565711056c859398d0b26081b46e71d2076de1d get mixed up."
CASSANDRA-18698,simulator/test is not included in IntelliJ workspace,
CASSANDRA-18697,Skip ColumnFamilyStore#topPartitions initialization when client or tool mode,"In {{org.apache.cassandra.db.ColumnFamilyStore}} the {{topPartitions}} is initialized when the keyspace is not a system keyspace. However, when running the cassandra library as client mode or tool mode, the initialization also happens. However, {{TopPartitionTracker}} performs queries to the {{system}} keyspace, which might not be available in most of the cases. For that reason, we should skip initialization of {{topPartitions}} when running on client mode or tool mode.

In utilities and external libraries, this can produce a warning that is displayed with a stack trace. This warning can be misleading for end users, and can cause confusion. But more importantly, the initialization of {{topPartitions}} is not required in this mode.

The warning is similar to this:
{code:java}
    WARN org.apache.cassandra.db.SystemKeyspace: Could not load stored top SIZES partitions for ...
    org.apache.cassandra.db.KeyspaceNotDefinedException: keyspace system does not exist
            at org.apache.cassandra.schema.Schema.validateTable(Schema.java:xxx) ~[?:?]
            at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:xxx) ~[?:?]
            at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:xxx) ~[?:?]
            at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:xxx) ~[?:?]
            at org.apache.cassandra.cql3.QueryProcessor.parseAndPrepare(QueryProcessor.java:xxx) ~[?:?]
            ... {code}"
CASSANDRA-18696,Update JMH,"[~marianne-manaog] was trying to do some profiling and noticed illegal access exceptions. Our version of JMH is from 2018; as per my recommendation, upgrading to the latest one from the end of 2022 solved her problems. The new version is tested with the JDK versions up to 17."
CASSANDRA-18695,Remove 3.x from the versions checked for prepared statement behaviour,"CASSANDRA-17248 introduced a different behaviour of prepared statements depending on the minimum cluster version. The old behaviour is used when the min cluster version is below 3.0.26, 3.11.12 or 4.0.2. 

However, from 5.0 we won't support direct upgrades from 3.0 or 3.11, so we can eliminate the checks for those versions."
CASSANDRA-18692,[Cassandra Analytics] Fix bulk writes with Buffered RowBufferMode ,"When setting {{Buffered}} {{RowBufferMode}} as part of the {{WriterOption's}} , {{org.apache.cassandra.spark.bulkwriter.RecordWriter}} ignores that configuration and uses the batch size to finalize an SSTable and start using a new one if new rows are available."
CASSANDRA-18691,generate.sh no longer works for repeated tests correctly,"Using the examples:

{noformat}
$ .circleci/generate.sh -e DTEST_BRANCH=CASSANDRA-8272 -e DTEST_REPO=https://github.com/adelapena/cassandra-dtest.git -e REPEATED_DTESTS=cqlsh_tests/test_cqlsh.py::TestCqlshSmoke -e REPEATED_DTESTS_COUNT=500

Detecting new or modified tests with git diff --diff-filter=AMR trunk...HEAD:

Setting environment variables:
  DTEST_BRANCH=CASSANDRA-8272: 
  DTEST_REPO=https://github.com/adelapena/cassandra-dtest.git: 
  REPEATED_DTESTS=cqlsh_tests/test_cqlsh.py::TestCqlshSmoke: 
  REPEATED_DTESTS_COUNT=500: 
{noformat}

but no changes are actually made."
CASSANDRA-18690,Fix the way we produce a DTest jar,"The problem is related to how we produce the {{dtest jar}}. That repackages all dependencies along with our classes in a single jar. Some dependencies might have digital signatures extracted into {{META-INF}} as, say, {{*.SF}} files. We need to remove them because they are invalid for the uber-jar we produce. Therefore, there are exclusions in {{zipgroupfileset}} operations, more or less like this:

{code:xml}
<zipgroupfileset dir=""${build.lib}"" includes=""*.jar"" excludes=""META-INF/*.SF""/>
{code}

However, this does not work as expected. The {{exclude}} attribute denotes jar files we don't want to repackage rather than filter on the content of the jar file. The new BouncyCastle jars include signatures in SF and DSA files, which all land in the final {{dtest.jar}} regardless of that filter.

See also https://stackoverflow.com/questions/35577351/exclude-jar-from-ant-build-using-zipgroupfileset/35593838#35593838

The issue was discovered and explained by [~jlewandowski] in CASSANDRA-17992"
CASSANDRA-18688,Limit Java runtime in 5.1 to JDK 11 and 17 in scripts; add a flag to opt out of that,"Currently, we limit our users from building with non-default Java versions in build.xml.

They can easily hack build.xml for test purposes with different versions.

Cassandra–5.1 will be run on JDK11 and JDK17, but on startup, we do not limit people to those two, but only to everything >= 11. We should also put an upper limit of 17 in our Cassandra startup scripts. We can also add a flag to opt-out if someone wants to test with newer versions."
CASSANDRA-18687,Add jvmargs for Netbeans and Eclipse for JDK11 and 17,"Add JVM args for Netbeans and Eclipse for JDK11 and 17, similar to what we do for IntellijIdea.

Otherwise, people who use NetBeans and Eclipse should be adding them manually. 

A good reference on how to do it would be generate-idea-files.

We would need to make changes to generate-eclipse-files for Eclipse.

I am not a NetBeans user but it seems we update directly the files, no ant target.

CC [~Bereng] and [~mck] as being one of the few users using those two IDEs

 "
CASSANDRA-18686,Test failure: test_move_backwards_and_cleanup,"{code:java}
ccmlib.node.ToolError: Subprocess ['nodetool', '-h', 'localhost', '-p', '7200', 'cleanup'] exited with non-zero status; exit status: 2; stderr: error: Node is involved in cluster membership changes. Not safe to run cleanup. -- StackTrace -- java.lang.RuntimeException: Node is involved in cluster membership changes. Not safe to run cleanup. at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:4004) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:262) at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112) at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46) at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237) at java.management/com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138) at java.management/com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:814) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:802) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1472) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1310) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1405) at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:360) at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200) at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197) at java.base/java.security.AccessController.doPrivileged(AccessController.java:712) at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196) at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:587) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:705) at java.base/java.security.AccessController.doPrivileged(AccessController.java:399) at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:704) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:833) self = <transient_replication_ring_test.TestTransientReplicationRing object at 0x7f7b4db079b0> @flaky(max_runs=1) @pytest.mark.no_vnodes def test_move_backwards_and_cleanup(self): """"""Test moving a node backwards without moving past a neighbor token"""""" move_token = '00005' expected_after_move = [gen_expected(range(0, 6), range(31, 40)), gen_expected(range(0, 21, 2)), gen_expected(range(1, 6, 2), range(6, 31)), gen_expected(range(7, 20, 2), range(21, 40))] expected_after_repair = [gen_expected(range(0, 6), range(31, 40)), gen_expected(range(0, 21)), gen_expected(range(6, 31)), gen_expected(range(21, 40))] > self.move_test(move_token, expected_after_move, expected_after_repair) transient_replication_ring_test.py:335: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ transient_replication_ring_test.py:267: in move_test cleanup_nodes(nodes) transient_replication_ring_test.py:43: in cleanup_nodes node.nodetool('cleanup') ../env3.6/lib/python3.6/site-packages/ccmlib/node.py:1006: in nodetool return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port)] + shlex.split(cmd)) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ process = <subprocess.Popen object at 0x7f7b3c571e80> cmd_args = ['nodetool', '-h', 'localhost', '-p', '7200', 'cleanup'] def handle_external_tool_process(process, cmd_args): out, err = process.communicate() if (out is not None) and isinstance(out, bytes): out = out.decode() if (err is not None) and isinstance(err, bytes): err = err.decode() rc = process.returncode if rc != 0: > raise ToolError(cmd_args, rc, out, err) E ccmlib.node.ToolError: Subprocess ['nodetool', '-h', 'localhost', '-p', '7200', 'cleanup'] exited with non-zero status; exit status: 2; E stderr: error: Node is involved in cluster membership changes. Not safe to run cleanup. E -- StackTrace -- E java.lang.RuntimeException: Node is involved in cluster membership changes. Not safe to run cleanup. E at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:4004) E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) E at java.base/java.lang.reflect.Method.invoke(Method.java:568) E at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72) E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) E at java.base/java.lang.reflect.Method.invoke(Method.java:568) E at java.base/sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:262) E at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112) E at java.management/com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46) E at java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237) E at java.management/com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138) E at java.management/com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252) E at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:814) E at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:802) E at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1472) E at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1310) E at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1405) E at java.management.rmi/javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829) E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) E at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) E at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) E at java.base/java.lang.reflect.Method.invoke(Method.java:568) E at java.rmi/sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:360) E at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:200) E at java.rmi/sun.rmi.transport.Transport$1.run(Transport.java:197) E at java.base/java.security.AccessController.doPrivileged(AccessController.java:712) E at java.rmi/sun.rmi.transport.Transport.serviceCall(Transport.java:196) E at java.rmi/sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:587) E at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828) E at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:705) E at java.base/java.security.AccessController.doPrivileged(AccessController.java:399) E at java.rmi/sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:704) E at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) E at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) E at java.base/java.lang.Thread.run(Thread.java:833) ../env3.6/lib/python3.6/site-packages/ccmlib/node.py:2306: ToolError{code}
Flaky on trunk
[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2390/workflows/719ea674-1f0f-4165-92ba-785469e1bb2f/jobs/27021]"
CASSANDRA-18685,Test failure: StandaloneSplitterWithCQLTesterTest/testSplittingMultipleSSTables_cdc_jdk17,"It seems like we can hit that path only sometimes and the test is flaky as I never saw it so far  failing in CircleCI.
Seen here:
https://ci-cassandra.apache.org/job/Cassandra-trunk-test-cdc/jdk=jdk_17_latest,label=cassandra,split=4/lastBuild/testReport/junit/org.apache.cassandra.tools/StandaloneSplitterWithCQLTesterTest/testSplittingMultipleSSTables_cdc_jdk17/


{code:java}
Stacktrace
junit.framework.AssertionFailedError: java.lang.reflect.InaccessibleObjectException: Unable to make field private final java.lang.Object java.nio.channels.spi.AbstractInterruptibleChannel.closeLock accessible: module java.base does not ""opens java.nio.channels.spi"" to unnamed module @57536d79
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:354)
	at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
	at java.base/java.lang.reflect.Field.checkCanSetAccessible(Field.java:178)
	at java.base/java.lang.reflect.Field.setAccessible(Field.java:172)
	at org.apache.cassandra.utils.concurrent.Ref.getFields(Ref.java:654)
	at org.apache.cassandra.utils.concurrent.Ref.getFields(Ref.java:657)
	at org.apache.cassandra.utils.concurrent.Ref.getFields(Ref.java:657)
	at org.apache.cassandra.utils.concurrent.Ref$Visitor.traverse(Ref.java:611)
	at org.apache.cassandra.utils.concurrent.Ref$Visitor.run(Ref.java:566)
	at org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:833)

	at org.apache.cassandra.tools.StandaloneSplitterWithCQLTesterTest.testSplittingMultipleSSTables(StandaloneSplitterWithCQLTesterTest.java:99)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
"
CASSANDRA-18684,[Analytics] Minor Refactoring to Improve Code Reusability,A very minor refactoring is needed in the `CassandraClusterInfo` class to enable reuse of the code block handling retrieval of all `NodeSettings`.
CASSANDRA-18683,Expose per partition on-disk usage through new DataFrame that utilizes the Index.db SSTable file components.,This addition to Cassandra Analytics provides a new PartitionSizeTableProvider that generates a DataFrame returning the compressed and uncompressed partition sizes for all partitions in a table by utilizing the Index.db SSTable file component.
CASSANDRA-18681,Internode legacy SSL storage port certificate is not hot reloaded on update,"In CASSANDRA-16666 the SSLContext cache was changed to clear individual {{EncryptionOptions}} from the SslContext cache if they needed reloading to reduce resource consumption. Before the change if ANY cert needed hot reloading, the SSLContext cache would be cleared for ALL certs.

If the legacy SSL storage port is configured, a new {{EncryptionOptions}} object is created in {{org.apache.cassandra.net.InboundSockets#addBindings}} just for binding the socket, but never gets cleared as the change in port means it no longer matches the configuration retrieved from {{DatabaseDescriptor}} in {{org.apache.cassandra.net.MessagingServiceMBeanImpl#reloadSslCertificates}}.

This is unlikely to be an issue in practice as the legacy SSL internode socket is only used in mixed version clusters with pre-4.0 nodes, so the cert only needs to stay valid until all nodes upgrade to 4.x or above.

One way to avoid this class of failures is to just check the entries present in the SSLContext cache."
CASSANDRA-18679,Attempted NTS creation logs even on failure,"If you try to create a keyspace with a nonexistent DC, you will still see a log like this:

{noformat}
INFO  [Native-Transport-Requests-1] 2023-07-21 11:02:40,062 NetworkTopologyStrategy.java:93 - Configured datacenter replicas are DC2:rf(3), DC1:rf(1)
{noformat}

even if you receive:
{noformat}
ConfigurationException: Unrecognized strategy option {DC2} passed to NetworkTopologyStrategy for keyspace test
{noformat}

which prevented the creation."
CASSANDRA-18678,CCM does not select Java version properly,"CASSANDRA-18106 added a new method to {{commons.py}} https://github.com/riptano/ccm/blob/master/ccmlib/common.py#L857 which is called when Cassandra version is >= 4.2


{code:python}
def get_supported_jdk_versions(install_dir):
    """"""
    Return the supported java versions from build.xml
    Only works in > 4.1
    """"""
     ...
                versions = ['8' if v == '1.8' else v for v in versions]
    ....
{code}

The problem is that this method returns a collection of strings, while later in {{commons.py}}:

{code:python}
...
        if current_java_version in versions:
            jvm_version = current_java_version
...
{code}

the condition expects the collection to consists of ints and the condition fails.

It is causing problems when you want to run the upgrade tests against Java 11 because it always goes to the next branch of the condition which results in selecting the first supported version, which is 8.
"
CASSANDRA-18677,BTI format's decision to preload partition index is incorrect,"On load, BTI format SSTables are supposed to walk through the partition index when the SSTable does not have a bloom filter, to prefetch the index in cache.

This decision [is currently inverted|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/format/bti/BtiTableReaderLoadingBuilder.java#L125C62-L125C62], causing both an unnecessary preload when a filter is present, and making filterless sstables inefficient. "
CASSANDRA-18676,CorruptedSSTablesCompactionsTest is flaky,"See e.g. [this run|https://app.circleci.com/pipelines/github/blambov/cassandra/510/workflows/fd484f76-b0f0-48c9-8672-d73bdc36b8ec/jobs/13575/tests].

The test was looked at for CASSANDRA-15879, but it is still failing from time to time. One of the failures appears to be introduced by CASSANDRA-14227 and the others by CASSANDRA-18134. The failures are genuine problems with handling corruption, not just test issues.

The {{CorruptSSTableException}} paths in {{SSTableIdentityIterator}} should likely also catch {{AssertionError}} and {{IllegalArgumentException}}, and most probably the tombstone verification should be done on the read path."
CASSANDRA-18673,Reduce size of per-SSTable index components,"The current per-SSTable index components are large because the primary keys that are stored in them include the token as part of the byte comparable. The byte comparable puts the token first meaning that we get very little prefix compression from either the trie or the sorted terms store. 

We can fix this by removing the token from the primary key serialization. This would allow us to get the prefix compression from the trie and the sorted terms store."
CASSANDRA-18672,Bump snakeyaml from 1.26 to 2.0,"snakeyaml 1.26 has CVEs. Bump version for snakeyaml from 1.26 to 2.0

To see the CVEs, goto [https://mvnrepository.com/artifact/org.apache.cassandra/cassandra-all/4.1.0] and seach for [org.yaml|https://mvnrepository.com/artifact/org.yaml] » [snakeyaml|https://mvnrepository.com/artifact/org.yaml/snakeyaml] under compile dependencies.Vulnerabilites are listed thusly:

Direct vulnerabilities:
[CVE-2022-41854|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41854]
[CVE-2022-38752|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-38752]
[CVE-2022-38751|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-38751]
[View 4 more ...|https://mvnrepository.com/artifact/org.yaml/snakeyaml/1.26#]
Vulnerabilities from dependencies:
[CVE-2022-22971|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22971]
[CVE-2022-22970|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22970]
[CVE-2022-22968|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22968]

GitHub Issue:

https://github.com/apache/cassandra/pull/2455"
CASSANDRA-18670,Importer should build SSTable indexes successfully before making new SSTables readable,"Importing SSTables w/ already built SAI components is pretty unproblematic. However, the case where we import SSTables without accompanying index components can be problematic in two ways, given the indexes are built after the SSTables themselves are added to the tracker:

1.) The new SSTables can be seen for reads on the base table before their attached indexes are built and ready to query in the index {{View}}.

2.) If a failure occurs in an index build, the import just leaves the index in a compromised state, and even though a failure to import will be reported to the user, the SSTables will be readable for queries on the base table.

If the index implementation allows it (and SAI does), {{SSTableImporter}} should make sure SSTable indexes are built before making SSTables visible as part of the base table. If an index build fails, the import should fail and it should appear to ongoing reads like it never happened, including the index continuing to be queryable."
CASSANDRA-18669,Concurrency is broken in the block balanced tree for queries,"Performance testing has highlighted that concurrent queries are broken against the block balanced tree because the mutable traversal state was pushed up to the walker that is linked to an SSTableIndex so is shared amongst all queries.

A new traversal state should be created for each query and marked as not threadsafe. We should add a test to show that concurrent queries work against the block balanced tree."
CASSANDRA-18665,Update jenkins groovy dsl to use in-tree scripts,
CASSANDRA-18664,Move jflex from runtime to build dependencies,"JFlex depends on java-cup-runtime which has a kind of exotic license saying that it is compatible with GPL. Therefore, we should investigate that case.

JFlex seems to be used only during the build to generate some sources for SASI indexes. Therefore, it should be possible to move JFlex from the runtime dependencies to the build dependencies and not include it in the distribution.

 "
CASSANDRA-18662,[Cassandra Analytics] Fix cassandra-analytics-core-example,The {{cassandra-analytics-core-example}} recently broke from a couple of commits and we need to make it usable again so that users can easily engage in the project.
CASSANDRA-18660,transient_replication_ring_test.py::TestTransientReplicationRing::test_bootstrap_and_cleanup,"{{transient_replication_ring_test.py::TestTransientReplicationRing::test_bootstrap_and_cleanup}} is flaky at least in CircleCI and {{trunk}}:

https://app.circleci.com/pipelines/github/adelapena/cassandra/3018/workflows/2c81b63f-63b8-4baf-8945-d3c680edede6/jobs/58482/tests

{code}
ccmlib.node.TimeoutError: 11 Jul 2023 13:59:08 [node3] after 120.11/120 seconds Missing: ['127.0.0.2:7000.* is now UP'] not found in system.log:
 Head: INFO  [Messaging-EventLoop-3-6] 2023-07-11 13:57:0
 Tail: ...7.0.0.3:7000(/127.0.0.1:56442)->/127.0.0.2:7000-LARGE_MESSAGES-da6108fe successfully connected, version = 12, framing = CRC, encryption = unencrypted
self = <transient_replication_ring_test.TestTransientReplicationRing object at 0x7f2f340c2c88>

    @flaky(max_runs=1)
    @pytest.mark.no_vnodes
    def test_bootstrap_and_cleanup(self):
        """"""Test bootstrapping a new node across a mix of repaired and unrepaired data""""""
        main_session = self.patient_cql_connection(self.node1)
        nodes = [self.node1, self.node2, self.node3]
    
        for i in range(0, 40, 2):
            self.insert_row(i, i, i, main_session)
    
        sessions = [self.exclusive_cql_connection(node) for node in [self.node1, self.node2, self.node3]]
    
        expected = [gen_expected(range(0, 11, 2), range(22, 40, 2)),
                    gen_expected(range(0, 22, 2), range(32, 40, 2)),
                    gen_expected(range(12, 31, 2))]
        self.check_expected(sessions, expected)
    
        # Make sure at least a little data is repaired, this shouldn't move data anywhere
        repair_nodes(nodes)
    
        self.check_expected(sessions, expected)
    
        # Ensure that there is at least some transient data around, because of this if it's missing after bootstrap
        # We know we failed to get it from the transient replica losing the range entirely
        nodes[1].stop(wait_other_notice=True)
    
        for i in range(1, 40, 2):
            self.insert_row(i, i, i, main_session)
    
>       nodes[1].start(wait_for_binary_proto=True)

transient_replication_ring_test.py:184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
transient_replication_ring_test.py:52: in new_start
    return old_start(*args, **kwargs)
../env3.6/lib/python3.6/site-packages/ccmlib/node.py:896: in start
    node.watch_log_for_alive(self, from_mark=mark)
../env3.6/lib/python3.6/site-packages/ccmlib/node.py:665: in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
../env3.6/lib/python3.6/site-packages/ccmlib/node.py:593: in watch_log_for
    head=reads[:50], tail=""...""+reads[len(reads)-150:]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 1689083828.8439646, timeout = 120
msg = ""Missing: ['127.0.0.2:7000.* is now UP'] not found in system.log:\n Head: INFO  [Messaging-EventLoop-3-6] 2023-07-11 1...127.0.0.2:7000-LARGE_MESSAGES-da6108fe successfully connected, version = 12, framing = CRC, encryption = unencrypted\n""
node = 'node3'

    @staticmethod
    def raise_if_passed(start, timeout, msg, node=None):
        if start + timeout < time.time():
>           raise TimeoutError.create(start, timeout, msg, node)
E           ccmlib.node.TimeoutError: 11 Jul 2023 13:59:08 [node3] after 120.11/120 seconds Missing: ['127.0.0.2:7000.* is now UP'] not found in system.log:
E            Head: INFO  [Messaging-EventLoop-3-6] 2023-07-11 13:57:0
E            Tail: ...7.0.0.3:7000(/127.0.0.1:56442)->/127.0.0.2:7000-LARGE_MESSAGES-da6108fe successfully connected, version = 12, framing = CRC, encryption = unencrypted

../env3.6/lib/python3.6/site-packages/ccmlib/node.py:56: TimeoutError
{code}

The CircleCI config to reproduce can be generated with:
{code}
.circleci/generate.sh -p \
  -e REPEATED_DTESTS_COUNT=500 \
  -e REPEATED_DTESTS=transient_replication_ring_test.py::TestTransientReplicationRing::test_bootstrap_and_cleanup
{code}
Flakiness seems ~1%"
CASSANDRA-18659,Upgrade Apache Commons CLI library to 1.5.0,Upgrade apache commons-cli from 1.1 (July 2007) to 1.5.0 (October 2021).
CASSANDRA-18658,Microbench tests fail on JDK11,"When I run 
{code:java}
ant microbench -Dbenchmark.name=ReadWriteTest{code}
 using jdk8 on ds-trunk, it finishes successfully, but when I run it on jdk11 on ds-trunk, I get this error. 
{code:java}
Caused by: java.lang.IllegalAccessException: access to public member failed: jdk.internal.ref.Cleaner.clean[Ljava.lang.Object;@5189c629/invokeVirtual, from org.apache.cassandra.io.util.FileUtils (unnamed module @6cbf4322){code}
We suspect that ant target does not use the jvmargs as other test targets do."
CASSANDRA-18653,Avoid unnecessary index builds and validations,"In the testing of CASSANDRA-18490 it was noted that the SSTableAddedNotification will trigger an index build that will always build the column index components even if they already exist and are valid. 

The same notification is also handled by the StorageAttachedIndexGroup that can validate the components. This can lead to the components being validated more than is necessary.

We should coordinate these notification handlers so there is no overlap of responsibility."
CASSANDRA-18652,Improve vector value validation errors,"Some of the error messages shown when trying to bind a vector column with a wrong value are a bit obscure. For example, if we try to write a value with the wrong number of dimensions:
{code}
createTable(""CREATE TABLE %s (pk int primary key, value vector<int, 2>)"");
execute(""INSERT INTO %s (pk, value) VALUES (0, ?)"", vector(1));
{code}
 The error is:
{code}
java.lang.IndexOutOfBoundsException: Attempted to read 4, but the size is 0
{code}
I understand that we cannot provide a lot of information about a non-understandable bind value, since we don't even know whether it's a vector or something entirely different. But we could try to improve it a bit and throw a {{MarshalException}} that will be translated to a {{InvalidRequestException}}, and provide generic messages similar to those used by collections:
{code}
InvalidRequestException: Not enough bytes to read a vector<int, 2>
{code}"
CASSANDRA-18650,Upgrade owasp to 8.3.1,"I believe I'm fighting with an issue this upgrade solves, but also I cannot think of any reason to not run the latest version."
CASSANDRA-18649,netty-all vulnerability: CVE-2023-34462,"This is failing owasp:

https://nvd.nist.gov/vuln/detail/CVE-2023-34462

{quote}
The `SniHandler` can allocate up to 16MB of heap for each channel during the TLS handshake. When the handler or the channel does not have an idle timeout, it can be used to make a TCP server using the `SniHandler` to allocate 16MB of heap.
{quote}"
CASSANDRA-18648,Improved DeletionTime serialization,"DeletionTime.markedForDeleteAt is a long useconds since Unix Epoch. But I noticed that with 7 bytes we can already encode ~2284 years. We can either shed the 8th byte, for reduced IO and disk, or can encode some sentinel values as flags there. [~blerer] suggested starting with DeletionTime.LIVE.

That would mean reading and writing 1 byte instead of 12 (8 mfda long + 4 ldts int). Yes we already avoid serializing DeletionTime (DT) in sstables at _row_ level entirely but not at _partition_ level and it is also serialized at index, metadata, etc. "
CASSANDRA-18647,CASTing a float to decimal adds wrong digits,"If I create a table with a *float* (32-bit) column, and cast it to the *decimal* type, the casting wrongly passes through the double (64-bit) type and picks up extra, wrong, digits. For example, if we have a column e of type ""float"", and run

INSERT INTO tbl (p, e) VALUES (1, 5.2)

SELECT CAST(e AS decimal) FROM tbl WHERE p=1

The result is the ""decimal"" value 5.199999809265137, with all those extra wrong digits. It would have been better to get back the decimal value 5.2, with only two significant digits.

It appears that this happens because Cassandra's implementation first converts the 32-bit float into a 64-bit double, and only then converts that - with all the silly extra digits it picked up in the first conversion - into a ""decimal"" value.

Contrast this with CAST(e AS text) which works correctly - it returns the string ""5.2"" - only the actual digits of the 32-bit floating point value are converted to the string, without inventing additional digits in the process."
CASSANDRA-18646,Add Azure snitch,Create Azure snitch to support Azure clouds.
CASSANDRA-18645,Upgrade guava on trunk,"Recently guava added JDK17 in CI and fixed some bugs down the road.

Upgrading before the major 5.0 release is something we should do. 

Also, the current version that Cassandra uses is from 2018. "
CASSANDRA-18643,jackson-core vulnerability: CVE-2022-45688,"This is failing owasp.

https://nvd.nist.gov/vuln/detail/CVE-2022-45688

{quote}
A stack overflow in the XML.toJSONObject component of hutool-json v5.8.10 allows attackers to cause a Denial of Service (DoS) via crafted JSON or XML data.
{quote}"
CASSANDRA-18640,Add vector similarity functions,"Add the CQL vector similarity functions described by [CEP-30|https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-30%3A+Approximate+Nearest+Neighbor%28ANN%29+Vector+Search+via+Storage-Attached+Indexes]:
* similarity_cosine
* similarity_euclidean
* similarity_dot_product

The three functions take two vectors of floats of the same dimensions and return a float indicating the similarity score for the vectors."
CASSANDRA-18639,Add duration to sstablemetadata utility,"The new additions will output metadata information for:
 * Duration

For Time Window Compaction (TWC), the min and max timestamps together with duration describe the bounds of the time window in the table.
{quote}{{Total partitions: 2430}}
{{Total rows: 8000}}

{{Total column set: 100000}}

{{...}}

{{Min Timestamp: 06/28/2023 15:15:04 (1688067443651650)}}
{{Max Timestamp: 06/28/2023 15:15:58 (1688067500268865)}}
{{Duration Days: 0 Hours: 0 Minutes: 0 Seconds: 53}}
{quote}
The online docs in sstablemetadata.adoc will need to be updated as well."
CASSANDRA-18638,ERROR: Invalid consistency level: NODE_LOCAL when using partition denylist in 4.1.2,"When I was testing the upgrade from Cassandra-3.11.15 to Cassandra-4.1.2 (single node), for the debugging purpose, I enabled partition_denylist_enabled and set the partition consistency level to {_}NODE_LOCAL{_}.

I encountered the following error message in the system log
{code:java}
ERROR [main] 2023-06-29 14:12:17,870 PartitionDenylist.java:156 - Failed to load partition denylist
java.lang.UnsupportedOperationException: Invalid consistency level: NODE_LOCAL
        at org.apache.cassandra.db.ConsistencyLevel.blockFor(ConsistencyLevel.java:168)
        at org.apache.cassandra.locator.ReplicaPlans.contactForRead(ReplicaPlans.java:558)
        at org.apache.cassandra.locator.ReplicaPlans.forRangeRead(ReplicaPlans.java:611)
        at org.apache.cassandra.service.reads.range.ReplicaPlanIterator.computeNext(ReplicaPlanIterator.java:76)
        at org.apache.cassandra.service.reads.range.ReplicaPlanIterator.computeNext(ReplicaPlanIterator.java:42)
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
        at java.util.Iterator.forEachRemaining(Iterator.java:115)
        at org.apache.cassandra.service.reads.range.RangeCommands.sufficientLiveNodesForSelectStar(RangeCommands.java:135)
        at org.apache.cassandra.schema.PartitionDenylist.checkDenylistNodeAvailability(PartitionDenylist.java:169)
        at org.apache.cassandra.schema.PartitionDenylist.initialLoad(PartitionDenylist.java:148)
        at org.apache.cassandra.service.StorageProxy.initialLoadPartitionDenylist(StorageProxy.java:2914)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1224)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1145)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:936)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:854)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:424)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:751)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:875)
INFO  [main] 2023-06-29 14:12:17,870 PartitionDenylist.java:163 - Exception while loading partition denylist cache. Scheduled retry in 5 seconds. {code}
h2. How to reproduce

I encountered it after the upgrade process. But I think it can also be reproduced without the upgrade process.

1. Set up Cassandra-4.1.2 single node, add the follow configurations to cassandra.yaml
{code:java}
denylist_consistency_level: NODE_LOCAL
partition_denylist_enabled: true{code}
2. Start up Cassandra, the error message will show in system.log file.
h2. Root Cause

After setting denylist_consistency_level to NODE LEVEL, when loading denylist, it invokes the _blockFor_ method
{code:java}
public int blockFor(AbstractReplicationStrategy replicationStrategy)
{
    switch (this)
    {
        case ONE:
        case LOCAL_ONE:
            return 1;
        case ANY:
            return 1;
        case TWO:
            return 2;
        case THREE:
            return 3;
        case QUORUM:
        case SERIAL:
            return quorumFor(replicationStrategy);
        case ALL:
            return replicationStrategy.getReplicationFactor().allReplicas;
        case LOCAL_QUORUM:
        case LOCAL_SERIAL:
            return localQuorumForOurDc(replicationStrategy);
        case EACH_QUORUM:
            if (replicationStrategy instanceof NetworkTopologyStrategy)
            {
                NetworkTopologyStrategy strategy = (NetworkTopologyStrategy) replicationStrategy;
                int n = 0;
                for (String dc : strategy.getDatacenters())
                    n += localQuorumFor(replicationStrategy, dc);
                return n;
            }
            else
            {
                return quorumFor(replicationStrategy);
            }
        default:
            throw new UnsupportedOperationException(""Invalid consistency level: "" + toString());
    }
} {code}
However, the _NODE_LOCAL_ enum is not included in those cases and thus results in the exception message.

Should this enum be included, or does Cassandra forbid users from using partition denylist with the _NODE_LOCAL_ consistency level (even for debugging purposes)?"
CASSANDRA-18635,Test failure: org.apache.cassandra.distributed.test.UpgradeSSTablesTest,"Seen here: https://app.circleci.com/pipelines/github/driftx/cassandra/1095/workflows/6114e2e3-8dcc-4bb0-b664-ae7d82c3349f/jobs/33405/tests

{noformat}
junit.framework.AssertionFailedError: expected:<0> but was:<2>
	at org.apache.cassandra.distributed.test.UpgradeSSTablesTest.upgradeSSTablesInterruptsOngoingCompaction(UpgradeSSTablesTest.java:86)
{noformat}"
CASSANDRA-18633,[Analytics] Add Caching of Node Settings to Improve Efficiency,"Currently, `CassandraClusterInfo` does not reuse results of a call to `allNodeSettingsBlocking` from `Sidecar`.
We should improve efficiency by caching these results in order to prevent multiple expensive cluster-wide calls."
CASSANDRA-18632,Save state about completed streams and display in nodetool netstats,"We monitor Bootstraps in progress by polling StreamManagerMBean.getCurrentStreams(). While this gives us good signal for how a bootstrap is progressing, we'd also like to check that all streams have completed successfully when a node has finished bootstrapping.

This patch adds a separate map in StreamManager to track completed streams, and reports those stream summaries in StreamManagerMBean.getCompletedStreams() and .getAllStreams()

It also adds an option to nodetool netstats to display completed streams alongside the current streams.

https://github.com/apache/cassandra/pull/2453"
CASSANDRA-18631,[Cassandra Analytics] Add Release Audit Tool (RAT) plugin,Add the Release Audit Tool plugin to Cassandra Analytics to automate the {{check}} process and make it part of the regular Analytics build.
CASSANDRA-18630,jackson-databind-2.13.2.2.jar vulnerability: CVE-2023-35116,"https://nvd.nist.gov/vuln/detail/CVE-2023-35116

{noformat}
 An issue was discovered jackson-databind thru 2.15.2 allows attackers to cause a denial of service or other unspecified impacts via crafted object that uses cyclic dependencies. NOTE: the vendor's perspective is that the product is not intended for use with untrusted input.
{noformat}"
CASSANDRA-18629,BLOG - Clarify wording on 3.x EOL post,"CASSANDRA-18531 generated some confusion so attempting to clean up the wording. 

https://cassandra.apache.org/_/blog/Apache-Cassandra-3.0.x-and-3.11.x-End-of-Life-Announcement.html"
CASSANDRA-18628,Handle special column name when upgrading from 2.2.19 to 3.0.29/3.11.15,"When I was migrating from 2.2.19 to 3.0.29/3.11.15, I noticed some of my columns could not be accessed because of the syntax problem. I had a column named ""Is"" in the 2.x. However, after the upgrade process, when I try to perform the same select command, it throws an exception since ""Is"" will be interpreted separately. 

I observed this after my cluster was fully upgraded. I cannot downgrade it back to 2.2.19. I cannot rename this column directly either.
h2. Steps to reproduce

Execute the following commands in Cassandra-2.2.19 (single node)
{code:java}
CREATE KEYSPACE  ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 2 };
CREATE TABLE IF NOT EXISTS ks.tb (c1 INT,c2 INT,c3 TEXT,rJr INT,c4 TEXT,c5 TEXT,ds INT, Is INT,
PRIMARY KEY (c2, c5, c3, rJr, Is, c1, ds, c4 ));

SELECT c2, ds, Is, c4 FROM ks.tb WHERE c2 = 1566089765 AND c5 = 'teststring1' AND c3 = 'teststring2' AND rJr = -1012111692; 

 c2 | ds | is | c4
----+----+----+----


(0 rows){code}
Then perform an upgrade to Cassandra-3.0.29/Cassandra-3.11.15
{code:java}
// Do the same select
SELECT c2, ds, Is, c4 FROM ks.tb WHERE c2 = 1566089765 AND c5 = 'teststring1' AND c3 = 'teststring2' AND rJr = -1012111692; 

SyntaxException: line 1:15 no viable alternative at input 'Is' (SELECT c2, ds, [Is]...){code}
The column still exists in the table, but I cannot access it.

I am wondering whether I can directly rename it in the new version or get some prompts telling me to rename is before the upgrade process. ""Is"" is also not marked as a reserved word in the [CQL Keywords doc|https://docs.datastax.com/en/cql-oss/3.3/cql/cql_reference/keywords_r.html]).

Any help is appreciated!"
CASSANDRA-18626,compaction_tombstone_warning_threshold and compaction_large_partition_warning_threshold cause deprecation warnings,"If you start trunk without any changes you will see at startup:

{noformat}
WARN  [main] 2023-06-26 15:59:36,613 YamlConfigurationLoader.java:426 - [compaction_tombstone_warning_threshold, compaction_large_partition_warning_threshold] parameters have been deprecated. They have new names and/or value format; For more information, please refer to NEWS.txt
{noformat}

We should give these the same treatment as CASSANDRA-18617, removing them from the default yaml to stop the warnings, but still accepting them with converters and @Replaces tags."
CASSANDRA-18624,Make Corretto Crypto Provider the Default,[Amazon Corretto Crypto Provider| https://github.com/corretto/amazon-corretto-crypto-provider] is an alternative provider of TLS and cryptographic functions that has significant performance benefits for Cassandra. It is Apache 2.0 licensed and has been deployed in several existing large fleets. 
CASSANDRA-18621,Update Cassandra logos on ASF project logos site,"It appears that the Cassandra logos on the ASF project logos website are outdated, as they include lowercase letters and no trademark: https://www.apache.org/logos/#cassandra

The logo currently being used on the website is this one: https://github.com/apache/cassandra-website/blob/trunk/site-content/source/modules/ROOT/images/logo-white.svg

To ensure proper usage by contributors and third parties, I propose the creation of the following logo set based on the current website logo:

Version 1 - update and create black and white versions
Version 2- remove
Version 3 - update and create black and white versions
Version 4 - no change

If these already exist somewhere, then please upload them to the ASF project logos page (requires a committer): https://www.apache.org/logos/about.html

Otherwise I will be asking our designer to create hi-res versions to be uploaded."
CASSANDRA-18618,Update tasks configuration to run checks locally when requested,"Currently CheckStyle and RAT are run with almost every single Ant target, which is annoying as when developing locally. The targets should be clear - ""test"" - runs the tests, ""jar"" - builds the project and creates jars, and then we should have a task ""check"" which runs all the static analysis, that is CheckStyle, RAT and Eclipse-Warnings (or whatever we decide to replace Eclipse-Warnings with).

Such goal should be include in ""artifacts"" and we should run it instead of ""eclipse-warnings"" on CircleCI. This way building, static analysis and testings are clearly separated.

"
CASSANDRA-18617,Disable the deprecated keyspace/table thresholds and convert them to Guardrails,"The non-guardrail thresholds 'keyspace_count_warn_threshold' and 'table_count_warn_threshold' configuration settings were first added with CASSANDRA-16309 in 4.0-beta4 and have subsequently been deprecated since 4.1-alpha in CASSANDRA-17195 when they were replaced/migrated to guardrails as part of CEP-3 (Guardrails).

These thresholds should now be removed from cassandra.yaml, while still allowed in existing yaml files.

The old thresholds will be disabled by removing their default values from Config.java, and any existing values for these thresholds will be converted to the new guardrails using the '@Replaces' tag on the corresponding guardrail values.

Since the old thresholds considered the number of system keyspace/tables in their values, the '@Replaces' conversion will subtract the current number of system tables from the old value and log a descriptive message.

See dev list discussion: https://lists.apache.org/thread/0zjg08hrd6xv7lhvo96frz456b2rvr8b
"
CASSANDRA-18616,Simulator Java 17 support,Add Java 17 support for the simulator
CASSANDRA-18615,CREATE INDEX Modifications for Initial Release of SAI,"After a lengthy discussion on the dev list, the community seems to have arrived at the following list of TODOs before we release SAI in 5.0:

1.) CREATE INDEX should be expanded to support {{USING … WITH OPTIONS…}}

Essentially, we should be able to do something like {{CREATE INDEX ON tbl(v) USING ’sai’ WITH OPTIONS = ...}} and {{CREATE INDEX ON tbl(v) USING ‘cassandra’}} as a more specific/complete way to emulate the current behavior of {{CREATE INDEX}}.

2.) Allow operators to configure, in the YAML, a.) whether an index implementation must be specified w/ USING and {{CREATE INDEX}} and b.) what the default implementation will be, if {{USING}} isn’t required.

3.) The defaults we ship w/ will avoid breaking existing {{CREATE INDEX}} usage. (i.e. A default is allowed, and that default will remain ‘cassandra’, or the legacy 2i)

With all this in place, users should be able create SAI indexes w/ the simplest possible syntax, no defaults will change, and operators will have the ability to change defaults to favor SAI whenever they like."
CASSANDRA-18613,Add support for vectors on UDFs,"CASSANDRA-18504 will add a new vector type, but [it won't be supported on UDFs|https://github.com/apache/cassandra/blob/5027e688da006e5d5bf9bfdf4719caddbf85986a/test/unit/org/apache/cassandra/cql3/validation/operations/CQLVectorTest.java#L248-L271]. The goal of this ticket is to add that support.

This will require adding a new {{o.a.c.cql3.functions.types.TypeCodec}} for vectors. Those codecs are mostly duplicates of the codecs on the Java driver. They are used for UDFs instead of the regular {{AbstractType}} to prevent pulling too many internal dependencies. The driver's vector codec has recently been added by [JAVA-3060|https://datastax-oss.atlassian.net/browse/JAVA-3060]."
CASSANDRA-18612,java-driver ip clearance,https://incubator.apache.org/ip-clearance/cassandra-java-driver.html
CASSANDRA-18609,snappy-java vulnerability: CVE-2023-34453,"Failing owasp:

[https://nvd.nist.gov/vuln/detail/CVE-2023-34453]

bq. Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing a fatal error. "
CASSANDRA-18608,"snappy-java vulnerability: CVE-2023-34455, CVE-2023-34454, CVE-2023-34453","Failing owasp:

[https://nvd.nist.gov/vuln/detail/CVE-2023-34455]
{quote}Due to use of an unchecked chunk length, an unrecoverable fatal error can occur in versions prior to 1.1.10.1.
{quote}

[https://nvd.nist.gov/vuln/detail/CVE-2023-34454]
{quote}Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing an unrecoverable fatal error. 
{quote}

[https://nvd.nist.gov/vuln/detail/CVE-2023-34453]
{quote}Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing a fatal error.
{quote}
"
CASSANDRA-18606,"Fix cross-refs in trunk - incorrect syntax, no components","The nav.adoc in modules/cassandra has xrefs with:
{code:java}
xref:developing:data-modeling/filename.adoc
{code}
when they should be:
{code:java}
xref:developing/data-modeling/filename.adoc
{code}

Issue arose due to the flattened antora component structure of the Apache C* docs (only ROOT and cassandra).

All xrefs need fixing."
CASSANDRA-18605,Cassandra Analytics - Adding support for TTL & Timestamps for bulk writes,In this patch we are adding support for Spark Bulk Writer writes with TTL & Timestamp. TTL/ Timestamp can be constant where all rows in the RDD are written with same TTL/Timestamp or per row based where each row has a corresponding TTL/Timestamp present in a separate column in the RDD.
CASSANDRA-18604,Write docs for CEP-30 Vector Search,Write docs for Vector search.
CASSANDRA-18603,Move checkstyle files into .build,"Checkstyle files are currently in the root folder whereas the rest of similar purposed files live under .build. Let's move them for consistency.

CC [~mck]"
CASSANDRA-18600,[Analytics] Add NOTICE.txt in Cassandra Analytics ,The Cassandra Analytics project is missing the {{NOTICE.txt}} file required for compliance with the ASF guidance
CASSANDRA-18599,Cassandra Analytics - Upgrade to use JUnit 5,"For future work, we’d like to use some features only available in JUnit 5 (TestTemplates being the immediate need).
Upgrade cassandra-analytics dependencies to JUnit 5 and fix any test issues."
CASSANDRA-18598,Use trie for literal index building,"The BlockBalancedTreeRamBuffer for numerics indexes uses a trie for building indexes in memory before flushing to disk. The RAMStringIndexer for literal indexes does not do this. It uses lucene classes to build the in memory index before flushing.

We should combine both of the in-memory index builders and have them both use the trie. 
"
CASSANDRA-18596,Assertion error when describing mv as table,"When describing materialized view as a table Cassandra gets an assertion error.

Steps to reproduce:
CREATE KEYSPACE test WITH replication = \{'class': 'NetworkTopologyStrategy', 'datacenter1': '3'} AND durable_writes = true;
CREATE TABLE test.table1 (key1 text,key2 int,value int,PRIMARY KEY (key1, key2));
CREATE MATERIALIZED VIEW test.table1_by_value AS SELECT key1, key2, value FROM test.table1 WHERE value IS NOT NULL AND key1 IS NOT NULL AND key2 IS NOT NULL PRIMARY KEY(value, key1, key2);
DESCRIBE MATERIALIZED VIEW test.table1;
DESCRIBE TABLE test.table1_by_value;
DESCRIBE TABLE test.non_existing;
 
From the above the ""DESCRIBE TABLE test.table1_by_value;"" throws an assertion error while ""DESCRIBE TABLE test.non_existing;"" returns a meaningful error msg."
CASSANDRA-18594,Standalone Jenkinsfile,"Spin off CASSANDRA-18133

Once all build and test scripts are in-tree, dockerised with their images in-tree too, the {{.jenkins/Jenkinsfile}} can be rewritten to be standalone. Today its inner stage jobs refer to already defined jobs by cassandra-builds' groovy dsl.

 

This addresses the epic's stated existing problems:
 - mixture of jenkins dsl groovy, declarative and scripting pipeline.
 - different pre-commit and post-commit jenkins pipelines are used.

In addition it addresses:
 - stage jobs don't always running on the same SHA as the pipeline's run,
 - a more readable Jenkinsfile"
CASSANDRA-18593,CircleCI: Add separate approval step for oa utests on separate workflows,"CASSANDRA-14227 added new {{j*_utests_oa}} jobs to run unit tests with the new, not-default ""oa"" sstable format. Those tests depend on the {{start_j*_unit_tests}} approval step on the separate workflows.

However, the separate workflow is meant to allow running jobs individually, and having a common approval step for both {{j*_unit_tests}} and {{j*_utests_oa}} prevents us from doing so. That's the case for example of the development of CASSANDRA-18504, where I want to run a single unit test job without caring about the specialization, but the common approval step forces me to run both jobs, duplicating the costs. An example run can be seen [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/2948/workflows/67e5bfcd-c8df-4558-a889-c8828e8dd310].

I think that {{j*_utests_oa}} should have its own separate approval step, the same way that {{{}j*_utest_cdc{}}}, {{{}j*_utest_fqltool{}}}, {{j*_utest_compression}} or {{j*_utest_system_keyspace_directory}} have their own approval step."
CASSANDRA-18592,CIDR filtering authorizer for Cassandra,"Introducing a new authorizer, to allow or disallow users accesses based on client's IP or CIDR group. Please see the confluence page for requirements and detailed design of this feature [https://cwiki.apache.org/confluence/display/CASSANDRA/CIDR+filtering+authorizer].

Will post PR link and dev discussion soon."
CASSANDRA-18590,Introduce IndexTermType to replace TypeUtils in SAI,"From a review comment by [~adelapena] on the [CASSANDRA-18067|https://issues.apache.org/jira/browse/CASSANDRA-18067] PR
{quote}Not directly related to this ticket, but it seems to me that the methods on TypeUtil do a lot of repeated work in the hot path just to determine the data type of the caller. These operations include finding the base type of reversed multiple times, multiple instanceof calls, etc. All these operations on each index are called with the same IndexContext#getValidator argument, needlessly repeating work for every column value.

I understand that most of the methods on TypeUtil are things that would normally be part of AbstractType, so each particular data type can provide its own implementation. But we don't want to couple the generic data types with SAI, so we have this class instead.

I think that we could add a kind of TermType class decorating AbstractType, and subclass it for every data type that gets special treatment (LONG, VARINT, DECIMAL and INET). Then, IndexContext would hold the adequate instance of TermType. This instance would be able to provide the same operations as the current TypeUtil, but without the type checks.{quote}
"
CASSANDRA-18588,Slow builds due to checkstyle,"Builds are terribly slow atm due to checkstyle. But there's an option to cache results and only run on the latest changed files to avoid running over the same files over and over again.

This brings build times from the current 1m30s to the old 5s."
CASSANDRA-18585,Alter Type does not validate changes like Create Type does,"Create Type attempts to block undesired field types, but this validation is not cared over to Alter Type Add Field; which allows you to add unexpected/desired types

{code}
Assertions.assertThatThrownBy(() -> createType(""CREATE TYPE %s (f counter)"")).hasRootCauseMessage(""A user type cannot contain counters"");
String type = createType(KEYSPACE, ""CREATE TYPE %s (a int)"");
schemaChange(String.format(""ALTER TYPE %s.%s ADD f counter"", KEYSPACE, type));
UserType udt = Keyspace.open(KEYSPACE).getMetadata().types.get(UTF8Type.instance.decompose(type)).get();
logger.warn(""UDT: {}"", udt);
{code}

{code}
UDT: org.apache.cassandra.db.marshal.UserType(cql_test_keyspace,747970655f3031,61:org.apache.cassandra.db.marshal.Int32Type,66:org.apache.cassandra.db.marshal.CounterColumnType)
{code}"
CASSANDRA-18584,CEP-29: NOT operator,"Implement new CQL operators:
- NOT CONTAINS,
- NOT CONTAINS KEY,
- NOT IN
- NOT LIKE

https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-29%3A+CQL+NOT+operator"
CASSANDRA-18583,Fix shutting down IsolatedJmx ,"{code:java}
public class MyTest extends TestBaseImpl
{
    @Test
    public void test() throws Throwable
    {
        try (Cluster cluster = Cluster.build(2).withConfig(c -> c.with(Feature.values())).start())
        {
            ClusterUtils.stopUnchecked(cluster.get(2));
            System.out.println(cluster.get(1).nodetoolResult(""status"").getStderr());
        }
    }
}  {code}
This will fail on this:
{code:java}
java.net.BindException: Address already in use (Bind failed)
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
	at java.net.ServerSocket.bind(ServerSocket.java:390)
	at java.net.ServerSocket.<init>(ServerSocket.java:252)
	at javax.net.DefaultServerSocketFactory.createServerSocket(ServerSocketFactory.java:231)
	at org.apache.cassandra.distributed.impl.CollectingRMIServerSocketFac
{code}

The problem is that IsolatedJMX clears whole map of TCPTransports in clearMapField as part of the stopJmx method. This results in JMX internals thinking the socket is not there so it tries to create it but it fails to do so because it is technically still bound.

The fix is consisting of cleaning just the bits belonging to that specific IsolatedJMX instance.
The investigation of the issue done by myself, the actual fix by [~drohrer]. Found while working on CASSANDRA-18572."
CASSANDRA-18582,BulkLoader withCipherSuites option is ignored,"The {{withCipherSuites}} option of {{BulkLoader}} is being ignored. It seems that since CASSANDRA-16362 the {{BulkLoader.buildSSLOptions}} method no longer applies the cipher suite options provided by {{clientEncryptionOptions}}.
"
CASSANDRA-18581,BLOG - Town Hall Replay: Cassandra Performance Tuning Like You’ve Been Doing it For Ten Years,"This ticket is to capture the work associated with publishing the June 2023 blog ""Town Hall Replay: Cassandra Performance Tuning Like You’ve Been Doing it For Ten Years""

This blog can be published as soon as possible, but if it cannot be published within a week of the noted publish date *(July 27)*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18580,Baseline Metrics for Accord Transactions,"Based on some conversations w/ [~benedict] and [~dcapwell], this is the initial set of metrics that seem both feasible to implement and useful as we monitor the health of a cluster performing Accord transactions:

1.) Basic latency metrics for transactions up to the point of COMMIT and rate metrics for preemption, failure, and timeouts at the coordinator.

This has already been implemented and split into read and write-specific metrics. Our position for now is that metrics around preemption should be useful in place of a more difficult-to-define metric around how many transactions are completed via recovery.

2.) Global cache stats/metrics (i.e. aggregated for all command stores)

We could, at some point, build metrics scoped to a specific {{CommandStore}}, but they might be awkward in MBean/JMX space, as command stores would have to be identified by ID or key range…the latter possibly being able to change across epochs. (An alternative would be just publishing command store-specific stats on-demand to a virtual table instead.)

3.) Something like a decaying histogram of the number of dependencies per transaction (or per partial transaction).

If this is getting worse over time, it could be useful to know/be a way for us to detect that contention is increasing. We should be able to hook this up to {{ProgressLog}} notifications. Recording for PartialDeps/PartialTxn (which ProgressLog gives us at pre-accept) seems acceptable, given this is a directional metric."
CASSANDRA-18579,No link to source packages,"I could not find a link to download source releases.
They should be provided alongside the binary releases."
CASSANDRA-18578,[Analytics] Circle CI Configuration for Cassandra Analytics,"We need to add the CircleCI configuration to build and test the Cassandra Analytics library. We need to test against all 4 profiles:

* cassandra-analytics-core-spark2-2.11-jdk8
* cassandra-analytics-core-spark2-2.12-jdk8
* cassandra-analytics-core-spark3-2.12-jdk11
* cassandra-analytics-core-spark3-2.13-jdk11
"
CASSANDRA-18577,Cassandra Catalyst Program,"The 5.0 has a lot of exciting new features the world should know about. We would like to revive the Cassandra MVP program as a community-run program to let people know about Cassandra 5.0 and how to use/contribute to Cassandra in general.
"
CASSANDRA-18576,Mixed single-column and multi-column clustering restrictions are not merged correctly,"During working on CEP-29, I found that single-column and multi-column clustering restrictions are not merged correctly and cause query failures or incorrect results.

The algorithm used in {{RestrictionSet.mergeRestrictions}} is incorrect, as its results depend on the (undefined) order of restrictions in a hashset.
In more complex scenarios, when there are more restrictions to merge, the algorithm results can even differ between the test runs.
"
CASSANDRA-18575,Backport Cassandra-10508 Remove hard-coded SSL cipher suites and protocols,Cassandra 3.0 has ciphers hard coded and thus not allow more recent and secure ciphers for storage connections complicating migrations to later versions.
CASSANDRA-18574,[Analytics] Fix example documentation ,"The documentation for the example job for Cassandra Analytics [link|https://github.com/apache/cassandra-analytics/blob/trunk/cassandra-analytics-core-example/README.md] configures Sidecar to use the local CCM cluster. This documentation is however broken since some feedback during the review of the Sidecar PR changed one of the yaml options from {{uploads_staging_dir}} to {{staging_dir}}.

We need to update the documentation to reflect that change."
CASSANDRA-18573,Minimise state kept in accord system tables by reusing state from message log,"Stop storing deps, partial txn, and writes in accord system table - fetch these registers from the message log instead."
CASSANDRA-18570,Fix org.apache.cassandra.transport.DriverBurnTest.measureLargeV4WithCompression-.jdk17 ,"h1.  
{code:java}
Regression
org.apache.cassandra.transport.DriverBurnTest.measureLargeV4WithCompression-.jdk17 (from org.apache.cassandra.transport.DriverBurnTest-.jdk17)

Failing for the past 1 build (Since #1590 ) Took 30 sec.      Failed 5 times in the last 30 runs. Flakiness: 24%, Stability: 83% Stacktrace
junit.framework.AssertionFailedError at org.apache.cassandra.transport.DriverBurnTest.perfTest(DriverBurnTest.java:425) at org.apache.cassandra.transport.DriverBurnTest.measureLargeV4WithCompression(DriverBurnTest.java:316) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 
{code}
The test is flaky since recently, failing every other time in Jenkins (burn tests are not running in CircleCI) First seen with run #1572 this commit - 

CASSANDRA-18025

CC [~stefan.miklosovic] and [~brandon.williams] 

 "
CASSANDRA-18569,Bti shouldn't be available in compatibility mode,When having a node in compatibility mode sstable tries shouldn't be an option.
CASSANDRA-18566,Avoid unnecessary deserialization of terminal arguments when executing CQL functions,"CQL functions unnecessarily deserialize their terminal arguments on every call.

For example, the function call in {{SELECT mask_inner(column, 1, 0) FROM t}} deserializes {{1}} and {{2}} for each returned row. As another example, the selector in {{SELECT column * 2 FROM t}} also deserializes {{2}} for each returned row.

The goal of this ticket is to improve the function execution code to avoid those deserializations."
CASSANDRA-18564,Test Failure: MixedModeAvailabilityV30AllOneTest.testAvailabilityCoordinatorUpgraded,"The JVM upgrade dtest {{MixedModeAvailabilityV3XAllOneTest.testAvailabilityCoordinatorUpgraded}} seems to be flaky at least in {{trunk}}:
{code}
junit.framework.AssertionFailedError: Error in test '4.0.11 -> [5.0]' while upgrading to '5.0'; successful upgrades []
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:348)
	at org.apache.cassandra.distributed.upgrade.MixedModeAvailabilityTestBase.testAvailability(MixedModeAvailabilityTestBase.java:154)
	at org.apache.cassandra.distributed.upgrade.MixedModeAvailabilityTestBase.testAvailabilityCoordinatorUpgraded(MixedModeAvailabilityTestBase.java:74)
Caused by: java.lang.AssertionError: Unexpected error while reading in case write-read consistency ALL-ONE with upgraded coordinator and 2 nodes down: org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses.
	at org.apache.cassandra.distributed.upgrade.MixedModeAvailabilityTestBase.lambda$testAvailability$6(MixedModeAvailabilityTestBase.java:145)
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:339)
Caused by: org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses.
	at org.apache.cassandra.service.reads.ReadCallback.awaitResults(ReadCallback.java:162)
	at org.apache.cassandra.service.reads.AbstractReadExecutor.awaitResponses(AbstractReadExecutor.java:387)
	at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:2124)
	at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1995)
	at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1873)
	at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1286)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:364)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:293)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:105)
	at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:122)
	at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:103)
	at org.apache.cassandra.distributed.impl.Coordinator.lambda$executeWithResult$0(Coordinator.java:66)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:750)
{code}
This has failed 143 times in 500 iterations of this CircleCI run:
* https://app.circleci.com/pipelines/github/adelapena/cassandra/2927/workflows/fcd1cd60-826b-484a-8e81-d3ba640f7de9/jobs/47659/tests

The failure has also recently appeared on Jenkins too:
* https://ci-cassandra.apache.org/job/Cassandra-trunk/1585/testReport/org.apache.cassandra.distributed.upgrade/MixedModeAvailabilityV3XAllOneTest/testAvailabilityCoordinatorUpgraded__jdk11/

Given that the failure has just appeared on Jenkins and it fails relatively easily on CircleCI, it's likely that it has been broken by a very recent change.
"
CASSANDRA-18563,Convert AccordStateCache cache from write-through to write-back,"Pre-requisite work for soon to go up PRs that continue shifting the bulk of persistence from system tables to {{AccordJournal}}. The switch to write-back caching should allow to bypass writes to the system tables entirely for some of the transactions.

Additionally fixes some bugs in the cache, e.g. {{AsyncLoader}} failing to load an object could cause entries to be forever stuck in {{PENDING}} state because it would never reach the code path that submits the load runnables to the executor (that is now the job of the cache). Also switched the list implementation from a hand-rolled ad-hoc one to the pre-existing {{IntrusiveLinkedList}}, plus various simplifications and cleanup."
CASSANDRA-18562,guava vulnerability CVE-2023-2976,This is failing the OWASP check.
CASSANDRA-18561,Extend Accord MessageType with a side effect flag,Tiny change to make it easier for implementations to decide if a protocol message should be persisted to the log.
CASSANDRA-18560,Incorrect IP used for gossip across DCs with prefer_local=true,"After installing a new node using 4.0.10 we experienced a situation where the new node attempted to connect to the private ip of a random number of nodes remote DCs which are only accessible via public ip for cross dc communications.

The only impact was new nodes outbound connections, inbound from pre-4.0.10 were not affected.  system.peers_v2 (below) showed that the preferred_ip and preferred_port as null, only those in 4.0.10 nodes dc have perferred_ip values as expected.

We believe the issue originated with https://issues.apache.org/jira/browse/CASSANDRA-16718 

Details on cluster:
 * All nodes have public IP configured as well as private IP
 * Listen/rpc addressrs are configured for private ip, broadcast is public IP
 * prefer_local=true is enabled for all nodes

The log that showed the connection failing:
{code:java}
INFO  [Messaging-EventLoop-3-8] 2023-06-01 00:14:21,565 NoSpamLogger.java:92 - /99.81.<redacted>:7000->/44.208.<redacted>:7000-URGENT_MESSAGES-[no-channel] failed to connectio.netty.channel.ConnectTimeoutException: connection timed out: /10.26.5.11:7000  at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$2.run(AbstractEpollChannel.java:576){code}
99 and 44 instances can only access each other using public ips.

gossipinfo output from 4.0.10 node
{code:java}
/44.208.<redacted>
  generation:1661113358
  heartbeat:25267691
  LOAD:25267683:1.7882044268E10
  SCHEMA:24692061:e98b918d-499f-3ccc-8dbe-5af31f685bda
  DC:13:us-east-1
  RACK:15:1a
  RELEASE_VERSION:6:4.0.5
  NET_VERSION:2:12
  HOST_ID:3:9a41e668-060d-4cfe-bb1e-013f5116422d
  RPC_READY:1407:true
  INTERNAL_ADDRESS_AND_PORT:9:10.26.5.11:7000
  NATIVE_ADDRESS_AND_PORT:4:44.208.<redacted>:9042
  STATUS_WITH_PORT:1393:NORMAL,-2262036356854762881
  SSTABLE_VERSIONS:7:big-nb
  TOKENS:1392:<hidden> {code}
Peers output from 4.0.10 node:
{code:java}
   peer           | peer_port | data_center         | host_id                              | native_address | native_port | preferred_ip | preferred_port | rack | release_version | schema_version                       | tokens----------------+-----------+---------------------+--------------------------------------+----------------+-------------+--------------+----------------+------+-----------------+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  44.208.<redacted> |      7000 |      us-east-1 | 9a41e668-060d-4cfe-bb1e-013f5116422d |  44.208.<redacted> |        9042 |         null |           null |   1a |           4.0.5 | e98b918d-499f-3ccc-8dbe-5af31f685bda |    {'-2262036356854762881', '-4197710115038136897', '-7072386316096662315', '2085255826742630980', '249732489387853170', '4976300208126705818', '7187184456885833289', '8777189009399731927'} {code}
To solve temporarily we routed outbound traffic to the private ip to public using iptables which resulted in successful outbound connections."
CASSANDRA-18558,remove dh_python use from debian packaging,"It looks like dh_python2 has been removed from debian, but it also looks like we don't need it:

{noformat}
E: dh_python2 dh_python2:408: no package to act on (python-foo or one with ${python:Depends} in Depends)
{noformat}"
CASSANDRA-18557,CEP-30 ANN Vector Search with SAI,"[CEP-30|https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-30%3A+Approximate+Nearest+Neighbor%28ANN%29+Vector+Search+via+Storage-Attached+Indexes]
 # Implement approximate nearest neighbor (ANN) vector search capability in Apache Cassandra using storage-attached indexes (SAI).
 # Support a vector of float32 embeddings as a new CQL type.
 # Add ANN search to work with normal Cassandra data flow (insertion, updating, and deleting rows). The implementation should support adding a new vector in log(N) time, and ANN queries in M log(N) time where N is the number of vectors and M is the number of sstables.
 # Compose with other SAI predicates.
 # Scatter/gather across replicas, combining topK from each to get global topK."
CASSANDRA-18556,Leak detected issue in cassandra 3.11.1,"We are running 36 nodes Cassandra cluster. All nodes have the some config, Cassandra version is 3.11.1. Suddenly, we are seeing fluctuation in storage available graphs for the 2 nodes. Attaching the last 7 days used storage graph for the Cassandra nodes. 

Further investigation, we saw errors in the logs of these 2 Cassandra node logs. 

{code:java}
ERROR [CompactionExecutor:164360] 2023-05-04 00:30:58,094 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:164360,1,main]
java.lang.IllegalArgumentException: null
    at java.nio.Buffer.position(Buffer.java:244) ~[na:1.8.0_131]
    at org.apache.cassandra.io.util.SafeMemoryWriter.reallocate(SafeMemoryWriter.java:59) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.util.SafeMemoryWriter.setCapacity(SafeMemoryWriter.java:68) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.IndexSummaryBuilder.prepareToCommit(IndexSummaryBuilder.java:250) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.doPrepare(BigTableWriter.java:524) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy.doPrepare(BigTableWriter.java:364) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.format.SSTableWriter.prepareToCommit(SSTableWriter.java:281) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:379) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:111) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:121) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.1.jar:3.11.1]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
WARN  [CompactionExecutor:164909] 2023-05-04 00:31:54,573 IndexSummaryBuilder.java:115 - min_index_interval of 128 is too low for 4298508892 expected keys of avg size 64; using interval of 145 instead
ERROR [Reference-Reaper:1] 2023-05-04 00:32:02,326 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4f374614) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1106289026:Memory@[7fa6ab9ec010..7fa72ad798d0) was not released before the reference was garbage collected {code}
Compactions also running continually run on these nodes.

I am unable to understand the root cause of it. Any help is appreciated."
CASSANDRA-18555,Expose decommission state to nodetool info,"Currently, when a node is being decommissioned and if any failure happens, then an exception is thrown back to the caller.

But Cassandra's decommission takes considerable time ranging from minutes to hours to days. There are various scenarios in that the caller may need to probe the status again:
 * The caller times out
 * It is not possible to keep the caller hanging for such a long time

And If the caller does not know what happened internally, then it cannot retry, etc., leading to other issues.

So, in this ticket, I am going to add a new nodetool/JMX command that can be invoked by the caller anytime, and it will return the correct status.

It might look like a smaller change, but when we need to operate Cassandra at scale in a large-scale fleet, then this becomes a bottleneck and require constant operator intervention."
CASSANDRA-18554,mTLS based client and internode authenticators,"Cassandra currently doesn't have any certificate based authenticator for both client connections and internode connections. If one wants to use certificate based authentication protocol like TLS, in which clients send their certificates for the TLS handshake, we can leverage the information from the client certificate to identify a client. Using this authentication mechanism one can avoid the pain of password generations, sharing and rotation.

Introducing following certificate based mTLS authenticators for internode and client connections
MutualTlsAuthenticator (client authentication)
MutualTlsInternodeAuthenticator (internode authentication)
MutualTlsWithPasswordFallbackAuthenticator (for optional mode operation for client authentication)

An implementation of MutualTlsCertificateValidator called SpiffeCertificateValidator whose identity is SPIFFE that is embedded in SAN of the client certificate. One can implement their own CertificateValidator to match their needs and configure it in Cassandra.yaml "
CASSANDRA-18553,Generate.sh -s param to skip autodetection of tests,When using generate.sh auto detection of modified tests always kicks in. That can be a problem during dev when you want to test a given set of tests without getting all the others in the way. Also when you want to run the script without having to checkout the extra branches auto detection needs.
CASSANDRA-18552,Debian packaging source should exclude git subdirectory,This balloons the source up to 400+MB instead of the ~13MB necessary.
CASSANDRA-18550,"Improve nodetool enable{audit,fullquery}log, CVE-2023-30601","The {{--archive-command}} parameter to {code}nodetool enable{audit,fullquery}log{code} allows an attacker to execute arbitrary commands as the user running cassandra.

Patch adds a configuration option which disallows using this parameter - for any existing users of --archive-command this can be re-enabled"
CASSANDRA-18549,Circle repeated runs don't skip checkstyle,The repeated runs jobs we have in CircleCi don't skip checkstyle hence taking much longer than needed
CASSANDRA-18548,[Analytics] Add .asf.yaml file to the Cassandra Analytics repository,We need to add the {{.asf.yaml}} file to be able to control notifications and the GitHub settings for the Cassandra Analytics project.
CASSANDRA-18547,Refactor cqlsh On/Off switch implementation and make the output consistent,"This change refactors the On/Off switch implemented in the class SwitchCommand and subclass  SwitchCommandWithValue of cqlshmain.py to use an Enum with static methods instead of custom classes.

The body of on_off_switch + enum definition requires just 15 lines of code vs 33 in SwitchCommand.

The existing code is hard to read, including the usage in the code, which instantiates a SwitchCommand object in-order to invoke the execute method:

 
{code:java}
self.tracing_enabled = SwitchCommand(""TRACING"", ""Tracing"").execute(self.tracing_enabled, parsed, self.printerr){code}
this can be replaced by a more familiar direct function call:
{code:java}
self.tracing_enabled = self.on_off_toggle(""TRACING"", ""Tracing"", self.tracing_enabled, parsed.get_binding('switch')){code}
 

The refactoring also updates the command output for consistency. Instead of the current:
{code:java}
> tracing on
Now Tracing is enabled
> paging on
Query paging is already enabled. Use PAGING OFF to disable.
> expand on
Now Expanded output is enabled
{code}
replace with more succinct and consistent, using 'ON/OFF' instead of enabled/disabled and removing the redundant 'Now':
{code:java}
> tracing on
TRACING set to ON
> paging on
PAGING is already ON
> expand on
EXPAND set to ON
{code}
 "
CASSANDRA-18546,Remove unnecessary shuffling of GossipDigests in Gossiper#makeRandomGossipDigest,"In going through trying to understand the Gossiper code I come across:
{code:java}
    /**
     * The gossip digest is built based on randomization
     * rather than just looping through the collection of live endpoints.
     *
     * @param gDigests list of Gossip Digests.
     */
    private void makeRandomGossipDigest(List<GossipDigest> gDigests) {code}
But I couldn't see what purpose randomization had. In fact in 3.11 it will call:
{code:java}
 doSort(gDigestList); {code}
On the receiving end, negating the purpose of the randomization.

 

In discussion with [~stefan.miklosovic] he found this ticket CASSANDRA-14174

So it seems to me this randomization may have been to allow for limited sizes of SYN messages. But this feature doesn't exist and as such by randomizing it is:
 * creating more garbage
 * using more CPU (sure its mostly trival; see next point)
 * more time spent on unnecessary functionality on the *single threaded* gossip stage.
 * complicating the code and making it more difficult to understand

In fact there is a bug in the implementation:
{code:java}
        int generation = 0;
        int maxVersion = 0;        // local epstate will be part of endpointStateMap
        List<InetAddress> endpoints = new ArrayList<InetAddress>(endpointStateMap.keySet());
        Collections.shuffle(endpoints, random);
        for (InetAddress endpoint : endpoints)
        {
            epState = endpointStateMap.get(endpoint);
            if (epState != null)
            {
                generation = epState.getHeartBeatState().getGeneration();
                maxVersion = getMaxEndpointStateVersion(epState);
            }
            gDigests.add(new GossipDigest(endpoint, generation, maxVersion));
        } {code}
If epState is null and we already had a non-null epState, then the next digest will use the generation and maxVersion of the previous iterated epState.

 

Here is change to remove this randomization and fix the above bug, [https://github.com/apache/cassandra/pull/2357/commits/1ba422ab5de35f7057c7621ec3607dcbca19768c]"
CASSANDRA-18545,[Analytics] Abstract mTLS provisioning via a SecretsProvider,This enhancement allows us to abstract the mTLS secrets provisioning through a {{SecretsProvider}} interface. This will allow custom implementations of the {{SecretsProvider}} to be able to hook into the secrets provisioning. We need to provide a default implementation {{SslConfigSecretsProvider}} which provides secrets via the {{SslConfig}} which is parsed from the reader options.
CASSANDRA-18544,Make cassandra-stress able to read all credentials from a file,"If there is username/password required in order to connect via JMX, there is currently no way how to do this as NodeProbe constructor in cassandra-stress does not accept any credentials. 

The way how we are reading the credentials should be refactored. Currently, if there are credentials on the command line, they will be visible in the logs. Making it visible on the command line for JMX credentials is not ideal either.

What I would like to see is to read all credentials which are necessary for cassandra-stress from ONE file. CQL and JMX combined. 

Because there is already some logic in place and it would be cool to have this backward compatible, we may still support command line credentials for CQL but we would make it deprecated and we would remove it in 6.0 so cassandra-stress will be reading credentials from the file only.

I looked into the implementation and I have an idea how to ""inject"" credentials where necessary so they would be used even we do not use them on the command line but I have not coded up anything yet."
CASSANDRA-18543,Waiting for gossip to settle does not wait for live endpoints,"When a node starts it will get endpoint states (via shadow round) but have all nodes marked as down. The problem is the wait to settle only checks the size of endpoint states is stable before starting Native transport. Once native transport starts it will receive queries and fail consistency levels such as LOCAL_QUORUM since it still thinks nodes are down.

This is problem for a number of large clusters for our customers. The cluster has quorum but due to this issue a node restart is causing a bunch of query errors.

My initial solution to this was to only check live endpoints size in addition to size of endpoint states. This worked but I noticed in testing this fix that there also a lot of duplication of checking the same node (via Echo messages) for liveness. So the patch also removes this duplication of checking node is UP in markAlive.

The final problem I found while testing is sometimes could still not see a change in live endpoints due to only 1 second polling, so the patch allows for overridding the settle parameters. I could not reliability reproduce this but think its worth providing a way to override these hardcoded values."
CASSANDRA-18541,AUTH requests use too much resources,"Hello. I see unexpected CPU usage in a rare situation that may be worth digging into.
We have C* 4.0.9 on Debian running on Java 11.0.18.
It's a small cluster of 3 nodes on commodity hardware (6 cores CPU, 32 Gb RAM, 2 x 512 Gb SSD NVME).
This ring has about 35 clients using Datastax Java Driver for Apache Cassandra.
In the driver connection settings, we use the following:
CONNECTION_POOL_LOCAL_SIZE = 400
CONNECTION_POOL_REMOTE_SIZE = 100
 
And for some reason, from time to time, it causes hundreds of AUTH requests per second that leads to an enormous CPU usage.
And yes, it's easy not to use these settings in the driver, leaving defaults that don't produce such an amount of AUTHs. But isn't it weird that ~150 AUTH rps consume ~1200% CPU?
Please see attached graphs.

I have the following in the settings:

authenticator: PasswordAuthenticator

authorizer: CassandraAuthorizer

roles_validity_in_ms: 600000

permissions_validity_in_ms: 600000

credentials_validity_in_ms: 600000

Please let me know if I can provide any other necessary information.
Thanks for your work. Cassandra is amazing :)"
CASSANDRA-18540,"negotiatedProtocolMustBeAcceptedProtocolTest tests fail with ""TLSv1.1 failed to negotiate"" on JDK17","Note: This depends on having a fix for CASSANDRA-18180, otherwise most/all tests in {{NativeTransportEncryptionOptionsTest}} and {{InternodeEncryptionOptionsTest}} are failing due to that issue.

Using the patch for CASSANDRA-18180, the {{negotiatedProtocolMustBeAcceptedProtocolTest}} test in both {{NativeTransportEncryptionOptionsTest}} and {{InternodeEncryptionOptionsTest}} fails with ""TLSv1.1 failed to negotiate"" on JDK17.

From what I can see, the {{negotiatedProtocolMustBeAcceptedProtocolTest}} is failing because in JDK11 and JDK17 the ""TLSv1.1"" protocol is disabled.

Since TLSv1.1 is disabled in JDK11 and 17, one possibility is to change the test to use TLSv1.2 instead of TLSv1.1. That should work directly with JDK11 and 17, since TLSv1.2 is one of the defaults, and it won't be an issue for JDK8 as that will be dropped.

Also, I think the point of the {{negotiatedProtocolMustBeAcceptedProtocolTest}} is to test that the {{accepted_protocols}} option is working correctly rather than the choice of _which_ protocol is used. Meaning, I don’t think the intent was to test TLSv1.1 specifically, rather that the mechanism of accepted protocols works and choosing TLSv1.1 was at the time convenient - but I could be wrong.

It also seems to me like bit of a coincidence that these tests are currently working on JDK11, at least on CI. Indeed, running locally with JDK11, these fail for me:

{noformat}
$ pwd
/Users/dan.jatnieks/apache/cassandra-4.0

$ java -version
openjdk version ""11.0.11"" 2021-04-20
OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode)

$ ant test-jvm-dtest-some -Dtest.name=org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest -Duse.jdk11=true

...

[junit-timeout] Testcase: negotiatedProtocolMustBeAcceptedProtocolTest(org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest):	FAILED
[junit-timeout] Should be possible to establish a TLSv1.1 connection expected:<NEGOTIATED> but was:<FAILED_TO_NEGOTIATE>
[junit-timeout] junit.framework.AssertionFailedError: Should be possible to establish a TLSv1.1 connection expected:<NEGOTIATED> but was:<FAILED_TO_NEGOTIATE>
[junit-timeout] 	at org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest.negotiatedProtocolMustBeAcceptedProtocolTest(NativeTransportEncryptionOptionsTest.java:160)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}

I believe these work on CI because of CASSANDRA-16848 - in that ticket, after 2021-Apr JDK8 dropped TLSv1.1 which led to a fix in [cassandra-build|https://github.com/apache/cassandra-builds/commit/d1a3a0c59b3c5c17697d6a6656cd5d4f3a1cdbe9] docker code to make sure TLSv1.1 is accepted. 

I say coincidence because this change also makes it work for JDK11 and JDK17, and I've been able to verify that making a change locally to the JDK {{java.security}} file. I’m not sure that at the time of CASSANDRA-16848 it was intended for any JDK versions.

The point of mentioning this is that if {{negotiatedProtocolMustBeAcceptedProtocolTest}} is changed to use TLSv1.2, and support for JDK8 is dropped, then the changes made in CASSANDRA-16848 could also be reverted.
"
CASSANDRA-18538,Disable chronicle analytics," At least chronicle-queue phones home to google analytics:  https://github.com/OpenHFT/Chronicle-Queue/blob/chronicle-queue-5.20.116/DISCLAIMER.adoc

but we can disable all chronicle analytics with a single flag: -Dchronicle.analytics.disable=true"
CASSANDRA-18537,Add JMX utility class to in-jvm dtest to ease development of new tests using JMX,"While reviewing CASSANDRA-18511, some repetitive code was identified across the 4 branches, and 2 different tests, that would also be repeated for any new usages of the JMX support in the in-jvm dtest framework. Therefore, a utility class should be added to the dtest-api's `shared` package that will simplify some of this repetitive and error-prone code."
CASSANDRA-18532,Cannot read or repair data after dropping a column when one of the nodes is down,"When a column is dropped while one of the nodes is down, when the node come back online, repair or querying the data fails.

This can be reproduced by running the following in-jvm dtest:


{code:java}
    @Test
    public void testDroppingColumnWhenOneNodeIsDown() throws IOException, ExecutionException, InterruptedException
    {
        try (Cluster cluster = init(Cluster.build(3).start()))
        {
            cluster.schemaChange(""create table "" + KEYSPACE + "".tab (id int primary key, v1 int, v2 int)"");
            cluster.disableAutoCompaction(KEYSPACE);

            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tab (id, v1, v2) VALUES (?, ?, ?)"", ConsistencyLevel.ALL, 1, 2, 3);
            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tab (id, v1) VALUES (?, ?)"", ConsistencyLevel.ALL, 4, 5);
            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tab (id, v2) VALUES (?, ?)"", ConsistencyLevel.ALL, 6, 7);

            cluster.get(3).shutdown(true).get();

            cluster.schemaChangeIgnoringStoppedInstances(""alter table "" + KEYSPACE + "".tab drop v1"");

            cluster.get(3).startup();

            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tab (id, v2) VALUES (?, ?)"", ConsistencyLevel.ALL, 1, 9);
            cluster.coordinator(1).execute(""insert into "" + KEYSPACE + "".tab (id, v2) VALUES (?, ?)"", ConsistencyLevel.ALL, 2, 11);

            NodeToolResult repairResult = cluster.get(1).nodetoolResult(""repair"", KEYSPACE, ""tab"");
            assertThat(repairResult.getRc()).isZero();

            SimpleQueryResult rows = cluster.coordinator(1).executeWithResult(""select * from "" + KEYSPACE + "".tab"", ConsistencyLevel.ALL);
            assertRows(rows.toObjectArrays(), row(1, 9), row(2, 11), row(4, null), row(6, 7));

            assertRows(cluster.coordinator(1).executeWithResult(""select * from "" + KEYSPACE + "".tab where id = ?"", ConsistencyLevel.ALL, 1).toObjectArrays(), row(1, 9));
            assertRows(cluster.coordinator(1).executeWithResult(""select * from "" + KEYSPACE + "".tab where id = ?"", ConsistencyLevel.ALL, 2).toObjectArrays(), row(2, 11));
            assertRows(cluster.coordinator(1).executeWithResult(""select * from "" + KEYSPACE + "".tab where id = ?"", ConsistencyLevel.ALL, 4).toObjectArrays(), row(4, null));
            assertRows(cluster.coordinator(1).executeWithResult(""select * from "" + KEYSPACE + "".tab where id = ?"", ConsistencyLevel.ALL, 6).toObjectArrays(), row(6, 7));
        }
    }
{code}


"
CASSANDRA-18531,BLOG - EOL Announcement for 3.x and 3.0.x,"This ticket is to capture the work associated with publishing the May 2023 blog ""Apache Cassandra 3.0.x and 3.11.x - End of Life Announcement"""
CASSANDRA-18529,Remove legacy command line options from cassandra-stress,"The cassandra-stress *mode* option allows specifying options for native protocol and cql3, but these don't seem useful as there would seem to be no other valid options now that cql3 is the standard and thrift no longer supported. 

    -mode native cql3 user=cassandra password=xxxxxx

can be simplified to:

    -mode user=cassandra password=xxxxxx

Also, the readme.txt in tools/stress states ""cassandra-stress supports benchmarking any Cassandra cluster of version 2.0+"" but maybe should be updated to a supported Cassandra version, i.e., 3.11.x."
CASSANDRA-18525,Add keyspace column to clients virtual table,Clients virtual table doesn’t have keyspace information. ‘clientstats' nodetool has this information. Adding keyspace column to clients virtual table helps not to fallback to nodetool command.
CASSANDRA-18524,CEP-15: (Accord) Separate durable and transient listeners,"Transient listeners should be handled differently and, ironically, should be more ""persistent"" in that they should not disappear when we evict state from cache. This patch separates listeners into `DurableAndIdempotent` and `Transient` with the latter being saved in a shared global register that also more easily permits us to ensure we do not invoke listeners redundantly (and for listeners themselves to know if we have done so). This is also a stepping stone to ensuring listeners survive cache eviction."
CASSANDRA-18523,CEP-15: (Accord) Join cluster without full transaction log,Joining replicas should not require the full transaction history to successfully start serving queries. This ticket introduces mechanisms for a replica to join (or catch up) with a data snapshot and all transactions that execute after that snapshot. This is a precursor for transaction state GC.
CASSANDRA-18522,LEAK DETECTED: org.apache.cassandra.io.util.FileHandle$Cleanup in TestSecondaryIndexes.test_failing_manual_rebuild_index,"A leak was detected in CI run: https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2344/workflows/1f57b9a0-3fc9-49c7-a821-52e24b025056/jobs/22547/parallel-runs/4/steps/4-106{{{}

==================================== ERRORS ===================================={}}}
{{_ ERROR at teardown of TestSecondaryIndexes.test_failing_manual_rebuild_index __}}
{{Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [Reference-Reaper] 2023-05-05 17:57:29,429 Ref.java:237 - LEAK DETECTED: a reference (class org.apache.cassandra.io.util.FileHandle$Cleanup@1392096802:/tmp/dtest-nl3k0_2v/test/node1/data0/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-1-big-Index.db) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1392096802:/tmp/dtest-nl3k0_2v/test/node1/data0/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-1-big-Index.db was not released before the reference was garbage collected', [node1] 'ERROR [Reference-Reaper] 2023-05-05 17:57:29,430 Ref.java:237 - LEAK DETECTED: a reference (class org.apache.cassandra.io.util.FileHandle$Cleanup@1379143890:/tmp/dtest-nl3k0_2v/test/node1/data1/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-2-big-Index.db) to class org.apache.cassandra.io.util.FileHandle$Cleanup@1379143890:/tmp/dtest-nl3k0_2v/test/node1/data1/k/t-4374ee60eb6e11ed99961b44b0ab6f7e/nc-2-big-Index.db was not released before the reference was garbage collected']}}
{{==============}}"
CASSANDRA-18521,Unify CQLTester#waitForIndex and SAITester#waitForIndexQueryable,"It has been [mentioned|https://github.com/maedhroz/cassandra/pull/11#discussion_r1190166765] in CASSANDRA-18217 that {{CQLTester#waitForIndex}} and {{SAITester#waitForIndexQueryable}} can be unified into a single method. 

The same discussion mentions that it's easy to forget to call those methods after calling {{CQLTester#createIndex}}. So that method can be renamed to {{createIndexAsync}}, and we can create a new method that creates the index and waits for it ({{createIndexSync}}, {{createIndexAndWaitForQueryable}}, or something like that)."
CASSANDRA-18519,CEP-15: (C*) Add notion of CommandsForRanges and make this durable in C*,"To add support for range transactions in C* we need to make sure

1) their state is durable and can be recovered on restart
2) have some way to find all CommandsForKey that are contained in the range transaction range
3) have some way to find all CommandsForRange that intersect this range

To do this, I propose the following

1) Create a new commands_for_range table that stores: (store, range) -> list<accord_timestamp_tuple> — this is byId, not sure if repair needs byExecuteId as well
2) For C*, store a in-memory mapping of Range -> List<TxnId>, and on-boot repopulate this cache.  This then can be used to construct the CommandsForRange needed by the transaction. This makes an assumption that many ranges will not exist, at least for the time being.
3) Change commands_for_keys to use LocalPartitioner, and order the table by (store, key)
4) When C* sees a range transaction, find all keys that are contained by the range by running the logical query ""SELECT key FROM commands_for_keys WHERE key BETWEEN range.start AND range.end"". Implementation has to make sure to handle many keys (may need to partition the range to increase parallel access, and may need to page through the table to see all keys (aka multiple ReadCommands)).  Once all keys are found, then must load into the CommandsForKeys cache

For #4, https://github.com/apache/cassandra-accord/pull/27 maybe able to optimize the logic to lazy load only what is actually needed rather than load the whole world"
CASSANDRA-18518, NullPointerException in cassandra.audit.AuditLogManager.querySuccess when querying endpoints via mgmtapi," NullPointerException in cassandra.audit.AuditLogManager.querySuccess when querying endpoints via mgmtapi

 

cassandra version: 4.1.1

cassandra.yaml:audit_logging_options:
cassandra.yaml-  enabled: true
cassandra.yaml-  logger:
cassandra.yaml:    - class_name: FileAuditLogger

 

When querying endpoints (/api/v0/metadata/endpoints) via mgmtapi, 200 OK is returned.  However, NPE is reported by AuditLogManager.
{code:java}
ERROR [epollEventLoopGroup-5-3] 2023-05-10 15:53:30,301 NoSpamLogger.java:111 - Failed notifying listeners
java.lang.NullPointerException: null
        at org.apache.cassandra.audit.AuditLogManager.querySuccess(AuditLogManager.java:244)
        at org.apache.cassandra.cql3.QueryEvents.notifyQuerySuccess(QueryEvents.java:78)
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:117)
        at org.apache.cassandra.transport.Message$Request.execute(Message.java:254)
        at org.apache.cassandra.transport.UnixSocketServer41x$UnixSockMessage.channelRead0(UnixSocketServer41x.java:106)
        at org.apache.cassandra.transport.UnixSocketServer41x$UnixSockMessage.channelRead0(UnixSocketServer41x.java:86)
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
        at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795)
        at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138)
        at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:480)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
 {code}
 

 "
CASSANDRA-18516,"""Connection refused"" problem","Hello! I am trying to deploy The Hive 4 on a VMware Workstation 17 player VM to test Splunk integration with The Hive. 
I am following the guide at this [link|https://docs.thehive-project.org/thehive/installation-and-configuration/installation/step-by-step-guide/], but I encountered an error at one of the stages, namely when launching cassandra using the cqlsh localhost 9042 command:

Connection error: ('Unable to connect to any servers', \{'127.0.0.1': error(111, ""Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused"")})

I tried to solve the problem based on the information from [this|https://stackoverflow.com/questions/29121904/cassandra-cqlsh-connection-refused] site, but it didn't help me.

OS Version - Ubuntu 22.04.2 LTS

I'm new in this field, I can provide any information you need"
CASSANDRA-18515,Optimize Initial Concurrency Selection for Range Read Algorithm During SAI Queries,"The range read algorithm relies on the Index API’s notion of estimated result rows to decide how many replicas to contact in parallel during its first round of requests. The more results expected from a replica for a token range, the fewer replicas the range read will initially try to contact. Like SASI, SAI floors that estimate to a huge negative number to make sure it’s selected over other indexes, and this floors the concurrency factor to 1. The actual formula looks like this:

{code:java}
// resultsPerRange, from SAI, is a giant negative number
concurrencyFactor = Math.max(1, Math.min(ranges.rangeCount(), (int) Math.ceil(command.limits().count() / resultsPerRange)));
{code}

Although that concurrency factor is updated as actual results stream in, only sending a single range request to a single replica in every case for SAI is not ideal. For example, assume I have a 3 node cluster and a keyspace at RF=1, with 10 rows spread across the 3 nodes, without vnodes. Issuing a query that matches all 10 rows with a LIMIT of 10 will make 2 or 3 serial range requests from the coordinator, one to each of the 3 nodes.

This can be fixed by allowing indexes to bypass the initial concurrency calculation allowing SAI queries to contact the entire ring in a single round of queries, or at worst the minimum number of rounds as bounded by the existing statutory maximum ranges per round.
"
CASSANDRA-18513,Limit the read size on the row level,"When deserializing a row, we have a limit on amount of data we can read per cell. If an sstable is corrupted in a certain way, it may read many cells with extreme length leading to oom errors. When we read a row, it has a row size in the beginning. The idea is to limit the read size for a cell to the minimum of the max cell length and the remaining row size. "
CASSANDRA-18512,nodetool describecluster command is not showing correct Down count.  ,"There are some nodes down in the cluster of Cassandra Version 4.x

# nodetool describecluster command output shows these ips as unreachable.

UNREACHABLE: [<ip1>, <ip2>, <ip3>]

Stats for all nodes:
        Live: 3
        Joining: 0
        Moving: 0
        Leaving: 0
        Unreachable: 3

But under data center , count of down pod is always shown as 0.

Data Centers: 
    dc1 #Nodes: 3 #Down: 0
    dc2 #Nodes: 3 #Down: 0

 

Steps to reproduce:
 # Setup two Data centers dc1,dc2, each datacenter was having 3 nodes - dc1:3,dc2:3
 # mark down any 3 nodes of two data centers.
 # Run nodetool describecluster command from the live node and check the Unreachable count , which is 3 and Down Count is 0 , both are not matched.

 

Expected Output: Unreachable and Down count should have the same value.

Data Centers:
        dc1 #Nodes: 3 #Down: 1

        dc2 #Nodes: 3 #Down: 2

 

 "
CASSANDRA-18511,Add support for JMX in the in-jvm dtest framework,"In many cases, it would be useful to be able to enable JMX endpoints within the in-jvm dtest framework, including the existing JMX Getter test, which used to simply spin up a JMX registry and then leave it running.  There are quite a few JMX-related functions that don’t have tests today, and some external usages of the in-jvm dtest framework could also benefit from exposing JMX like we did Native before."
CASSANDRA-18507,Partial compaction can resurrect deleted data,"If there isn't enough disk space available to compact all existing sstables, Cassandra will attempt to perform a partial compaction by removing sstables from the set of candidate sstables to be compacted, starting with the largest one. It is possible that the sstable removed from the set of sstables to compact contains data for which there are tombstones in another (more recent) sstable. Since the overlaps between sstables is computed when the {{CompactionController}} is created, and the {{CompactionController}} is created before the removal of any sstables from the set of sstables to be compacted this computed overlap will be outdated when checking which sstables are covered by certain tombstones. This leads to the faulty conclusion that the tombstones can be pruned during the compaction, causing the data to be resurrected.

The issue is present in Cassandra 4.0 and 4.1. Cassandra 3.11 creates the {{CompactionController}} after the set of sstables to compact has been reduced, and is thus not affected. {{trunk}} does not appear to support partial compactions at all, but instead refuses to compact when the disk is full.

This regression appears to have been introduced by CASSANDRA-13068."
CASSANDRA-18506,Deduplicate the MixedModeAvailability upgrade jvm-dtests,"There's V30 and V3X duplicates of each subclass test to MixedModeAvailabilityTestBase.

The V3X tests will always also be run in the V30.

Suggestion is to dedup and remove the version from the names. "
CASSANDRA-18505,NPE when deserializing malformed collections from client,"When deserializing collections sent from the client, if an element in the collection is incorrectly serialized, Collections.getValue can return null if the length of the element is negative.  Currently this isn't detected and serialization continues, calling validate and throwing an NPE in serializers that don't handle null value buffers.

Detect the malformed input and throw a better MarshalException so it will be converted to an InvalidRequestException for the client.
"
CASSANDRA-18504,"Added support for type VECTOR<type, dimension>","Based off several mailing list threads (see ""[POLL] Vector type for ML”, ""[DISCUSS] New data type for vector search”, and ""Adding vector search to SAI with heirarchical navigable small world graph index”), its desirable to add a new type “VECTOR” that has the following properties

1) fixed length array
2) elements may not be null
3) flatten array (aka multi-cell = false)"
CASSANDRA-18503,CircleCI java distributed tests to run in Medium containers,"With the paid configuration we have in-tree it is enough to run the java distributed regular and upgrade tests in Medium containers, instead of Large containers"
CASSANDRA-18500,Add guardrail for partition size,"Add a guardrail for max partition size, for example:
{code:java}
partition_size_warn_threshold: 50MiB
partition_size_fail_threshold: 100MiB
{code}
Most probably this guardrail would only be checked when writing a new sstable to disk (fush/compact). Triggering the guardrail on sstable write would emit a log message and a diagnostic event, but it wouldn't reject the write."
CASSANDRA-18499,Test Failure: upgrade_through_versions_test.py::TestUpgrade::test_rolling_upgrade_with_internode_ssl,"The upgrade Python dtest {{upgrade_through_versions_test.py::TestUpgrade::test_rolling_upgrade_with_internode_ssl}} seems to fail on CircleCI at least for trunk:
 * [https://app.circleci.com/pipelines/github/adelapena/cassandra/2882/workflows/b9abc2b2-2e79-47b2-b7ce-c549e75293bc/jobs/42698/tests]

{code:java}
self = <upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD object at 0x7f89fc0f17f0>

    @pytest.mark.timeout(3000)
    def test_rolling_upgrade_with_internode_ssl(self):
        """"""
        Rolling upgrade test using internode ssl.
        """"""
>       self.upgrade_scenario(rolling=True, internode_ssl=True)

upgrade_tests/upgrade_through_versions_test.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD object at 0x7f89fc0f17f0>
populate = True, create_schema = True, rolling = True, after_upgrade_call = ()
internode_ssl = True

    def upgrade_scenario(self, populate=True, create_schema=True, rolling=False, after_upgrade_call=(), internode_ssl=False):
        # Record the rows we write as we go:
        if populate:
            self.prepare()
        self.row_values = set()
        cluster = self.cluster
        if cluster.version() >= '3.0':
            cluster.set_configuration_options({'enable_user_defined_functions': 'true',
                                               'enable_scripted_user_defined_functions': 'true'})
        elif cluster.version() >= '2.2':
            cluster.set_configuration_options({'enable_user_defined_functions': 'true'})
    
        if internode_ssl:
            logger.debug(""***using internode ssl***"")
            generate_ssl_stores(self.fixture_dtest_setup.test_path)
            self.cluster.enable_internode_ssl(self.fixture_dtest_setup.test_path)
    
        if populate:
            # Start with 3 node cluster
            logger.debug('Creating cluster (%s)' % self.test_version_metas[0].version)
            cluster.populate(3)
            [node.start(use_jna=True, wait_for_binary_proto=True) for node in cluster.nodelist()]
        else:
            logger.debug(""Skipping cluster creation (should already be built)"")
    
        # add nodes to self for convenience
        for i, node in enumerate(cluster.nodelist(), 1):
            node_name = 'node' + str(i)
            setattr(self, node_name, node)
    
        if create_schema:
            if rolling:
                self._create_schema_for_rolling()
            else:
                self._create_schema()
        else:
            logger.debug(""Skipping schema creation (should already be built)"")
    
        self._log_current_ver(self.test_version_metas[0])
    
        if rolling:
            # start up processes to write and verify data
            write_proc, verify_proc, verification_queue = self._start_continuous_write_and_verify(wait_for_rowcount=5000)
    
            # upgrade through versions
            for version_meta in self.test_version_metas[1:]:
                if version_meta.family > '3.11' and internode_ssl:
                    seeds =[]
                    for seed in cluster.seeds:
>                       seeds.append(seed.ip_addr + ':7001')
E                       AttributeError: 'str' object has no attribute 'ip_addr'

upgrade_tests/upgrade_through_versions_test.py:422: AttributeError
{code}
I haven't seen this failure on Jenkins yet."
CASSANDRA-18497,snakeyaml vulnerability: CVE-2023-2251,"This is failing the OWASP scan.

https://nvd.nist.gov/vuln/detail/CVE-2023-2251"
CASSANDRA-18495,Warnings when using the perl driver to connect to Cassandra,"When I use the [perl driver|https://github.com/TvdW/perl-DBD-Cassandra] to connect to Cassandra 4.0.3 and onwards, I get the following error just for initialization of a connection.

The error I get is
{noformat}
`USE <keyspace>` with prepared statements is considered to be an anti-pattern due to ambiguity in non-qualified table names. Please consider removing instances of `Session#setKeyspace(<keyspace>)`, `Session#execute(""USE <keyspace>"")` and `cluster.newSession(<keyspace>)` from your code, and always use fully qualified table names (e.g. <keyspace>.<table>). Keyspace used: null, statement keyspace: null, statement id: <id> at Cassandra/Client/Connection.pm line 957.
{noformat}
 

This is just from initialization of the perl driver connection while choosing the keyspace. [https://github.com/TvdW/perl-DBD-Cassandra/blob/master/Cassandra-Client/lib/Cassandra/Client/Connection.pm#L562] before running any queries. It is emitted even if I use qualified prepared statement.

The same warning does not pop up in Datastax java driver initialization.

On debugging, I found that this warning does not emit for all unqualified prepared statements. It only emits for unqualified prepared ""USE ks"" statement. But the ""USE ks"" can never be qualified. So the warning is a bit vague on what is the recommended approach.

And from the perspective of the driver, it is setting the keyspace of the connection for the first time. The same warning does not happen on the datastax java driver and that is because it uses QUERY to set the keyspace on connection. (I tried to follow the same approach on the perl driver - https://github.com/TvdW/perl-DBD-Cassandra/pull/35 )

The warnings are not very clear on what is deprecated and what is not. Does it deprecate only the use of prepared statement of ""USE ks""? or does it deprecate ""USE ks"" completely? And it is not being emitted for other unqualified prepared statements but only for a USE statement which cannot be qualified at all."
CASSANDRA-18494,Upgrade to lucene-core 9.7.0,The lucene-core library is currently at 7.5. This should be updated to whatever the latest stable version of lucene is.
CASSANDRA-18490,Checksum per-SSTable and per-column SAI components after streaming,"The SAI code currently does not checksum validate per-column index data files at any point. It does checksum validate per-sstable components after a full rebuild and it checksum validates the per-column metadata on opening.

We should checksum validate all index components on startup, full rebuild and streaming.

EDIT: The changes we ultimately made here were:

- Fix checksum calculation in {{IncrementalChecksumSequentialWriter}}
- Checksum per-SSTable and per-column components after streaming
- Avoid validating indexes when full rebuild is requested"
CASSANDRA-18488,"WEBSITE - Replace homepage, Events page banners with C* Summit","Cassandra Forward is in the past, so this addresses the fact by simply replacing the banner and the ""register now"" CTA with an equivalent thing for the December C* Summit.

The (very simple) PR is available for inspection here: [https://github.com/apache/cassandra-website/pull/219]

See images:

 !banner_on_index_page.png|width=300!

 !banner_on_events_page.png|width=300!"
CASSANDRA-18487,MappedByteBufferIndexInputProvider can better throw UndeclaredThrowableException,"MappedByteBufferIndexInputProvider has the following code:
{code:java}
 private static BufferCleaner newBufferCleaner(final MethodHandle unmapper) {

...

try {
        unmapper.invokeExact(buffer);
      } catch (Throwable t) {
        throw new IOException(""Unable to unmap the mapped buffer: "" + resourceDescription, t);
      }

 }{code}
This method rethrow IOException, when calling invokeExact fails. However, other methods throw UndeclaredThrowableException for the same errors. For example, SnowballProgram.find_among is as follows:
{code:java}
protected int find_among(Among v[]) {

 ..,
 
try {
          res = (boolean) w.method.invokeExact(this);
        } catch (Error | RuntimeException e) {
          throw e;
        } catch (Throwable e) {
          throw new UndeclaredThrowableException(e);
        }
}{code}
Another example is AttributeFactory.{color:#000000}createInstance{color}

:
{code:java}
 
 protected A createInstance() {
        try {
          // be explicit with casting, so javac compiles correct call to polymorphic signature:
          final AttributeImpl impl = (AttributeImpl) constr.invokeExact();
          // now cast to generic type:
          return (A) impl;
        } catch (Error | RuntimeException e) {
          throw e;
        } catch (Throwable e) {
          throw new UndeclaredThrowableException(e);
        }
      }

   {code}
UndeclaredThrowableException looks more reasonable.  Can this be fixed?"
CASSANDRA-18486,LeveledCompactionStrategy does not check its constructor,"LeveledCompactionStrategy  has the following code:

 
{code:java}
public static Map<String, String> validateOptions(Map<String, String> options) throws ConfigurationException
    {
        Map<String, String> uncheckedOptions = AbstractCompactionStrategy.validateOptions(options);

        String size = options.containsKey(SSTABLE_SIZE_OPTION) ? options.get(SSTABLE_SIZE_OPTION) : ""1"";
        try
        {
            int ssSize = Integer.parseInt(size);
            if (ssSize < 1)
            {
                throw new ConfigurationException(String.format(""%s must be larger than 0, but was %s"", SSTABLE_SIZE_OPTION, ssSize));
            }
        }
        catch (NumberFormatException ex)
        {
            throw new ConfigurationException(String.format(""%s is not a parsable int (base10) for %s"", size, SSTABLE_SIZE_OPTION), ex);
        }
...
} {code}
This method throws ConfigurationException when the configuration file is wrong. The thrown exception is easy to understand, and calling code can catch this exception to handle configuration files with wrong formats. However, the constructor of this class does not check configuration files:

 

 

 
{code:java}
 public LeveledCompactionStrategy(ColumnFamilyStore cfs, Map<String, String> options)

{    ... 
     
configuredMaxSSTableSize = Integer.parseInt(options.get(SSTABLE_SIZE_OPTION));
 ...

configuredLevelFanoutSize = Integer.parseInt(options.get(LEVEL_FANOUT_SIZE_OPTION));

...
}

{code}
As a result, this class can throw NumberFormatException when configuration files are wrong, and will not throw ConfigurationException. 

It is strange for calling code to call the static validation method before calling its constructor. Can this problem be fixed?

 

 "
CASSANDRA-18485,CEP-15: (C*) Enhance in-memory FileSystem to work with mmap and support tests to add custom logic,"The Simulator currently uses JimFS for its FileSystem, but this lacks any way to add custom logic when file operations are performed; it also lacks mmap support which requires all disk access logic to provide non-mmap solutions as well.

As part of the Simulator work, testing disk corruption will require resolving both these issues, so will need a new FileSystem to rule them all…

This ticket is to define the new FileSystem and add the integration to Simulator, JVM-Dtest, and Unit tests, but does not directly add the fault injections that Simulator will be doing, that will be follow up work.

Goals:
* FileSystem that works for unit, jvm-dtest, and simulator tests
* FileSystem that allows tests to intercept file operations"
CASSANDRA-18482,Test Failure: HintsDisabledTest.testHintedHandoffDisabled,"Test throws timeout exception before the CL.ONE request completes. Need to increase the timeout and make sure hints have had the opportunity to deliver.
 
{{code}}
Error Message
Operation timed out - received only 0 responses.
Stacktrace
org.apache.cassandra.exceptions.WriteTimeoutException: Operation timed out - received only 0 responses.
	at org.apache.cassandra.service.AbstractWriteResponseHandler.throwTimeout(AbstractWriteResponseHandler.java:152)
	at org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:130)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:901)
	at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:1151)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:519)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:493)
	at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:122)
	at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:103)
	at org.apache.cassandra.distributed.impl.Coordinator.lambda$executeWithResult$0(Coordinator.java:66)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
{{code}}"
CASSANDRA-18479,Add basic text tokenisation and analysis,"[CASSANDRA-16092|https://issues.apache.org/jira/browse/CASSANDRA-16092] removed support for any text analysis or tokenisation. 

SAI currently supports the following analyzers:
* normalize - text normalization using NFC normalization
* case_sensitive - allow control over the case sensitivity of an index
* ascii - allow ascii folding of text

"
CASSANDRA-18477,Do not require allow filtering when all primary keys are specified in SELECT ,"This was discussed in

https://lists.apache.org/thread/loj6jgv54szdvyt3wmvbtwwrrg1dtlxq

Basically, when I have this table:

{code}
create table ks.tb (p1 int, c1 int, col1 int, col2 int, primary key (p1, c1));
{code}

and I do this

{code}
select * from ks.tb where p1 = 1 and c1 = 2 and col2 = 1;
{code}

this will fail and it will require to use ALLOW FILTERING just because we are also specifying ""col2"". This is clearly a bug - there is no reason to require it if we are going to fetch one row only as all partition and clustering keys were specified."
CASSANDRA-18476,"NativeTransportEncryptionOptionsTest fails with ant testsome, passes in intellij","environment:

linux, java 11:

 

{{$ set|grep JAVA}}
{{JAVA11_HOME=/home/jakub/.sdkman/candidates/java/11.0.11.hs-adpt}}
{{JAVA_HOME=/home/jakub/.sdkman/candidates/java/current}}
{{$ which java}}
{{/home/jakub/.sdkman/candidates/java/current/bin/java}}
{{$ java -version}}
{{openjdk version ""11.0.11"" 2021-04-20}}
{{OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9)}}
{{OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode)}}

repro steps:
 # clone cassandra (trunk HEAD as of writing this report: 8df072e104f6ae5391de12d58e9f973f08cb2c57)
 # {{ant testsome -Dtest.name=org.apache.cassandra.distributed.test.NativeTransportEncryptionOptionsTest -Dtest.method=optionalTlsConnectionAllowedToRegularPortTest}} fails after ~2m30s (see attached ant-logs.txt)
 # {{ant generate-idea-files}}
 # open project in idea, navigate to {{{}NativeTransportEncryptionOptionsTest{}}}, right click on {{optionalTlsConnectionAllowedToRegularPortTest}} and {{{}Run test{}}}; the test passes after ~12s (see attached intellij-logs.txt)"
CASSANDRA-18474,Incremental repairs fail on mixed IPv4/v6 addresses serializing SyncRequest,"The {{SyncRequest}} message assumes the initiator/src/dst are all the IP address
type as an optimization when calculating serialized size. It needs to handle mixed address families.

{code}
/1.2.3.4:7000->/[::1]:7000-SMALL_MESSAGES-f83ce0bc dropping message of type SYNC_REQ due to error""
{code}

{Exception}
{code}
org.apache.cassandra.net.InvalidSerializedSizeException: Invalid serialized size; expected 158, actual size at least 170, for verb SYNC_REQ
        at org.apache.cassandra.net.OutboundConnection$EventLoopDelivery.doRun(OutboundConnection.java:816)
        at org.apache.cassandra.net.OutboundConnection$Delivery.run(OutboundConnection.java:687)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:829)
{code}
"
CASSANDRA-18473,Storage Attached Indexes (Phase 2),"At the completion of CASSANDRA-16052, we should be able to release the core capabilities of SAI in a stable, production-ready package. Once that begins to gain traction, we'll be able to make improvements and add features for the next major release. The major initial theme of this epic is likely to be performance, but it will likely expand to include features like basic text analysis, etc."
CASSANDRA-18472,Docker images can no longer be built due to virtualenv from pip,"{noformat}
 => [linux/amd64 35/56] WORKDIR /home/cassandra                                                                                                                                                              0.1s
 => [linux/amd64 36/56] RUN echo 'export ANT_HOME=/usr/share/ant' >> /home/cassandra/.bashrc &&     echo 'export JAVA8_HOME=/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)' >> /home/cassandra/.b  0.2s
 => ERROR [linux/amd64 37/56] RUN virtualenv --python=python2.7 env2.7                                                                                                                                       0.5s
------
 > [linux/amd64 37/56] RUN virtualenv --python=python2.7 env2.7:
#100 0.424 RuntimeError: failed to find interpreter for Builtin discover of python_spec='python2.7'
------
ubuntu2004_j11.docker:128
--------------------
 126 |     # included in the base image, the compiled objects are not updated by pip at run time, which can
 127 |     # cause errors if the tests rely on new driver functionality or bug fixes.
 128 | >>> RUN virtualenv --python=python2.7 env2.7
 129 |     RUN chmod +x env2.7/bin/activate
 130 |     RUN /bin/bash -c ""export CASS_DRIVER_NO_CYTHON=1 CASS_DRIVER_NO_EXTENSIONS=1 && source ~/env2.7/bin/activate && pip2 install --upgrade pip && pip2 install -r /opt/requirements.txt && pip2 freeze --user""
--------------------
error: failed to solve: rpc error: code = Unknown desc = process ""/bin/sh -c virtualenv --python=python2.7 env2.7"" did not complete successfully: exit code: 1
{noformat}"
CASSANDRA-18471,CEP-15 Accord: NotWitnessed commands can receive an invalidate promise but would return Zero instead,"While working on CASSANDRA-18451 I hit the following failure

{code}
Failed on seed -5929214838499924343
accord.burn.SimulationException: Failed on seed -5929214838499924343
Caused by: java.lang.AssertionError: Unexpected exception encountered
	at accord.impl.basic.PropagatingPendingQueue.poll(PropagatingPendingQueue.java:73)
	at accord.impl.basic.Cluster.processPending(Cluster.java:179)
	at accord.impl.basic.Cluster.run(Cluster.java:296)
	at accord.burn.BurnTest.burn(BurnTest.java:309)
	at accord.burn.BurnTest.run(BurnTest.java:386)
	at accord.burn.BurnTest.testOne(BurnTest.java:372)
	Suppressed: java.lang.IllegalStateException: Received replies from a node that must have known the route, but that did not include it
		at accord.coordinate.Invalidate.invalidate(Invalidate.java:204)
		at accord.coordinate.Invalidate.handle(Invalidate.java:131)
		at accord.coordinate.Invalidate.onSuccess(Invalidate.java:105)
		at accord.coordinate.Invalidate.onSuccess(Invalidate.java:51)
		at accord.impl.basic.Cluster.lambda$processNext$1(Cluster.java:209)
		at accord.impl.basic.Cluster.now(Cluster.java:260)
		at accord.impl.basic.Cluster.processNext(Cluster.java:206)
		at accord.impl.basic.Cluster.processPending(Cluster.java:183)
{code}

In a debugger was able to figure out the state and create a unit test to hit the same situation

{code}
class InvalidateTest
{
    @Test
    void test() throws ExecutionException
    {
        try (MockCluster cluster = MockCluster.builder().replication(2).nodes(2).build())
        {
            Node n1 = cluster.get(1);
            Node n2 = cluster.get(2);

            RoutingKey n1RoutingKey = n1.topology().current().get(0).range.end();
            IntKey.Raw n1key = IntKey.key(((IntKey.Routing) n1RoutingKey).key);

            RoutingKey n2RoutingKey = n1.topology().current().get(1).range.end();
            IntKey.Raw n2key = IntKey.key(((IntKey.Routing) n2RoutingKey).key);

            Keys keys = Keys.of(n1key, n2key);


            Node coordinator = n1;
            TxnId txnId = coordinator.nextTxnId(Txn.Kind.Read, Routable.Domain.Key);
            Txn txn = readOnly(keys);

            AsyncChains.getUninterruptibly(n2.commandStores().unsafeForKey(n2key).execute(PreLoadContext.contextFor(txnId, keys), store -> {
                Ranges ranges = store.ranges().currentRanges();
                PartialTxn partial = txn.slice(ranges, true);
                FullKeyRoute route = keys.toRoute(n2RoutingKey);
//                RoutingKey progressKey = n2RoutingKey.toUnseekable(); // if this is non-null this passes
                RoutingKey progressKey = null;
                CheckedCommands.preaccept(store, txnId, partial, route, progressKey);
                CheckedCommands.accept(store, txnId, Ballot.ZERO, route.slice(ranges), partial.keys().slice(ranges), progressKey, txnId, PartialDeps.builder(ranges).build());
            }));
            AsyncChains.getUninterruptibly(new AsyncChains.Head<Outcome>() {
                @Override
                protected void start(BiConsumer<? super Outcome, Throwable> callback) {
                    Invalidate.invalidate(coordinator, txnId, keys.toUnseekables(), callback);
                }
            });
        }
    }

    private static Txn readOnly(Seekables<?, ?> keys)
    {
        Read read = MockStore.read(keys);
        Query query = Mockito.mock(Query.class);
        return new Txn.InMemory(keys, read, query);
    }
}
{code}"
CASSANDRA-18467,Update generate-idea-files for J17,"There was a discussion in CASSANDRA-18258 how to update generate-idea-files.

The final agreement was to create one target to cover both Java 11 and Java 17.

It will be good to figure out CASSANDRA-18263 and reshuffle arguments and tasks based on what we decide to use as gc in testing for both Java 11 and Java 17."
CASSANDRA-18466,Paxos only repair is treated as an incremental repair,"Paxos only repair tries to continue or is treated as an incremental repair. This happened on 4.1.0 and 4.1.1 when trying to run repair in preparation for enabling paxos_state_purging. The repair was in preparation mode triggered multiple anti-compactions on the nodes. Running the command with --full behaves in the expected way, ie. only the paxos data is repaired and it's finished within a few seconds.
{code:java}
nodetool repair --paxos-only // This does not behave as expected, does it complete quickly and seems to be waiting on anticompactions
{code}
{code:java}
nodetool repair --full --paxos-only // Completes within a few seconds as expected
{code}"
CASSANDRA-18464,Enable Direct I/O For CommitLog Files,"Relocating from [dev@ email thread.|https://lists.apache.org/thread/j6ny17q2rhkp7jxvwxm69dd6v1dozjrg]

 

I shared my investigation about Commitlog I/O issue on large core count system in my previous email dated July-22 and link to the thread is given below.
[https://lists.apache.org/thread/xc5ocog2qz2v2gnj4xlw5hbthfqytx2n]

Basically, two solutions looked possible to improve the CommitLog I/O.
 # Multi-threaded syncing
 # Using Direct-IO through JNA

I worked on 2nd option considering the following benefit compared to the first one
 # Direct I/O read/write throughput is very high compared to non-Direct I/O. Learnt through FIO benchmarking.
 # Reduces kernel file cache uses which in-turn reduces kernel I/O activity for Commitlog files only.
 # Overall CPU usage reduced for flush activity. JVisualvm shows CPU usage < 30% for Commitlog syncer thread with Direct I/O feature
 # Direct I/O implementation is easier compared to multi-threaded

As per the community suggestion, less in code complex is good to have. Direct I/O enablement looked promising but there was one issue. 
Java version 8 does not have native support to enable Direct I/O. So, JNA library usage is must. The same implementation should also work across other versions of Java (like 11 and beyond).

I have completed Direct I/O implementation and summary of the attached patch changes are given below.
 # This implementation is not using Java file channels and file is opened through JNA to use Direct I/O feature.
 # New Segment are defined named “DirectIOSegment”  for Direct I/O and “NonDirectIOSegment” for non-direct I/O (NonDirectIOSegment is test purpose only).
 # JNA write call is used to flush the changes.
 # New helper functions are defined in NativeLibrary.java and platform specific file. Currently tested on Linux only.
 # Patch allows user to configure optimum block size  and alignment if default values are not OK for CommitLog disk.
 # Following configuration options are provided in Cassandra.yaml file
a. use_jna_for_commitlog_io : to use jna feature
b. use_direct_io_for_commitlog : to use Direct I/O feature.
c. direct_io_minimum_block_alignment: 512 (default)
d. nvme_disk_block_size: 32MiB (default and can be changed as per the required size)

 Test matrix is complex so CommitLog related testcases and TPCx-IOT benchmark was tested. It works with both Java 8 and 11 versions. Compressed and Encrypted based segments are not supported yet and it can be enabled later based on the Community feedback.

 Following improvement are seen with Direct I/O enablement.
 # 32 cores >= ~15%
 # 64 cores >= ~80%

 Also, another observation would like to share here. Reading Commitlog files with Direct I/O might help in reducing node bring-up time after the node crash.

 Tested with commit ID: 91f6a9aca8d3c22a03e68aa901a0b154d960ab07

 The attached patch enables Direct I/O feature for Commitlog files. Please check and share your feedback."
CASSANDRA-18463,CEP-21 Reinstate client notifications for joining/leaving/moving nodes,"This functionality was disabled by some of the recent changes to {{StorageService}}, causing a number of test failures. 
"
CASSANDRA-18462,CEP-21 Fix tools tests,"The {{LocalLog}} instance created for use with offline tools should not be initialised with the standard set of listeners.
"
CASSANDRA-18461,CEP-21 Avoid NPE when getting dc/rack for not yet registered endpoints,"If a snitch is asked for location info for a node not yet added to the cluster, it should not NPE. In future, it may be desirable to fine tune the actual behaviour, but for now returning a default would be an improvement.
"
CASSANDRA-18460,CEP-21 Ensure that ClusterMetadata::forceEpoch keeps component epochs consistent,"{{forceEpoch}} is used to make the application of multiple transformations in series appear as a single atomic update. It's primary use is in {{UnsafeJoin}} (i.e. join without bootstrap) to apply the usual sequence of start/mid/finish join in a single transformation. In such cases, we must ensure that no component of {{ClusterMetadata}} which maintains its own last-modified epoch, ends up with an epoch greater than the one of the enclosing {{ClusterMetadata}}. 
"
CASSANDRA-18459,CEP-21 Rewrite o.a.c.distributed.test.SchemaTest,"Several of the premises that this test is based on are no longer valid. Rewrite it so that instead of executing schema changes node-locally to prevent them propagating, we can have the non-cms node pause before enacting a schema change to enable us to verify that the expected exceptions are thrown. Also, local schema reset and pulling during startup are not relevant in TCM, so we can simplify the tests to ensure that a down node learns of any missed updates when it restarts."
CASSANDRA-18458,"CEP-21 During startup, don't open SSTables until local metadata log replay is complete","Eagerly opening SSTables when their {{ColumnFamilyStore}} is first instantiated presents problems when replaying the local metadata log during startup. Schema modifications which _had_ been applied by the time an SSTable was written may not have been replayed yet, causing serialization errors. To address this, we can defer opening SSTables until after the local log replay is complete.
"
CASSANDRA-18457,CEP-21 Ensure that SchemaTransformation impls correctly set TableMetadata epoch,Many existing {{SchemaTransformation}} implementations (not to mention as yet unimplemented ones) have the potential to modify {{TableMetadata}} and it is brittle to require each of them to take care of updating the metadata epoch. We should have {{ClusterMetadata.Transformer}} or the {{AlterSchema}} transformation itself handle this automatically.
CASSANDRA-18456,CEP-21 During startup request replay from CMS asynchronously,"During the startup sequence, nodes first replay any locally persisted metadata changes then request any newer, unseen updates from the CMS. This second part should be both async and optional, meaning a CMS failure or partition shouldn't prevent nodes from starting up. "
CASSANDRA-18455,Test failure: test_putget – dtest.putget_test.TestPutGet,"Seen on current trunk, here:

[https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/2416/tests]
{code:java}
Error failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node3] 'ERROR [PendingRangeCalculator:1] 2023-04-08 00:54:06,468 JVMStabilityInspector.java:68 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]\njava.lang.AssertionError: Unknown keyspace system_traces\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:325)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:163)\n\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\n\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:163)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:152)\n\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\n\tat org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)']"" Stacktrace Unexpected error found in node logs (see stdout for full details). Errors: [[node3] 'ERROR [PendingRangeCalculator:1] 2023-04-08 00:54:06,468 JVMStabilityInspector.java:68 - Exception in thread Thread[PendingRangeCalculator:1,5,PendingRangeCalculator]\njava.lang.AssertionError: Unknown keyspace system_traces\n\tat org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:325)\n\tat org.apache.cassandra.db.Keyspace.lambda$open$0(Keyspace.java:163)\n\tat org.apache.cassandra.utils.concurrent.LoadingMap.blockingLoadIfAbsent(LoadingMap.java:105)\n\tat org.apache.cassandra.schema.Schema.maybeAddKeyspaceInstance(Schema.java:251)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:163)\n\tat org.apache.cassandra.db.Keyspace.open(Keyspace.java:152)\n\tat org.apache.cassandra.service.PendingRangeCalculatorService.lambda$new$1(PendingRangeCalculatorService.java:58)\n\tat org.apache.cassandra.concurrent.SingleThreadExecutorPlus$AtLeastOnce.run(SingleThreadExecutorPlus.java:60)\n\tat org.apache.cassandra.concurrent.ExecutionFailure$1.run(ExecutionFailure.java:133)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.lang.Thread.run(Thread.java:750)']{code}"
CASSANDRA-18453,Use WithProperties to ensure that system properties are handled,"The {{WithProperties}} is used to handle system properties to set and reset values during the test run, instead of try-catch it uses the try-with-resource approach which facilitates test development.

We need to replace all the try-catch clauses that work with system properties with {{WithProperties}} and try-with-resource for all the similar cases and where it is technically possible.

Example:
{code:java}
        try
        {
            COMMITLOG_IGNORE_REPLAY_ERRORS.setBoolean(true);
            testRecoveryWithGarbageLog();
        }
        finally
        {
            COMMITLOG_IGNORE_REPLAY_ERRORS.clearValue();
        }
{code}
Can be replaced with:
{code:java}
        try (WithProperties = new WithProperties().with(COMMITLOG_IGNORE_REPLAY_ERRORS, ""true""))
        {
            testRecoveryWithGarbageLog();
        }
{code}"
CASSANDRA-18452,BLOG - Announcing Monthly Apache Cassandra Town Halls,"This ticket is to capture the work associated with publishing the April 2023 blog ""Announcing Monthly Apache Cassandra Town Halls""

If this blog cannot be published by the *April 14, 2022 publish date*, please contact me, suggest changes, or correct the date when possible in the pull request for the appropriate time that the blog will go live (on both the blog.adoc and the blog post's file)."
CASSANDRA-18451,CEP-15: (C*) Improve the chaos generation for Burn Tests: slow/flakey connections and dropped messages,"Burn test should be enhanced to add the following:

1) message dropping from one node to another (this is different than the current partition set logic in accord.impl.basic.Cluster#partitionSet)
2) for messages with callbacks, trigger failure case
3) redundant message delivery

Related work:
* Simulator’s org.apache.cassandra.simulator.systems.SimulatedAction#applyToMessage
- Figures out what delivery action to perform via org.apache.cassandra.simulator.FutureActionScheduler#shouldDeliver
-- timeout if dropPartition[from] != dropPartition[to] // either to/from is in drop partition, but not both
-- config asked to override and deliver
-- 50/50 chance to deliver, after that 50/50 to deliver w/ timeout, after that cause a failure
- in C* failure is an enum with Timeout and Unknown
- knows the schedule time and the message expire time, and can promote a DELIVER event to DELIVER_AND_TIMEOUT
- triggers the timeout"
CASSANDRA-18449,"Integer(int), Long(long), Float(double) were deprecated in JDK9","Now when we are moving with Cassandra 5.0 to 11+17, it is good to declutter at least a bit our build log which  contains 76 deprecation warnings.

Half of them are for deprecation of Integer(int), Long(long) and Float(double).

Oracle advises to use factory methods which are likely to yield significantly better space and time performance too. The factory methods are also marked with 
@IntrinsicCandidate
 "
CASSANDRA-18448,"Missing ""SSTable Count"" metric  when using nodetool with ""--format"" option","Hi, 

I'm using ""nodetool cfstats --format json"" to gather some metrics/infomation about our tables. 
I noticed that the ""SSTable Count"" is missing when using ""–format"" option. 

If I don't use ""–format""  option, I can set ""SSTable Count"" in the output. 

*Output of ""nodetool cfstats --format json | jq"":* 
{code:java}
{  ""total_number_of_tables"": 38,  ""stress_test"": {    ""write_latency_ms"": 0.8536725334338424,    ""tables"": {      ""res1"": {        ""average_tombstones_per_slice_last_five_minutes"": null,        ""bloom_filter_off_heap_memory_used"": ""159256"",        ""memtable_switch_count"": 754,        ""maximum_tombstones_per_slice_last_five_minutes"": 0,        ""memtable_cell_count"": 0,        ""memtable_data_size"": ""0"",        ""average_live_cells_per_slice_last_five_minutes"": null,        ""local_read_latency_ms"": ""NaN"",        ""local_write_latency_ms"": ""NaN"",        ""pending_flushes"": 0,        ""compacted_partition_minimum_bytes"": 785940,        ""local_read_count"": 0,        ""sstable_compression_ratio"": 0.6294161376582798,        ""dropped_mutations"": ""52751"",        ""bloom_filter_false_positives"": 0,        ""off_heap_memory_used_total"": ""58842196"",        ""memtable_off_heap_memory_used"": ""0"",        ""index_summary_off_heap_memory_used"": ""18972"",        ""bloom_filter_space_used"": ""159408"",        ""sstables_in_each_level"": [],        ""compacted_partition_maximum_bytes"": 4055269,        ""space_used_total"": ""302694398635"",        ""local_write_count"": 297111,        ""compression_metadata_off_heap_memory_used"": ""58663968"",        ""number_of_partitions_estimate"": 99614,        ""maximum_live_cells_per_slice_last_five_minutes"": 0,        ""space_used_live"": ""302694398635"",        ""compacted_partition_mean_bytes"": 3827283,        ""bloom_filter_false_ratio"": ""0.00000"",        ""percent_repaired"": 0,        ""space_used_by_snapshots_total"": ""0""      }    },    ""read_latency_ms"": null,    ""pending_flushes"": 0,    ""write_count"": 594308,    ""read_latency"": null,    ""read_count"": 0  }}
 {code}
*Output of ""nodetool cfstats"":* 
{code:java}
----------------
Keyspace : stress_test
        Read Count: 0
        Read Latency: NaN ms
        Write Count: 594308
        Write Latency: 0.8536725334338424 ms
        Pending Flushes: 0
                Table: res1
                SSTable count: 19                
                Space used (live): 302694398635
                Space used (total): 302694398635
                Space used by snapshots (total): 0
                Off heap memory used (total): 58842196
                SSTable Compression Ratio: 0.6294161376582798
                Number of partitions (estimate): 99614
                Memtable cell count: 0
                Memtable data size: 0
                Memtable off heap memory used: 0
                Memtable switch count: 754
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 297111
                Local write latency: NaN ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 159408
                Bloom filter off heap memory used: 159256
                Index summary off heap memory used: 18972
                Compression metadata off heap memory used: 58663968
                Compacted partition minimum bytes: 785940
                Compacted partition maximum bytes: 4055269
                Compacted partition mean bytes: 3827283
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 52751*  
----------------
 {code}
 "
CASSANDRA-18445,Fix flaky paxosRepairHistoryIsntUpdatedInForcedRepair,"According to Butler paxosRepairHistoryIsntUpdatedInForcedRepair is flaky in the past week.

[https://butler.cassandra.apache.org/#/ci/upstream/compare/Cassandra-4.1/cassandra-4.1]

Seen also in CircleCI:

[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2332/workflows/ee3072a7-6fe3-4f53-b13a-41945ab760ed/jobs/21155/tests]
{code:java}
junit.framework.AssertionFailedError: Repair failed with errors: [Repair session b99f5e60-d96f-11ed-9541-834fbeef5777 for range [(9223372036854775745,-8839064868652493483], (-8070450532247928835,-7686143364045646511], (-4611686018427387919,-4227378850225105595], (-3458764513820540947,-3074457345618258623], (-31,384307168202282293], (1152921504606846941,1537228672809129265], (4611686018427387857,4995993186629670181], (5764607523034234829,6148914691236517153], (-8839064868652493483,-8454757700450211159], (384307168202282293,768614336404564617], (-7686143364045646511,-7301836195843364187], (1537228672809129265,1921535841011411589], (-6917529027641081863,-6533221859438799539], (-5764607523034234891,-5380300354831952567], (-2305843009213693975,-1921535841011411651], (-1152921504606847003,-768614336404564679], (2305843009213693913,2690150177415976237], (3458764513820540885,3843071682022823209], (6917529027641081801,7301836195843364125], (8070450532247928773,8454757700450211097], (-1921535841011411651,-1537228672809129327], (7301836195843364125,7686143364045646449], (-7301836195843364187,-6917529027641081863], (-384307168202282355,-31], (1921535841011411589,2305843009213693913], (8839064868652493421,9223372036854775745], (-5380300354831952567,-4995993186629670243], (3843071682022823209,4227378850225105533], (-8454757700450211159,-8070450532247928835], (-6148914691236517215,-5764607523034234891], (768614336404564617,1152921504606846941], (3074457345618258561,3458764513820540885], (-4227378850225105595,-3843071682022823271], (4995993186629670181,5380300354831952505], (-3074457345618258623,-2690150177415976299], (6148914691236517153,6533221859438799477], (-6533221859438799539,-6148914691236517215], (2690150177415976237,3074457345618258561], (-4995993186629670243,-4611686018427387919], (-2690150177415976299,-2305843009213693975], (4227378850225105533,4611686018427387857], (6533221859438799477,6917529027641081801], (-768614336404564679,-384307168202282355], (8454757700450211097,8839064868652493421], (-3843071682022823271,-3458764513820540947], (-1537228672809129327,-1152921504606847003], (5380300354831952505,5764607523034234829], (7686143364045646449,8070450532247928773]] failed with error UNKNOWN failure response from /127.0.0.3:7012, Repair command #1 finished with error] at org.apache.cassandra.distributed.test.PaxosRepair2Test.lambda$repair$54f7d7c2$1(PaxosRepair2Test.java:186) at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:81) at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:47) at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:57) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Thread.java:829){code}"
CASSANDRA-18443,Deadlock updating sstable metadata if disk boundaries need reloading,"{{CompactionStrategyManager.handleNotification}} holds the read lock while processing notifications. When handling metadata changed notifications, an extra call is made to maybeReloadDiskBoundaries which tries to grab the write lock and deadlocks the thread.

Partial stacktrace
{code}
        at jdk.internal.misc.Unsafe.park(java.base@11.0.16/Native Method)
        - parking to wait for  <0x00000005cc000078> (a java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire
        at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.maybeReloadDiskBoundaries(CompactionStrategyManager.java:495)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getCompactionStrategyFor(CompactionStrategyManager.java:343)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleMetadataChangedNotification(CompactionStrategyManager.java:796)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:838)
        at org.apache.cassandra.db.lifecycle.Tracker.notifySSTableMetadataChanged(Tracker.java:482)
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:838)
{code}

Deadlocking with the read lock held blocks the SlabpoolCleaner while notifying ColumnFamilyStore so memtables are prevented from being flushed and recycled, causing any thread applying a mutation to the database (at least GossipStage and MutationStage) to be considered down by peers and/or back up with pending requests.

All the cases investigated were during single sstable upleveling by {{org.apache.cassandra.db.compaction.SingleSSTableLCSTask}} added in CASSANDRA-12526.

Other less critical work was also affected, JMX calls to get estimated remaining compaction tasks, the index summary manager redistributing summaries, the StatusLogger trying to log dropped messages, and the ValidationManager.

Workaround is to reboot the affected host.

The fix is to just remove the redundant disk boundary reload check on that path.
"
CASSANDRA-18441,Improvements to SSTable format configuration,"CEP-17 and CASSANDRA-17056 abstracted some interfaces for SSTable format implementations and defined a method of plugging in specific configurations. 

This method is brittle and asks users to specify format identifiers whose configuration does not provide value but can be the source of conflicts and problems. On the other hand it makes important choices non-obvious, as the selection of format to write is given by the order of configured interfaces.

An improved specification mechanism needs to be put in place before Cassandra 5 is released."
CASSANDRA-18439,Revert some accesses to internals after CASSANDRA-18329 lands,"The current version of JAMM in Cassandra does not handle properly the module system.

To workaround some of the failures we see when starting Cassandra and running tests, a numerous _-  -add-opens-_ and  _add-exports_ were added temporarily until Jamm is fixed.  

Those need to be revised and potentially reverted.

[https://github.com/apache/cassandra/blob/trunk/conf/jvm17-server.options#L94-L107]

(potentially also in the jvm11-server.options file too)

[https://github.com/apache/cassandra/blob/trunk/conf/jvm17-clients.options#L55-L67]

(again, potentially also in jvm11-client.options file too)

CASSANDRA-18329 should cover jamm upgrade which will happen when new jamm version is released."
CASSANDRA-18438,Refactor cloud snitches to get rid of duplicate code,"Nowadays we have got about four public cloud platform snitchs ： EC2 snitch for aws, google cloud snitch for google cloud, alibaba cloud snitch for alibaba cloud and multi region snitch for ec2. And the common place for the first three is that we just need to query the zone center to get the ec2 / ecs id , so I think we can refactor the code , and  if some new public cloud platform want to add one more snitch for himself, there is no need to pull a pr for him and  configure some options in yaml is enough .
Besides it would be even better that we may reuse the multic region snitch for ec2 for other public cloud platform."
CASSANDRA-18437,Fix org.apache.cassandra.transport.MessagePayloadTest-.jdk17,"[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2182/workflows/e3dc630b-b7a4-4f5b-8f29-489bf43ad90f]
{code:java}
java.lang.RuntimeException: java.lang.NoSuchFieldException: modifiers at org.apache.cassandra.transport.MessagePayloadTest.resetCqlQueryHandlerField(MessagePayloadTest.java:98) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) Caused by: java.lang.NoSuchFieldException: modifiers at java.base/java.lang.Class.getDeclaredField(Class.java:2610) at org.apache.cassandra.transport.MessagePayloadTest.resetCqlQueryHandlerField(MessagePayloadTest.java:88){code}"
CASSANDRA-18436,Unit tests in org.apache.cassandra.cql3.EmptyValuesTest class occasionally failing with JDK17," 

All of them failed with the below stack trace for the same assertion failing:
{code:java}
junit.framework.AssertionFailedError: at org.apache.cassandra.cql3.EmptyValuesTest.verify(EmptyValuesTest.java:90) at org.apache.cassandra.cql3.EmptyValuesTest.verifyJsonInsert(EmptyValuesTest.java:112) at org.apache.cassandra.cql3.EmptyValuesTest.testEmptyDecimal(EmptyValuesTest.java:192) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}
 

Unfortunately I do not have a link to the CI run as this was seen last in private infra and not in CircleCI. Maybe we want to check with the multiplexer for flakiness. "
CASSANDRA-18431,Cassandra doesn't start on JDK17,"CASSANDRA-17199 added a new call to jamm measureDeep which hits some JDK internals and prevents us from starting Cassandra with JDK17. This will be solved with CASSANDRA-18329.

Until new version of jamm lands we can workaround the problem either by adding add-opens or by replacing temporarily

{code:java}
public static final long IPV6_SOCKET_ADDRESS_SIZE = ObjectSizes.measureDeep(new InetSocketAddress(getIpvAddress(16), 42));
{code}

with

{code:java}
public static final long IPV6_SOCKET_ADDRESS_SIZE = 168;
{code}

measured with JOL.

Then we can switch back to the current call [here|https://github.com/apache/cassandra/commit/4444721b6de555352bf0ac3ef7e36f94dc832f41#diff-1122d7d3efe9721af7244d373e66378f7e90cb05fd65859a52e8a3ea58a7c8f9R45] later."
CASSANDRA-18430,When decommissioning should set Severity to limit traffic,"When we are decommissioning we first set LEAVING, then LEFT, then disable networking; timeouts start to follow at this last stage. LEFT nodes should not be seen as part of the ring, but that may not be seen before the network is disabled.  To better mitigate timeouts we should set severity as part of decom during the LEAVING phase; by setting severity reads should deprioritize traffic to this node.

Remote DC writes do not leverage proximity or severity and instead use random for its select, writes may still timeout even though we know the node is leaving, and severity is set… to work in this model we should update remote DC writes to deprioritize nodes with severity set"
CASSANDRA-18429,Upgrade Zstd to 1.5.5,https://lists.apache.org/thread/9d0bgpycb3t666pqk4xz9fl5xzdzgo7k
CASSANDRA-18428,Implement/override equals and hashCode methods in the ServerEncryptionOptions class,"We have {{equals and hashCode}}  methods in [EncryptionOptions|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/config/EncryptionOptions.java#L551] object but not (overridden/extended) in [ServerEncryptionOptions.|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/config/EncryptionOptions.java#L600]

Code is using the EncryptionOptions as the key in the [ConcurrentHashMap|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/security/SSLFactory.java#L84] in the SSLFactory.java.  Hence technically we must have a equals/hashCode override in the ServerEncryptionOptions to account for fields that matter additionally (e.g. outbound_keystore/password). 

We discussed this over the [cassandra-dev slack channel|https://the-asf.slack.com/archives/CK23JSY2K/p1680118849081399] and it seems agreeable to make this change."
CASSANDRA-18427,Test failure: pdtest: dtest-novnode.ttl_test.TestDistributedTTL.test_ttl_is_respected_on_delayed_replication,"{{Failed 1 times in the last 30 runs. Flakiness: 3%, Stability: 96%}}

{code}
Error Message
AssertionError: Expected [[1, 1, None, None], [2, 2, None, None]] from SELECT * FROM ttl_table;, but got [[2, 2, None, None]]
Stacktrace
self = <ttl_test.TestDistributedTTL object at 0x7fce080d8c10>

    def test_ttl_is_respected_on_delayed_replication(self):
        """""" Test that ttl is respected on delayed replication """"""
        self.prepare()
        self.node2.stop()
        self.session1.execute(""""""
            INSERT INTO ttl_table (key, col1) VALUES (1, 1) USING TTL 5;
        """""")
        self.session1.execute(""""""
            INSERT INTO ttl_table (key, col1) VALUES (2, 2) USING TTL 1000;
        """""")
>       assert_all(
            self.session1,
            ""SELECT * FROM ttl_table;"",
            [[1, 1, None, None], [2, 2, None, None]]
        )

ttl_test.py:480: 
{code}

At face value this didn't look like a classic ""oops I timed out"", but now that I read that error message I'm not as sure. So I'd check that first and just close out if it's a timeout by another name until we get the infra stabilized."
CASSANDRA-18425,Test failure: utest: org.apache.cassandra.db.RepairedDataTombstonesTest.readTestPartitionTombstones-.jdk11,"{{Failed 1 times in the last 5 runs. Flakiness: 25%, Stability: 80%}}

{code}
Error Message
val=5
Stacktrace
junit.framework.AssertionFailedError: val=5
	at org.apache.cassandra.db.RepairedDataTombstonesTest.readTestPartitionTombstones(RepairedDataTombstonesTest.java:189)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}"
CASSANDRA-18423,Test failure: org.apache.cassandra.io.sstable.indexsummary.IndexSummaryTest.testLargeIndexSummary,"Failed 6 times in the last 6 runs. Flakiness: 0%, Stability: 0%

{noformat}
unit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.util.Vector.forEach(Vector.java:1394)
{noformat}

Note that only the 'default' version of this test is failing: https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/org.apache.cassandra.io.sstable.indexsummary/IndexSummaryTest/testLargeIndexSummary"
CASSANDRA-18422,"CEP-15 (Accord) Original and recover coordinators may hit a race condition with PreApply where reads and writes are interleaved, causing one of the coordinators to see the writes from the other","While verifying CASSANDRA-18364 I saw the following history violation in simulator

{code}
[junit-timeout] Testcase: simulationTest(org.apache.cassandra.simulator.test.ShortAccordSimulationTest)-.jdk1.8:        Caused an ERROR
[junit-timeout] Failed on seed 0xadaca81151490353
[junit-timeout] org.apache.cassandra.simulator.SimulationException: Failed on seed 0xadaca81151490353
[junit-timeout] Caused by: java.lang.AssertionError: History violations detected
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.logAndThrow(PaxosSimulation.java:315)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.isDone(PaxosSimulation.java:278)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation$2.hasNext(PaxosSimulation.java:249)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PaxosSimulation.run(PaxosSimulation.java:224)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AbstractPairOfSequencesPaxosSimulation.run(AbstractPairOfSequencesPaxosSimulation.java:297)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation.run(PairOfSequencesAccordSimulation.java:62)
[junit-timeout]         at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:374)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:39)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner$Run.run(AccordSimulationRunner.java:30)
[junit-timeout]         at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:355)
[junit-timeout]         at org.apache.cassandra.simulator.paxos.AccordSimulationRunner.main(AccordSimulationRunner.java:76)
[junit-timeout]         at org.apache.cassandra.simulator.test.ShortAccordSimulationTest.simulationTest(ShortAccordSimulationTest.java:32)
[junit-timeout]         Suppressed: org.apache.cassandra.simulator.paxos.HistoryViolation: Inconsistent sequences on 1: [2, 0, 1, 6, 8, 9, 13, 14, 16, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 43, 47, 48, 49, 54, 56, 57, 58, 60, 64, 68, 70, 71, 74, 76, 79, 80, 83, 85, 87, 87] vs [2, 0, 1, 6, 8, 9, 13, 14, 16, 19, 20, 22, 23, 25, 26, 28, 29, 31, 32, 34, 35, 37, 40, 43, 47, 48, 49, 54, 56, 57, 58, 60, 64, 68, 70, 71, 74, 76, 79, 80, 83, 85, 87]+90
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier$Register.updateSequence(StrictSerializabilityVerifier.java:607)
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier$Register.access$100(StrictSerializabilityVerifier.java:576)
[junit-timeout]                 at accord.verify.StrictSerializabilityVerifier.apply(StrictSerializabilityVerifier.java:825)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.lambda$close$0(StrictSerializabilityValidator.java:66)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.convertHistoryViolation(StrictSerializabilityValidator.java:89)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator.access$200(StrictSerializabilityValidator.java:27)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.StrictSerializabilityValidator$1.close(StrictSerializabilityValidator.java:66)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.LoggingHistoryValidator$1.close(LoggingHistoryValidator.java:63)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.verify(PairOfSequencesAccordSimulation.java:218)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:135)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PairOfSequencesAccordSimulation$ReadWriteOperation.accept(PairOfSequencesAccordSimulation.java:171)
[junit-timeout]                 at org.apache.cassandra.simulator.paxos.PaxosSimulation$Operation.accept(PaxosSimulation.java:83)
[junit-timeout]                 at org.apache.cassandra.simulator.systems.SimulatedActionCallable$1.run(SimulatedActionCallable.java:47)
[junit-timeout]                 at org.apache.cassandra.simulator.systems.InterceptingExecutor$InterceptingPooledExecutor$WaitingThread.lambda$new$1(InterceptingExecutor.java:317)
[junit-timeout]                 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
[junit-timeout]                 at java.lang.Thread.run(Thread.java:750)
{code}

Adding logging to track message passing, reads, and writes, I have the following ordering

{code}
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[isolatedExecutor:3]  node3 2023-04-03 12:54:30,200 send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,208 CS:[2] OP:0xea64a268 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,209 CS:[2] OP:0x9761fb36 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,210 CS:[2] OP:0xe62230f2 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_PREACCEPT_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_PREACCEPT_RSP))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,211 CS:[2] OP:0xc5563e5d send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,212 CS:[2] OP:0xc5563e5d send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,212 CS:[2] OP:0xc5563e5d send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,329 CS:[2] OP:0xa3e62850 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,334 CS:[2] OP:0xf8562cfb reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,338 CS:[2] OP:0xcfd2540f reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,338 CS:[2] OP:0xc4cf5af8 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_RECOVER_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_RECOVER_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,340 CS:[2] OP:0x9e4f00f0 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,343 CS:[2] OP:0xb60153ab reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,344 CS:[2] OP:0xac20a1d6 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.1:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,345 CS:[2] OP:0xa73f7484 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_GET_DEPS_REQ), (from:/127.0.0.2:7012, type:REQUEST_RESPONSE verb:ACCORD_GET_DEPS_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,347 CS:[2] OP:0xfc37fb1a send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,349 CS:[2] OP:0xff574276 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,349 CS:[2] OP:0xff574276 Performing read
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,351 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,351 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_READ_RSP))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 Performing coordinated write
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 send(/127.0.0.1:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,359 send(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,360 send(/127.0.0.3:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,363 CS:[2] OP:0x8bdb6795 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,363 CS:[2] OP:0x8bdb6795 Performing read
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,364 CS:[2] OP:0x92e94460 Performing write
[MutationStage:4]     node3 2023-04-03 12:54:30,364 Performing write: pre
[MutationStage:4]     node3 2023-04-03 12:54:30,365 Performing write: post
[ReadStage:1]         node3 2023-04-03 12:54:30,365 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,365 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,369 Performing read; post
[ReadStage:1]         node3 2023-04-03 12:54:30,369 Performing read; pre
[ReadStage:1]         node3 2023-04-03 12:54:30,369 reply(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_COMMIT_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_READ_RSP))
[CommandStore[2]:1]   node1 2023-04-03 12:54:30,370 CS:[2] OP:0xa59dc286 Performing write
[CommandStore[2]:1]   node2 2023-04-03 12:54:30,374 CS:[2] OP:0xab0f3ca4 Performing write
[MutationStage:1]     node2 2023-04-03 12:54:30,374 Performing write: pre
[MutationStage:1]     node2 2023-04-03 12:54:30,375 Performing write: post
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,376 Performing coordinated write
[MutationStage:3]     node1 2023-04-03 12:54:30,376 Performing write: pre
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.1:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.2:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,377 send(/127.0.0.3:7012, (from:/127.0.0.3:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ))
[CommandStore[2]:1]   node3 2023-04-03 12:54:30,382 reply(/127.0.0.2:7012, (from:/127.0.0.2:7012, type:IMMEDIATE verb:ACCORD_APPLY_REQ), (from:/127.0.0.3:7012, type:REQUEST_RESPONSE verb:ACCORD_APPLY_RSP))
[MutationStage:3]     node1 2023-04-03 12:54:30,382 Performing write: post
{code}

(The transaction has a returning select and an auto-read, which is why there are double logs for reads)

Here we see the following timing

{code}
T00 node3 starts txn
T01 node3 sends COMMIT
T02 node2 starts recover
T03 all nodes ack to the pending recover
T04 node2 sends COMMIT
T05 node3 performs reads needed for txn
T06 node3 sends read results to node2
T07 node2 performs write locally and send APPLY
T08 node3 performs write
T09 node3 performs reads needed for txn
T10 node3 send reads to node3
T11 node3 performs write and sends APPLY
T12 node3 ACKs APPLY to node2
{code}

Given the fact the simulator got a response back and didn’t get a preempt, this implies that the original coordinator was able to complete the full transaction without issues and reply back, but the reads/writes were interleaved between node3 and node2 causing the second write to observe the first write"
CASSANDRA-18419,"CEP-21 During multi step operations, defer token map update until completion of final step","Updating the token map, used by {{UniformRangePlacement}} to calculate placements, too early can cause stream plans to be calculated incorrectly. The eager update of the token map was intended to ensure that subsequent (and possibly concurrent) range movements would remain compatible with the operation's end state, but this is not necessary as the affected ranges are locked and intersecting operations are rejected."
CASSANDRA-18418,CEP-21 Dereference TableMetadata in simple partition builder,"In some places we retain static {{TableMetadata}} instances and use them to construct a {{PartitionUpdate}}.  For instance, TraceKeyspace contains Sessions & Events static members which are created at startup when the current epoch is Epoch.EMPTY. These are later used to construct mutations when tracing is enabled and when the mutations are serialised and sent between replica & coordinator the epoch comparisons in PartitionUpdate deserializer trigger an IncompatibleSchemaException. Ultimately we should not expose static {{TableMetadata}} instances in this way."
CASSANDRA-18417,CEP-21 Implement multi-dc placement simulator for NTS,"Both for simulations in {{MetadataChangeSimulationTest}} and for future tests using the general Simulator framework, we should support rack and DC aware placements.
"
CASSANDRA-18416,CEP-21 Ensure that global log replication factor is maintained after decommission,"If a CMS node is decommissioned, another node should automatically become a CMS member to take its place. If there are no non-CMS peers, the decommission should be rejected."
CASSANDRA-18415,CEP-21 Fix (re)building MVs,"This hasn't really been addressed in the TCM branch yet and is currently quite broken, as indicated by a number of dtest failures. 
 "
CASSANDRA-18414,CEP-21 Re-enable stdout/sterr redirection at startup,This was temporarily disabled during early TCM development
CASSANDRA-18413,CEP-21 Add request failure reason to indicate invalid routing of a read or write,"In the context of a read or write request, this would indicate that a node does not consider itself a replica of the specified token or range. This implies that one or both of the coordinator and replica has an outdated view of cluster metadata and may result in the operation failing if the specified consistency level can no longer be satisfied using the coordinator's initial replica plan.
 "
CASSANDRA-18412,CEP-21 Secondary indexes should not be rebuilt on restart,"When replaying the local metadata log at startup, a {{CREATE INDEX}} statement triggers an unnecessary rebuild."
CASSANDRA-18411,CEP-21 Improve support for start/end tokens in nodetool rebuild,"We should handle the case {{nodetool rebuild}} with {{-st}} and {{-et}} where the tokens don't align exactly with local ranges
"
CASSANDRA-18410,CEP-21 Fix nodetool ring and effective ownership,This is in need of updating to match the nodetool output in trunk
CASSANDRA-18409,CEP-21 Make StorageService treatment of bootstrapping nodes consistent with previous implementation,"Correctly return true from {{isBootstrapMode()}} if the node is bootstrapping. Also, {{getJoiningNodes()}} should exclude nodes which have registered, but are yet to start joining."
CASSANDRA-18408,CEP-21 Implement retries for log replay on CMS members,"This should enable {{AbstractLocalProcessor}} implementations to backoff and retry when executing distributed reads of the consistent log, in much the same way that {{RemoteProcessor}} does when making replay requests to CMS members."
CASSANDRA-18407,CEP-21 Remove paranoid check which fails due to pre-existing mismatch on replica vs query range,"If short read protection is activated, a replica plan is constructed to fetch additional partitions from a single replica in the original set. As additional partitions are requested, the data range of the query is reduced but the original \{{Replica}} instance is reused, resulting in a mismatch between the range of the query and that of the replica. This is not new to TCM and is present in trunk. It is called out in a comment, but thought to be harmless as the narrowed range should be a subset of the original data range which did match the replica's. Early TCM patches introduced an addition check here on the two ranges and so rejects SRP queries which trunk does not.  
 "
CASSANDRA-18406,CEP-21 Always use Paxos.v2 for global log reads/writes regardless of cluster configuration,"As there is no legacy use of the global log keyspace and it is purely a system keyspace, we should always use paxos V2 for SERIAL reads and writes there.
 "
CASSANDRA-18405,CEP-21 Various fixes to schema related in-jvm dtests,"* JVMDTestTest
    schema change with stopped node, must not be CMS
* SSTableIdGenerationTest
    This test revolves around creating sstables with sequential ids, then
    restarting the node after enabling sstable uuids, creating more sstables
    and then counting the number of each type of sstable. It seems to rely
    on the local host id being unavailable when the node restarts, which
    causes the mutations in the commitlog, which have already been flushed
    to be replayed as the null local host id doesn't match the actual one in
    the sstable metadata. In turn this leads to an extra sstable being
    created after the restart, duplicating (some of) what was in the earlier
    sstables.
* MigrationCoordinatorTest
    deleted"
CASSANDRA-18404,CEP-21 Improve seedlist inspection at startup,"The intention is to only force a new node directly into becoming the first CMS node in local, single node installs or CCM cluster. Otherwise, we should always attempt to discover peers to determine an existing CMS or to initiate a new election by proposing ourself.
 "
CASSANDRA-18403,CEP-21 Always populate local gossip state at startup,"Properly initialise the gossip state for the local node during startup
 "
CASSANDRA-18402,CEP-21 Add debounce to log replay,"To avoid overload we should debounce log replay requests between non-CMS nodes and CMS members. If a non-CMS node already has a replay request inflight, any subsequent requests should wait for the inflight response and return that. As soon as a response has been received, that request should be considered stale and a new one initiated.
 "
CASSANDRA-18401,Investigate preloading ccm repositories in the docker image,"In CASSANDRA-18391 it was discovered that to skip some upgrade tests, the ccm repository first needed to be populated with older versions.  While that case was solved, it may still be beneficial to preload the ccm repositories in the docker image so they don't need to be fetched at all."
CASSANDRA-18400,Nodetool info should report on the new networking cache,"CASSANDRA-15229 separated the chunk and network cache, creating a new network_cache_size parameter.

However, *nodetool info* does not report the in-use size of this cache or a hit ratio as it does for key, row, counter and chunk cache.  
{quote}Exceptions : 4

Key Cache : entries 2852, size 297.59 KiB, capacity 100 MiB, 2406561 hits, 2409424 requests, 0.999 recent hit rate, 14400 save period in seconds

Row Cache : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds

Counter Cache : entries 0, size 0 bytes, capacity 50 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds

Chunk Cache : entries 1118, size 55.02 MiB, capacity 992 MiB, 4695794 misses, 7179421 requests, 0.346 recent hit rate, 24.145 microseconds miss latency

Percent Repaired : 0.0%
{quote}
However, when its full, it will be logged:
{quote}[INFO ] [epollEventLoopGroup-5-12] cluster_id=1 ip_address=127.1.1.1  NoSpamLogger.java:92 - Maximum memory usage reached (128.000MiB), cannot allocate chunk of 8.000MiB
{quote}
It should report a line similar to the above:
{quote}Network Cache : entries ?, size ? MiB, capacity ? MiB, ? misses, ? requests, ? recent hit rate
{quote}
Also, not sure why the above show NaN for row and counter cache, is there is a divide by zero error when the entries are zero?"
CASSANDRA-18398,CEP-25: Trie-indexed SSTable format,"Implementation of Big Trie-Indexed (BTI) SSTable format, per [CEP-25|https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-25%3A+Trie-indexed+SSTable+format]."
CASSANDRA-18397,CEP-26: Unified Compaction Strategy,"Implementation of Unified Compaction Strategy per [CEP-26|https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-26%3A+Unified+Compaction+Strategy].

Further documentation of the most current state of the solution can be found in [the included markdown documentation|https://github.com/blambov/cassandra/blob/CASSANDRA-18397/src/java/org/apache/cassandra/db/compaction/UnifiedCompactionStrategy.md]."
CASSANDRA-18396,Dtests marked with @ported_to_in_jvm can be skipped since 4.1,"During the CASSANDRA-15536 epic we ported multiple Python dtests to in-JVM dtests.

The ported Python dtests are still present but marked with a new {{@ported_to_in_jvm}} annotation. JVM dtests didn't support vnodes at that time, so when a Python dtest is marked with that annotation it's only run for vnodes config, whereas it's skipped if vnodes are off.

However, we have had support for vnodes on JVM dtests since 4.1. Thus, I think we should modify the {{@ported_to_in_jvm}} annotation to also skip configs with vnodes if all the nodes are in 4.1 or later."
CASSANDRA-18395,Rename internal state() method in AbstractFuture to not conflict with Java 19 changes,"From Java 19 we have new method Future.state and it conflicts with our private method. Fix bellow. 

[https://download.java.net/java/early_access/jdk20/docs/api/java.base/java/util/concurrent/Future.html#state()]

 
{code:java}
+++ b/src/java/org/apache/cassandra/utils/concurrent/AbstractFuture.java
@@ -494,11 +494,11 @@ public abstract class AbstractFuture<V> implements Future<V>
     public String toString()
     {
         String description = description();
-        String state = state();
+        String state = stateInfo();
         return description == null ? state : (state + ' ' + description);
     }
 
-    private String state()
+    private String stateInfo()
     {
         Object result = this.result;
         if (isSuccess(result)){code}"
CASSANDRA-18394,Add nodetool  to show top n read/write table,add nodetool to show the read/write top n table name
CASSANDRA-18393,Flaky test: org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.testConditionalUpdate[1: clusterMinVersion=3.11]-compression.jdk1.8 on trunk,"Failed 1 times in the last 1 runs. Flakiness: 0%, Stability: 0%
{code}
Error Message
5.0.0-SNAPSHOT boolean:false

Stacktrace
junit.framework.AssertionFailedError: 5.0.0-SNAPSHOT boolean:false
	at org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.lambda$data$1(InsertUpdateIfConditionTest.java:70)
	at org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.beforeSetup(InsertUpdateIfConditionTest.java:95)
	at org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.before(InsertUpdateIfConditionTest.java:89)
{code}"
CASSANDRA-18392,flaky test org.apache.cassandra.net.ConnectionTest.testMessageDeliveryOnReconnect-compression.jdk1.8 on trunk,"Failed 3 times in the last 5 runs. Flakiness: 100%, Stability: 40%

expected:<1> but was:<0>

Stack:
{code}
junit.framework.AssertionFailedError: expected:<1> but was:<0>
	at org.apache.cassandra.net.ConnectionTest.lambda$testMessageDeliveryOnReconnect$45(ConnectionTest.java:772)
	at org.apache.cassandra.net.ConnectionTest.doTestManual(ConnectionTest.java:265)
	at org.apache.cassandra.net.ConnectionTest.testManual(ConnectionTest.java:238)
	at org.apache.cassandra.net.ConnectionTest.testMessageDeliveryOnReconnect(ConnectionTest.java:751)
{code}"
CASSANDRA-18391,consistent timeout: dtest-upgrade.upgrade_tests.cql_tests.cls.test_cql3_non_compound_range_tombstones on trunk,"Failed 30 times in the last 30 runs. Flakiness: 0%, Stability: 0%

link: https://ci-cassandra.apache.org/job/Cassandra-trunk/1511/testReport/dtest-upgrade.upgrade_tests.cql_tests/cls/test_cql3_non_compound_range_tombstones/
Error message: failed on setup with ""Failed: Timeout >900.0s"""
CASSANDRA-18389,jackson-core-2.13.2.jar vulnerability: CVE-2022-45688,This is currently failing in the OWASP scan.
CASSANDRA-18378,CEP-15 (Accord) accord.messages.Defer rejects Recurrent retry of Commit,"{code}
java.lang.IllegalStateException: Recurrent retry of Commit{…}
	at accord.messages.Defer.add(Defer.java:63)
	at accord.messages.Commit.apply(Commit.java:167)
	at accord.messages.Commit.apply(Commit.java:42)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:321)
	at org.apache.cassandra.service.accord.async.AsyncOperation$ForFunction.apply(AsyncOperation.java:308)
	at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:226)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:268)
{code}"
CASSANDRA-18377,CEP-15 (Accord) AsyncOperation can not fail as it has already reached FINISHED,"{code}
java.lang.IllegalArgumentException: Unexpected state FINISHED
	at accord.utils.Invariants.illegalArgument(Invariants.java:54)
	at accord.utils.Invariants.checkArgument(Invariants.java:202)
	at org.apache.cassandra.service.accord.async.AsyncOperation.fail(AsyncOperation.java:182)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:273)
{code}

And

{code}
java.lang.IllegalArgumentException: Unexpected state FINISHED
	at accord.utils.Invariants.illegalArgument(Invariants.java:54)
	at accord.utils.Invariants.checkArgument(Invariants.java:202)
	at org.apache.cassandra.service.accord.async.AsyncOperation.fail(AsyncOperation.java:182)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:273)
	at org.apache.cassandra.service.accord.async.AsyncOperation.callback(AsyncOperation.java:157)
	at accord.utils.async.AsyncCallbacks.lambda$null$0(AsyncCallbacks.java:31)
{code}"
CASSANDRA-18375,CEP-15 (Accord) Expected reply message with verb ACCORD_INFORM_OF_TXNID_RSP but got ACCORD_SIMPLE_RSP,"{code}
java.lang.IllegalArgumentException: Expected reply message with verb ACCORD_INFORM_OF_TXNID_RSP but got ACCORD_SIMPLE_RSP
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:440)
	at org.apache.cassandra.service.accord.AccordMessageSink.reply(AccordMessageSink.java:125)
	at accord.local.Node.reply(Node.java:361)
	at accord.messages.AbstractEpochRequest.accept(AbstractEpochRequest.java:63)
	at accord.messages.InformOfTxnId.accept(InformOfTxnId.java:66)
	at accord.messages.InformOfTxnId.accept(InformOfTxnId.java:31)
	at org.apache.cassandra.service.accord.async.AsyncOperation.finish(AsyncOperation.java:165)
	at org.apache.cassandra.service.accord.async.AsyncOperation.finish(AsyncOperation.java:176)
	at org.apache.cassandra.service.accord.async.AsyncOperation.runInternal(AsyncOperation.java:247)
	at org.apache.cassandra.service.accord.async.AsyncOperation.run(AsyncOperation.java:268)
	at org.apache.cassandra.service.accord.async.AsyncOperation.callback(AsyncOperation.java:157)
	at accord.utils.async.AsyncCallbacks.lambda$null$0(AsyncCallbacks.java:31)
{code}"
CASSANDRA-18374,Create a guardrail to prevent writing of timestamps in the distant future,"We ran into an issue where data inserted was USING TIMESTAMP that was using nanoseconds which cassandra was converting to something like this:

```

{{""tstamp"" : ""+55089-04-29T09:06:44.854775807Z""}}

{{```}}

Which means we were not able to delete or update the row. We had to write a delete with a timestamp 1 second ahead of the above timestamp and then run a garbagecollect to remove the data."
CASSANDRA-18373,Node Draining Should Abort All Current SSTables Imports,"SS tables imports that will end up being ignored due to the node draining should fail instead of succeeding.
Each active SS tables import should periodically check for the node status and see whether or not it is `DRAINING`.
In case the node starts draining the import should abort immediately by throwing an `InterruptedException`."
CASSANDRA-18372,A link to the pull request can be attached to the JIRA issue as soon as the PR is raised,"According to the ASF recommendations [1] a link to a pull request might be automatically attached to the corresponding JIRA issue if the pull request matches the following title: ""CASSANDRA-XXXX Something to improve"".

To achieve this, the {{.asf.yml}} must be updated as follows:

{code}
notifications:
  ...
  jira_options: link worklog
{code}

[1] https://cwiki.apache.org/confluence/display/INFRA/Git+-+.asf.yaml+features#Git.asf.yamlfeatures-Jiranotificationoptions"
CASSANDRA-18371,Snapshots with dots in their name are not returned in listsnapshots,
CASSANDRA-18370,BulkLoader tool initializes schema unnecessarily via streaming - 4.0,"Similar to CASSANDRA-17740, {{BulkLoader}} initializes the schema/system keyspace, which caused the issue in bulk loader tool. See more details in CASSANDRA-17740. Please help fix the issue in 4.0 branch as well. Thanks."
CASSANDRA-18369,Importing cassandra driver causes modern pytest to fail in certain circumstances,"When collecting on running test without exclusion on Pytest test name, importing cassandra driver causes a very strangely looking error:

 
{quote}# from cassandra.cluster import Cluster
# cassandra/cluster.py:48: in init cassandra.cluster
# ???
# cassandra/connection.py:40: in init cassandra.connection
# ???
# cassandra/protocol.py:698: in genexpr
# ???
# cassandra/protocol.py:698: in genexpr
# ???
# E KeyError: '@py_builtins'{quote}
This happened for Apache Airflow when we wanted to enable pytest collection to include all files, rather than only ""test_*.py"" files, because we found that there were at least few modules in airlfow that did not start with ""test"". 

It turned out that the culprit was Assert Rewrite done by pytest that tried to rewrite ""type_codes.py"" module of Cassandra.

One of the ways to avoid the problem is to disable assert rewrite by ""–assert=plain"".  However then asserts are not as useful. Better solution (PR is coming) is to add PYTEST_DONT_REWRITE in the docstring:

https://docs.pytest.org/en/latest/how-to/assert.html#disabling-assert-rewriting

For now we are mitigating that issue in Airflow by manually patching cassandra code, however better solution will be to add the PYTEST_DONT_REWRITE directive directly in Cassandra's Python driver's repository.

Fix is coming."
CASSANDRA-18368,Redhat 40x repo signature is not valid,"Looks like part of the packaging upload troubles we had with the 4.0.8 release:

{noformat}
gpg --verify repomd.xml.asc repomd.xml
gpg: Signature made Wed 19 Oct 2022 20:19:43 AEDT
gpg:                using RSA key A4C465FEA0C552561A392A61E91335D77E3E87CB
gpg: BAD signature from ""Michael Semb Wever <mick@thelastpickle.com>"" [unknown]
{noformat}"
CASSANDRA-18366,Test failure: org.apache.cassandra.distributed.test.FailingRepairTest - testFailingMessage[VALIDATION_REQ/parallel/true],First seen [here|https://app.circleci.com/pipelines/github/driftx/cassandra/928/workflows/f4e93a72-d4aa-47a2-996f-aa3fb018d848/jobs/16206] this test times out for me consistently on both j8 and j11 where 4.1 and trunk do not.
CASSANDRA-18364,CEP-15: (C*) Accord message processing should avoid being passed on to a Stage and run directly in the messageing handler,Accord message processing should avoid being passed on to a Stage and run directly in the messageing handler.  This logic should validate that all messages are non-blocking and async handle replies.
CASSANDRA-18363,Test failure: cqlsh_tests.test_cqlsh.TestCqlsh.test_list_queries,Looks like this needs to be updated for DDM: https://app.circleci.com/pipelines/github/driftx/cassandra/930/workflows/0dc6fa5b-65eb-4ea8-9238-2d78b889d9bc/jobs/16238/tests
CASSANDRA-18361,Test Failure: secondary_indexes_test.py::TestSecondaryIndexes::test_failing_manual_rebuild_index,"The Python dtest {{secondary_indexes_test.py::TestSecondaryIndexes::test_failing_manual_rebuild_index}} is flaky, at least for trunk:

* https://butler.cassandra.apache.org/#/ci/upstream/workflow/Cassandra-trunk/failure/secondary_indexes_test/TestSecondaryIndexes/test_failing_manual_rebuild_index
* https://ci-cassandra.apache.org/job/Cassandra-trunk/1501/testReport/dtest.secondary_indexes_test/TestSecondaryIndexes/test_failing_manual_rebuild_index/

{code}
Error Message
failed on teardown with ""Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [Reference-Reaper] 2023-03-23 00:23:43,597 Ref.java:237 - LEAK DETECTED: a reference (class org.apache.cassandra.io.util.FileHandle$Cleanup@967019010:/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-hgjoy8rq/test/node1/data0/k/t-b7dae870c91011eda58f05bc40bfcaa1/nc-1-big-Index.db) to class org.apache.cassandra.io.util.FileHandle$Cleanup@967019010:/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-hgjoy8rq/test/node1/data0/k/t-b7dae870c91011eda58f05bc40bfcaa1/nc-1-big-Index.db was not released before the reference was garbage collected']""
Stacktrace
Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [Reference-Reaper] 2023-03-23 00:23:43,597 Ref.java:237 - LEAK DETECTED: a reference (class org.apache.cassandra.io.util.FileHandle$Cleanup@967019010:/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-hgjoy8rq/test/node1/data0/k/t-b7dae870c91011eda58f05bc40bfcaa1/nc-1-big-Index.db) to class org.apache.cassandra.io.util.FileHandle$Cleanup@967019010:/home/cassandra/cassandra/cassandra-dtest/tmp/dtest-hgjoy8rq/test/node1/data0/k/t-b7dae870c91011eda58f05bc40bfcaa1/nc-1-big-Index.db was not released before the reference was garbage collected']
{code}
The failure can be reproduced in CircleCI:
* https://app.circleci.com/pipelines/github/adelapena/cassandra/2732/workflows/829434ab-2d1a-4e1c-8c7f-42449fcfda22

The CircleCI config I used to reproduce the test failure can be generated with:
{code}
.circleci/generate.sh -p \
  -e REPEATED_DTESTS_COUNT=200 \
  -e REPEATED_DTESTS=secondary_indexes_test.py::TestSecondaryIndexes::test_failing_manual_rebuild_index
{code}"
CASSANDRA-18359,NullPointerException on SnapshotLoader.loadSnapshots,"Node startup fail with on 4.1.1:

{noformat}
INFO [main] 2023-03-23 18:13:13,585 MigrationCoordinator.java:257 - Starting migration coordinator and scheduling pulling schema versions every PT1M
ERROR [main] 2023-03-23 18:13:13,592 CassandraDaemon.java:898 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.service.snapshot.SnapshotLoader$Visitor.preVisitDirectory(SnapshotLoader.java:106)
	at org.apache.cassandra.service.snapshot.SnapshotLoader$Visitor.preVisitDirectory(SnapshotLoader.java:77)
	at java.base/java.nio.file.Files.walkFileTree(Files.java:2732)
	at org.apache.cassandra.service.snapshot.SnapshotLoader.loadSnapshots(SnapshotLoader.java:162)
	at org.apache.cassandra.service.snapshot.SnapshotManager.loadSnapshots(SnapshotManager.java:114)
	at org.apache.cassandra.service.snapshot.SnapshotManager.start(SnapshotManager.java:88)
	at org.apache.cassandra.service.StorageService.startSnapshotManager(StorageService.java:1050)
	at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:1043)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:842)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:775)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:425)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:752)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:876)
{noformat}"
CASSANDRA-18356,Cassandra Debian Repository is not available and will be redirected to landing.jfrog.com,"Hi,
Debian Repo ist not available and redirected to landing.jfrog.com

Time flies when you’re having fun.
Your 14-day trial may be over, but you have options!

Could you pls fix that?

Thanks in advance
Zoltan"
CASSANDRA-18355,CEP-15: Transaction Result Serialization Efficiency,"There are two things we probably don’t need to serialize and write to the Accord state tables:
 
1.) Internal/external read responses
2.) The full result of the transaction"
CASSANDRA-18354,Remove obsolete 'six' package reintroduced by a merge,"The 4.1.x and trunk commits for CASSANDRA-18088 inadvertently re-introduced 'six' and obsolete python 2.7 conditional checks and should be removed.

I.e., SaferScanner = Py36SaferScanner if six.PY3 else Py2SaferScanner – but there is no Py2SaferScanner anymore."
CASSANDRA-18353,"Cqlsh command ""COPY … TO STDOUT"" fails with ""… object is not callable""","Since 4.1.0, cqlsh fails on COPY commands with standard output as target. 

Steps to reproduce:
{noformat}
$> docker run -d --name cassandra --rm cassandra:4.1.1
$> docker exec cassandra cqlsh -e 'COPY system.local(cluster_name) TO STDOUT'
  <stdin>:1:'NoneType' object is not callable
{noformat}

Possibly a regression introduced by this commit [6341319|https://github.com/apache/cassandra/commit/634131961af9c1d88b34797c1c45000f71a76dae#diff-584645e6e932edd7a17f03c79ae87b1f1f8ed54919a31ce8785af156b89c0b76L260].

Also, it looks like it has happened before: CASSANDRA-12497"
CASSANDRA-18352,Add Option to Timebox write timestamps,"In several cases it is desirable to have client provided timestamps generated at the application-level. This can be error prone, however. In particular, applications can choose timestamps that may be nonsensical for a given application. One dangerous manifestation of this is the ""doomstone"" (a tombstone far in the future of any realistic write). This feature would allow either operators or users to specify a minimum and maximum timebound of ""reasonable"" timestamps. The default would be negative infinity, positive infinity to maintain backwards compatibility. Writes that are USING TIMESTAMP with a timestamp outside of the timebox will see an exception. "
CASSANDRA-18350,CEP-21: Multiple tests in DescribeStatementTest fail on committing SchemaChangeRequest,"See https://app.circleci.com/pipelines/github/beobal/cassandra/406/workflows/00cdb02e-4b3e-477a-b997-403121172249/jobs/4202/tests

This reproduces locally.

{noformat}
ERROR [main] 2023-03-17 12:44:33,810 AbstractLocalProcessor.java:52 - Caught error while trying to perform a local commit
java.lang.IllegalStateException: Escaping infinite loop after 10 tries. Current epoch: Epoch{epoch=25}. Next epoch: Epoch{epoch=26}.
	at org.apache.cassandra.tcm.AbstractLocalProcessor.commitLocally(AbstractLocalProcessor.java:114)
	at org.apache.cassandra.tcm.AbstractLocalProcessor.commit(AbstractLocalProcessor.java:48)
	at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:342)
	at org.apache.cassandra.schema.Schema.submit(Schema.java:289)
	at org.apache.cassandra.cql3.statements.schema.AlterSchemaStatement.execute(AlterSchemaStatement.java:132)
	at org.apache.cassandra.cql3.statements.schema.AlterSchemaStatement.executeLocally(AlterSchemaStatement.java:88)
	at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:474)
	at org.apache.cassandra.cql3.CQLTester.executeFormattedQuery(CQLTester.java:1296)
	at org.apache.cassandra.cql3.CQLTester.execute(CQLTester.java:1275)
{noformat}

{noformat}
java.lang.RuntimeException: Error setting schema for test (query was: CREATE KEYSPACE IF NOT EXISTS cql_test_keyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'})

	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:1173)
	at org.apache.cassandra.cql3.CQLTester.beforeTest(CQLTester.java:357)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Caused by: java.lang.IllegalStateException: Couldn't commit the transformation SchemaChangeRequest{, schemaTransformation=CreateKeyspaceStatement (cql_test_keyspace)} after 10 tries
	at org.apache.cassandra.tcm.ClusterMetadataService.commit(ClusterMetadataService.java:385)
	at org.apache.cassandra.schema.Schema.submit(Schema.java:289)
	at org.apache.cassandra.cql3.statements.schema.AlterSchemaStatement.execute(AlterSchemaStatement.java:132)
	at org.apache.cassandra.cql3.statements.schema.AlterSchemaStatement.executeLocally(AlterSchemaStatement.java:88)
	at org.apache.cassandra.cql3.CQLTester.schemaChange(CQLTester.java:1168)
{noformat}"
CASSANDRA-18349,CEP-21: o.a.c.d.t.MoveTest appears to call StorageService#getLocalTokens() before any saved tokens exist,"See: https://app.circleci.com/pipelines/github/beobal/cassandra/406/workflows/00cdb02e-4b3e-477a-b997-403121172249/jobs/4201/tests

{noformat}
java.lang.AssertionError
	at org.apache.cassandra.service.StorageService.getLocalTokens(StorageService.java:2256)
	at org.apache.cassandra.distributed.test.MoveTest.lambda$move$e42fa1e3$1(MoveTest.java:67)
...
{noformat}"
CASSANDRA-18348,CEP-21: AlterTest#unknownMemtableConfigurationTest fails on possibly no longer valid assumptions around schema propagation,"See: [https://app.circleci.com/pipelines/github/beobal/cassandra/406/workflows/00cdb02e-4b3e-477a-b997-403121172249/jobs/4201/tests]

The test hangs on L156, which the test assumes should succeed w/ node 1 having correct local configuration:

{noformat}
cluster.schemaChange(""ALTER TABLE "" + KEYSPACE + "".tbl WITH memtable = 'testconfig'"", false, node1);
{noformat}

In the logs...

{noformat}
ERROR [node2_isolatedExecutor:1] node2 2023-03-17 12:26:07,943 JVMStabilityInspector.java:68 - Exception in thread Thread[node2_isolatedExecutor:1,5,isolatedExecutor]
org.apache.cassandra.exceptions.ConfigurationException: Memtable configuration ""testconfig"" not found.
	at org.apache.cassandra.schema.MemtableParams.parseConfiguration(MemtableParams.java:155)
	at java.base/java.util.HashMap.computeIfAbsent(HashMap.java:1133)
	at org.apache.cassandra.schema.MemtableParams.get(MemtableParams.java:111)
	at org.apache.cassandra.cql3.statements.schema.TableAttributes.build(TableAttributes.java:129)
	at org.apache.cassandra.cql3.statements.schema.TableAttributes.asNewTableParams(TableAttributes.java:65)
	at org.apache.cassandra.cql3.statements.schema.AlterTableStatement$AlterOptions.validate(AlterTableStatement.java:444)
	at org.apache.cassandra.distributed.impl.Coordinator.unsafeExecuteInternal(Coordinator.java:114)
{noformat}

{noformat}
ERROR 17:26:12 Could not process the entry
java.lang.AssertionError: null
	at org.apache.cassandra.tcm.log.LocalLog.processPendingInternal(LocalLog.java:272)
	at org.apache.cassandra.tcm.log.LocalLog$Async$AsyncRunnable.run(LocalLog.java:473)
	at org.apache.cassandra.concurrent.InfiniteLoopExecutor.loop(InfiniteLoopExecutor.java:121)
{noformat}

{noformat}
java.lang.IllegalStateException: Schema agreement not reached. Schema versions of the instances: [00000000-0000-0000-0000-000000000009, 00000000-0000-0000-0000-000000000006]

	at org.apache.cassandra.distributed.impl.AbstractCluster$ChangeMonitor.waitForCompletion(AbstractCluster.java:895)
	at org.apache.cassandra.distributed.impl.AbstractCluster.lambda$schemaChange$8(AbstractCluster.java:824)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
{noformat}"
CASSANDRA-18347,CEP-21: Startup failures in Python dtests around TCM_REPLAY_REQ,"There are currently widespread, locally reproducible failures in the Python dtests against the {{cep-21-tcm}} branch. For example...
 
{noformat}pytest --cassandra-dir=/Users/maedhroz/Forks/cassandra topology_test.py::TestTopology::test_decommissioned_node_cant_rejoin{noformat}

{noformat}pytest --cassandra-dir=/Users/maedhroz/Forks/cassandra materialized_views_test.py::TestMaterializedViews::test_query_new_column{noformat}

{noformat}pytest --cassandra-dir=/Users/maedhroz/Forks/cassandra read_repair_test.py::TestSpeculativeReadRepair::test_normal_read_repair{noformat}

https://app.circleci.com/pipelines/github/maedhroz/cassandra/701/workflows/44a5c7e0-0de0-4839-bbd0-80771fe23843/jobs/7251

https://app.circleci.com/pipelines/github/beobal/cassandra/406/workflows/00cdb02e-4b3e-477a-b997-403121172249/jobs/4204/tests

The death spiral in the node startup logs starts like this…

{noformat}
WARN  [Messaging-EventLoop-3-1] 2023-03-17 11:55:34,037 NoSpamLogger.java:108 - /127.0.0.2:7000->/127.0.0.1:7000-SMALL_MESSAGES-[no-channel] dropping message of type TCM_REPLAY_REQ whose timeout expired before reaching the network
ERROR [InternalResponseStage:3] 2023-03-17 11:55:34,038 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.1:7000], checkLive=false}
INFO  [Messaging-EventLoop-3-12] 2023-03-17 11:55:34,099 InboundConnectionInitiator.java:567 - /127.0.0.2:7000(/127.0.0.2:49763)->/127.0.0.2:7000-SMALL_MESSAGES-1b9301b6 messaging connection established, version = 13, framing = CRC, encryption =
unencrypted
INFO  [Messaging-EventLoop-3-9] 2023-03-17 11:55:34,099 OutboundConnection.java:1164 - /127.0.0.2:7000(/127.0.0.2:49763)->/127.0.0.2:7000-SMALL_MESSAGES-a9302b2e successfully connected, version = 13, framing = CRC, encryption = unencrypted
WARN  [InternalMetadataStage:5] 2023-03-17 11:55:34,100 NoSpamLogger.java:108 - Not currently a member of the CMS
INFO  [Messaging-EventLoop-3-13] 2023-03-17 11:55:34,102 InboundConnectionInitiator.java:567 - /127.0.0.2:7000(/127.0.0.2:49764)->/127.0.0.2:7000-URGENT_MESSAGES-f887f6fa messaging connection established, version = 13, framing = CRC, encryption =
 unencrypted
INFO  [Messaging-EventLoop-3-11] 2023-03-17 11:55:34,102 OutboundConnection.java:1164 - /127.0.0.2:7000(/127.0.0.2:49764)->/127.0.0.2:7000-URGENT_MESSAGES-5cd0c637 successfully connected, version = 13, framing = CRC, encryption = unencrypted
ERROR [InternalResponseStage:4] 2023-03-17 11:55:49,237 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /
127.0.0.3:7000, /127.0.0.1:7000], checkLive=false}
WARN  [InternalMetadataStage:8] 2023-03-17 11:55:49,394 NoSpamLogger.java:108 - Not currently a member of the CMS
WARN  [Messaging-EventLoop-3-1] 2023-03-17 11:56:04,636 NoSpamLogger.java:108 - /127.0.0.2:7000->/127.0.0.1:7000-SMALL_MESSAGES-[no-channel] dropping message of type TCM_REPLAY_REQ whose timeout expired before reaching the network
ERROR [InternalResponseStage:5] 2023-03-17 11:56:04,637 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000, /
127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000], checkLive=false}
WARN  [InternalMetadataStage:11] 2023-03-17 11:56:04,892 NoSpamLogger.java:108 - Not currently a member of the CMS
...
ERROR [InternalResponseStage:6] 2023-03-17 11:56:20,335 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.1:7000], checkLive=false}
WARN  [InternalMetadataStage:14] 2023-03-17 11:56:20,391 NoSpamLogger.java:108 - Not currently a member of the CMS
ERROR [InternalResponseStage:7] 2023-03-17 11:56:21,750 RemoteProcessor.java:164 - Got error from /127.0.0.3:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /
127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.3:7000], checkLive=false}
WARN  [Messaging-EventLoop-3-1] 2023-03-17 11:56:35,535 NoSpamLogger.java:108 - /127.0.0.2:7000->/127.0.0.1:7000-SMALL_MESSAGES-[no-channel] dropping message of type TCM_REPLAY_REQ whose timeout expired before reaching the network
ERROR [InternalResponseStage:8] 2023-03-17 11:56:35,537 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /
127.0.0.3:7000, /127.0.0.1:7000], checkLive=false}
WARN  [InternalMetadataStage:17] 2023-03-17 11:56:35,693 NoSpamLogger.java:108 - Not currently a member of the CMS
ERROR [InternalResponseStage:9] 2023-03-17 11:56:37,135 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /
127.0.0.3:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.3:7000, /127.0.0.1:7000], checkLive=false}
WARN  [InternalMetadataStage:20] 2023-03-17 11:56:37,540 NoSpamLogger.java:108 - Not currently a member of the CMS
ERROR [InternalResponseStage:10] 2023-03-17 11:56:50,935 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000,
/127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000], checkLive=false}
WARN  [InternalMetadataStage:23] 2023-03-17 11:56:51,191 NoSpamLogger.java:108 - Not currently a member of the CMS
{noformat}

...and ends here:

{noformat}
ERROR [InternalResponseStage:11] 2023-03-17 11:56:53,036 RemoteProcessor.java:164 - Got error from /127.0.0.1:7000: TIMEOUT when sending TCM_REPLAY_REQ, retrying on CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000,
/127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.3:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000], checkLive=false}
Exception (java.lang.IllegalStateException) encountered during startup: Could not succeed sending TCM_REPLAY_REQ to CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000
, /127.0.0.3:7000, /127.0.0.3:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000], checkLive=false} after 10 tries
ERROR [main] 2023-03-17 11:56:53,546 CassandraDaemon.java:929 - Exception encountered during startup
java.lang.IllegalStateException: Could not succeed sending TCM_REPLAY_REQ to CandidateIterator{candidates=[/127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.3:7000, /12
7.0.0.1:7000, /127.0.0.2:7000, /127.0.0.1:7000, /127.0.0.2:7000, /127.0.0.3:7000, /127.0.0.1:7000], checkLive=false} after 10 tries
        at org.apache.cassandra.tcm.RemoteProcessor.sendWithCallback(RemoteProcessor.java:181)
        at org.apache.cassandra.tcm.RemoteProcessor.replayAndWait(RemoteProcessor.java:118)
        at org.apache.cassandra.tcm.ClusterMetadataService$SwitchableProcessor.replayAndWait(ClusterMetadataService.java:577)
        at org.apache.cassandra.tcm.Startup.initializeForDiscovery(Startup.java:149)
        at org.apache.cassandra.tcm.Startup.initialize(Startup.java:84)
        at org.apache.cassandra.tcm.Startup.initialize(Startup.java:59)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:267)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:777)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:907)
...
{noformat}"
CASSANDRA-18346,Error Unknown column during deserialization missing keyspace and table name,"The ERROR message generated in ColumnSubselection.java when a column name is not found only prints the column name, not the keyspace and table.  It can be difficult to track down the source when more than one table uses the same name.  E.g., 'id'.
{quote}{{if (column == null)}}
{
{{        column = metadata.getDroppedColumn(name);}}
{{        if (column == null)}}
{{                throw new UnknownColumnException(""Unknown column "" + UTF8Type.instance.getString(name) + "" during deserialization"");}}
{{}}}
{quote}
Example:

[ERROR] cluster_id=15 ip_address=192.168.65.10  java.lang.RuntimeException: Unknown column id during deserialization

Proposed:

[ERROR] cluster_id=15 ip_address=192.168.65.10  java.lang.RuntimeException: Unknown column id in table cycling.route during deserialization"
CASSANDRA-18345,Enable streaming SAI components as part of repair,"SAI registers it's components with the SSTable descriptor expecting them to form part of the SSTable lifecycle, including streaming. This is not the case.

The current SSTable format in Cassandra uses a fixed set of components (Components.STREAMING_COMPONENTS) when streaming SSTables. This needs to change to use the set of components that are registered with a specific SSTable by calls to SSTable.registerComponents.

"
CASSANDRA-18344,"Store PreAccept, Accept, Commit, and Apply in a durable log before processing by CommandStores","Write PreAccept, Accept, Commit, and Apply messages durably to a journal.
CommandStore will then processes the messages in write order.
 
This is a prerequisite JIRA for several incoming improvements, such as reducing the data we store in system tables, making command stores and progress log persistent, simplifying caching.
The journal supports invalidating individual entries by id and sets of owners, and looking up records by id, both needed for near-future Accord work.
 
Several complete/near-complete parts of the generic journal implementation have been temporarily taken out until we need them (soon, once bootstrap/state reclamation are implemented).
These include invalidation support (in memory and on-disk implementations), and segment compaction (not needed without invalidations).
 "
CASSANDRA-18343,JDK17 - fix nodetool_test.TestNodetool.test_sjk," 
[https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/2321/workflows/de1f521d-c5cb-4ddd-bc45-9ec71b577bf3/jobs/19923/tests]
 
{code:java}
AssertionError: Expected 'SJK hh' output assert False == True self = <nodetool_test.TestNodetool object at 0x7f9ab6d95908> @since('4.0') def test_sjk(self): """""" Verify that SJK generally works. """""" cluster = self.cluster cluster.populate([1]).start() node = cluster.nodelist()[0] out, err, _ = node.nodetool('sjk --help') logger.debug(out) hasPattern = False for line in out.split(os.linesep): if "" ttop [Thread Top] Displays threads from JVM process"" == line: hasPattern = True assert hasPattern == True, ""Expected help about SJK ttop"" out, err, _ = node.nodetool('sjk') logger.debug(out) hasPattern = False for line in out.split(os.linesep): if "" ttop [Thread Top] Displays threads from JVM process"" == line: hasPattern = True assert hasPattern == True, ""Expected help about SJK ttop"" out, err, _ = node.nodetool('sjk hh -n 10 --live') logger.debug(out) hasPattern = False for line in out.split(os.linesep): if re.match('.*Instances.*Bytes.*Type.*', line): hasPattern = True > assert hasPattern == True, ""Expected 'SJK hh' output"" E AssertionError: Expected 'SJK hh' output E assert False == True nodetool_test.py:482: AssertionError{code}"
CASSANDRA-18340,Bump snakeyaml from 1.26 to 2.0,"snakeyaml 1.26 has CVEs. Bump version for snakeyaml from 1.26 to 2.0

To see the CVEs, goto [https://mvnrepository.com/artifact/org.apache.cassandra/cassandra-all/4.1.0] and seach for [org.yaml|https://mvnrepository.com/artifact/org.yaml] » [snakeyaml|https://mvnrepository.com/artifact/org.yaml/snakeyaml] under compile dependencies.Vulnerabilites are listed thusly:

 

Direct vulnerabilities:
[CVE-2022-41854|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-41854]
[CVE-2022-38752|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-38752]
[CVE-2022-38751|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-38751]
[View 4 more ...|https://mvnrepository.com/artifact/org.yaml/snakeyaml/1.26#]
Vulnerabilities from dependencies:
[CVE-2022-22971|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22971]
[CVE-2022-22970|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22970]
[CVE-2022-22968|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2022-22968]

............."
CASSANDRA-18338,Test failure: org.apache.cassandra.distributed.test.ByteBuddyExamplesTest.countTest,"jdk8 and jdk11 all faild at some time, jdk8 failed once, and jdk11 failed twice.


for jdk 11:
org.apache.cassandra.distributed.test.ByteBuddyExamplesTest.countTest-.jdk11 (from org.apache.cassandra.distributed.test.ByteBuddyExamplesTest-.jdk11)
Error Message
expected:<1> but was:<2>
Stacktrace
junit.framework.AssertionFailedError: expected:<1> but was:<2>
	at org.apache.cassandra.distributed.test.ByteBuddyExamplesTest.lambda$countTest$81c80a4a$1(ByteBuddyExamplesTest.java:93)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)

for jdk 8:
Error Message
expected:<1> but was:<4>
Stacktrace
junit.framework.AssertionFailedError: expected:<1> but was:<4>
	at org.apache.cassandra.distributed.test.ByteBuddyExamplesTest.lambda$countTest$81c80a4a$1(ByteBuddyExamplesTest.java:93)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:750)


https://ci-cassandra.apache.org/job/Cassandra-4.1/286/testReport/org.apache.cassandra.distributed.test/ByteBuddyExamplesTest/countTest_2/

https://ci-cassandra.apache.org/job/Cassandra-trunk/1489/testReport/org.apache.cassandra.distributed.test/ByteBuddyExamplesTest/countTest__jdk11/"
CASSANDRA-18337,Operations.migrateReadRequiredOperations fails due to concurrent access when TransactionStatement is prepared,"{code}
java.util.NoSuchElementException
	at java.base/java.util.ArrayList$Itr.next(ArrayList.java:1000)
	at org.apache.cassandra.cql3.Operations.migrateReadRequiredOperations(Operations.java:71)
	at org.apache.cassandra.cql3.Operations.migrateReadRequiredOperations(Operations.java:63)
	at org.apache.cassandra.cql3.statements.ModificationStatement.getTxnWriteFragment(ModificationStatement.java:828)
	at org.apache.cassandra.cql3.statements.TransactionStatement.createWriteFragments(TransactionStatement.java:290)
	at org.apache.cassandra.cql3.statements.TransactionStatement.createUpdate(TransactionStatement.java:309)
	at org.apache.cassandra.cql3.statements.TransactionStatement.createTxn(TransactionStatement.java:334)
	at org.apache.cassandra.cql3.statements.TransactionStatement.execute(TransactionStatement.java:375)
{code}

this was caused by having shared mutable state!  when we start creating the txn objects we would also mutate the mutations that had operations that need to be run in the txn, this has an issue when the txn is run from prepared statements as the object is shared by multiple threads, causing the array to be mutated while iterating."
CASSANDRA-18336,Do not remove SSTables when cause of FSReadError is OutOfMemoryError while using best_effort disk failure policy,"1.When this exception occurs in the system
{code:java}
// 
ERROR [CompactionExecutor:351627] 2023-02-21 17:59:20,721 CassandraDaemon.java:581 - Exception in thread Thread[CompactionExecutor:351627,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Map failed
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:167)
    at org.apache.cassandra.io.util.MmappedRegions$State.add(MmappedRegions.java:310)
    at org.apache.cassandra.io.util.MmappedRegions$State.access$400(MmappedRegions.java:246)
    at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:170)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61)
    at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104)
    at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:365)
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:337)
    at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:172)
    at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:124)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:64)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:137)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:193)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:77)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:100)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:298)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1016)
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:163)
    ... 23 common frames omitted
Caused by: java.lang.OutOfMemoryError: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map0(Native Method)
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1013)


{code}
2.Restart the node, Verifying logfile transaction ,All sstables are deleted
{code:java}
// code placeholder
INFO  [main] 2023-02-21 18:00:23,350 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Index.db 
INFO  [main] 2023-02-21 18:00:23,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_c923b230-b077-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,510 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,518 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,543 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,546 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,550 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,590 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,606 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,610 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,685 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,687 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,688 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,727 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,775 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,787 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,020 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,022 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,077 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,078 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,092 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,097 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,098 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,118 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,133 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,484 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,490 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,492 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,502 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Index.db 
INFO  [main] 2023-02-21 18:00:48,045 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Filter.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Summary.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,166 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,202 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,841 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Data.db 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Filter.db 
INFO  [main] 2023-02-21 18:01:22,825 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Index.db 
INFO  [main] 2023-02-21 18:01:24,891 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Summary.db 
INFO  [main] 2023-02-21 18:01:24,892 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,190 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,462 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Index.db 
INFO  [main] 2023-02-21 18:02:02,466 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Filter.db 
INFO  [main] 2023-02-21 18:02:02,467 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,763 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Summary.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,765 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Index.db 
INFO  [main] 2023-02-21 18:02:05,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Filter.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Summary.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Data.db 
INFO  [main] 2023-02-21 18:02:41,367 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:41,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:41,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Index.db 
INFO  [main] 2023-02-21 18:02:42,034 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Filter.db 
INFO  [main] 2023-02-21 18:02:42,049 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Index.db 
INFO  [main] 2023-02-21 18:04:02,784 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Filter.db 
INFO  [main] 2023-02-21 18:04:02,785 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,889 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Index.db 
INFO  [main] 2023-02-21 18:04:03,384 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Filter.db 
INFO  [main] 2023-02-21 18:04:03,418 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Data.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Summary.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:38,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Filter.db 
INFO  [main] 2023-02-21 18:04:38,293 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Index.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Summary.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Data.db 
INFO  [main] 2023-02-21 18:05:28,014 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Digest.crc32 
INFO  [main] 2023-02-21 18:05:28,015 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Statistics.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-TOC.txt 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Filter.db 
INFO  [main] 2023-02-21 18:05:28,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Index.db 
INFO  [main] 2023-02-21 18:05:28,277 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Data.db 
INFO  [main] 2023-02-21 18:06:17,552 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,553 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:17,554 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:17,566 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Index.db 
INFO  [main] 2023-02-21 18:06:17,567 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Filter.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Data.db 
INFO  [main] 2023-02-21 18:06:24,899 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:24,900 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:24,932 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Filter.db 
INFO  [main] 2023-02-21 18:06:24,949 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Index.db 
INFO  [main] 2023-02-21 18:06:29,880 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Data.db 
INFO  [main] 2023-02-21 18:08:11,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Index.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Filter.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,718 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Data.db 
INFO  [main] 2023-02-21 18:08:22,177 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Filter.db 
INFO  [main] 2023-02-21 18:08:22,212 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Index.db 
INFO  [main] 2023-02-21 18:08:22,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Summary.db 
INFO  [main] 2023-02-21 18:08:22,642 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Data.db 
INFO  [main] 2023-02-21 18:09:16,035 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:16,036 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:16,163 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Index.db 
INFO  [main] 2023-02-21 18:09:16,302 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Filter.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Summary.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Data.db 
INFO  [main] 2023-02-21 18:09:30,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Filter.db 
INFO  [main] 2023-02-21 18:09:30,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Index.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Summary.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,487 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,692 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,741 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,742 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Filter.db 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Index.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Summary.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,760 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Index.db 
INFO  [main] 2023-02-21 18:10:18,081 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Filter.db 
INFO  [main] 2023-02-21 18:10:18,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Data.db 
INFO  [main] 2023-02-21 18:11:06,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Summary.db 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:06,080 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Index.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Filter.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Data.db 
INFO  [main] 2023-02-21 18:11:16,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Summary.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:16,902 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:16,903 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Index.db 
INFO  [main] 2023-02-21 18:11:17,170 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Filter.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Summary.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Data.db 
INFO  [main] 2023-02-21 18:11:59,054 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:59,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Index.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Filter.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Summary.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Data.db 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Index.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Filter.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Data.db 
INFO  [main] 2023-02-21 18:12:42,749 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Filter.db 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Index.db 
INFO  [main] 2023-02-21 18:12:42,820 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,821 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Data.db 
INFO  [main] 2023-02-21 18:12:55,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:55,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Filter.db 
INFO  [main] 2023-02-21 18:12:55,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Index.db 
INFO  [main] 2023-02-21 18:12:57,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Data.db 
INFO  [main] 2023-02-21 18:15:07,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Summary.db 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:07,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Index.db 
INFO  [main] 2023-02-21 18:15:07,909 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Filter.db 
INFO  [main] 2023-02-21 18:15:07,950 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,262 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,327 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,550 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,856 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,857 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Data.db 
INFO  [main] 2023-02-21 18:16:19,652 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Summary.db 
INFO  [main] 2023-02-21 18:16:19,653 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Digest.crc32 
INFO  [main] 2023-02-21 18:16:19,654 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Statistics.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-TOC.txt 
INFO  [main] 2023-02-21 18:16:19,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,772 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Data.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Summary.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,232 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Data.db 
INFO  [main] 2023-02-21 18:17:17,432 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:17,449 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Index.db 
INFO  [main] 2023-02-21 18:17:17,969 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Filter.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Index.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Filter.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Summary.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Index.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Filter.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Data.db 
INFO  [main] 2023-02-21 18:18:47,122 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Index.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Filter.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,411 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,413 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,634 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,636 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Data.db 
INFO  [main] 2023-02-21 18:19:17,136 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:17,137 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Index.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Filter.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Data.db 
INFO  [main] 2023-02-21 18:19:36,881 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Filter.db 
INFO  [main] 2023-02-21 18:19:36,884 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Index.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Data.db 
INFO  [main] 2023-02-21 18:19:45,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:45,482 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Filter.db 
INFO  [main] 2023-02-21 18:19:45,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Index.db 
INFO  [main] 2023-02-21 18:19:45,639 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Summary.db 
INFO  [main] 2023-02-21 18:19:45,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Data.db 
INFO  [main] 2023-02-21 18:20:48,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:48,587 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:48,619 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Index.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Filter.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Summary.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Data.db 
INFO  [main] 2023-02-21 18:20:51,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:51,052 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Filter.db 
INFO  [main] 2023-02-21 18:20:51,061 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Index.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Summary.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Summary.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,142 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Data.db 
INFO  [main] 2023-02-21 18:21:22,503 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Summary.db 
INFO  [main] 2023-02-21 18:21:22,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Filter.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Index.db 
INFO  [main] 2023-02-21 18:21:22,660 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Data.db 
INFO  [main] 2023-02-21 18:22:11,241 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Index.db 
INFO  [main] 2023-02-21 18:22:11,328 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Filter.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Data.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Index.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Filter.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Summary.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Data.db 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Index.db 
INFO  [main] 2023-02-21 18:22:51,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Filter.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Summary.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,456 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:54,646 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,648 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:22:54,650 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Index.db 
INFO  [main] 2023-02-21 18:22:54,656 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,673 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,694 Keyspace.java:386 - Creating replication strategy kairosdb params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}
INFO  [main] 2023-02-21 18:22:54,715 ColumnFamilyStore.java:385 - Initializing kairosdb.data_points
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:54,720 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830647-big (179.084MiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830662-big (4.039MiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830663-big (3.589MiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830661-big (39.789MiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830664-big (6.007MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830583-big (190.543MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830440-big (191.089MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:54,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830513-big (194.560MiB)
INFO  [main] 2023-02-21 18:22:54,947 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,977 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_index-8742543087ba11eba3799bdca9e7ad04/mc-1-big (7.580MiB)
INFO  [main] 2023-02-21 18:22:55,023 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_time_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,054 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26770-big (0.075KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,070 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26769-big (0.052KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,077 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26768-big (2.671MiB)
INFO  [main] 2023-02-21 18:22:55,131 ColumnFamilyStore.java:385 - Initializing kairosdb.row_keys
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,135 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-796510-big (7.682MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,190 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-769597-big (50.002MiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,203 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-75-big (87.496MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,209 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-256221-big (51.492MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,211 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-550752-big (50.323MiB)
INFO  [main] 2023-02-21 18:22:55,357 ColumnFamilyStore.java:385 - Initializing kairosdb.service_index
INFO  [main] 2023-02-21 18:22:55,381 ColumnFamilyStore.java:385 - Initializing kairosdb.spec
INFO  [main] 2023-02-21 18:22:55,393 ColumnFamilyStore.java:385 - Initializing kairosdb.string_index
INFO  [main] 2023-02-21 18:22:55,409 ColumnFamilyStore.java:385 - Initializing kairosdb.tag_indexed_row_keys
INFO  [main] 2023-02-21 18:22:55,419 Keyspace.java:386 - Creating replication strategy system_auth params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 18:22:55,425 ColumnFamilyStore.java:385 - Initializing system_auth.network_permissions
INFO  [main] 2023-02-21 18:22:55,440 ColumnFamilyStore.java:385 - Initializing system_auth.resource_role_permissons_index
INFO  [main] 2023-02-21 18:22:55,457 ColumnFamilyStore.java:385 - Initializing system_auth.role_members
INFO  [main] 2023-02-21 18:22:55,473 ColumnFamilyStore.java:385 - Initializing system_auth.role_permissions
INFO  [main] 2023-02-21 18:22:55,485 ColumnFamilyStore.java:385 - Initializing system_auth.roles
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,518 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_auth/roles-5bc52802de2535edaeab188eecebb090/mc-1-big (0.100KiB)
INFO  [main] 2023-02-21 18:22:55,543 Keyspace.java:386 - Creating replication strategy system_distributed params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}
INFO  [main] 2023-02-21 18:22:55,558 ColumnFamilyStore.java:385 - Initializing system_distributed.parent_repair_history
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,577 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-49-big (1.398KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,585 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-44-big (1.376KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,591 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-24-big (0.863KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,593 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-5-big (0.644KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,594 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-12-big (1.130KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,595 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-16-big (0.990KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-39-big (1.646KiB)
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-57-big (2.019KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,605 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-68-big (0.920KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,606 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-107-big (0.728KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,607 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-43-big (0.592KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,608 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-31-big (1.451KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-80-big (0.944KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-106-big (0.589KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,622 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-88-big (0.935KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-20-big (1.151KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-76-big (1.481KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,624 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/mc-1-big (16.284KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-95-big (3.737KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-84-big (1.031KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,633 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-64-big (1.132KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,636 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-100-big (0.561KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,656 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-99-big (1.159KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,683 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-104-big (0.720KiB)
INFO  [main] 2023-02-21 18:22:55,733 ColumnFamilyStore.java:385 - Initializing system_distributed.repair_history
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-45-big (2.741KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-25-big (1.789KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-89-big (1.814KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-112-big (1.327KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,749 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-32-big (3.927KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,756 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-67-big (3.370KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,757 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-81-big (3.249KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,758 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-110-big (1.354KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-44-big (1.250KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-50-big (3.340KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-93-big (2.319KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-85-big (1.789KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,770 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-113-big (1.334KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,773 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-101-big (3.239KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,774 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-21-big (2.331KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,785 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-40-big (4.339KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,789 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-17-big (2.308KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,794 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-106-big (0.766KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,797 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-13-big (2.822KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-105-big (2.460KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-6-big (1.802KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,805 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-74-big (2.896KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,808 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/mc-1-big (0.812KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,811 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-59-big (5.973KiB) {code}
3.Bugs can be reproduced.Just set  vm.max_ map_ count as a small value, and then trigger OOM, and restart the node."
CASSANDRA-18335,Push LocalSessions info logs to debug,"I don't think these are particularly useful to have at info, but the impetus for this ticket is specifically [this message|https://github.com/apache/cassandra/blob/cassandra-4.0/src/java/org/apache/cassandra/repair/consistent/LocalSessions.java#L456] during automatic cleanup about skipping deleting sessions that seems to confuse or alarm people."
CASSANDRA-18333,Add max_sstable_size and max_sstable_duration metrics virtual tables,We added max_sstable_size and max_sstable_duration metrics in CASSANDRA-18283 however there are not system_views vtables for these metrics yet.
CASSANDRA-18332,Backport CASSANDRA-17205 to 4.0 branch (strong ref leak),"See description in CASSANDRA-17205; this should have been applied on 4.0 and merged up but was overlooked.

 

Also double-check that strong leaks are logged at ERROR instead of WARN on both 4.0, 4.1, and trunk (see [comment|https://issues.apache.org/jira/browse/CASSANDRA-18176?focusedCommentId=17687184&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17687184])"
CASSANDRA-18331,Extend implicit allow-filtering to clustering keys as well,"This was overlooked in CASSANDRA-18238. 

We should be able to do selects on vtables not only when selecting on regular columns but also on clustering ones. Currently we can do that on regulars only."
CASSANDRA-18330,Delivery of CEP-21: Transactional Cluster Metadata,
CASSANDRA-18329,Upgrade jamm; fix eclipse problems,"Jamm is currently under maintenance that will solve JDK11 issues and enable it to work with post JDK11+ versions up to JDK17.

This ticket will serve as a placeholder for upgrading Jamm in Cassandra when the new Jamm release is out. "
CASSANDRA-18328,Remove deprecated CQL functions dateof and unixtimestampof,"The CQL functions {{dateof}} and {{unixtimestampof}} were [deprecated on Cassandra 2.2.0|https://github.com/apache/cassandra/commit/c08aaabd95d4872593c29807de6ec1485cefa7fa], almost eight years ago. They were deprecated in favour of the then new {{totimestamp}} and {{tounixtimestamp}} functions.

A note about their deprecation was added to [{{NEWS.txt}}|https://github.com/apache/cassandra/blob/trunk/NEWS.txt#L1421-L1423], and they were marked as deprecated on [{{CQL.textile}}|https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile#time-conversion-functions]. They are also listed as deprecated on [the new doc|https://github.com/apache/cassandra/blob/trunk/doc/modules/cassandra/pages/cql/functions.adoc#time-conversion-functions].

We can finally remove those functions in 5.0, since they have been deprecated for so long.

Discussion thread [here|https://lists.apache.org/thread/0gs824fpsngn5dr0yq2x1h8qsflj5sr0].

"
CASSANDRA-18326,Debian package repository misconfiguration,"Debian apt is failing on current jfrog repository access for 40x releases with:

{code}
W: Conflicting distribution: https://debian.cassandra.apache.org 40x InRelease (expected 40x but got 40)

E: Repository 'https://debian.cassandra.apache.org 40x InRelease' changed its 'Codename' value from '40x' to '40'

N: This must be accepted explicitly before updates for this repository can be applied. See apt-secure(8) manpage for details.
{code}

This is caused by the typo in [dists/40x/Release|https://debian.cassandra.apache.org/dists/40x/Release] containing
{code}
Codename: 40
{code}
but it is expected to be
{code}
Codename: 40x
{code}
"
CASSANDRA-18325,Test failure: dtest.bootstrap_test.TestBootstrap.test_cleanup," 
{code:java}
assert not True
+ where True = <bound method Event.is_set of <threading.Event object at 0x7f5b16fda070>>()
+ where <bound method Event.is_set of <threading.Event object at 0x7f5b16fda070>> = <threading.Event object at 0x7f5b16fda070>.is_set
Stacktrace
self = <bootstrap_test.TestBootstrap object at 0x7f5b25f4f880>
def test_cleanup(self):
""""""
@jira_ticket CASSANDRA-11179
Make sure we remove processed files during cleanup
""""""
cluster = self.cluster
cluster.set_environment_variable('CASSANDRA_TOKEN_PREGENERATION_DISABLED', 'True')
cluster.set_configuration_options(values=
{'concurrent_compactors': 4}
)
cluster.populate(1)
cluster.start()
node1, = cluster.nodelist()
for x in range(0, 5):
node1.stress(['write', 'n=100k', 'no-warmup', '-schema', 'compaction(strategy=SizeTieredCompactionStrategy,enabled=false)', 'replication(factor=1)', '-rate', 'threads=10'])
node1.flush()
node2 = new_node(cluster)
node2.start(wait_for_binary_proto=True)
event = threading.Event()
failed = threading.Event()
jobs = 1
thread = threading.Thread(target=self._monitor_datadir, args=(node1, event, len(node1.get_sstables(""keyspace1"", ""standard1"")), jobs, failed))
thread.setDaemon(True)
thread.start()
node1.nodetool(""cleanup -j {} keyspace1 standard1"".format(jobs))
event.set()
thread.join()
> assert not failed.is_set()
E assert not True
E + where True = <bound method Event.is_set of <threading.Event object at 0x7f5b16fda070>>()
E + where <bound method Event.is_set of <threading.Event object at 0x7f5b16fda070>> = <threading.Event object at 0x7f5b16fda070>.is_set
bootstrap_test.py:912: AssertionError
{code}
 

failed twice"
CASSANDRA-18323,Remove org.apache.cassandra.hadoop code,
CASSANDRA-18322,"Warn about unqualified prepared statement only if it is a select, update, delete, insert","Hi, 

We get the following warnings when we use prepared statements with ""create keyspace ... "" or ""drop keyspace"" statements.

""

{{USE <keyspace>}} with prepared statements is considered to be an anti-pattern due to ambiguity in non-qualified table names. Please consider removing instances of {{{}Session#setKeyspace(<keyspace>){}}}, {{Session#execute(""USE <keyspace>"")}} and {{cluster.newSession(<keyspace>)}} from your code, and always use fully qualified table names (e.g. <keyspace>.<table>). Keyspace used: null, statement keyspace: null, statement id: 8153d922390fdf9a9963bfeda85b2f3b at 

""

Such statements are already full-qualified. So, why are we getting this warning? 

Regards
Mohammad"
CASSANDRA-18320,Incompatible file system thrown while running Simulator,"{code}
java.io.UncheckedIOException
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:831)
	at org.apache.cassandra.io.util.PathUtils.propagateUnchecked(PathUtils.java:816)
	at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:257)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:381)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.apache.cassandra.io.util.PathUtils.forEach(PathUtils.java:155)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:378)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.util.ArrayList.forEach(ArrayList.java:1259)
	at java.util.stream.SortedOps$RefSortingSink.end(SortedOps.java:395)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:483)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)
	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)
	at org.apache.cassandra.io.util.PathUtils.forEach(PathUtils.java:155)
	at org.apache.cassandra.io.util.PathUtils.deleteRecursive(PathUtils.java:378)
	at org.apache.cassandra.distributed.impl.AbstractCluster.close(AbstractCluster.java:1047)
	at org.apache.cassandra.simulator.ClusterSimulation.close(ClusterSimulation.java:816)
	at org.apache.cassandra.simulator.SimulationRunner$Run.run(SimulationRunner.java:370)
	at org.apache.cassandra.simulator.SimulationRunner$BasicCommand.run(SimulationRunner.java:345)
	at org.apache.cassandra.simulator.paxos.PaxosSimulationRunner.main(PaxosSimulationRunner.java:148)
	at org.apache.cassandra.simulator.test.ShortPaxosSimulationTest.simulationTest(ShortPaxosSimulationTest.java:33)
Caused by: java.nio.file.DirectoryNotEmptyException: /cassandra/node1/commitlog
	at com.google.common.jimfs.FileSystemView.checkEmpty(FileSystemView.java:535)
	at com.google.common.jimfs.FileSystemView.checkDeletable(FileSystemView.java:517)
	at com.google.common.jimfs.FileSystemView.delete(FileSystemView.java:479)
	at com.google.common.jimfs.FileSystemView.deleteFile(FileSystemView.java:465)
	at com.google.common.jimfs.JimfsFileSystemProvider.delete(JimfsFileSystemProvider.java:261)
	at java.nio.file.Files.delete(Files.java:1126)
	at org.apache.cassandra.io.util.PathUtils.delete(PathUtils.java:252)
{code}"
CASSANDRA-18318,Implement parsing schema provider for external SUT in Harry,
CASSANDRA-18317,Properly synchronize CQLSSTableWriter#build on the Schema.instance,"The {{CQLSSTableWriter#build}} method should properly synchronize on the {{Schema.instance}} class to prevent concurrent Schema operations fail, [when the offline tool also updates schema].

For example, a table creation operation, which modifies the keyspaces tables metadata, might end up missing the update when a concurrent call to the {{CQLSSTableWriter#build}} method is accessing the {{Schema}} instance."
CASSANDRA-18316,Add feature flag for dynamic data masking,"Dynamic data masking ([CEP-20|https://cwiki.apache.org/confluence/display/CASSANDRA/CEP-20%3A+Dynamic+Data+Masking]) is a new feature, so it will need a feature flag in {{cassandra.yaml}}. Something like:
{code}
# If enabled, dynamic data masking allows to attach CQL masking functions to the columns of a table.
# Users without the UNMASK permission will see an obscured version of the values of the columns with an attached mask.
# If dynamic data masking is disabled it won't be allowed to create new column masks, although it will still be possible
# to drop any previously existing masks. Also, any existing mask will be ignored at query time, so all users will see
# the clear values of the masked columns.
dynamic_data_masking_enabled: false
{code}"
CASSANDRA-18315,Implement Concurrent Quiescent Checker ,"It is possible to implement a concurrent quiescent checker, which allows for a high degree of concurrency, while allowing to pinpoint partition state precisely. 

Additional minor improvements:

      * formatting
      * thread shutdown on failed run creation
      * idempotent query execution
      * retry delay"
CASSANDRA-18314,Lift MessagingService.minimum_version to 40,"MessagingService's VERSION_30 and VERSION_3014 don't have to be supported in Cassandra 5.0 anymore.

(Cassandra 5.0 currently is still using VERSION_40)

Patch: https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/18314/trunk

Raises the question whether backward compatibility to the previous major is defined by the Cassandra version or by the internal component version (MessagingService)."
CASSANDRA-18313,Remove legacy 3.x buffer pool metrics,"Remove BufferPoolMetrics.register3xAlias() 

Patch: https://github.com/apache/cassandra/compare/trunk...thelastpickle:cassandra:mck/18313/trunk"
CASSANDRA-18311,BufferPool incorrectly counts memoryInUse when putUnusedPortion is used,"The counter is incorrectly decremented by the size of the unused portion of the provided buffer.
It should be decremented by the number of bytes actually returned to the pool (that may be different than ""size""). The number should be calculated as a difference between original and resulting buffer capacity."
CASSANDRA-18310,Cassandra start up with ERROR of sstable_formats,"Exception (org.apache.cassandra.exceptions.ConfigurationException) encountered during startup: Invalid yaml. Please remove properties [sstable_formats] from your cassandra.yaml
Invalid yaml. Please remove properties [sstable_formats] from your cassandra.yaml
ERROR [main] 2023-03-09 18:03:34,899 CassandraDaemon.java:911 - Exception encountered during startup: Invalid yaml. Please remove properties [sstable_formats] from your cassandra.yaml"
CASSANDRA-18309,Remove git hook for pre-push as it is redundant and causes issues when merging to mainline,"{code}
[cep-15-accord][~/repos/apache-cassandra]$ git push origin cep-15-accord
Warning: Permanently added 'github.com' (ED25519) to the list of known hosts.
Entering 'modules/accord'
Username for 'https://github.com':
{code}

This is caused by .build/git/git-hooks/pre-push/100-push-submodules.sh logic

{code}
  local -r cmd='
branch=""$(git rev-parse --abbrev-ref HEAD)""
[[ ""$branch"" == ""HEAD"" ]] && exit 0

default_remote=""$(git config --local --get branch.""${branch}"".remote || true)""
remote=""${default_remote:-origin}""

git push --atomic ""$remote"" ""$branch""
'
  git submodule foreach --recursive ""$cmd""
{code}

This logic was to make sure that the submodule is pushed before you push your changes, but this is slightly redundant as .build/git/git-hooks/pre-commit/100-verify-submodules-pushed.sh will not allow you to commit the submodule SHA until it can confirm its on GitHub."
CASSANDRA-18308,Fix broken sstableverify dtest,"I accidentally broken sstableverify dtest in CASSANDRA-17056 for versions <= 4.1
"
CASSANDRA-18307,Release 4.0.8 not available on jfrog package repositories,Release 4.0.8 was published to dist/downloads.a.o only and seems not available at apache.jfrog.io neither for Debian nor RPM.
CASSANDRA-18305,Enhance nodetool compactionstats with existing MBean metrics,"Nodetool compactionstats reports only on active compactions, if nothing is active, you see only:
{quote}$nodetool compactionstats

pending tasks: 0
{quote}
but in the MBean Compaction/TotalCompactionsCompleted there are recent statistic in events/second for:
 * Count
 * FifteenMinueRate
 * FiveMinueRate
 * MeanRate
 * OneMinuteRate

1) It would be useful to see in addition:
{quote}pending tasks: 0

compactions completed: 20

    1 minute rate:    0/second

   5 minute rate:    2.3/second

  15 minute rate:   4.6/second
{quote}
2) Since compaction_throughput_mb_per_sec is a throttling parameter in cassandra.yaml (default 64 MBps), it would be nice to show the actual compaction throughput and be able to observe if you're close to the limit.  I.e., 
{quote}compaction throughput 13.2 MBps / 16 MBps (82.5%)
{quote}
3) for completness, compactionstats should list the number of concurrent compactors configured, perhaps simply add to existing 'pending tasks' line:
{quote}4 concurrent compactors, 0 pending tasks
{quote}"
CASSANDRA-18304,hinted_handoff_enabled=false is not honored,"I've had some dtests with disabled hints failing.

After investigation it seems that CASSANDRA-17164 moved hint submission on timeout from [RequestCallbacks.onExpired|https://github.com/apache/cassandra/commit/d2923275e360a1ee9db498e748c269f701bb3a8b#diff-b73c13ea8cae91a215495917fe5e90d55c9f4a283f9e053110992bc9a60004b8L176] to [AbstractWriteResponseHandler.onFailure|https://github.com/apache/cassandra/commit/d2923275e360a1ee9db498e748c269f701bb3a8b#diff-3b202de0d077638bede7bf4076a15eb4d483b717f955f11e743efb8d27c6eb1dR285], but it no longer checks if {{CallbackInfo.shouldHint}} which checks for {{StorageProxy.shouldHint}} which ultimately checks if {{{}hinted_handoff_enabled=true{}}}.

This could cause some tests which expect hints to be disabled to fail intermittently."
CASSANDRA-18303,Feature documentation lost / moved out of focus,"Documentation added with CASSANDRA-17344 wasn't moved with CASSANDRA-17976 and now the documentation on the website page ""Cassandra/Operating/Virtual tables"" shows an outdated version for [4.1|https://cassandra.apache.org/doc/4.1/cassandra/operating/virtualtables.html] and [latest|https://cassandra.apache.org/doc/latest/cassandra/operating/virtualtables.html]."
CASSANDRA-18302,Fix AIOOBE and improve validation messages for transaction statements,"Currently it happens sometimes that ArrayIndexOutOfBoundsException is thrown from asCql method when validation transaction statement. In addition, asCql does not return precisely the query user entered so the whole error message can be misleading.
"
CASSANDRA-18299,Add support for prepared statements for accord transactions,When you try to prepare an accord transaction we get a NullPointerException in method org.apache.cassandra.cql3.statements.QualifiedStatement#isFullyQualified. This is due to TransactionStatement extending but not overriding all the methods (and setting name to null).
CASSANDRA-18298,Remove WaitingOnFreeMemtableSpace metric,"It doesn't seem like this metric is [ever updated|https://github.com/apache/cassandra/search?q=WaitingOnFreeMemtableSpace], or ever was.

While we're at it we should remove any other unused metric."
CASSANDRA-18294,die disk failure policy will not kill jvm as documented,"After Cassandra has successfully starts up with disk_failure_policy die, when encounter a file system error, Cassandra server will only throw exception instead of shut down gossip and client transport and kill JVM. Document: [https://cassandra.apache.org/doc/latest/cassandra/configuration/cass_yaml_file.html#disk_failure_policy]

 

The reason for this is the default FS error handler is not handing policy die correctly. Instead of shutting down gossip and native transport, it throws an error.

 

The JVMStabilityInspectorTest is passing because the error handler is not set so no exception is thrown."
CASSANDRA-18293,Remove JAVA8_HOME and JAVA11_HOME from circle configs,"In CASSANDRA-18106 we decided to take the approach of avoiding these variables since they can be surprising and aren't needed.

They are still surprising us though and need to be removed."
CASSANDRA-18292,Gossip stateMapOrdering does not have correct ordering when both EndpointState are in the bootstrapping set,"There is a bug when stateMapOrdering sees two or more EndpointState that are both in the bootstrapping set, this may cause the ordering to change causing Collections.sort to fail
"
CASSANDRA-18291,Too early schema version change in system local table,"Schema version in the system local table is updated after the schema changes is saved but before it is applied. 

Found by [~maxtomassi]
"
CASSANDRA-18289,Test Failure: sslnodetonode_test.TestNodeToNodeSSLEncryption.test_ssl_client_auth_required_fail,"from
- https://ci-cassandra.apache.org/job/Cassandra-trunk/1469/testReport/dtest-offheap.sslnodetonode_test/TestNodeToNodeSSLEncryption/test_ssl_client_auth_required_fail/
- https://ci-cassandra.apache.org/job/Cassandra-trunk-dtest-offheap/1255/label=cassandra-dtest,split=26/testReport/junit/dtest-offheap.sslnodetonode_test/TestNodeToNodeSSLEncryption/test_ssl_client_auth_required_fail/

Stacktrace
{noformat}
self = <sslnodetonode_test.TestNodeToNodeSSLEncryption object at 0x7f9ff4fb84f0>

    def test_ssl_client_auth_required_fail(self):
        """"""peers need to perform mutual auth (cient auth required), but do not supply the local cert""""""
        credNode1 = sslkeygen.generate_credentials(""127.0.0.1"")
        credNode2 = sslkeygen.generate_credentials(""127.0.0.2"")
    
        self.setup_nodes(credNode1, credNode2, client_auth=True)
    
        self.fixture_dtest_setup.allow_log_errors = True
        self.cluster.start(no_wait=True)
        time.sleep(2)
    
        found = self._grep_msg(self.node1, _LOG_ERR_HANDSHAKE, _LOG_ERR_GENERAL)
>       assert found
E       assert False

sslnodetonode_test.py:83: AssertionError
{noformat}"
CASSANDRA-18288,Test Failure: TopPartitionsTest.basicRegularTombstonesTest,"from
- https://ci-cassandra.apache.org/job/Cassandra-trunk/1469/testReport/org.apache.cassandra.distributed.test/TopPartitionsTest/basicRegularTombstonesTest_Incremental___jdk11/
- https://ci-cassandra.apache.org/job/Cassandra-trunk-jvm-dtest/1534/jdk=jdk_11_latest,label=cassandra,split=8/testReport/junit/org.apache.cassandra.distributed.test/TopPartitionsTest/basicRegularTombstonesTest_Incremental___jdk11/

Stacktrace
{noformat}
java.lang.RuntimeException: 
	at org.psjava.util.AssertStatus.assertTrue(AssertStatus.java:18)
	at org.psjava.util.AssertStatus.assertTrue(AssertStatus.java:5)
	at org.apache.cassandra.distributed.test.TopPartitionsTest.lambda$basicRegularTombstonesTest$e2f532b5$1(TopPartitionsTest.java:226)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
{noformat}
Standard Output
{noformat}
INFO  [main] <main> 2023-02-25 00:15:20,407 Reflections.java:219 - Reflections took 1661 ms to scan 9 urls, producing 1832 keys and 7268 values
INFO  [main] <main> 2023-02-25 00:15:21,602 Reflections.java:219 - Reflections took 1131 ms to scan 9 urls, producing 1832 keys and 7268 values
Node id topology:
node 1: dc = datacenter0, rack = rack0
node 2: dc = datacenter0, rack = rack0
Configured node count: 2, nodeIdTopology size: 2
DEBUG [main] node1 2023-02-25 00:15:22,419 InternalLoggerFactory.java:63 - Using SLF4J as the default logging framework
DEBUG [main] node1 2023-02-25 00:15:22,426 PlatformDependent0.java:417 - -Dio.netty.noUnsafe: false
DEBUG [main] node1 2023-02-25 00:15:22,427 PlatformDependent0.java:897 - Java version: 11
DEBUG [main] node1 2023-02-25 00:15:22,428 PlatformDependent0.java:130 - sun.misc.Unsafe.theUnsafe: available
DEBUG [main] node1 2023-02-25 00:15:22,429 PlatformDependent0.java:154 - sun.misc.Unsafe.copyMemory: available
DEBUG [main] node1 2023-02-25 00:15:22,430 PlatformDependent0.java:192 - java.nio.Buffer.address: available
DEBUG [main] node1 2023-02-25 00:15:22,431 PlatformDependent0.java:257 - direct buffer constructor: available
DEBUG [main] node1 2023-02-25 00:15:22,432 PlatformDependent0.java:331 - java.nio.Bits.unaligned: available, true
DEBUG [main] node1 2023-02-25 00:15:22,434 PlatformDependent0.java:393 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): available
DEBUG [main] node1 2023-02-25 00:15:22,435 PlatformDependent0.java:403 - java.nio.DirectByteBuffer.<init>(long, int): available
DEBUG [main] node1 2023-02-25 00:15:22,435 PlatformDependent.java:1079 - sun.misc.Unsafe: available
DEBUG [main] node1 2023-02-25 00:15:22,448 PlatformDependent.java:1181 - maxDirectMemory: 1056309248 bytes (maybe)
DEBUG [main] node1 2023-02-25 00:15:22,449 PlatformDependent.java:1200 - -Dio.netty.tmpdir: /home/cassandra/cassandra/tmp (java.io.tmpdir)
DEBUG [main] node1 2023-02-25 00:15:22,449 PlatformDependent.java:1279 - -Dio.netty.bitMode: 64 (sun.arch.data.model)
DEBUG [main] node1 2023-02-25 00:15:22,450 PlatformDependent.java:177 - -Dio.netty.maxDirectMemory: 1056309248 bytes
DEBUG [main] node1 2023-02-25 00:15:22,451 PlatformDependent.java:184 - -Dio.netty.uninitializedArrayAllocationThreshold: 1024
DEBUG [main] node1 2023-02-25 00:15:22,452 CleanerJava9.java:71 - java.nio.ByteBuffer.cleaner(): available
DEBUG [main] node1 2023-02-25 00:15:22,453 PlatformDependent.java:204 - -Dio.netty.noPreferDirect: false
DEBUG [main] node2 2023-02-25 00:15:23,126 InternalLoggerFactory.java:63 - Using SLF4J as the default logging framework
DEBUG [main] node2 2023-02-25 00:15:23,133 PlatformDependent0.java:417 - -Dio.netty.noUnsafe: false
DEBUG [main] node2 2023-02-25 00:15:23,134 PlatformDependent0.java:897 - Java version: 11
DEBUG [main] node2 2023-02-25 00:15:23,135 PlatformDependent0.java:130 - sun.misc.Unsafe.theUnsafe: available
DEBUG [main] node2 2023-02-25 00:15:23,136 PlatformDependent0.java:154 - sun.misc.Unsafe.copyMemory: available
DEBUG [main] node2 2023-02-25 00:15:23,136 PlatformDependent0.java:192 - java.nio.Buffer.address: available
DEBUG [main] node2 2023-02-25 00:15:23,137 PlatformDependent0.java:257 - direct buffer constructor: available
DEBUG [main] node2 2023-02-25 00:15:23,138 PlatformDependent0.java:331 - java.nio.Bits.unaligned: available, true
DEBUG [main] node2 2023-02-25 00:15:23,139 PlatformDependent0.java:393 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): available
DEBUG [main] node2 2023-02-25 00:15:23,140 PlatformDependent0.java:403 - java.nio.DirectByteBuffer.<init>(long, int): available
DEBUG [main] node2 2023-02-25 00:15:23,140 PlatformDependent.java:1079 - sun.misc.Unsafe: available
DEBUG [main] node2 2023-02-25 00:15:23,141 PlatformDependent.java:1181 - maxDirectMemory: 1056309248 bytes (maybe)
DEBUG [main] node2 2023-02-25 00:15:23,141 PlatformDependent.java:1200 - -Dio.netty.tmpdir: /home/cassandra/cassandra/tmp (java.io.tmpdir)
DEBUG [main] node2 2023-02-25 00:15:23,141 PlatformDependent.java:1279 - -Dio.netty.bitMode: 64 (sun.arch.data.model)
DEBUG [main] node2 2023-02-25 00:15:23,143 PlatformDependent.java:177 - -Dio.netty.maxDirectMemory: 1056309248 bytes
DEBUG [main] node2 2023-02-25 00:15:23,143 PlatformDependent.java:184 - -Dio.netty.uninitializedArrayAllocationThreshold: 1024
DEBUG [main] node2 2023-02-25 00:15:23,144 CleanerJava9.java:71 - java.nio.ByteBuffer.cleaner(): available
DEBUG [main] node2 2023-02-25 00:15:23,144 PlatformDependent.java:204 - -Dio.netty.noPreferDirect: false
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,251 DatabaseDescriptor.java:434 - Syncing log with batch mode
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,252 DatabaseDescriptor.java:466 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,256 DatabaseDescriptor.java:520 - Global memtable on-heap threshold is enabled at 10MiB
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,257 DatabaseDescriptor.java:524 - Global memtable off-heap threshold is enabled at 251MiB
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,257 DatabaseDescriptor.java:591 - Native transport rate-limiting disabled.
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,546 SimpleSeedProvider.java:99 - Only resolving one IP per DNS record - 127.0.0.1:7012 resolves to /127.0.0.1:7012
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,562 SSLFactory.java:251 - Initializing hot reloading SSLContext
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,620 CassandraDaemon.java:637 - Hostname: b849a9e1fa8a:7012:7001
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,621 CassandraDaemon.java:644 - JVM vendor/version: OpenJDK 64-Bit Server VM/11.0.17
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,623 CassandraDaemon.java:645 - Heap size: 77.500MiB/1007.375MiB
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,624 CassandraDaemon.java:650 - CodeHeap 'non-nmethods' Non-heap memory: init = 2555904(2496K) used = 1332480(1301K) committed = 2555904(2496K) max = 5828608(5692K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,632 CassandraDaemon.java:650 - Metaspace Non-heap memory: init = 0(0K) used = 36712928(35852K) committed = 38060032(37168K) max = -1(-1K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,632 CassandraDaemon.java:650 - CodeHeap 'profiled nmethods' Non-heap memory: init = 2555904(2496K) used = 5426048(5298K) committed = 5439488(5312K) max = 122912768(120032K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,632 CassandraDaemon.java:650 - Compressed Class Space Non-heap memory: init = 0(0K) used = 4344736(4242K) committed = 4849664(4736K) max = 1073741824(1048576K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,633 CassandraDaemon.java:650 - Par Eden Space Heap memory: init = 22413312(21888K) used = 22544384(22016K) committed = 22544384(22016K) max = 139591680(136320K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,633 CassandraDaemon.java:650 - Par Survivor Space Heap memory: init = 2752512(2688K) used = 2752512(2688K) committed = 2752512(2688K) max = 17432576(17024K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,633 CassandraDaemon.java:650 - CodeHeap 'non-profiled nmethods' Non-heap memory: init = 2555904(2496K) used = 1390464(1357K) committed = 2555904(2496K) max = 122916864(120036K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,633 CassandraDaemon.java:650 - CMS Old Gen Heap memory: init = 55967744(54656K) used = 17377336(16970K) committed = 55967744(54656K) max = 899284992(878208K)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,634 CassandraDaemon.java:652 - Classpath: /usr/share/ant/lib/ant-launcher.jar:/usr/share/ant/lib/ant-antlr.jar:/usr/share/ant/lib/ant-apache-bcel.jar:/usr/share/ant/lib/ant-apache-bsf.jar:/usr/share/ant/lib/ant-apache-log4j.jar:/usr/share/ant/lib/ant-apache-oro.jar:/usr/share/ant/lib/ant-apache-regexp.jar:/usr/share/ant/lib/ant-apache-resolver.jar:/usr/share/ant/lib/ant-apache-xalan2.jar:/usr/share/ant/lib/ant-commons-logging.jar:/usr/share/ant/lib/ant-commons-net.jar:/usr/share/ant/lib/ant-javamail.jar:/usr/share/ant/lib/ant-jdepend.jar:/usr/share/ant/lib/ant-jmf.jar:/usr/share/ant/lib/ant-jsch.jar:/usr/share/ant/lib/ant-junit.jar:/usr/share/ant/lib/ant-junit4.jar:/usr/share/ant/lib/ant-swing.jar:/usr/share/ant/lib/ant-testutil.jar:/usr/share/ant/lib/ant-xz.jar:/usr/share/ant/lib/ant.jar:/home/cassandra/cassandra/build/classes/stress:/home/cassandra/cassandra/build/classes/fqltool:/home/cassandra/cassandra/build/test/classes:/home/cassandra/cassandra/build/apache-cassandra-4.2-SNAPSHOT.jar:/home/cassandra/cassandra/build/lib/jars/HdrHistogram-2.1.9.jar:/home/cassandra/cassandra/build/lib/jars/ST4-4.0.8.jar:/home/cassandra/cassandra/build/lib/jars/agrona-1.17.1.jar:/home/cassandra/cassandra/build/lib/jars/airline-0.8.jar:/home/cassandra/cassandra/build/lib/jars/antlr-3.5.2.jar:/home/cassandra/cassandra/build/lib/jars/antlr-runtime-3.5.2.jar:/home/cassandra/cassandra/build/lib/jars/asm-9.3.jar:/home/cassandra/cassandra/build/lib/jars/assertj-core-3.15.0.jar:/home/cassandra/cassandra/build/lib/jars/big-math-2.3.0.jar:/home/cassandra/cassandra/build/lib/jars/byteman-4.0.20.jar:/home/cassandra/cassandra/build/lib/jars/byteman-bmunit-4.0.20.jar:/home/cassandra/cassandra/build/lib/jars/byteman-install-4.0.20.jar:/home/cassandra/cassandra/build/lib/jars/byteman-submit-4.0.20.jar:/home/cassandra/cassandra/build/lib/jars/caffeine-2.9.2.jar:/home/cassandra/cassandra/build/lib/jars/cassandra-driver-core-3.11.0-shaded.jar:/home/cassandra/cassandra/build/lib/jars/checker-qual-3.10.0.jar:/home/cassandra/cassandra/build/lib/jars/chronicle-bytes-2.20.111.jar:/home/cassandra/cassandra/build/lib/jars/chronicle-core-2.20.126.jar:/home/cassandra/cassandra/build/lib/jars/chronicle-queue-5.20.123.jar:/home/cassandra/cassandra/build/lib/jars/chronicle-threads-2.20.111.jar:/home/cassandra/cassandra/build/lib/jars/chronicle-wire-2.20.117.jar:/home/cassandra/cassandra/build/lib/jars/commons-beanutils-1.7.0.jar:/home/cassandra/cassandra/build/lib/jars/commons-beanutils-core-1.8.0.jar:/home/cassandra/cassandra/build/lib/jars/commons-cli-1.1.jar:/home/cassandra/cassandra/build/lib/jars/commons-codec-1.9.jar:/home/cassandra/cassandra/build/lib/jars/commons-collections-3.2.1.jar:/home/cassandra/cassandra/build/lib/jars/commons-configuration-1.6.jar:/home/cassandra/cassandra/build/lib/jars/commons-digester-1.8.jar:/home/cassandra/cassandra/build/lib/jars/commons-el-1.0.jar:/home/cassandra/cassandra/build/lib/jars/commons-httpclient-3.0.1.jar:/home/cassandra/cassandra/build/lib/jars/commons-lang3-3.11.jar:/home/cassandra/cassandra/build/lib/jars/commons-math-2.1.jar:/home/cassandra/cassandra/build/lib/jars/commons-math3-3.2.jar:/home/cassandra/cassandra/build/lib/jars/commons-net-1.4.1.jar:/home/cassandra/cassandra/build/lib/jars/compile-command-annotations-1.2.0.jar:/home/cassandra/cassandra/build/lib/jars/compress-lzf-0.8.4.jar:/home/cassandra/cassandra/build/lib/jars/concurrent-trees-2.4.0.jar:/home/cassandra/cassandra/build/lib/jars/ecj-4.6.1.jar:/home/cassandra/cassandra/build/lib/jars/error_prone_annotations-2.5.1.jar:/home/cassandra/cassandra/build/lib/jars/ftplet-api-1.0.0.jar:/home/cassandra/cassandra/build/lib/jars/ftpserver-core-1.0.0.jar:/home/cassandra/cassandra/build/lib/jars/ftpserver-deprecated-1.0.0-M2.jar:/home/cassandra/cassandra/build/lib/jars/guava-27.0-jre.jar:/home/cassandra/cassandra/build/lib/jars/hadoop-core-1.0.3.jar:/home/cassandra/cassandra/build/lib/jars/hadoop-minicluster-1.0.3.jar:/home/cassandra/cassandra/build/lib/jars/hadoop-test-1.0.3.jar:/home/cassandra/cassandra/build/lib/jars/high-scale-lib-1.0.6.jar:/home/cassandra/cassandra/build/lib/jars/hppc-0.8.1.jar:/home/cassandra/cassandra/build/lib/jars/hsqldb-1.8.0.10.jar:/home/cassandra/cassandra/build/lib/jars/ipaddress-5.3.3.jar:/home/cassandra/cassandra/build/lib/jars/j2objc-annotations-1.3.jar:/home/cassandra/cassandra/build/lib/jars/jackson-annotations-2.13.2.jar:/home/cassandra/cassandra/build/lib/jars/jackson-core-2.13.2.jar:/home/cassandra/cassandra/build/lib/jars/jackson-databind-2.13.2.2.jar:/home/cassandra/cassandra/build/lib/jars/jackson-datatype-jsr310-2.13.2.jar:/home/cassandra/cassandra/build/lib/jars/jacocoagent.jar:/home/cassandra/cassandra/build/lib/jars/jamm-0.3.2.jar:/home/cassandra/cassandra/build/lib/jars/jasper-compiler-5.5.12.jar:/home/cassandra/cassandra/build/lib/jars/jasper-runtime-5.5.12.jar:/home/cassandra/cassandra/build/lib/jars/java-cup-runtime-11b-20160615.jar:/home/cassandra/cassandra/build/lib/jars/javax.inject-1.jar:/home/cassandra/cassandra/build/lib/jars/jbcrypt-0.4.jar:/home/cassandra/cassandra/build/lib/jars/jcl-over-slf4j-1.7.25.jar:/home/cassandra/cassandra/build/lib/jars/jcommander-1.30.jar:/home/cassandra/cassandra/build/lib/jars/jctools-core-3.1.0.jar:/home/cassandra/cassandra/build/lib/jars/jersey-core-1.0.jar:/home/cassandra/cassandra/build/lib/jars/jersey-server-1.0.jar:/home/cassandra/cassandra/build/lib/jars/jets3t-0.7.1.jar:/home/cassandra/cassandra/build/lib/jars/jetty-6.1.26.jar:/home/cassandra/cassandra/build/lib/jars/jetty-util-6.1.26.jar:/home/cassandra/cassandra/build/lib/jars/jflex-1.8.2.jar:/home/cassandra/cassandra/build/lib/jars/jna-5.13.0.jar:/home/cassandra/cassandra/build/lib/jars/json-simple-1.1.jar:/home/cassandra/cassandra/build/lib/jars/jsp-2.1-6.1.14.jar:/home/cassandra/cassandra/build/lib/jars/jsp-api-2.1-6.1.14.jar:/home/cassandra/cassandra/build/lib/jars/jsr305-2.0.2.jar:/home/cassandra/cassandra/build/lib/jars/jsr311-api-1.0.jar:/home/cassandra/cassandra/build/lib/jars/jvm-attach-api-1.5.jar:/home/cassandra/cassandra/build/lib/jars/kfs-0.3.jar:/home/cassandra/cassandra/build/lib/jars/log4j-over-slf4j-1.7.25.jar:/home/cassandra/cassandra/build/lib/jars/logback-classic-1.2.9.jar:/home/cassandra/cassandra/build/lib/jars/logback-core-1.2.9.jar:/home/cassandra/cassandra/build/lib/jars/lz4-java-1.8.0.jar:/home/cassandra/cassandra/build/lib/jars/metrics-core-3.1.5.jar:/home/cassandra/cassandra/build/lib/jars/metrics-jvm-3.1.5.jar:/home/cassandra/cassandra/build/lib/jars/metrics-logback-3.1.5.jar:/home/cassandra/cassandra/build/lib/jars/mina-core-2.0.0-M5.jar:/home/cassandra/cassandra/build/lib/jars/mxdump-0.14.jar:/home/cassandra/cassandra/build/lib/jars/netty-all-4.1.58.Final.jar:/home/cassandra/cassandra/build/lib/jars/netty-tcnative-boringssl-static-2.0.36.Final.jar:/home/cassandra/cassandra/build/lib/jars/ohc-core-0.5.1.jar:/home/cassandra/cassandra/build/lib/jars/ohc-core-j8-0.5.1.jar:/home/cassandra/cassandra/build/lib/jars/oro-2.0.8.jar:/home/cassandra/cassandra/build/lib/jars/psjava-0.1.19.jar:/home/cassandra/cassandra/build/lib/jars/reporter-config-base-3.0.3.jar:/home/cassandra/cassandra/build/lib/jars/reporter-config3-3.0.3.jar:/home/cassandra/cassandra/build/lib/jars/servlet-api-2.5-6.1.14.jar:/home/cassandra/cassandra/build/lib/jars/sigar-1.6.4.jar:/home/cassandra/cassandra/build/lib/jars/sjk-cli-0.14.jar:/home/cassandra/cassandra/build/lib/jars/sjk-core-0.14.jar:/home/cassandra/cassandra/build/lib/jars/sjk-json-0.14.jar:/home/cassandra/cassandra/build/lib/jars/sjk-stacktrace-0.14.jar:/home/cassandra/cassandra/build/lib/jars/slf4j-api-1.7.25.jar:/home/cassandra/cassandra/build/lib/jars/snakeyaml-1.26.jar:/home/cassandra/cassandra/build/lib/jars/snappy-java-1.1.8.4.jar:/home/cassandra/cassandra/build/lib/jars/snowball-stemmer-1.3.0.581.1.jar:/home/cassandra/cassandra/build/lib/jars/stream-2.5.2.jar:/home/cassandra/cassandra/build/lib/jars/xmlenc-0.52.jar:/home/cassandra/cassandra/build/lib/jars/zstd-jni-1.5.4-1.jar:/home/cassandra/cassandra/build/test/lib/jars/Saxon-HE-10.3.jar:/home/cassandra/cassandra/build/test/lib/jars/antlr-2.7.7.jar:/home/cassandra/cassandra/build/test/lib/jars/antlr4-runtime-4.9.1.jar:/home/cassandra/cassandra/build/test/lib/jars/asm-9.3.jar:/home/cassandra/cassandra/build/test/lib/jars/asm-analysis-9.3.jar:/home/cassandra/cassandra/build/test/lib/jars/asm-commons-9.3.jar:/home/cassandra/cassandra/build/test/lib/jars/asm-tree-9.3.jar:/home/cassandra/cassandra/build/test/lib/jars/asm-util-9.3.jar:/home/cassandra/cassandra/build/test/lib/jars/asm-xml-6.0.jar:/home/cassandra/cassandra/build/test/lib/jars/awaitility-4.0.3.jar:/home/cassandra/cassandra/build/test/lib/jars/byte-buddy-1.12.13.jar:/home/cassandra/cassandra/build/test/lib/jars/byte-buddy-agent-1.12.13.jar:/home/cassandra/cassandra/build/test/lib/jars/checkstyle-8.40.jar:/home/cassandra/cassandra/build/test/lib/jars/commons-beanutils-1.9.4.jar:/home/cassandra/cassandra/build/test/lib/jars/commons-collections-3.2.2.jar:/home/cassandra/cassandra/build/test/lib/jars/commons-io-2.6.jar:/home/cassandra/cassandra/build/test/lib/jars/commons-logging-1.2.jar:/home/cassandra/cassandra/build/test/lib/jars/commons-math3-3.2.jar:/home/cassandra/cassandra/build/test/lib/jars/dtest-api-0.0.13.jar:/home/cassandra/cassandra/build/test/lib/jars/guava-18.0.jar:/home/cassandra/cassandra/build/test/lib/jars/hamcrest-2.2.jar:/home/cassandra/cassandra/build/test/lib/jars/harry-core-0.0.1.jar:/home/cassandra/cassandra/build/test/lib/jars/jackson-annotations-2.11.3.jar:/home/cassandra/cassandra/build/test/lib/jars/jackson-core-2.13.2.jar:/home/cassandra/cassandra/build/test/lib/jars/jackson-databind-2.11.3.jar:/home/cassandra/cassandra/build/test/lib/jars/jackson-dataformat-yaml-2.13.2.jar:/home/cassandra/cassandra/build/test/lib/jars/java-allocation-instrumenter-3.1.0.jar:/home/cassandra/cassandra/build/test/lib/jars/javassist-3.28.0-GA.jar:/home/cassandra/cassandra/build/test/lib/jars/jimfs-1.1.jar:/home/cassandra/cassandra/build/test/lib/jars/jmh-core-1.21.jar:/home/cassandra/cassandra/build/test/lib/jars/jmh-generator-annprocess-1.21.jar:/home/cassandra/cassandra/build/test/lib/jars/jopt-simple-4.6.jar:/home/cassandra/cassandra/build/test/lib/jars/jsr305-3.0.2.jar:/home/cassandra/cassandra/build/test/lib/jars/junit-4.12.jar:/home/cassandra/cassandra/build/test/lib/jars/mockito-core-4.7.0.jar:/home/cassandra/cassandra/build/test/lib/jars/mockito-inline-4.7.0.jar:/home/cassandra/cassandra/build/test/lib/jars/objenesis-3.2.jar:/home/cassandra/cassandra/build/test/lib/jars/org.jacoco.agent-0.8.8.jar:/home/cassandra/cassandra/build/test/lib/jars/org.jacoco.ant-0.8.8.jar:/home/cassandra/cassandra/build/test/lib/jars/org.jacoco.core-0.8.8.jar:/home/cassandra/cassandra/build/test/lib/jars/org.jacoco.report-0.8.8.jar:/home/cassandra/cassandra/build/test/lib/jars/picocli-4.6.1.jar:/home/cassandra/cassandra/build/test/lib/jars/quicktheories-0.26.jar:/home/cassandra/cassandra/build/test/lib/jars/reflections-0.10.2.jar:/home/cassandra/cassandra/build/test/lib/jars/semver4j-3.1.0.jar:/home/cassandra/cassandra/build/test/lib/jars/simulator-asm.jar:/home/cassandra/cassandra/build/test/lib/jars/simulator-bootstrap.jar:/home/cassandra/cassandra/build/test/lib/jars/slf4j-api-1.7.32.jar:/home/cassandra/cassandra/build/test/stress-classes:/home/cassandra/cassandra/build/test/fqltool-classes:/home/cassandra/cassandra/test/conf:/home/cassandra/cassandra/build/classes/main:/usr/share/java/ant-launcher-1.10.7.jar
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,634 CassandraDaemon.java:654 - JVM Arguments: [-Dstorage-config=/home/cassandra/cassandra/test/conf, -Djava.awt.headless=true, -javaagent:/home/cassandra/cassandra/lib/jamm-0.3.2.jar, -ea, -Djava.io.tmpdir=/home/cassandra/cassandra/tmp, -Dcassandra.debugrefcount=true, -Xss384k, -XX:SoftRefLRUPolicyMSPerMB=0, -XX:ActiveProcessorCount=2, -XX:HeapDumpPath=build/test, -Dcassandra.test.driver.connection_timeout_ms=10000, -Dcassandra.test.driver.read_timeout_ms=24000, -Dcassandra.memtable_row_overhead_computation_step=100, -Dcassandra.test.use_prepared=true, -Dcassandra.test.sstableformatdevelopment=true, -Djava.security.egd=file:/dev/urandom, -Dcassandra.testtag=.jdk11, -Dcassandra.keepBriefBrief=${cassandra.keepBriefBrief}, -Dcassandra.strict.runtime.checks=true, -Dcassandra.reads.thresholds.coordinator.defensive_checks_enabled=true, -Dcassandra.test.flush_local_schema_changes=false, -Dcassandra.test.messagingService.nonGracefulShutdown=true, -Dcassandra.use_nix_recursive_delete=true, -Djdk.attach.allowAttachSelf=true, -XX:+UseConcMarkSweepGC, -XX:+CMSParallelRemarkEnabled, -XX:SurvivorRatio=8, -XX:MaxTenuringThreshold=1, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:CMSWaitDuration=10000, -XX:+CMSParallelInitialMarkEnabled, -XX:+CMSEdenChunksRecordAlways, --add-exports=java.base/jdk.internal.misc=ALL-UNNAMED, --add-exports=java.base/jdk.internal.ref=ALL-UNNAMED, --add-exports=java.base/sun.nio.ch=ALL-UNNAMED, --add-exports=java.management.rmi/com.sun.jmx.remote.internal.rmi=ALL-UNNAMED, --add-exports=java.rmi/sun.rmi.registry=ALL-UNNAMED, --add-exports=java.rmi/sun.rmi.server=ALL-UNNAMED, --add-exports=java.sql/java.sql=ALL-UNNAMED, --add-opens=java.base/java.lang.module=ALL-UNNAMED, --add-opens=java.base/java.net=ALL-UNNAMED, --add-opens=java.base/jdk.internal.loader=ALL-UNNAMED, --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED, --add-opens=java.base/jdk.internal.reflect=ALL-UNNAMED, --add-opens=java.base/jdk.internal.math=ALL-UNNAMED, --add-opens=java.base/jdk.internal.module=ALL-UNNAMED, --add-opens=java.base/jdk.internal.util.jar=ALL-UNNAMED, --add-opens=jdk.management/com.sun.management.internal=ALL-UNNAMED, -DQT_SHRINKS=0, -XX:-CMSClassUnloadingEnabled, -Dio.netty.tryReflectionSetAccessible=true, -Dlegacy-sstable-root=/home/cassandra/cassandra/test/data/legacy-sstables, -Dinvalid-legacy-sstable-root=/home/cassandra/cassandra/test/data/invalid-legacy-sstables, -Dcassandra.ring_delay_ms=1000, -Dcassandra.tolerate_sstable_size=true, -Dcassandra.skip_sync=true, -Xmx1024m]
INFO  [node1_ScheduledTasks:1] node1 2023-02-25 00:15:23,662 StorageService.java:195 - Overriding RING_DELAY to 15000ms
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,666 MonotonicClock.java:208 - Scheduling approximate time conversion task with an interval of 10000 milliseconds
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,667 MonotonicClock.java:344 - Scheduling approximate time-check task with a precision of 2 milliseconds
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,708 InternalThreadLocalMap.java:83 - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,708 InternalThreadLocalMap.java:86 - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
WARN  00:15:23 jemalloc shared library could not be preloaded to speed up memory allocations
WARN  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,815 StartupChecks.java:200 - jemalloc shared library could not be preloaded to speed up memory allocations
WARN  00:15:23 JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.
WARN  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,824 StartupChecks.java:258 - JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.
WARN  00:15:23 The JVM is not configured to stop on OutOfMemoryError which can cause data corruption. Use one of the following JVM options to configure the behavior on OutOfMemoryError:  -XX:+ExitOnOutOfMemoryError, -XX:+CrashOnOutOfMemoryError, or -XX:OnOutOfMemoryError=""<cmd args>;<cmd args>""
WARN  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,825 StartupChecks.java:315 - The JVM is not configured to stop on OutOfMemoryError which can cause data corruption. Use one of the following JVM options to configure the behavior on OutOfMemoryError:  -XX:+ExitOnOutOfMemoryError, -XX:+CrashOnOutOfMemoryError, or -XX:OnOutOfMemoryError=""<cmd args>;<cmd args>""
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,827 SigarLibrary.java:46 - Initializing SIGAR library
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,841 SigarLog.java:60 - no libsigar-amd64-linux.so in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
org.hyperic.sigar.SigarException: no libsigar-amd64-linux.so in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
	at org.hyperic.sigar.Sigar.loadLibrary(Sigar.java:172)
	at org.hyperic.sigar.Sigar.<clinit>(Sigar.java:100)
	at org.apache.cassandra.utils.SigarLibrary.<init>(SigarLibrary.java:49)
	at org.apache.cassandra.utils.SigarLibrary.<clinit>(SigarLibrary.java:30)
	at org.apache.cassandra.service.StartupChecks$8.execute(StartupChecks.java:367)
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:175)
	at org.apache.cassandra.service.CassandraDaemon.runStartupChecks(CassandraDaemon.java:494)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$12(Instance.java:602)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,843 SigarLibrary.java:59 - Could not initialize SIGAR library 'org.hyperic.sigar.FileSystem[] org.hyperic.sigar.Sigar.getFileSystemListNative()' 
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,843 SigarLibrary.java:183 - Sigar could not be initialized, test for checking degraded mode omitted.
WARN  00:15:23 Maximum number of memory map areas per process (vm.max_map_count) 65530 is too low, recommended value: 1048575, you can change it with sysctl.
WARN  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,844 StartupChecks.java:493 - Maximum number of memory map areas per process (vm.max_map_count) 65530 is too low, recommended value: 1048575, you can change it with sysctl.
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,846 StartupChecks.java:425 - No 'read_ahead_kb' setting found for device overlay of data directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data2.
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,855 StartupChecks.java:513 - Checking directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,860 StartupChecks.java:513 - Checking directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data1
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,861 StartupChecks.java:513 - Checking directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,861 StartupChecks.java:513 - Checking directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/commitlog
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,861 StartupChecks.java:513 - Checking directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/saved_caches
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:23,862 StartupChecks.java:513 - Checking directory /home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/hints
WARN  00:15:24 epoll not available
java.lang.UnsupportedOperationException: Native transport was explicit disabled with -Dio.netty.transport.noNative=true
	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33)
	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:165)
	at org.apache.cassandra.net.SocketFactory$Provider.optimalProvider(SocketFactory.java:164)
	at org.apache.cassandra.net.SocketFactory.<init>(SocketFactory.java:185)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:261)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:288)
	at org.apache.cassandra.net.MessagingService$MSHandle.<clinit>(MessagingService.java:253)
	at org.apache.cassandra.net.MessagingService.instance(MessagingService.java:258)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.<init>(DefaultSchemaUpdateHandler.java:81)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandlerFactory.getSchemaUpdateHandler(DefaultSchemaUpdateHandlerFactory.java:33)
	at org.apache.cassandra.schema.Schema.<init>(Schema.java:114)
	at org.apache.cassandra.schema.Schema.<clinit>(Schema.java:78)
	at org.apache.cassandra.service.StartupChecks$13.execute(StartupChecks.java:660)
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:175)
	at org.apache.cassandra.service.CassandraDaemon.runStartupChecks(CassandraDaemon.java:494)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$12(Instance.java:602)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
WARN  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,398 NativeTransportService.java:166 - epoll not available
java.lang.UnsupportedOperationException: Native transport was explicit disabled with -Dio.netty.transport.noNative=true
	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:33)
	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:165)
	at org.apache.cassandra.net.SocketFactory$Provider.optimalProvider(SocketFactory.java:164)
	at org.apache.cassandra.net.SocketFactory.<init>(SocketFactory.java:185)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:261)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:288)
	at org.apache.cassandra.net.MessagingService$MSHandle.<clinit>(MessagingService.java:253)
	at org.apache.cassandra.net.MessagingService.instance(MessagingService.java:258)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.<init>(DefaultSchemaUpdateHandler.java:81)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandlerFactory.getSchemaUpdateHandler(DefaultSchemaUpdateHandlerFactory.java:33)
	at org.apache.cassandra.schema.Schema.<init>(Schema.java:114)
	at org.apache.cassandra.schema.Schema.<clinit>(Schema.java:78)
	at org.apache.cassandra.service.StartupChecks$13.execute(StartupChecks.java:660)
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:175)
	at org.apache.cassandra.service.CassandraDaemon.runStartupChecks(CassandraDaemon.java:494)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$12(Instance.java:602)
	at org.apache.cassandra.concurrent.FutureTask$1.call(FutureTask.java:96)
	at org.apache.cassandra.concurrent.FutureTask.call(FutureTask.java:61)
	at org.apache.cassandra.concurrent.FutureTask.run(FutureTask.java:71)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,398 SocketFactory.java:154 - using netty NIO event loop for pool prefix Messaging-AcceptLoop
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,405 MultithreadEventLoopGroup.java:44 - -Dio.netty.eventLoopThreads: 4
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,436 NioEventLoop.java:106 - -Dio.netty.noKeySetOptimization: false
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,436 NioEventLoop.java:107 - -Dio.netty.selectorAutoRebuildThreshold: 512
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,444 SocketFactory.java:154 - using netty NIO event loop for pool prefix node1_Messaging-EventLoop
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,445 SocketFactory.java:154 - using netty NIO event loop for pool prefix Streaming-EventLoop
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,591 Keyspace.java:367 - Creating replication strategy system params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.LocalStrategy}}
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,602 Keyspace.java:371 - New replication settings for keyspace system - invalidating disk boundary caches
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,632 ColumnFamilyStore.java:489 - Initializing system.IndexInfo
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,637 AbstractAllocatorMemtable.java:91 - Memtables allocating with on-heap slabs
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,715 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.IndexInfo
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,724 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,726 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.IndexInfo
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,788 ColumnFamilyStore.java:489 - Initializing system.batches
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,796 ColumnFamilyStore.java:489 - Initializing system.paxos
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,797 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.paxos
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,797 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,797 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data1}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data2}], positions=null, ringVersion=0, directoriesVersion=0} for system.paxos
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,806 SecondaryIndexManager.java:210 - Index [PaxosUncommittedIndex] registered and writable.
INFO  [node1_SecondaryIndexManagement:1] node1 2023-02-25 00:15:24,817 SecondaryIndexManager.java:665 - Index [PaxosUncommittedIndex] became queryable after successful build.
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,818 ColumnFamilyStore.java:489 - Initializing system.paxos_repair_history
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,818 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.paxos_repair_history
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,819 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,819 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.paxos_repair_history
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,824 ColumnFamilyStore.java:489 - Initializing system.local
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,824 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.local
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,825 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,825 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.local
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,830 ColumnFamilyStore.java:489 - Initializing system.peers_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,831 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.peers_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,831 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,831 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.peers_v2
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,835 ColumnFamilyStore.java:489 - Initializing system.peers
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,836 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.peers
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,836 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,836 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.peers
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,840 ColumnFamilyStore.java:489 - Initializing system.peer_events_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,841 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.peer_events_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,841 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,841 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.peer_events_v2
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,846 ColumnFamilyStore.java:489 - Initializing system.peer_events
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,846 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.peer_events
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,847 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,847 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.peer_events
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,853 ColumnFamilyStore.java:489 - Initializing system.compaction_history
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,853 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.compaction_history
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,854 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,854 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data1}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data2}], positions=null, ringVersion=0, directoriesVersion=0} for system.compaction_history
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,859 ColumnFamilyStore.java:489 - Initializing system.sstable_activity
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,860 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.sstable_activity
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,860 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,860 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.sstable_activity
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,866 ColumnFamilyStore.java:489 - Initializing system.sstable_activity_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,867 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.sstable_activity_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,867 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,867 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.sstable_activity_v2
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,884 ColumnFamilyStore.java:489 - Initializing system.size_estimates
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,885 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.size_estimates
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,885 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,885 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.size_estimates
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,890 ColumnFamilyStore.java:489 - Initializing system.table_estimates
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,890 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.table_estimates
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,890 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,891 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.table_estimates
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,895 ColumnFamilyStore.java:489 - Initializing system.available_ranges_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,896 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.available_ranges_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,896 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,896 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.available_ranges_v2
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,900 ColumnFamilyStore.java:489 - Initializing system.available_ranges
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,900 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.available_ranges
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,901 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,901 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.available_ranges
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,904 ColumnFamilyStore.java:489 - Initializing system.transferred_ranges_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,904 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.transferred_ranges_v2
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,904 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,904 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.transferred_ranges_v2
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,907 ColumnFamilyStore.java:489 - Initializing system.transferred_ranges
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,908 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.transferred_ranges
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,908 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,908 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.transferred_ranges
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,911 ColumnFamilyStore.java:489 - Initializing system.view_builds_in_progress
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,912 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.view_builds_in_progress
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,912 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,912 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.view_builds_in_progress
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,915 ColumnFamilyStore.java:489 - Initializing system.built_views
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,916 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.built_views
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,916 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,916 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}], positions=null, ringVersion=0, directoriesVersion=0} for system.built_views
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,920 ColumnFamilyStore.java:489 - Initializing system.prepared_statements
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,921 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.prepared_statements
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,921 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,921 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data1}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data2}], positions=null, ringVersion=0, directoriesVersion=0} for system.prepared_statements
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,926 ColumnFamilyStore.java:489 - Initializing system.repairs
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,927 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for system.repairs
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,927 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,927 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data0}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data1}, DataDirectory{location=/home/cassandra/cassandra/tmp/dtests8095700069672955140/node1/data2}], positions=null, ringVersion=0, directoriesVersion=0} for system.repairs
INFO  [node1_isolatedExecutor:1] node1 2023-02-25 00:15:24,931 ColumnFamilyStore.java:489 - Initializing system.top_partitions
DEBUG [node1_isolatedExecu
...[truncated 4094032 chars]...
erkle trees with merkle trees size 2, 387 partitions, 180 bytes
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,466 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl2, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,470 Validator.java:221 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -     [0..0]: 463
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -     [1..2]: 50
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -     [0..0]: 210
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -     [1..2]: 46
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 Validator.java:223 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -     [0..204]: 466
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -   [205..939]: 10
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -  [940..1674]: 4
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 - [1675..2409]: 12
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 - [2410..5226]: 21
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,471 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -     [0..378]: 213
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,471 EstimatedHistogram.java:337 -  [379..1316]: 11
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 - [1317..2254]: 9
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 - [2255..3192]: 13
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 -     [0..0]: 256
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 - [3193..4146]: 10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,472 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 -     [0..0]: 513
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,472 Validator.java:254 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Local completed merkle tree for /127.0.0.1:7012 for distributed_test_keyspace.tbl10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,472 EstimatedHistogram.java:337 -     [0..0]: 256
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,472 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,472 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl2
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,472 ValidationManager.java:152 - Validation of 99 partitions (~82.471KiB) finished in 21 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl10, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,472 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,472 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl10 from /127.0.0.1:7012
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,472 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,473 ValidationManager.java:152 - Validation of 0 partitions (~156.163KiB) finished in 12 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl2, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,473 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,473 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl5
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,473 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl2 from /127.0.0.2:7012
INFO  [node1_RepairJobTask:5] node1 2023-02-25 00:21:15,474 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 1ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,474 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,474 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl5, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,474 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl4
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,475 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,475 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl4, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 Validator.java:221 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -     [0..0]: 463
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -     [1..2]: 50
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -     [0..0]: 210
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -     [1..2]: 46
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 Validator.java:223 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -     [0..204]: 466
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -   [205..939]: 10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,485 EstimatedHistogram.java:337 -  [940..1674]: 4
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 - [1675..2409]: 12
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 - [2410..5226]: 21
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 -     [0..378]: 213
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 -  [379..1316]: 11
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 - [1317..2254]: 9
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 - [2255..3192]: 13
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 EstimatedHistogram.java:337 - [3193..4146]: 10
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,486 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl5
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,486 ValidationManager.java:152 - Validation of 99 partitions (~83.662KiB) finished in 13 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl5, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,487 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,487 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl3
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,487 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,487 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl5 from /127.0.0.2:7012
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,487 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
INFO  [node1_RepairJobTask:7] node1 2023-02-25 00:21:15,488 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 1ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,488 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl3, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,492 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,492 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,492 EstimatedHistogram.java:337 -     [0..0]: 256
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,492 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,492 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,492 EstimatedHistogram.java:337 -     [0..0]: 256
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,492 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl3
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,493 ValidationManager.java:152 - Validation of 0 partitions (~64.395KiB) finished in 6 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl3, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,493 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,493 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl8
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,493 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,494 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,494 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl3 from /127.0.0.2:7012
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,494 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl8, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node1_RepairJobTask:5] node1 2023-02-25 00:21:15,494 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 0ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,499 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,499 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,499 EstimatedHistogram.java:337 -     [0..0]: 256
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,499 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,500 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,500 EstimatedHistogram.java:337 -     [0..0]: 256
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,500 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl8
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,500 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,501 ValidationManager.java:152 - Validation of 0 partitions (~64.158KiB) finished in 7 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl8, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,501 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,501 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl8 from /127.0.0.2:7012
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,501 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,502 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,502 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl10, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,502 Validator.java:221 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 -     [0..0]: 463
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 -     [1..2]: 50
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 -     [0..0]: 210
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 -     [1..2]: 46
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 Validator.java:223 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 -     [0..315]: 467
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 -  [316..1458]: 10
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 - [1459..2601]: 3
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 - [2602..3744]: 12
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,503 EstimatedHistogram.java:337 - [3745..8113]: 21
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,504 EstimatedHistogram.java:337 -     [0..584]: 214
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,504 EstimatedHistogram.java:337 -  [585..2041]: 10
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,504 EstimatedHistogram.java:337 - [2042..3498]: 9
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,504 EstimatedHistogram.java:337 - [3499..4955]: 13
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,504 EstimatedHistogram.java:337 - [4956..6412]: 10
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,504 Validator.java:254 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Local completed merkle tree for /127.0.0.1:7012 for distributed_test_keyspace.tbl4
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,504 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,504 ValidationManager.java:152 - Validation of 99 partitions (~149.472KiB) finished in 32 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl4, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,504 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,504 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl4 from /127.0.0.1:7012
INFO  [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,506 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 0 sstables in distributed_test_keyspace.tbl13
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,506 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 0 partitions, 180 bytes
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,506 Validator.java:150 - Prepared AEService trees of size 2 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl13, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,506 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,507 EstimatedHistogram.java:337 -     [0..0]: 1
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,507 EstimatedHistogram.java:337 -     [0..0]: 1
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,507 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,507 EstimatedHistogram.java:337 -     [0..0]: 1
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,507 EstimatedHistogram.java:337 -     [0..0]: 1
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,507 Validator.java:254 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Local completed merkle tree for /127.0.0.1:7012 for distributed_test_keyspace.tbl13
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,507 MerkleTree.java:741 - Allocating direct buffer of size 114 for an off-heap merkle tree
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,507 ValidationManager.java:152 - Validation of 0 partitions (~0.000KiB) finished in 2 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl13, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,507 MerkleTree.java:741 - Allocating direct buffer of size 114 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,507 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl13 from /127.0.0.1:7012
INFO  [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,509 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 0 sstables in distributed_test_keyspace.tbl8
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 0 partitions, 180 bytes
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 Validator.java:150 - Prepared AEService trees of size 2 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl8, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 EstimatedHistogram.java:337 -     [0..0]: 1
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 EstimatedHistogram.java:337 -     [0..0]: 1
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 EstimatedHistogram.java:337 -     [0..0]: 1
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 EstimatedHistogram.java:337 -     [0..0]: 1
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,510 Validator.java:254 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Local completed merkle tree for /127.0.0.1:7012 for distributed_test_keyspace.tbl8
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,510 MerkleTree.java:741 - Allocating direct buffer of size 114 for an off-heap merkle tree
DEBUG [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,510 MerkleTree.java:741 - Allocating direct buffer of size 114 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,510 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl8 from /127.0.0.1:7012
DEBUG [node1_ValidationExecutor:1] node1 2023-02-25 00:21:15,510 ValidationManager.java:152 - Validation of 0 partitions (~0.000KiB) finished in 3 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl8, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node1_RepairJobTask:7] node1 2023-02-25 00:21:15,510 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 0ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,519 Validator.java:221 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -     [0..0]: 463
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -     [1..2]: 50
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -     [0..0]: 210
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -     [1..2]: 46
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 Validator.java:223 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -     [0..204]: 466
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -   [205..939]: 10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -  [940..1674]: 4
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 - [1675..2409]: 12
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 - [2410..5226]: 21
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -     [0..378]: 213
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 -  [379..1316]: 11
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 - [1317..2254]: 9
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 - [2255..3192]: 13
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,520 EstimatedHistogram.java:337 - [3193..4146]: 10
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,521 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,525 ValidationManager.java:152 - Validation of 99 partitions (~82.471KiB) finished in 24 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl10, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,526 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl4
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,526 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,526 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,526 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,527 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl10 from /127.0.0.2:7012
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,527 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl4, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node1_RepairJobTask:5] node1 2023-02-25 00:21:15,527 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 0ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,542 Validator.java:221 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -     [0..0]: 463
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -     [1..2]: 50
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -     [0..0]: 210
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -     [1..2]: 46
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 Validator.java:223 - Validated 99 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -     [0..315]: 467
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -  [316..1458]: 10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 - [1459..2601]: 3
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 - [2602..3744]: 12
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 - [3745..8113]: 21
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -     [0..584]: 214
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 -  [585..2041]: 10
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 - [2042..3498]: 9
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 - [3499..4955]: 13
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,543 EstimatedHistogram.java:337 - [4956..6412]: 10
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,543 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl4
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,544 ValidationManager.java:152 - Validation of 99 partitions (~149.472KiB) finished in 18 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl4, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,544 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,544 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl13
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,545 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,545 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,545 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl4 from /127.0.0.2:7012
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,545 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl13, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node1_RepairJobTask:7] node1 2023-02-25 00:21:15,545 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 0ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,548 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,548 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,548 EstimatedHistogram.java:337 -     [0..0]: 256
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,548 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,548 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,548 EstimatedHistogram.java:337 -     [0..0]: 256
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,549 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl13
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,549 ValidationManager.java:152 - Validation of 0 partitions (~64.006KiB) finished in 5 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl13, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,549 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
INFO  [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,549 CassandraValidationIterator.java:212 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905], parentSessionId=50de4900-b4a2-11ed-8341-618b49b30905: Performing validation compaction on 3 sstables in distributed_test_keyspace.tbl11
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,550 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,550 ValidationManager.java:85 - Created 2 merkle trees with merkle trees size 2, 387 partitions, 180 bytes
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,550 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl13 from /127.0.0.2:7012
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,550 Validator.java:150 - Prepared AEService trees of size 768 for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl11, [(-1,9223372036854775805], (9223372036854775805,-1]]]
INFO  [node1_RepairJobTask:5] node1 2023-02-25 00:21:15,550 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 0ms)
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 Validator.java:221 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partitions per leaf are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 EstimatedHistogram.java:337 -     [0..0]: 256
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 Validator.java:223 - Validated 0 partitions for 50debe30-b4a2-11ed-8341-618b49b30905.  Partition sizes are:
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 EstimatedHistogram.java:337 -     [0..0]: 513
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 EstimatedHistogram.java:337 -     [0..0]: 256
INFO  [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,554 Validator.java:249 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Sending completed merkle tree to /127.0.0.1:7012 for distributed_test_keyspace.tbl11
DEBUG [node2_ValidationExecutor:1] node2 2023-02-25 00:21:15,554 ValidationManager.java:152 - Validation of 0 partitions (~69.046KiB) finished in 5 msec, for [repair #50debe30-b4a2-11ed-8341-618b49b30905 on distributed_test_keyspace/tbl11, [(-1,9223372036854775805], (9223372036854775805,-1]]]
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,555 MerkleTree.java:741 - Allocating direct buffer of size 42016 for an off-heap merkle tree
DEBUG [node1_Messaging-EventLoop-3-1] node1 2023-02-25 00:21:15,555 MerkleTree.java:741 - Allocating direct buffer of size 21024 for an off-heap merkle tree
INFO  [node1_AntiEntropyStage:1] node1 2023-02-25 00:21:15,555 RepairSession.java:220 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Received merkle tree for tbl11 from /127.0.0.2:7012
INFO  [node1_RepairJobTask:7] node1 2023-02-25 00:21:15,556 RepairJob.java:331 - Created 0 sync tasks based on 2 merkle tree responses for 50de4900-b4a2-11ed-8341-618b49b30905 (took: 1ms)
INFO  [node1_Repair#51:1] node1 2023-02-25 00:21:15,556 RepairSession.java:333 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] Session completed successfully
INFO  [node1_Repair#51:1] node1 2023-02-25 00:21:15,556 RepairRunnable.java:175 - Repair session 50debe30-b4a2-11ed-8341-618b49b30905 for range [(-1,9223372036854775805], (9223372036854775805,-1]] finished
DEBUG [node1_Repair#51:1] node1 2023-02-25 00:21:15,556 AbstractRepairTask.java:96 - Repair result: [RepairSessionResult{sessionId=50debe30-b4a2-11ed-8341-618b49b30905, keyspace='distributed_test_keyspace', ranges=[(-1,9223372036854775805], (9223372036854775805,-1]], repairJobResults=[org.apache.cassandra.repair.RepairResult@37dc8ad2, org.apache.cassandra.repair.RepairResult@72754a8d, org.apache.cassandra.repair.RepairResult@61382c65, org.apache.cassandra.repair.RepairResult@604bcd66, org.apache.cassandra.repair.RepairResult@339251a7, org.apache.cassandra.repair.RepairResult@6beb547a, org.apache.cassandra.repair.RepairResult@4b100666, org.apache.cassandra.repair.RepairResult@750ec29f, org.apache.cassandra.repair.RepairResult@54b0aaf0, org.apache.cassandra.repair.RepairResult@65c96c98, org.apache.cassandra.repair.RepairResult@2ea17571, org.apache.cassandra.repair.RepairResult@2d385000, org.apache.cassandra.repair.RepairResult@7c0e887d, org.apache.cassandra.repair.RepairResult@528d86c8, org.apache.cassandra.repair.RepairResult@77a27b15], skippedReplicas=false}]
INFO  [node1_Repair#51:1] node1 2023-02-25 00:21:15,556 RepairRunnable.java:141 - Previewed data was in sync
INFO  [node1_Repair#51:1] node1 2023-02-25 00:21:15,557 RepairRunnable.java:223 - [preview repair #50de4900-b4a2-11ed-8341-618b49b30905]Repair command #51 finished in 0 seconds
DEBUG [node2_AntiEntropyStage:1] node2 2023-02-25 00:21:15,558 RepairMessageVerbHandler.java:229 - cleaning up repair
DEBUG [node1_Repair#51:1] node1 2023-02-25 00:21:15,559 RepairSession.java:382 - [preview repair #50debe30-b4a2-11ed-8341-618b49b30905] session task executor shut down gracefully
INFO  [node2_isolatedExecutor:5] node2 2023-02-25 00:21:15,811 Gossiper.java:2159 - Announcing shutdown
DEBUG [node2_isolatedExecutor:5] node2 2023-02-25 00:21:15,813 StorageService.java:3025 - Node /127.0.0.2:7012 state shutdown, token [9223372036854775805]
INFO  [node2_isolatedExecutor:5] node2 2023-02-25 00:21:15,813 StorageService.java:3028 - Node /127.0.0.2:7012 state jump to shutdown
DEBUG [node2_isolatedExecutor:5] node2 2023-02-25 00:21:15,814 StorageService.java:3025 - Node /127.0.0.2:7012 state shutdown, token [9223372036854775805]
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,814 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,814 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 0ms)
INFO  [node2_isolatedExecutor:5] node2 2023-02-25 00:21:15,814 StorageService.java:3028 - Node /127.0.0.2:7012 state jump to shutdown
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,814 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,814 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:876 - Starting pending range calculation for distributed_test_keyspace
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:881 - Pending range calculation for distributed_test_keyspace completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:876 - Starting pending range calculation for distributed_test_keyspace
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:15,815 TokenMetadata.java:881 - Pending range calculation for distributed_test_keyspace completed (took: 0ms)
INFO  [node1_GossipStage:1] node1 2023-02-25 00:21:15,816 Gossiper.java:1382 - InetAddress /127.0.0.2:7012 is now DOWN
INFO  [node2_isolatedExecutor:7] node2 2023-02-25 00:21:15,817 HintsService.java:235 - Paused hints dispatch
INFO  [node1_GossipStage:1] node1 2023-02-25 00:21:15,817 MessagingService.java:501 - Interrupted outbound connections to /127.0.0.2:7012
DEBUG [node1_GossipStage:1] node1 2023-02-25 00:21:15,818 FailureDetector.java:370 - Forcing conviction of /127.0.0.2:7012
INFO  [node1_isolatedExecutor:6] node1 2023-02-25 00:21:15,818 Gossiper.java:2159 - Announcing shutdown
DEBUG [node1_isolatedExecutor:6] node1 2023-02-25 00:21:15,818 StorageService.java:3025 - Node /127.0.0.1:7012 state shutdown, token [-1]
INFO  [node1_isolatedExecutor:6] node1 2023-02-25 00:21:15,818 StorageService.java:3028 - Node /127.0.0.1:7012 state jump to shutdown
DEBUG [node1_GossipStage:1] node1 2023-02-25 00:21:15,819 StorageService.java:3025 - Node /127.0.0.2:7012 state shutdown, token [9223372036854775805]
INFO  [node1_GossipStage:1] node1 2023-02-25 00:21:15,819 StorageService.java:3028 - Node /127.0.0.2:7012 state jump to shutdown
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,819 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 1ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:876 - Starting pending range calculation for distributed_test_keyspace
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,820 TokenMetadata.java:881 - Pending range calculation for distributed_test_keyspace completed (took: 0ms)
DEBUG [node1_isolatedExecutor:6] node1 2023-02-25 00:21:15,820 StorageService.java:3025 - Node /127.0.0.1:7012 state shutdown, token [-1]
INFO  [node1_isolatedExecutor:6] node1 2023-02-25 00:21:15,820 StorageService.java:3028 - Node /127.0.0.1:7012 state jump to shutdown
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:876 - Starting pending range calculation for distributed_test_keyspace
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,826 TokenMetadata.java:881 - Pending range calculation for distributed_test_keyspace completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:876 - Starting pending range calculation for distributed_test_keyspace
DEBUG [node1_PendingRangeCalculator:1] node1 2023-02-25 00:21:15,827 TokenMetadata.java:881 - Pending range calculation for distributed_test_keyspace completed (took: 0ms)
DEBUG [node1_GossipStage:1] node1 2023-02-25 00:21:15,827 Gossiper.java:608 - Marked /127.0.0.2:7012 as shutdown
INFO  [node1_isolatedExecutor:5] node1 2023-02-25 00:21:15,830 HintsService.java:235 - Paused hints dispatch
INFO  [node2_Messaging-EventLoop-3-2] node2 2023-02-25 00:21:16,356 InboundConnectionInitiator.java:567 - /127.0.0.1:7012(/127.0.0.1:39098)->/127.0.0.2:7012-URGENT_MESSAGES-35ea5742 messaging connection established, version = 12, framing = CRC, encryption = unencrypted
INFO  [node1_Messaging-EventLoop-3-2] node1 2023-02-25 00:21:16,356 OutboundConnection.java:1155 - /127.0.0.1:7012(/127.0.0.1:39098)->/127.0.0.2:7012-URGENT_MESSAGES-b7abeff9 successfully connected, version = 12, framing = CRC, encryption = unencrypted
DEBUG [node2_GossipStage:1] node2 2023-02-25 00:21:16,360 StorageService.java:3025 - Node /127.0.0.1:7012 state shutdown, token [-1]
INFO  [node2_GossipStage:1] node2 2023-02-25 00:21:16,360 StorageService.java:3028 - Node /127.0.0.1:7012 state jump to shutdown
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,365 TokenMetadata.java:876 - Starting pending range calculation for system_traces
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,365 TokenMetadata.java:881 - Pending range calculation for system_traces completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,366 TokenMetadata.java:876 - Starting pending range calculation for system_distributed
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,366 TokenMetadata.java:881 - Pending range calculation for system_distributed completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,366 TokenMetadata.java:876 - Starting pending range calculation for system_auth
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,366 TokenMetadata.java:881 - Pending range calculation for system_auth completed (took: 0ms)
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,366 TokenMetadata.java:876 - Starting pending range calculation for distributed_test_keyspace
DEBUG [node2_PendingRangeCalculator:1] node2 2023-02-25 00:21:16,366 TokenMetadata.java:881 - Pending range calculation for distributed_test_keyspace completed (took: 0ms)
DEBUG [node2_isolatedExecutor:5] node2 2023-02-25 00:21:17,817 MessagingService.java:540 - Shutting down: timeout=60s, gracefully=false, shutdownExecutors=true
INFO  [node2_isolatedExecutor:5] node2 2023-02-25 00:21:17,817 MessagingService.java:548 - Waiting for messaging service to quiesce
DEBUG [node1_isolatedExecutor:6] node1 2023-02-25 00:21:17,827 MessagingService.java:540 - Shutting down: timeout=60s, gracefully=false, shutdownExecutors=true
INFO  [node1_isolatedExecutor:6] node1 2023-02-25 00:21:17,827 MessagingService.java:548 - Waiting for messaging service to quiesce
DEBUG [node2_COMMIT-LOG-WRITER] node2 2023-02-25 00:21:19,837 HeapUtils.java:134 - Heap dump creation on uncaught exceptions is disabled.
DEBUG [node1_COMMIT-LOG-WRITER] node1 2023-02-25 00:21:19,864 HeapUtils.java:134 - Heap dump creation on uncaught exceptions is disabled.
{noformat}"
CASSANDRA-18287,Test Failure: InsertUpdateIfConditionTest.testMultiExistConditionOnSameRowClustering,"from
- https://ci-cassandra.apache.org/job/Cassandra-trunk/1467/testReport/org.apache.cassandra.cql3.validation.operations/InsertUpdateIfConditionTest/testMultiExistConditionOnSameRowClustering_0__clusterMinVersion_3_0__cdc_jdk11_2/
- https://ci-cassandra.apache.org/job/Cassandra-trunk-test-cdc/1547/jdk=jdk_11_latest,label=cassandra,split=7/testReport/junit/org.apache.cassandra.cql3.validation.operations/InsertUpdateIfConditionTest/testMultiExistConditionOnSameRowClustering_0__clusterMinVersion_3_0__cdc_jdk11/

Stacktrace
{noformat}
junit.framework.AssertionFailedError: 4.2.0-SNAPSHOT boolean:false
	at org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.lambda$data$0(InsertUpdateIfConditionTest.java:66)
	at org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.beforeSetup(InsertUpdateIfConditionTest.java:95)
	at org.apache.cassandra.cql3.validation.operations.InsertUpdateIfConditionTest.before(InsertUpdateIfConditionTest.java:89)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}
Standard Output
{noformat}
INFO  [main] 2023-02-21 11:54:47,699 YamlConfigurationLoader.java:104 - Configuration location: file:////home/cassandra/cassandra/build/test/cassandra.cdc.yaml
DEBUG [main] 2023-02-21 11:54:47,703 YamlConfigurationLoader.java:124 - Loading settings from file:////home/cassandra/cassandra/build/test/cassandra.cdc.yaml
WARN  [main] 2023-02-21 11:54:47,951 YamlConfigurationLoader.java:427 - [scripted_user_defined_functions_enabled] parameters have been deprecated. They have new names and/or value format; For more information, please refer to NEWS.txt
INFO  [main] 2023-02-21 11:54:48,028 Config.java:1200 - Node configuration:[allocate_tokens_for_keyspace=null; allocate_tokens_for_local_replication_factor=null; allow_extra_insecure_udfs=false; allow_filtering_enabled=true; allow_insecure_udfs=false; alter_table_enabled=true; audit_logging_options=AuditLogOptions{enabled=false, logger='BinAuditLogger{}', included_keyspaces='', excluded_keyspaces='system,system_schema,system_virtual_schema', included_categories='', excluded_categories='', included_users='', excluded_users='', audit_logs_dir='audit', archive_command='', roll_cycle='HOURLY', block=true, max_queue_weight=268435456, max_log_size=17179869184, max_archive_retries=10}; auth_cache_warming_enabled=false; auth_read_consistency_level=LOCAL_QUORUM; auth_write_consistency_level=EACH_QUORUM; authenticator=null; authorizer=null; auto_bootstrap=true; auto_hints_cleanup_enabled=true; auto_optimise_full_repair_streams=false; auto_optimise_inc_repair_streams=false; auto_optimise_preview_repair_streams=false; auto_snapshot=true; auto_snapshot_ttl=null; autocompaction_on_startup_enabled=true; automatic_sstable_upgrade=false; available_processors=-1; back_pressure_enabled=false; back_pressure_strategy=null; batch_size_fail_threshold=50KiB; batch_size_warn_threshold=5KiB; batchlog_replay_throttle=1024KiB; block_for_peers_in_remote_dcs=false; block_for_peers_timeout_in_secs=10; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=false; cache_load_timeout=30s; cas_contention_timeout=1800ms; cdc_block_writes=true; cdc_enabled=true; cdc_free_space_check_interval=250ms; cdc_on_repair_enabled=true; cdc_raw_directory=build/test/cassandra/cdc_raw; cdc_total_space=0MiB; check_for_duplicate_rows_during_compaction=true; check_for_duplicate_rows_during_reads=true; client_encryption_options=<REDACTED>; client_error_reporting_exclusions=SubnetGroups{subnets=[]}; client_request_size_metrics_enabled=true; cluster_name=Test Cluster; collection_size_fail_threshold=null; collection_size_warn_threshold=null; column_index_cache_size=2KiB; column_index_size=4KiB; column_value_size_fail_threshold=null; column_value_size_warn_threshold=null; columns_per_table_fail_threshold=-1; columns_per_table_warn_threshold=-1; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=build/test/cassandra/commitlog; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size=5MiB; commitlog_sync=batch; commitlog_sync_batch_window_in_ms=1.0; commitlog_sync_group_window=0ms; commitlog_sync_period=0ms; commitlog_total_space=null; compact_tables_enabled=true; compaction_large_partition_warning_threshold=100MiB; compaction_throughput=0MiB/s; compaction_tombstone_warning_threshold=100000; concurrent_compactors=4; concurrent_counter_writes=32; concurrent_index_builders=2; concurrent_materialized_view_builders=1; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_validations=0; concurrent_writes=32; consecutive_message_errors_threshold=1; coordinator_read_size_fail_threshold=4096KiB; coordinator_read_size_warn_threshold=1024KiB; corrupted_tombstone_strategy=exception; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200s; counter_cache_size=null; counter_write_request_timeout=5000ms; credentials_cache_active_update=false; credentials_cache_max_entries=1000; credentials_update_interval=null; credentials_validity=2s; data_disk_usage_max_disk_size=null; data_disk_usage_percentage_fail_threshold=-1; data_disk_usage_percentage_warn_threshold=-1; data_file_directories=[Ljava.lang.String;@686449f9; default_keyspace_rf=1; denylist_consistency_level=QUORUM; denylist_initial_load_retry=5s; denylist_max_keys_per_table=1000; denylist_max_keys_total=10000; denylist_range_reads_enabled=true; denylist_reads_enabled=true; denylist_refresh=600s; denylist_writes_enabled=true; diagnostic_events_enabled=false; disk_access_mode=mmap; disk_failure_policy=ignore; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=ssd; drop_compact_storage_enabled=true; drop_keyspace_enabled=true; drop_truncate_table_enabled=true; dump_heap_on_uncaught_exception=true; dynamic_snitch=true; dynamic_snitch_badness_threshold=1.0; dynamic_snitch_reset_interval=10m; dynamic_snitch_update_interval=100ms; endpoint_snitch=org.apache.cassandra.locator.SimpleSnitch; entire_sstable_inter_dc_stream_throughput_outbound=24MiB/s; entire_sstable_stream_throughput_outbound=24MiB/s; failure_detector=FailureDetector; fields_per_udt_fail_threshold=-1; fields_per_udt_warn_threshold=-1; file_cache_enabled=true; file_cache_round_up=null; file_cache_size=null; flush_compression=fast; force_new_prepared_statement_behaviour=false; full_query_logging_options=FullQueryLoggerOptions{log_dir='', archive_command='', roll_cycle='HOURLY', block=true, max_queue_weight=268435456, max_log_size=17179869184}; gc_log_threshold=200ms; gc_warn_threshold=1s; group_by_enabled=true; heap_dump_path=build/test; hint_window_persistent_enabled=true; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle=1024KiB; hints_compression=null; hints_directory=build/test/cassandra/hints; hints_flush_period=10s; ideal_consistency_level=null; in_select_cartesian_product_fail_threshold=-1; in_select_cartesian_product_warn_threshold=-1; incremental_backups=true; index_summary_capacity=null; index_summary_resize_interval=60m; initial_range_tombstone_list_allocation_size=1; initial_token=null; inter_dc_stream_throughput_outbound=24MiB/s; inter_dc_tcp_nodelay=true; internode_application_receive_queue_capacity=4MiB; internode_application_receive_queue_reserve_endpoint_capacity=128MiB; internode_application_receive_queue_reserve_global_capacity=512MiB; internode_application_send_queue_capacity=4MiB; internode_application_send_queue_reserve_endpoint_capacity=128MiB; internode_application_send_queue_reserve_global_capacity=512MiB; internode_authenticator=null; internode_compression=none; internode_error_reporting_exclusions=SubnetGroups{subnets=[]}; internode_max_message_size=null; internode_socket_receive_buffer_size=0B; internode_socket_send_buffer_size=0B; internode_streaming_tcp_user_timeout=300s; internode_tcp_connect_timeout=2s; internode_tcp_user_timeout=30s; internode_timeout=true; items_per_collection_fail_threshold=-1; items_per_collection_warn_threshold=-1; key_cache_keys_to_save=2147483647; key_cache_migrate_during_compaction=true; key_cache_save_period=4h; key_cache_size=null; keyspace_count_warn_threshold=40; keyspaces_fail_threshold=-1; keyspaces_warn_threshold=-1; listen_address=127.0.0.1; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; local_read_size_fail_threshold=8192KiB; local_read_size_warn_threshold=4096KiB; local_system_data_file_directory=null; materialized_views_enabled=true; materialized_views_per_table_fail_threshold=-1; materialized_views_per_table_warn_threshold=-1; max_concurrent_automatic_sstable_upgrades=1; max_hint_window=3h; max_hints_delivery_threads=2; max_hints_file_size=128MiB; max_hints_size_per_host=0B; max_mutation_size=null; max_space_usable_for_compactions_in_percentage=0.95; max_streaming_retries=3; max_top_size_partition_count=10; max_top_tombstone_partition_count=10; max_value_size=256MiB; maximum_replication_factor_fail_threshold=-1; maximum_replication_factor_warn_threshold=-1; memtable=org.apache.cassandra.config.Config$MemtableOptions@6035b93b; memtable_allocation_type=offheap_objects; memtable_cleanup_threshold=null; memtable_flush_writers=0; memtable_heap_space=null; memtable_offheap_space=null; min_free_space_per_drive=50MiB; min_tracked_partition_size=1MiB; min_tracked_partition_tombstone_count=5000; minimum_replication_factor_fail_threshold=-1; minimum_replication_factor_warn_threshold=-1; native_transport_allow_older_protocols=true; native_transport_flush_in_batches_legacy=false; native_transport_idle_timeout=0ms; native_transport_max_auth_threads=4; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size=16MiB; native_transport_max_negotiable_protocol_version=null; native_transport_max_request_data_in_flight=null; native_transport_max_request_data_in_flight_per_ip=null; native_transport_max_requests_per_second=1000000; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; native_transport_rate_limiting_enabled=false; native_transport_receive_queue_capacity=1MiB; network_authorizer=null; networking_cache_size=null; num_tokens=null; otc_backlog_expiration_interval_ms=200; otc_coalescing_enough_coalesced_messages=8; otc_coalescing_strategy=DISABLED; otc_coalescing_window_us=200; page_size_fail_threshold=-1; page_size_warn_threshold=-1; partition_denylist_enabled=false; partition_keys_in_select_fail_threshold=-1; partition_keys_in_select_warn_threshold=-1; partitioner=org.apache.cassandra.dht.ByteOrderedPartitioner; paxos_cache_size=null; paxos_contention_max_wait=null; paxos_contention_min_delta=null; paxos_contention_min_wait=null; paxos_contention_wait_randomizer=null; paxos_on_linearizability_violations=ignore; paxos_purge_grace_period=60s; paxos_repair_enabled=true; paxos_repair_parallelism=-1; paxos_state_purging=null; paxos_topology_repair_no_dc_checks=false; paxos_topology_repair_strict_each_quorum=false; paxos_variant=v1; periodic_commitlog_sync_lag_block=null; permissions_cache_active_update=false; permissions_cache_max_entries=1000; permissions_update_interval=null; permissions_validity=2s; phi_convict_threshold=8.0; prepared_statements_cache_size=1MiB; range_request_timeout=10000ms; range_tombstone_list_growth_factor=1.5; read_before_write_list_operations_enabled=true; read_consistency_levels_disallowed=[]; read_consistency_levels_warned=[]; read_request_timeout=5000ms; read_thresholds_enabled=true; reject_repair_compaction_threshold=2147483647; repair_command_pool_full_strategy=queue; repair_command_pool_size=0; repair_request_timeout=120000ms; repair_session_max_tree_depth=null; repair_session_space=null; repair_state_expires=3d; repair_state_size=100000; repaired_data_tracking_for_partition_reads_enabled=false; repaired_data_tracking_for_range_reads_enabled=false; report_unconfirmed_repaired_data_mismatches=false; request_timeout=10000ms; role_manager=null; roles_cache_active_update=false; roles_cache_max_entries=1000; roles_update_interval=null; roles_validity=2s; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0s; row_cache_size=16MiB; row_index_read_size_fail_threshold=8192KiB; row_index_read_size_warn_threshold=4096KiB; rpc_address=null; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; sasi_indexes_enabled=true; saved_caches_directory=build/test/cassandra/saved_caches; scripted_user_defined_functions_enabled=false; secondary_indexes_enabled=true; secondary_indexes_per_table_fail_threshold=-1; secondary_indexes_per_table_warn_threshold=-1; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1:7012}; server_encryption_options=<REDACTED>; simplestrategy_enabled=true; skip_paxos_repair_on_topology_change=false; skip_paxos_repair_on_topology_change_keyspaces=[]; skip_stream_disk_space_check=false; slow_query_log_timeout=500ms; snapshot_before_compaction=false; snapshot_links_per_second=0; snapshot_on_duplicate_row_detection=false; snapshot_on_repaired_data_mismatch=false; ssl_storage_port=17012; sstable_preemptive_open_interval=50MiB; sstable_read_rate_persistence_enabled=false; start_native_transport=true; startup_checks={}; storage_port=7012; stream_entire_sstables=true; stream_throughput_outbound=23841858MiB/s; streaming_connections_per_host=1; streaming_keep_alive_period=300s; streaming_slow_events_log_timeout=10s; streaming_state_expires=3d; streaming_state_size=40MiB; streaming_stats_enabled=true; table_count_warn_threshold=150; table_properties_disallowed=[]; table_properties_ignored=[]; table_properties_warned=[]; tables_fail_threshold=-1; tables_warn_threshold=-1; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; top_partitions_enabled=true; trace_type_query_ttl=1d; trace_type_repair_ttl=7d; transfer_hints_on_decommission=true; transient_replication_enabled=false; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@665df3c6; traverse_auth_from_root=false; trickle_fsync=false; trickle_fsync_interval=10240KiB; truncate_request_timeout=60000ms; uncompressed_tables_enabled=true; unlogged_batch_across_partitions_warn_threshold=10; use_deterministic_table_id=false; use_offheap_merkle_trees=true; use_statements_enabled=true; user_defined_functions_enabled=true; user_defined_functions_fail_timeout=1500ms; user_defined_functions_threads_enabled=true; user_defined_functions_warn_timeout=500ms; user_function_timeout_policy=die; user_timestamps_enabled=true; uuid_sstable_identifiers_enabled=false; validation_preview_purge_head_start=3600s; windows_timer_interval=0; write_consistency_levels_disallowed=[]; write_consistency_levels_warned=[]; write_request_timeout=2000ms; zero_ttl_on_twcs_enabled=true; zero_ttl_on_twcs_warned=true]
DEBUG [main] 2023-02-21 11:54:48,030 DatabaseDescriptor.java:434 - Syncing log with batch mode
INFO  [main] 2023-02-21 11:54:48,030 DatabaseDescriptor.java:477 - DiskAccessMode is mmap, indexAccessMode is mmap
INFO  [main] 2023-02-21 11:54:48,031 DatabaseDescriptor.java:520 - Global memtable on-heap threshold is enabled at 251MiB
INFO  [main] 2023-02-21 11:54:48,031 DatabaseDescriptor.java:524 - Global memtable off-heap threshold is enabled at 251MiB
INFO  [main] 2023-02-21 11:54:48,032 DatabaseDescriptor.java:591 - Native transport rate-limiting disabled.
INFO  [main] 2023-02-21 11:54:48,046 DatabaseDescriptor.java:626 - cdc_enabled is true. Starting casssandra node with Change-Data-Capture enabled.
DEBUG [main] 2023-02-21 11:54:48,055 InternalLoggerFactory.java:63 - Using SLF4J as the default logging framework
DEBUG [main] 2023-02-21 11:54:48,072 PlatformDependent0.java:417 - -Dio.netty.noUnsafe: false
DEBUG [main] 2023-02-21 11:54:48,073 PlatformDependent0.java:897 - Java version: 11
DEBUG [main] 2023-02-21 11:54:48,074 PlatformDependent0.java:130 - sun.misc.Unsafe.theUnsafe: available
DEBUG [main] 2023-02-21 11:54:48,075 PlatformDependent0.java:154 - sun.misc.Unsafe.copyMemory: available
DEBUG [main] 2023-02-21 11:54:48,075 PlatformDependent0.java:192 - java.nio.Buffer.address: available
DEBUG [main] 2023-02-21 11:54:48,076 PlatformDependent0.java:257 - direct buffer constructor: available
DEBUG [main] 2023-02-21 11:54:48,077 PlatformDependent0.java:331 - java.nio.Bits.unaligned: available, true
DEBUG [main] 2023-02-21 11:54:48,078 PlatformDependent0.java:393 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): available
DEBUG [main] 2023-02-21 11:54:48,079 PlatformDependent0.java:403 - java.nio.DirectByteBuffer.<init>(long, int): available
DEBUG [main] 2023-02-21 11:54:48,079 PlatformDependent.java:1079 - sun.misc.Unsafe: available
DEBUG [main] 2023-02-21 11:54:48,091 PlatformDependent.java:1181 - maxDirectMemory: 1056309248 bytes (maybe)
DEBUG [main] 2023-02-21 11:54:48,092 PlatformDependent.java:1200 - -Dio.netty.tmpdir: /home/cassandra/cassandra/tmp (java.io.tmpdir)
DEBUG [main] 2023-02-21 11:54:48,092 PlatformDependent.java:1279 - -Dio.netty.bitMode: 64 (sun.arch.data.model)
DEBUG [main] 2023-02-21 11:54:48,093 PlatformDependent.java:177 - -Dio.netty.maxDirectMemory: 1056309248 bytes
DEBUG [main] 2023-02-21 11:54:48,093 PlatformDependent.java:184 - -Dio.netty.uninitializedArrayAllocationThreshold: 1024
DEBUG [main] 2023-02-21 11:54:48,094 CleanerJava9.java:71 - java.nio.ByteBuffer.cleaner(): available
DEBUG [main] 2023-02-21 11:54:48,095 PlatformDependent.java:204 - -Dio.netty.noPreferDirect: false
DEBUG [main] 2023-02-21 11:54:48,098 NativeLibraryLoader.java:73 - -Dio.netty.native.workdir: /home/cassandra/cassandra/tmp (io.netty.tmpdir)
DEBUG [main] 2023-02-21 11:54:48,098 NativeLibraryLoader.java:78 - -Dio.netty.native.deleteLibAfterLoading: true
DEBUG [main] 2023-02-21 11:54:48,099 NativeLibraryLoader.java:82 - -Dio.netty.native.tryPatchShadedId: true
DEBUG [main] 2023-02-21 11:54:48,101 NativeLibraryLoader.java:346 - Unable to load the library 'netty_tcnative_linux_x86_64', trying other loading mechanism.
java.lang.UnsatisfiedLinkError: no netty_tcnative_linux_x86_64 in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
	at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2673)
	at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830)
	at java.base/java.lang.System.loadLibrary(System.java:1873)
	at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:385)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:377)
	at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:341)
	at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136)
	at io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:96)
	at io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:592)
	at io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:136)
	at org.apache.cassandra.security.SSLFactory.<clinit>(SSLFactory.java:73)
	at org.apache.cassandra.security.AbstractSslContextFactory.<clinit>(AbstractSslContextFactory.java:56)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:315)
	at org.apache.cassandra.utils.FBUtilities.newSslContextFactory(FBUtilities.java:694)
	at org.apache.cassandra.config.EncryptionOptions.initializeSslContextFactory(EncryptionOptions.java:294)
	at org.apache.cassandra.config.EncryptionOptions.applyConfig(EncryptionOptions.java:214)
	at org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig(DatabaseDescriptor.java:835)
	at org.apache.cassandra.config.DatabaseDescriptor.applyAll(DatabaseDescriptor.java:382)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:210)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:194)
	at org.apache.cassandra.ServerTestUtils.daemonInitialization(ServerTestUtils.java:62)
	at org.apache.cassandra.cql3.CQLTester.<clinit>(CQLTester.java:195)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.runners.Parameterized.allParameters(Parameterized.java:280)
	at org.junit.runners.Parameterized.<init>(Parameterized.java:248)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)
	at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
	at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
	at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)
	at junit.framework.JUnit4TestAdapter.<init>(JUnit4TestAdapter.java:30)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:496)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041)
DEBUG [main] 2023-02-21 11:54:48,103 NativeLibraryLoader.java:141 - netty_tcnative_linux_x86_64 cannot be loaded from java.library.path, now trying export to -Dio.netty.native.workdir: /home/cassandra/cassandra/tmp
java.lang.UnsatisfiedLinkError: no netty_tcnative_linux_x86_64 in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
	at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2673)
	at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830)
	at java.base/java.lang.System.loadLibrary(System.java:1873)
	at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
	at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:351)
	at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136)
	at io.netty.util.internal.NativeLibraryLoader.loadFirstAvailable(NativeLibraryLoader.java:96)
	at io.netty.handler.ssl.OpenSsl.loadTcNative(OpenSsl.java:592)
	at io.netty.handler.ssl.OpenSsl.<clinit>(OpenSsl.java:136)
	at org.apache.cassandra.security.SSLFactory.<clinit>(SSLFactory.java:73)
	at org.apache.cassandra.security.AbstractSslContextFactory.<clinit>(AbstractSslContextFactory.java:56)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:315)
	at org.apache.cassandra.utils.FBUtilities.newSslContextFactory(FBUtilities.java:694)
	at org.apache.cassandra.config.EncryptionOptions.initializeSslContextFactory(EncryptionOptions.java:294)
	at org.apache.cassandra.config.EncryptionOptions.applyConfig(EncryptionOptions.java:214)
	at org.apache.cassandra.config.DatabaseDescriptor.applySimpleConfig(DatabaseDescriptor.java:835)
	at org.apache.cassandra.config.DatabaseDescriptor.applyAll(DatabaseDescriptor.java:382)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:210)
	at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:194)
	at org.apache.cassandra.ServerTestUtils.daemonInitialization(ServerTestUtils.java:62)
	at org.apache.cassandra.cql3.CQLTester.<clinit>(CQLTester.java:195)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.runners.Parameterized.allParameters(Parameterized.java:280)
	at org.junit.runners.Parameterized.<init>(Parameterized.java:248)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:104)
	at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:86)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
	at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)
	at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
	at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:33)
	at junit.framework.JUnit4TestAdapter.<init>(JUnit4TestAdapter.java:30)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:496)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041)
	Suppressed: java.lang.UnsatisfiedLinkError: no netty_tcnative_linux_x86_64 in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
		at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2673)
		at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830)
		at java.base/java.lang.System.loadLibrary(System.java:1873)
		at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
		at io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:385)
		at java.base/java.security.AccessController.doPrivileged(Native Method)
		at io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:377)
		at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:341)
		... 44 common frames omitted
DEBUG [main] 2023-02-21 11:54:48,137 NativeLibraryLoader.java:342 - Successfully loaded the library /home/cassandra/cassandra/tmp/libnetty_tcnative_linux_x86_6417977019823310401248.so
DEBUG [main] 2023-02-21 11:54:48,137 OpenSsl.java:149 - Initialize netty-tcnative using engine: 'default'
DEBUG [main] 2023-02-21 11:54:48,137 OpenSsl.java:174 - netty-tcnative using native library: BoringSSL
DEBUG [main] 2023-02-21 11:54:48,226 ResourceLeakDetector.java:129 - -Dio.netty.leakDetection.level: simple
DEBUG [main] 2023-02-21 11:54:48,226 ResourceLeakDetector.java:130 - -Dio.netty.leakDetection.targetRecords: 4
DEBUG [main] 2023-02-21 11:54:48,234 AbstractByteBuf.java:63 - -Dio.netty.buffer.checkAccessible: true
DEBUG [main] 2023-02-21 11:54:48,235 AbstractByteBuf.java:64 - -Dio.netty.buffer.checkBounds: true
DEBUG [main] 2023-02-21 11:54:48,236 ResourceLeakDetectorFactory.java:196 - Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@77b7ffa4
DEBUG [main] 2023-02-21 11:54:48,270 InternalThreadLocalMap.java:83 - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
DEBUG [main] 2023-02-21 11:54:48,270 InternalThreadLocalMap.java:86 - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
DEBUG [main] 2023-02-21 11:54:48,273 PooledByteBufAllocator.java:154 - -Dio.netty.allocator.numHeapArenas: 4
DEBUG [main] 2023-02-21 11:54:48,273 PooledByteBufAllocator.java:155 - -Dio.netty.allocator.numDirectArenas: 4
DEBUG [main] 2023-02-21 11:54:48,273 PooledByteBufAllocator.java:157 - -Dio.netty.allocator.pageSize: 8192
DEBUG [main] 2023-02-21 11:54:48,273 PooledByteBufAllocator.java:162 - -Dio.netty.allocator.maxOrder: 11
DEBUG [main] 2023-02-21 11:54:48,273 PooledByteBufAllocator.java:166 - -Dio.netty.allocator.chunkSize: 16777216
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:167 - -Dio.netty.allocator.smallCacheSize: 256
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:168 - -Dio.netty.allocator.normalCacheSize: 64
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:169 - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:170 - -Dio.netty.allocator.cacheTrimInterval: 8192
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:171 - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:172 - -Dio.netty.allocator.useCacheForAllThreads: true
DEBUG [main] 2023-02-21 11:54:48,274 PooledByteBufAllocator.java:173 - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
DEBUG [main] 2023-02-21 11:54:48,282 ByteBufUtil.java:87 - -Dio.netty.allocator.type: pooled
DEBUG [main] 2023-02-21 11:54:48,283 ByteBufUtil.java:96 - -Dio.netty.threadLocalDirectBufferSize: 0
DEBUG [main] 2023-02-21 11:54:48,283 ByteBufUtil.java:99 - -Dio.netty.maxThreadLocalCharBufferSize: 16384
DEBUG [main] 2023-02-21 11:54:48,287 ResourceLeakDetectorFactory.java:196 - Loaded default ResourceLeakDetector: io.netty.util.ResourceLeakDetector@1e53135d
DEBUG [main] 2023-02-21 11:54:48,293 Recycler.java:102 - -Dio.netty.recycler.maxCapacityPerThread: 4096
DEBUG [main] 2023-02-21 11:54:48,293 Recycler.java:103 - -Dio.netty.recycler.maxSharedCapacityFactor: 2
DEBUG [main] 2023-02-21 11:54:48,293 Recycler.java:104 - -Dio.netty.recycler.linkCapacity: 16
DEBUG [main] 2023-02-21 11:54:48,294 Recycler.java:105 - -Dio.netty.recycler.ratio: 8
DEBUG [main] 2023-02-21 11:54:48,294 Recycler.java:106 - -Dio.netty.recycler.delayedQueue.ratio: 8
DEBUG [main] 2023-02-21 11:54:48,310 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 => ECDHE-ECDSA-AES128-GCM-SHA256
DEBUG [main] 2023-02-21 11:54:48,310 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 => ECDHE-ECDSA-AES128-GCM-SHA256
DEBUG [main] 2023-02-21 11:54:48,310 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 => ECDHE-RSA-AES128-GCM-SHA256
DEBUG [main] 2023-02-21 11:54:48,310 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_128_GCM_SHA256 => ECDHE-RSA-AES128-GCM-SHA256
DEBUG [main] 2023-02-21 11:54:48,310 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 => ECDHE-ECDSA-AES256-GCM-SHA384
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 => ECDHE-ECDSA-AES256-GCM-SHA384
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 => ECDHE-RSA-AES256-GCM-SHA384
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_256_GCM_SHA384 => ECDHE-RSA-AES256-GCM-SHA384
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-ECDSA-CHACHA20-POLY1305
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-ECDSA-CHACHA20-POLY1305
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-RSA-CHACHA20-POLY1305
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-RSA-CHACHA20-POLY1305
DEBUG [main] 2023-02-21 11:54:48,311 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-PSK-CHACHA20-POLY1305
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_PSK_WITH_CHACHA20_POLY1305_SHA256 => ECDHE-PSK-CHACHA20-POLY1305
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA => ECDHE-ECDSA-AES128-SHA
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_128_CBC_SHA => ECDHE-ECDSA-AES128-SHA
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA => ECDHE-RSA-AES128-SHA
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_128_CBC_SHA => ECDHE-RSA-AES128-SHA
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_PSK_WITH_AES_128_CBC_SHA => ECDHE-PSK-AES128-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_PSK_WITH_AES_128_CBC_SHA => ECDHE-PSK-AES128-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,312 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA => ECDHE-ECDSA-AES256-SHA
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_ECDSA_WITH_AES_256_CBC_SHA => ECDHE-ECDSA-AES256-SHA
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA => ECDHE-RSA-AES256-SHA
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_RSA_WITH_AES_256_CBC_SHA => ECDHE-RSA-AES256-SHA
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_ECDHE_PSK_WITH_AES_256_CBC_SHA => ECDHE-PSK-AES256-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_ECDHE_PSK_WITH_AES_256_CBC_SHA => ECDHE-PSK-AES256-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_RSA_WITH_AES_128_GCM_SHA256 => AES128-GCM-SHA256
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_RSA_WITH_AES_128_GCM_SHA256 => AES128-GCM-SHA256
DEBUG [main] 2023-02-21 11:54:48,313 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_RSA_WITH_AES_256_GCM_SHA384 => AES256-GCM-SHA384
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_RSA_WITH_AES_256_GCM_SHA384 => AES256-GCM-SHA384
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_RSA_WITH_AES_128_CBC_SHA => AES128-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_RSA_WITH_AES_128_CBC_SHA => AES128-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_PSK_WITH_AES_128_CBC_SHA => PSK-AES128-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_PSK_WITH_AES_128_CBC_SHA => PSK-AES128-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_RSA_WITH_AES_256_CBC_SHA => AES256-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_RSA_WITH_AES_256_CBC_SHA => AES256-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_PSK_WITH_AES_256_CBC_SHA => PSK-AES256-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,314 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_PSK_WITH_AES_256_CBC_SHA => PSK-AES256-CBC-SHA
DEBUG [main] 2023-02-21 11:54:48,315 CipherSuiteConverter.java:330 - Cipher suite mapping: TLS_RSA_WITH_3DES_EDE_CBC_SHA => DES-CBC3-SHA
DEBUG [main] 2023-02-21 11:54:48,315 CipherSuiteConverter.java:331 - Cipher suite mapping: SSL_RSA_WITH_3DES_EDE_CBC_SHA => DES-CBC3-SHA
DEBUG [main] 2023-02-21 11:54:48,315 OpenSsl.java:369 - Supported protocols (OpenSSL): [SSLv2Hello, TLSv1, TLSv1.1, TLSv1.2, TLSv1.3] 
DEBUG [main] 2023-02-21 11:54:48,315 OpenSsl.java:370 - Default cipher suites (OpenSSL): [TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384]
DEBUG [main] 2023-02-21 11:54:48,529 YamlConfigurationLoader.java:124 - Loading settings from file:////home/cassandra/cassandra/build/test/cassandra.cdc.yaml
INFO  [ScheduledTasks:1] 2023-02-21 11:54:48,572 StorageService.java:195 - Overriding RING_DELAY to 1000ms
WARN  [main] 2023-02-21 11:54:48,607 YamlConfigurationLoader.java:427 - [scripted_user_defined_functions_enabled] parameters have been deprecated. They have new names and/or value format; For more information, please refer to NEWS.txt
DEBUG [main] 2023-02-21 11:54:48,607 SimpleSeedProvider.java:99 - Only resolving one IP per DNS record - 127.0.0.1:7012 resolves to /127.0.0.1:7012
DEBUG [main] 2023-02-21 11:54:48,625 SSLFactory.java:251 - Initializing hot reloading SSLContext
INFO  [main] 2023-02-21 11:54:48,990 MonotonicClock.java:208 - Scheduling approximate time conversion task with an interval of 10000 milliseconds
INFO  [main] 2023-02-21 11:54:48,991 MonotonicClock.java:344 - Scheduling approximate time-check task with a precision of 2 milliseconds
INFO  [main] 2023-02-21 11:54:49,103 CommitLog.java:175 - No commitlog files found; skipping replay
DEBUG [main] 2023-02-21 11:54:49,275 NativeLibraryLoader.java:346 - Unable to load the library 'netty_transport_native_epoll_x86_64', trying other loading mechanism.
java.lang.UnsatisfiedLinkError: no netty_transport_native_epoll_x86_64 in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
	at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2673)
	at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830)
	at java.base/java.lang.System.loadLibrary(System.java:1873)
	at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:385)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:377)
	at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:341)
	at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136)
	at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:250)
	at io.netty.channel.epoll.Native.<clinit>(Native.java:69)
	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:39)
	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:165)
	at org.apache.cassandra.net.SocketFactory$Provider.optimalProvider(SocketFactory.java:164)
	at org.apache.cassandra.net.SocketFactory.<init>(SocketFactory.java:185)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:261)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:288)
	at org.apache.cassandra.net.MessagingService$MSHandle.<clinit>(MessagingService.java:253)
	at org.apache.cassandra.net.MessagingService.instance(MessagingService.java:258)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.<init>(DefaultSchemaUpdateHandler.java:81)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandlerFactory.getSchemaUpdateHandler(DefaultSchemaUpdateHandlerFactory.java:33)
	at org.apache.cassandra.schema.Schema.<init>(Schema.java:114)
	at org.apache.cassandra.schema.Schema.<clinit>(Schema.java:78)
	at org.apache.cassandra.db.Keyspace.setInitialized(Keyspace.java:129)
	at org.apache.cassandra.ServerTestUtils.prepareServer(ServerTestUtils.java:128)
	at org.apache.cassandra.cql3.CQLTester.prepareServer(CQLTester.java:278)
	at org.apache.cassandra.cql3.CQLTester.setUpClass(CQLTester.java:326)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:534)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041)
DEBUG [main] 2023-02-21 11:54:49,275 NativeLibraryLoader.java:141 - netty_transport_native_epoll_x86_64 cannot be loaded from java.library.path, now trying export to -Dio.netty.native.workdir: /home/cassandra/cassandra/tmp
java.lang.UnsatisfiedLinkError: no netty_transport_native_epoll_x86_64 in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
	at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2673)
	at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830)
	at java.base/java.lang.System.loadLibrary(System.java:1873)
	at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
	at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:351)
	at io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:136)
	at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:250)
	at io.netty.channel.epoll.Native.<clinit>(Native.java:69)
	at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:39)
	at org.apache.cassandra.service.NativeTransportService.useEpoll(NativeTransportService.java:165)
	at org.apache.cassandra.net.SocketFactory$Provider.optimalProvider(SocketFactory.java:164)
	at org.apache.cassandra.net.SocketFactory.<init>(SocketFactory.java:185)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:261)
	at org.apache.cassandra.net.MessagingService.<init>(MessagingService.java:288)
	at org.apache.cassandra.net.MessagingService$MSHandle.<clinit>(MessagingService.java:253)
	at org.apache.cassandra.net.MessagingService.instance(MessagingService.java:258)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandler.<init>(DefaultSchemaUpdateHandler.java:81)
	at org.apache.cassandra.schema.DefaultSchemaUpdateHandlerFactory.getSchemaUpdateHandler(DefaultSchemaUpdateHandlerFactory.java:33)
	at org.apache.cassandra.schema.Schema.<init>(Schema.java:114)
	at org.apache.cassandra.schema.Schema.<clinit>(Schema.java:78)
	at org.apache.cassandra.db.Keyspace.setInitialized(Keyspace.java:129)
	at org.apache.cassandra.ServerTestUtils.prepareServer(ServerTestUtils.java:128)
	at org.apache.cassandra.cql3.CQLTester.prepareServer(CQLTester.java:278)
	at org.apache.cassandra.cql3.CQLTester.setUpClass(CQLTester.java:326)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:534)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041)
	Suppressed: java.lang.UnsatisfiedLinkError: no netty_transport_native_epoll_x86_64 in java.library.path: [/usr/java/packages/lib, /usr/lib/x86_64-linux-gnu/jni, /lib/x86_64-linux-gnu, /usr/lib/x86_64-linux-gnu, /usr/lib/jni, /lib, /usr/lib]
		at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2673)
		at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:830)
		at java.base/java.lang.System.loadLibrary(System.java:1873)
		at io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:38)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:566)
		at io.netty.util.internal.NativeLibraryLoader$1.run(NativeLibraryLoader.java:385)
		at java.base/java.security.AccessController.doPrivileged(Native Method)
		at io.netty.util.internal.NativeLibraryLoader.loadLibraryByHelper(NativeLibraryLoader.java:377)
		at io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:341)
		... 33 common frames omitted
DEBUG [main] 2023-02-21 11:54:49,294 NativeLibraryLoader.java:342 - Successfully loaded the library /home/cassandra/cassandra/tmp/libnetty_transport_native_epoll_x86_649185331534395234597.so
DEBUG [main] 2023-02-21 11:54:49,296 NetUtil.java:135 - -Djava.net.preferIPv4Stack: false
DEBUG [main] 2023-02-21 11:54:49,296 NetUtil.java:136 - -Djava.net.preferIPv6Addresses: false
DEBUG [main] 2023-02-21 11:54:49,298 NetUtilInitializations.java:129 - Loopback interface: lo (lo, 127.0.0.1)
DEBUG [main] 2023-02-21 11:54:49,299 NetUtil.java:169 - /proc/sys/net/core/somaxconn: 128
DEBUG [main] 2023-02-21 11:54:49,300 SocketFactory.java:154 - using netty EPOLL event loop for pool prefix Messaging-AcceptLoop
DEBUG [main] 2023-02-21 11:54:49,305 MultithreadEventLoopGroup.java:44 - -Dio.netty.eventLoopThreads: 4
DEBUG [main] 2023-02-21 11:54:49,331 SocketFactory.java:154 - using netty EPOLL event loop for pool prefix Messaging-EventLoop
DEBUG [main] 2023-02-21 11:54:49,331 SocketFactory.java:154 - using netty EPOLL event loop for pool prefix Streaming-EventLoop
INFO  [main] 2023-02-21 11:54:49,413 QueryProcessor.java:129 - Initialized prepared statement caches with 1 MiB
INFO  [main] 2023-02-21 11:54:49,594 Keyspace.java:367 - Creating replication strategy system params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.LocalStrategy}}
DEBUG [main] 2023-02-21 11:54:49,603 Keyspace.java:371 - New replication settings for keyspace system - invalidating disk bou
...[truncated 2224809 chars]...
ished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc_txn_flush_adddbe40-b1de-11ed-8676-e95316b4a0e5.log 
DEBUG [CompactionExecutor:1] 2023-02-21 11:55:47,720 Directories.java:536 - FileStore / (overlay) has 372503968973 bytes available, checking if we can write 248 bytes
INFO  [CompactionExecutor:1] 2023-02-21 11:55:47,720 CompactionTask.java:161 - Compacting (adecd970-b1de-11ed-8676-e95316b4a0e5) [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-58-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-59-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-60-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-57-big-Data.db:level=0, ]
INFO  [MemtableFlushWriter:1] 2023-02-21 11:55:47,722 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc_txn_flush_addecfb0-b1de-11ed-8676-e95316b4a0e5.log 
DEBUG [MemtableFlushWriter:2] 2023-02-21 11:55:47,723 ColumnFamilyStore.java:1340 - Flushed to [BigTableReader(path='/home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-60-big-Data.db')] (1 sstables, 5.103KiB), biggest 5.103KiB, smallest 5.103KiB
DEBUG [MemtableFlushWriter:1] 2023-02-21 11:55:47,724 ColumnFamilyStore.java:1340 - Flushed to [BigTableReader(path='/home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-60-big-Data.db')] (1 sstables, 5.183KiB), biggest 5.183KiB, smallest 5.183KiB
DEBUG [CompactionExecutor:3] 2023-02-21 11:55:47,725 Directories.java:536 - FileStore / (overlay) has 372503941734 bytes available, checking if we can write 493 bytes
INFO  [CompactionExecutor:3] 2023-02-21 11:55:47,725 CompactionTask.java:161 - Compacting (aded9cc0-b1de-11ed-8676-e95316b4a0e5) [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-57-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-58-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-60-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-59-big-Data.db:level=0, ]
INFO  [PerDiskMemtableFlushWriter_0:2] 2023-02-21 11:55:47,726 Flushing.java:147 - Writing Memtable-local@144647205(0.060KiB serialized bytes, 4 ops, 0.411KiB (0%) on-heap, 0.157KiB (0%) off-heap), flushed range = [null, null)
INFO  [PerDiskMemtableFlushWriter_0:2] 2023-02-21 11:55:47,726 Flushing.java:173 - Completed flushing /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-80-big-Data.db (0.042KiB) for commitlog position CommitLogPosition(segmentId=1676980489089, position=28)
INFO  [PerDiskMemtableFlushWriter_0:2] 2023-02-21 11:55:47,726 Flushing.java:147 - Writing Memtable-triggers@1918682881(0.008KiB serialized bytes, 1 ops, 0.247KiB (0%) on-heap, 0.024KiB (0%) off-heap), flushed range = [null, null)
INFO  [PerDiskMemtableFlushWriter_0:2] 2023-02-21 11:55:47,726 Flushing.java:173 - Completed flushing /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-60-big-Data.db (0.035KiB) for commitlog position CommitLogPosition(segmentId=1676980489089, position=28)
INFO  [CompactionExecutor:1] 2023-02-21 11:55:47,787 CompactionTask.java:252 - Compacted (adecd970-b1de-11ed-8676-e95316b4a0e5) 4 sstables to [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-61-big,] to level=0.  0.242KiB to 0.078KiB (~32% of original) in 65ms.  Read Throughput = 3.694KiB/s, Write Throughput = 1.191KiB/s, Row Throughput = ~3/s.  6 total partitions merged to 2.  Partition merge counts were {2:1, 4:1, }. Time spent writing keys = 5ms
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,787 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-58-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,787 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-58-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,788 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-59-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,788 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-59-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,789 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-60-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,789 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-60-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,790 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-57-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,790 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc-57-big
INFO  [MemtableFlushWriter:2] 2023-02-21 11:55:47,802 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc_txn_flush_aded75b0-b1de-11ed-8676-e95316b4a0e5.log 
DEBUG [MemtableFlushWriter:2] 2023-02-21 11:55:47,804 ColumnFamilyStore.java:1340 - Flushed to [BigTableReader(path='/home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-80-big-Data.db')] (1 sstables, 4.974KiB), biggest 4.974KiB, smallest 4.974KiB
INFO  [MemtableFlushWriter:1] 2023-02-21 11:55:47,804 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc_txn_flush_aded9cca-b1de-11ed-8676-e95316b4a0e5.log 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,804 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/types-5a8b1ca866023f77a0459273d308917a/nc_txn_compaction_adecd970-b1de-11ed-8676-e95316b4a0e5.log 
DEBUG [CompactionExecutor:2] 2023-02-21 11:55:47,805 Directories.java:536 - FileStore / (overlay) has 372503038976 bytes available, checking if we can write 586 bytes
INFO  [CompactionExecutor:2] 2023-02-21 11:55:47,805 CompactionTask.java:161 - Compacting (adf9d1c0-b1de-11ed-8676-e95316b4a0e5) [/home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-78-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-80-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-79-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-77-big-Data.db:level=0, ]
DEBUG [MemtableFlushWriter:1] 2023-02-21 11:55:47,806 ColumnFamilyStore.java:1340 - Flushed to [BigTableReader(path='/home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-60-big-Data.db')] (1 sstables, 5.183KiB), biggest 5.183KiB, smallest 5.183KiB
DEBUG [CompactionExecutor:4] 2023-02-21 11:55:47,807 Directories.java:536 - FileStore / (overlay) has 372503007846 bytes available, checking if we can write 761 bytes
INFO  [CompactionExecutor:4] 2023-02-21 11:55:47,815 CompactionTask.java:161 - Compacting (adfa1fe0-b1de-11ed-8676-e95316b4a0e5) [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-59-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-57-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-58-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-60-big-Data.db:level=0, ]
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,817 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-57-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,817 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-57-big
INFO  [PerDiskMemtableFlushWriter_0:1] 2023-02-21 11:55:47,817 Flushing.java:147 - Writing Memtable-aggregates@1673272970(0.008KiB serialized bytes, 1 ops, 0.247KiB (0%) on-heap, 0.024KiB (0%) off-heap), flushed range = [null, null)
INFO  [PerDiskMemtableFlushWriter_0:1] 2023-02-21 11:55:47,818 Flushing.java:173 - Completed flushing /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-60-big-Data.db (0.035KiB) for commitlog position CommitLogPosition(segmentId=1676980489089, position=28)
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,819 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-60-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,819 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-60-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,821 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-58-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,821 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-58-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,823 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-59-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,823 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-59-big
INFO  [CompactionExecutor:3] 2023-02-21 11:55:47,823 CompactionTask.java:252 - Compacted (aded9cc0-b1de-11ed-8676-e95316b4a0e5) 4 sstables to [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-61-big,] to level=0.  0.249KiB to 0.114KiB (~45% of original) in 96ms.  Read Throughput = 2.584KiB/s, Write Throughput = 1.186KiB/s, Row Throughput = ~5/s.  5 total partitions merged to 2.  Partition merge counts were {1:1, 4:1, }. Time spent writing keys = 12ms
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,834 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc_txn_compaction_aded9cc0-b1de-11ed-8676-e95316b4a0e5.log 
INFO  [CompactionExecutor:2] 2023-02-21 11:55:47,877 CompactionTask.java:252 - Compacted (adf9d1c0-b1de-11ed-8676-e95316b4a0e5) 4 sstables to [/home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-81-big,] to level=0.  0.324KiB to 0.178KiB (~54% of original) in 71ms.  Read Throughput = 4.518KiB/s, Write Throughput = 2.477KiB/s, Row Throughput = ~2/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }. Time spent writing keys = 12ms
INFO  [CompactionExecutor:4] 2023-02-21 11:55:47,877 CompactionTask.java:252 - Compacted (adfa1fe0-b1de-11ed-8676-e95316b4a0e5) 4 sstables to [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-61-big,] to level=0.  0.180KiB to 0.045KiB (~25% of original) in 61ms.  Read Throughput = 2.934KiB/s, Write Throughput = 0.733KiB/s, Row Throughput = ~1/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }. Time spent writing keys = 3ms
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,877 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-78-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,878 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-78-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,878 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-80-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,879 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-80-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,880 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-79-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,880 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-79-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,881 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-59-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,881 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-59-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,882 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-77-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,882 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc-77-big
INFO  [MemtableFlushWriter:2] 2023-02-21 11:55:47,885 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc_txn_flush_adf9d1ca-b1de-11ed-8676-e95316b4a0e5.log 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,894 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/nc_txn_compaction_adf9d1c0-b1de-11ed-8676-e95316b4a0e5.log 
DEBUG [MemtableFlushWriter:2] 2023-02-21 11:55:47,896 ColumnFamilyStore.java:1340 - Flushed to [BigTableReader(path='/home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-60-big-Data.db')] (1 sstables, 5.351KiB), biggest 5.351KiB, smallest 5.351KiB
DEBUG [MemtablePostFlush:1] 2023-02-21 11:55:47,896 CommitLog.java:339 - Commit log segment CommitLogSegment(build/test/cassandra/commitlog/CommitLog-7-1676980489088.log) is unused
DEBUG [MemtablePostFlush:1] 2023-02-21 11:55:47,896 AbstractCommitLogSegmentManager.java:374 - Segment CommitLogSegment(build/test/cassandra/commitlog/CommitLog-7-1676980489088.log) is no longer active and will be deleted now
DEBUG [CompactionExecutor:2] 2023-02-21 11:55:47,897 Directories.java:536 - FileStore / (overlay) has 372502089523 bytes available, checking if we can write 184 bytes
INFO  [CompactionExecutor:2] 2023-02-21 11:55:47,897 CompactionTask.java:161 - Compacting (ae07db80-b1de-11ed-8676-e95316b4a0e5) [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-60-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-59-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-57-big-Data.db:level=0, /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-58-big-Data.db:level=0, ]
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,897 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-57-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,897 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-57-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,898 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-58-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,898 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-58-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,899 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-60-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,900 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc-60-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,902 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/triggers-4df70b666b05325195a132b54005fd48/nc_txn_compaction_adfa1fe0-b1de-11ed-8676-e95316b4a0e5.log 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,904 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/cql_test_keyspace/table_60-ad8b5880b1de11ed8676e95316b4a0e5/nc-1-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,904 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/cql_test_keyspace/table_60-ad8b5880b1de11ed8676e95316b4a0e5/nc-1-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,923 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/cql_test_keyspace/table_60-ad8b5880b1de11ed8676e95316b4a0e5/nc_txn_unknowncompactiontype_ae0850b0-b1de-11ed-8676-e95316b4a0e5.log 
INFO  [OptionalTasks:1] 2023-02-21 11:55:47,934 Keyspace.java:367 - Creating replication strategy cql_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [OptionalTasks:1] 2023-02-21 11:55:47,934 ViewManager.java:125 - Not submitting build tasks for views in keyspace cql_test_keyspace as storage service is not initialized
DEBUG [main] 2023-02-21 11:55:47,938 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{a4a06655-b3eb-3c1b-a343-6319d326463c --> 510b5122-227d-32e1-af9a-16b60a5d2901, diff=KeyspacesDiff{created=[KeyspaceMetadata{name=cql_test_keyspace_alt, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}], dropped=[], altered=[]}}
INFO  [main] 2023-02-21 11:55:47,939 Keyspace.java:367 - Creating replication strategy cql_test_keyspace_alt params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
DEBUG [main] 2023-02-21 11:55:47,939 Keyspace.java:371 - New replication settings for keyspace cql_test_keyspace_alt - invalidating disk boundary caches
DEBUG [main] 2023-02-21 11:55:47,939 TokenMetadata.java:876 - Starting pending range calculation for cql_test_keyspace_alt
DEBUG [main] 2023-02-21 11:55:47,939 TokenMetadata.java:881 - Pending range calculation for cql_test_keyspace_alt completed (took: 0ms)
INFO  [main] 2023-02-21 11:55:47,940 CQLTester.java:885 - CREATE TABLE cql_test_keyspace.table_61 (k int PRIMARY KEY, v1 int,)
DEBUG [main] 2023-02-21 11:55:47,944 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{510b5122-227d-32e1-af9a-16b60a5d2901 --> e5255989-c96a-3863-a2f2-95f878d009b8, diff=KeyspacesDiff{created=[], dropped=[], altered=[KeyspaceDiff{before=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}, after=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61], views=[], functions=[], types=[]}, tables=Diff{created=[cql_test_keyspace.table_61], dropped=[], altered=[]}, views=Diff{created=[], dropped=[], altered=[]}, types=Diff{created=[], dropped=[], altered=[]}, udfs=Diff{created=[], dropped=[], altered=[]}, udas=Diff{created=[], dropped=[], altered=[]}}]}}
INFO  [main] 2023-02-21 11:55:47,944 Keyspace.java:367 - Creating replication strategy cql_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 11:55:47,945 ColumnFamilyStore.java:489 - Initializing cql_test_keyspace.table_61
DEBUG [main] 2023-02-21 11:55:47,946 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for cql_test_keyspace.table_61
DEBUG [main] 2023-02-21 11:55:47,946 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [main] 2023-02-21 11:55:47,946 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=build/test/cassandra/data}], positions=null, ringVersion=0, directoriesVersion=0} for cql_test_keyspace.table_61
INFO  [CompactionExecutor:2] 2023-02-21 11:55:47,947 CompactionTask.java:252 - Compacted (ae07db80-b1de-11ed-8676-e95316b4a0e5) 4 sstables to [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-61-big,] to level=0.  0.180KiB to 0.045KiB (~25% of original) in 49ms.  Read Throughput = 3.606KiB/s, Write Throughput = 0.901KiB/s, Row Throughput = ~1/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }. Time spent writing keys = 5ms
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,947 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-60-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,947 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-60-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,948 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-57-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,949 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-57-big
INFO  [main] 2023-02-21 11:55:47,949 ViewManager.java:125 - Not submitting build tasks for views in keyspace cql_test_keyspace as storage service is not initialized
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,950 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-59-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,950 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-59-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,951 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-58-big-Data.db 
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,951 SSTable.java:127 - Deleting sstable: /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc-58-big
INFO  [NonPeriodicTasks:1] 2023-02-21 11:55:47,972 LogTransaction.java:242 - Unfinished transaction log, deleting /home/cassandra/cassandra/build/test/cassandra/data/system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/nc_txn_compaction_ae07db80-b1de-11ed-8676-e95316b4a0e5.log 
INFO  [GossipTasks:1] 2023-02-21 11:55:48,193 Gossiper.java:1091 - FatClient /127.0.0.2:7012 has been silent for 1000ms, removing from gossip
DEBUG [GossipStage:1] 2023-02-21 11:55:48,193 MigrationCoordinator.java:532 - Removing and ignoring endpoint /127.0.0.2:7012
DEBUG [GossipStage:1] 2023-02-21 11:55:48,193 Gossiper.java:671 - removing endpoint /127.0.0.2:7012
DEBUG [GossipStage:1] 2023-02-21 11:55:48,193 Gossiper.java:639 - evicting /127.0.0.2:7012 from gossip
DEBUG [GossipTasks:1] 2023-02-21 11:55:50,194 Gossiper.java:1127 - 2000 elapsed, /127.0.0.2:7012 gossip quarantine over
ERROR [Strong-Reference-Leak-Detector:1] 2023-02-21 11:55:50,306 NoSpamLogger.java:111 - Strong self-ref loop detected [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/indexes-0feb57ac311f382fba6d9024d305702f/nc-61-big, private org.apache.cassandra.io.util.FileHandle org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier.dfile-org.apache.cassandra.io.util.FileHandle, private final org.apache.cassandra.io.util.RebuffererFactory org.apache.cassandra.io.util.FileHandle.rebuffererFactory-org.apache.cassandra.cache.ChunkCache$CachingRebufferer, final org.apache.cassandra.cache.ChunkCache org.apache.cassandra.cache.ChunkCache$CachingRebufferer.this$0-org.apache.cassandra.cache.ChunkCache, private final org.apache.cassandra.utils.memory.BufferPool org.apache.cassandra.cache.ChunkCache.bufferPool-org.apache.cassandra.utils.memory.BufferPool, private final java.util.Set org.apache.cassandra.utils.memory.BufferPool.localPoolReferences-java.util.Collections$SetFromMap, private final java.util.Map java.util.Collections$SetFromMap.m-java.util.concurrent.ConcurrentHashMap, private final java.util.Map java.util.Collections$SetFromMap.m-org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef, private final org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef.chunks-org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks, private org.apache.cassandra.utils.memory.BufferPool$Chunk org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.chunk0-org.apache.cassandra.utils.memory.BufferPool$Chunk, private volatile org.apache.cassandra.utils.memory.BufferPool$LocalPool org.apache.cassandra.utils.memory.BufferPool$Chunk.owner-org.apache.cassandra.utils.memory.BufferPool$LocalPool, private final java.lang.Thread org.apache.cassandra.utils.memory.BufferPool$LocalPool.owningThread-io.netty.util.concurrent.FastThreadLocalThread, private java.lang.Runnable java.lang.Thread.target-io.netty.util.concurrent.FastThreadLocalRunnable, private final java.lang.Runnable io.netty.util.concurrent.FastThreadLocalRunnable.runnable-java.util.concurrent.ThreadPoolExecutor$Worker, final java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ThreadPoolExecutor$Worker.this$0-org.apache.cassandra.concurrent.ThreadPoolExecutorPlus, private final java.util.HashSet java.util.concurrent.ThreadPoolExecutor.workers-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, java.util.HashMap$Node java.util.HashMap$Node.next-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-java.util.concurrent.ThreadPoolExecutor$Worker, final java.lang.Thread java.util.concurrent.ThreadPoolExecutor$Worker.thread-io.netty.util.concurrent.FastThreadLocalThread, private java.lang.ThreadGroup java.lang.Thread.group-java.lang.ThreadGroup, private final java.lang.ThreadGroup java.lang.ThreadGroup.parent-java.lang.ThreadGroup, private final java.lang.ThreadGroup java.lang.ThreadGroup.parent-java.lang.ThreadGroup, java.lang.ThreadGroup[] java.lang.ThreadGroup.groups-[Ljava.lang.ThreadGroup;, java.lang.ThreadGroup[] java.lang.ThreadGroup.groups-java.lang.ThreadGroup, java.lang.Thread[] java.lang.ThreadGroup.threads-[Ljava.lang.Thread;, java.lang.Thread[] java.lang.ThreadGroup.threads-io.netty.util.concurrent.FastThreadLocalThread, private java.lang.Runnable java.lang.Thread.target-io.netty.util.concurrent.FastThreadLocalRunnable, private final java.lang.Runnable io.netty.util.concurrent.FastThreadLocalRunnable.runnable-java.util.concurrent.ThreadPoolExecutor$Worker, final java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ThreadPoolExecutor$Worker.this$0-org.apache.cassandra.concurrent.ScheduledThreadPoolExecutorPlus, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask, private java.util.concurrent.Callable java.util.concurrent.FutureTask.callable-java.util.concurrent.Executors$RunnableAdapter, private final java.lang.Runnable java.util.concurrent.Executors$RunnableAdapter.task-org.apache.cassandra.concurrent.ExecutionFailure$1, final java.lang.Runnable org.apache.cassandra.concurrent.ExecutionFailure$1.val$wrap-org.apache.cassandra.db.memtable.AbstractAllocatorMemtable$1, final org.apache.cassandra.db.memtable.Memtable$Owner org.apache.cassandra.db.memtable.AbstractAllocatorMemtable$1.val$owner-org.apache.cassandra.db.ColumnFamilyStore, public final org.apache.cassandra.db.Keyspace org.apache.cassandra.db.ColumnFamilyStore.keyspace-org.apache.cassandra.db.Keyspace, private final org.apache.cassandra.schema.SchemaProvider org.apache.cassandra.db.Keyspace.schema-org.apache.cassandra.schema.Schema, private final org.apache.cassandra.utils.concurrent.LoadingMap org.apache.cassandra.schema.Schema.keyspaceInstances-org.apache.cassandra.utils.concurrent.LoadingMap, private final org.cliffc.high_scale_lib.NonBlockingHashMap org.apache.cassandra.utils.concurrent.LoadingMap.internalMap-org.cliffc.high_scale_lib.NonBlockingHashMap, private final org.cliffc.high_scale_lib.NonBlockingHashMap org.apache.cassandra.utils.concurrent.LoadingMap.internalMap-org.apache.cassandra.utils.concurrent.AsyncPromise, volatile java.lang.Object org.apache.cassandra.utils.concurrent.AbstractFuture.result-org.apache.cassandra.db.Keyspace, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-java.util.concurrent.ConcurrentHashMap, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-org.apache.cassandra.db.ColumnFamilyStore, private final org.apache.cassandra.db.lifecycle.Tracker org.apache.cassandra.db.ColumnFamilyStore.data-org.apache.cassandra.db.lifecycle.Tracker, private final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-java.util.concurrent.CopyOnWriteArrayList, private final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-org.apache.cassandra.db.compaction.CompactionStrategyManager, private final org.apache.cassandra.db.compaction.CompactionStrategyHolder org.apache.cassandra.db.compaction.CompactionStrategyManager.unrepaired-org.apache.cassandra.db.compaction.CompactionStrategyHolder, private final java.util.List org.apache.cassandra.db.compaction.CompactionStrategyHolder.strategies-java.util.ArrayList, transient java.lang.Object[] java.util.ArrayList.elementData-[Ljava.lang.Object;, transient java.lang.Object[] java.util.ArrayList.elementData-org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, protected final java.util.Set org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.sstables-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-org.apache.cassandra.io.sstable.format.big.BigTableReader, private final org.apache.cassandra.utils.concurrent.Ref org.apache.cassandra.io.sstable.format.SSTableReader.selfRef-org.apache.cassandra.utils.concurrent.Ref]
ERROR [Strong-Reference-Leak-Detector:1] 2023-02-21 11:55:50,444 NoSpamLogger.java:111 - Strong self-ref loop detected [/home/cassandra/cassandra/build/test/cassandra/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/nc-81-big-Data.db, final org.apache.cassandra.io.util.RebuffererFactory org.apache.cassandra.io.util.FileHandle$Cleanup.rebufferer-org.apache.cassandra.cache.ChunkCache$CachingRebufferer, final org.apache.cassandra.cache.ChunkCache org.apache.cassandra.cache.ChunkCache$CachingRebufferer.this$0-org.apache.cassandra.cache.ChunkCache, private final org.apache.cassandra.utils.memory.BufferPool org.apache.cassandra.cache.ChunkCache.bufferPool-org.apache.cassandra.utils.memory.BufferPool, private final java.util.Set org.apache.cassandra.utils.memory.BufferPool.localPoolReferences-java.util.Collections$SetFromMap, private final java.util.Map java.util.Collections$SetFromMap.m-java.util.concurrent.ConcurrentHashMap, private final java.util.Map java.util.Collections$SetFromMap.m-org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef, private final org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef.chunks-org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks, private org.apache.cassandra.utils.memory.BufferPool$Chunk org.apache.cassandra.utils.memory.BufferPool$MicroQueueOfChunks.chunk0-org.apache.cassandra.utils.memory.BufferPool$Chunk, private volatile org.apache.cassandra.utils.memory.BufferPool$LocalPool org.apache.cassandra.utils.memory.BufferPool$Chunk.owner-org.apache.cassandra.utils.memory.BufferPool$LocalPool, private final java.lang.Thread org.apache.cassandra.utils.memory.BufferPool$LocalPool.owningThread-io.netty.util.concurrent.FastThreadLocalThread, private java.lang.Runnable java.lang.Thread.target-io.netty.util.concurrent.FastThreadLocalRunnable, private final java.lang.Runnable io.netty.util.concurrent.FastThreadLocalRunnable.runnable-java.util.concurrent.ThreadPoolExecutor$Worker, final java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ThreadPoolExecutor$Worker.this$0-org.apache.cassandra.concurrent.ThreadPoolExecutorPlus, private final java.util.HashSet java.util.concurrent.ThreadPoolExecutor.workers-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, java.util.HashMap$Node java.util.HashMap$Node.next-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-java.util.concurrent.ThreadPoolExecutor$Worker, final java.lang.Thread java.util.concurrent.ThreadPoolExecutor$Worker.thread-io.netty.util.concurrent.FastThreadLocalThread, private java.lang.ThreadGroup java.lang.Thread.group-java.lang.ThreadGroup, private final java.lang.ThreadGroup java.lang.ThreadGroup.parent-java.lang.ThreadGroup, private final java.lang.ThreadGroup java.lang.ThreadGroup.parent-java.lang.ThreadGroup, java.lang.ThreadGroup[] java.lang.ThreadGroup.groups-[Ljava.lang.ThreadGroup;, java.lang.ThreadGroup[] java.lang.ThreadGroup.groups-java.lang.ThreadGroup, java.lang.Thread[] java.lang.ThreadGroup.threads-[Ljava.lang.Thread;, java.lang.Thread[] java.lang.ThreadGroup.threads-io.netty.util.concurrent.FastThreadLocalThread, private java.lang.Runnable java.lang.Thread.target-io.netty.util.concurrent.FastThreadLocalRunnable, private final java.lang.Runnable io.netty.util.concurrent.FastThreadLocalRunnable.runnable-java.util.concurrent.ThreadPoolExecutor$Worker, final java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ThreadPoolExecutor$Worker.this$0-org.apache.cassandra.concurrent.ScheduledThreadPoolExecutorPlus, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue, private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask, private java.util.concurrent.Callable java.util.concurrent.FutureTask.callable-java.util.concurrent.Executors$RunnableAdapter, private final java.lang.Runnable java.util.concurrent.Executors$RunnableAdapter.task-org.apache.cassandra.concurrent.ExecutionFailure$1, final java.lang.Runnable org.apache.cassandra.concurrent.ExecutionFailure$1.val$wrap-org.apache.cassandra.db.memtable.AbstractAllocatorMemtable$1, final org.apache.cassandra.db.memtable.Memtable$Owner org.apache.cassandra.db.memtable.AbstractAllocatorMemtable$1.val$owner-org.apache.cassandra.db.ColumnFamilyStore, public final org.apache.cassandra.db.Keyspace org.apache.cassandra.db.ColumnFamilyStore.keyspace-org.apache.cassandra.db.Keyspace, private final org.apache.cassandra.schema.SchemaProvider org.apache.cassandra.db.Keyspace.schema-org.apache.cassandra.schema.Schema, private final org.apache.cassandra.utils.concurrent.LoadingMap org.apache.cassandra.schema.Schema.keyspaceInstances-org.apache.cassandra.utils.concurrent.LoadingMap, private final org.cliffc.high_scale_lib.NonBlockingHashMap org.apache.cassandra.utils.concurrent.LoadingMap.internalMap-org.cliffc.high_scale_lib.NonBlockingHashMap, private final org.cliffc.high_scale_lib.NonBlockingHashMap org.apache.cassandra.utils.concurrent.LoadingMap.internalMap-org.apache.cassandra.utils.concurrent.AsyncPromise, volatile java.lang.Object org.apache.cassandra.utils.concurrent.AbstractFuture.result-org.apache.cassandra.db.Keyspace, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-java.util.concurrent.ConcurrentHashMap, private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-org.apache.cassandra.db.ColumnFamilyStore, private final org.apache.cassandra.db.lifecycle.Tracker org.apache.cassandra.db.ColumnFamilyStore.data-org.apache.cassandra.db.lifecycle.Tracker, private final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-java.util.concurrent.CopyOnWriteArrayList, private final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-org.apache.cassandra.db.compaction.CompactionStrategyManager, private final org.apache.cassandra.db.compaction.CompactionStrategyHolder org.apache.cassandra.db.compaction.CompactionStrategyManager.unrepaired-org.apache.cassandra.db.compaction.CompactionStrategyHolder, private final java.util.List org.apache.cassandra.db.compaction.CompactionStrategyHolder.strategies-java.util.ArrayList, transient java.lang.Object[] java.util.ArrayList.elementData-[Ljava.lang.Object;, transient java.lang.Object[] java.util.ArrayList.elementData-org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, protected final java.util.Set org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.sstables-java.util.HashSet, private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap, transient java.util.HashMap$Node[] java.util.HashMap.table-[Ljava.util.HashMap$Node;, transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node, final java.lang.Object java.util.HashMap$Node.key-org.apache.cassandra.io.sstable.format.big.BigTableReader, protected final org.apache.cassandra.io.util.FileHandle org.apache.cassandra.io.sstable.format.SSTableReader.dfile-org.apache.cassandra.io.util.FileHandle, final org.apache.cassandra.utils.concurrent.Ref org.apache.cassandra.utils.concurrent.SharedCloseableImpl.ref-org.apache.cassandra.utils.concurrent.Ref]
INFO  [main] 2023-02-21 11:55:50,974 CQLTester.java:885 - CREATE TABLE cql_test_keyspace.table_62 (k text, s text static, i int, v text, PRIMARY KEY (k, i) )
DEBUG [main] 2023-02-21 11:55:50,978 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{e5255989-c96a-3863-a2f2-95f878d009b8 --> da9b222a-dc5d-308f-a434-42a0b4698a1b, diff=KeyspacesDiff{created=[], dropped=[], altered=[KeyspaceDiff{before=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61], views=[], functions=[], types=[]}, after=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61, cql_test_keyspace.table_62], views=[], functions=[], types=[]}, tables=Diff{created=[cql_test_keyspace.table_62], dropped=[], altered=[]}, views=Diff{created=[], dropped=[], altered=[]}, types=Diff{created=[], dropped=[], altered=[]}, udfs=Diff{created=[], dropped=[], altered=[]}, udas=Diff{created=[], dropped=[], altered=[]}}]}}
INFO  [main] 2023-02-21 11:55:50,979 Keyspace.java:367 - Creating replication strategy cql_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 11:55:50,980 ColumnFamilyStore.java:489 - Initializing cql_test_keyspace.table_62
DEBUG [main] 2023-02-21 11:55:50,980 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for cql_test_keyspace.table_62
DEBUG [main] 2023-02-21 11:55:50,981 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [main] 2023-02-21 11:55:50,981 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=build/test/cassandra/data}], positions=null, ringVersion=0, directoriesVersion=0} for cql_test_keyspace.table_62
INFO  [main] 2023-02-21 11:55:50,985 ViewManager.java:125 - Not submitting build tasks for views in keyspace cql_test_keyspace as storage service is not initialized
INFO  [main] 2023-02-21 11:55:51,006 CQLTester.java:885 - CREATE TABLE cql_test_keyspace.table_63(k int, s int static, i int, v text, PRIMARY KEY(k, i))
DEBUG [main] 2023-02-21 11:55:51,016 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{da9b222a-dc5d-308f-a434-42a0b4698a1b --> 944d426f-b092-35a8-9d7c-4d5d79c85796, diff=KeyspacesDiff{created=[], dropped=[], altered=[KeyspaceDiff{before=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61, cql_test_keyspace.table_62], views=[], functions=[], types=[]}, after=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61, cql_test_keyspace.table_62, cql_test_keyspace.table_63], views=[], functions=[], types=[]}, tables=Diff{created=[cql_test_keyspace.table_63], dropped=[], altered=[]}, views=Diff{created=[], dropped=[], altered=[]}, types=Diff{created=[], dropped=[], altered=[]}, udfs=Diff{created=[], dropped=[], altered=[]}, udas=Diff{created=[], dropped=[], altered=[]}}]}}
INFO  [main] 2023-02-21 11:55:51,017 Keyspace.java:367 - Creating replication strategy cql_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 11:55:51,031 ColumnFamilyStore.java:489 - Initializing cql_test_keyspace.table_63
DEBUG [main] 2023-02-21 11:55:51,032 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for cql_test_keyspace.table_63
DEBUG [main] 2023-02-21 11:55:51,033 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [main] 2023-02-21 11:55:51,033 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=build/test/cassandra/data}], positions=null, ringVersion=0, directoriesVersion=0} for cql_test_keyspace.table_63
INFO  [main] 2023-02-21 11:55:51,038 ViewManager.java:125 - Not submitting build tasks for views in keyspace cql_test_keyspace as storage service is not initialized
INFO  [main] 2023-02-21 11:55:51,049 CQLTester.java:885 - CREATE TABLE cql_test_keyspace.table_64 (k int, i int, v1 int, v2 int, s int static, PRIMARY KEY (k, i))
DEBUG [main] 2023-02-21 11:55:51,055 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{944d426f-b092-35a8-9d7c-4d5d79c85796 --> cae112f7-06ef-3a6e-ac15-4d16a0a4e1da, diff=KeyspacesDiff{created=[], dropped=[], altered=[KeyspaceDiff{before=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61, cql_test_keyspace.table_62, cql_test_keyspace.table_63], views=[], functions=[], types=[]}, after=KeyspaceMetadata{name=cql_test_keyspace, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[cql_test_keyspace.table_61, cql_test_keyspace.table_62, cql_test_keyspace.table_63, cql_test_keyspace.table_64], views=[], functions=[], types=[]}, tables=Diff{created=[cql_test_keyspace.table_64], dropped=[], altered=[]}, views=Diff{created=[], dropped=[], altered=[]}, types=Diff{created=[], dropped=[], altered=[]}, udfs=Diff{created=[], dropped=[], altered=[]}, udas=Diff{created=[], dropped=[], altered=[]}}]}}
INFO  [main] 2023-02-21 11:55:51,062 Keyspace.java:367 - Creating replication strategy cql_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 11:55:51,063 ColumnFamilyStore.java:489 - Initializing cql_test_keyspace.table_64
DEBUG [main] 2023-02-21 11:55:51,063 DiskBoundaryManager.java:54 - Refreshing disk boundary cache for cql_test_keyspace.table_64
DEBUG [main] 2023-02-21 11:55:51,063 DiskBoundaryManager.java:93 - Got local ranges [] (ringVersion = 0)
DEBUG [main] 2023-02-21 11:55:51,063 DiskBoundaryManager.java:57 - Updating boundaries from null to DiskBoundaries{directories=[DataDirectory{location=build/test/cassandra/data}], positions=null, ringVersion=0, directoriesVersion=0} for cql_test_keyspace.table_64
INFO  [main] 2023-02-21 11:55:51,066 ViewManager.java:125 - Not submitting build tasks for views in keyspace cql_test_keyspace as storage service is not initialized
DEBUG [main] 2023-02-21 11:55:51,075 DefaultSchemaUpdateHandler.java:259 - Schema updated: SchemaTransformationResult{cae112f7-06ef-3a6e-ac15-4d16a0a4e1da --> 44a5a37c-4b50-3e1e-a00b-641622638df6, diff=KeyspacesDiff{created=[], dropped=[KeyspaceMetadata{name=cql_test_keyspace_alt, kind=REGULAR, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}], altered=[]}}
WARN  [main] 2023-02-21 11:55:51,077 Gossiper.java:2168 - No local state, state is in silent shutdown, or node hasn't joined, not announcing shutdown
{noformat}"
CASSANDRA-18285,[CircleCI] Add JDK 11 Java and Python upgrade tests in trunk,"Currently we test upgrades with Java 8, in preparation to drop it we need first to ensurer we test with 11. -This has to be added not only in trunk,, but also 4.1-
This ticket will be used for trunk. A separate one  will be opened for 4.0 and 4.1.
I believe [~mck] has this ready to push in Jenkins at some point so this ticket should accommodate the addition of those upgrade tests in CircleCI for 4.1 and trunk."
CASSANDRA-18284,Create 2023 Google Season of Documentation proposal and related blog post,"Create a Google Season of Documentation proposal and related blog post.
 * Apply for Open Collective account.
 * Apply for GSoD grant. Requires writing blog post for URL for application.
 * FYI - ticket will be close once GSoD application is submitted.
 ** If grant is successful, modify blog post to advertise for tech writer applicants.
 ** Select tech writers."
CASSANDRA-18283,Enhance diagnostic nodetool tablestats output,"The nodetool tablestats command lacks some available details which would be very useful to report upon.  This is especially helpful in database-as-a-service environments where servers and their disk files are not directly observable by users.

1. Currently, for LCS tablestats reports useful details about the number of sstables in each level:

          SSTable count: 6635

          SSTables in each level: [1, 9, 98, 805, 5722, 0, 0, 0, 0]

This type of additional detail about the sstables is absent from STCS and TWCS as it only reports the table count. 

1a) For STCS, tablestats should report the max sstable file size on disk. This is useful to know if compaction has failed due to disk space or if a forced compaction created a jumbo table.

1b) For TWCS, tablestats should report the min & max timestamp, and duration of the sstables representing windows.  This is useful to know if out-of-window writes or rows w/out a TTL have lead many more sstables on disk than expected by the time window configuration.

STCs example:

          SSTable count: 6635

          SSTable STCS max size: 122,000,000,000

TWCs example:

         SSTable count: 6635

          SSTables Time Window 15 DAYS, max duration : 362d 7h 16m 49s

2. While tablestats reports both memtable and disk file sstable statistics. It is useful these are in the same command, but it would clarify the output to separate mem vs disk into two sections

i.e., 

     -- File statistics

     SSTable count: 6635

     SSTables in each level: [1, 9, 98, 805, 5722, 0, 0, 0, 0] 

     -- Memtable statistics

     Bloom filter false positives: 12184123

     Bloom filter false ratio: 0.07203

     Bloom filter space used: 16874424

     Bloom filter off heap memory used: 16821344

     Index summary off heap memory used: 7525546

     Space used (live): 1324067896238

3.  Read / Write count should also be reported as a ratio, such as:

     Local read count: 202961459

     Local write count: 40554481

     Local read/write ratio: 5:1    <new>

     Local read latency: 1.957 ms

     Local write count: 40554481

     Local write latency: 0.040 ms"
CASSANDRA-18280,Investigate initial size of GrowableByteArrayDataOutput in RAMIndexOutput,"The GrowableByteArrayDataOutput in RAMIndexOutput is currently initialized with a size of 128 bytes. There is no explanation as to why this size was chosen. 

The GrowableByteArrayDataOutput does not lazily allocate memory but only ever allocates enough for each write operation. This can lead to a lot of fresh allocations and calls to System.arrayCopy. 

Since RAMIndexOutput is used to build the on-disk postings in SAI it is likely that the size of the in-memory array is going to grow considerably more that 128 bytes.

We should investigate changing this initial value to something higher and possibly changing the GrowableByteArrayDataOutput class to allocate in blocks rather than write increments."
