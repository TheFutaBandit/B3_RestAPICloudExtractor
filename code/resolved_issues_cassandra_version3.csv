Bug ID,Bug Summary,Bug Description,Affected Version,Fix Version
CASSANDRA-19895,Update OWASP dependency checker to version 10.0.4,"Version 10.0.0 we are at stopped to work.

https://github.com/jeremylong/DependencyCheck?tab=readme-ov-file#mandatory-upgrade-notice",N/A,"3.0.31, 3.11.18, 4.0.14, 4.1.7, 5.0.1, 5.1"
CASSANDRA-19752,Debian packaging fails after openjdk-8* and java8* removed from bullseye,"No candidates for {{`openjdk-8-jdk | java8-jdk`}}

Failure occurs at the {{mk-build-deps}} step…
{noformat}
Broken cassandra-build-deps:amd64 Depends on openjdk-8-jdk:amd64 < none @un H >
     Removing cassandra-build-deps:amd64 because I can't find openjdk-8-jdk:amd64
Broken cassandra-build-deps:amd64 Depends on java8-jdk:amd64 < none @un H >
     Removing cassandra-build-deps:amd64 because I can't find java8-jdk:amd64
     Or group remove for cassandra-build-deps:amd64
…
mk-build-deps: Unable to install cassandra-build-deps at /usr/bin/mk-build-deps line 457.
mk-build-deps: Unable to install all build-dep packages
{noformat}
ref: https://ci-cassandra.apache.org/job/Cassandra-3.0-artifacts/jdk=jdk_1.8_latest,label=cassandra/428/console ",N/A,"2.2.20, 3.0.31, 3.11.18"
CASSANDRA-19738,Update dependency-check library to version 10.0.0,"Currently, we are at 9.0.5, which gives me locally basically this (1)

Version 10.0.0 was released today and check is passing again.

We should update it everywhere to 10.0.0

(1) https://github.com/jeremylong/DependencyCheck/issues/6515",N/A,"3.0.31, 3.11.18, 4.0.14, 5.0-rc1, 5.1-alpha1"
CASSANDRA-19708,Remove sid from bullseye docker images,"sid is flakey, often broken and takes days for correct packages to be uploaded.

ref: https://ci-cassandra.apache.org/job/Cassandra-4.1-artifacts/jdk=jdk_1.8_latest,label=cassandra/611/ 

sid is only used for jdk8

looks like replacing it with temurin might be a safer/stable choice.

",N/A,"3.0.x, 3.0.31, 3.11.18, 4.0.14, 4.1.6, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19681,Debian repository is missing 3.11.17 package,"The Debian package for Cassandra 3.11.17 is missing from the JFrog artifactory. The [package index|https://apache.jfrog.io/artifactory/cassandra-deb/dists/311x/main/binary-amd64/Packages] is still referencing version 3.11.16:

{code}
Package: cassandra
Version: 3.11.16
...
Filename: pool/main/c/cassandra/cassandra_3.11.16_all.deb
...

Package: cassandra-tools
Source: cassandra
Version: 3.11.16
...
Filename: pool/main/c/cassandra/cassandra-tools_3.11.16_all.deb
...
{code}

When I tried to install the latest C* 3.11 version on Ubuntu, 3.11.16 got installed instead of 3.11.17.

Note that this was originally reported by [Roman on Stack Exchange|https://dba.stackexchange.com/questions/340007/].",N/A,3.11.17
CASSANDRA-19650,CCM wrongly interprets CASSANDRA_USE_JDK11 for Cassandra 4.x,"CCM interprets {{CASSANDRA_USE_JDK11}} only by its existence in the environment rather than by its actual value (true/false). 

I can see two solutions:
- make it interpret {{CASSANDRA_USE_JDK11}} properly
- do not take into account {{CASSANDRA_USE_JDK11}} in the current env and set it or unset it automatically when starting a node basing on which Java version was selected
",N/A,"3.0.31, 3.11.18, 4.0.14, 4.1.6"
CASSANDRA-19628,Correct testing instructions on the website,"At https://cassandra.apache.org/_/development/testing.html it says to issue these statements for cqlsh tests:

{noformat}
ccm updateconf ""enable_user_defined_functions: true""
ccm updateconf ""enable_scripted_user_defined_functions: true""
ccm updateconf ""cdc_enabled: true""
{noformat}

But these actually break the configuration so it won't start.",N/A,"3.0.31, 4.0.14, 4.1.6, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19614,Add java version to prepare_release.sh human check step,"With the many versions of java one may need to float these days, it would be nice for prepare_release.sh to show the version you are using when it asks the ""Is this what you want?"" question showing the latest commit, that way there is a chance to correct things before it tags.",N/A,"3.0.31, 3.11.18, 4.0.13, 4.1.5, 5.0-rc1, 5.1"
CASSANDRA-19606,Fix building debian packages,"Trying to run cassandra-deb-packaging.sh will result in the docker image looping:

{noformat}
Errors were encountered while processing:
 ed
 quilt
 cassandra-build-deps
E: Sub-process /usr/bin/dpkg returned an error code (1)
(Reading database ... 36721 files and directories currently installed.)
Removing cassandra-build-deps (5.0~beta2-20240501gitae9be29918) ...
mk-build-deps: Unable to install all build-dep packages
mk-build-deps failed… trying again after 10s… 
{noformat}",N/A,"3.0.31, 3.11.18, 4.0.13, 4.1.5, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19559,prepare_release.sh should check for mvn,"Part of the 'prepare' phase of releasing includes publishing Maven artifacts, which requires that it be installed.  The script should check for this since it's quite easy to miss.",N/A,"3.0.31, 3.11.17, 4.0.13, 4.1.5, 5.0-rc1, 5.1"
CASSANDRA-19551,CCM nodes share the same environment variable map breaking upgrade tests,"In {{node.py}} {{__environment_variables}} is generally always set with a map that is passed in from {{cluster.py}} so it is [shared between nodes|https://github.com/riptano/ccm/blob/ac264706c8ca007cc584871ce907d48db334d36d/ccmlib/node.py#L151] and if nodes modify the map, such as in {{start}} when [updating the Java version|https://github.com/riptano/ccm/blob/ac264706c8ca007cc584871ce907d48db334d36d/ccmlib/node.py#L860] then when {{get_env}} runs it will [overwrite the Java version|https://github.com/riptano/ccm/blob/ac264706c8ca007cc584871ce907d48db334d36d/ccmlib/node.py#L244] that is selected by {{update_java_version}}.

This results in {{nodetool drain}} failing when upgrading from 3.11 to 4.0 in some of the upgrade tests because after the first node upgrades to 4.0 it's not longer possible for the subsequent nodes to select a Java version that isn't 11 because it's overridden by  {{__environment_variables}}.

I'm not even 100% clear on why the code in {{start}} should update {{__environment_variables}} at all if we calculate the correct java version on every invocation of other tools.",N/A,"3.0.31, 3.11.17, 4.0.13, 5.0-rc1, 5.1"
CASSANDRA-19496,Add properties for redirecting build-resolve to mirrors,When running upgrade tests in CI it's not always possible to reach the public mirrors. Currently we have properties for configuring private mirrors in 4.0+ but we don't have this for 3.x.,N/A,"3.0.30, 3.11.17"
CASSANDRA-19484,Add support for providing nvdDatafeedUrl to OWASP,"This allows you to point to a mirror that is faster and doesn’t require an API key.

This is kind of painful to make work in {{ant}} because you can't specify the property at all if you want to use the API and I couldn't find a way to get {{ant}} to conditionally supply the property without having a dedicated invocation of the {{dependency-check}} task with/without the parameter {{nvdDataFeedUrl}} specified.

 ",N/A,"3.0.30, 3.11.17, 4.0.13, 4.1.5, 5.0, 5.1"
CASSANDRA-19469,python dtest packages should pin versions,We have for example in CASSANDRA-19464 seen how allowing modules to automatically upgrade themselves causes us problems.  We should pin all of these to specific versions so that the environment is entirely reproducible.,N/A,"3.0.30, 3.11.17, 4.0.13, 4.1.5, 5.0, 5.1"
CASSANDRA-19427,Fix concurrent access of ClientWarn causing AIOBE for SELECT WHERE IN queries with multiple coordinator-local partitions,"On one of our clusters, we noticed rare but periodic ArrayIndexOutOfBoundsExceptions:

 
{code:java}
message=""Uncaught exception on thread Thread[ReadStage-3,5,main]""
exception=""java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2579)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ArrayIndexOutOfBoundsException""{code}
 

 

The error was in a Runnable, so the stacktrace didn't directly indicate where the error was coming from. We enabled JFR to log the underlying exception that was thrown:
 
{code:java}
message=""Uncaught exception on thread Thread[ReadStage-2,5,main]"" exception=""java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 0
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2579)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds for length 0
at java.base/java.util.ArrayList.add(ArrayList.java:487)
at java.base/java.util.ArrayList.add(ArrayList.java:499)
at org.apache.cassandra.service.ClientWarn$State.add(ClientWarn.java:84)
at org.apache.cassandra.service.ClientWarn$State.access$000(ClientWarn.java:77)
at org.apache.cassandra.service.ClientWarn.warn(ClientWarn.java:51)
at org.apache.cassandra.db.ReadCommand$1MetricRecording.onClose(ReadCommand.java:596)
at org.apache.cassandra.db.transform.BasePartitions.runOnClose(BasePartitions.java:70)
at org.apache.cassandra.db.transform.BaseIterator.close(BaseIterator.java:95)
at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:2260)
at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2575)
... 6 more""{code}
 
 

An AIOBE on ArrayList.add(E) should only be possible when multiple threads attempt to call the method at the same time.

 

This was seen while executing a SELECT WHERE IN query with multiple partition keys. This exception could happen when multiple local reads are dispatched by the coordinator in org.apache.cassandra.service.reads.AbstractReadExecutor#makeRequests. In this case, multiple local reads exceed the tombstone warning threshold, so multiple tombstone warnings are added to the same ClientWarn.State reference.  Currently, org.apache.cassandra.service.ClientWarn.State#warnings is an ArrayList, which isn't safe for concurrent modification, causing the AIOBE to be thrown.

 

I have a patch available for this, and I'm preparing it now. The patch is simple - it just changes org.apache.cassandra.service.ClientWarn.State#warnings to a thread-safe CopyOnWriteArrayList. I also have a jvm-dtest that demonstrates the issue but doesn't need to be merged - it shows how a SELECT WHERE IN query with local reads that add client warnings can add to the same ClientWarn.State from different threads. I'll push that in a separate branch just for demonstration purposes.

 

Demonstration branch: [https://github.com/apache/cassandra/compare/trunk...aratno:cassandra:CASSANDRA-19427-aiobe-clientwarn-demo]

Fix branch: [https://github.com/apache/cassandra/compare/trunk...aratno:cassandra:CASSANDRA-19427-aiobe-clientwarn-fix] (PR linked below)

 

This appears to have been an issue since at least 3.11, that was the earliest release I checked.",N/A,"3.11.17, 4.0.13, 4.1.5, 5.0-rc1, 5.0, 5.1-alpha1"
CASSANDRA-19422,Fix Git repository links,"I am creating an issue based on this PR (1)

(1) https://github.com/apache/cassandra/pull/3120",N/A,"3.0.30, 3.11.17, 4.0.13, 4.1.5, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19409,Test Failure: dtest-upgrade.upgrade_tests.upgrade_through_versions_test.*,"Failing in Jenkins:
 * [dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD/test_parallel_upgrade_with_internode_ssl/]
 * [dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade-novnode-large.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_parallel_upgrade_with_internode_ssl/]
 * [dtest-upgrade-novnode.upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD.test_parallel_upgrade|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade-novnode.upgrade_tests.upgrade_through_versions_test/TestProtoV3Upgrade_AllVersions_EndsAt_Trunk_HEAD/test_parallel_upgrade/]
 * [dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_parallel_upgrade/]
 * [dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_parallel_upgrade_with_internode_ssl|https://ci-cassandra.apache.org/job/Cassandra-5.0/170/testReport/junit/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_parallel_upgrade_with_internode_ssl/]",N/A,"3.0.30, 3.11.17, 4.0.13, 4.1.5, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19350,Add cli log level to circle config,"We try to set the [log cli level|https://github.com/apache/cassandra/blob/trunk/.circleci/config_template.yml#L3538] so that dtest logging is captured to stdout, but we don't do it [everywhere|https://github.com/apache/cassandra/blob/trunk/.circleci/config_template.yml#L3063] so we miss some logs.",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19326,Switch memtable_allocation_type from offheap_objects to heap_buffers in test/conf/cassandra.yaml,"By default we use heap_buffers in cassandra.yaml, but the unit tests were switched for testing to offheap_objects. This needs to be reverted. We should be testing with heap_buffers on all branches, we do it only in cassandra-3.0 at the moment. ",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19291,Fix NEWS.txt Compact Storage section,"In CASSANDRA-16733 we added a note that Compact Storage will no longer be supported in 5.0. The idea was that drop_compact_storage would be pulled out of the experimental version. 
This did not happen, and compact storage is still around. 
I think this will not be handled at least until 6.0 (major breaking changes) and it is good to be corrected. More and more people are upgrading to 4.0+ and they are confused. ",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19146,Upgrade owasp to 9.0.x,"From https://github.com/jeremylong/DependencyCheck :

{quote}
Upgrading to 9.0.0 or later is mandatory; previous versions of dependency-check utilize the NVD data feeds which will be deprecated on Dec 15th, 2023. 
{quote}",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19142,logback-core-1.2.12.jar vulnerability: CVE-2023-6378,"https://nvd.nist.gov/vuln/detail/CVE-2023-6378

{quote}
A serialization vulnerability in logback receiver component part of logback version 1.4.11 allows an attacker to mount a Denial-Of-Service attack by sending poisoned data. 
{quote}",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-rc1, 5.0"
CASSANDRA-19111,Add j17|j11_dtests_repeat and j17|j11_dtests_vnode_repeat among circle jobs,"j17_dtests_repeat and j17_dtests_vnode_repeat do exist in circleci but they are not referenced in workflows. I basically can not run multiplexer on dtests in circle.

Same happens for their j11 counterparts.

Same applies for 3.0 where j8_dtests_repeat is, again, not referenced in workflows. I guess it will be same for all branches we have.",N/A,"3.0.x, 3.11.x, 4.0.x, 4.1.x, 5.0-alpha2, 5.0, 5.x"
CASSANDRA-19110,"apt-key deprecation, replace with gpg --dearmor in the docs.","the command `apt-key` is deprecated and soon to be removed, especially on Ubuntu.
the directory `/usr/share/keyrings` for shared keys are also being removed.

I suggest to convert the docs from
{code:java}
curl https://downloads.apache.org/cassandra/KEYS | sudo apt-key add - {code}
to a simpler command:
{code:java}
curl https://downloads.apache.org/cassandra/KEYS | sudo gpg --dearmor -o /etc/apt/keyrings/cassandra-archive-keyring.gpg {code}
The path `/etc/apt/keyrings` doesn't exists by default on Ubuntu 20.04 but it does on 22.04.



I also suggest to add the source.list.d text from 
{code:java}
$ echo ""deb https://debian.cassandra.apache.org 42x main"" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

deb https://debian.cassandra.apache.org 42x main{code}
to 
{code:java}
$ echo ""deb [signed-by=/etc/apt/keyrings/cassandra-archive-keyring.gpg] https://debian.cassandra.apache.org 42x main"" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

deb [signed-by=/etc/apt/keyrings/cassandra-archive-keyring.gpg] https://debian.cassandra.apache.org 42x main {code}
I have made a [PR|https://github.com/apache/cassandra/pull/2936]

I have tested the gpg --dearmor on a VM with Ubuntu 22.04 myself recently and it works just fine.",N/A,"3.0.30, 3.11.17, 4.0.13, 4.1.5, 5.0-rc1, 5.0, 5.1"
CASSANDRA-19020,cqlsh should allow failure to import cqlshlib.serverversion,"cqlshlib.serverversion is created by ant, recording the server's version so that python can see if it matches cqlsh later.  This can make work for other things that need to be aware of it like CASSANDRA-18594, so we should relax it a bit since this really has no value outside of warning humans they have a mismatch.",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-beta1, 5.1"
CASSANDRA-18986,SHA1 keys prevent installation on RHEL 9,"Due to the presence of SHA1 keys they have to be explicitly allowed before C* can be installed on RHEL 9-based systems: 

{quote}
Importing GPG key 0xF2833C93:
 Userid     : ""Eric Evans <eevans@sym-link.com>""
 Fingerprint: CEC8 6BB4 A0BA 9D0F 9039 7CAE F835 8FA2 F283 3C93
 From       : https://downloads.apache.org/cassandra/KEYS
Is this ok [y/N]: y
Key imported successfully
Importing GPG key 0x8D77295D:
 Userid     : ""Eric Evans <eevans@sym-link.com>""
 Fingerprint: C496 5EE9 E301 5D19 2CCC F2B6 F758 CE31 8D77 295D
 From       : https://downloads.apache.org/cassandra/KEYS
Is this ok [y/N]: y
Key imported successfully
Importing GPG key 0x2B5C1B00:
 Userid     : ""Sylvain Lebresne (pcmanus) <sylvain@datastax.com>""
 Fingerprint: 5AED 1BF3 78E9 A19D ADE1 BCB3 4BD7 36A8 2B5C 1B00
 From       : https://downloads.apache.org/cassandra/KEYS
Is this ok [y/N]: y
warning: Signature not supported. Hash algorithm SHA1 not available.
Key import failed (code 2). Failing package is: cassandra-4.0.11-1.noarch
 GPG Keys are configured as: https://downloads.apache.org/cassandra/KEYS
The downloaded packages were saved in cache until the next successful transaction.
You can remove cached packages by executing 'yum clean packages'.
Error: GPG check FAILED
{quote}

This can be worked around by allowing SHA1:

{quote}
update-crypto-policies --set DEFAULT:SHA1
{quote}

https://www.redhat.com/en/blog/rhel-security-sha-1-package-signatures-distrusted-rhel-9
",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0, 5.1"
CASSANDRA-18943,http/2 vulnerability: CVE-2023-44487,"https://nvd.nist.gov/vuln/detail/CVE-2023-44487

Basically anything using http/2 is covered by this, but we don't use it.",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha2, 5.0, 5.1"
CASSANDRA-18935,Fix nodetool enable/disablebinary to correctly set rpc," 
{code:java}
        if ((nativeFlag != null && Boolean.parseBoolean(nativeFlag)) || (nativeFlag == null && DatabaseDescriptor.startNativeTransport()))
        {
            startNativeTransport();
            StorageService.instance.setRpcReady(true);
        } {code}
The startup code here only sets RpcReady if native transport is enabled. If you call 
{code:java}
nodetool enablebinary{code}
then this flag doesn't get set.

But with the change from CASSANDRA-13043 it requires RpcReady set to true in order to get a leader for the counter update.

Not sure what the correct fix is here, seems to only really use this flag for counters. So thinking perhaps the fix is to just move this outside the if condition.

 ",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-beta1, 5.0, 5.1"
CASSANDRA-18910,Debian packaging broken by quilt?,"Something has changed in the docker image that is breaking the debian packaging in all versions, similar to this:

{quote}
dpkg-buildpackage: info: source package cassandra
dpkg-buildpackage: info: source version 4.1.4-20231004git486acc68f1
dpkg-buildpackage: info: source distribution unstable
dpkg-buildpackage: info: source changed by build <build@1518e06a5507>
dpkg-buildpackage: info: host architecture amd64
 dpkg-source --tar-ignore=.git --before-build .
 fakeroot debian/rules clean
QUILT_PATCHES=debian/patches \
        quilt --quiltrc /dev/null pop -a -R || test $? = 1
No patch removed
make: *** [/usr/share/quilt/quilt.make:23: unpatch] Error 1
dpkg-buildpackage: error: fakeroot debian/rules clean subprocess returned exit status 2
{quote}",N/A,"3.0.x, 3.11.x, 4.0.x, 4.1.x, 5.0, 5.x"
CASSANDRA-18886,snappy-java-1.1.10.1 vulnerability: CVE-2023-43642,"https://nvd.nist.gov/vuln/detail/CVE-2023-43642

Another DoS which is probably not a huge deal, but we can upgrade.",N/A,"3.0.x, 3.11.x, 4.0.x, 4.1.x, 5.0, 5.x"
CASSANDRA-18878,Upgrade snappy java library,"The snappy java library needs to be updated to fix the latest CVEs.
[https://github.com/xerial/snappy-java/security/advisories/GHSA-55g7-9cwv-5qfv]",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha2, 5.0, 5.1"
CASSANDRA-18854,Gossip never recovers from a single failed echo,"As discovered on CASSANDRA-18792, if an initial echo request is lost, the node will never be marked up.  This appears to be a regression caused by CASSANDRA-18543.",N/A,"3.11.17, 4.0.12, 4.1.4, 5.0-alpha2, 5.0, 5.1"
CASSANDRA-18838,make stop-server shell out of the box,,N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha2, 5.0"
CASSANDRA-18824,Backport CASSANDRA-16418: Cleanup behaviour during node decommission caused missing replica,"Node decommission triggers data transfer to other nodes. While this transfer is in progress,
receiving nodes temporarily hold token ranges in a pending state. However, the cleanup process currently doesn't consider these pending ranges when calculating token ownership.
As a consequence, data that is already stored in sstables gets inadvertently cleaned up.

STR:
 * Create two node cluster
 * Create keyspace with RF=1
 * Insert sample data (assert data is available when querying both nodes)
 * Start decommission process of node 1
 * Start running cleanup in a loop on node 2 until decommission on node 1 finishes
 * Verify of all rows are in the cluster - it will fail as the previous step removed some of the rows

It seems that the cleanup process does not take into account the pending ranges, it uses only the local ranges - [https://github.com/apache/cassandra/blob/caad2f24f95b494d05c6b5d86a8d25fbee58d7c2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L466].

There are two solutions to the problem.

One would be to change the cleanup process in a way that it start taking pending ranges into account. Even thought it might sound tempting at first it will require involving changes and a lot of testing effort.

Alternatively we could interrupt/prevent the cleanup process from running when any pending range on a node is detected. That sounds like a reasonable alternative to the problem and something that is relatively easy to implement.

The bug has been already fixed in 4.x with CASSANDRA-16418, the goal of this ticket is to backport it to 3.x.",N/A,"3.0.30, 3.11.17, 4.0.13, 4.1.4, 5.0-rc1, 5.0, 5.1"
CASSANDRA-18818,Add cqlshrc.sample and credentials.sample into Debian package,RPM package contains these two files buy DEB does not. Debian users suffer from not having these files available so they need to craft them completely from scratch. ,N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha2, 5.0"
CASSANDRA-18807,Missing license info and headers identified in 5.0-alpha1 candidate,"LICENSE.txt is missing
python-smhasher
OrderedDict
MagnetoDB
java-driver

LICENSE.txt has mistakes around
Chronicle-Bytes
Guava
PATRICIA Trie

pylib docker files needed license headers

ref: https://lists.apache.org/thread/3db8n220qdnpx3jd40pwbtlf14w1b6m7",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha1, 5.0, 5.1"
CASSANDRA-18803,Refactor validation logic in StorageService.rebuild,This is a follow-up ticket of CASSANDRA-14319,N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-18767,tablestats should show speculative retries,"NodeProbe makes [provisions|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/NodeProbe.java#L1921] for it, but it is never used later.",N/A,"3.0.30, 3.11.16, 4.0.12, 5.0-alpha1, 5.0, 5.1"
CASSANDRA-18760,Backport CASSANDRA-16905 to older branches,"Recently hit an un-recoverable situation in Cassandra 4.0.10 after dropping a 'map' column and adding it back as a 'blob', which caused corruption that neither offline nor online scrub could fix.
When dropping a 'blob' column and attempting to add it back as a 'map' type, the operation is blocked with:

{code:java}
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot re-add previously dropped column 'col1' of type map<int, tinyint>, incompatible with previous type blob""
{code}

We need to do the same going from 'map' to 'blob' to avoid this potentially very bad scenario that can cause data loss.
",N/A,"3.0.30, 3.11.16, 4.0.12"
CASSANDRA-18756,TimeWindowCompactionStrategy with unsafe_aggressive_sstable_expiration keeps overlaping SSTable references,"When {{unsafe_aggressive_sstable_expiration}} is turned on, TWCS should not create or maintain an iterator of overlapping sstables. However, because {{TimeWindowCompactionController}} inherits from {{CompactionController}} and only sets {{ignoreOverlaps}} after the base class has constructed the overlap iterator, it ends up making an overlap iterator and then never updating it.

The end result is that such a compaction keeps references to lots of and likely _all_ other SSTables on the node and thus delays the deletion of obsolete ones by hours or even days.",N/A,"3.11.17, 4.0.12, 4.1.4, 5.0-alpha2, 5.0, 5.1"
CASSANDRA-18751,Fix Requires for Java for RPM packages,"We have to change Requires in redhat/cassandra.spec to

java-1.8.0 for 3.0 and 3.11
(java-1.8.0 or java-11) for 4.0. and 4.1
(java-11 or java-17) for 5.0 and trunk

noboolean would contain only java-1.8.0 and java-11 respectively where applicable.",N/A,"3.0.30, 3.11.16, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-18745,cqlsh should warn on server version mismatch,"When cqlsh is used against a different version of the server than the one that it shipped with, it should emit a warning so that users aren't confused when some things don't work correctly.",N/A,"3.0.30, 3.11.16, 4.0.12, 4.1.4, 5.0-alpha1, 5.0, 5.1"
CASSANDRA-18739,UDF functions fail to load on rolling restart,"UDFs fail to reload properly after a rolling restart.
h3. *Symptom:*

NPE thrown when used after restart.
h3. *Steps to recreate:*
 # Create a cluster as per cql file
 # Populate the cluster with data.cql.
 # Execute SELECT city_measurements(city, measurement, 16.5) AS m FROM current
 # expect min and max values for cities.
 # Performing a rolling restart on one server.
 # When the server is back up
 # Execute SELECT city_measurements(city, measurement, 16.5) AS m FROM current
 # expect: error result with NPE message.


{*}Analysis{*}:

During system restart the SchemaKeyspace.fetchNonSystemKeyspaces() is called, when a keyspace with a UDF is loaded the SchemaKeyspace method createUDFFromRow() is called, this in turn calls UDFunction.create() which eventually calls back to UDFunction constructor where the Schema.instance.getKeyspaceMetadata() is called with the keyspace for the UDF name as the argument. However, the keyspace for the UDF name is being constructed and is not yet in the instance so the method returns null for the KeyspaceMetadata. That null KeyspaceMetadata is then used in the udfContext.

Later when the UDF method is called, if there is a need to call a method on the keyspaceMetadata, such as udfContext.newUDTValue() where the implementation uses keyspaceMetadata.types, a null pointer is thrown.

I have verified this affects version 4.0, 4.1 and trunk. I have not verified 3.x but I suspect it is the same there.

I modified UDFunction constructor to assert that the metadata was not null and received the following stack trace

ERROR [main] 2023-08-09 11:44:46,408 CassandraDaemon.java:911 - Exception encountered during startup
java.lang.AssertionError: No metadata for temperatures.city_measurements_sfunc
    at org.apache.cassandra.cql3.functions.UDFunction.<init>(UDFunction.java:240)
    at org.apache.cassandra.cql3.functions.JavaBasedUDFunction.<init>(JavaBasedUDFunction.java:195)
    at org.apache.cassandra.cql3.functions.UDFunction.create(UDFunction.java:276)
    at org.apache.cassandra.schema.SchemaKeyspace.createUDFFromRow(SchemaKeyspace.java:1182)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchUDFs(SchemaKeyspace.java:1131)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchFunctions(SchemaKeyspace.java:1119)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:859)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:848)
    at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:836)
    at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:132)
    at org.apache.cassandra.schema.Schema.loadFromDisk(Schema.java:121)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:287)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:765)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:889)

 

{{*Possible solution:*}}

*Version 4.x*

Create a KeyspaceMetadata.Builder class that uses accepts the types, tables and views but uses a builder for the functions.

Add a KeyspaceMetadata constructor to accept the KeyspaceMetadata.Builder so that the function builder keyspaceMetadata value can be set correctly during construction of the KeyspaceMetadata.

Modify SchemaKeyspace.fetchKeyspace(string) so that it uses the KeyspaceMetadata.Builder.

 

*Version 5.x*

Similar to 4.x except that the KeyspaceMetadata.Builder will have to have builders for Views and Tables because the functions necessary to construct those objects will not be available until the KeyspaceMetadata.Builder constructs it.

 ",N/A,"3.11.17, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-18725,IsolatedJMX should not release all TCPEndpoints on instance shutdown,"In the original implementation of the JMX feature, we fixed some memory leaks by clearing some internal state in Java’s TCPEndpoint. However, that implementation was overly aggressive and cleared the whole map, vs. just removing the endpoints created by the individual instances. This causes issues when you remove a node from the cluster (as all of the endpoints are cleared, not just the ones in use by that instance).
 
In stead, we should check if the endpoint was created by the instance in question and only remove it if it was.",N/A,"3.11.17, 4.0.12, 4.1.4, 5.0-alpha1, 5.x"
CASSANDRA-18724,Investigate unmatched suppressions,"Since we upgraded OWASP, it now reports unmatched suppressions (""Suppression Rule had zero matches"") we should investigate and possibly remove.",N/A,"3.0.30, 3.11.16, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-18709,Test failure: dtest.jmx_test.TestJMX.test_compactionstats,"Seen here:

[https://ci-cassandra.apache.org/job/Cassandra-trunk/1646/testReport/dtest.jmx_test/TestJMX/test_compactionstats/]
h3.  
{code:java}
Error Message
AttributeError: 'NoneType' object has no attribute 'named'

Stacktrace
self = <jmx_test.TestJMX object at 0x7fe78cfc2940> def test_compactionstats(self): """""" @jira_ticket CASSANDRA-10504 @jira_ticket CASSANDRA-10427 Test that jmx MBean used by nodetool compactionstats properly updates the progress of a compaction """""" cluster = self.cluster cluster.populate(1) node = cluster.nodelist()[0] cluster.start() # Run a quick stress command to create the keyspace and table node.stress(['write', 'n=1', 'no-warmup']) # Disable compaction on the table node.nodetool('disableautocompaction keyspace1 standard1') node.nodetool('setcompactionthroughput 1') node.stress(['write', 'n=150K', 'no-warmup']) node.flush() # Run a major compaction. This will be the compaction whose # progress we track. node.nodetool_process('compact') # We need to sleep here to give compaction time to start # Why not do something smarter? Because if the bug regresses, # we can't rely on jmx to tell us that compaction started. time.sleep(5) compaction_manager = make_mbean('db', type='CompactionManager') with JolokiaAgent(node) as jmx: progress_string = jmx.read_attribute(compaction_manager, 'CompactionSummary')[0] # Pause in between reads # to allow compaction to move forward time.sleep(2) updated_progress_string = jmx.read_attribute(compaction_manager, 'CompactionSummary')[0] var = 'Compaction@{uuid}(keyspace1, standard1, {progress}/{total})bytes' if self.cluster.version() >= LooseVersion('4.0'): # CASSANDRA-15954 var = 'Compaction({taskUuid}, {progress} / {total} bytes)@{uuid}(keyspace1, standard1)' > progress = int(parse.search(var, progress_string).named['progress']) E AttributeError: 'NoneType' object has no attribute 'named' jmx_test.py:218: AttributeError
{code}
 ",N/A,"3.0.30, 3.11.16, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-18664,Move jflex from runtime to build dependencies,"JFlex depends on java-cup-runtime which has a kind of exotic license saying that it is compatible with GPL. Therefore, we should investigate that case.

JFlex seems to be used only during the build to generate some sources for SASI indexes. Therefore, it should be possible to move JFlex from the runtime dependencies to the build dependencies and not include it in the distribution.

 ",N/A,"3.11.16, 4.0.12, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18650,Upgrade owasp to 8.3.1,"I believe I'm fighting with an issue this upgrade solves, but also I cannot think of any reason to not run the latest version.",N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18649,netty-all vulnerability: CVE-2023-34462,"This is failing owasp:

https://nvd.nist.gov/vuln/detail/CVE-2023-34462

{quote}
The `SniHandler` can allocate up to 16MB of heap for each channel during the TLS handshake. When the handler or the channel does not have an idle timeout, it can be used to make a TCP server using the `SniHandler` to allocate 16MB of heap.
{quote}",N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18647,CASTing a float to decimal adds wrong digits,"If I create a table with a *float* (32-bit) column, and cast it to the *decimal* type, the casting wrongly passes through the double (64-bit) type and picks up extra, wrong, digits. For example, if we have a column e of type ""float"", and run

INSERT INTO tbl (p, e) VALUES (1, 5.2)

SELECT CAST(e AS decimal) FROM tbl WHERE p=1

The result is the ""decimal"" value 5.199999809265137, with all those extra wrong digits. It would have been better to get back the decimal value 5.2, with only two significant digits.

It appears that this happens because Cassandra's implementation first converts the 32-bit float into a 64-bit double, and only then converts that - with all the silly extra digits it picked up in the first conversion - into a ""decimal"" value.

Contrast this with CAST(e AS text) which works correctly - it returns the string ""5.2"" - only the actual digits of the 32-bit floating point value are converted to the string, without inventing additional digits in the process.",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18643,jackson-core vulnerability: CVE-2022-45688,"This is failing owasp.

https://nvd.nist.gov/vuln/detail/CVE-2022-45688

{quote}
A stack overflow in the XML.toJSONObject component of hutool-json v5.8.10 allows attackers to cause a Denial of Service (DoS) via crafted JSON or XML data.
{quote}",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18630,jackson-databind-2.13.2.2.jar vulnerability: CVE-2023-35116,"https://nvd.nist.gov/vuln/detail/CVE-2023-35116

{noformat}
 An issue was discovered jackson-databind thru 2.15.2 allows attackers to cause a denial of service or other unspecified impacts via crafted object that uses cyclic dependencies. NOTE: the vendor's perspective is that the product is not intended for use with untrusted input.
{noformat}",N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18609,snappy-java vulnerability: CVE-2023-34453,"Failing owasp:

[https://nvd.nist.gov/vuln/detail/CVE-2023-34453]

bq. Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing a fatal error. ",N/A,"3.0.x, 3.11.x, 4.0.x, 4.1.x, 5.x"
CASSANDRA-18608,"snappy-java vulnerability: CVE-2023-34455, CVE-2023-34454, CVE-2023-34453","Failing owasp:

[https://nvd.nist.gov/vuln/detail/CVE-2023-34455]
{quote}Due to use of an unchecked chunk length, an unrecoverable fatal error can occur in versions prior to 1.1.10.1.
{quote}

[https://nvd.nist.gov/vuln/detail/CVE-2023-34454]
{quote}Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing an unrecoverable fatal error. 
{quote}

[https://nvd.nist.gov/vuln/detail/CVE-2023-34453]
{quote}Due to unchecked multiplications, an integer overflow may occur in versions prior to 1.1.10.1, causing a fatal error.
{quote}
",N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18585,Alter Type does not validate changes like Create Type does,"Create Type attempts to block undesired field types, but this validation is not cared over to Alter Type Add Field; which allows you to add unexpected/desired types

{code}
Assertions.assertThatThrownBy(() -> createType(""CREATE TYPE %s (f counter)"")).hasRootCauseMessage(""A user type cannot contain counters"");
String type = createType(KEYSPACE, ""CREATE TYPE %s (a int)"");
schemaChange(String.format(""ALTER TYPE %s.%s ADD f counter"", KEYSPACE, type));
UserType udt = Keyspace.open(KEYSPACE).getMetadata().types.get(UTF8Type.instance.decompose(type)).get();
logger.warn(""UDT: {}"", udt);
{code}

{code}
UDT: org.apache.cassandra.db.marshal.UserType(cql_test_keyspace,747970655f3031,61:org.apache.cassandra.db.marshal.Int32Type,66:org.apache.cassandra.db.marshal.CounterColumnType)
{code}",N/A,"3.0.30, 3.11.17, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-18583,Fix shutting down IsolatedJmx ,"{code:java}
public class MyTest extends TestBaseImpl
{
    @Test
    public void test() throws Throwable
    {
        try (Cluster cluster = Cluster.build(2).withConfig(c -> c.with(Feature.values())).start())
        {
            ClusterUtils.stopUnchecked(cluster.get(2));
            System.out.println(cluster.get(1).nodetoolResult(""status"").getStderr());
        }
    }
}  {code}
This will fail on this:
{code:java}
java.net.BindException: Address already in use (Bind failed)
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:387)
	at java.net.ServerSocket.bind(ServerSocket.java:390)
	at java.net.ServerSocket.<init>(ServerSocket.java:252)
	at javax.net.DefaultServerSocketFactory.createServerSocket(ServerSocketFactory.java:231)
	at org.apache.cassandra.distributed.impl.CollectingRMIServerSocketFac
{code}

The problem is that IsolatedJMX clears whole map of TCPTransports in clearMapField as part of the stopJmx method. This results in JMX internals thinking the socket is not there so it tries to create it but it fails to do so because it is technically still bound.

The fix is consisting of cleaning just the bits belonging to that specific IsolatedJMX instance.
The investigation of the issue done by myself, the actual fix by [~drohrer]. Found while working on CASSANDRA-18572.",N/A,"3.11.x, 4.0.x, 4.1.x, 5.x"
CASSANDRA-18575,Backport Cassandra-10508 Remove hard-coded SSL cipher suites and protocols,Cassandra 3.0 has ciphers hard coded and thus not allow more recent and secure ciphers for storage connections complicating migrations to later versions.,N/A,3.0.30
CASSANDRA-18562,guava vulnerability CVE-2023-2976,This is failing the OWASP check.,N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18558,remove dh_python use from debian packaging,"It looks like dh_python2 has been removed from debian, but it also looks like we don't need it:

{noformat}
E: dh_python2 dh_python2:408: no package to act on (python-foo or one with ${python:Depends} in Depends)
{noformat}",N/A,"2.2.20, 3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18553,Generate.sh -s param to skip autodetection of tests,When using generate.sh auto detection of modified tests always kicks in. That can be a problem during dev when you want to test a given set of tests without getting all the others in the way. Also when you want to run the script without having to checkout the extra branches auto detection needs.,N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18552,Debian packaging source should exclude git subdirectory,This balloons the source up to 400+MB instead of the ~13MB necessary.,N/A,"3.0.29, 3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18543,Waiting for gossip to settle does not wait for live endpoints,"When a node starts it will get endpoint states (via shadow round) but have all nodes marked as down. The problem is the wait to settle only checks the size of endpoint states is stable before starting Native transport. Once native transport starts it will receive queries and fail consistency levels such as LOCAL_QUORUM since it still thinks nodes are down.

This is problem for a number of large clusters for our customers. The cluster has quorum but due to this issue a node restart is causing a bunch of query errors.

My initial solution to this was to only check live endpoints size in addition to size of endpoint states. This worked but I noticed in testing this fix that there also a lot of duplication of checking the same node (via Echo messages) for liveness. So the patch also removes this duplication of checking node is UP in markAlive.

The final problem I found while testing is sometimes could still not see a change in live endpoints due to only 1 second polling, so the patch allows for overridding the settle parameters. I could not reliability reproduce this but think its worth providing a way to override these hardcoded values.",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18511,Add support for JMX in the in-jvm dtest framework,"In many cases, it would be useful to be able to enable JMX endpoints within the in-jvm dtest framework, including the existing JMX Getter test, which used to simply spin up a JMX registry and then leave it running.  There are quite a few JMX-related functions that don’t have tests today, and some external usages of the in-jvm dtest framework could also benefit from exposing JMX like we did Native before.",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18503,CircleCI java distributed tests to run in Medium containers,"With the paid configuration we have in-tree it is enough to run the java distributed regular and upgrade tests in Medium containers, instead of Large containers",N/A,"3.11.16, 4.0.12, 4.1.4, 5.0-alpha1, 5.0, 5.1"
CASSANDRA-18497,snakeyaml vulnerability: CVE-2023-2251,"This is failing the OWASP scan.

https://nvd.nist.gov/vuln/detail/CVE-2023-2251",N/A,"3.0.29, 3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18472,Docker images can no longer be built due to virtualenv from pip,"{noformat}
 => [linux/amd64 35/56] WORKDIR /home/cassandra                                                                                                                                                              0.1s
 => [linux/amd64 36/56] RUN echo 'export ANT_HOME=/usr/share/ant' >> /home/cassandra/.bashrc &&     echo 'export JAVA8_HOME=/usr/lib/jvm/java-8-openjdk-$(dpkg --print-architecture)' >> /home/cassandra/.b  0.2s
 => ERROR [linux/amd64 37/56] RUN virtualenv --python=python2.7 env2.7                                                                                                                                       0.5s
------
 > [linux/amd64 37/56] RUN virtualenv --python=python2.7 env2.7:
#100 0.424 RuntimeError: failed to find interpreter for Builtin discover of python_spec='python2.7'
------
ubuntu2004_j11.docker:128
--------------------
 126 |     # included in the base image, the compiled objects are not updated by pip at run time, which can
 127 |     # cause errors if the tests rely on new driver functionality or bug fixes.
 128 | >>> RUN virtualenv --python=python2.7 env2.7
 129 |     RUN chmod +x env2.7/bin/activate
 130 |     RUN /bin/bash -c ""export CASS_DRIVER_NO_CYTHON=1 CASS_DRIVER_NO_EXTENSIONS=1 && source ~/env2.7/bin/activate && pip2 install --upgrade pip && pip2 install -r /opt/requirements.txt && pip2 freeze --user""
--------------------
error: failed to solve: rpc error: code = Unknown desc = process ""/bin/sh -c virtualenv --python=python2.7 env2.7"" did not complete successfully: exit code: 1
{noformat}",N/A,"3.0.29, 3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18448,"Missing ""SSTable Count"" metric  when using nodetool with ""--format"" option","Hi, 

I'm using ""nodetool cfstats --format json"" to gather some metrics/infomation about our tables. 
I noticed that the ""SSTable Count"" is missing when using ""–format"" option. 

If I don't use ""–format""  option, I can set ""SSTable Count"" in the output. 

*Output of ""nodetool cfstats --format json | jq"":* 
{code:java}
{  ""total_number_of_tables"": 38,  ""stress_test"": {    ""write_latency_ms"": 0.8536725334338424,    ""tables"": {      ""res1"": {        ""average_tombstones_per_slice_last_five_minutes"": null,        ""bloom_filter_off_heap_memory_used"": ""159256"",        ""memtable_switch_count"": 754,        ""maximum_tombstones_per_slice_last_five_minutes"": 0,        ""memtable_cell_count"": 0,        ""memtable_data_size"": ""0"",        ""average_live_cells_per_slice_last_five_minutes"": null,        ""local_read_latency_ms"": ""NaN"",        ""local_write_latency_ms"": ""NaN"",        ""pending_flushes"": 0,        ""compacted_partition_minimum_bytes"": 785940,        ""local_read_count"": 0,        ""sstable_compression_ratio"": 0.6294161376582798,        ""dropped_mutations"": ""52751"",        ""bloom_filter_false_positives"": 0,        ""off_heap_memory_used_total"": ""58842196"",        ""memtable_off_heap_memory_used"": ""0"",        ""index_summary_off_heap_memory_used"": ""18972"",        ""bloom_filter_space_used"": ""159408"",        ""sstables_in_each_level"": [],        ""compacted_partition_maximum_bytes"": 4055269,        ""space_used_total"": ""302694398635"",        ""local_write_count"": 297111,        ""compression_metadata_off_heap_memory_used"": ""58663968"",        ""number_of_partitions_estimate"": 99614,        ""maximum_live_cells_per_slice_last_five_minutes"": 0,        ""space_used_live"": ""302694398635"",        ""compacted_partition_mean_bytes"": 3827283,        ""bloom_filter_false_ratio"": ""0.00000"",        ""percent_repaired"": 0,        ""space_used_by_snapshots_total"": ""0""      }    },    ""read_latency_ms"": null,    ""pending_flushes"": 0,    ""write_count"": 594308,    ""read_latency"": null,    ""read_count"": 0  }}
 {code}
*Output of ""nodetool cfstats"":* 
{code:java}
----------------
Keyspace : stress_test
        Read Count: 0
        Read Latency: NaN ms
        Write Count: 594308
        Write Latency: 0.8536725334338424 ms
        Pending Flushes: 0
                Table: res1
                SSTable count: 19                
                Space used (live): 302694398635
                Space used (total): 302694398635
                Space used by snapshots (total): 0
                Off heap memory used (total): 58842196
                SSTable Compression Ratio: 0.6294161376582798
                Number of partitions (estimate): 99614
                Memtable cell count: 0
                Memtable data size: 0
                Memtable off heap memory used: 0
                Memtable switch count: 754
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 297111
                Local write latency: NaN ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 159408
                Bloom filter off heap memory used: 159256
                Index summary off heap memory used: 18972
                Compression metadata off heap memory used: 58663968
                Compacted partition minimum bytes: 785940
                Compacted partition maximum bytes: 4055269
                Compacted partition mean bytes: 3827283
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 52751*  
----------------
 {code}
 ",N/A,"3.11.15, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18401,Investigate preloading ccm repositories in the docker image,"In CASSANDRA-18391 it was discovered that to skip some upgrade tests, the ccm repository first needed to be populated with older versions.  While that case was solved, it may still be beneficial to preload the ccm repositories in the docker image so they don't need to be fetched at all.",N/A,"3.0.29, 3.11.16, 4.0.10, 4.1.2"
CASSANDRA-18396,Dtests marked with @ported_to_in_jvm can be skipped since 4.1,"During the CASSANDRA-15536 epic we ported multiple Python dtests to in-JVM dtests.

The ported Python dtests are still present but marked with a new {{@ported_to_in_jvm}} annotation. JVM dtests didn't support vnodes at that time, so when a Python dtest is marked with that annotation it's only run for vnodes config, whereas it's skipped if vnodes are off.

However, we have had support for vnodes on JVM dtests since 4.1. Thus, I think we should modify the {{@ported_to_in_jvm}} annotation to also skip configs with vnodes if all the nodes are in 4.1 or later.",N/A,"3.0.29, 3.11.15, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18391,consistent timeout: dtest-upgrade.upgrade_tests.cql_tests.cls.test_cql3_non_compound_range_tombstones on trunk,"Failed 30 times in the last 30 runs. Flakiness: 0%, Stability: 0%

link: https://ci-cassandra.apache.org/job/Cassandra-trunk/1511/testReport/dtest-upgrade.upgrade_tests.cql_tests/cls/test_cql3_non_compound_range_tombstones/
Error message: failed on setup with ""Failed: Timeout >900.0s""",N/A,"3.0.29, 3.11.15, 4.0.9, 5.0-alpha1, 5.0"
CASSANDRA-18389,jackson-core-2.13.2.jar vulnerability: CVE-2022-45688,This is currently failing in the OWASP scan.,N/A,"3.11.15, 4.0.9, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18346,Error Unknown column during deserialization missing keyspace and table name,"The ERROR message generated in ColumnSubselection.java when a column name is not found only prints the column name, not the keyspace and table.  It can be difficult to track down the source when more than one table uses the same name.  E.g., 'id'.
{quote}{{if (column == null)}}
{
{{        column = metadata.getDroppedColumn(name);}}
{{        if (column == null)}}
{{                throw new UnknownColumnException(""Unknown column "" + UTF8Type.instance.getString(name) + "" during deserialization"");}}
{{}}}
{quote}
Example:

[ERROR] cluster_id=15 ip_address=192.168.65.10  java.lang.RuntimeException: Unknown column id during deserialization

Proposed:

[ERROR] cluster_id=15 ip_address=192.168.65.10  java.lang.RuntimeException: Unknown column id in table cycling.route during deserialization",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-18336,Do not remove SSTables when cause of FSReadError is OutOfMemoryError while using best_effort disk failure policy,"1.When this exception occurs in the system
{code:java}
// 
ERROR [CompactionExecutor:351627] 2023-02-21 17:59:20,721 CassandraDaemon.java:581 - Exception in thread Thread[CompactionExecutor:351627,1,main]
org.apache.cassandra.io.FSReadError: java.io.IOException: Map failed
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:167)
    at org.apache.cassandra.io.util.MmappedRegions$State.add(MmappedRegions.java:310)
    at org.apache.cassandra.io.util.MmappedRegions$State.access$400(MmappedRegions.java:246)
    at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:170)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:73)
    at org.apache.cassandra.io.util.MmappedRegions.<init>(MmappedRegions.java:61)
    at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104)
    at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:365)
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:337)
    at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:172)
    at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:124)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:64)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:137)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:193)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:77)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:100)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:298)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.IOException: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1016)
    at org.apache.cassandra.io.util.ChannelProxy.map(ChannelProxy.java:163)
    ... 23 common frames omitted
Caused by: java.lang.OutOfMemoryError: Map failed
    at java.base/sun.nio.ch.FileChannelImpl.map0(Native Method)
    at java.base/sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:1013)


{code}
2.Restart the node, Verifying logfile transaction ,All sstables are deleted
{code:java}
// code placeholder
INFO  [main] 2023-02-21 18:00:23,350 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Index.db 
INFO  [main] 2023-02-21 18:00:23,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8819408-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_c923b230-b077-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,510 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,517 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,518 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830658-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,521 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,526 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830657-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830660-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,541 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,543 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,545 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,546 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830659-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_461935b0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,550 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,577 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,579 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828386-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,580 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,584 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,585 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828385-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,590 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,592 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,602 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828384-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,606 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_69071e60-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,610 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,644 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,684 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828320-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,685 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,687 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,688 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,727 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828183-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,728 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Filter.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Summary.db 
INFO  [main] 2023-02-21 18:00:46,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Data.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:46,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:46,771 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828255-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:46,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_8b8205e0-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:46,775 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:46,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Index.db 
INFO  [main] 2023-02-21 18:00:46,787 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830650-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,020 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_008f3d00-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,022 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,072 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828337-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,074 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,077 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,078 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,092 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828375-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,093 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,097 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,098 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,113 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828354-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_6f265950-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,118 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,133 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,134 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827806-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,247 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,255 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828112-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,369 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Index.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Filter.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Summary.db 
INFO  [main] 2023-02-21 18:00:47,374 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Data.db 
INFO  [main] 2023-02-21 18:00:47,484 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Digest.crc32 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-Statistics.db 
INFO  [main] 2023-02-21 18:00:47,485 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827506-big-TOC.txt 
INFO  [main] 2023-02-21 18:00:47,490 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_fb014430-b18e-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:00:47,492 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:00:47,502 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Index.db 
INFO  [main] 2023-02-21 18:00:48,045 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Filter.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Summary.db 
INFO  [main] 2023-02-21 18:00:48,053 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,166 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,202 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7859661-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,272 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,276 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Data.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:21,500 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830374-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:21,501 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Index.db 
INFO  [main] 2023-02-21 18:01:21,841 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Filter.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Summary.db 
INFO  [main] 2023-02-21 18:01:21,842 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Data.db 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Digest.crc32 
INFO  [main] 2023-02-21 18:01:22,779 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-Statistics.db 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821887-big-TOC.txt 
INFO  [main] 2023-02-21 18:01:22,780 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Filter.db 
INFO  [main] 2023-02-21 18:01:22,825 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Index.db 
INFO  [main] 2023-02-21 18:01:24,891 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Summary.db 
INFO  [main] 2023-02-21 18:01:24,892 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,190 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,461 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5298119-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,462 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Index.db 
INFO  [main] 2023-02-21 18:02:02,466 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Filter.db 
INFO  [main] 2023-02-21 18:02:02,467 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Data.db 
INFO  [main] 2023-02-21 18:02:02,763 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Summary.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:02,764 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8826214-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:02,765 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Index.db 
INFO  [main] 2023-02-21 18:02:05,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Filter.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Summary.db 
INFO  [main] 2023-02-21 18:02:05,388 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Data.db 
INFO  [main] 2023-02-21 18:02:41,367 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Digest.crc32 
INFO  [main] 2023-02-21 18:02:41,368 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-Statistics.db 
INFO  [main] 2023-02-21 18:02:41,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7315697-big-TOC.txt 
INFO  [main] 2023-02-21 18:02:41,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Index.db 
INFO  [main] 2023-02-21 18:02:42,034 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Filter.db 
INFO  [main] 2023-02-21 18:02:42,049 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,731 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,732 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6687036-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,770 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Index.db 
INFO  [main] 2023-02-21 18:04:02,784 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Filter.db 
INFO  [main] 2023-02-21 18:04:02,785 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Data.db 
INFO  [main] 2023-02-21 18:04:02,889 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Summary.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829808-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:02,890 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Index.db 
INFO  [main] 2023-02-21 18:04:03,384 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Filter.db 
INFO  [main] 2023-02-21 18:04:03,418 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Data.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Summary.db 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Digest.crc32 
INFO  [main] 2023-02-21 18:04:38,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-Statistics.db 
INFO  [main] 2023-02-21 18:04:38,245 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6839605-big-TOC.txt 
INFO  [main] 2023-02-21 18:04:38,246 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Filter.db 
INFO  [main] 2023-02-21 18:04:38,293 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Index.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Summary.db 
INFO  [main] 2023-02-21 18:04:39,438 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Data.db 
INFO  [main] 2023-02-21 18:05:28,014 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Digest.crc32 
INFO  [main] 2023-02-21 18:05:28,015 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-Statistics.db 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5373453-big-TOC.txt 
INFO  [main] 2023-02-21 18:05:28,041 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Filter.db 
INFO  [main] 2023-02-21 18:05:28,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Index.db 
INFO  [main] 2023-02-21 18:05:28,277 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Data.db 
INFO  [main] 2023-02-21 18:06:17,552 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,553 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:17,554 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:17,565 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5607362-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:17,566 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Index.db 
INFO  [main] 2023-02-21 18:06:17,567 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Filter.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Summary.db 
INFO  [main] 2023-02-21 18:06:17,568 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Data.db 
INFO  [main] 2023-02-21 18:06:24,899 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Digest.crc32 
INFO  [main] 2023-02-21 18:06:24,900 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:06:24,932 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-Statistics.db 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5301216-big-TOC.txt 
INFO  [main] 2023-02-21 18:06:24,933 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Filter.db 
INFO  [main] 2023-02-21 18:06:24,949 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Index.db 
INFO  [main] 2023-02-21 18:06:29,880 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Data.db 
INFO  [main] 2023-02-21 18:08:11,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:11,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5450265-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:11,667 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Index.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Filter.db 
INFO  [main] 2023-02-21 18:08:11,717 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Summary.db 
INFO  [main] 2023-02-21 18:08:11,718 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Data.db 
INFO  [main] 2023-02-21 18:08:22,177 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Digest.crc32 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-Statistics.db 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8741761-big-TOC.txt 
INFO  [main] 2023-02-21 18:08:22,178 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Filter.db 
INFO  [main] 2023-02-21 18:08:22,212 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Index.db 
INFO  [main] 2023-02-21 18:08:22,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Summary.db 
INFO  [main] 2023-02-21 18:08:22,642 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Data.db 
INFO  [main] 2023-02-21 18:09:16,035 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:16,036 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:16,162 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5496274-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:16,163 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Index.db 
INFO  [main] 2023-02-21 18:09:16,302 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Filter.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Summary.db 
INFO  [main] 2023-02-21 18:09:16,303 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Data.db 
INFO  [main] 2023-02-21 18:09:30,352 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Digest.crc32 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:09:30,353 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-Statistics.db 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-6882038-big-TOC.txt 
INFO  [main] 2023-02-21 18:09:30,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Filter.db 
INFO  [main] 2023-02-21 18:09:30,377 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Index.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Summary.db 
INFO  [main] 2023-02-21 18:09:32,789 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,487 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,692 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,741 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,742 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5356040-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Filter.db 
INFO  [main] 2023-02-21 18:10:17,743 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Index.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Summary.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Data.db 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Digest.crc32 
INFO  [main] 2023-02-21 18:10:17,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-Statistics.db 
INFO  [main] 2023-02-21 18:10:17,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821799-big-TOC.txt 
INFO  [main] 2023-02-21 18:10:17,760 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Index.db 
INFO  [main] 2023-02-21 18:10:18,081 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Filter.db 
INFO  [main] 2023-02-21 18:10:18,117 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Data.db 
INFO  [main] 2023-02-21 18:11:06,042 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Summary.db 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:06,043 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:06,079 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6087561-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:06,080 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Index.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Filter.db 
INFO  [main] 2023-02-21 18:11:06,159 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Data.db 
INFO  [main] 2023-02-21 18:11:16,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Summary.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:16,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:16,902 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8790094-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:16,903 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Index.db 
INFO  [main] 2023-02-21 18:11:17,170 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Filter.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Summary.db 
INFO  [main] 2023-02-21 18:11:17,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Data.db 
INFO  [main] 2023-02-21 18:11:59,054 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Digest.crc32 
INFO  [main] 2023-02-21 18:11:59,055 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-Statistics.db 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5504090-big-TOC.txt 
INFO  [main] 2023-02-21 18:11:59,076 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Index.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Filter.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Summary.db 
INFO  [main] 2023-02-21 18:11:59,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Data.db 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:28,397 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8132762-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:28,398 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Index.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Filter.db 
INFO  [main] 2023-02-21 18:12:28,400 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Data.db 
INFO  [main] 2023-02-21 18:12:42,749 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:42,750 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:42,774 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6383747-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Filter.db 
INFO  [main] 2023-02-21 18:12:42,775 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Index.db 
INFO  [main] 2023-02-21 18:12:42,820 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Summary.db 
INFO  [main] 2023-02-21 18:12:42,821 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Data.db 
INFO  [main] 2023-02-21 18:12:55,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Digest.crc32 
INFO  [main] 2023-02-21 18:12:55,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-Statistics.db 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5649641-big-TOC.txt 
INFO  [main] 2023-02-21 18:12:55,630 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Filter.db 
INFO  [main] 2023-02-21 18:12:55,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Index.db 
INFO  [main] 2023-02-21 18:12:57,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Data.db 
INFO  [main] 2023-02-21 18:15:07,614 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Summary.db 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:07,615 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:07,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5530049-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:07,641 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Index.db 
INFO  [main] 2023-02-21 18:15:07,909 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Filter.db 
INFO  [main] 2023-02-21 18:15:07,950 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,262 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,263 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,325 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8694773-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,326 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,327 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,535 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8828410-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,536 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,550 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Summary.db 
INFO  [main] 2023-02-21 18:15:58,551 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Data.db 
INFO  [main] 2023-02-21 18:15:58,856 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Digest.crc32 
INFO  [main] 2023-02-21 18:15:58,857 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-Statistics.db 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8827236-big-TOC.txt 
INFO  [main] 2023-02-21 18:15:58,858 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Index.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Filter.db 
INFO  [main] 2023-02-21 18:15:58,860 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Data.db 
INFO  [main] 2023-02-21 18:16:19,652 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Summary.db 
INFO  [main] 2023-02-21 18:16:19,653 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Digest.crc32 
INFO  [main] 2023-02-21 18:16:19,654 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-Statistics.db 
INFO  [main] 2023-02-21 18:16:19,664 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5499870-big-TOC.txt 
INFO  [main] 2023-02-21 18:16:19,665 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:16:19,772 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Data.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,537 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5221771-big-Summary.db 
INFO  [main] 2023-02-21 18:17:13,538 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Index.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Filter.db 
INFO  [main] 2023-02-21 18:17:13,549 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,232 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8807317-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,233 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Summary.db 
INFO  [main] 2023-02-21 18:17:14,236 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Data.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:14,339 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830077-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:14,340 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Index.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Filter.db 
INFO  [main] 2023-02-21 18:17:14,354 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Data.db 
INFO  [main] 2023-02-21 18:17:17,432 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Digest.crc32 
INFO  [main] 2023-02-21 18:17:17,433 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:17:17,449 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-Statistics.db 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8816850-big-TOC.txt 
INFO  [main] 2023-02-21 18:17:17,450 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Index.db 
INFO  [main] 2023-02-21 18:17:17,969 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Filter.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Summary.db 
INFO  [main] 2023-02-21 18:17:17,970 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,709 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,710 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5790369-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,711 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Index.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Filter.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Summary.db 
INFO  [main] 2023-02-21 18:18:43,739 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Data.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:43,885 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8829549-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:43,886 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Index.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Filter.db 
INFO  [main] 2023-02-21 18:18:44,106 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Data.db 
INFO  [main] 2023-02-21 18:18:47,122 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821888-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:47,123 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Index.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Filter.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Summary.db 
INFO  [main] 2023-02-21 18:18:47,199 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,411 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,412 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-6861414-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,413 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,414 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Data.db 
INFO  [main] 2023-02-21 18:18:49,634 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Digest.crc32 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-Statistics.db 
INFO  [main] 2023-02-21 18:18:49,635 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8820208-big-TOC.txt 
INFO  [main] 2023-02-21 18:18:49,636 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Index.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Filter.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Summary.db 
INFO  [main] 2023-02-21 18:18:49,699 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Data.db 
INFO  [main] 2023-02-21 18:19:17,136 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:17,137 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8413340-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:17,253 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Index.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Filter.db 
INFO  [main] 2023-02-21 18:19:17,310 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Data.db 
INFO  [main] 2023-02-21 18:19:36,881 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:36,882 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7039803-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:36,883 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Filter.db 
INFO  [main] 2023-02-21 18:19:36,884 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Index.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Summary.db 
INFO  [main] 2023-02-21 18:19:36,917 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Data.db 
INFO  [main] 2023-02-21 18:19:45,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Digest.crc32 
INFO  [main] 2023-02-21 18:19:45,482 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-Statistics.db 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5428383-big-TOC.txt 
INFO  [main] 2023-02-21 18:19:45,483 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Filter.db 
INFO  [main] 2023-02-21 18:19:45,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Index.db 
INFO  [main] 2023-02-21 18:19:45,639 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Summary.db 
INFO  [main] 2023-02-21 18:19:45,640 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Data.db 
INFO  [main] 2023-02-21 18:20:48,586 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:48,587 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:48,618 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5580716-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:48,619 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Index.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Filter.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Summary.db 
INFO  [main] 2023-02-21 18:20:48,666 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Data.db 
INFO  [main] 2023-02-21 18:20:51,050 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Digest.crc32 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-Statistics.db 
INFO  [main] 2023-02-21 18:20:51,051 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8781750-big-TOC.txt 
INFO  [main] 2023-02-21 18:20:51,052 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Filter.db 
INFO  [main] 2023-02-21 18:20:51,061 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Index.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Summary.db 
INFO  [main] 2023-02-21 18:20:51,121 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,118 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,140 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5400271-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Summary.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Data.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:19,141 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821263-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:19,142 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Index.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Filter.db 
INFO  [main] 2023-02-21 18:21:19,160 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Data.db 
INFO  [main] 2023-02-21 18:21:22,503 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Summary.db 
INFO  [main] 2023-02-21 18:21:22,504 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Digest.crc32 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-Statistics.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8744460-big-TOC.txt 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Filter.db 
INFO  [main] 2023-02-21 18:21:22,505 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Index.db 
INFO  [main] 2023-02-21 18:21:22,660 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Data.db 
INFO  [main] 2023-02-21 18:22:11,241 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:11,242 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-5334445-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:11,244 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Index.db 
INFO  [main] 2023-02-21 18:22:11,328 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Filter.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Summary.db 
INFO  [main] 2023-02-21 18:22:11,335 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Data.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:42,109 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/mc-217343-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:42,110 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Index.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Filter.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Summary.db 
INFO  [main] 2023-02-21 18:22:42,481 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,519 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,520 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,539 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-7588148-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,540 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,570 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Data.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821882-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:50,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Index.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Filter.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Summary.db 
INFO  [main] 2023-02-21 18:22:50,736 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Data.db 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:51,729 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8821885-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:51,730 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Index.db 
INFO  [main] 2023-02-21 18:22:51,758 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Filter.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Summary.db 
INFO  [main] 2023-02-21 18:22:51,759 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,456 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Digest.crc32 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-CompressionInfo.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-Statistics.db 
INFO  [main] 2023-02-21 18:22:54,457 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8716073-big-TOC.txt 
INFO  [main] 2023-02-21 18:22:54,646 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_unknowncompactiontype_695c4f33-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,648 LogTransaction.java:536 - Verifying logfile transaction [nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log in /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b]
INFO  [main] 2023-02-21 18:22:54,650 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Index.db 
INFO  [main] 2023-02-21 18:22:54,656 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830665-big-Data.db 
INFO  [main] 2023-02-21 18:22:54,673 LogTransaction.java:240 - Unfinished transaction log, deleting /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb_txn_compaction_60e393e0-b1ce-11ed-a081-5d5a5c990823.log 
INFO  [main] 2023-02-21 18:22:54,694 Keyspace.java:386 - Creating replication strategy kairosdb params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}
INFO  [main] 2023-02-21 18:22:54,715 ColumnFamilyStore.java:385 - Initializing kairosdb.data_points
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:54,720 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830647-big (179.084MiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830662-big (4.039MiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830663-big (3.589MiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830661-big (39.789MiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:54,721 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830664-big (6.007MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830583-big (190.543MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,739 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830440-big (191.089MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:54,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/data_points-870fab7087ba11eb8b50d3c6960df21b/nb-8830513-big (194.560MiB)
INFO  [main] 2023-02-21 18:22:54,947 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:54,977 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_index-8742543087ba11eba3799bdca9e7ad04/mc-1-big (7.580MiB)
INFO  [main] 2023-02-21 18:22:55,023 ColumnFamilyStore.java:385 - Initializing kairosdb.row_key_time_index
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,054 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26770-big (0.075KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,070 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26769-big (0.052KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,077 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_key_time_index-87a4234087ba11eba3799bdca9e7ad04/nb-26768-big (2.671MiB)
INFO  [main] 2023-02-21 18:22:55,131 ColumnFamilyStore.java:385 - Initializing kairosdb.row_keys
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,135 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-796510-big (7.682MiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,190 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-769597-big (50.002MiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,203 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-75-big (87.496MiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,209 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/mc-256221-big (51.492MiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,211 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/kairosdb/row_keys-8793f6a087ba11eb8b50d3c6960df21b/nb-550752-big (50.323MiB)
INFO  [main] 2023-02-21 18:22:55,357 ColumnFamilyStore.java:385 - Initializing kairosdb.service_index
INFO  [main] 2023-02-21 18:22:55,381 ColumnFamilyStore.java:385 - Initializing kairosdb.spec
INFO  [main] 2023-02-21 18:22:55,393 ColumnFamilyStore.java:385 - Initializing kairosdb.string_index
INFO  [main] 2023-02-21 18:22:55,409 ColumnFamilyStore.java:385 - Initializing kairosdb.tag_indexed_row_keys
INFO  [main] 2023-02-21 18:22:55,419 Keyspace.java:386 - Creating replication strategy system_auth params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}
INFO  [main] 2023-02-21 18:22:55,425 ColumnFamilyStore.java:385 - Initializing system_auth.network_permissions
INFO  [main] 2023-02-21 18:22:55,440 ColumnFamilyStore.java:385 - Initializing system_auth.resource_role_permissons_index
INFO  [main] 2023-02-21 18:22:55,457 ColumnFamilyStore.java:385 - Initializing system_auth.role_members
INFO  [main] 2023-02-21 18:22:55,473 ColumnFamilyStore.java:385 - Initializing system_auth.role_permissions
INFO  [main] 2023-02-21 18:22:55,485 ColumnFamilyStore.java:385 - Initializing system_auth.roles
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,518 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_auth/roles-5bc52802de2535edaeab188eecebb090/mc-1-big (0.100KiB)
INFO  [main] 2023-02-21 18:22:55,543 Keyspace.java:386 - Creating replication strategy system_distributed params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}
INFO  [main] 2023-02-21 18:22:55,558 ColumnFamilyStore.java:385 - Initializing system_distributed.parent_repair_history
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,577 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-49-big (1.398KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,585 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-44-big (1.376KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,591 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-24-big (0.863KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,593 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-5-big (0.644KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,594 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-12-big (1.130KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,595 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-16-big (0.990KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-39-big (1.646KiB)
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,598 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-57-big (2.019KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,605 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-68-big (0.920KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,606 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-107-big (0.728KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,607 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-43-big (0.592KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,608 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-31-big (1.451KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-80-big (0.944KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,611 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-106-big (0.589KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,622 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-88-big (0.935KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-20-big (1.151KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,623 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-76-big (1.481KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,624 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/mc-1-big (16.284KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-95-big (3.737KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,626 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-84-big (1.031KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,633 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-64-big (1.132KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,636 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-100-big (0.561KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,656 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-99-big (1.159KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,683 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/nb-104-big (0.720KiB)
INFO  [main] 2023-02-21 18:22:55,733 ColumnFamilyStore.java:385 - Initializing system_distributed.repair_history
INFO  [SSTableBatchOpen:14] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-45-big (2.741KiB)
INFO  [SSTableBatchOpen:2] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-25-big (1.789KiB)
INFO  [SSTableBatchOpen:7] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-89-big (1.814KiB)
INFO  [SSTableBatchOpen:3] 2023-02-21 18:22:55,747 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-112-big (1.327KiB)
INFO  [SSTableBatchOpen:19] 2023-02-21 18:22:55,749 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-32-big (3.927KiB)
INFO  [SSTableBatchOpen:4] 2023-02-21 18:22:55,756 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-67-big (3.370KiB)
INFO  [SSTableBatchOpen:18] 2023-02-21 18:22:55,757 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-81-big (3.249KiB)
INFO  [SSTableBatchOpen:10] 2023-02-21 18:22:55,758 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-110-big (1.354KiB)
INFO  [SSTableBatchOpen:11] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-44-big (1.250KiB)
INFO  [SSTableBatchOpen:12] 2023-02-21 18:22:55,761 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-50-big (3.340KiB)
INFO  [SSTableBatchOpen:6] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-93-big (2.319KiB)
INFO  [SSTableBatchOpen:21] 2023-02-21 18:22:55,765 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-85-big (1.789KiB)
INFO  [SSTableBatchOpen:1] 2023-02-21 18:22:55,770 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-113-big (1.334KiB)
INFO  [SSTableBatchOpen:15] 2023-02-21 18:22:55,773 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-101-big (3.239KiB)
INFO  [SSTableBatchOpen:22] 2023-02-21 18:22:55,774 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-21-big (2.331KiB)
INFO  [SSTableBatchOpen:8] 2023-02-21 18:22:55,785 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-40-big (4.339KiB)
INFO  [SSTableBatchOpen:23] 2023-02-21 18:22:55,789 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-17-big (2.308KiB)
INFO  [SSTableBatchOpen:13] 2023-02-21 18:22:55,794 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-106-big (0.766KiB)
INFO  [SSTableBatchOpen:16] 2023-02-21 18:22:55,797 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-13-big (2.822KiB)
INFO  [SSTableBatchOpen:9] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-105-big (2.460KiB)
INFO  [SSTableBatchOpen:5] 2023-02-21 18:22:55,802 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-6-big (1.802KiB)
INFO  [SSTableBatchOpen:24] 2023-02-21 18:22:55,805 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-74-big (2.896KiB)
INFO  [SSTableBatchOpen:17] 2023-02-21 18:22:55,808 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/mc-1-big (0.812KiB)
INFO  [SSTableBatchOpen:20] 2023-02-21 18:22:55,811 SSTableReaderBuilder.java:351 - Opening /historyData/cassandra/data/system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/nb-59-big (5.973KiB) {code}
3.Bugs can be reproduced.Just set  vm.max_ map_ count as a small value, and then trigger OOM, and restart the node.",N/A,"3.0.29, 3.11.15, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18294,die disk failure policy will not kill jvm as documented,"After Cassandra has successfully starts up with disk_failure_policy die, when encounter a file system error, Cassandra server will only throw exception instead of shut down gossip and client transport and kill JVM. Document: [https://cassandra.apache.org/doc/latest/cassandra/configuration/cass_yaml_file.html#disk_failure_policy]

 

The reason for this is the default FS error handler is not handing policy die correctly. Instead of shutting down gossip and native transport, it throws an error.

 

The JVMStabilityInspectorTest is passing because the error handler is not set so no exception is thrown.",N/A,"3.0.29, 4.0.9, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18261,Update and unify copyright year,"Stefan suggested we update the year in various place for our copyright, and I noticed in build.xml we do this differently between branches.  In 3.0 we use ${YEAR} and then in later branches we have hardcoded dates.  This ticket serves to update these and unify how we specify them across branches.",N/A,"3.0.29, 3.11.15, 4.0.9, 4.1.1"
CASSANDRA-18249,Docker image for releases,"The release process is currently driven bu this [doc|https://github.com/apache/cassandra-website/blob/trunk/site-content/source/modules/ROOT/pages/development/release_process.adoc] mainly.

Some of the requirements are a Debian based distro and a number of packages. As recently discovered this could be a problem, Ubuntu 20.04 LTS doesn't have createrepo available i.e.

To avoid such problems, add repeatability, have a controlled env where releases are cut, enable more people to cut them, etc [~mck] suggested creating a docker image to that purpose probably based off [this|https://github.com/apache/cassandra-builds/blob/trunk/docker/bullseye-image.docker]",N/A,"3.0.29, 3.11.15, 4.0.9, 5.0-alpha1, 5.0"
CASSANDRA-18207,Nodetool documentation not working,"Clicking on the nodetool in the Tool branch of the doco tree doesn't do anything.

Also, links found in Google search (eg [https://cassandra.apache.org/doc/latest/cassandra/tools/nodetool/tablehistograms.html)] result in ""Not Found"" error.",N/A,"3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18198,"""AttributeError: module 'py' has no attribute 'io'"" reported in multiple tests","{{title = 'Timeout'}}
{{stream = <_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>}}
{{{}sep = '+'{}}}{{{}def write_title(title, stream=None, sep=""~""):{}}}
{{{}""""""Write a section title.{}}}{{{}If *stream* is None sys.stderr will be used, *sep* is used to{}}}
{{draw the line.}}
{{""""""}}
{{if stream is None:}}
{{stream = sys.stderr}}
{{> width = py.io.get_terminal_width()}}
{{E AttributeError: module 'py' has no attribute 'io}}

 

is reported in multiple tests as noted below.

possibly a class loader issue associated with CASSANDRA-18150

4.1
[https://ci-cassandra.apache.org/job/Cassandra-4.1/256/testReport/dtest-offheap.repair_tests.incremental_repair_test/TestIncRepair/test_multiple_full_repairs_lcs]

3.11
[https://ci-cassandra.apache.org/job/Cassandra-3.11/424/testReport/dtest.bootstrap_test/TestBootstrap/test_simultaneous_bootstrap/]

3.0
[https://ci-cassandra.apache.org/job/Cassandra-3.0/328/testReport/dtest-upgrade.upgrade_tests.upgrade_supercolumns_test/TestSCUpgrade/test_upgrade_super_columns_through_all_versions/]",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18197,Builds being often aborted because of python3 ./scripts/gen-nodetool-docs.py being slow,"As discussed on CASSANDRA-18181, recently we see builds aborted as python3 ./scripts/gen-nodetool-docs.py is too slow. 

According to [~brandon.williams] most of the time in the script is spent in create_adoc, which is serially forking nodetool help for every command to gather it.

CC [~Anthony Grasso]",N/A,"3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18188,Test failure in upgrade_tests.cql_tests.cls.test_limit_ranges,"https://ci-cassandra.apache.org/job/Cassandra-trunk/1434/testReport/dtest-upgrade.upgrade_tests.cql_tests/cls/test_limit_ranges/

{noformat}
self = <abc.TestCQLNodes2RF1_Upgrade_indev_4_1_x_To_indev_trunk object at 0x7f27f9268e10>

    def test_limit_ranges(self):
        """""" Validate LIMIT option for 'range queries' in SELECT statements """"""
        cursor = self.prepare(ordered=True)
    
        cursor.execute(""""""
            CREATE TABLE clicks (
                userid int,
                url text,
                time bigint,
                PRIMARY KEY (userid, url)
            ) WITH COMPACT STORAGE;
        """""")
    
>       for is_upgraded, cursor in self.do_upgrade(cursor):

upgrade_tests/cql_tests.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
upgrade_tests/upgrade_base.py:197: in do_upgrade
    node1.start(wait_for_binary_proto=True)
../venv/lib/python3.11/site-packages/ccmlib/node.py:896: in start
    node.watch_log_for_alive(self, from_mark=mark)
../venv/lib/python3.11/site-packages/ccmlib/node.py:665: in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
../venv/lib/python3.11/site-packages/ccmlib/node.py:584: in watch_log_for
    time.sleep(1)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

title = 'Timeout'
stream = <_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>
sep = '+'

    def write_title(title, stream=None, sep=""~""):
        """"""Write a section title.
    
        If *stream* is None sys.stderr will be used, *sep* is used to
        draw the line.
        """"""
        if stream is None:
            stream = sys.stderr
>       width = py.io.get_terminal_width()
E       AttributeError: module 'py' has no attribute 'io'

../venv/lib/python3.11/site-packages/pytest_timeout.py:444: AttributeError
{noformat}",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18183,rat targets do not adhere to build.dir property,"I detected this when I was trying to put my build dir to ramdisk. I have plenty of RAM available on my workstation (64GB) and I was thinking about moving ""build"" dir to ramdisk so I could make it faster a little bit and also spare some write cycles to ssd. It can look like irrelevant improvement but I think that if devs are building the project repeatedly times and times again, this can easily add up.
{code:java}
mkdir /tmp/cassandra
# in /etc/fstab
tmpfs /tmp/cassandra tmpfs defaults,noatime,size=2048M,x-gvfs-show,mode=1777 0 0
# then sudo mount -a
# I have worktree setup so the build for each branch will end up in different dir:
# mkdir -p /tmp/cassandra/{trunk,cassandra-4.1,cassandra-4.0,cassandra-3.11,cassandra-3.0}
{code}
Then in build.properties for each respective branch:
{code:java}
ant.gen-doc.skip: true
build.dir: /tmp/cassandra/trunk/build
build.dir.lib: /tmp/cassandra/trunk/build/lib
{code}
The problem with this is that it fails on rat, because there is not ""build.dir"" property used, it is hardcoded to ""build"" but there is not anything to rat on so it will hang.

To have the very same experience, I am also creating a symlink 
 
{code}
ln -s /tmp/cassandra/trunk/build build
{code}

so ""cd build / ls build"" in the root of the repository will take me to ramdisk. The problem with this is that there is ""build/"" in .gitignore but not ""build"" (as file) so the repository is in dirty state. I suggest to add ""build"" to .gitignore as part of this PR as that is just an opportunistic fix really.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18169,Warning at startup in 3.11.11 or above version of Cassandra,"We are seeing the following warning in Cassandra 3.11.11/14 at startup : 
{code:java}
WARN  [main] 2022-12-27 16:41:28,016 CommitLogReplayer.java:253 - Origin of 2 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored
DEBUG [main] 2022-12-27 16:41:28,016 CommitLogReplayer.java:254 - Ignored commitLogIntervals from the following sstables: [/data/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/me-65-big-Data.db, /data/cassandra/data/system/local-7ad54392bcdd35a684174e047860b377/me-64-big-Data.db] {code}

It looks like HostID metadata is missing at startup in the system.local table. 
We noticed that this issue does not exist in the 4.0.X version of Cassandra. 

Could you please fix it in 3.11.X Cassandra? ",N/A,3.11.x
CASSANDRA-18157,Test Failure: TestUpgrade_current_2_1_x_To_indev_3_11_x.test_bootstrap_multidc (upgrade),"Flaky.

{noformat}
Unexpected error found in node logs (see stdout for full details). Errors: [[node4] ""WARN  [MessagingService-Incoming-/127.0.0.1] 2022-12-20 14:00:30,243 IncomingTcpConnection.java:107 - UnknownColumnFamilyException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find table for cfId 5bc52802-de25-35ed-aeab-188eecebb090. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
\tat org.apache.cassandra.config.CFMetaData$Serializer.deserialize(CFMetaData.java:1594)
\tat org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:765)
\tat org.apache.cassandra.db.ReadCommand$Serializer.deserialize(ReadCommand.java:704)
\tat org.apache.cassandra.io.ForwardingVersionedSerializer.deserialize(ForwardingVersionedSerializer.java:50)
\tat org.apache.cassandra.net.MessageIn.read(MessageIn.java:123)
\tat org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:207)
\tat org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:195)
\tat org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:98)""]
{noformat}",N/A,"3.0.x, 3.11.x"
CASSANDRA-18156,Test Failure: repair_tests.deprecated_repair_test.TestDeprecatedRepairNotifications.test_deprecated_repair_error_notification,"Failing since https://ci-cassandra.apache.org/job/Cassandra-3.0/313/testReport/dtest.repair_tests.deprecated_repair_test/TestDeprecatedRepairNotifications which is https://github.com/apache/cassandra/commit/13d495aa7d5b7a7c121fcc9e105f79107c5c2a1c from CASSANDRA-17254 

but can be reproduced earlier in circleci, e.g. https://app.circleci.com/pipelines/github/michaelsembwever/cassandra/64/workflows/f99abc05-aa9b-42de-a424-44ab7c247e76/jobs/3175 

Only two commits are in this time-range (10th to 30th November) in cassandra-dtest
 - https://github.com/apache/cassandra-dtest/commit/7f2b8eda5c52fb6f637aa7166e2d48cd34a64eec (CASSANDRA-17679)
 - https://github.com/apache/cassandra-dtest/commit/e0d3cc90558a17b63634d15ee6df339ceb87b225 (CASSANDRA-15402)",N/A,"3.0.29, 3.11.15"
CASSANDRA-18153,"Memtable being flushed without hostId in version ""me"" and newer during CommitLogReplay","On ticket CASSANDRA-16619 some files were changed to allow Cassandra to store HostID in the new ""me"" SSTable version.

But SSTables flushed during CommitLogReplay miss this HostID info.

 

In the next Cassandra startup, if these SSTables were still present, system.log will show:


{{WARN Origin of 3 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored}}

{{WARN }}{{{}Origin of 3 sstables is unknown or doesn't match the local node; commitLogIntervals for them were ignored{}}}{{{}{}}}{{ }}

 

And debug.log will show a list of SSTables, witch can include ""md"" and ""me"" version (before upgradesstables):

 

{{Ignored commitLogIntervals from the following sstables: [/var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/me-3-big-Data.db, /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-1-big-Data.db, /var/lib/cassandra/data/system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/md-2-big-Data.db]}}

 

https://issues.apache.org/jira/browse/CASSANDRA-16619",N/A,"3.0.29, 3.11.15, 4.0.9, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18150,Prefer snakeyaml's SafeConstructor over Constructor,"CVE-2022-1471 allows RCE through the Constructor class.  While this isn't a concern since yaml is only used for configuration, it is simple enough to switch to SafeConstructor and harden the server a little more.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18149,"snakeyaml vulnerabilities: CVE-2021-4235, CVE-2022-1471, CVE-2022-3064","The OWASP scan is reporting these for both snakeyaml-1.11 and snakeyaml-1.26.

These are similar to CASSANDRA-17907 in that they require access to the yaml to have any effect.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18148,netty-all vulnerability: CVE-2022-41881,This is showing in the OWASP scan.,N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18147,netty-all vulnerability: CVE-2022-41915,This is being reported by the OWASP scan.,N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18146,commons-cli vulnerability: CVE-2021-37533,"This CVE is being reported by the OWASP scan for:

commons-cli-1.1.jar: CVE-2021-37533
commons-codec-1.9.jar: CVE-2021-37533
commons-math3-3.2.jar: CVE-2021-37533

additionally commons-lang3-3.1.jar is also reported on 3.x.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18143,upgradesstables does not always upgrade tables in proper order.,"The SSTableUpgrader accepts tools in the hash order provided by Directories.SSTableLister rather than ordering them to ensure that they are upgraded in the proper order.

They should be ordered by their id. The comparator for SSTableId is available in SSTableIdFactory.COMPARATOR. 
 
Dev discussion thread: https://lists.apache.org/thread/w6pm5hbdxt295mtvlckv0joyk8x4o8nf",N/A,"3.0.29, 3.11.15, 4.0.9, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18130,Log hardware and container params during test runs to help troubleshoot intermittent failures,"{color:#000000}We’ve long had flakiness in our containerized ASF CI environment that we don’t see in circleci. The environment itself is both containerized and heterogenous, so there are differences in both the hardware environment and the software environment in which it executes. For reference, see: [https://github.com/apache/cassandra-builds/blob/trunk/ASF-jenkins-agents.md#current-agents]{color}
{color:#000000} {color}
{color:#000000}We should log a variety of hardware, container, and software environment details to help get to the bottom of where some test failures may be occurring. As we don’t have shell access to the machines it’ll be easier to have this information logged / retrieved during test runs than to try and profile each host independently.{color}",N/A,"2.2.20, 3.0.31, 3.11.18, 4.0.13, 4.1.5, 5.0-rc1, 5.0, 5.1"
CASSANDRA-18127,Python upgrade tests can be run successfully with Large instead of XLarge containers,After CASSANDRA-16328 and CASSANDRA-17912 it seems we can run the Python upgrade tests with Large instead of XLarge containers and save some CircleCI credits.,N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18126,Add to the IntelliJ Git Window issue navigation links to Cassandra's Jira,"It is possible to navigate from the IntelliJ IDEA Git window to a corresponding Cassandra issue, the Apache Jira if mentioned in the git message. The example in the attachments shows how _CASSANDRA-*_ letters are turned on in the commit message to an appropriate Cassandra Jira link.

We should update the IntelliJ IDEA configuration and make this behaviour a default for the {{ant generate-idea-files}} process.

To achieve this manually you can update your {{.idea/vcs.xml}} file in the Cassandra project with the following:
{code:java}
<?xml version=""1.0"" encoding=""UTF-8""?>
<project version=""4"">
<component name=""VcsDirectoryMappings"">
<mapping directory=""$PROJECT_DIR$"" vcs=""Git"" />
</component>
<component name=""IssueNavigationConfiguration"">
<option name=""links"">
<list>
<IssueNavigationLink>
<option name=""issueRegexp"" value=""CASSANDRA-(\d+)"" />
<option name=""linkRegexp""value=""https://issues.apache.org/jira/browse/CASSANDRA-$1""/>
</IssueNavigationLink>
</list>
</option>
</component>
</project>
{code}",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18118,Do not leak 2015 memtable synthetic Epoch,"This [Epoch|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/rows/EncodingStats.java#L48] can [leak|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/Memtable.java#L392] affecting all the timestamps logic.  It has been observed in a production env it can i.e. prevent proper sstable and tombstone cleanup.

To reproduce create the following table:
{noformat}
drop keyspace test;
create keyspace test WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};
CREATE TABLE test.test (
    key text PRIMARY KEY,
    id text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '2', 'tombstone_compaction_interval': '3000', 'tombstone_threshold': '0.1', 'unchecked_tombstone_compaction': 'true'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.0
    AND default_time_to_live = 10
    AND gc_grace_seconds = 10
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

CREATE INDEX id_idx ON test.test (id);
{noformat}

And stress load it with:
{noformat}
insert into test.test (key,id) values('$RANDOM_UUID $RANDOM_UUID', 'eaca36a1-45f1-469c-a3f6-3ba54220363f') USING TTL 10
{noformat}

Notice how all inserts have a 10s TTL, the default 10s TTL and gc_grace is also at 10s. This is to speed up the repro:
- Run the load for a couple minutes and track sstables disk usage. You will see it does only increase, nothing gets cleaned up and it doesn't stop growing (notice all this is well past the 10s gc_grace and TTL)
- Running a flush and a compaction while under load against the keyspace, table or index doesn't solve the issue.
- Stopping the load and running a compaction doesn't solve the issue. Flushing does though.
- On the original observation where TTL was around 600s and gc_grace around 1800s we could get GBs of sstables that weren't cleaned up or compacted away after hours of work.
- Reproduction can also happen on plain sstables by repeatedly inserting/deleting/overwriting the same values over and over again without 2i indices or TTL being involved.

The problem seems to be [EncodingStats|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/db/rows/EncodingStats.java#L48] using a synthetic Epoch in 2015 which plays nice with Vint serialization.  Unfortunately {{Memtable}} is using that to keep track of the {{minTimestamp}} which can leak the 2015 Epoch. This confuses any logic consuming that timestamp. In this particular case purge and fully expired sstables weren't properly detected.
",N/A,"3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18105,TRUNCATED data come back after a restart or upgrade,"When we use the TRUNCATE command to delete all data in the table, the deleted data come back after a node restart or upgrade. This problem happens at the latest releases (2.2.19, 3.0.28, or 4.0.7)
h1. Steps to reproduce
h2. To reproduce it at release (3.0.28 or 4.0.7)

Start up a single Cassandra node. Using the default configuration and execute the following cqlsh commands.
{code:java}
CREATE KEYSPACE IF NOT EXISTS ks WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
CREATE TABLE  ks.tb (c3 TEXT,c4 TEXT,c2 INT,c1 TEXT, PRIMARY KEY (c1, c2, c3 ));
INSERT INTO ks.tb (c3, c1, c2) VALUES ('val1','val2',1);
CREATE INDEX IF NOT EXISTS tb ON ks.tb ( c3);
TRUNCATE TABLE ks.tb;
DROP INDEX IF EXISTS ks.tb; {code}
Execute a read command
{code:java}
cqlsh> SELECT c2 FROM ks.tb; 

 c2
----

(0 rows) {code}
Then, we flush the node and kill the Cassandra daemon by
{code:java}
bin/nodetool flush
pgrep -f cassandra | xargs kill -9 {code}
We restart the node. When the node has started, perform the same read, and the deleted data comes back again.
{code:java}
cqlsh> SELECT c2 FROM ks.tb; 

 c2
----
  1

(1 rows) {code}
h2. To reproduce it at release (2.2.19)

We don't need to kill the Cassandra daemon. Use bin/nodetool stopdaemon is enough. The other steps are the same as reproducing it at 4.0.7 or 3.0.28.
{code:java}
bin/nodetool -h ::FFFF:127.0.0.1 flush 
bin/nodetool -h ::FFFF:127.0.0.1 stopdaemon{code}
 

I have put the full log to reproduce it for release 4.0.7 and 2.2.19 in the comments.",N/A,"3.0.29, 3.11.15, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18096,Do not spam the logs with MigrationCoordinator not able to pull schemas,"When a node is joining a cluster, there is this output upon startup:

{code}
cassandra_node_6  | INFO  [GossipStage:1] 2022-12-06 12:48:07,187 Gossiper.java:1413 - Node /172.19.0.5:7000 is now part of the cluster
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
cassandra_node_6  | WARN MigrationCoordinator.java:650 - Can't send schema pull request: node /172.19.0.5:7000 is down.
{code}

This is there for a lot of already existing nodes. You got the idea. This log is misleading, it indeed can not pull requests because ""node is down"" but it is not down, it just thinks it is because Gossiper has not had a chance to receive any gossip about these nodes _yet_.

I put there more logs and it writes this:

{code}
 MigrationCoordinator.java:655 - Can't send schema pull request: node /172.19.0.5:7000 is down: NORMAL, isAlive: false
{code}

When I do this:

{code}
        if (!gossiper.hasEndpointState(endpoint))
            return;

        if (!gossiper.isAlive(endpoint))
        {
            EndpointState endpointStateForEndpoint = gossiper.getEndpointStateForEndpoint(endpoint);
            String status = Gossiper.getGossipStatus(endpointStateForEndpoint);
            logger.warn(""Can't send schema pull request: node {} is down: {}, isAlive: {}"", endpoint, status, endpointStateForEndpoint.isAlive());
            callback.onFailure(endpoint, RequestFailureReason.UNKNOWN);
            return;
        }
{code}

So it is in NORMAL but it is not alive yet which is quite strange.

The fix is to still return prematurely but we would not skip the logging on WARN only in case isAlive is false and status is _not_NORMAL. We would however still log on TRACE at least.

(1) https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/schema/MigrationCoordinator.java#L648-L653",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18083,snakeyaml-1.26.jar: CVE-2022-41854,https://nvd.nist.gov/vuln/detail/CVE-2022-41854,N/A,"3.0.29, 3.11.15, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18032,When generate.sh fails its rc=0,"{code}
$ ./generate.sh -a
Generating new config.yml file with low resources and LOWRES/MIDRES/HIGHRES templates from config-2_1.yml
./generate.sh: line 171: circleci: command not found
patching file ./config-2_1.yml
Hunk #4 succeeded at 1511 (offset 9 lines).
Hunk #5 succeeded at 1525 (offset 9 lines).
Hunk #6 succeeded at 1540 (offset 9 lines).
Hunk #7 succeeded at 1554 (offset 9 lines).
Hunk #8 succeeded at 1569 (offset 9 lines).
Hunk #9 succeeded at 1583 (offset 9 lines).
Hunk #10 succeeded at 1598 (offset 9 lines).
Hunk #11 succeeded at 1616 (offset 9 lines).
Hunk #12 succeeded at 1631 (offset 9 lines).
Hunk #13 succeeded at 1649 (offset 9 lines).
Hunk #14 succeeded at 1664 (offset 9 lines).
Hunk #15 succeeded at 1682 (offset 9 lines).
Hunk #16 succeeded at 1697 (offset 9 lines).
./generate.sh: line 177: circleci: command not found
patching file ./config-2_1.yml
./generate.sh: line 183: circleci: command not found
{code}",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18028,pylib cqlsh tests fail on cython,"Currently pylib/cqlsh tests _with_ cython where not being ran in [jenkins|https://ci-cassandra.apache.org/job/Cassandra-4.1-cqlsh-tests/181/]. Notice how cython yes and no run the same.

[~mck] managed to find where the flag was being dropped https://github.com/apache/cassandra-builds/pull/83

But cython compile failures arise where an updated is [suggested|https://github.com/cython/cython/issues/1943]",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-18025,cassandra-stress: not all contact point are passed down to driver,"Seem like c-s is randomly selecting a node from the nodes passed down to it in the command line, and use that node as contact point to the driver.

 

When using c-s together with other management operations (for example expending/shrinking the cluster), we can get into situation some of the nodes mentioned in the command line aren't reachable/available, and c-s instead of applying the best practice of having multiple contact points, pass down only one that can be unavailable and fail completely without trying any of the other nodes mentioned in the command line

we just fixed that in our fork of cassandra-stress:

[https://github.com/scylladb/scylla-tools-java/pull/314]

 ",N/A,"3.0.30, 3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-18024,Circle repeated jobs shouldn't appear on default config files,"It seems that when pushing a PR, the auto multiplexing of new tests triggers all multiplexing jobs, even if there are no tests present for that job.

That is wasteful as it means spinning up many nodes etc for nothing.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-18013,Splitter sometimes creates different number of splits than requested,"{{Splitter}} in some cases may produce one split more than requested. When it happens, it  fails with assertion error when assertions are enabled.

Test to reproduce the issue:

{code:java}
        Splitter splitter = getSplitter(Murmur3Partitioner.instance);
        long lt = 0;
        long rt = 31;
        Range<Token> range = new Range<>(getWrappedToken(Murmur3Partitioner.instance, BigInteger.valueOf(lt)),
                                         getWrappedToken(Murmur3Partitioner.instance, BigInteger.valueOf(rt)));

        for (int i = 1; i <= (rt - lt); i++)
        {
            List<Token> splits = splitter.splitOwnedRanges(i, Arrays.asList(new Splitter.WeightedRange(1.0d, range)), false);
            logger.info(""{} splits of {} are: {}"", i, range, splits);
            Assertions.assertThat(splits).hasSize(i);
        }
{code}

",N/A,"3.11.15, 4.0.8, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-18012,Remove -l / -m / -h designation and have two options: free or paid tier circle config,"Currently the -h designation is wasteful and should not be used, and the -f designation won't actually successfully run to completion.

We should default to a ""free tier"" config (probably print a warning that it's generating config w/subset of full tests that should not be used to validate commits in big warning letters) that runs a subset of the test suites for users working on patches to validate their work (unit test only? unit + in-jvm dtest? TBD), and add a flag to generate a config using larger containers + parallelization for the ""paid"" tier for folks w/paid circleci accounts.,

It looks like -p is available right now fwiw.",N/A,"3.0.29, 3.11.15, 4.0.9, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18001,Add missing tests suites to CircleCI,"Large Python DTests (with/without vnodes), cqlshlib not tested in all branches and with all jdks; Java distributed tests not running with J8/J11 4.0+

A few bugs identified down the road have been fixed. More in the comments",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-18000,CircleCI: Skip checkstyle in the Ant-based repeated tests,"The CircleCI jobs for repeating Ant-based tests include the {{checkstyle}} and {{checkstyle-test}} targets. Those targets are skipped for Java 11 but not for Java 8.

Including those targets on every test iteration produces a significant difference in the run times of repeated test jobs for j8 and j11:
 * [https://app.circleci.com/pipelines/github/adelapena/cassandra/2328/workflows/122d58b9-b454-4a99-8c46-f7777b7ef225/jobs/23225]
 * [https://app.circleci.com/pipelines/github/adelapena/cassandra/2328/workflows/50f33d9b-c7c6-4aa9-bac9-22ac78ad6b8c/jobs/23224]

We should use the {{no-checkstyle}} flag in the iterations done by those jobs. That should significantly speed up the jobs and save resources.

 

Thanks to [~bereng] , who detected the difference in the running times between j8 and j11.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-17997,Improve git branch handling for CircleCI generate.sh,"The generate.sh script assumes a base git branch that is local and named after the official repo branch (e.g. `cassandra-3.11`). This may not be a local branch if the developer has recently cloned the repo and is creating a work branch, and will lead to the git commands in generate.sh failing:

 

```

fatal: ambiguous argument 'cassandra-3.11...HEAD': unknown revision or path not in the working tree.
Use '--' to separate paths from revisions, like this:
'git <command> [<revision>...] -- [<file>...]'

```

We should be able to make some sanity checks to better guide or warn the developer if things aren't set up properly to check against git.",N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-17995,CircleCI: generate.sh without options modifies config,"Calling {{.circleci/generate.sh}} without any options should print the help and exit without editing the {{config.yml}} file. However, after CASSANDRA-17939 it wrongly enters into the automatic detection of new or modified tests.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17987,CircleCI: Add jobs for running specialized unit tests with Java 11,"CircleCI has a set of jobs for running specialiazed unit tests that are only run with Java 8:
 * utests_compression
 * utests_system_keyspace_directory
 * utests_trie
 * utests_stress
 * utests_long
 * utests_fqltool

It should probably be possible to run these tests with Java 11 tool.

Rather than creating a ticket for every job, it's probably easier to use a single ticket for all of them. This should give us an overall vision for deciding job names, approval steps, etc. Also, the required config changes should be quite minimal and doing all of them at once should save us both effort and test runs.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-17974,running Ant rat targets without git fails,"This was detected while reviewing CASSANDRA-17753

When trying it build without git command, I am getting this:

{code}
_rat_init:

_build_ratinclude:
    [exec] Execute failed: java.io.IOException: Cannot run program ""git"" (in directory ""/home/smiklosovic/test/cassandra-CASSANDRA-17753""): error=2, No such file or directory
    
BUILD FAILED
/home/smiklosovic/test/cassandra-CASSANDRA-17753/.build/build-rat.xml:38: ""Warning: Could not find file /home/smiklosovic/test/cassandra-CASSANDRA-17753/build/.versioned_files to copy
{code}

This was ran on a system without Git and against code downloaded from Github in a zip file.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17970,Avoid anticompaction mixing data from two different time windows with TWCS,When grouping sstables for anticompaction we can currently get sstables from different time windows when running TWCS,N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-17966,jackson-databind vulnerability CVE-2022-42003 CVE-2022-42004,"As the summary says, jackson-databind 2.13.2.2 which was upgraded for a vulnerability in CASSANDRA-17556 is vulnerable again.",N/A,"3.11.14, 4.0.7, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-17965,cassandra-driver-core vulnerability CVE-2019-2684,"As the summary says, CVE-2019-2684 affects cassandra-driver-core including both versions we use, 3.0.1 and 3.11.0.",N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-17964,Some tests are never executed due to naming violation - fix it and add checkstyle where applicable,"[BatchTests|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/cql3/BatchTests.java] doesn't follow naming convention to be run as unit tests and, thus, is never run.
The rule in build expects names as `*Test`.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-17950,Enable dtest-offheap in CircleCI,,N/A,"3.11.15, 4.0.8, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17939,CircleCI: Automatically detect and repeat new or modified JUnit tests,The purpose of this ticket is adding a new CircleCI job that automatically detects new or modified test classes and runs them repeatedly. That way we wouldn't need to manually specify those tests with {{.circleci/generate.sh}}.,N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17932,create a multiplexer job for Jenkins,It would be nice to have more parity with Circle and have the ability to multiplex a test or group of tests in Jenkins.  This would be especially helpful when working on intermittent issues that only appear in Jenkins.,N/A,"3.0.x, 3.11.x, 4.0.x, 4.1.x, 5.x"
CASSANDRA-17921,Harden JMX by resolving beanshooter issues,"Fix JMX security vulnerabilities

As reported by Murray McAllister, there are multiple JMX vulnerabilities
in the default Cassandra configuration on 3.0, 3.11, 4.0 and trunk,
across Java 8 and Java 11. These are limited to authenticated JMX users
only.

Vulnerabilities:
1. (Java 8 and 11) Remote Java Library loading and execution via MLet
2. (Java 11 only) Remote Java file reads via DiagnosticCommandMBean's
   compilerDirectivesAdd implementation leaking arbitrary file contents
3. (Java 11 only) Remote .so library loading via JVMTI

qtc-de/beanshooter is a JMX enumeration tool that uses these mechanisms
and others:
https://github.com/qtc-de/beanshooter/blob/2ec4f7a4b44a29f52315973fe944eb34bc772063/beanshooter/src/de/qtc/beanshooter/mbean/diagnostic/Dispatcher.java#L48

Remote file reads via compilerDirectiveAdd does not appear to be
reproducible on Java 8 (cassandra-{3.0,3.11}, Java 1.8.0_345-b01 from
Adoptium / Temurin). Using qtc-de/beanshooter and cassandra-3.0
(a78db628):
{code}
$ java -jar target/beanshooter-3.0.0-jar-with-dependencies.jar diagnostic read --verbose 127.0.0.1 7199 /tmp/hello
[-] A method with signature compilerDirectivesAdd([Ljava.lang.String;) does not exist on the endpoint.
[-] If you invoked a deployed MBean, make sure that the correct version was deployed.
[-] Cannot continue from here.
{code}

Java 8 also appears to not be vulnerable to remote library loading:
{code}
$ java -jar target/beanshooter-3.0.0-jar-with-dependencies.jar diagnostic load --verbose 127.0.0.1 7199 /tmp/hello
[-] A method with signature jvmtiAgentLoad([Ljava.lang.String;) does not exist on the endpoint.
[-] If you invoked a deployed MBean, make sure that the correct version was deployed.
[-] Cannot continue from here.
{code}

But Java 8 does appear to be vulnerable to MLet:
{code}
$ java -jar target/beanshooter-3.0.0-jar-with-dependencies.jar tonka deploy --stager-url http://localhost:8000 127.0.0.1 7199
[+] Starting MBean deployment.
[+]
[+]     Deplyoing MBean: TonkaBean
[+]
[+]             MBean class is not known by the server.
[+]             Starting MBean deployment.
[+]
[+]                     Deplyoing MBean: MLet
[+]                     MBean with object name DefaultDomain:type=MLet was successfully deployed.
[+]
[+]             Loading MBean from http://localhost:8000
[+]
[+]                     Creating HTTP server on: localhost:8000
[+]                     Creating MLetHandler for endpoint: /
[+]                     Creating JarHandler for endpoint: /fb0f34fe7c4f456bb44c07d9650dbf1e
[+]                     Starting HTTP server.
[+]
[+]                     Incoming request from: localhost
[+]                     Requested resource: /
[+]                     Sending mlet:
[+]
[+]                             Class:     de.qtc.beanshooter.tonkabean.TonkaBean
[+]                             Archive:   fb0f34fe7c4f456bb44c07d9650dbf1e
[+]                             Object:    MLetTonkaBean:name=TonkaBean,id=1
[+]                             Codebase:  http://localhost:8000
[+]
[+]                     Incoming request from: localhost
[+]                     Requested resource: /fb0f34fe7c4f456bb44c07d9650dbf1e
[+]                     Sending jar file with md5sum: 39d35ebd20aee73fbb83928584a530d7
[+]
[+]     MBean with object name MLetTonkaBean:name=TonkaBean,id=1 was successfully deployed.
{code}

Java 11 appears to be vulnerable to all three vulnerabilities, using JDK
Adoptium / Temurin 11.0.16.1+1 and cassandra-4.0 (5beab63b).

This patch fixes the above issues by introducing a new system property:
`cassandra.jmx.security.profile`, which can be set to ""restrictive""
(default) or ""lax"". The restrictive profile blocks the mechanisms for
all three vulnerabilities, by introducing a JMX
MBeanServerAccessController. Users can use the lax profile if they
require these mechanisms, or use their own authorization proxy by
specifying `cassandra.jmx.authorizer`.",N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17919,Capital P gets confused in the parser for a Duration in places where IDENT are needed,"This was found while adding Accord Transaction syntax into CQL and fuzz testing to validate all possible cases… in doing this the following was found

{code}
String query = ""BEGIN TRANSACTION\n"" +
                           ""  LET P = (SELECT v FROM "" + keyspace + "".tbl WHERE k=? AND c=?);\n"" +
                           ""  LET row2 = (SELECT v FROM "" + keyspace + "".tbl WHERE k=? AND c=?);\n"" +
                           ""  SELECT v FROM "" + keyspace + "".tbl WHERE k=? AND c=?;\n"" +
                           ""  IF P IS NULL AND row2.v = ? THEN\n"" +
                           ""    INSERT INTO "" + keyspace + "".tbl (k, c, v) VALUES (?, ?, ?);\n"" +
                           ""  END IF\n"" +
                           ""COMMIT TRANSACTION"";
{code}

Fails with

{code}
SyntaxException: line 2:6 mismatched input 'P' expecting IDENT (BEGIN TRANSACTION  LET [P]...)
{code}

The new LET syntax found this, but was able to reproduce in other cases

{code}
cqlsh:ks> CREATE TABLE P (k INT PRIMARY KEY);
SyntaxException: line 1:13 no viable alternative at input 'P' (CREATE TABLE [P]...)
cqlsh:ks>
cqlsh:ks> CREATE TABLE p (k INT PRIMARY KEY);
cqlsh:ks>
{code}",N/A,"3.11.15, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-17912,Fix CircleCI config for running Python upgrade tests on 3.0 and 3.11,"It was noticed that Circle CI pushes to run irrelevant for 3.0 and 3.11 Python upgrade tests. Those are properly skipped in Jenkins.

Example runs:

[CircleCI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1931/workflows/39ef5261-a6bf-4e77-a412-e750e322a231/jobs/15293] - 3.0 run

[Jenkins|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1944/] - while typing this I actually noticed [one test|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1944/testReport/junit/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_3_11_X_HEAD/test_parallel_upgrade_with_internode_ssl/] only looking suspicious in Jenkins as it ends the upgrade in 3.11 when we test 3.0... 

This is good to be fixed for two reasons - reduce the noise so we do not miss legit failures and we should not spend resources to try to run those tests. ",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1-rc1"
CASSANDRA-17907,Remediate CVE-2022-25857 - org.yaml_snakeyaml version 1.26 has vulnerabilities,"|org.yaml_snakeyaml|[CVE-2022-25857|https://nvd.nist.gov/vuln/detail/CVE-2022-25857]|Fixed in: 1.31
21 days ago| |6|Impacted versions: <1.31
Discovered: a day ago
Published: 21 days ago
The package org.yaml:snakeyaml from 0 and before 1.31 are vulnerable to Denial of Service (DoS) due missing to nested depth limitation for collections.|",N/A,"3.11.14, 4.0.7, 4.1-beta1, 4.1"
CASSANDRA-17906,Test splits generated with --only-resource-intensive-tests only work on hosts with >= 27GB memory,"As the title says, [here|https://github.com/apache/cassandra-builds/blob/trunk/build-scripts/cassandra-dtest-pytest.sh#L87] we use the ""only resource intensive"" flag to filter to just those tests, but machines with less than 27G will fail [this one|https://github.com/apache/cassandra-dtest/blob/trunk/conftest.py#L113].",N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-beta1, 4.1"
CASSANDRA-17901,ccm logs not being collected to nightlies,"At least some of the time, Jenkins is failing to collect ccm logs and is uploading a zero byte tar archive to nightlies, for example: https://nightlies.apache.org/cassandra/trunk/Cassandra-trunk-dtest/1100/Cassandra-trunk-dtest/label=cassandra-dtest,split=5/

{quote}
09:25:06  + tar -cJf ccm_logs.tar.xz './tmp/*/test/*/logs/*'
09:25:06  tar: ./tmp/*/test/*/logs/*: Cannot stat: No such file or directory
09:25:06  tar: Exiting with failure status due to previous errors
{quote}",N/A,"3.0.28, 3.11.14, 4.1-beta1, 4.1"
CASSANDRA-17885,Add solution for CASSANDRA-17581 to older branches for tests,"Some of our tests use old branches for the purposes of testing upgrades and behavior in mixed versions, but those lacking CASSANDRA-17581 will fail on nodetool with a modern JDK.  We can port this fix to those branches and adjust the tests to use the newest version of them to solve this going forward.",N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-rc1"
CASSANDRA-17879,"It is not possible to autocomplete ""WITH"" when creating materialized view","I noticed that when I type this:

{code}
CREATE MATERIALIZED VIEW ks.mv2 AS SELECT * FROM t WHERE k IS NOT NULL AND c1 IS NOT NULL AND c2 IS NOT NULL PRIMARY KEY (c1,k,c2) <TAB>
{code}

nothing happens after pressing tab, there should be options shown as for table case.",N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17871,Update debian packages for bullseye,We are updating the buster docker image used to build debian packages to bullseye (which is probably a bit overdue) in CASSANDRA-17854 Package build dependencies for versions using python2 need to be updated for this to work.,N/A,"2.2.20, 3.0.29, 3.11.15, 4.0.8, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-17862,Scrubber falls into infinite loop when the last partition is broken in data file,"Scrubber cannot deal with the situation when the last partition in a data file is broken but the index is fine. When compression is enabled, scrubber falls into infinite loop.

This affects all 3.0, 3.11, 4.0, 4.1 and trunk.
",N/A,"3.0.28, 3.11.14, 4.0.7, 4.1-alpha1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17848,Fix incorrect resource name in LIST PERMISSION output,"When producing the resource name, it seems to assume that the content in the `[]` is the function's input type, where it could also be part of the function name, as long as it is quoted. Here is an example to reproduce. In cqlsh,

{code:java}
> CREATE FUNCTION test.""admin_created_udf[org.apache.cassandra.db.marshal.LongType]""(input int) RETURNS NULL ON NULL INPUT RETURNS int LANGUAGE java AS 'return 42;';

> LIST EXECUTE OF user;
 role  | username | resource                                | permission
-------+----------+-----------------------------------------+------------
 user  |    user  | <function test.admin_created_udf(long)> |    EXECUTE

(1 rows)
{code}

The input should be ""int"", but in the output, it says ""long"". 

If the content enclosed by ""[]"" is not a valid class, the LIST PERMISSION request always fails for the user with ""ConfigurationException: Unable to find abstract-type class"".

The bug is discovered by Piotr Sarna.",N/A,"3.0.29, 3.11.15, 4.0.8, 4.1.1, 5.0-alpha1, 5.0"
CASSANDRA-17840,IndexOutOfBoundsException in Paging State Version Inference (V3 State Received on V4 Connection),"In {{PagingState.java}}, {{index}} is an integer field, and we add long values to it without a {{Math.toIntExact}} check. While we’re checking for negative return values returned by {{getUnsignedVInt}}, there's a chance that the value returned by it is so large that addition operation would cause integer overflow, or the value itself is large enough to cause overflow.",N/A,"3.11.14, 4.0.6, 4.1-beta1, 5.0-alpha1, 5.0"
CASSANDRA-17818,Fix error message handling when trying to use CLUSTERING ORDER with non-clustering column,"Imagine ck1, ck2, v columns. For ""CLUSTERING ORDER ck1 ASC, v DESC"" error msg will suggest that information for ck2 is missing. But if you add it it will still be wrong as ""v"" cannot be used. So the problem here is really about using non-clustering column rather than about not providing information about some clustering column.

The following is example from 3.11, but the code is the same in 4.0, 4.1, trunk:
{code:java}
cqlsh:k_test> CREATE TABLE test2 (pk int, ck1 int, ck2 int, v int, PRIMARY KEY ((pk),ck1, ck2)) WITH CLUSTERING ORDER BY (v ASC);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Missing CLUSTERING ORDER for column ck1""

cqlsh:k_test> CREATE TABLE test2 (pk int, ck1 int, ck2 int, v int, PRIMARY KEY ((pk),ck1, ck2)) WITH CLUSTERING ORDER BY (ck1 ASC, v ASC);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Missing CLUSTERING ORDER for column ck2""

cqlsh:k_test> CREATE TABLE test2 (pk int, ck1 int, ck2 int, v int, PRIMARY KEY ((pk),ck1, ck2)) WITH CLUSTERING ORDER BY (ck1 ASC, ck2 DESC, v ASC);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Only clustering key columns can be defined in CLUSTERING ORDER directive""{code}
We need to be sure that we return to the user the same correct error message in all three cases and it should be ""Only clustering key columns can be defined in CLUSTERING ORDER directive""

+Additional information for newcomers+
 * [This|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/statements/schema/CreateTableStatement.java#L251-L252] is where we handle the issue incorrectly as proved by the example. The easiest way to handle this issue would be to  check the key set content of {_}clusteringOrder{_}.
 * It would be good also to add more unit tests in [CreateTableValidationTest|https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/schema/CreateTableValidationTest.java] to cover different cases. 
 * I suggest we create patch first for 3.11 and then we can propagate it up to the next versions.",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-17806,Flaky test_rolling_upgrade,"The fix on CASSANDRA-17140 needs to be extended into other places as it seems it now fails only one in a billion but still we can fix that one.

{noformat}
Regression

dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_3_11_X_HEAD.test_rolling_upgrade (from Cassandra dtests)
Failing for the past 1 build (Since
#115 )
Took 10 min.
Failed 1 times in the last 9 runs. Flakiness: 12%, Stability: 88%
Error Message

RuntimeError: A subprocess has terminated early. Subprocess statuses: Process-1 (is_alive: True), Process-2 (is_alive: False), attempting to terminate remaining subprocesses now.

Stacktrace

self = <upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_3_11_X_HEAD object at 0x7f4d313e4e50>

    @pytest.mark.timeout(3000)
    def test_rolling_upgrade(self):
        """"""
            Test rolling upgrade of the cluster, so we have mixed versions part way through.
            """"""
>       self.upgrade_scenario(rolling=True)

upgrade_tests/upgrade_through_versions_test.py:340: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
upgrade_tests/upgrade_through_versions_test.py:417: in upgrade_scenario
    self._check_on_subprocs(self.fixture_dtest_setup.subprocs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <upgrade_tests.upgrade_through_versions_test.TestProtoV3Upgrade_AllVersions_RandomPartitioner_EndsAt_3_11_X_HEAD object at 0x7f4d313e4e50>
subprocs = [<Process name='Process-1' pid=10867 parent=389 stopped exitcode=-SIGKILL daemon>, <Process name='Process-2' pid=10881 parent=389 stopped exitcode=1 daemon>]

    def _check_on_subprocs(self, subprocs):
        """"""
            Check on given subprocesses.
    
            If any are not alive, we'll go ahead and terminate any remaining alive subprocesses since this test is going to fail.
            """"""
        subproc_statuses = [s.is_alive() for s in subprocs]
        if not all(subproc_statuses):
            message = ""A subprocess has terminated early. Subprocess statuses: ""
            for s in subprocs:
                message += ""{name} (is_alive: {aliveness}), "".format(name=s.name, aliveness=s.is_alive())
            message += ""attempting to terminate remaining subprocesses now.""
            self._terminate_subprocs()
>           raise RuntimeError(message)
E           RuntimeError: A subprocess has terminated early. Subprocess statuses: Process-1 (is_alive: True), Process-2 (is_alive: False), attempting to terminate remaining subprocesses now.

upgrade_tests/upgrade_through_versions_test.py:475: RuntimeError
{noformat}
",N/A,"3.0.28, 3.11.14, 4.0.6, 4.1-beta1, 5.0-alpha1, 5.0"
CASSANDRA-17760,upgrade_tests/upgrade_udtfix_test.py failing in Circle CI for Cassandra 3.0 and 3.11,"The tests are skipped in Jenkins and failing in CircleCI:

[3.0|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1783/workflows/03577cf2-2be9-4b14-8339-d9ae4f4b4a22/jobs/13034]

[3.11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1779/workflows/ec3fe1cf-649c-46d1-aaff-eb3e71cec57e/jobs/13030]",N/A,3.11.x
CASSANDRA-17759,Altering / creating of a keyspace on insufficient number of replicas should filter out gosspping only members,"When there is a CQL CREATE / ALTER KEYSPACE query executed on a gossipping-only member of a cluster (-Dcassandra.join_ring=false) where the replication factor is bigger than the number of the nodes, there is currently a warning emitted, which is ok, but this number also includes the gossipping node itself. This is incorrect as such a node is not part of a ring hence it does not hold any data.

This is not happening on ""data"" nodes (members of the ring) as from data nodes perspective, gossipping-only members are not visible.

We should filter gossipping-only members out of the computation.

EDIT:

For the sake of the completeness, I leave here the original description of this ticket:

The original issue for which we refused to do any action:

Imagine there is a 5-node cluster where two nodes are gossipping-only members (-Dcassandra.join_ring=false) - or in other words, 3 data nodes and 2 ""coordinator"" nodes.

Coordinator nodes are capable to speak CQL as well so requests can be executed against them. If we create a keyspace against such node, like ""create keyspace ks1 with replication =

{class = ""NTS"", ""dc1"": 5}

, this query succeeds but if we set CONSISTENCY to ALL in cqlsh and we try to insert some data into a table of such keyspace, it will fail - because it does not have enough replicas. It has only 3.

If this query is executed on data node (a proper member of a ring), this should fail too. I think there is a mechanism how to do this, like by Guardrails but there is no check which would include gossipping-only members into consideration.

Ideally, we might introduce a check which would check that the replication factor is at most as big as the number of members - irrelevant of their current status, they just have to be members of the ring.",N/A,"3.11.14, 4.0.6, 4.1-beta1, 5.0-alpha1, 5.0"
CASSANDRA-17752,fix restarting of services on gossipping-only members,"When a node is started with -Dcassandra.join_ring=false, it is possible to talk to it via CQL. If we disable it via ""nodetool disablebinary"", it is not possible to enable it back with ""nodetool enablebinary"".

The reason why is that enablebinary command eventually calls ""StorageService#checkServiceAllowedToStart"" and it throws when:

1) the node is draining (which does not happen)
2) it is shutting down (which does not happen)
3) is not in NORMAL state - which happens.

The state of the gossipping-only member is STARTING for ever.

The check should make an exception here, like if it is in STARTING but -Dcassandra.join_ring=false, start it anyway.",N/A,"3.0.28, 3.11.14, 4.0.6, 4.1-beta1, 5.0-alpha1, 5.0"
CASSANDRA-17748,Move deb/rpm repositories from dist/downloads .a.o to apache.jfrog.io,"Move our official debian and redhat repositories from downloads.apache.org to Apache's JFrog Artifactory server at apache.jfrog.io 

That is, the following URLs would be moved from
```
https://downloads.apache.org/cassandra/debian/
https://downloads.apache.org/cassandra/redhat/
```
to
```
https://apache.jfrog.io/artifactory/cassandra-deb/
https://apache.jfrog.io/artifactory/cassandra-rpm/
```

The rationale to do this is to avoid the strict opinionated checksum and signature requirements on downloads.a.o (dist.a.o), as the debian and redhat repositories have their own system for integrity and signing (which we already do).

Furthermore, as these repositories and their binaries are ""convenience binaries"" and not the official Cassandra source binaries, they do not need to be on downloads.a.o and can be served from apache.jfrog.io. This is similar to maven binaries (and docker images). Apache Arrow is already taking this approach: https://arrow.apache.org/install/ 

An advantage to using apache.frog.io is that these repositories maintain all past patch versions on each repo series (major/minor). This has been requested by users a number of times, for the sake of rolling back to a previous patch version. downloads.a.o can only contain the latest version.

This will BREAK everyone's existing `/etc/apt/sources.list.d/cassandra.sources.list` and `/etc/yum.repos.d/cassandra.repo` files. Folk will need to update these files to point to the new repo URLs. This would require an announcement to both users@ and dev@. I do not know how we can avoid this breakage. We could put in a simple README.md in the original URL locations explaining the breakage and how to fix.
",N/A,"3.11.14, 4.0.6, 4.1-beta1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17742,DOC - Fix typo in CQL Data Definition page,"Fix typo in the first paragraph of the CQL [Data Definition|https://cassandra.apache.org/doc/latest/cassandra/cql/ddl.html] page:

bq. ... It is common for a cluster to define only one keyspace for an {color:#DE350B}actie{color} application.

I created this Pull Request to highlight the error

https://github.com/apache/cassandra/pull/1722",N/A,"3.11.x, 4.0.x, 4.1.x"
CASSANDRA-17727,"Remove read_repair table option from <4.0 docs, and restore read_repair_chance docs","CASSANDRA-16763 added the read_repair docs to all versions. It should only have been added to 4.0 onwards.

The 3.11 docs is now missing the read_repair_chance information.

See doc/modules/cassandra/pages/cql/ddl.adoc and doc/modules/cassandra/pages/operating/read_repair.adoc ",N/A,3.11.14
CASSANDRA-17712,Remove javadocs step from CI and release process,"Currently the javadocs step is both:
- Taking up jenkins cycles and generating a [large output|https://ci-cassandra.apache.org/job/Cassandra-trunk-artifacts/1313/jdk=jdk_1.8_latest,label=cassandra/consoleFull] in CI when building artifacts
- Failing randomly on javadoc issues such as {{AlterTableStatement.java:135: error: text not allowed in element}}

Apidocs are not being bundled, uploaded or used anywhere. Hence it would be best to remove javadocs generation on every CI. Mainly removing javadoc from artifacts and mvn-install.",N/A,"3.0.28, 3.11.14, 4.0.6, 4.1-beta1, 4.1"
CASSANDRA-17709,CQL.textile needs to be made current,"Per CASSANDRA-17570 https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile is outdated, and the document we ship inside packages, so this should be made current.",N/A,"3.11.14, 4.0.7, 4.1-beta1, 4.1"
CASSANDRA-17701,Failing test: TestRepair.test_failure_during_validation ,"https://ci-cassandra.apache.org/job/Cassandra-3.11/371/testReport/dtest.repair_tests.repair_test/TestRepair/test_failure_during_validation/

{noformat}
Unexpected error found in node logs (see stdout for full details). Errors: [[node1] 'ERROR [Repair#1:1] 2022-06-19 16:45:53,295 CassandraDaemon.java:237 - Exception in thread Thread[Repair#1:1,5,RMI Runtime]\njava.util.concurrent.CancellationException: Task was cancelled.\n\tat com.google.common.util.concurrent.AbstractFuture.cancellationExceptionWithCause(AbstractFuture.java:392)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:137)\n\tat com.google.common.util.concurrent.Futures.getUnchecked(Futures.java:1509)\n\tat org.apache.cassandra.repair.RepairJob.run(RepairJob.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.CancellationException: Future.cancel() was called.\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.complete(AbstractFuture.java:378)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.cancel(AbstractFuture.java:355)\n\tat com.google.common.util.concurrent.AbstractFuture.cancel(AbstractFuture.java:131)\n\tat com.google.common.util.concurrent.Futures$CombinedFuture.setOneValue(Futures.java:1752)\n\tat com.google.common.util.concurrent.Futures$CombinedFuture.access$400(Futures.java:1608)\n\tat com.google.common.util.concurrent.Futures$CombinedFuture$2.run(Futures.java:1686)\n\tat com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)\n\tat com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)\n\tat com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)\n\tat com.google.common.util.concurrent.AbstractFuture.cancel(AbstractFuture.java:134)\n\tat org.apache.cassandra.repair.RepairSession.forceShutdown(RepairSession.java:320)\n\tat org.apache.cassandra.repair.RepairSession.convict(RepairSession.java:359)\n\tat org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:297)\n\tat org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:891)\n\tat org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:239)\n\tat org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n\t... 4 common frames omitted']
{noformat}",N/A,"3.0.29, 3.11.15"
CASSANDRA-17697,netty-all 4.0.44 is affected by CVE-2020-7238,"{noformat}
Dependency-Check Failure:
One or more dependencies were identified with vulnerabilities that have a CVSS score greater than or equal to '1.0': 
netty-all-4.0.44.Final.jar: CVE-2020-7238
{noformat}

Similar to CASSANDRA-17633, the HTTP request smuggling vulnerabilities continue.",N/A,"3.0.28, 3.11.14"
CASSANDRA-17664,Retry failed stage jobs in jenkins,"To avoid failing pipeline builds on CI infrastructure faults (disks, network, etc), retry stage jobs three times before marking them as FAILURE.

Intention is not to retry on UNSTABLE (tests failing).

This has already been done (and tested) for devbranch pipeline (pre-commit) [here|https://github.com/apache/cassandra-builds/pull/72].",N/A,"2.2.20, 3.0.28, 3.11.14, 4.0.5, 4.1-beta1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17660,jvm-dtest upgrade failures due to 3.x Ping not allowing serialize,"trunk jvm upgrade tests periodically fail due to the ping message not being able to be serialized on 3.x branches.  We need support for jvm-dtest even if we ignore the messages.

{code}
Suppressed: java.lang.UnsupportedOperationException
		at org.apache.cassandra.net.PingMessage$PingMessageSerializer.serialize(PingMessage.java:45)
		at org.apache.cassandra.net.PingMessage$PingMessageSerializer.serialize(PingMessage.java:41)
		at org.apache.cassandra.distributed.impl.Instance.serializeMessage(Instance.java:362)
		at org.apache.cassandra.distributed.impl.Instance$1.allowIncomingMessage(Instance.java:302)
		at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:866)
		at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:224)
		at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:193)
		at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:96)
{code}",N/A,"3.0.28, 3.11.14"
CASSANDRA-17645,python dtest upgrade manifest needs versions made current,"As the title says, it's currently testing 4.0.3, 3.11.12, and 3.0.26.",N/A,"3.0.28, 3.11.14, 4.0.5, 4.1-beta1, 4.1"
CASSANDRA-17633,netty vulnerable to CVE-2022-24823,"{noformat}
Dependency-Check Failure:
One or more dependencies were identified with vulnerabilities that have a CVSS score greater than or equal to '1.0': 
netty-all-4.0.44.Final.jar: CVE-2022-24823
See the dependency-check report for more details.
{noformat}

We already have suppressions for 4.0.44 and I suspect this will be another but should be investigated.",N/A,"3.0.28, 3.11.14, 4.0.5, 4.1-alpha1, 4.1-beta1, 4.1"
CASSANDRA-17628,CQL writetime and ttl functions should be forbidden for multicell columns,"CQL {{writetime}} and {{ttl}} functions are currently forbidden for collections, frozen or not. Also, they are always allowed for UDTs, frozen or not:
{code}
CREATE TYPE udt (a int, b int);
CREATE TABLE t (k int PRIMARY KEY, s set<int>, fs frozen<set<int>>, t udt, ft frozen<udt>);

SELECT writetime(s) FROM t; -- fails
SELECT writetime(st) FROM t; -- fails
SELECT writetime(t) FROM t; -- allowed
SELECT writetime(ft) FROM t; -- allowed
{code}
This is done by checking in [{{Selectable.WritetimeOrTTL#newSelectorFactory}}|https://github.com/apache/cassandra/blob/cassandra-4.0.4/src/java/org/apache/cassandra/cql3/selection/Selectable.java#L250] whether the column is a collection or not. However, I think that what we should check is whether the column is multi-cell. That way the function would work with frozen collections and UDTs, and it would reject unfrozen collections and UDTs:
{code}
SELECT writetime(s) FROM t; -- fails
SELECT writetime(st) FROM t; -- allowed
SELECT writetime(t) FROM t; -- fails
SELECT writetime(ft) FROM t; -- allowed
{code} ",N/A,"3.0.28, 3.11.14, 4.0.5, 4.1-alpha1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17623,"Frozen maps may be serialized unsorted, causing inability to query later","CASSANDRA-7859 introduced the ability to use frozen collections as parts of primary keys. This +requires+ all frozen maps to be persisted with their entries sorted by the map keys. If the map is +not+ sorted correctly, it becomes impossible to project all of the map values out of the map using the map projection/selection syntax. For example, the select below would fail if the map was not sorted correctly and the higher-valued key was persisted first:
{code:sql}
CREATE TABLE test.test (k text, c frozen<map<text, text>>, PRIMARY KEY (k, c));
INSERT INTO test.test (k, c) VALUES ('key', {'z':'second_value', 'a':'first_value'});
SELECT k, c['a'] from test.test where k='key' -- c['a'] would return NULL here
{code}

Additionally, if you attempted to select just that row by using the complete map value in a WHERE clause, which is also supported, it would return no rows unless the map provided by the query processor just happened to be sorted the same way as the persisted value.

However, there is a bug in Maps.java where we don't actually use a SortedMap in {{Maps.Value#fromSerialized}}, which manifests if a client sends an unsorted map as a bound parameter to a query on insert or select. In either case, the map may not be sorted correctly, leading to either invalid data being persisted to disk (in the INSERT case) or the query not being able to be executed/returning 0 rows even though a row _should_ exist (SELECT).

This bug affects any usage of parameterized queries (tested with the DataStax driver, and was originally discovered when using the CQLSSTableWriter code to write data locally).",N/A,"3.0.28, 3.11.14, 4.0.5, 4.1-alpha1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17617,CQLSH unicode control character list is too liberal,"It appears that the list of escaped unicode control characters [here|https://github.com/apache/cassandra/blob/53a67ff2c36d90d337aba1409498de29931d4279/pylib/cqlshlib/formatting.py#L32] is a bit too liberal. It seems to include characters such as '1' (0x31) and '0' (0x30) which do not need to be escaped. It seems that the actual range should be 0x00 - 0x1F and 0x7F+ as corroborated [by this page|https://en.wikipedia.org/wiki/Unicode_control_characters].

 

This causes unnecessary escaping and regex substitutions on the CQLSH end whenever common characters such as any punctuation or a 0 or a 1 appear in the text column of a table. One might notice that a table with a text column filled with 2's will take much less time to print than one with all 0's for this reason.",N/A,"3.11.14, 4.0.5, 4.1-rc1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17611,Fix replace_address_test.TestReplaceAddress.test_fail_when_seed,"Seen [with|https://ci-cassandra.apache.org/job/Cassandra-3.11/349/testReport/dtest.replace_address_test/TestReplaceAddress/test_fail_when_seed/] and [without vnode|https://ci-cassandra.apache.org/job/Cassandra-3.11/348/testReport/dtest-novnode.replace_address_test/TestReplaceAddress/test_fail_when_seed/]:
{code:java}
Error Message
test teardown failure

Stacktrace
Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [MessagingService-Incoming-/127.0.0.1] 2022-04-28 22:04:36,765 CassandraDaemon.java:244 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main] java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:58) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:148) at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:1035) at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:215) at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:195) at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:98), ERROR [MessagingService-Incoming-/127.0.0.1] 2022-04-28 22:04:36,765 CassandraDaemon.java:244 - Exception in thread Thread[MessagingService-Incoming-/127.0.0.1,5,main] java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:58) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:148) at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:1035) at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:215) at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:195) at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:98)]
{code}
 ",N/A,"3.0.27, 3.11.13"
CASSANDRA-17583,Fix flaky test - org.apache.cassandra.distributed.test.MessageForwardingTest.mutationsForwardedToAllReplicasTest,"h3. Error Message

/127.0.0.3 appending to commitlog traces expected:<100> but was:<98>
h3. Stacktrace
{noformat}
junit.framework.AssertionFailedError: /127.0.0.3 appending to commitlog traces expected:<100> but was:<98> at org.apache.cassandra.distributed.test.MessageForwardingTest.lambda$mutationsForwardedToAllReplicasTest$8(MessageForwardingTest.java:92) at java.base/java.util.HashMap.forEach(HashMap.java:1336) at org.apache.cassandra.distributed.test.MessageForwardingTest.mutationsForwardedToAllReplicasTest(MessageForwardingTest.java:91) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{noformat}",N/A,"3.0.28, 3.11.14, 4.0.5, 4.1-beta1, 5.0-alpha1, 5.0"
CASSANDRA-17581,"nodetool with Java 8u331 returns ""URISyntaxException: 'Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199'"""," Error when {{{color:#0747a6}new NodeProbe(""127.0.0.1"", 7199){color}}} with {color:#de350b}JDK 1.8.0_332{color}:
{code:java}
java.io.IOException: Failed to retrieve RMIServer stub:
 javax.naming.InvalidNameException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199
 Root exception is java.lang.IllegalArgumentException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199 {code}

Here is the error stack trace:
{noformat}
2022-04-24 07:22:40 [grizzly-http-server-2] [INFO] c.b.h.b.s.c.CassandraMetrics - Probe to cassandra node: '127.0.0.1:7199'
2022-04-24 07:22:40 [grizzly-http-server-2] [WARN] c.b.h.b.s.c.CassandraMetrics - Unable to get metrics from host '127.0.0.1':
java.io.IOException: Failed to retrieve RMIServer stub: javax.naming.InvalidNameException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199 [Root exception is java.lang.IllegalArgumentException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199]
    at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:369) ~[?:1.8.0_332]
    at javax.management.remote.JMXConnectorFactory.connect(JMXConnectorFactory.java:270) ~[?:1.8.0_332]
    at org.apache.cassandra.tools.NodeProbe.connect(NodeProbe.java:191) ~[cassandra-all-3.10.jar:3.10]
    at org.apache.cassandra.tools.NodeProbe.<init>(NodeProbe.java:158) ~[cassandra-all-3.10.jar:3.10]
    at com.baidu.hugegraph.backend.store.cassandra.CassandraMetrics.newNodeProbe(CassandraMetrics.java:308) ~[hugegraph-cassandra-0.13.0.jar:?]
    at com.baidu.hugegraph.backend.store.cassandra.CassandraMetrics.getMetricsByHost(CassandraMetrics.java:100) ~[hugegraph-cassandra-0.13.0.jar:?]
    at com.baidu.hugegraph.backend.store.cassandra.CassandraMetrics.executeAllHosts(CassandraMetrics.java:299) ~[hugegraph-cassandra-0.13.0.jar:?]
    at com.baidu.hugegraph.backend.store.cassandra.CassandraMetrics.metrics(CassandraMetrics.java:86) ~[hugegraph-cassandra-0.13.0.jar:?]
    at com.baidu.hugegraph.backend.store.cassandra.CassandraStore.lambda$registerMetaHandlers$0(CassandraStore.java:99) ~[hugegraph-cassandra-0.13.0.jar:?]
    at com.baidu.hugegraph.backend.store.MetaDispatcher.dispatchMetaHandler(MetaDispatcher.java:45) ~[hugegraph-core-0.13.0.jar:0.13.0.0]
    at com.baidu.hugegraph.backend.store.AbstractBackendStore.metadata(AbstractBackendStore.java:53) ~[hugegraph-core-0.13.0.jar:0.13.0.0]
    at com.baidu.hugegraph.backend.tx.AbstractTransaction.metadata(AbstractTransaction.java:109) ~[hugegraph-core-0.13.0.jar:0.13.0.0]
    at com.baidu.hugegraph.StandardHugeGraph.metadata(StandardHugeGraph.java:975) ~[hugegraph-core-0.13.0.jar:0.13.0.0]
    at com.baidu.hugegraph.auth.HugeGraphAuthProxy.metadata(HugeGraphAuthProxy.java:669) ~[hugegraph-api-0.13.0.jar:0.67.0.0]
    at com.baidu.hugegraph.api.metrics.MetricsAPI.backend(MetricsAPI.java:87) ~[hugegraph-api-0.13.0.jar:0.67.0.0]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_332]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_332]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_332]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_332]
    at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$TypeOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:205) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) ~[jersey-common-2.25.1.jar:?]
    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) ~[jersey-common-2.25.1.jar:?]
    at org.glassfish.jersey.internal.Errors.process(Errors.java:315) ~[jersey-common-2.25.1.jar:?]
    at org.glassfish.jersey.internal.Errors.process(Errors.java:297) ~[jersey-common-2.25.1.jar:?]
    at org.glassfish.jersey.internal.Errors.process(Errors.java:267) ~[jersey-common-2.25.1.jar:?]
    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317) ~[jersey-common-2.25.1.jar:?]
    at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154) ~[jersey-server-2.25.1.jar:?]
    at org.glassfish.jersey.grizzly2.httpserver.GrizzlyHttpContainer.service(GrizzlyHttpContainer.java:384) ~[jersey-container-grizzly2-http-2.25.1.jar:?]
    at org.glassfish.grizzly.http.server.HttpHandler$1.run(HttpHandler.java:200) ~[grizzly-http-server-2.4.4.jar:2.4.4]
    at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.doWork(AbstractThreadPool.java:569) ~[grizzly-framework-2.4.4.jar:2.4.4]
    at org.glassfish.grizzly.threadpool.AbstractThreadPool$Worker.run(AbstractThreadPool.java:549) ~[grizzly-framework-2.4.4.jar:2.4.4]
    at java.lang.Thread.run(Thread.java:750) [?:1.8.0_332]
Caused by: javax.naming.InvalidNameException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.newNamingException(rmiURLContext.java:295) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.parseCompat(rmiURLContext.java:223) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.parse(rmiURLContext.java:109) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext.getRootURLContext(rmiURLContext.java:314) ~[?:1.8.0_332]
    at com.sun.jndi.toolkit.url.GenericURLContext.lookup(GenericURLContext.java:215) ~[?:1.8.0_332]
    at javax.naming.InitialContext.lookup(InitialContext.java:417) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.findRMIServerJNDI(RMIConnector.java:1955) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.findRMIServer(RMIConnector.java:1922) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:287) ~[?:1.8.0_332]
    ... 40 more
Caused by: java.lang.IllegalArgumentException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199
    at java.net.URI.create(URI.java:852) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.parseCompat(rmiURLContext.java:213) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.parse(rmiURLContext.java:109) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext.getRootURLContext(rmiURLContext.java:314) ~[?:1.8.0_332]
    at com.sun.jndi.toolkit.url.GenericURLContext.lookup(GenericURLContext.java:215) ~[?:1.8.0_332]
    at javax.naming.InitialContext.lookup(InitialContext.java:417) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.findRMIServerJNDI(RMIConnector.java:1955) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.findRMIServer(RMIConnector.java:1922) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:287) ~[?:1.8.0_332]
    ... 40 more
Caused by: java.net.URISyntaxException: Malformed IPv6 address at index 7: rmi://[127.0.0.1]:7199
    at java.net.URI$Parser.fail(URI.java:2873) ~[?:1.8.0_332]
    at java.net.URI$Parser.parseIPv6Reference(URI.java:3494) ~[?:1.8.0_332]
    at java.net.URI$Parser.parseServer(URI.java:3244) ~[?:1.8.0_332]
    at java.net.URI$Parser.parseAuthority(URI.java:3180) ~[?:1.8.0_332]
    at java.net.URI$Parser.parseHierarchical(URI.java:3122) ~[?:1.8.0_332]
    at java.net.URI$Parser.parse(URI.java:3078) ~[?:1.8.0_332]
    at java.net.URI.<init>(URI.java:588) ~[?:1.8.0_332]
    at java.net.URI.create(URI.java:850) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.parseCompat(rmiURLContext.java:213) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext$Parser.parse(rmiURLContext.java:109) ~[?:1.8.0_332]
    at com.sun.jndi.url.rmi.rmiURLContext.getRootURLContext(rmiURLContext.java:314) ~[?:1.8.0_332]
    at com.sun.jndi.toolkit.url.GenericURLContext.lookup(GenericURLContext.java:215) ~[?:1.8.0_332]
    at javax.naming.InitialContext.lookup(InitialContext.java:417) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.findRMIServerJNDI(RMIConnector.java:1955) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.findRMIServer(RMIConnector.java:1922) ~[?:1.8.0_332]
    at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:287) ~[?:1.8.0_332]
    ... 40 more {noformat}",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17575,forceCompactionForTokenRange when using a wrapped range may include sstables not within that range,"This was found in CASSANDRA-17537

When you compact the range (32, 31] this should include everything BUT 32, but in the test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest#testTokenRangeCompaction it found that SSTables with the bounds (32, 32) were getting included in the set of SSTables to compact",N/A,"3.11.14, 4.0.6, 4.1-beta1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17556,jackson-databind 2.13.2 is vulnerable to CVE-2020-36518,Seems like it's technically possible to cause a DoS with nested json.,N/A,"3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17534,CQLSH help topic links are broken between textile and online files,"CQLSH help topics opens a generic CQL page, not the specific topic.  This happens when docs are not installed locally, and it tries to open the online version.

The file cqlsh.py has:

{{    # default location of local CQL.html}}
{{    if os.path.exists(CASSANDRA_PATH + '/doc/cql3/CQL.html'):}}
{{     ....}}
{{    else:}}
{{    # fallback to online version}}
{{    CASSANDRA_CQL_HTML = CASSANDRA_CQL_HTML_FALLBACK}}

For example,

{{cqlsh> help truncate;}}

opens the page:

[https://cassandra.apache.org/doc/latest/cassandra/cql/index.html#truncateStmt]

but there is no information about 'truncate' on that page and the anchor #truncateStmt doesn't exist.

the correct page anchor is:

[https://cassandra.apache.org/doc/latest/cassandra/cql/ddl.html#truncate-statement]

The local file anchors appear they may be incorrect as well:

[https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile#truncate]

To reproduce this, 'pip install cqlsh' on a machine without a Cassandra package install.",N/A,"3.0.30, 3.11.16, 4.0.12, 4.1.4, 5.0-alpha1, 5.0"
CASSANDRA-17524,Schema mutations may not be completed on drain,"The drain logic (invoked explicitly with nodetool or from the JVM
shutdown hook) closes down executor stages that can create mutations (counter,
view, mutation) before closing down the commitlog. The gossip
stage also commits schema mutations, and should be treated the same way.

The messaging service is shut down as part of drain, so there should be
no new Gossip messages received, however any messages still queued
in the executor could still run after the commitlog allocator is shut down as
part of drain, causing the gossip stage thread to hang indefinitely waiting
for a new segment that never arrives.

Here is an example from an in-JVM dtest, showing an update to the peers table as it shuts down.
{code:java}
park:-1, Unsafe (jdk.internal.misc)
park:323, LockSupport (java.util.concurrent.locks)
await:289, WaitQueue$Standard$AbstractSignal (org.apache.cassandra.utils.concurrent)
await:282, WaitQueue$Standard$AbstractSignal (org.apache.cassandra.utils.concurrent)
awaitUninterruptibly:186, Awaitable$Defaults (org.apache.cassandra.utils.concurrent)
awaitUninterruptibly:259, Awaitable$AbstractAwaitable (org.apache.cassandra.utils.concurrent)
awaitAvailableSegment:283, AbstractCommitLogSegmentManager (org.apache.cassandra.db.commitlog)
advanceAllocatingFrom:257, AbstractCommitLogSegmentManager (org.apache.cassandra.db.commitlog)
allocate:55, CommitLogSegmentManagerStandard (org.apache.cassandra.db.commitlog)
add:282, CommitLog (org.apache.cassandra.db.commitlog)
beginWrite:50, CassandraKeyspaceWriteHandler (org.apache.cassandra.db)
applyInternal:622, Keyspace (org.apache.cassandra.db)
apply:506, Keyspace (org.apache.cassandra.db)
apply:215, Mutation (org.apache.cassandra.db)
apply:220, Mutation (org.apache.cassandra.db)
apply:229, Mutation (org.apache.cassandra.db)
executeInternalWithoutCondition:644, ModificationStatement (org.apache.cassandra.cql3.statements)
executeLocally:635, ModificationStatement (org.apache.cassandra.cql3.statements)
executeInternal:431, QueryProcessor (org.apache.cassandra.cql3)
updateTokens:804, SystemKeyspace (org.apache.cassandra.db)
updateTokenMetadata:2941, StorageService (org.apache.cassandra.service)
handleStateNormal:3057, StorageService (org.apache.cassandra.service)
onChange:2498, StorageService (org.apache.cassandra.service)
markAsShutdown:607, Gossiper (org.apache.cassandra.gms)
doVerb:39, GossipShutdownVerbHandler (org.apache.cassandra.gms)
lambda$new$0:78, InboundSink (org.apache.cassandra.net)
accept:-1, 581110313 (org.apache.cassandra.net.InboundSink$$Lambda$2638)
accept:64, InboundSink$Filtered (org.apache.cassandra.net)
accept:50, InboundSink$Filtered (org.apache.cassandra.net)
accept:97, InboundSink (org.apache.cassandra.net)
accept:45, InboundSink (org.apache.cassandra.net)
run:433, InboundMessageHandler$ProcessMessage (org.apache.cassandra.net)
run:124, ExecutionFailure$1 (org.apache.cassandra.concurrent)
runWorker:1128, ThreadPoolExecutor (java.util.concurrent)
run:628, ThreadPoolExecutor$Worker (java.util.concurrent)
run:30, FastThreadLocalRunnable (io.netty.util.concurrent)
run:829, Thread (java.lang)
{code}
This causes an exception during shutdown for the in-JVM dtest as it is
unable to shutdown {{{}Stage.GOSSIP{}}}, but does not prevent regular
shutdown for Cassandra as the executors are not stopped. The schema update
would be lost, despite requesting a graceful shutdown.",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17516,Official Cassandra packages are missing runtime dependency procps-ng,"Nodetool depends on the free command-line utility, but the official Cassandra RPM does not explicitly install it. free comes from procps-ng, so this package should be made an explicit dependency in the RPM's spec file.

Here's what you get when invoking nodetool without free installed:

bash-5.1# nodetool status
/etc/cassandra/conf/cassandra-env.sh: line 21: free: command not found
expr: syntax error: unexpected argument '2'
expr: syntax error: unexpected argument '2'
/etc/cassandra/conf/cassandra-env.sh: line 59: [: : integer expression expected
/etc/cassandra/conf/cassandra-env.sh: line 63: [: : integer expression expected
/etc/cassandra/conf/cassandra-env.sh: line 67: [: : integer expression expected
expr: syntax error: unexpected argument '4'
/etc/cassandra/conf/cassandra-env.sh: line 81: [: : integer expression expected

I have attached a patch that should fix this issue.",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17492,OWASP failures on lower branches 3/2022,In CASSANDRA-17459 we fixed some security problems that we missed for 3.0 and 3.11: CASSANDRA-17460 for jackson and the driver suppression.,N/A,"3.0.27, 3.11.13"
CASSANDRA-17474,Failed to instantiate SLF4J LoggerFactory in Cassandra 3.11.12,"After upgrading to 3.11.12, Cassandra crashes on startup because of a class that cannot be found in slf4j.

{code}

Failed to instantiate SLF4J LoggerFactory
Reported exception:
java.lang.NoClassDefFoundError: org/slf4j/event/LoggingEvent
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
    at java.lang.Class.privateGetPublicMethods(Class.java:2902)
    at java.lang.Class.getMethods(Class.java:1615)
    at ch.qos.logback.core.joran.util.beans.BeanDescriptionFactory.create(BeanDescriptionFactory.java:35)
    at ch.qos.logback.core.joran.util.beans.BeanDescriptionCache.getBeanDescription(BeanDescriptionCache.java:47)
    at ch.qos.logback.core.joran.util.PropertySetter.<init>(PropertySetter.java:68)
    at ch.qos.logback.core.joran.action.NestedComplexPropertyIA.isApplicable(NestedComplexPropertyIA.java:65)
    at ch.qos.logback.core.joran.spi.Interpreter.lookupImplicitAction(Interpreter.java:233)
    at ch.qos.logback.core.joran.spi.Interpreter.getApplicableActionList(Interpreter.java:252)
    at ch.qos.logback.core.joran.spi.Interpreter.startElement(Interpreter.java:142)
    at ch.qos.logback.core.joran.spi.Interpreter.startElement(Interpreter.java:128)
    at ch.qos.logback.core.joran.spi.EventPlayer.play(EventPlayer.java:50)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:165)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:152)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:110)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:53)
    at ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:65)
    at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:140)
    at org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:84)
    at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:55)
    at org.slf4j.LoggerFactory.bind(LoggerFactory.java:129)
    at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:108)
    at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:302)
    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:276)
    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)
    at org.apache.cassandra.service.CassandraDaemon.<clinit>(CassandraDaemon.java:117)
Caused by: java.lang.ClassNotFoundException: org.slf4j.event.LoggingEvent
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
    ... 27 more
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/slf4j/event/LoggingEvent
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
    at java.lang.Class.privateGetPublicMethods(Class.java:2902)
    at java.lang.Class.getMethods(Class.java:1615)
    at ch.qos.logback.core.joran.util.beans.BeanDescriptionFactory.create(BeanDescriptionFactory.java:35)
    at ch.qos.logback.core.joran.util.beans.BeanDescriptionCache.getBeanDescription(BeanDescriptionCache.java:47)
    at ch.qos.logback.core.joran.util.PropertySetter.<init>(PropertySetter.java:68)
    at ch.qos.logback.core.joran.action.NestedComplexPropertyIA.isApplicable(NestedComplexPropertyIA.java:65)
    at ch.qos.logback.core.joran.spi.Interpreter.lookupImplicitAction(Interpreter.java:233)
    at ch.qos.logback.core.joran.spi.Interpreter.getApplicableActionList(Interpreter.java:252)
    at ch.qos.logback.core.joran.spi.Interpreter.startElement(Interpreter.java:142)
    at ch.qos.logback.core.joran.spi.Interpreter.startElement(Interpreter.java:128)
    at ch.qos.logback.core.joran.spi.EventPlayer.play(EventPlayer.java:50)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:165)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:152)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:110)
    at ch.qos.logback.core.joran.GenericConfigurator.doConfigure(GenericConfigurator.java:53)
    at ch.qos.logback.classic.util.ContextInitializer.configureByResource(ContextInitializer.java:65)
    at ch.qos.logback.classic.util.ContextInitializer.autoConfig(ContextInitializer.java:140)
    at org.slf4j.impl.StaticLoggerBinder.init(StaticLoggerBinder.java:84)
    at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:55)
    at org.slf4j.LoggerFactory.bind(LoggerFactory.java:129)
    at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:108)
    at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:302)
    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:276)
    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)
    at org.apache.cassandra.service.CassandraDaemon.<clinit>(CassandraDaemon.java:117)
Caused by: java.lang.ClassNotFoundException: org.slf4j.event.LoggingEvent
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
    ... 27 more

{code}

It seems like the version of logback introduced in https://issues.apache.org/jira/browse/CASSANDRA-17204 requires a more up to date version of slf4j-api.jar than what is currently shipped with Cassandra (see https://www.slf4j.org/codes.html#log4j_version).

Replacing the shipped version (1.7.7) with the most recent 1.7.X (1.7.36 at the time of this writing) seemingly solves the problem.  ",N/A,3.11.13
CASSANDRA-17444,"DOC - Update title for topologies, CDC archiving properties pages","Documentation revision request.

Below are details.
https://github.com/apache/cassandra/pull/1469",N/A,"3.11.12, 4.0.3"
CASSANDRA-17428,Fail build when Ant is not of a certain version,"As of writing this ticket, trunk is known to be buildable with Ant 1.10 and users have reported build failures on Ant 1.9 (see CASSANDRA-16831).

There should be a check which fails the build if Ant used to build Cassanra source code was not at least 1.10.",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17415,dropping of a materialized view does not create a snapshot with dropped- prefix,"When auto_snapshot: true and MV is dropped, the name of the snapshot does not start with ""dropped-"" prefix as a normal table would. This is an issue for 3.11.x only. In 4.x, the code was refactored a lot and it does not happen there.",N/A,3.11.13
CASSANDRA-17407,Validate existence of DCs when repairing,"With the new validation of data centers in the replication factor, it might be good to give similar treatment to repair.

Currently the behavior of the --in-dc flag only validates that it contains the local data center.

If a list is given containing nonexistent data centers, the repair will pass without errors or warning as long as this list also contains the local data center.

My suggestion would be to validate all the data centers and give an error when a nonexistent data center is given.

 ",N/A,"3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17394,"WEBSITE - February 2022 blog ""Upgrade Advisory: 3.0, 3.11, 4.0 Possible for Remote Code Execution for Scripted UDFs""","This ticket is to capture the work associated with publishing the February 2022 blog ""Upgrade Advisory: 3.0, 3.11, 4.0 Possible for Remote Code Execution for Scripted UDFs""

If this blog cannot be published by the *February 18, 2022 publish date*, please contact me/suggest changes when possible in the pull request for the appropriate time that the blog will go live (on blog.adoc and in the blog post .adoc).",N/A,"3.0.0, 3.0.11, 4.0"
CASSANDRA-17388,"DOC - Add entries to CHANGES.txt, NEWS.txt about CVE-2021-44521",Document changes made by CASSANDRA-17352 in {{NEWS.txt}} and {{CHANGES.txt}}.,N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-17387,Test failure: dtest-offheap.counter_test.TestCounters.test_13691,"This test fails only off heap on 3.11, hard to say when it started as it was always failing since we have Butler. Marking it critical as it shows breaking config change.
h3. Stacktrace

Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [main] 2022-02-10 05:06:27,462 DatabaseDescriptor.java (line 117) Fatal configuration error org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [memtable_allocation_type] from your cassandra.yaml at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:137) at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:100) at",N/A,3.11.13
CASSANDRA-17386,Test failure: TestCqlshOutput failing tests ,"The following TestCqlshOutput tests are failing consistently on 3.11 and as far as Butler goes, since the very beginning it started collecting data so I am not able to say at this point what exactly broke them:

Test_static_cf_output, test_boolean_output, test_count_output, test_string_output_ascii, test_null_output, test_columness_key_output, test_numeric_output",N/A,"3.0.27, 3.11.13"
CASSANDRA-17383,Fix user_functions_test.py::TestUserFunctions::test_udf_overload,"user_functions_test.py::TestUserFunctions::test_udf_overload
is failing after CASSANDRA-17190 on all branches except trunk.

 ",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17368,Investigate OWASP failure report,"4.0: netty-all-4.1.58.Final.jar: CVE-2021-43797, CVE-2021-37136, CVE-2021-37137

3.11: netty-all-4.0.44.Final.jar: CVE-2021-43797, CVE-2021-37136, CVE-2021-37137

3.0: netty-all-4.0.44.Final.jar: CVE-2021-43797, CVE-2021-37136, CVE-2021-37137
",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17361,"DOC - STCS page has LCS in title, broken links in compaction page","The STCS page here, [https://cassandra.apache.org/doc/latest/cassandra/operating/compaction/stcs.html], says ""Leveled Compaction Strategy"" in the title where it should say ""Size-tiered Compaction Strategy.

*EDIT:* This needs to be fixed in the following branches:
* {{trunk}} - https://cassandra.apache.org/doc/trunk/cassandra/operating/compaction/stcs.html
* {{4.0}} - https://cassandra.apache.org/doc/4.0/cassandra/operating/compaction/stcs.html
* {{3.11}} - https://cassandra.apache.org/doc/3.11/cassandra/operating/compaction/stcs.html

While looking into this ticket, I've also discovered that the links to the compaction strategy sub-pages are broken in the {{operating/compaction/index.adoc}} and will need to be fixed in the following branches:
* {{trunk}} - https://cassandra.apache.org/doc/trunk/cassandra/operating/compaction/index.html
* {{4.0}} - https://cassandra.apache.org/doc/4.0/cassandra/operating/compaction/index.html
* {{3.11}} - https://cassandra.apache.org/doc/3.11/cassandra/operating/compaction/index.html",N/A,"3.11.12, 4.0.3"
CASSANDRA-17352,CVE-2021-44521: Apache Cassandra: Remote code execution for scripted UDFs,"When running Apache Cassandra with the following configuration:

enable_user_defined_functions: true
enable_scripted_user_defined_functions: true
enable_user_defined_functions_threads: false 

it is possible for an attacker to execute arbitrary code on the host. The attacker would need to have enough permissions to create user defined functions in the cluster to be able to exploit this. Note that this configuration is documented as unsafe, and will continue to be considered unsafe after this CVE.

This issue is being tracked as CASSANDRA-17352

Mitigation:

Set `enable_user_defined_functions_threads: true` (this is default)
or
3.0 users should upgrade to 3.0.26
3.11 users should upgrade to 3.11.12
4.0 users should upgrade to 4.0.2

Credit:

This issue was discovered by Omer Kaspi of the JFrog Security vulnerability research team.",N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-17349,Fix flaky test - dtest-novnode.repair_tests.repair_test.TestRepair.test_simple_sequential_repair,"Failed 2 times in the last 9 runs. Flakiness: 37%, Stability: 77%
Error Message

cassandra.DriverException: ID mismatch while trying to reprepare (expected b'ba2c66a4f13080265ea718e037637d4a', got b'52faf62235132756a26828817a81168d'). This prepared statement won't work anymore. This usually happens when you run a 'USE...' query after the statement was prepared.

Stacktrace

self = <repair_tests.repair_test.TestRepair object at 0x7ff6850f5a60>

    def test_simple_sequential_repair(self):
        """"""
            Calls simple repair test with a sequential repair
            """"""
>       self._simple_repair(sequential=True)

repair_tests/repair_test.py:363: ",N/A,3.0.27
CASSANDRA-17348,Fix flaky test - org.apache.cassandra.distributed.test.GossipTest.nodeDownDuringMove,"Failed 5 times in the last 12 runs. Flakiness: 36%, Stability: 58%

Error Message

java.lang.AssertionError

Stacktrace

java.lang.RuntimeException: java.lang.AssertionError
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.waitOn(IsolatedExecutor.java:218)
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$sync$19(IsolatedExecutor.java:124)
	at org.apache.cassandra.distributed.test.GossipTest.nodeDownDuringMove(GossipTest.java:124)
Caused by: java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getTokens(TokenMetadata.java:568)
	at org.apache.cassandra.distributed.test.GossipTest.lambda$nodeDownDuringMove$805d71c$1(GossipTest.java:122)
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$null$17(IsolatedExecutor.java:123)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83)
	at java.lang.Thread.run(Thread.java:748)
",N/A,"3.0.27, 3.11.13"
CASSANDRA-17346,Hint documentation page has a misaligned table of settings / description / default values,"Within the versioned documentation, the page for configuring hints contains a misaligned table of hint settings, caused the the hinted_handoff_disabled_datacenters row, which shifts all columns 1 further, mis-aligning the rest of the content and then removing the final cell. Screenshot attached of the start of the misalignment on the table.",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17338,Fix flaky test - test_cqlsh_completion.TestCqlshCompletion," Failed 4 times in the last 24 runs. Flakiness: 30%, Stability: 83%

A bunch of the test_completion_* tests fail occasionally with an eyebleed inducing mismatched output.",N/A,"3.0.27, 3.11.13"
CASSANDRA-17337,Fix flaky test - dtest.secondary_indexes_test.TestSecondaryIndexes.test_6924_dropping_ks,"Failed 1 times in the last 30 runs. Flakiness: 6%, Stability: 96%
Error Message

cassandra.OperationTimedOut: errors={'127.0.0.2:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.2:9042

",N/A,"3.11.x, 4.0.x, 4.1-rc1, 4.1, 4.1.x"
CASSANDRA-17331,For rpm packaging replace centos8 docker images with almalinux,"Centos went EOL, and CI started failing today with
{code}
Failed to download metadata for repo 'appstream': Cannot prepare internal mirrorlist: No URLs in mirrorlist
{code}
ref: https://ci-cassandra.apache.org/job/Cassandra-trunk-artifacts/1010/jdk=jdk_1.8_latest,label=cassandra/consoleFull

Switching to almalinux seems one common path forward…",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17328,Fix flaky test - dtest.repair_tests.repair_test.TestRepair.test_dc_parallel_repair,"Failed 4 times in the last 26 runs. Flakiness: 28%, Stability: 84%
Error Message

cassandra.DriverException: ID mismatch while trying to reprepare (expected b'ba2c66a4f13080265ea718e037637d4a', got b'52faf62235132756a26828817a81168d'). This prepared statement won't work anymore. This usually happens when you run a 'USE...' query after the statement was prepared.",N/A,3.0.27
CASSANDRA-17327,Fix flaky test - dtest.offline_tools_test.TestOfflineTools.test_sstableverify,"Failed 15 times in the last 16 runs. Flakiness: 13%, Stability: 6%
Error Message

IndexError: list index out of range",N/A,3.0.27
CASSANDRA-17314,Test Failure: org.apache.cassandra.db.ScrubTest.testScrubCorruptedCounterRow,"Failed 10 times in the last 14 runs. Flakiness: 61%, Stability: 28%

Error Message
Timeout occurred. Please note the time in the report does not reflect the time until the timeout.

{code}
Stacktrace
junit.framework.AssertionFailedError: Timeout occurred. Please note the time in the report does not reflect the time until the timeout.
	at java.util.Vector.forEach(Vector.java:1277)
	at java.util.Vector.forEach(Vector.java:1277)
	at java.util.Vector.forEach(Vector.java:1277)
	at jdk.nashorn.internal.scripts.Script$3$\^eval\_.:program(<eval>:13)
	at jdk.nashorn.internal.runtime.ScriptFunctionData.invoke(ScriptFunctionData.java:637)
	at jdk.nashorn.internal.runtime.ScriptFunction.invoke(ScriptFunction.java:494)
	at jdk.nashorn.internal.runtime.ScriptRuntime.apply(ScriptRuntime.java:393)
	at jdk.nashorn.api.scripting.NashornScriptEngine.evalImpl(NashornScriptEngine.java:449)
	at jdk.nashorn.api.scripting.NashornScriptEngine.evalImpl(NashornScriptEngine.java:406)
	at jdk.nashorn.api.scripting.NashornScriptEngine.evalImpl(NashornScriptEngine.java:402)
	at jdk.nashorn.api.scripting.NashornScriptEngine.eval(NashornScriptEngine.java:155)
	at javax.script.AbstractScriptEngine.eval(AbstractScriptEngine.java:264)
	at java.util.Vector.forEach(Vector.java:1277)
{code}",N/A,3.0.28
CASSANDRA-17312,dtest-large.replace_address_test.TestReplaceAddress.test_restart_failed_replace (from Cassandra dtests),"Consistently failing on 3.0.x

https://ci-cassandra.apache.org/job/Cassandra-3.0/240/testReport/dtest-large.replace_address_test/TestReplaceAddress/test_restart_failed_replace_2/

Failed 8 times in the last 16 runs. Flakiness: 73%, Stability: 50%

Error Message
ccmlib.node.TimeoutError: 26 Jan 2022 23:07:02 [replacement] after 90.12/90 seconds Missing: ['Starting listening for CQL clients'] not found in system.log:  Head: INFO  [main] 2022-01-26 23:04:33,906 YamlConfigura  Tail: ...endingRangeCalculator:1] 2022-01-26 23:06:41,472 TokenMetadata.java:226 - Token -3193255413308472407 changing ownership from /127.0.0.3 to /127.0.0.4

{code}
Stacktrace
self = <replace_address_test.TestReplaceAddress object at 0x7f99546197c0>

    @since('2.2')
    @pytest.mark.resource_intensive
    def test_restart_failed_replace(self):
        """"""
            Test that if a node fails to replace, it can join the cluster even if the data is wiped.
            """"""
>       self._test_restart_failed_replace(mode='wipe')

replace_address_test.py:479: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
replace_address_test.py:539: in _test_restart_failed_replace
    self.replacement_node.start(jvm_args=[""-Dcassandra.replace_address_first_boot={}""
../venv/lib/python3.8/site-packages/ccmlib/node.py:901: in start
    self.wait_for_binary_interface(from_mark=self.mark)
../venv/lib/python3.8/site-packages/ccmlib/node.py:689: in wait_for_binary_interface
    self.watch_log_for(""Starting listening for CQL clients"", **kwargs)
../venv/lib/python3.8/site-packages/ccmlib/node.py:588: in watch_log_for
    TimeoutError.raise_if_passed(start=start, timeout=timeout, node=self.name,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 1643238332.8472316, timeout = 90
msg = ""Missing: ['Starting listening for CQL clients'] not found in system.log:\n Head: INFO  [main] 2022-01-26 23:04:33,906...26 23:06:41,472 TokenMetadata.java:226 - Token -3193255413308472407 changing ownership from /127.0.0.3 to /127.0.0.4\n""
node = 'replacement'

    @staticmethod
    def raise_if_passed(start, timeout, msg, node=None):
        if start + timeout < time.time():
>           raise TimeoutError.create(start, timeout, msg, node)
E           ccmlib.node.TimeoutError: 26 Jan 2022 23:07:02 [replacement] after 90.12/90 seconds Missing: ['Starting listening for CQL clients'] not found in system.log:
E            Head: INFO  [main] 2022-01-26 23:04:33,906 YamlConfigura
E            Tail: ...endingRangeCalculator:1] 2022-01-26 23:06:41,472 TokenMetadata.java:226 - Token -3193255413308472407 changing ownership from /127.0.0.3 to /127.0.0.4

../venv/lib/python3.8/site-packages/ccmlib/node.py:56: TimeoutError
{code}

This test can be run isolation via 'pytest --force-resource-intensive-tests --cassandra-dir=~/cassandra replace_address_test.py::TestReplaceAddress::test_restart_failed_replace'",N/A,3.0.27
CASSANDRA-17302,Test Failure: dtest-offheap.topology_test.TestTopology.test_decommissioned_node_cant_rejoin,"https://ci-cassandra.apache.org/job/Cassandra-4.0/317/testReport/dtest-offheap.topology_test/TestTopology/test_decommissioned_node_cant_rejoin/

Failed 1 times in the last 20 runs. Flakiness: 5%, Stability: 95%

Error Message
AssertionError: assert None  +  where None = <function search at 0x7f0de9492c10>('This node was decommissioned and will not rejoin the ring', '', re.MULTILINE)  +    where <function search at 0x7f0de9492c10> = re.search  +    and   '' = <built-in method join of str object at 0x7f0de963b4b0>([])  +      where <built-in method join of str object at 0x7f0de963b4b0> = '\n'.join  +    and   re.MULTILINE = re.MULTILINE

{code}
Stacktrace
self = <topology_test.TestTopology object at 0x7f0de5899430>

    @since('3.0')
    def test_decommissioned_node_cant_rejoin(self):
        """"""
            @jira_ticket CASSANDRA-8801
    
            Test that a decommissioned node can't rejoin the cluster by:
    
            - creating a cluster,
            - decommissioning a node, and
            - asserting that the ""decommissioned node won't rejoin"" error is in the
            logs for that node and
            - asserting that the node is not running.
            """"""
        rejoin_err = 'This node was decommissioned and will not rejoin the ring'
        self.fixture_dtest_setup.ignore_log_patterns = list(self.fixture_dtest_setup.ignore_log_patterns) + [
            rejoin_err]
    
        self.cluster.populate(3).start()
        node1, node2, node3 = self.cluster.nodelist()
    
        logger.debug('decommissioning...')
        node3.decommission(force=self.cluster.version() >= '4.0')
        logger.debug('stopping...')
        node3.stop()
        logger.debug('attempting restart...')
        node3.start(wait_other_notice=False)
        try:
            # usually takes 3 seconds, so give it a generous 15
            node3.watch_log_for(rejoin_err, timeout=15)
        except TimeoutError:
            # TimeoutError is not very helpful to the reader of the test output;
            # let that pass and move on to string assertion below
            pass
    
>       assert re.search(rejoin_err,
                         '\n'.join(['\n'.join(err_list) for err_list in node3.grep_log_for_errors()]), re.MULTILINE)
E       AssertionError: assert None
E        +  where None = <function search at 0x7f0de9492c10>('This node was decommissioned and will not rejoin the ring', '', re.MULTILINE)
E        +    where <function search at 0x7f0de9492c10> = re.search
E        +    and   '' = <built-in method join of str object at 0x7f0de963b4b0>([])
E        +      where <built-in method join of str object at 0x7f0de963b4b0> = '\n'.join
E        +    and   re.MULTILINE = re.MULTILINE

topology_test.py:416: AssertionError
{code}",N/A,"3.0.30, 3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-17296,Test Failure: dtest-upgrade.upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD.test_rolling_upgrade,"2 failures in 30, looks flaky on timing / subprocess termination.

https://ci-cassandra.apache.org/job/Cassandra-trunk/920/testReport/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD/test_rolling_upgrade/

Failed 2 times in the last 30 runs. Flakiness: 10%, Stability: 93%
Error Message
RuntimeError: A subprocess has terminated early. Subprocess statuses: Process-1 (is_alive: True), Process-2 (is_alive: False), attempting to terminate remaining subprocesses now.
Stacktrace
self = <upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD object at 0x7f22685cebb0>

    @pytest.mark.timeout(3000)
    def test_rolling_upgrade(self):
        """"""
            Test rolling upgrade of the cluster, so we have mixed versions part way through.
            """"""
>       self.upgrade_scenario(rolling=True)

upgrade_tests/upgrade_through_versions_test.py:320: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
upgrade_tests/upgrade_through_versions_test.py:398: in upgrade_scenario
    self._check_on_subprocs(self.fixture_dtest_setup.subprocs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD object at 0x7f22685cebb0>
subprocs = [<Process name='Process-1' pid=28667 parent=314 stopped exitcode=-SIGKILL daemon>, <Process name='Process-2' pid=28686 parent=314 stopped exitcode=1 daemon>]

    def _check_on_subprocs(self, subprocs):
        """"""
            Check on given subprocesses.
    
            If any are not alive, we'll go ahead and terminate any remaining alive subprocesses since this test is going to fail.
            """"""
        subproc_statuses = [s.is_alive() for s in subprocs]
        if not all(subproc_statuses):
            message = ""A subprocess has terminated early. Subprocess statuses: ""
            for s in subprocs:
                message += ""{name} (is_alive: {aliveness}), "".format(name=s.name, aliveness=s.is_alive())
            message += ""attempting to terminate remaining subprocesses now.""
            self._terminate_subprocs()
>           raise RuntimeError(message)
E           RuntimeError: A subprocess has terminated early. Subprocess statuses: Process-1 (is_alive: True), Process-2 (is_alive: False), attempting to terminate remaining subprocesses now.

upgrade_tests/upgrade_through_versions_test.py:456: RuntimeError",N/A,"3.0.30, 4.0.12, 4.1.4, 5.0-alpha2, 5.0, 5.1"
CASSANDRA-17275,FAQ page contains bullet list of div tags,"Within the versioned documentation, the FAQ page has a bullet list at the top which relates to the div's within the layout that is used for navigation. This bullet list is not a set of hyperlinks / clickable, they are just shown.",N/A,"3.11.14, 4.0.5, 4.1-beta1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17273,Lazy transaction log replica creation allows incorrect replica content divergence during compaction,"Recently encountered this around compaction/anticompaction:

{noformat}
2022-01-13 10:18:24,325 ERROR [main] org.apache.cassandra.db.lifecycle.LogTransaction - Unexpected disk state: failed to read transaction log [mf_txn_anticompactionafterrepair_2f826324-742c-11ec-b293-65cae21e111c.log in .../d1/data/.../files-c351f12917af3a5cbc57791cdf178a1f, .../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f]
Files and contents follow:
.../d1/data/.../files-c351f12917af3a5cbc57791cdf178a1f/mf_txn_anticompactionafterrepair_2f826324-742c-11ec-b293-65cae21e111c.log
	ADD:[.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350438-big,0,8][2380834168]
	REMOVE:[.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350435-big,1642049328006,8][2338829485]
	REMOVE:[.../d1/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350436-big,1642049366291,8][4248366924]
	COMMIT:[,0,0][2613697770]
.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/mf_txn_anticompactionafterrepair_2f826324-742c-11ec-b293-65cae21e111c.log
	ADD:[.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350437-big,0,8][4051162457]
		***Does not match <ADD:[.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350438-big,0,8][2380834168]> in first replica file
	ADD:[.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350438-big,0,8][2380834168]
	REMOVE:[.../d2/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350435-big,1642049328006,8][2338829485]
	REMOVE:[.../d1/data/.../files-c351f12917af3a5cbc57791cdf178a1f/prod_p203-files-mf-350436-big,1642049366291,8][4248366924]
	COMMIT:[,0,0][2613697770]
{noformat}

We have two data directories and two transaction log files, but one is missing an ADD entry when the contents of the two log replicas should be identical. One scenario that can cause this is the following:

1. Start anticompaction on a single file, in directory {{/tmp/d0}}.

2. Call {{trackNew()}} with 2 new files, both in a single directory, but in directory {{/tmp/d1}}. This initializes the log file in {{/tmp/d1}}, but there is still no log file in {{/tmp/d0}}.

3. Anticompaction only writes to one of the files in {{/tmp/d1}} (say all other keys were outside the repaired range).

4. When anticompaction is done, the empty writer is aborted and we call {{untrackNew()}}, which removes the added file from the registered log “records"" (BUT NOT FROM DISK in {{/tmp/d1}}).

5. The REMOVE record is added. This references {{/tmp/d0}}. We lazily create the log file there by dumping all the records we have in memory to that file, which does not include the aborted SSTable above.

6. Now the log files contain:

{noformat}
/tmp/d1/logfile.log:
ADD:[/tmp/d1/AntiCompactionTest/AntiCompactionTest-e4fdddf0746e11ecb73ad5a997381615/AntiCompactionTest-AntiCompactionTest-mf-2-big,0,8][3268492367]
ADD:[/tmp/d1/AntiCompactionTest/AntiCompactionTest-e4fdddf0746e11ecb73ad5a997381615/AntiCompactionTest-AntiCompactionTest-mf-3-big,0,8][2813724425]
REMOVE:[/tmp/d0/AntiCompactionTest/AntiCompactionTest-e4fdddf0746e11ecb73ad5a997381615/AntiCompactionTest-AntiCompactionTest-mf-1-big,1642078019000,8][2401235379]
COMMIT:[,0,0][2613697770]
** /tmp/d0/logfile.log:
ADD:[/tmp/d1/AntiCompactionTest/AntiCompactionTest-e4fdddf0746e11ecb73ad5a997381615/AntiCompactionTest-AntiCompactionTest-mf-3-big,0,8][2813724425]
REMOVE:[/tmp/d0/AntiCompactionTest/AntiCompactionTest-e4fdddf0746e11ecb73ad5a997381615/AntiCompactionTest-AntiCompactionTest-mf-1-big,1642078019000,8][2401235379]
COMMIT:[,0,0][2613697770]
{noformat}",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17272,LeveledCompactionStrategy disk space check improvements,"We currently allow reducing scope (removing sstables from the compaction) when starting STCS-in-L0 with LCS if the compaction is too large for the available disk space. We can do the same for L0 -> L1 compactions - but we can only remove L0 sstables to avoid causing overlap in L1.

Also, in 3.0, when starting an LCS compaction we try to [get a writeable location|https://github.com/apache/cassandra/blob/b1a8a56c563b85ab9a34d3bbf9c16278dd441157/src/java/org/apache/cassandra/db/compaction/writers/CompactionAwareWriter.java#L128] where the full result of the compaction will fit - here we should only get a directory where the first sstable fits, otherwise the compaction might fail even though there is enough space in total among the data directories ",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17268,Pin pytest-timeout version,"File {{requirements.txt}} doesn't specify a version for pytest-timeout which will download the latest. Any version >1.4.2 requires pytest >5 and we're pinned at 3.6.4 atm. A fresh install will download and attempt to use the latest pytest-timeout version and fail:

{noformat}
Collecting pytest-timeout
 Using cached pytest_timeout-2.0.2-py3-none-any.whl (11 kB)
ERROR: pytest-timeout 2.0.2 has requirement pytest>=5.0.0, but you'll have pytest 3.6.4 which is incompatible.
{noformat}

it isn't currently failing on local and CI envs bc v1.4.2 is usually cached in the system and the automatic fallback to 1.4.2 prevents it.

The solution is to pin the version to 1.4.2",N/A,"2.2.20, 3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17267,Snapshot true size is miscalculated,"As far as I understand, the snapshot ""size on disk"" is the total size of the snapshot, while the ""true size"" is the (size_on_disk - size_of_live_sstables).

I created a snapshot on a 3.11 node without traffic and I expected the ""true size"" to be 0KB since the original sstables were still present, but this didn't seem to be the case:
{noformat}
$ nodetool listsnapshots
Snapshot Details:
Snapshot name Keyspace name Column family name True size Size on disk
test          ks1           tbl1               4.86 KiB  5.69 KiB

Total TrueDiskSpaceUsed: 4.86 KiB
{noformat}",N/A,"3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17256,Fixes for intermittent in-JVM dtest failures,"Improvements to StreamingTransferTest, MixedModeMessageForwardTest and SchemaDisagreementTest flakes.

For 3.0, adopt the ring settling properties used on trunk. For all releases increase to 15s for ring settling.
For all branches, disable autocompaction during shutdown.
For 4.0 and up, shut down the scheduled executors a little later.
",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17255,Circle move git:// to https://,"Effective today this https://github.blog/2021-09-01-improving-git-protocol-security-github/ is being enforced which fails all our circle builds on the clone stage:

{noformat}
Cloning into '/home/cassandra/cassandra'...
fatal: remote error: 
  The unauthenticated git protocol on port 9418 is no longer supported.
Please see https://github.blog/2021-09-01-improving-git-protocol-security-github/ for more information.

Exited with code exit status 128
CircleCI received exit code 128
{noformat}

Tests show that replacing git:// to https:// fixes the problem.

CC [~mck] as it may also affect ci-jenkins",N/A,"2.2.x, 3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17254,nodetool toppartitions can fail because ByteBuffer.array() returns more bytes than would be considered valid by UTF8Serializer.validate,"The error below is caused by the use of [{{ByteBuffer.array()}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L1628]. Doing so not only makes the hex key potentially incorrect but causes invalid data to be passed to {{AbstractType.getString}} and ultimately {{UTF8Validator.validate}}. 

{code}
error: String didn't validate.
-- StackTrace --
org.apache.cassandra.serializers.MarshalException: String didn't validate.
	at org.apache.cassandra.serializers.UTF8Serializer.validate(UTF8Serializer.java:35)
	at org.apache.cassandra.db.marshal.AbstractType.getString(AbstractType.java:129)
	at org.apache.cassandra.db.ColumnFamilyStore.finishLocalSampling(ColumnFamilyStore.java:1633)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}",N/A,3.0.29
CASSANDRA-17252,ConnectionLimitHandler may leaks connection count if remote connection drops,"In some cases, Netty does not return the original IP used for per-IP counting when the channel becomes inactive,
which throws an NPE before decrementing the active per-IP count.


{code:java}
java.lang.NullPointerException
at org.apache.cassandra.transport.ConnectionLimitHandler.channelInactive(ConnectionLimitHandler.java:101)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
       at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
       at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:389)
       at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
       at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
       at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
       at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
       at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819)
       at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
       at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
       at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
       at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
       at java.base/java.lang.Thread.run(Thread.java:834)
{code}
",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17243,Fix BYTES_PER_MEGABIT in StreamManager,"While working on CASSANDRA-15234 I noticed BYTES_PER_MEGABIT constant in the 
{code:java}
StreamManager
{code}
 class. It was introduced in CASSANDRA-16959.
The current formula converts actually bytes to mebibits. 

The change needed for 3.0, 3.11 and 4.0(I am currently changing rate parameters to be in MiB/s for trunk as part of CASSANDRA-15234):

{code:java}
public static final double BYTES_PER_MEGABIT = (1000 * 1000) / 8; // from bits
{code}


CC [~adelapena]
",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17239,Race in CompactionExecutorTest,"CompactionExecutorTest has a race between the runnable/callable under test completing
and the {{afterExecute}} method stashing it for the test.  Replace the wait/sleep loop
with a {{SimpleCondition}} that is signaled once the test task throwable has been recorded.

This seems fairly hard to hit but has happened on CI.  It took about 2600 iterations on my MacBook to trigger, but you can artificially hit frequently by adding a sleep at the start of the afterExecute method.
",N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-17213,CompactStorageUpgradeTest.compactStorageUpgradeTest fails w/OOM,"[https://ci-cassandra.apache.org/job/Cassandra-trunk/882/testReport/org.apache.cassandra.distributed.upgrade/CompactStorageUpgradeTest/compactStorageUpgradeTest/]
h3. Error Message

GC overhead limit exceeded
h3. Stacktrace

java.lang.OutOfMemoryError: GC overhead limit exceeded at sun.net.www.ParseUtil.encodePath(ParseUtil.java:105) at sun.misc.URLClassPath$JarLoader.checkResource(URLClassPath.java:969) at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1056) at sun.misc.URLClassPath.getResource(URLClassPath.java:249) at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:363) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:362) at org.apache.cassandra.distributed.shared.InstanceClassLoader.findClass(InstanceClassLoader.java:140) at org.apache.cassandra.distributed.shared.InstanceClassLoader.loadClassInternal(InstanceClassLoader.java:123) at org.apache.cassandra.distributed.shared.InstanceClassLoader.loadClass(InstanceClassLoader.java:109) at org.codehaus.jackson.map.introspect.BasicClassIntrospector.<clinit>(BasicClassIntrospector.java:62) at org.codehaus.jackson.map.ObjectMapper.<clinit>(ObjectMapper.java:188) at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:74) at org.apache.cassandra.distributed.impl.Instance.<init>(Instance.java:144) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper$$Lambda$21599/1714755496.apply(Unknown Source) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.newInstance(AbstractCluster.java:247) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.<init>(AbstractCluster.java:226) at org.apache.cassandra.distributed.UpgradeableCluster.newInstanceWrapper(UpgradeableCluster.java:46) at org.apache.cassandra.distributed.UpgradeableCluster.newInstanceWrapper(UpgradeableCluster.java:36) at org.apache.cassandra.distributed.impl.AbstractCluster.newInstanceWrapperInternal(AbstractCluster.java:515) at org.apache.cassandra.distributed.impl.AbstractCluster.<init>(AbstractCluster.java:470) at org.apache.cassandra.distributed.UpgradeableCluster.<init>(UpgradeableCluster.java:40) at org.apache.cassandra.distributed.UpgradeableCluster.<init>(UpgradeableCluster.java:36) at org.apache.cassandra.distributed.UpgradeableCluster$Builder.lambda$new$0(UpgradeableCluster.java:86) at org.apache.cassandra.distributed.UpgradeableCluster$Builder$$Lambda$73/1631826609.newCluster(Unknown Source) at org.apache.cassandra.distributed.shared.AbstractBuilder.createWithoutStarting(AbstractBuilder.java:158) at org.apache.cassandra.distributed.shared.AbstractBuilder.start(AbstractBuilder.java:140) at org.apache.cassandra.distributed.UpgradeableCluster.create(UpgradeableCluster.java:73) at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:223) at org.apache.cassandra.distributed.upgrade.CompactStorageUpgradeTest.compactStorageUpgradeTest(CompactStorageUpgradeTest.java:159)
h3. Standard Output

out of memory on output stream

 

Appears consistent",N/A,"3.0.28, 3.11.14, 4.0.5, 4.1-alpha1, 4.1, 5.0-alpha1, 5.0"
CASSANDRA-17204,Upgrade to Logback 1.2.9 (security),"Logback 1.2.8 has been released with a fix for a potential vulnerability in its JNDI lookup.
 * [http://logback.qos.ch/news.html]
 * [https://jira.qos.ch/browse/LOGBACK-1591]

{quote}*14th of December, 2021, Release of version 1.2.8*
We note that the vulnerability mentioned in LOGBACK-1591 requires write access to logback's configuration file as a prerequisite.
* • In response to LOGBACK-1591, we have disabled all JNDI lookup code in logback until further notice. This impacts {{ContextJNDISelector}} and {{<insertFromJNDI>}} element in configuration files.
* Also in response to LOGBACK-1591, we have removed all database (JDBC) related code in the project with no replacement.

We note that the vulnerability mentioned in LOGBACK-1591 requires write access to logback's configuration file as a prerequisite. A successful RCE requires all of the following to be true:

* write access to logback.xml
* use of versions < 1.2.8
* reloading of poisoned configuration data, which implies application restart or scan=""true"" set prior to attack

Therefore and as an additional precaution, in addition to upgrading to version 1.2.8, we also recommend users to set their logback configuration files as read-only.
{quote}
This is not as bad as CVE-2021-44228 in Log4j <2.15.0 (Log4Shell), but should probably be fixed anyway.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17202,Avoid unnecessary String.format in QueryProcessor when getting stored prepared statement ,"In the _QueryProcessor#getStoredPreparedStatement_ if the statement is found in the prepared statements cache, there is always unnecessary string creation using String.format in order to execute the _checkTrue_ assertion. The string construction is necessary only when the queries are not equal.
{code:java}
public static ResultMessage.Prepared getStoredPreparedStatement(String queryString, String clientKeyspace)
throws InvalidRequestException
{
    MD5Digest statementId = computeId(queryString, clientKeyspace);
    Prepared existing = preparedStatements.getIfPresent(statementId);
    if (existing == null)
        return null;

    checkTrue(queryString.equals(existing.rawCQLStatement),
            String.format(""MD5 hash collision: query with the same MD5 hash was already prepared. \n Existing: '%s'"", existing.rawCQLStatement));
 {code}
Hopefully the JIT can optimize this once the _checkTrue_ is inlined, but it's getting on my nerves as it's popping up on my flame graphs all the time.

 ",N/A,"3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-17140,Broken test_rolling_upgrade - upgrade_tests.upgrade_through_versions_test.TestUpgrade_indev_3_0_x_To_indev_4_0_x,"The tests ""test_rolling_upgrade"" fail with the below error. 
 
[https://app.circleci.com/pipelines/github/yifan-c/cassandra/279/workflows/6340cd42-0b27-42c2-8418-9f8b56c57bea/jobs/1990]
 
I am able to alway produce it by running the test locally too. 
{{$ pytest --execute-upgrade-tests-only --upgrade-target-version-only --upgrade-version-selection all --cassandra-version=4.0 upgrade_tests/upgrade_through_versions_test.py::TestUpgrade_indev_3_11_x_To_indev_4_0_x::test_rolling_upgrade}}
 
{code:java}
self = <upgrade_tests.upgrade_through_versions_test.TestUpgrade_indev_3_0_x_To_indev_4_0_x object at 0x7ffba4242fd0>
subprocs = [<Process(Process-1, stopped[SIGKILL] daemon)>, <Process(Process-2, stopped[1] daemon)>]

    def _check_on_subprocs(self, subprocs):
        """"""
            Check on given subprocesses.
    
            If any are not alive, we'll go ahead and terminate any remaining alive subprocesses since this test is going to fail.
            """"""
        subproc_statuses = [s.is_alive() for s in subprocs]
        if not all(subproc_statuses):
            message = ""A subprocess has terminated early. Subprocess statuses: ""
            for s in subprocs:
                message += ""{name} (is_alive: {aliveness}), "".format(name=s.name, aliveness=s.is_alive())
            message += ""attempting to terminate remaining subprocesses now.""
            self._terminate_subprocs()
>           raise RuntimeError(message)
E           RuntimeError: A subprocess has terminated early. Subprocess statuses: Process-1 (is_alive: True), Process-2 (is_alive: False), attempting to terminate remaining subprocesses now.{code}",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-17139,readWriteDuringBootstrapTest - org.apache.cassandra.distributed.test.ring.BootstrapTest,"The test was seen failing in both 
[CircleCI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1188/workflows/06afaf5b-6951-4b3d-8fbf-6ef2aef04e52/jobs/7025] and [Jenkins|https://jenkins-cm4.apache.org/job/Cassandra-4.0/267/testReport/junit/org.apache.cassandra.distributed.test.ring/BootstrapTest/readWriteDuringBootstrapTest/] lately. ",N/A,"3.0.27, 4.0.6"
CASSANDRA-17134,Update CircleCI config to use the latest Docker image,CircleCI config should be updated to use the latest docker image on all branches.,N/A,"2.1.23, 3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17080,Fix test: dtest-upgrade.upgrade_tests.drop_compact_storage_upgrade_test.TestDropCompactStorage.test_drop_compact_storage_mixed_cluster,"!https://ci-cassandra.apache.org/static/a177fe56/images/32x32/health-80plus.png! Failed 28 times in the last 28 runs. Flakiness: 0%, Stability: 0%
  
 Example of failure: [https://ci-cassandra.apache.org/job/Cassandra-trunk/801/testReport/junit/dtest-upgrade.upgrade_tests.drop_compact_storage_upgrade_test/TestDropCompactStorage/test_drop_compact_storage_mixed_cluster/]
   
{code:java}
upgrade_tests/drop_compact_storage_upgrade_test.py:149: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <upgrade_tests.drop_compact_storage_upgrade_test.TestDropCompactStorage object at 0x7fa0e7f1ceb0>
session = <cassandra.cluster.Session object at 0x7fa0e7c56d30>
assert_msg = 'Cannot DROP COMPACT STORAGE as some nodes in the cluster ([/127.0.0.2:7000, /127.0.0.1:7000]) are not on 4.0+ yet. Please upgrade those nodes and run `upgradesstables` before retrying.'

    def drop_compact_storage(self, session, assert_msg):
        try:
            session.execute(""ALTER TABLE drop_compact_storage_test.test DROP COMPACT STORAGE"")
            pytest.fail(""No exception has been thrown"")
        except InvalidRequest as e:
>           assert assert_msg in str(e)
E           assert 'Cannot DROP COMPACT STORAGE as some nodes in the cluster ([/127.0.0.2:7000, /127.0.0.1:7000]) are not on 4.0+ yet. Please upgrade those nodes and run `upgradesstables` before retrying.' in 'Error from server: code=2200 [Invalid query] message=""Cannot DROP COMPACT STORAGE as some nodes in the cluster ([/127....1:7000, /127.0.0.2:7000]) are not on 4.0+ yet. Please upgrade those nodes and run `upgradesstables` before retrying.""'
E            +  where 'Error from server: code=2200 [Invalid query] message=""Cannot DROP COMPACT STORAGE as some nodes in the cluster ([/127....1:7000, /127.0.0.2:7000]) are not on 4.0+ yet. Please upgrade those nodes and run `upgradesstables` before retrying.""' = str(InvalidRequest('Error from server: code=2200 [Invalid query] message=""Cannot DROP COMPACT STORAGE as some nodes in the...1:7000, /127.0.0.2:7000]) are not on 4.0+ yet. Please upgrade those nodes and run `upgradesstables` before retrying.""'))

upgrade_tests/drop_compact_storage_upgrade_test.py:45: AssertionError
{code}",N/A,"3.0.26, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17053,Let IntelliJ run each test class in a separate JVM,"When we run multiple unit test with ANT command, each test class is run in its own JVM instance - we have to do that because many tests set up some static stuff - I don't want to discuss that in this ticket though - it works and must work this way for now. 

However, when we run tests in IntelliJ, especially when we want to run all test classes from the whole package, or selected subset of tests, IntelliJ tries to run all of them in a single JVM which obviously leads to problems. 

Frankly, IntelliJ introduced (I don't know exactly when) a new feature so that each test class can be run in a separate JVM, just like we do when we run with ANT. I this ticket I want to modify the default test configuration for IntelliJ so that it works in the way described above.
",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17050,Upgrade tests fail with InvocationTargetException,Upgrade tests are currently failing due to the new dtest-api changes and their integration with trunk.,N/A,"2.2.20, 3.0.26, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17049,Fix rare NPE caused by batchlog replay / node decomission races,"Batchlog replay process collects addresses of the hosts that have been hinted to, so it can flush hints for them to disk before confirming deletion of the replayed batches. If a node has been decommissioned during replay, however, when the time comes to flush the hints at the very end of replay, {{StorageService.getHostIdForEndpoint()}} will return {{null}} for its address, which will, down the line, cause {{HintsCatalog::get()}} to be invoked with a {{null}} host id argument, causing an NPE.

The simple fix is to check returned host ids for addresses for nulls, and collect hinted host ids instead of hinted addresses.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17047,Dropping a column can break queries until the schema is fully propagated (TAKE 2),"With a table like:
{code}
CREATE TABLE ks.tbl (id int primary key, v1 int, v2 int, v3 int)
{code}

and we drop {{v2}}, we get this exception on the replicas which haven't seen the schema change:
{code}
ERROR [SharedPool-Worker-1] node2 2020-06-24 09:49:08,107 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,node2]
java.lang.IllegalStateException: [ColumnDefinition{name=v1, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, position=-1}, ColumnDefinition{name=v2, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, position=-1}, ColumnDefinition{name=v3, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, position=-1}] is not a subset of [v1 v3]
	at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:546) ~[main/:na]
	at org.apache.cassandra.db.Columns$Serializer.serializeSubset(Columns.java:478) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:184) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:114) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:102) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:132) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[main/:na]
...
{code}

Note that it doesn't matter if we {{SELECT *}} or {{SELECT id, v1}}

CASSANDRA-15899 tried to fix the problem when columns are dropped as well as when columns are added. Unfortunately the fix introduced an issue and had to be reverted in CASSANDRA-16735. 

If the scenario for ADDED columns is tricky, the original scenario for DROPPED columns can  be solved in a safe way at the {{ColumnFilter}} level. By consequence, I think that we should at least solve that scenario.

[~bdeggleston], [~samt], [~ifesdjeen] does my proposal makes sense to you?    
",N/A,"3.0.x, 3.11.x, 4.0.x"
CASSANDRA-17043,CircleCI dtest multiplexer with MIDRES needs more resources,"The CircleCI jobs for regular dtests jobs have more resources in MIDRES, which is necessary for some dtests to reliably success. However, the dtest multiplexer uses the same resources for LOWRES and MIDRES.

I think that the dtest multiplexer should always use the same resources as the regular dtests. Using too small resources in the multiplexer can lead to failures that don't reproduce in the regular dtest jobs, like the one we found in [CASSANDRA-16334|https://app.circleci.com/pipelines/github/adelapena/cassandra/1020/workflows/63908694-e4d7-40b1-9418-7a4b87826233/jobs/9422] when trying to repeatedly run a resource-hungry dtest, or like [this other one|https://app.circleci.com/pipelines/github/adelapena/cassandra/1020/workflows/63908694-e4d7-40b1-9418-7a4b87826233/jobs/9422] while running {{test_network_topology}}.

This happens because I forgot to update the diff patch when adding the multiplexer. This doesn't affect HIGHRES because in that case the patch changes the configuration of the test executor, while in MIDRES a new executor is defined.

 ",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-17028,Backport snakeyaml 1.26 upgrade,"In CASSANDRA-16150 snakeyaml was upgraded, but only in the 4.0 line.  We should backport this to 3.11 and 3.0 as well.",N/A,3.11.12
CASSANDRA-17014,Make -Dtest.methods optional in all Ant test targets,"The Ant build file contains several targets to run specific tests:
 * testsome
 * long-testsome
 * burn-testsome
 * cql-test-some
 * stress-test-some
 * test-jvm-dtest-some

These targets take two arguments, {{test.name}} and {{test.methods}}. The first argument, {{test.name}}, is always mandatory. However, {{test.methods}} can be either mandatory or optional depending on the target and the project branch. For example, in {{testsome}} the argument is mandatory in 3.0 and 3.11 and optional in 4.0 and trunk. Also, in trunk {{test.methods}} is optional for all the targets except {{cql-test-some}}.

The purpose of this ticket is making {{test.methods}} consistently optional for all the {{*some}} targets.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16996,Prevent broken concurrent schema read/writes,See CASSANDRA-16856 where the concurrent read/write path was left out,N/A,"3.11.12, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16995,Add tests for Resource fromName/getName,As a part of CASSANDRA-16977 we identified a problem functions ({{FunctionResource.fromName}}) parsing logic. We agreed that {{Resource.fromName/}}{{Resource.getName}} should be symmetrical. I'd like to ensure that by having a corresponding unit test.,N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16990,Update jbcrypt library to 0.4 from 0.3m to resolve CVE-2015-0886,"We are using jbcrypto of version 0.3m across all versions, this version of the library was never changed since 1.1.2.

In 0.3m they found out this (1) and (2, 3 for better explanation / reference)

I think we are affected by this, it is possible to set 31 rounds here (4) which would hit the same same logic afteward these tickets are talking about.

1) [https://nvd.nist.gov/vuln/detail/CVE-2015-0886]

2) [http://www.mindrot.org/projects/jBCrypt/news/rel04.html]

3) [https://bugzilla.mindrot.org/show_bug.cgi?id=2097]

4) [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/auth/CassandraRoleManager.java#L105-L117]

I  hence propose to update the library to 0.4 where this is fixed.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16989,Add environment variables to CircleCI config generation script,"The purpose of this ticket is adding arguments to the CircleCI config generation script allowing to set the values specific environment variables such as {{DTEST_REPO}} or {{DTEST_BRANCH}} in the generated {{.circleci/config.yml}} file. For example, we could generate a CircleCI config file with MIDRES specifying a dtest repo and branch by running:
{code}
generate.sh -m \
  -e DTEST_REPO=git://github.com/adelapena/cassandra-dtest.git \
  -e DTEST_BRANCH=CASSANDRA-8272 
{code}
Or we could set the test multiplexer for repeating a specific test with HIGHRES:
{code}
generate.sh -h \
  -e REPEATED_UTEST_TARGET=testsome \
  -e REPEATED_UTEST_CLASS=org.apache.cassandra.cql3.ViewTest \
  -e REPEATED_UTEST_METHODS=testCompoundPartitionKey,testStaticTable \
  -e REPEATED_UTEST_COUNT=100
{code}
This can be useful on its own so we don't have to manually edit the {{config-2_1.yml}}/{{config.yml}}, and it can also be useful for automation scripts manipulating these files.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16986,DROP Table should not recycle active CommitLog segments,"Right now, DROP TABLE recycles all active CL segments and explicitly marks intervals as clean for all dropping tables. I believe that this is not necessary.

Recycling of CL segments was introduced in CASSANDRA-3578. Back then, it was necessary to recycle all active segments because:
1. CommitLog reused old segments after they were clean. This is no longer the case, I believe, since CASSANDRA-6809.
2. CommitLog segments must have been closed and recycled on {{DROP TABLE}} to avoid resurrecting data if a table with the same name is created. This was an issue because tables didn't have unique ids yet (CASSANDRA-5202).

Given that {{DROP TABLE}} triggers flush, which in turn cleans CL intervals in Keyspace#unloadCF, I think that we can avoid the call to {{forceRecycleAll}} there.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16977,ArrayIndexOutOfBoundsException in FunctionResource#fromName,"{{FunctionResource}} can't handle functions with 0 args where it throws:

{noformat}
2021-04-08 10:45:40,984 ErrorMessage.java:387 - Unexpected exception during request
java.lang.ArrayIndexOutOfBoundsException: 1
at org.apache.cassandra.auth.FunctionResource.fromName(FunctionResource.java:178)
{noformat}


",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16975,CompactionTask#runMayThrow should not release new SSTables for offline transactions,"Right now, {{CompactionTask#runMayThrow}} releases new SSTables for offline transactions ([code|https://github.com/apache/cassandra/blob/f7c71f65c000c2c3ef7df1b034b8fdd822a396d8/src/java/org/apache/cassandra/db/compaction/CompactionTask.java#L227-L230]). This change was added in CASSANDRA-8962, prior to the introduction of lifecycle transactions in CASSANDRA-8568. I suspect that this behavior might be undesired and could have just fallen through the cracks.

To my knowledge, this code does not cause any known bugs solely because in-tree tools do not access the SSTables they produce before exiting. However, if someone is to write, say, offline compaction daemon, it might break on subsequent compactions because newly created SSTables will be released.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16973,Fix org.apache.cassandra.concurrent.LongSharedExecutorPoolTest.testPromptnessOfExecution,"org.apache.cassandra.concurrent.LongSharedExecutorPoolTest.testPromptnessOfExecution fails in [3.11|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1119/testReport/junit/org.apache.cassandra.concurrent/LongSharedExecutorPoolTest/testPromptnessOfExecution/]
h3.  
{code:java}
Stacktrace
junit.framework.AssertionFailedError at org.apache.cassandra.concurrent.LongSharedExecutorPoolTest.testPromptnessOfExecution(LongSharedExecutorPoolTest.java:169) at org.apache.cassandra.concurrent.LongSharedExecutorPoolTest.testPromptnessOfExecution(LongSharedExecutorPoolTest.java:102)

Standard Output
Completed 0K batches with 0.0M events Running for 120s with load multiplier 0.5

Standard Error
SLF4J: The following set of substitute loggers may have been accessed SLF4J: during the initialization phase. Logging calls during this SLF4J: phase were not honored. However, subsequent logging calls to these SLF4J: loggers will work as normally expected. SLF4J: See also
http://www.slf4j.org/codes.html#substituteLogger
SLF4J: org.apache.cassandra.LogbackStatusListener
{code}
 ",N/A,"3.0.26, 3.11.12"
CASSANDRA-16972,Fix org.apache.cassandra.cql3.ViewTest.testTruncateWhileBuilding ,"[org.apache.cassandra.cql3.ViewTest.testTruncateWhileBuilding|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1119/testReport/junit/org.apache.cassandra.cql3/ViewTest/testTruncateWhileBuilding/]  fails in 3.11

 
{code:java}
Error Message
expected:<0> but was:<1>

Stacktrace
junit.framework.AssertionFailedError: expected:<0> but was:<1> at org.apache.cassandra.Util.spinAssertEquals(Util.java:575) at org.apache.cassandra.cql3.ViewTest.testTruncateWhileBuilding(ViewTest.java:1656) at org.jboss.byteman.contrib.bmunit.BMUnitRunner$9.evaluate(BMUnitRunner.java:342) at org.jboss.byteman.contrib.bmunit.BMUnitRunner$6.evaluate(BMUnitRunner.java:241) at org.jboss.byteman.contrib.bmunit.BMUnitRunner$1.evaluate(BMUnitRunner.java:75)
{code}
 ",N/A,"3.0.26, 3.11.12"
CASSANDRA-16969,Scrub does not detect invalid partition keys,"The standalone scrubber [gets the key|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/Scrubber.java#L202] from the file but never validates it, and this will propagate to the new sstable so it will be corrupted when read later.",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16960,Improve MV TTL error message,"Old MVs could have been created with a {{default_time_to_live}} before the time of CASSANDRA-12868.

A few years forward customers altering that MV for other reasons might get a very confusing message which can benefit from some clarification.

{code}
ALTER MATERIALIZED VIEW XXXXX_view WITH gc_grace_seconds = 10800;

Cannot set or alter default_time_to_live for a materialized view. Data in a materialized view always expire at the same time than the corresponding data in the parent table.
{code}
",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16959,nodetool setstreamthroughput accepts invalid arguments that are not immediately applied,"Both {{nodetool setstreamthroughput}} and {{nodetool setinterdcstreamthroughput}} accept a negative throughput. The throughput value is not immediately applied to the corresponding rate limiters. Instead, the value is [set in the {{Config}}|https://github.com/apache/cassandra/blob/09c89e5f5f8604301c233130dfb6e82a36ae30f3/src/java/org/apache/cassandra/service/StorageService.java#L1488] and it's only applied to the singleton rate limiter when new sstable stream writer are created (see [here|https://github.com/apache/cassandra/blob/09c89e5f5f8604301c233130dfb6e82a36ae30f3/src/java/org/apache/cassandra/streaming/StreamManager.java#L66-L76]). This could happen much later than the definition of the new throughput, and by then the setting of the new rate in the rate limiter will fail with an {{IllegalArgumentException}} due to the negative value.

I think we should either immediately reject negative throughputs or consider them unlimited, as we do with zero. Also we should probably apply the new throughput to the rate limiter immediately, since I don't see why we should wait to start using the new throughput.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16954,Flaky tests due to teardown failure,"Different dtests in several CircleCI builds failed with teardown failure due to network failure with the error:
{code}
test teardown failure
Unexpected error found in node logs (see stdout for full details). Errors: [WARN  [epollEventLoopGroup-5-4] 2021-09-14 09:35:15,897 ExceptionHandlers.java:134 - Unknown exception in client networking
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer, WARN  [epollEventLoopGroup-5-4] 2021-09-14 09:35:15,897 ExceptionHandlers.java:134 - Unknown exception in client networking
io.netty.channel.unix.Errors$NativeIoException: readAddress(..) failed: Connection reset by peer]
{code}

For example, {{test_view_metadata_cleanup}} from {{materialized_views_test.TestMaterializedViews}} failed in [this build|https://app.circleci.com/pipelines/github/k-rus/cassandra/18/workflows/0cb193f3-ffe8-41c1-a376-43c91634579e/jobs/185/tests#failed-test-0] or {{test_expiration_overflow_policy_reject}} from {{ttl_test.TestTTL}} failed in [this build|https://app.circleci.com/pipelines/github/k-rus/cassandra/8/workflows/da99468d-c513-4a4f-9fd3-48b67482ce3e/jobs/67/tests#failed-test-0]",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16948,offline_tools_test.py::TestOfflineTools::test_sstableverify fails on 3.11,"As noted by [~stefan.miklosovic]:

===Flaky Test Report===

test_sstableverify failed and was not selected for rerun.
        <class 'AssertionError'>
        assert None
 +  where None = <function search at 0x7fef074b5550>(('WARNING: Corrupted SSTable : ' + '/tmp/dtest-z6njep37/test/node1/data2/keyspace1/standard1-9b45a1f0149411ecbc69f72e6826361e/me-12-big-Data.db'), ""Subprocess sstableverify on keyspace1 : standard1 with options: ['-v'] exited with non-zero status; exit status: 1; \...p/dtest-z6njep37/test/node1/cdc_raw; setting cdc_total_space_in_mb to 3831.  You can override this in cassandra.yaml\n"")
 +    where <function search at 0x7fef074b5550> = re.search
        [<TracebackEntry /home/ubuntu/cassandra-dtest/offline_tools_test.py:303>]
",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16944,Single partition reads can read more SSTables than required ,"For some scenarios involving row deletions, range deletions or static columns, the logic of {{SinglePartitionReadCommand.queryMemtableAndSSTablesInTimestampOrder}} might trigger more SSTables reads that expected. 

For row deletions and range deletions the reasons is that the logic do not take them into account. Once we hit a deleted row (caused by a row deletion or a range deletion) with a timestamp higher than the one of the next SStable we know that we can stop reading more SSTables.

For static columns the problems seems to have been introduced by the changes in CASSANDRA-16671.      ",N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-16917,Create property to hold local maven repository location,"In their current state, both build.xml and .build/build-resolver.xml refer to the local maven repository by ""${user.home}/.m2/repository"" in multiple places.
It would be nice instead to have a property store this location, and refer the property vs the actual location itself. 
If one needs to change this maven repository location for their environmental purposes, this change makes it easy since it will need to be changed only at one place (the value of the property) vs changing all the direct references to the repository location.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16906,DOC - Formulae in Evaluating DM page does not display correctly,"In the [Evaluating and Refining Data Models|https://cassandra.apache.org/doc/latest/cassandra/data_modeling/data_modeling_refining.html] page, all of formulas does not display correctly:

 !スクリーンショット 2021-08-31 18.03.57.png|width=500! 

h3. Environment
browser: chrome 92.0.4515.159
os: macOS Big Sur 11.5.2

h3. Slack 

[https://the-asf.slack.com/archives/CJZLTM05A/p1630400708095100]
[https://the-asf.slack.com/archives/CJZLTM05A/p1630400951095900]",N/A,"3.11.15, 4.0.9, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-16902,A user should be able to view permissions of role they created,"Currently users are denied to view permissions to see a role they created:
{code}
CREATE ROLE parent WITH PASSWORD = 'x' AND LOGIN = true;
GRANT CREATE ON ALL ROLES TO parent;
LOGIN parent;
CREATE ROLE child WITH PASSWORD = 'x' AND LOGIN = true;
LIST ALL PERMISSIONS OF 'child'; -- You are not authorized to view child's permissions
{code}
When a user creates a role they should get the {{DESCRIBE}} permission on that role by default.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16897,Remove duplicate 'lib.download.sha' entries in build-resolver.xml,As reported by [~jasonstack] [here|https://the-asf.slack.com/archives/CK23JSY2K/p1630325518017000].,N/A,3.11.12
CASSANDRA-16883,Weak visibility guarantees of Accumulator can lead to failure to recognize digest mismatches,"The context for this problem is largely the same as CASSANDRA-16807. The difference is that for 4.0+, CASSANDRA-16097 added an assertion to {{DigestResolver#responseMatch()}} that ensures the responses snapshot has at least one visible element (although of course only one element trivially cannot generate a mismatch and short-circuits immediately). In 3.0 and 3.11, this assertion does not exist, and when the underlying problem occurs (i.e. zero responses are visible on {{Accumulator}} when there should be 2), we can silently avoid the digest matching entirely. This seems like it would make it both impossible to do a potentially necessary full data read to resolve the correct response and prevent repair.

The fix here should be similar to the one in CASSANDRA-16807, although there might be some test infrastructure that needs porting in order to make that work.",N/A,"3.0.26, 3.11.12"
CASSANDRA-16882,Save CircleCI resources with optional test jobs,"This ticket implements the addition of approval steps in the CircleCI workflows as it was proposed in [this email|https://lists.apache.org/thread.html/r57bab800d037c087af01b3779fd266d83b538cdd29c120f74a5dbe63%40%3Cdev.cassandra.apache.org%3E] sent to the dev list:

The current CircleCI configuration automatically runs the unit tests, JVM dtests and cqhshlib tests. This is done by default for every commit or, with some configuration, for every push.

Along the lifecycle of a ticket it is quite frequent to have multiple commits and pushes, all running these test jobs. I'd say that frequently it is not necessary to run the tests for some of those intermediate commits and pushes. For example, one can show proofs of concept, or have multiple rounds of review before actually running the tests. Running the tests for every change can produce an unnecessary expense of CircleCI resources.

I think we could make running those tests optional, as well as clearly specifying in the documentation what are the tests runs that are mandatory before actually committing. We could do this in different ways:
 # Make the entire CircleCI workflow optional, so the build job requires
 manual approval. Once the build is approved the mandatory test jobs would
 be run without any further approval, exactly as it's currently done.
 # Make all the test jobs optional, so every test job requires manual approval, and the documentation specifies which tests are mandatory in the final steps of a ticket.
 # Make all the mandatory test jobs depend on a single optional job, so we have a single button to optionally run all the mandatory tests.

I think any of these changes, or a combination of them, would significantly
 reduce the usage of resources without making things less tested. The only
 downside I can think of is that we would need some additional clicks on the
 CircleCI GUI.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16871,Add resource flags to CircleCi config generation script,"Currently we have three versions of the CircleCI config file using different resources. Changing the resources configuration is as easy as copying the desired template file, for example:
{code}
cp .circleci/config.yml.MIDRES .circleci/config.yml
{code}
If we want to make changes to the file, for example to set a specific dtest repo or running the test multiplexer, we can run the provided generation script, copy the template file and probably exclude the additional changes:
{code}
# edit config-2_1.yml
.circleci/generate.sh
cp .circleci/config.yml.MIDRES .circleci/config.yml
# undo the changes in config.yml.LOWRES, config.yml.MIDRES and config.yml.HIGHRES
{code}
A very common alternative to this is just editing the environment variables in the automatically generated {{config.yml}} file, which are repeated some 19 times across the file:
{code}
cp .circleci/config.yml.MIDRES .circleci/config.yml
# edit config.yml, where env vars are repeated
{code}
I think we could do this slightly easier by adding a set of flags to the generation script to apply the resources patch directly to {{config.yml}}, without changing the templates:
{code}
# edit config-2_1.yml
.circleci/generate.sh -m
{code}
This has the advantage of not requiring manually editing the automatically generated file and also providing some validation.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16868,Secondary indexes on primary key columns can miss some writes,"Secondary indexes on primary key columns can miss some writes. For example, an update after a deletion won't create an index entry:
{code:java}
CREATE TABLE t (pk int, ck int, v int, PRIMARY KEY (pk, ck));
CREATE INDEX ON t(ck);
INSERT INTO t(pk, ck, v) VALUES (1, 2, 3); -- creates an index entry (right)
DELETE FROM t WHERE pk = 1 AND ck = 2; -- deletes the previous index entry (right)
UPDATE t SET v = 3 WHERE pk = 1 AND ck = 2; -- doesn't create a new index entry (wrong)
SELECT * FROM t WHERE ck = 2; -- doesn't find the row (wrong)
{code}
This happens because the update uses the {{LivenssInfo}} of the previously deleted row (see [here|https://github.com/apache/cassandra/blob/cassandra-3.0.25/src/java/org/apache/cassandra/index/internal/CassandraIndex.java#L439]). The same happens when updating an expired row:
{code:java}
CREATE TABLE t (pk int, ck int, v int, PRIMARY KEY (pk, ck));
CREATE INDEX ON t(ck);
UPDATE t USING TTL 1 SET v = 3 WHERE pk = 1 AND ck = 2; -- creates a non-expiring index entry (right)
-- wait for the expiration of the above row
SELECT * FROM t WHERE ck = 2; -- deletes the index entry (right)
UPDATE t SET v = 3 WHERE pk = 1 AND ck = 2; -- doesn't create an index entry (wrong)
SELECT * FROM t WHERE ck = 2; -- doesn't find the row (wrong)
{code}
I think that the fix for this is just using the {{getPrimaryKeyIndexLiveness}} in {{updateRow}}, as it's used in {{insertRow}}.

Another related problem is that {{getPrimaryKeyIndexLiveness}} uses [the most recent TTL in the columns contained on the indexed row fragment|https://github.com/apache/cassandra/blob/cassandra-3.0.25/src/java/org/apache/cassandra/index/internal/CassandraIndex.java#L519] as the TTL of the index entry, producing an expiring index entry that ignores the columns without TTL that are already present in flushed sstables. So we can find this other error when setting a TTL over flushed indexed data:
{code:java}
CREATE TABLE t(k1 int, k2 int, v int, PRIMARY KEY ((k1, k2)));
CREATE INDEX idx ON t(k1);
INSERT INTO t (k1, k2, v) VALUES (1, 2, 3);
-- flush
UPDATE t USING TTL 1 SET v=0 WHERE k1=1 AND k2=2; -- creates an index entry with TTL (wrong)
-- wait for TTL expiration
SELECT TTL(v) FROM t WHERE k1=1; -- doesn't find the row (wrong)
{code}
The straightforward fix is just ignoring the TTL of the columns for indexes on primary key components, so we don't produce expiring index entries in that case. The index entries will be eventually deleted during index reads, when we are sure that they are not pointing to any live data.
  ",N/A,"3.0.26, 3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16862,Fix flaky test DatabaseDescriptorRefTest,"While working on another ticket I found out that DatabaseDescriptorRefTest is failing consistently locally for me and one more community member on 3.11, 4.0 and trunk.


{code:java}
 java.lang.AssertionError: thread started in clientInitialization 
Expected :5
Actual   :8
<Click to see difference>


	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.apache.cassandra.config.DatabaseDescriptorRefTest.testDatabaseDescriptorRef(DatabaseDescriptorRefTest.java:285)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:221)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
{code}
",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16858,CircleCI MIDRES might be missing some changes in 3.11,"I've noticed that when we run {{.circleci/generate.sh}} in 3.11 the file {{config.yml.MIDRES}} is modified, when I'd say it shouldn't be changed. If I'm right running that script should be noop when there hasn't been changes in {{config-2_1.yml}}, and indeed the file isn't modified in the other branches.

Not sure yet, but I think that this might be the result of not having generated a new {{config.yml.MIDRES}} with the script when merging 3.0 into 3.11 during CASSANDRA-16804.",N/A,3.11.12
CASSANDRA-16856,Prevent broken concurrent schema pulls,"There's a race condition around pulling schema changes, that can occur in case the schema changes push/propagation mechanism is not immediately effective (e.g. because of network delay, or because of the pulling node being down, etc.).

If schema changes happen on node 1, these changes do not reach node 2 immediately through the SCHEMA.PUSH mechanism, and are first recognized during gossiping, the corresponding SCHEMA.PULL request from node 2 can catch the node 1 schema in the middle of it being modified by another schema change request. This can easily lead to problems (e.g. if a new table is being added, and the node 2 request reads the changes that need to be applied to  system_schema.tables, but not the ones that need to be applied to system_schema.columns).

This PR addresses that by synchronizing the SCHEMA.PULL ""RPC call"" executed in node 1 by a request from node 2 with the method for applying schema changes in node 1.",N/A,"3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16854,Exclude Jackson 1.x dependency that leaks via old hadoop-core dependency,"build.xml has a dependency for an old hadoop-core version (1.0.3). This is likely needed for some Hadoop compatibility code under `src/java/org/apache/cassandra/hadoop`. Since 1.0.3 was released in 2012, its dependencies are very old; in particular it depends on ""jackson-mapper-asl"" 1.0.1 (from 2009!).
An earlier issue CASSANDRA-15867 referenced this dependency as well (but did not actually remove it for some reason, which marked as resolved).

Although `hadoop-core` dependency is marked as ""provided"" (and should then not be included in distributed version) it seems best to avoid downloading it during build to ""build/lib/jars"". This can be done by adding 2 exclusions for ""hadoop-core"" and ""hadoop-minicluster"" dependencies in build.xml.
I will provide a patch.

I also tried updating ""hadoop-core"" to the latest public version (1.2.1), which would have upgraded jackson-mapper-asl to 1.8.8, but that breaks the build due to some other version incompatibility (asm, possibly). If anyone wants to tackle that issue it could be a good follow-up task; ""hadoop-core"" itself has been moved to ""hadoop-client"" it seems (and there's ""hadoop-commons"" too... confusing).

 ",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16853,CircleCI folder in dtest is confusing,"I made a patch to dtest without affecting Cassandra. cassandra-dtest repo contains folder with CircleCI configuration file. So I tried to run a [build from CircleCI|https://app.circleci.com/pipelines/github/k-rus/cassandra-dtest/3/workflows/6ea8058d-37b0-40e8-89be-d9b4afdf279c/jobs/4] and it failed with an error related to configuration:
{code:java}
ERROR: flake8 3.9.2 has requirement pycodestyle<2.8.0,>=2.7.0, but you'll have pycodestyle 2.3.1 which is incompatible.
{code}
The CircleCI config contains
{code:java}
sudo pip install pycodestyle==2.3.1 flake8
{code}
, so it should lead to the error.

From the discussion in community I learned that dtest are run as part of Cassandra build and not on cassandra-dtest directly. Is the config in dtest maintained? If not, it might be good to remove. Or may be add some comment about it.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16852,testsome fails in the CircleCI multiplexer because the class JStackJUnitTask can't be found,"As [~adelapena] explained on [CASSANDRA-16827|https://issues.apache.org/jira/browse/CASSANDRA-16827?focusedCommentId=17398060&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17398060], the problem might be in the flag _-Dno-build-test=true_ that the multiplexer uses to not run _build-test_ in every iteration, relying on that this has already been  done by the build task. This is a performance optimisation that can be especially effective in fast tests.
It would be ideal to have the initial multiplexer behavior restored, but we can always run _ant build-test_ at the beginning of every multiplexer runner while still not running it on every iteration. This seems enough to keep the multiplexer working but it requires some extra builds and therefore higher costs.

The goal of this ticket is to find the root of this failure and fix the multiplexer either by restoring its initial flow or adding _ant build-test_ at the beginning of every multiplexer runner, if there is no more effective solution. ",N/A,"3.0.26, 3.11.12"
CASSANDRA-16851,Update from Jackson 2.9 to 2.12,"Given that Jackson 2.9 support has ended, it would be good to move at least to the next minor version (2.10, patch 2.10.5) or later – latest stable being 2.12.4.
 I can test to see if anything breaks, but looking at existing Jackson usage there shouldn't be many issues.

Assuming upgrade is acceptable there's the question of which branches to apply it to; I will first test it against 4.0.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16847,nodetool assassinate of a node that failed bootstrap may lead to IndexOutOfBoundsException,"User [Yorick|https://the-asf.slack.com/team/U026YMYS2QN] reported on ASF Slack that a node which failed to bootstrap is stuck in {{hibernate}} status and persists in {{gossipinfo}}:
{noformat}
/10.x.x.108
  generation:1625493756
  heartbeat:86
  STATUS:2:hibernate,true
  LOAD:79:96124.0
  SCHEMA:14:59adb24e-f3cd-3e02-97f0-5b395827453f
  DC:10:DC1
  RACK:12:RAC3
  RELEASE_VERSION:6:3.11.10
  INTERNAL_IP:8:10.x.x.108
  RPC_ADDRESS:5:10.x.x.108
  NET_VERSION:3:11
  HOST_ID:4:5b254d51-fc58-4ca2-856f-fe7878752131
  TOKENS:1:<hidden> {noformat}
Attempts to assassinate the node returns {{IndexOutOfBoundsException}}:
{noformat}
ERROR [GossipStage:1] 2021-08-11 09:10:03,440 CassandraDaemon.java:244 - Exception in thread Thread[GossipStage:1,5,main]
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
        at java.util.ArrayList.rangeCheck(ArrayList.java:659) ~[na:1.8.0_292]
        at java.util.ArrayList.get(ArrayList.java:435) ~[na:1.8.0_292]
        at com.google.common.collect.Iterables.get(Iterables.java:728) ~[guava-18.0.jar:na]
        at org.apache.cassandra.gms.VersionedValue$VersionedValueFactory.makeTokenString(VersionedValue.java:156) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.gms.VersionedValue$VersionedValueFactory.left(VersionedValue.java:178) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.gms.Gossiper.lambda$assassinateEndpoint$1(Gossiper.java:695) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_292]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.10.jar:3.11.10]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_292] {noformat}",N/A,"3.11.12, 4.0.2"
CASSANDRA-16841,Unexpectedly ignored dtests,"An issue, which I was hit:

When one class in a dtest file is marked as resource intensive, then all tests in all classes are treated as resource intensive. For example, [repair_tests/repair_test.py|https://github.com/apache/cassandra-dtest/blob/trunk/repair_tests/repair_test.py] contains three classes and the last class is marked as resource intensive:
{code:java}
@pytest.mark.resource_intensive
class TestRepairDataSystemTable(Tester):
{code}
So if I try to run an unmarked class: 
{code:java}
pytest --cassandra-dir=../cassandra repair_tests/repair_test.py::TestRepair --collect-only --skip-resource-intensive-tests
{code}
then all tests are ignored
{code:java}
collected 36 items / 36 deselected 
{code}
This is because a test is treated to be marked if any class in the same file has the mark. This bug was introduced in the fix of CASS-16399. Before only upgrade tests had such behaviour, i.e., if a class is marked as upgrade test, then all tests are upgrade test in the file.

 

This bug, for example, means that if the same file contains one class marked with vnodes and another class with no_vnodes, then no tests will be executed in the file.

I also noticed another issue that If a test run is executed with the argument {{-only-resource-intensive-tests}} and there is no sufficient resources for resource intensive tests, then no tests were executed. Thus it was necessary to provide {{-force-resource-intensive-tests}} in addition.

Suggestions for the solutions:
 # Require to mark each class and remove the special case of upgrade tests. This will simplify the implementation and might be more obvious for new comers.
 # Treat {{-only-resource-intensive-tests}} in the same way as {{-force-resource-intensive-tests}}, so it will be enough to just specify it even with no sufficient resources.

*Update:* comments were provided to keep only the first suggestion and do not implement the second suggestion. 

 ",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16839,Truncation snapshots unnecessarily created on node startup,"When testing cassandra 4.0 on ccm I noticed that everytime I restart a node, truncation snapshots are created for the tables {{system.table_estimates}} and {{system.size_estimates}}:

{noformat}
$ ccm create -n 1 test -s
$ ccm node1 stop
$ ccm node1 start
$ ccm node1 stop
$ ccm node1 start
$ ccm node1 nodetool listsnapshots

Snapshot Details:
Snapshot name                           Keyspace name Column family name True size Size on disk
truncated-1628599001857-table_estimates system        table_estimates    0 bytes   13 bytes
truncated-1628599099560-table_estimates system        table_estimates    0 bytes   13 bytes
truncated-1628599001736-size_estimates  system        size_estimates     0 bytes   13 bytes
truncated-1628599057438-table_estimates system        table_estimates    6.16 KiB  6.19 KiB
truncated-1628599099458-size_estimates  system        size_estimates     0 bytes   13 bytes
truncated-1628599057340-size_estimates  system        size_estimates     5.73 KiB  5.76 KiB

Total TrueDiskSpaceUsed: 0 bytes
{noformat}

Not sure if this is expected behavior, but feels like a bug to me.

Reproduced on 4.0, not sure if it reproduces on lower versions.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16836,Materialized views incorrect quoting of UDF,"Creating a MV with a UDF needing quotes will explode on inserts after restart

 
{code:sql}
create keyspace test WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 3};

use test;

CREATE TABLE t (k int PRIMARY KEY, v int);

CREATE FUNCTION ""Double"" (input int) 

   CALLED ON NULL INPUT 

   RETURNS int 

   LANGUAGE java 

   AS 'return input*2;';

CREATE MATERIALIZED VIEW mv AS SELECT * FROM t 

   WHERE k < test.""Double""(2) 

   AND k IS NOT NULL 

   AND v IS NOT NULL 

   PRIMARY KEY (v, k);
 {code}

Now restart the node, run an insert and you get an error

{noformat}
INSERT INTO t(k, v) VALUES (3, 1);
ERROR [MutationStage-2] 2021-08-10 09:55:56,662 StorageProxy.java:1551 - Failed to apply mutation locally : 
org.apache.cassandra.exceptions.InvalidRequestException: Unknown function test.double called
	at org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest(RequestValidations.java:217)
	at org.apache.cassandra.cql3.functions.FunctionCall$Raw.prepare(FunctionCall.java:155)
	at org.apache.cassandra.cql3.SingleColumnRelation.toTerm(SingleColumnRelation.java:123)
	at org.apache.cassandra.cql3.SingleColumnRelation.newSliceRestriction(SingleColumnRelation.java:231)
	at org.apache.cassandra.cql3.Relation.toRestriction(Relation.java:144)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:188)
	at org.apache.cassandra.cql3.restrictions.StatementRestrictions.<init>(StatementRestrictions.java:135)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepareRestrictions(SelectStatement.java:1067)
	at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:937)
	at org.apache.cassandra.db.view.View.getSelectStatement(View.java:180)
	at org.apache.cassandra.db.view.View.getReadQuery(View.java:204)
	at org.apache.cassandra.db.view.TableViews.updatedViews(TableViews.java:368)
	at org.apache.cassandra.db.view.ViewManager.updatesAffectView(ViewManager.java:85)
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:538)
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:513)
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:220)
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:229)
	at org.apache.cassandra.service.StorageProxy$4.runMayThrow(StorageProxy.java:1545)
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2324)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
WriteFailure: Error from server: code=1500 [Replica(s) failed to execute write] message=""Operation failed - received 0 responses and 1 failures: UNKNOWN from localhost/127.0.0.1:7000"" info={'consistency': 'ONE', 'required_responses': 1, 'received_responses': 0, 'failures': 1, 'error_code_map': {'127.0.0.1': '0x0000'}}

{noformat}",N/A,"3.11.12, 4.1-alpha1, 4.1"
CASSANDRA-16835,"Scrub still uses ""row"" to mean ""partitions"", and has broken code","2 issues in the scrub code:
 1) It still uses ""row"" to mean ""partition"". And not only in the code, but also in user messages. As we've fairly systematically remove such instances elsewhere in 3.0+, having it in scrub is going to confuse users which will almost surely misinterpret the results. If scrub says that it dropped 2 unreadable ""rows"" from your sstable, you might be ok with that when we're actually talking about CQL rows, but not if we talk of 2 full partitions.
 2) There is a branch at the end of scrub that is supposed to handle the case where scrubbing a sstable generates no output at all (the sstable is completely hosed usually), mostly providing a more user friendly message. The code is broken (and has been for a long time, since CASSANDRA-7066 I believe) however such that this branch can simply never be taken (even when it should). While admittedly pretty minor, no reason to leave it that way.",N/A,"3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16833,Deploy multi-architecture (amd+arm) docker testing images,"Docker's [buildx|https://github.com/docker/buildx/] supports building multi-architecture images.

This can be used to deploy our testing images: [apache/cassandra-testing-ubuntu2004-java11|https://hub.docker.com/layers/apache/cassandra-testing-ubuntu2004-java11] and [apache/cassandra-testing-ubuntu2004-java11-w-dependencies|https://hub.docker.com/layers/apache/cassandra-testing-ubuntu2004-java11-w-dependencies]; for both {{linux/amd64}} and {{linux/arm64}} architectures. 

(Up until now ci-cassandra.a.o is having to manually local build these images during each test run.)",N/A,"2.2.20, 3.0.26, 3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16827,"""java: package org.apache.tools.ant does not exist"" error ","After CASSANDRA-16557 I am getting ""java: package org.apache.tools.ant does not exist"" error when I try to run unit tests in IntelliJ.

 ",N/A,3.11.12
CASSANDRA-16825,Make client warnings thread safe and sent only once,,N/A,"3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16822,Wrong cqlsh python library location in cassandra-3.11.11-1 rhel packages ,"cqlsh does not work because cqlshlib is in wrong location while I think it should be in python2.7 for cassandra-3.11

cassandra.spec seems to define python interpreter to /usr/bin/python so I think build environment has been changed after 3.11.10 so /usr/bin/python points to python3 instead of python2.

cassandra-3.11.10 did have chqlshlib in python2.7 site-packages
{noformat}
$ rpm -qpl cassandra-3.11.11-1.noarch.rpm |grep cql
warning: cassandra-3.11.11-1.noarch.rpm: Header V4 RSA/SHA512 Signature, key ID 0b84c041: NOKEY
/etc/cassandra/default.conf/cqlshrc.sample
/usr/bin/cqlsh
/usr/bin/cqlsh.py
/usr/bin/debug-cql
/usr/lib/python3.6/site-packages/cqlshlib
/usr/lib/python3.6/site-packages/cqlshlib/__init__.py
/usr/lib/python3.6/site-packages/cqlshlib/copyutil.py
/usr/lib/python3.6/site-packages/cqlshlib/cql3handling.py
/usr/lib/python3.6/site-packages/cqlshlib/cqlhandling.py
/usr/lib/python3.6/site-packages/cqlshlib/cqlshhandling.py
/usr/lib/python3.6/site-packages/cqlshlib/displaying.py
/usr/lib/python3.6/site-packages/cqlshlib/formatting.py
/usr/lib/python3.6/site-packages/cqlshlib/helptopics.py
/usr/lib/python3.6/site-packages/cqlshlib/pylexotron.py
/usr/lib/python3.6/site-packages/cqlshlib/saferscanner.py
/usr/lib/python3.6/site-packages/cqlshlib/sslhandling.py
/usr/lib/python3.6/site-packages/cqlshlib/tracing.py
/usr/lib/python3.6/site-packages/cqlshlib/util.py
/usr/lib/python3.6/site-packages/cqlshlib/wcwidth.py
{noformat}
 

Pull request to cassandra-3.11: https://github.com/apache/cassandra/pull/1124",N/A,"2.2.20, 3.0.26, 3.11.12"
CASSANDRA-16817,Fix ERROR message which prints data information in the logs,"{{StorageProxy.mutateMV}} might log [an error message|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageProxy.java#L880] that prints user data in the logs beyond the row key, for example:
{code}
ERROR [MutationStage-2] 2021-07-28 13:08:52,609 StorageProxy.java:1002 - Error applying local view update to keyspace k: Mutation(keyspace='k', key='00000001', modifications=[
  [k.mv] key=1 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
    Row[info=[ts=1627474132606719] ]: k=0, v=MY CONFIDENTIAL DATA |
])
{code}
We should probably change that log message so it doesn't print the entire mutation but only the keyspace, tables and partition key of the mutation.",N/A,"3.0.26, 3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16809,Dockerise cqlshlib tests,"The cqlshlib tests are the last remaining tests that don't run in docker.

Before 4.0 the cqlshlib builds are [hardcoded|https://github.com/apache/cassandra/blob/cassandra-3.11/pylib/cassandra-cqlsh-tests.sh#L60] to python2, but the test docker image [doesn't support python2|https://github.com/apache/cassandra-builds/blob/trunk/docker/testing/ubuntu2004_j11.docker#L99-L117].

(Maybe this is why cqlshlib tests don't exist in pre 4.0 circleci?)

Dockerising the cqlshlib tests, makes it a lot easier to keep jenkins' agents clean of zombie processes. 

Patches: 
- cassandra-builds: https://github.com/apache/cassandra-builds/pull/50
- 3.11: https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/dockerise-cqlsh-tests/3.11
- 3.0: https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/dockerise-cqlsh-tests/3.0

Patches are not required for 4.0 and trunk, as the {{--no-site-packages}} flag to virtualenv has already been removed from them.",N/A,"3.0.26, 3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16804,Create config.yml.MIDRES for 3.0 and 3.11,"[config.yml.MIDRES|https://github.com/apache/cassandra/blob/trunk/.circleci/config.yml.MIDRES] was created for 4.0 some time ago but not for 3.0 and 3.11.

This task is to facilitate that backport",N/A,"3.0.26, 3.11.11"
CASSANDRA-16803,"jvm-dtest-upgrade failing MixedModeReadTest.mixedModeReadColumnSubsetDigestCheck, ClassNotFoundException: com.vdurmont.semver4j.Semver","Caused by CASSANDRA-16649

Reproducible locally. Oddly enough can be reproduced on cassandra-4.0 branch as well, though CI is not failing.

fyi [~ifesdjeen]",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16796,Clear pending ranges for a SHUTDOWN peer,"If a node involved in a MOVE operation should fail, peers can sometimes maintain pending ranges for it even when it has left the ring and/or been replaced (in practice until the peer is next bounced). This in turn can lead to bogus unavailable responses to clients if a replica for the any of the pending ranges should go down.

If the moving node crashes hard, a subsequent replacement will correctly fail as long as cassandra.consistent.rangemovement is set to true because the new node will learn the MOVING status from the remaining peers. A graceful shutdown, however, causes that status to be replaced with SHUTDOWN, but doesn't update TokenMetadata, so pending ranges remain for the down node, even after it has been removed from the ring.",N/A,"3.0.25, 3.11.11, 4.0.1"
CASSANDRA-16770,Avoid sending cdc column in serialization header to 3.0 nodes,"We try to not send the cdc column to any 3.0 nodes, but it is still there in the SerializationHeader",N/A,3.11.12
CASSANDRA-16763,Create Cassandra documentation content for new website,"We need create the content (asciidoc) to render the Cassandra documentation using Antora. This work can commence once the following has happened:
 * Website and documentation proof of concept is done - CASSANDRA-16029
 * Website design and concept is done - CASSANDRA-16115
 * Website and document tooling is done - CASSANDRA-16066 
 * Website UI components are done - CASSANDRA-16762",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16762,Create content and UI components for new website,"We need create the content (asciidoc) and UI components to render the website using Antora. This work can commence once the following has happened:
 * Website and documentation proof of concept is done - CASSANDRA-16029
 * Website design and concept is done - CASSANDRA-16115
 * Website and document tooling is done - CASSANDRA-16066 

 ",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16757,Fix org.apache.cassandra.distributed.upgrade.CompactStorage2to3UpgradeTest,"{color:#172b4d}[https://jenkins-cm4.apache.org/job/Cassandra-3.0/153/testReport/junit/org.apache.cassandra.distributed.upgrade/CompactStorage2to3UpgradeTest/testDropCompactWithClusteringAndValueColumn/]{color}
{code:java}
Error Message
org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses.

Stacktrace
java.lang.RuntimeException: org.apache.cassandra.exceptions.ReadTimeoutException: Operation timed out - received only 0 responses. at org.apache.cassandra.distributed.impl.IsolatedExecutor.waitOn(IsolatedExecutor.java:209) at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$sync$5(IsolatedExecutor.java:109) at org.apache.cassandra.distributed.impl.Coordinator.executeWithResult(Coordinator.java:69) at org.apache.cassandra.distributed.api.ICoordinator.execute(ICoordinator.java:32) at org.apache.cassandra.distributed.upgrade.CompactStorage2to3UpgradeTest$ResultsRecorder.validateResults(CompactStorage2to3UpgradeTest.java:356) at
{code}
[https://jenkins-cm4.apache.org/job/Cassandra-3.0/153/testReport/junit/org.apache.cassandra.distributed.upgrade/CompactStorage2to3UpgradeTest/singleColumn/] 
{code:java}
Error Message
Metaspace

Stacktrace
java.lang.OutOfMemoryError: Metaspace at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:756) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:468) at java.net.URLClassLoader.access$100(URLClassLoader.java:74) at java.net.URLClassLoader$1.run(URLClassLoader.java:369) at java.net.URLClassLoader$1.run(URLClassLoader.java:363) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:362) at org.apache.cassandra.distributed.shared.InstanceClassLoader.loadClassInternal(InstanceClassLoader.java:101) at org.apache.cassandra.distributed.shared.InstanceClassLoader.loadClass(InstanceClassLoader.java:87) at org.codehaus.jackson.map.deser.StdDeserializerProvider.<init>(StdDeserializerProvider.java:89) at org.codehaus.jackson.map.ObjectMapper.<init>(ObjectMapper.java:391) at org.codehaus.jackson.map.ObjectMapper.<init>(ObjectMapper.java:358) at org.codehaus.jackson.map.ObjectMapper.<init>(ObjectMapper.java:338) at org.apache.cassandra.utils.FBUtilities.<clinit>(FBUtilities.java:74) at org.apache.cassandra.distributed.impl.Instance.<init>(Instance.java:144) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper$$Lambda$2095/2058566824.apply(Unknown Source) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.newInstance(AbstractCluster.java:182) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegateForStartup(AbstractCluster.java:163) at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:200) at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:189) at org.apache.cassandra.distributed.upgrade.CompactStorage2to3UpgradeTest.singleColumn(CompactStorage2to3UpgradeTest.java:109)
{code}
 

 ",N/A,3.0.25
CASSANDRA-16754,Flaky o.a.c.distributed.test.SchemaTest,"The JVM dtest {{org.apache.cassandra.distributed.test.SchemaTest}} is flaky:
 [https://ci-cassandra.apache.org/job/Cassandra-4.0.0/34/testReport/junit/org.apache.cassandra.distributed.test/SchemaTest/readRepairWithCompaction_2/]
{code:java}
Error Message
FSWriteError in /home/cassandra/cassandra/tmp/dtests5018452609443646984/node2/commitlog/CommitLog-7-1624286958980.log
Stacktrace
java.lang.RuntimeException: FSWriteError in /home/cassandra/cassandra/tmp/dtests5018452609443646984/node2/commitlog/CommitLog-7-1624286958980.log
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$10(Instance.java:582)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: FSWriteError in /home/cassandra/cassandra/tmp/dtests5018452609443646984/node2/commitlog/CommitLog-7-1624286958980.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:256)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:273)
	at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.handleReplayedSegment(AbstractCommitLogSegmentManager.java:349)
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:178)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$10(Instance.java:508)
Caused by: java.nio.file.NoSuchFileException: /home/cassandra/cassandra/tmp/dtests5018452609443646984/node2/commitlog/CommitLog-7-1624286958980.log
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116)
	at java.base/sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:249)
	at java.base/sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:105)
	at java.base/java.nio.file.Files.delete(Files.java:1142)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:250)
{code}
Although it doesn't fail frequently on CI it's quite easy to reproduce it locally.

I think the failure is caused by the two tests on the class not waiting for the future returned by {{IInstance#shutdown()}}.",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16747,Block/Warn creating keyspace with RF > Cassandra node count," 

Using Cassandra 3.11.10, 3 nodes cluster Cassandra allows to create a keyspace having `replication_factor` greater than the number of available nodes.

 
{code:java}
-- successfully created Keyspace with RF = 3 > node count = 3

CREATE KEYSPACE mykeyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 5 };
{code}
From [this discussion|https://community.datastax.com/questions/11623/create-keyspace-with-rf-physical-cassandra-node-co.html] on community.datastax.com. It appears that this would lead to serious operational problems.

Would it be OK to improve the `CREATE KEYSPACE` to block (or display warning) when attempting to set `replication_factor` > node count?

 ",N/A,3.11.13
CASSANDRA-16745,Fix test failures when running via IntelliJ,"After running `ant generate-idea-files` and opening the project in IntelliJ IDEA 2021.1.2, tests that referenced PropertyFileSnitch (like NetworkTopologyStrategyTest.testProperties) failed with error org.apache.cassandra.exceptions.ConfigurationException: Unable to read cassandra-topology.properties.

 

I fixed this issue in IntelliJ by marking test/conf as Test Resources (under Project Structure), and have patches to fix `ant generate-idea-files` so the manual reconfiguration is not necessary:

 

3.0 - [https://github.com/aratno/cassandra/tree/abe-fix-intellij-test-3-0]",N/A,3.0.25
CASSANDRA-16741,Remediate Cassandra 3.11.10 JAR dependency vulnerability - com.google.guava_guava,"A JAR dependency is flagged in Cassandra 3.11.10 as having vulnerabilities that have been fixed in newer releases. The following is the Cassandra 3.11.10 source tree for their JAR dependencies: [https://github.com/apache/cassandra/tree/181a4969290f1c756089b2993a638fe403bc1314/lib] . 

JAR *com.google.guava_guava* version *18.0* has the following vulnerability and is fixed in version *30.0*. Recommendation is to upgrade to version *30.1.1-jre* or greater.

 
||id||cvss||desc||link||packageName||packageVersion||severity||status||vecStr||
|CVE-2018-10237|5.9|Unbounded memory allocation in Google Guava 11.0 through 24.x before 24.1.1 allows remote attackers to conduct denial of service attacks against servers that depend on this library and deserialize attacker-provided data, because the AtomicDoubleArray class (when serialized with Java serialization) and the CompoundOrdering class (when serialized with GWT serialization) perform eager allocation without appropriate checks on what a client has sent and whether the data size is reasonable.|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2018-10237|com.google.guava_guava|18.0|medium|fixed in 24.1.1|CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:N/I:N/A:H|
|CVE-2020-8908|3.3|A temp directory creation vulnerability exists in all versions of Guava, allowing an attacker with access to the machine to potentially access data in a temporary directory created by the Guava API com.google.common.io.Files.createTempDir(). By default, on unix-like systems, the created directory is world-readable (readable by an attacker with access to the system). The method in question has been marked @Deprecated in versions 30.0 and later and should not be used. For Android developers, we recommend choosing a temporary directory API provided by Android, such as context.getCacheDir(). For other Java developers, we recommend migrating to the Java 7 API java.nio.file.Files.createTempDirectory() which explicitly configures permissions of 700, or configuring the Java runtime\'s java.io.tmpdir system property to point to a location whose permissions are appropriately configured.|https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2020-8908|com.google.guava_guava|18.0|low|fixed in 30.0|CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:L/I:N/A:N|

A possible fix strategy is to simply update the JAR to their newest version.
 * See [https://mvnrepository.com/artifact/com.google.guava/guava/30.1.1-jre]",N/A,3.11.x
CASSANDRA-16737,ALTER ... ADD can increase the number of SSTables being read,"With the following SSTables:
{code:java}
CREATE TABLE my_table (pk int, ck int, v1 int, PRIMARY KEY(pk, ck))

INSERT INTO my_table (pk, ck, v1) VALUES (1, 1, 1) USING TIMESTAMP 1000;
--> flush()
INSERT INTO my_table (pk, ck, v1) VALUES (1, 1, 2) USING TIMESTAMP 2000;
--> flush()
INSERT INTO my_table  (pk, ck, v1) VALUES (1, 1, 3) USING TIMESTAMP 3000
--> flush()
{code}
the following query:
{code:java}
SELECT pk, ck, v1 FROM my_table WHERE pk = 1 AND ck = 1{code}
will only read the third SSTable.

If we add a column to the table (e.g. {{ALTER TABLE my_table ADD v2 int}}) and rerun the query, the query will read the 3 SSTables.

The reason for this behavior is due to the fact that C* is trying to read all the {{fetched}} columns to ensure that it will return a row if at least one of its column is non null.

In practice for CQL tables, C* does not need to fetch all columns if the row contains a primary key liveness as it is enough to guaranty that the row exists. By consequence, even after the addition of the new column C* should read only the third SSTable.",N/A,"3.11.11, 4.0.1"
CASSANDRA-16735,Adding columns via ALTER TABLE can generate corrupt sstables,"This is similar to CASSANDRA-13004 and was caused by CASSANDRA-15899

Basically the column placeholders introduced in 15899 can get read-repaired in to the memtable and later flushed to disk and in some cases this can conflict with the actual column (if the actual column is a collection for example) and cause CorruptSSTableExceptions.

Fix is probably to just revert 15899, at least until if and when we find a solution that we can rely on. Will post that + test next week.",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16734,Remediate Cassandra 3.11.10 JAR dependency vulnerabilities ,"Several JAR dependencies are flagged in Cassandra 3.11.10 as having vulnerabilities that have been fixed in newer releases. 
 The following is the Cassandra 3.11.10 source tree for their JAR dependencies: [https://github.com/apache/cassandra/tree/181a4969290f1c756089b2993a638fe403bc1314/lib]

A possible fix strategy is to simply update the JARs to their newest version. See the JAR files available for each vulnerable library:
 * See [https://mvnrepository.com/artifact/com.fasterxml.jackson.core/jackson-databind/2.9.10.8]
 * See [https://mvnrepository.com/artifact/io.netty/netty-all/4.1.65.Final]
 * See [https://mvnrepository.com/artifact/org.apache.thrift/libthrift/0.9.3-1]
 * See [https://mvnrepository.com/artifact/com.thinkaurelius.thrift/thrift-server/0.3.9]
 * See [https://mvnrepository.com/artifact/com.google.guava/guava/30.1.1-jre]
 * See [https://mvnrepository.com/artifact/ch.qos.logback/logback-core/1.2.3]
 * See [https://mvnrepository.com/artifact/org.yaml/snakeyaml/1.29]
 * See [https://mvnrepository.com/artifact/commons-codec/commons-codec/1.15]",N/A,3.11.11
CASSANDRA-16733,Allow operators to disable 'ALTER ... DROP COMPACT STORAGE' statements,"{{ALTER ... DROP COMPACT STORAGE}} statements have not been extensively tested and suffer from several issues like:

* As COMPACT tables did not have primary key liveness there empty rows
inserted AFTER the ALTER will be returned whereas the one inserted before
the ALTER will not.
* Also due to the lack of primary key liveness the amount of SSTables being
read will increase resulting in slower queries (CASSANDRA-16675)
* After DROP COMPACT it becomes possible to ALTER the table in a way that
makes all the row disappears
* There is a loss of functionality around null clustering when dropping
compact storage (CASSANDRA-16069)

To avoid running into those issues this ticket will introduce a new flag that allow operators to disable those statements on their clusters.

see https://www.mail-archive.com/dev@cassandra.apache.org/msg16789.html",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16732,Unable to replace node if cluster is in mixed major version,"This should be independent of cluster size, but in my example repro, I have a 6 node cluster where 5 nodes are on 3.0, one node is on 2.1, and replacing any of the 3.0 nodes (such that new node bootstraps from neighbors) fails during the bootstrap phase of the new node with the below exception.

This version of C* includes fix for CASSANDRA-16692.

{code:java}
ERROR [main] 2021-06-11 07:47:36,500 CassandraDaemon.java:822 - Exception encountered during startup
java.lang.RuntimeException: Didn't receive schemas for all known versions within the timeout. Use -Dcassandra.skip_schema_check=true to skip this check.
	at org.apache.cassandra.service.StorageService.waitForSchema(StorageService.java:909) ~[nf-cassandra-3.0.25.1.jar:3.0.25.1]
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:960) ~[nf-cassandra-3.0.25.1.jar:3.0.25.1]
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:751) ~[nf-cassandra-3.0.25.1.jar:3.0.25.1]
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:659) ~[nf-cassandra-3.0.25.1.jar:3.0.25.1]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:374) [nf-cassandra-3.0.25.1.jar:3.0.25.1]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:616) [nf-cassandra-3.0.25.1.jar:3.0.25.1]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:809) [nf-cassandra-3.0.25.1.jar:3.0.25.1]
{code}
",N/A,3.0.25
CASSANDRA-16727,LICENSE Text addition to Layout.html breaks Cassandra Website rendering,"The newest docs contain a license agreement comment found in the layout.html of the theme.  This occurs before front matter indicator...
 [https://github.com/apache/cassandra/blob/cassandra-3.11/doc/source/_theme/cassandra_theme/layout.html]

Jekyll assumes that the front matter while be at the top ""—"".  This causes Jekyll to break when serving the page on the website.

There are a few other docs that seem to have this problem, and serving the latest docs of the website breaks the rendering.",N/A,"3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16712,DROP COMPACT STORAGE does not invalidate prepared statements as it should ,"{{DROP COMPACT STORAGE}} might change the number of columns of a table but as the prepared statement cache is not invalidate the query will not return the correct set of columns until the statement as been invalidate. That can lead to a surprising behavior where the resultset change at un expected point in time.

This problem only affect 3.0 and 3.11",N/A,"3.0.25, 3.11.11"
CASSANDRA-16703,Exception thrown by custom QueryHandler constructor is ignored,"When a exception is thrown during the instantiation of the _cassandra.custom_query_handler_class,_ depending on the exception thrown cassandra will simply log an info message and proceed with the bootstraping with the standard _QueryHandler_ as a fallback measure: [https://github.com/apache/cassandra/blob/cassandra-3.11.10/src/java/org/apache/cassandra/service/ClientState.java#L107|https://github.com/apache/cassandra/blob/3b553d8e13dbdbe59119de9c917d9aacc440741e/src/java/org/apache/cassandra/service/ClientState.java#L104]

The end-user will never know if the custom _QueryHandler_ is actually registered or not, unless he notices the info message on the logs.

Ideally, the message should be logged as error and JVM should stop as it cannot proceed according with the user expected configuration.

*Scenario*:

In our scenario, we have a custom _QueryHandler_ that receives specific configuration, and we throw a _ConfigurationException_ at instantiation time in case of any invalid config value. It is expected that cassandra stop the bootstraping instead of skipping the QH.",N/A,"3.0.25, 3.11.11, 4.0.1"
CASSANDRA-16702,Latest 3.x Cassandra (3.11.10) fails to start after updating corretto jdk from 8.252 to 8.292,"Cassandra fails to start with the following exception
{code:java}
[2021-05-22 03:19:04,790] [Apache Cassandra Error] java.lang.UnsatisfiedLinkError: C:\Users\alexey.barsov\AppData\Local\Temp\jna--1630081341\jna2627169091367567840.dll: Can't find dependent libraries
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at java.lang.ClassLoader$NativeLibrary.load(Native Method)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1934)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1817)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at java.lang.Runtime.load0(Runtime.java:810)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at java.lang.System.load(System.java:1088)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:851)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:826)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at com.sun.jna.Native.<clinit>(Native.java:140)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at org.apache.cassandra.utils.WindowsTimer.<clinit>(WindowsTimer.java:35)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:630)
 [2021-05-22 03:19:04,790] [Apache Cassandra Error] at com.jetbrains.cassandra.service.CassandraServiceMain.start(CassandraServiceMain.java:92)
 [2021-05-22 03:19:04,791] [Apache Cassandra Error] at com.jetbrains.launcher.AppProxy$6$1.call(AppProxy.java:99)
 [2021-05-22 03:19:04,791] [Apache Cassandra Error] at com.jetbrains.launcher.AppProxy$6$1.call(AppProxy.java:97)
 [2021-05-22 03:19:04,791] [Apache Cassandra Error] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 [2021-05-22 03:19:04,791] [Apache Cassandra Error] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 [2021-05-22 03:19:04,791] [Apache Cassandra Error] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 [2021-05-22 03:19:04,791] [Apache Cassandra Error] at java.lang.Thread.run(Thread.java:748){code}
To Reproduce
 Run Cassandra 3.11.0 on corretto jdk 8.292

Platform information
 OS: Windows 10
 Jdk Version: OpenJDK 64-Bit Server VM Corretto-8.292.10.1 (build 25.292-b10, mixed mode)

 

Note: related issue in corretto jdk 8 issue tracker: https://github.com/corretto/corretto-8/issues/308",N/A,"3.0.26, 3.11.11, 4.0.1"
CASSANDRA-16697,generate-idea-files taks unnecessarily builds the project,"This is especially painful when the project cannot be built because we are in the middle of solving some merge conflicts, but we want to fetch and update dependencies. It seems be to be unnecessary to build the project or to do other checks when we just want to load the project into our IDE.

From what I've tested, it is enough to limit dependencies of that task to:
{{init,maven-ant-tasks-init,resolver-dist-lib,gen-cql3-grammar,generate-jflex-java,createVersionPropFile}}
",N/A,"3.0.25, 3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16695,cqlsh should prefer newer TLS version by default,"Some new JDK releases started to disable TLSv1.0 and TLSv1.1.

[https://www.oracle.com/java/technologies/javase/8u291-relnotes.html]

 

However, the code in:

[https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/sslhandling.py#L56-L65]

is defaulting to those rather old versions,

which could lead to the following problem:
{code:java}
('Unable to connect to any servers', {'10.101.34.89:9042': error(1, u""Tried connecting to [('10.101.34.89', 9042)]. Last error: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:618)"")}) {code}
 

Python2 default TLS protocol

[https://docs.python.org/2/library/ssl.html#ssl.PROTOCOL_TLS]

Python3 default TLS protocol

[https://docs.python.org/3/library/ssl.html#ssl.PROTOCOL_TLS]

 

 ",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16692,Unable to replace node with stale schema," 

After CASSANDRA-15158 operators are no longer permitted to replace a terminated node with a stale schema. That is, launching a node to replace NodeA in the following scenario:
{code:java}
NodeA (terminated): schema=V0
All others (alive): schema=V1{code}
yields:
{code:java}
ERROR [main] 2021-04-30 19:10:23,410 CassandraDaemon.java:822 - Exception encountered during startup
java.lang.RuntimeException: Didn't receive schemas for all known versions within the timeout
        at org.apache.cassandra.service.StorageService.waitForSchema(StorageService.java:887) ~[nf-cassandra-3.0.24.1.jar:3.0.24.1]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:937) ~[nf-cassandra-3.0.24.1.jar:3.0.24.1]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:746) ~[nf-cassandra-3.0.24.1.jar:3.0.24.1]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:654) ~[nf-cassandra-3.0.24.1.jar:3.0.24.1]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:374) [nf-cassandra-3.0.24.1.jar:3.0.24.1]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:616) [nf-cassandra-3.0.24.1.jar:3.0.24.1]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:809) [nf-cassandra-3.0.24.1.jar:3.0.24.1]{code}
This can be reproduced like so:
 # Shut down C* on one node in a 3.0.24 cluster
 # Create a new keyspace and table from one of the other nodes
 # Terminate and replace the node on which C* was shut down.

Waiting for agreement of the nodes not being replaced seems prudent as not doing so could induce data loss, the node being replaced should be exempted from this check.

Reference CASSANDRA-16577 for more context.",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16690,Flaky NativeAllocatorTest.testBookKeeping,"Flaky [here|https://ci-cassandra.apache.org/job/Cassandra-4.0/52/testReport/junit/org.apache.cassandra.utils.memory/NativeAllocatorTest/testBookKeeping_cdc/]

{noformat}
Error Message

java.lang.AssertionError: expected:<0> but was:<1>

Stacktrace

java.util.concurrent.ExecutionException: java.lang.AssertionError: expected:<0> but was:<1>
	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
	at org.apache.cassandra.utils.memory.NativeAllocatorTest.testBookKeeping(NativeAllocatorTest.java:154)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.lang.AssertionError: expected:<0> but was:<1>
	at org.apache.cassandra.utils.memory.NativeAllocatorTest.lambda$testBookKeeping$2(NativeAllocatorTest.java:131)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}
",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16688,CircleCI - python dtests failing because of ccm issue,"Circle CI fails to run dtests.

[https://app.circleci.com/pipelines/github/adelapena/cassandra/483/workflows/e9a5f84e-e465-40e6-92f4-08e8632edf47/jobs/4315]
{code:java}
Obtaining ccm from git+
[https://github.com/riptano/ccm.git@cassandra-test#egg=ccm]
 (from -r /home/cassandra/cassandra-dtest/requirements.txt (line 9))
  Updating ./env3.6/src/ccm clone (to revision cassandra-test)
  Running command git fetch -q --tags
WARNING: Discarding git+
[https://github.com/riptano/ccm.git@cassandra-test#egg=ccm]
. Command errored out with exit status 1: git fetch -q --tags Check the logs for full command output.
ERROR: Could not find a version that satisfies the requirement ccm (unavailable)
ERROR: No matching distribution found for ccm (unavailable)
WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.



{code}",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16687,Support upgrade tests in CircleCI multiplexer,The CircleCI jobs for running a test repeatedly added by CASSANDRA-16625 don't support upgrade tests. In the case of Python upgrade dtests we only need the {{--execute-upgrade-tests}} when calling {{pytest}}. For Java upgrade tests we need to build the dtest jars before running the repeated tests.,N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.1-alpha1, 4.1"
CASSANDRA-16683,"StandaloneVerifier does not fail when unable to verify SSTables, it only fails if Corruption is thrown","offline_tools_test.py::TestOfflineTools::test_sstableverify has the following check

{code}
       try:
           (out, error, rc) = node1.run_sstableverify(""keyspace1"", ""standard1"", options=['-v'])
       except ToolError as e:
           # Process sstableverify output to normalize paths in string to Python casing as above
           error = re.sub(""(?<=Corrupted: ).*"", lambda match: os.path.normcase(match.group(0)), str(e))

           assert re.search(""Corrupted: "" + sstable1, error)
           assert e.exit_status == 1, str(e.exit_status)
{code}

This checks if the corrupt log is present IFF ToolError is thrown, but does not validate that the error is actually thrown.  I tried calling the same logic before the try to validate and see that it does not fail.  If we fix the test to check for error we also see that the log that is returned to the user does not match 2.2’s behavior but instead returns different logic as digest validation throws IOException, which we do not convert to a CorruptSSTableException (which is the message the test checks for).

This also shows another big issue, that when the digest fails verify passes",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16681,org.apache.cassandra.utils.memory.LongBufferPoolTest - tests are flaky,"Jenkins history:

[https://jenkins-cm4.apache.org/job/Cassandra-4.0/50/testReport/junit/org.apache.cassandra.utils.memory/LongBufferPoolTest/testPoolAllocateWithRecyclePartially/history/]

Fails being run in a loop in CircleCI:

https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/844/workflows/945011f4-00ac-4678-89f6-5c0db0a40169/jobs/5008

 ",N/A,"3.11.14, 4.0.7, 4.1-rc1, 5.0-alpha1, 5.0"
CASSANDRA-16680,TimeWindowCompactionStrategyTest flaky,"Seen in Jenkins:

https://ci-cassandra.apache.org/job/Cassandra-devbranch/785/

Failed two times with the multiplexer 

[https://app.circleci.com/pipelines/github/adelapena/cassandra/461/workflows/7a837b82-c0d1-4e10-8932-c5908d2585de/jobs/4114]",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.1-alpha1, 4.1"
CASSANDRA-16673,Avoid race in AbstractReplicationStrategy endpoint caching,"We should make sure we track which ringVersion we are caching in AbstractReplicationStrategy to avoid a race where we might return the wrong EndpointsForRange.

{code}
Caused by: java.lang.IllegalArgumentException: 9010454139840013625 is not contained within (9223372036854775801,-4611686018427387905]
	at org.apache.cassandra.locator.EndpointsForRange.forToken(EndpointsForRange.java:59)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalReplicasForToken(AbstractReplicationStrategy.java:104)
	at org.apache.cassandra.locator.ReplicaLayout.forTokenReadLiveSorted(ReplicaLayout.java:330)
	at org.apache.cassandra.locator.ReplicaPlans.forRead(ReplicaPlans.java:594)
{code}",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16672,Retag ccm,CCM repro's trunk is several commits ahead from {{cassandra-test}} tag. Probably an oversight but retagging needs CI to be ran just to make sure nothing broke in between.,N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16671,Cassandra can return no row when the row columns have been deleted.,"It is the semantic of CQL that a (CQL) row exists as long as it has one non-null column (including the PK columns).

To determine if a row has some *non-null primary key*, Cassandra relies on the row primary key liveness. 

For example:

{code}
CREATE TABLE test (pk int, ck int, v int, PRIMARY KEY(pk, ck));
INSERT INTO test(pk, ck, v) VALUES (1, 1, 1);
DELETE v FROM test WHERE pk = 1 AND ck = 1
SELECT v FROM test;
{code}
will return
{code}
v
---
null 
{code}

{{UPDATE}} statements do not set the row primary key liveness by consequence if the user had used an {{UPDATE}} statement instead of an {{INSERT}} the {{SELECT}} query would *not have returned any rows*.

CASSANDRA-16226 introduced a regression by stopping early in the timestamp ordered logic if an {{UPDATE}} statement covering all the columns was found in an SSTable. As the row returned did not have a primary key liveness if another node was also returning a column deletion, the expected row will not be returned.

The problem can be reproduced with the following test:
{code}
   @Test
    public void testSelectWithUpdatedColumnOnOneNodeAndColumnDeletionOnTheOther() throws Throwable
    {
        try (Cluster cluster = init(builder().withNodes(2).start()))
        {
            cluster.schemaChange(withKeyspace(""CREATE TABLE %s.tbl (pk int, ck text, v int, PRIMARY KEY (pk, ck))""));
            cluster.get(1).executeInternal(withKeyspace(""INSERT INTO %s.tbl (pk, ck, v) VALUES (1, '1', 1) USING TIMESTAMP 1000""));
            cluster.get(1).flush(KEYSPACE);
            cluster.get(1).executeInternal(withKeyspace(""UPDATE %s.tbl USING TIMESTAMP 2000 SET v = 2 WHERE pk = 1 AND ck = '1'""));
            cluster.get(1).flush(KEYSPACE);

            cluster.get(2).executeInternal(withKeyspace(""DELETE v FROM %s.tbl USING TIMESTAMP 3000 WHERE pk=1 AND ck='1'""));
            cluster.get(2).flush(KEYSPACE);

            assertRows(cluster.coordinator(2).execute(withKeyspace(""SELECT * FROM %s.tbl WHERE pk=1 AND ck='1'""), ConsistencyLevel.ALL),
                       row(1, ""1"", null)); // <-- FAIL
            assertRows(cluster.coordinator(2).execute(withKeyspace(""SELECT v FROM %s.tbl WHERE pk=1 AND ck='1'""), ConsistencyLevel.ALL),
                       row((Integer) null));

        }
    }
{code}

 cc: [~maedhroz], [~ifesdjeen]

",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16654,Junit RepeatableRunner flaky tests helper,"Some flakies only fail when the full suite is ran. If it's a quick test you can do thousands of runs if a few seconds. Looping at the sh level is not an option as every loop takes a few seconds.

As I have found this very useful I am proposing committing it",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16649,Add Versions.Major.v4X to Java DTests (after cassandra-4.0 branch was created),"- https://github.com/apache/cassandra-in-jvm-dtest-api/compare/trunk...thelastpickle:mck/16649
- https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/16649/trunk ",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16647,Update how_to_commit documentation after cassandra-4.0 branch,"
- 3.11: https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/16642/3.11 (also backports CASSANDRA-16626)
- cassandra-4.0: https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/16642/4.0
- trunk: https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/16642/trunk",N/A,"3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16638,compactions/repairs hangs (backport CASSANDRA-16552),"Hi

We meet an issue during repairs (but more probably compaction issue in fact) since we upgraded from 3.11.1 to 3.11.10.

We are using reaper, but the issue doesn't seem to come from it (according to [~adejanovski@hotmail.com] ). When the problem happens, repairs driven by reaper are blocked.

Basically reaper hangs with the message ""All nodes are busy or have too many pending compactions for the remaining candidate segments."" and indeed one node has a lot of compaction pending tasks :

 
{code:java}
$ nodetool compactionstats
pending tasks: 95
- mt_metrics.metric_32: 95 
{code}
Errors in log are :

 
{code:java}
WARN [CompactionExecutor:12909] 2021-04-28 08:59:51,241 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/var/lib/cassandra/d
....
WARN [CompactionExecutor:12909] 2021-04-28 09:00:19,484 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/var/lib/cassandra/d
....
WARN [CompactionExecutor:12908] 2021-04-28 09:00:51,241 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/var/lib/cassandra/d
....
WARN [CompactionExecutor:12907] 2021-04-28 08:58:51,097 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350757-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350755-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350738-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350759-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350761-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350740-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/metric_32-23300de089c311e882a61bd0fd209f48/md-350751-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/mt_metrics/
.... 
{code}

The error happened several times in few weeks and up to now always concerns LCS tables.

a.dejanoski mentioned me https://issues.apache.org/jira/browse/CASSANDRA-15242 but I have no trace of messages like ""disk boundaries are out of date for keyspacename.tablename"" or ""Refreshing disk boundary cache for keyspacename.tablename"".

The workaround is simple : just restart the node once it is identified. Pending compactions tasks rerun well.

We have the issue on 2 of our clusters on 3.11.10.
Does someone else met the issue ?",N/A,3.11.11
CASSANDRA-16634,Garbagecollect should not output all tables to L0 with LeveledCompactionStrategy,"nodetool garbagecollect always outputs to L0 with LeveledCompactionStrategy.

This is awful.  On a large LCS table, this means that at the end of the garbagecollect process, all data is in L0.

 

This results in an awful sequence of useless temporary space usage and write amplification:
 # L0 is repeatedly size-tiered compacted until it doesn't have too many SSTables.  If the original LCS table had 2000 tables... this takes a long time
 # L0 is compacted to L1 in one to a couple very very large compactions
 # L1 is compacted to L2, L3 to L4, etc.  Write amplification galore

Due to the above, 'nodetool garbagecollect' is close to worthless for large LCS tables.  A full compaction is always less write amplification and similar temp disk space required.  The only exception is if you can use 'nodetool garbagecolect' part-way, and then use 'nodetool stop' to cancel it before L0 is too large.  In this case if you are lucky, and the order that it chose to process SSTables coincides with tables that have the most  disk space to clear, you might free up enough disk space to succeed in your original goal.

 

However, from what I can tell, there is no good reason to move the output to L0.  Leaving the output table in the same SSTableLevel as the source table does not violate any of the LeveledCompactionStrategy placement rules, as the output by definition has a token range equal to or smaller than the source.

The only drawback is if the size of the output files is significantly smaller than the source, in which case the source level would be under-sized.   But that seems like a problem that LCS has to handle, not garbagecollect.

LCS could have a ""pull up"" operation where it does something like the following.   Assume a table has L4 as the max level, and L3 and L4 are both 'under-sized'.  L3 can attempt to 'pull up' any tables from L4 that do not overlap with the token ranges of the L3 tables.  After that, it can choose to do some compactions that mix L3 and L4 to pull up data into L3 if it is still significantly under-sized.

From what I can tell, garbagecollect should just re-write tables in place, and leave the compaction strategy to deal with any consequences.

Moving to L0 is a bad idea.  In addition to the extra write amplification and extreme increase in temporary disk space required, I observed the following:

A 'nodetool garbagecollect' was placing a lot of pressure on a L0 of a node.  We stopped it about 20% through the process, and it managed to compact down the top couple levels.  So we tried to run 'garbagecollect' again, but the first tables it chose to operate on were in L1, not the 'leafs' in L5!   This was because the order of SSTables chosen currently does not consider the level, and instead looks purely at the max timestamp in the  file.  But because we moved _very old_ data from L5 into L0 as a result of the prior gabagecollect, manytables in L1 and L2 now had very wide ranges between their min and max timestamps – essentially some of the oldest and newest data all in one table.    This breaks the usual structure of an LCS table where the oldest data is at the high levels.

 

I hope that others agree that this is a bug, and deserving of a fix.

I have a very simple patch for this that I will be creating a PR for soon.  3 lines for the code change, 70 lines for a new unit test.

 ",N/A,"3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16633,Only include versioned files in rat-report,"Recent changes to the ant build.xml file to include running the Apache Rat reporting tool break the build when there are unversioned files in the source tree.
We could modify the build to only include versioned files in the Rat report.
",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16628,Logging bug during the node replacement and token assignment,"Hello Team,

 

I noticed a minor logging issue when a Cassandra node is trying to boot-up with a new IP address but the existing data directory. The IP address and Token fields are inter-changed.

 

*Sample Log:* 

{{WARN [GossipStage:1] 2021-04-23 18:24:06,348 StorageService.java:2425 - Not updating host ID 27031833-5141-46e0-b032-bef67137ae49 for /10.24.3.9 because it's mine}}
{{INFO [GossipStage:1] 2021-04-23 18:24:06,349 StorageService.java:2356 - Nodes () and /10.24.3.9 have the same token /10.24.3.10. Ignoring -1124147225848710462}}
{{INFO [GossipStage:1] 2021-04-23 18:24:06,350 StorageService.java:2356 - Nodes () and /10.24.3.9 have the same token /10.24.3.10. Ignoring -1239985462983206335}}

*Steps to Reproduce:*

Replace a Cassandra node with the a new IP address with the same data directory and the logs should show the messages.

*Cassandra Version*: 3.11.6

 

Please let me know if you need more details",N/A,"3.0.25, 3.11.11, 4.0-rc2"
CASSANDRA-16625,Add a CircleCI job to run some tests repeatedly,"I think it could be useful to have an optional CircleCI job to run some specific tests n times. That way, tickets could attach CircleCI runs showing that the changes don't make a certain ticket flaky or, conversely, that they fix a flaky test. Doing this systematically should mitigate the risk of introducing new flaky tests, and I guess it would be more convenient and easy to share than running the tests locally or on a private CI system.

It would also be nice to have something similar in Jenkins, but I'm focusing this ticket on CircleCI because it's available also for non-committers, so assignees can run their tests before setting the tickets as ready for review.",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.1-alpha1, 4.1"
CASSANDRA-16620,Improve failure message for rat plugin,"The RAT plugins failure message isn't informative, only referring to the file (build/rat.txt) where the offending files can be found.

In CI systems, without access to that file, it is cumbersome to figure out what those offending files are.

Add to the failure message a grep of the first few offending files.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16619,Loss of commit log data possible after sstable ingest,"SSTable metadata contains commit log positions of the sstable. These positions are used to filter out mutations from the commit log on restart and only make sense for the node on which the data was flushed.

If an SSTable is moved between nodes they may cover regions that the receiving node has not yet flushed, and result in valid data being lost should these sections of the commit log need to be replayed.

Solution:
The chosen solution introduces a new sstable metadata (StatsMetadata) - originatingHostId (UUID), which is the local host id of the node on which the sstable was created, or null if not known. Commit log intervals from an sstable are taken into account during Commit Log replay only when the originatingHostId of the sstable matches the local node's hostId.

For new sstables the originatingHostId is set according to StorageService's local hostId.
For compacted sstables the originatingHostId set according to StorageService's local hostId, and only commit log intervals from local sstables is preserved in the resulting sstable.

discovered by [~jakubzytka]
",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16607,Fix flaky test testRequestResponse – org.apache.cassandra.net.MockMessagingServiceTest,"https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/659/tests/

{code}
Error
expected:<1> but was:<0>
Stacktrace
junit.framework.AssertionFailedError: expected:<1> but was:<0>
	at org.apache.cassandra.net.MockMessagingServiceTest.testRequestResponse(MockMessagingServiceTest.java:81)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Standard Output
INFO  [main] 2021-04-15 08:22:46,838 YamlConfigurationLoader.java:93 - Configuration location: file:/home/cassandra/cassandra/test/conf/cassandra.yaml
DEBUG [main] 2021-04-15 08:22:46,840 YamlConfigurationLoader.java:112 - Loading settings from file:/home/cassandra/cassandra/test/conf/cassandra.yaml
DEBUG [main] 2021-04-15 08:22:46,899 InternalLoggerFactory.java:63 - Using SLF4J as the default logging framework
DEBUG [main] 2021-04-15 08:22:46,911 PlatformDependent0.java:417 - -Dio.netty.noUnsaf
...[truncated 61235 chars]...
te NORMAL, token [a57d4b7f61f49471614b7ac41f16477e]
DEBUG [main] 2021-04-15 08:22:49,840 StorageService.java:2674 - New node /127.0.0.1:7069 at token a57d4b7f61f49471614b7ac41f16477e
DEBUG [main] 2021-04-15 08:22:49,848 StorageService.java:2727 - Node /127.0.0.1:7069 state NORMAL, token [a57d4b7f61f49471614b7ac41f16477e]
INFO  [main] 2021-04-15 08:22:49,848 StorageService.java:2730 - Node /127.0.0.1:7069 state jump to NORMAL
DEBUG [main] 2021-04-15 08:22:49,849 StorageService.java:1619 - NORMAL
{code}",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16606,"Update libthrift jar to at least 0.9.3-1, investigate 0.14.0","Cassandra 3.x and 2.x uses libthrift 0.9.2, which has a number of vulnerabilities associated with it which are applicable to Cassandra;

CVE-2015-3254
CVE-2018-1320 (CASSANDRA-15424)
CVE-2019-0205 (CASSANDRA-15420)

Updating to 0.9.3-1 will mitigate these, however that branch suffers CVE-2020-13949. 

To mitigate risks from using out of date libthrift versions, Cassandra should be updated to use 0.14.0

",N/A,"3.0.26, 3.11.12"
CASSANDRA-16605,The dependencies in the Cassandra DEB package prevent installation on Ubuntu 20.04+,"The DEB package for Cassandra has a dependency on 'python >= 2.7'.

In Ubuntu 18.04 there is a package called 'python' which installs python2.7.

This package has been removed in Ubuntu 20.04 so the dependency cannot be satisified.

 

Proposed solution:

Change the dependency 'python >= 2.7' to something like 'python2 >= 2.7 OR python3 >= 3.6'.",N/A,"3.0.25, 3.11.11"
CASSANDRA-16604,Parallelise docker container runs for tests in ci-cassandra.a.o,"This was raised on the dev ML, where the consensus was to remove it: https://lists.apache.org/thread.html/r1ca3c72b90fa6c57c1cb7dcd02a44221dcca991fe7392abd8c29fe95%40%3Cdev.cassandra.apache.org%3E

The idea is to then replace ant test parallelism with docker container parallelism.

PoC patch: 
https://github.com/apache/cassandra-builds/compare/trunk...thelastpickle:mck/16587-2/trunk

This is just a quick PoC, aimed at the ci-cassandra agents that have
4 cores and 16gb ram available to each executor, but I imagine instead
something that spawns a number of containers based on system
resources, like we currently do with get-cores and get-mem. 

Also worth noting the overhead here, compared with the ant parallelism approach, docker
builds everything in each container from scratch, but this too can be
improved easily enough.

Cleaning up any remnant `-Dtest.runners=` options is also part of this ticket.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc2, 4.0, 4.1-alpha1, 4.1"
CASSANDRA-16601,Flaky CassandraIndexTest,"See failure [here|https://ci-cassandra.apache.org/job/Cassandra-trunk/436/testReport/junit/org.apache.cassandra.index.internal/CassandraIndexTest/indexCorrectlyMarkedAsBuildAndRemoved_cdc/]


{noformat}
Error Message

expected:<1> but was:<0>

Stacktrace

junit.framework.AssertionFailedError: expected:<1> but was:<0>
	at org.apache.cassandra.index.internal.CassandraIndexTest.indexCorrectlyMarkedAsBuildAndRemoved(CassandraIndexTest.java:588)
{noformat}

",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16595,Remove test parallelism from ant build.xml in all branches,"Cassandra's build.xml supports parallel test runners. This functionality is available through {{`-Dtest.runners`}} and the {{`testparallel`}} ant macro.

Having not been actively used and atrophied over time, it breaks a number of tests. The distributed in-jvm tests don't work at all with parallel runners (currently they need `-Dtest.runners=1` specified to work). And there are plenty of flakies, from where
tests use fixed ports (StorageServiceServerTest), to byteman (eg BMUnitRunner), and around conf files on disk.

This was raised on the dev ML, where the consensus was to remove it: https://lists.apache.org/thread.html/r1ca3c72b90fa6c57c1cb7dcd02a44221dcca991fe7392abd8c29fe95%40%3Cdev.cassandra.apache.org%3E

The idea is to then replace ant test parallelism with docker container parallelism.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16592,The token function in where clause return incorrect data when using token equal condition and Specified a non-exist token value,"I get incorrect value when use query like 'select Token(pk1,pk2),pk1,pk2 from ks.table1 where token(pk1,pk2) = tokenValue'. The returned token value mismatch the where condition.

This problem is reproduced in 3.11.3 and 4.0.

Here is my schema and select statement
{code:java}
// schema
cqlsh> desc testprefix.cprefix_03 ;CREATE TABLE testprefix.cprefix_03 (
    pk1 int,
    pk2 int,
    ck1 text,
    ck2 text,
    t1 int,
    PRIMARY KEY ((pk1, pk2), ck1, ck2)
) WITH CLUSTERING ORDER BY (ck1 ASC, ck2 ASC)
    AND additional_write_policy = '99p'
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND cdc = false
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND default_time_to_live = 0
    AND extensions = {}
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair = 'BLOCKING'
    AND speculative_retry = '99p';


{code}
execute cql query
{code:java}
// code placeholder
cqlsh> SELECT Token(pk1,pk2), pk1,pk2  from testprefix.cprefix_03 WHERE  token(pk1, pk2) =-9223372036854775808 LIMIT 2; 
system.token(pk1, pk2) | pk1    | pk2
------------------------+--------+---------
   -9222849988925915479 | 394560 | 3394560
   -9222849988925915479 | 394560 | 3394560
(2 rows)

cqlsh> SELECT Token(pk1,pk2) from testprefix.cprefix_03 where pk1 = 394560 and pk2 = 3394560 LIMIT 2; 
system.token(pk1, pk2)
------------------------
   -9222849988925915479
   -9222849988925915479
(2 rows)

cqlsh> SELECT Token(pk1,pk2), pk1,pk2  from testprefix.cprefix_03 WHERE  token(pk1, pk2) =-9222849988925915479 LIMIT 2; 
system.token(pk1, pk2) | pk1    | pk2
------------------------+--------+---------
   -9222849988925915479 | 394560 | 3394560
   -9222849988925915479 | 394560 | 3394560
(2 rows){code}
we can find  that token value in the condition  are inconsistent with the values in the result.

--------------------------------------------------------------------------------------------

Then review the source code, to seek the anwser. 
{code:java}
// code placeholder
private static void addRange(SSTableReader sstable, AbstractBounds<PartitionPosition> requested, List<AbstractBounds<PartitionPosition>> boundsList)
{
    if (requested instanceof Range && ((Range)requested).isWrapAround())    //  first condition
    {
        if (requested.right.compareTo(sstable.first) >= 0)
        {
            // since we wrap, we must contain the whole sstable prior to stopKey()
            Boundary<PartitionPosition> left = new Boundary<PartitionPosition>(sstable.first, true);
            Boundary<PartitionPosition> right;
            right = requested.rightBoundary();
            right = minRight(right, sstable.last, true);
            if (!isEmpty(left, right))
                boundsList.add(AbstractBounds.bounds(left, right));
        }
        if (requested.left.compareTo(sstable.last) <= 0)
        {
            // since we wrap, we must contain the whole sstable after dataRange.startKey()
            Boundary<PartitionPosition> right = new Boundary<PartitionPosition>(sstable.last, true);
            Boundary<PartitionPosition> left;
            left = requested.leftBoundary();
            left = maxLeft(left, sstable.first, true); // second condition
            if (!isEmpty(left, right))
                boundsList.add(AbstractBounds.bounds(left, right));
        }
    }
    else
    {
        assert requested.left.compareTo(requested.right) <= 0 || requested.right.isMinimum();
        Boundary<PartitionPosition> left, right;
        left = requested.leftBoundary();
        right = requested.rightBoundary();
        left = maxLeft(left, sstable.first, true);
        // apparently isWrapAround() doesn't count Bounds that extend to the limit (min) as wrapping
        right = requested.right.isMinimum() ? new Boundary<PartitionPosition>(sstable.last, true)
                                                : minRight(right, sstable.last, true);
        if (!isEmpty(left, right))
            boundsList.add(AbstractBounds.bounds(left, right));
    }
}
{code}
 * we use token equal ,so isWrapAround is true.
 * requestd.left = requestd.right = -9223372036854775808,
 * the real sst dataBoundary.left = -9222849988925915479
 * so the maxLeft return the real dataBoudary.left. We get the incorrect  data

 ",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16588,NPE getting host_id in Gossiper.isSafeForStartup,"As seen here: https://ci-cassandra.apache.org/job/Cassandra-devbranch/604/testReport/junit/org.apache.cassandra.distributed.upgrade/MixedModeGossipTest/testStatusFieldShouldExistInOldVersionNodesEdgeCase/

{noformat}
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.isSafeForStartup(Gossiper.java:952)
	at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:657)
	at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:933)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:784)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:729)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$10(Instance.java:541)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

I believe what is happening is a GossipDigestAck has been queued to ack the shutdown state from the node on the seed, but isn't actually sent until the node has restarted and gone into shadow.  Since the ack contains the node's IP, it assumes a host_id will be there but since this is not an actual shadow response, it is not.",N/A,"3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16581,Failure to execute queries should emit a KPI other than read timeout/unavailable so it can be alerted/tracked,When we are unable to parse a message we do not have a way to detect this from a monitoring point of view so can get into situations where we believe the database is fine but the clients are on-fire.  This case popped up in the 2.1 to 3.0 upgrade as paging state wasn’t mixed-mode safe.,N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16578,NativeLibrary#getProcessID() does not handle `UnsatisfiedLinkError`,"NativeLibrary#getProcessID() does not handle `UnsatisfiedLinkError` (derived from Error, not Exception) as the other native methods do. Therefore, it can never return -1 when it fails for this reason, and can break callers that would otherwise be able to handle the situation gracefully. Most other methods in the class do this, but this one is missing the handling of this error.",N/A,"3.0.26, 3.11.12, 4.0.1"
CASSANDRA-16577,Node waits for schema agreement on removed nodes,"CASSANDRA-15158 might have introduced a bug where bootstrapping nodes wait for schema agreement from nodes that have been removed if token allocation for keyspace is enabled.

 

It is fairly easy to reproduce with the following steps:
{noformat}
// Create 3 node cluster
ccm create test --vnodes -n 3 -s -v 3.11.10

// Remove two nodes
ccm node2 decommission
ccm node3 decommission
ccm node2 remove
ccm node3 remove

// Create keyspace to change the schema. It works if the schema never changes.
ccm node1 cqlsh -x ""CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};""

// Add allocate parameter
ccm updateconf 'allocate_tokens_for_keyspace: k'

// Add node2 again to cluster
ccm add node2 -i 127.0.0.2 -j 7200 -r 2200
ccm node2 start{noformat}
 

This will cause node2 to throw exception on startup:
{noformat}
WARN  [main] 2021-04-08 14:10:53,272 StorageService.java:941 - There are nodes in the cluster with a different schema version than us we did not merged schemas from, our version : (a5da47ec-ffe3-3111-b2f3-325f771f1539), outstanding versions -> endpoints : {8e9ec79e-5ed2-3949-8ac8-794abfee3837=[/127.0.0.3]}
ERROR [main] 2021-04-08 14:10:53,274 CassandraDaemon.java:803 - Exception encountered during startup
java.lang.RuntimeException: Didn't receive schemas for all known versions within the timeout
        at org.apache.cassandra.service.StorageService.waitForSchema(StorageService.java:947) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.dht.BootStrapper.allocateTokens(BootStrapper.java:206) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:177) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:1073) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:753) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:687) ~[apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:395) [apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:633) [apache-cassandra-3.11.10.jar:3.11.10]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:786) [apache-cassandra-3.11.10.jar:3.11.10]
INFO  [StorageServiceShutdownHook] 2021-04-08 14:10:53,279 HintsService.java:209 - Paused hints dispatch
WARN  [StorageServiceShutdownHook] 2021-04-08 14:10:53,280 Gossiper.java:1670 - No local state, state is in silent shutdown, or node hasn't joined, not announcing shutdown
INFO  [StorageServiceShutdownHook] 2021-04-08 14:10:53,280 MessagingService.java:985 - Waiting for messaging service to quiesce
INFO  [ACCEPT-/127.0.0.2] 2021-04-08 14:10:53,281 MessagingService.java:1346 - MessagingService has terminated the accept() thread
INFO  [StorageServiceShutdownHook] 2021-04-08 14:10:53,416 HintsService.java:209 - Paused hints dispatch{noformat}
 

 

 ",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16573,CQLSH copy defaults appear to be incorrect on website,"The documentation on the website for the defaults of CQLSH appear to be incorrect and contain numerous errors (at least for the latest and greatest)

For this page:
[https://cassandra.apache.org/doc/latest/tools/cqlsh.html]

{{MINBATCHSIZE}} is listed as defaulting to 2.  Code says this is 10.
https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L355

Chunksize says 1000, actually set to 5000.
https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L352


NumProcessis is also off...
""NUMPROCESSES
 The number of child worker processes to create for COPY tasks. Defaults to a max of 4 for COPY FROM and 16 for COPY TO. However, at most (num_cores - 1) processes will be created.""

 Default is the number of cores -1 or 16 which ever is smaller, and you can set this value to anything.  See the following code



https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L361
https://github.com/apache/cassandra/blob/trunk/pylib/cqlshlib/copyutil.py#L407

 

 

 ",N/A,3.11.x
CASSANDRA-16572,j8_dtest_jars_build is broken in CI,"j8_dtest_jars_build is broken. It constantly fails at building the dtest jar from the target branch after building the trunk. 

One example, https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/741/workflows/c3de5483-85b0-48a4-b1fa-1c52f89680c7/jobs/4063

It seems to be caused by the recent ant build changes. The lib dir contains the jars leftover from the build output of trunk.
",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16567,Fix test testCompoundPartitionKey - org.apache.cassandra.cql3.ViewTest,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/874/workflows/0b0a1e36-107a-43c7-815f-bf8e61d3028d/jobs/5227/tests

{code}
junit.framework.AssertionFailedError: Got more rows than expected. Expected 0 but got 1.
	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1185)
	at org.apache.cassandra.cql3.ViewTest.testCompoundPartitionKey(ViewTest.java:817)
{code}",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16562,Fix flaky testSkipScrubCorruptedCounterRowWithTool,See CASSANDRA-16532 where extra flakiness was detected on 3.0 and 3.11 branches for {{testSkipScrubCorruptedCounterRowWithTool}},N/A,"3.0.x, 3.11.x"
CASSANDRA-16558,Fix rat check (April 2021),The rat plugin in build.xml is a mess and not properly catching missing license headers.,N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16557,"(3.0,3.11) Migrate dependency handling from maven-ant-tasks to resolver-ant-tasks","Backport CASSANDRA-16391 to 2.2, 3.0, and 3.11

This ticket is on hold until the ASF legal clears up its policy, and the project confirms the approach it wants to take with {{lib/}} dependencies. See [~benedict] comment [here|https://issues.apache.org/jira/browse/CASSANDRA-16391?focusedCommentId=17313754&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17313754].",N/A,"3.0.25, 3.11.11"
CASSANDRA-16555,Add support  for AWS Ec2 IMDSv2,"In order to patch a vulnerability, Amazon came up with a new version of their metadata service.

It's no longer unrestricted but now requires a token (in a header), in order to access the metadata service.
See [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-instance-metadata-service.html] for more information.

Cassandra currently doesn't offer an out-of-the-box snitch class to support this.
See [https://cassandra.apache.org/doc/latest/operating/snitch.html#snitch-classes]

This issue asks to add support for this as a separate snitch class.
We'll probably do a PR for this, as we are in the process of developing one.",N/A,"3.0.30, 3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-16541,LGPL dependency in cassandra-all 3.11.10,"Dear Cassandra devs.

While integrating Cassandra in Apache Hop functionality we found out that the cassandra-all maven package drags in an older version of jboss-logging via reporter-config3:
{code:java}
+- org.apache.cassandra:cassandra-all:jar:3.11.10:compile
|  +- com.addthis.metrics:reporter-config3:jar:3.0.3:compile
|  |  +- com.addthis.metrics:reporter-config-base:jar:3.0.3:compile
|  |  \- org.hibernate:hibernate-validator:jar:4.3.0.Final:compile
|  |     +- javax.validation:validation-api:jar:1.0.0.GA:compile
|  |     \- org.jboss.logging:jboss-logging:jar:3.1.0.CR2:compile

{code}
jboss-logging 3.1.0.CR2 is from November 2011 and the library only changed to APL in 2012.

[https://github.com/jboss-logging/jboss-logging/blob/master/src/main/resources/META-INF/LICENSE.txt]

 ",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0"
CASSANDRA-16535,Fail to init Cassandra Startup with an API connected with other cluster machines (3.11.9 version),"The code in the function ""VerifyPortsAreAvailable"", look for the ports: 7000, 9042, 7199 and 9160 in the ""netstat -an"" command output.

 

If we have an API connected to the cluster in a node, and we try to start the execution of cassandra (with that API working, accesing ports 9042 of the other cluster machines), the output of that command return us that the port is already in use, and Cassandra´s startup process fails, showing the output message : ""port already in use"" in the .log file.

Thus, if we change the code of the function, adding the line: "" -and $line -match ""LISTENING"" "", just next to the ""$line -match ""TCP"" "", we will only interrupt the execution if the load port is in the state: ""LISTENING"", but no if it´s an active connection on the other node of the cluster.

!image-2021-03-24-21-12-32-709.png|width=470,height=159!

!image-2021-03-24-21-13-46-704.png|width=526,height=119!",N/A,3.11.11
CASSANDRA-16528,Update Cassandra dependencies to fix security vulnerabilities,"There are a couple of security vulnerabilities that show up in libraries that cassandra pulls in.
 # apache commons-collections v 3.2.1
 # apache commons-beanutils v 1.7.0

For number one, there is a well-known security vulnerability in apache commons-collection 3.2.1 (see [https://www.kb.cert.org/vuls/id/576313] and https://issues.apache.org/jira/browse/COLLECTIONS-580). This is fixed/mitigated in commons-collections 3.2.2.

All current versions of cassandra (including 4.0beta4) pull in commons-collections 3.2.1 via apache-rat 0.10. Is it possible to upgrade apache-rat to version 0.12 in order to pull in the latest version of commons-collections? See [https://github.com/apache/creadur-rat/commit/2380409fbcd02b418eceacfdc1e486bdbbca9632].

I made the below change in 3.0.24 src and recompiled without errors.
{code:java}
// code placeholder
diff --git a/cassandra/cassandra-3.0-src/build.xml b/cassandra/cassandra-3.0-src/build.xml
index 73c9889d81..ed236443d4 100644
--- a/cassandra/cassandra-3.0-src/build.xml
+++ b/cassandra/cassandra-3.0-src/build.xml
@@ -402,3 +402,3 @@
           <dependency groupId=""org.reflections"" artifactId=""reflections"" version=""0.9.12"" />
-          <dependency groupId=""org.apache.rat"" artifactId=""apache-rat"" version=""0.10"">
+          <dependency groupId=""org.apache.rat"" artifactId=""apache-rat"" version=""0.12"">
              <exclusion groupId=""commons-lang"" artifactId=""commons-lang""/>
@@ -1605,3 +1605,3 @@
     <artifact:dependencies pathId=""rat.classpath"">
-      <dependency groupId=""org.apache.rat"" artifactId=""apache-rat-tasks"" version=""0.6"" />
+      <dependency groupId=""org.apache.rat"" artifactId=""apache-rat-tasks"" version=""0.12"" />
       <remoteRepository refid=""central""/>
{code}
 

For number two, I was able to discern that beanutils is coming from hadoop-core which is version 1.0.3.  I believe this also is quite out of date and could be upgraded. 

Could someone take a look and see if these version upgrades are possible?

{{}}",N/A,"3.0.26, 3.11.12, 4.0.1"
CASSANDRA-16523,Improve handling of unflushed hint files,In some situations hint files can have a run of trailing zeros at the end - for example when hard-rebooting machines. These zeros can be safely ignored and we should avoid invoking the disk failure policy in these cases.,N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16519,Remove ant targets list-jvm-dtests and ant list-jvm-upgrade-dtests ,"Nobody uses them.

CircleCI uses {{`circleci tests glob …`}} https://github.com/apache/cassandra/blob/trunk/.circleci/config-2_1.yml#L881 

And ci-cassandra.a.o is [soon going|https://issues.apache.org/jira/browse/CASSANDRA-16402] to be using {{`split -n r/S/N …`}}
https://github.com/thelastpickle/cassandra-https://github.com/thelastpickle/cassandra-builds/commit/96b0d768757e61b23bee191f287eeca4b14c14cc#diff-4195e47b46326c5e879c949a1300aa403d4562fb70eb872dbeb597e22e18f9e0R19


references:
- https://the-asf.slack.com/archives/CK23JSY2K/p1615796046117100",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16518,Node restart during joining sets protocol version to V3,"While joining nodes to a cluster, an old node crashed. The old node was recovered however clients (datastax java) refused to connect to it.

The driver error:
{noformat}
Detected added or restarted Cassandra host /<ip>:<port> but ignoring it since it does not support the version V4 of the native protocol which is currently in use.{noformat}

In the recovered node cassandra logs:
{noformat}
INFO  o.a.c.transport.ConfiguredLimit Detected peers which do not fully support protocol V4. Capping max negotiable version to V3{noformat}

I confirmed that ALL the nodes in the cluster, joining or otherwise, were apache-cassandra-3.11.6 so that error message was rather confusing.

 Eventually after digging through the code we got to the bottom of the issue:

https://issues.apache.org/jira/browse/CASSANDRA-15193 adds a check for node version, which reverts the protocol version to V3 if any peer fails the version check. Joining nodes have NULL for their version in the peers table, which fails the version check.

 ",N/A,"3.0.27, 3.11.13"
CASSANDRA-16495,Scheduled (Delayed) Schema Pull Tasks May Run After MIGRATION Stage Shutdown During Decommission,"A new test added in CASSANDRA-16181 stumbled across this, although it doesn’t happen consistently. When [failure occurs|https://app.circleci.com/pipelines/github/maedhroz/cassandra/235/workflows/eb8133ce-9373-4136-b404-ceca167353f6/jobs/1355/tests], it appears to be because a delayed schema pull happens after decommission shuts down the MIGRATION stage’s thread pool.

{noformat}
ERROR [node1_isolatedExecutor:1] node1 2021-02-15 19:35:36,284 CassandraDaemon.java:579 - Exception in thread Thread[node1_NonPeriodicTasks:1,5,node1] java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:72) 
at java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825) 
at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355) 
at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:176) 
at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:118) at org.apache.cassandra.concurrent.Stage.submit(Stage.java:129) 
at org.apache.cassandra.schema.MigrationCoordinator.lambda$scheduleSchemaPull$2(MigrationCoordinator.java:362) 
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) 
at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) 
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) 
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) 
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) 
at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}

A fix might be as simple as shutting down ScheduledExecutors.nonPeriodicTasks in StorageService#decommission(). See the original discussion [here|https://issues.apache.org/jira/browse/CASSANDRA-16181?focusedCommentId=17293329&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17293329].",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16483,ColumnFilter::toString doesn't return a valid CQL fragment,"This was changed in CASSANDRA-16415 to include indications about queried vs fetched reagular & static columns. However, the result is used by {{AbstractReadQuery::toCQLString}}, which causes it to produce an illegal query string.

This breaks a couple of dtests because they're looking for CQL strings in logs, which are no longer found:
* {{upgrade_tests/paging_test.py::TestPagingWithDeletions::test_failure_threshold_deletions}}
* {{cql_test.py::TestCQLSlowQuery}} has a couple of failing tests, {{test_local_query/test_remote_query}}

We should also check audit and fql logs (and any other place where {{toCQLString}} is used.
",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16477,Fix centos packaging for arm64,The cryptography python library (needed by urllib3) does not install on arm64 using centos 7 and python2.,N/A,"2.1.23, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16467,"speculative retry should allow more friendly params, allowing upgrade from 2.x not to break","2.x speculative retry params are case insensitive, while 3.0 and 3.11 have added case sensitivity. As as result of this, one of our internal applications suffered an issue during 
C* upgrade from 2.x to 3.0.

This ticket is to propose making 3.0 and 3.11 speculative_retry params case insensitive as well (essentially a slightly modified backport of CASSANDRA-13876, but not to allow something like ""99p"" which 4.0 allows)",N/A,"3.0.25, 3.11.11"
CASSANDRA-16464,Upgrade to logback package 1.2.0 or later fix high vulnerabilities,"Tag | Distro | CVE ID | Severity | Packages | Source Package | Fix Package Version
-- | -- | -- | -- | -- | -- | --
v0.1.22 | Ubuntu-bionic | CVE-2017-5929 | critical | ch.qos.logback_logback-core | 1.1.3 | fixed in 1.2.0",N/A,3.0.25
CASSANDRA-16462,Upgrade to Jackson Databind 2.9.10.8 or later fix high vulnerabilities ,"There are 22 high CVEs


CVE ID | Severity | Packages | Source Package | Fixed Package Version
-- | -- | -- | -- | --
CVE-2020-24750 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.6
CVE-2020-24616 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.6
CVE-2020-14195 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.5
CVE-2020-14062 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.5
CVE-2020-14061 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.5
CVE-2020-14060 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.5
CVE-2020-35491 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-35490 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-35728 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2021-20190 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.7
CVE-2020-25649 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.10.5.1, 2.9.10.7, 2.6.7.4
CVE-2020-36187 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36188 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36189 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36186 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36185 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36183 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36184 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36182 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36179 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36180 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8
CVE-2020-36181 | high | com.fasterxml.jackson.core_jackson-databind | 2.9.10.4 | fixed in 2.9.10.8",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16457,Hint messages are incorrectly re-serialized for filtering in in-jvm dtests,"Hint messages for dropped tables can still be dispatched, but they’re ignored on the receiving side all usual code paths. Since we’re attempting to re-serialize hint message for dropped table in in-jvm tests, we exercise path that is impossible in regular code, and for which there is no protocol specification. 


Stack trace: 

{code}
INFO  [AsyncAppender-Worker-ASYNC] 2021-02-17 18:50:13,759 SubstituteLogger.java:169 - ERROR [MutationStage-2] 2021-02-17 18:50:13,726 AbstractLocalAwareExecutorService.java:166 - Uncaught exception on thread Thread[MutationStage-2,5,node4]
java.lang.NullPointerException: null
	at org.apache.cassandra.hints.Hint$Serializer.serializedSize(Hint.java:150)
	at org.apache.cassandra.hints.HintMessage$Serializer.serializedSize(HintMessage.java:86)
	at org.apache.cassandra.hints.HintMessage$Serializer.serializedSize(HintMessage.java:77)
	at org.apache.cassandra.net.Message$Serializer.payloadSize(Message.java:1289)
	at org.apache.cassandra.net.Message$Serializer.access$1200(Message.java:607)
	at org.apache.cassandra.net.Message.payloadSize(Message.java:1341)
	at org.apache.cassandra.net.Message.access$900(Message.java:66)
	at org.apache.cassandra.net.Message$Serializer.serializePost40(Message.java:759)
	at org.apache.cassandra.net.Message$Serializer.serialize(Message.java:618)
	at org.apache.cassandra.distributed.impl.Instance.serializeMessage(Instance.java:322)
	at org.apache.cassandra.distributed.impl.Instance.lambda$registerInboundFilter$4(Instance.java:273)
	at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:62)
	at org.apache.cassandra.net.InboundSink$Filtered.accept(InboundSink.java:49)
	at org.apache.cassandra.net.InboundSink.accept(InboundSink.java:93)
	at org.apache.cassandra.distributed.impl.Instance.lambda$null$6(Instance.java:365)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
	at relocated.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
{code}",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16450,MigrationCoordinatorTest#testWeKeepSendingRequests() Failing Sporadically,"This test has now failed at least on trunk and 3.11. Here are the data points we have:

*trunk*

https://ci-cassandra.apache.org/job/Cassandra-trunk/273/testReport/org.apache.cassandra.schema/MigrationCoordinatorTest/testWeKeepSendingRequests/

https://app.circleci.com/pipelines/github/adelapena/cassandra/181/workflows/84fa978e-6df6-455a-b9d0-8e1024e2657f/jobs/1390


*3.11*

https://app.circleci.com/pipelines/github/dcapwell/cassandra/801/workflows/34d9dd52-444a-465d-88ff-074db4a03132/jobs/4597

The only previous mention of this issue was [here|https://issues.apache.org/jira/browse/CASSANDRA-15158?focusedCommentId=17165619&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17165619]. In any case, that issue seems not to have resolved the problem.",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16448,Fix class loading for UDF in in-jvm dtests ,"User defined functions can not be loaded into in-jvm dtests, since UDF picks up system class loader rather than instance class loader. 
",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16447,Expose StorageServiceMBean#getKeyspaceReplicationInfo from 4.0 to 3.11,"As part of CASSANDRA-13853, there was a method introduced into StorageServiceMBean via which an operator can retrieve information about a particular keyspace. This method is very handy not only in the context of 13853 and I am lacking that method in 3.11 as it was not ported there. I would like to have that method in 3.11 too for observability purposes as well as for the feature parity with 4.0.

https://github.com/apache/cassandra/pull/897

https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/505/",N/A,3.11.11
CASSANDRA-16444,Fix flaky test testMultiExpressionQueriesWhereRowSplitBetweenSSTables - org.apache.cassandra.index.sasi.SASIIndexTest,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/862/workflows/d2b10373-5bd1-4895-a738-1c28587cae62/jobs/5136

{code}
junit.framework.AssertionFailedError: []
	at org.apache.cassandra.index.sasi.SASIIndexTest.testMultiExpressionQueriesWhereRowSplitBetweenSSTables(SASIIndexTest.java:589)
	at org.apache.cassandra.index.sasi.SASIIndexTest.testMultiExpressionQueriesWhereRowSplitBetweenSSTables(SASIIndexTest.java:468)
{code}",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16433,Version family is probably broken for Cassandra 2 and 3.11 in dtests,"It looks like the version families are a bit confused - when we determine version family for the current build, we do:

{code:python}
    if current_version.vstring.startswith('2.0'):
        version_family = '2.0.x'
    elif current_version.vstring.startswith('2.1'):
        version_family = '2.1.x'
    elif current_version.vstring.startswith('2.2'):
        version_family = '2.2.x'
    elif current_version.vstring.startswith('3.0'):
        version_family = '3.0.x'
    elif '3.1' <= current_version < '4.0':
        version_family = '3.x'
    elif '4.0' <= current_version < '4.1':
        version_family = 'trunk'
    else:
        # when this occurs, it's time to update this manifest a bit!
        raise RuntimeError(""4.1+ not yet supported on upgrade tests!"")
{code}

but later, in the upgrade manifest we have:

{code:python}
indev_2_1_x = VersionMeta(name='indev_2_1_x', family='2.1', variant='indev', version='github:apache/cassandra-2.1', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))
current_2_1_x = VersionMeta(name='current_2_1_x', family='2.1', variant='current', version='2.1.20', min_proto_v=1, max_proto_v=3, java_versions=(7, 8))

indev_2_2_x = VersionMeta(name='indev_2_2_x', family='2.2', variant='indev', version='github:apache/cassandra-2.2', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))
current_2_2_x = VersionMeta(name='current_2_2_x', family='2.2', variant='current', version='2.2.13', min_proto_v=1, max_proto_v=4, java_versions=(7, 8))

indev_3_0_x = VersionMeta(name='indev_3_0_x', family='3.0', variant='indev', version='github:apache/cassandra-3.0', min_proto_v=3, max_proto_v=4, java_versions=(8,))
current_3_0_x = VersionMeta(name='current_3_0_x', family='3.0', variant='current', version='3.0.23', min_proto_v=3, max_proto_v=4, java_versions=(8,))

indev_3_11_x = VersionMeta(name='indev_3_11_x', family='3.11', variant='indev', version='github:apache/cassandra-3.11', min_proto_v=3, max_proto_v=4, java_versions=(8,))
current_3_11_x = VersionMeta(name='current_3_11_x', family='3.11', variant='current', version='3.11.9', min_proto_v=3, max_proto_v=4, java_versions=(8,))
{code}

later on in the code we have some manifest filtering:

{code:python}
            if filter_for_current_family and not origin_meta.matches_current_env_version_family and not destination_meta.matches_current_env_version_family:
                logger.debug(""skipping class creation, origin version {} and destination version {} do not match target version {}, and --upgrade-target-version-only was set"".format(origin_meta.name, destination_meta.name, VERSION_FAMILY))
                continue
{code}

This does not cause any problems for {{trunk}}, but when I tried to run some upgrade tests on 3.11 build, I could not do anything. 

Therefore we need to either change families in manifest as follows:
- 2.1 -> 2.1.x
- 2.2 -> 2.2.x
- 3.0 -> 3.0.x
- 3.11 -> 3.x

or change how we assign version family for the current build

",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16428,Fix selections of JDKs in debian docker images on arm64,"The debian docker [image|https://github.com/apache/cassandra-builds/blob/trunk/docker/buster-image.docker] used by the [cassandra-deb-packaging.sh|https://github.com/apache/cassandra-builds/blob/trunk/build-scripts/cassandra-deb-packaging.sh] tries to set the jdk like
{code}
update-java-alternatives --set java-1.8.0-openjdk-amd64
{code}

This won't work on arm64.
Suggestion is to replace it with the following:
{code}
update-java-alternatives --set $(update-java-alternatives -l | cut -d"" "" -f1 | grep java-1.8)
{code}",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16427,In-JVM dtest paging does not handle Group By correctly,"In-JVM dtest paging is using a pager that disregards the type of the executed query, resulting into `GROUP BY` queries being executed like normal SELECT queries without GROUP BY clause.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16419,Cassandra-builds should remove the unused (non-dangling) docker images in Jenkins,"The docker `prune` commands used in the Jenkins builds only remove the dangling images. What we are expecting is to remove the unused ones as well. 

The commands currently used are missing the option `-a | --all`.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16415,Digest mismatches during upgrade,"The test has been failing and can always be reproduced in the recent CI. 

Stack trace: 
{code:java}
junit.framework.AssertionFailedError: Found Digest Mismatch
 at org.apache.cassandra.distributed.upgrade.MixedModeReadTest.checkTraceForDigestMismatch(MixedModeReadTest.java:89)
 at org.apache.cassandra.distributed.upgrade.MixedModeReadTest.lambda$mixedModeReadColumnSubsetDigestCheck$0(MixedModeReadTest.java:63)
 at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:171)
 at org.apache.cassandra.distributed.upgrade.MixedModeReadTest.mixedModeReadColumnSubsetDigestCheck(MixedModeReadTest.java:76) {code}
The initial investigation shows that 
 * The test only fails in the setup phase of mixedModeReadColumnSubsetDigestCheck. The cluster version is *Versions.Major.v3X*
 * The test failure is likely a consequence of CASSANDRA-15962. After dropping the commit in branch cassandra-3.11 and rebuild the dtest jar, the upgrade test can pass. Meanwhile, dropping the other commits does not help. ",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16405,Handle both TimeoutError and NodeError when expecting node start failure,"*Summary*

Some tests are validating situation, when node (or cluster) does not start correctly.

They rely on `TimeoutError` to be raised, but technically it is also possible that `ccm` will raise `NodeError` without waiting for timeout to be met.

*Why we need this change*

We can improve `ccm` to fail fast in case node being started terminates. This would:
 * make unexpected test failures faster to fail (not waiting 90 or 120s)
 * shorten overall test duration, even if timeout is given

ccm work (in progress): [https://github.com/riptano/ccm/pull/724]

*Proposed improvement*

Handle both TimeoutError and NodeError as expected node failure.

*PR*

https://github.com/apache/cassandra-dtest/pull/120

 ",N/A,"2.1.23, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16402,In Jenkins move and split jvm dtests to 'Distributed Test' stage,"The jvm-dtests are taking long enough now to warrant moving them from the 'Test' stage to the 'Distributed Test' stage in the Jenkins pipeline.

That is, jvm-dtests is now taking significantly longer than any of the other unit test jobs in the 'Test' stage, increasing the time the time the whole pipeline takes.

Moving both jvm-dtest and jvm-dtest-upgrade to the `Distributed Test`, and splitting them (changing the job to a matrix), will reduce the pipeline runtime. Such grouping also makes sense logically.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16401,Remove the Jenkins plaintext reports,"Remove the generation of the {{cassandra-test-report.txt}} plain text report (that was added in CASSANDRA-15729). 

The xml report is now at 3GB and too big to fit through any xsl transformation. It is only going to get bigger as more tests and test stages are added.

This size failure is seen in the pipeline logs as
{noformat}
docker run --rm -v /home/jenkins/jenkins-slave/workspace/Cassandra-trunk@2:/tmp apache_cassandra_ci/generate_plaintext_test_report
Warning: at xsl:stylesheet on line 4 column 49 of cassandra-test-report.xsl:  Running an XSLT 1.0 stylesheet with an XSLT 2.0 processor
 java.lang.IndexOutOfBoundsException: [660540429,660540717]
	at net.sf.saxon.tinytree.LargeStringBuffer.subSequence(Unknown Source)
	at net.sf.saxon.tinytree.TinyTextImpl.getStringValue(Unknown Source)
	at net.sf.saxon.tinytree.TinyParentNodeImpl.getStringValue(Unknown Source)
        …
{noformat}

In addition we can and do now archive everything to https://nightlies.apache.org/cassandra/ , solving the original requirements of CASSANDRA-15729 of providing a permanent archive of test results.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16399,Selecting resource intensive tests is not consistent,"It looks like there are two problems:
 - collection of tests to run fails when there are log messages in stderr
 - collection of resource intensive tests (with
{code:java}
--only-resource-intensive-tests{code}
) is broken

 ",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16395,Increase node start timeout for selected bootstrap and replace node tests,"*Summary*

Node start timeouts should be explicitly extended to more than default 90s (boostrap with reset state, replace node tests) because the default 90s will start to work after ccm changes.

*Why we need this change*

There is a bug in [https://github.com/riptano/ccm] that node.start() timeout (or more precisely node.wait_for_binary_proto() timeout is in practice 600s. This is the time to wait for certain log message:

[https://github.com/riptano/ccm/blob/484476494bda6d71f895826358722a7b1c47a3cf/ccmlib/node.py#L642|https://github.com/riptano/ccm/blob/cassandra-test/ccmlib/node.py#L642]

This bug will be fixed by: [https://github.com/riptano/ccm/pull/725]

*Proposed improvement*

Explicitly raise node start timeout to 120s or 180s (depending on the scenario) by using existing `Node` api to provide timeout as int (in seconds) instead of bool.

Note that this is available after [https://github.com/riptano/ccm/pull/725] is merged but should not break test logic before it is merged.

*PR*

[https://github.com/apache/cassandra-dtest/pull/113/files]

 ",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16394,Fix schema aggreement race conditions in in-JVM dtests ,"There there are two race conditions in in-JVM dtest schema agreement, which are causing test failures:

1. First is caused by the fact we’re starting waiting for schema propagation already after the schema agreement was reached (which was resulting into us endlessly waiting for an agreement that has already been established);
 2. The other one was because the callback to notify about successful agreement can be triggered already after the other node has notified about it, and control flow might have moved cluster to a different configuration.

Example of exception:
{code:java}
Caused by: java.lang.IllegalStateException: Schema agreement not reached
	at org.apache.cassandra.distributed.impl.AbstractCluster$ChangeMonitor.waitForCompletion(AbstractCluster.java:?)
	at org.apache.cassandra.distributed.impl.AbstractCluster.lambda$schemaChange$5(AbstractCluster.java:?)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:?)
	at java.util.concurrent.FutureTask.run(FutureTask.java:?)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:?)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:?)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:?)
	at java.lang.Thread.run(Thread.java:?)
{code}",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16393,Update lib/jflex-1.6.0.jar to match upstream jflex-1.6.0.jar,"jflex-1.6.0.jar was added to lib/ when SASI was added, as part of CASSANDRA-10661

The file in lib/ does not match the upstream file.",N/A,"3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16387,UpgradeTest sporadically failing on schema updates,"We’ve observed {{UpdateTest}} failing during what appears to be a schema change:

https://app.circleci.com/pipelines/github/maedhroz/cassandra/192/workflows/ed5305e6-e4f9-420e-9f0a-6153333746dc/jobs/1068

It almost looks like the Gossiper can’t find its own endpoint state in the endpoint state map, and the failure is not consistent, which might suggest a race.",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16383,Python dtests to use ccm tag instead of the `cassandra-test` branch,"The python dtests (cassandra-dtest repo) creates its clusters using ccm.

The version of ccm it uses is the HEAD of the {{cassandra-test}} branch.  This is referenced in the [requirements.txt|https://github.com/apache/cassandra-dtest/blob/trunk/requirements.txt#L3].

The history for why a separate branch of ccm is used by dtests is explained in https://github.com/apache/cassandra-dtest/pull/13 

Long story short: the separate branch avoids two headaches
- the 'latest commit to master' broke all the c* builds surprises', and 
- the 'i have to cut a release of ccm just to get a ccm change into use by dtests'

But the two branches: {{master}} and {{cassandra-test}}; have effectively been treated as two separate repositories, with (non-fast-forward) merges happening in both directions. This makes the git history of both branches messy and difficult to use, and it makes the merge strategy confusing. Bi-directional merging between branches is considered a poor practice by many (Laura Wingerd's 'The Flow of Change' [presentation|https://www.perforce.com/sites/default/files/flow-change-wingerd.pdf] and [book|https://books.google.no/books?id=mlm61wb2v3MC&pg=PT191&lpg=PT191&dq=%22don%27t+drive+through+hedges%22&source=bl&ots=I_rYBRJwTD&sig=ACfU3U1iKLORDQii5uiTveaKPOpa3cFqng&hl=en&sa=X&ved=2ahUKEwju96-DpZnuAhWytYsKHeW6D5EQ6AEwAHoECAEQAg#v=onepage&q=%22don't%20drive%20through%20hedges%22&f=false] refers to this as ""don't drive through hedges"" and encourages the ""merge down, copy up"" approach against the ""tofu scale: firm above, soft below""). 

To date, AFAIK no merges between the branches have occurred since January 2018.

A possible improvement to this process is to replace the {{cassandra-test}} branch with a floating tag (of the same name).

That way new commits to {{master}} are not automatically used by the python dtests. And changes made to ccm and intended/needed to be used by the dtests can be put in use by re-tagging {{cassandra-test}} to master's HEAD. 

The re-tagging approach is
{code}
git tag -a -f cassandra-test
git push origin :refs/tags/cassandra-test
git push origin --tags
{code}",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16377,Add dtest-upgrade and dtest-large-novnode to Jenkins pipeline,"Now that dtest-upgrade test runs are passing they can be considered for addition to the jenkins pipeline. And dtest-large-novnode only as one failure so should be added too.  

https://the-asf.slack.com/archives/CK23JSY2K/p1610132643162700 ",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16373,Deploy testing docker images to hub.docker.com/u/apache/ ,"INFRA-21103 demonstrates PMC individuals can get access to the apache organisation on dockerhub (and the dockerhub ""cassandra"" team).

We have a number of docker images (used in testing and CI) that are currently deployed under individual user accounts on dockerhub. These can now be deployed under https://hub.docker.com/u/apache",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16372,"Import from csv of empty strings in list fails with a ParseError: Empty values are not allowed,  given up without retries"," 

Cqlsh fail to import an empty string which is present in a list data type.

{color:#ff0000}_In those conditions, simple csv backups can discard rows and data can be corrupted._{color}

 

*Conditions*

 
{code:java}
# cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.6 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.

CREATE TABLE test.test_1 (
    uid uuid PRIMARY KEY,
    texts list<text>
);

insert into test.test_1 ( uid, texts ) VALUES( 833fee3f-d4f9-418b-9387-84ac2cda5cb7, ['But if you now try to wash your hands,', ''] );

{code}
 

Now exporting and and re-importing data fails:

 
{code:java}
cqlsh> select * from test.test_1;

 uid                                  | texts
--------------------------------------+------------------------------------------------
 833fee3f-d4f9-418b-9387-84ac2cda5cb7 | ['But if you now try to wash your hands,', '']


cqlsh> COPY test.test_1 (uid, texts) TO 'ctm.csv'; 
Using 7 child processesStarting copy of test.test_1 with columns [uid, texts].
Processed: 1 rows; Rate: 9 rows/s; Avg. rate: 9 rows/s 1 rows exported to 1 files in 0.148 seconds. 

cqlsh> truncate table test.test_1;


cqlsh> COPY test.test_1 (uid, texts) FROM 'ctm.csv';
Using 7 child processes
Starting copy of test.test_1 with columns [uid, texts].
Failed to import 1 rows: ParseError - Failed to parse ['But if you now try to wash your hands,', ''] : Empty values are not allowed,  given up without retries
Failed to process 1 rows; failed rows written to import_test_test_1.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.415 seconds (0 skipped).


cqlsh> select * from test.test_1; 
uid  | pid
-----+-----

{code}
 

 

 ",N/A,"3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16371,JMH Reports in ci-cassandra.apache.org,"Enable the  jenkins [jmh reporting plugin|https://plugins.jenkins.io/jmh-report/] and add a jenkins job to run the `ant microbench` target.

Patches
 - cassandra-builds: https://github.com/apache/cassandra-builds/compare/trunk...thelastpickle:mck/jenkins-microbench
 - cassandra: https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_jmh_report

Example results are:
 - CI JMH report: https://ci-cassandra.apache.org/job/Cassandra-devbranch-microbench/jdk=jdk_11_latest,label=cassandra/1/jmh-run-report/  (screenshot attached)

The value of these reports is limited, with different hardware jenkins agents and two executors on each agent. But visibility from the reports shows what is possible, how agents are different (and behave under saturation), and most importantly when jmh classes are broken (there are no unit tests on these benchmark tests).
",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16355,Fix flaky test incompletePropose - org.apache.cassandra.distributed.test.CASTest,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/853/workflows/0766c0de-956e-4831-aa40-9303748a2708/jobs/5030

{code}
junit.framework.AssertionFailedError: Expected: [[1, 1, 2]]
Actual: []

	at org.apache.cassandra.distributed.shared.AssertUtils.fail(AssertUtils.java:193)
	at org.apache.cassandra.distributed.shared.AssertUtils.assertEquals(AssertUtils.java:163)
	at org.apache.cassandra.distributed.shared.AssertUtils.assertRows(AssertUtils.java:63)
	at org.apache.cassandra.distributed.test.CASTest.incompletePropose(CASTest.java:124)
{code}",N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16350,"Improve compaction param ""provide_overlapping_tombstones"" handling","We currently have no cqlsh autocompletion for {{provide_overlapping_tombstones}}. We should also improve the validation for the parameter given as it is currently quite unhelpful:

{code}
cqlsh:x> create table z (id int primary key, x int) with compaction = {'class': 'SizeTieredCompactionStrategy', 'provide_overlapping_tombstones':'xyz'};
NoHostAvailable:
{code}

and an exception in the logs;
{code}
ERROR [Native-Transport-Requests-1] 2020-12-14 13:25:46,575 QueryMessage.java:121 - Unexpected error during query
java.lang.IllegalArgumentException: No enum constant org.apache.cassandra.schema.CompactionParams.TombstoneOption.XYZ
        at java.lang.Enum.valueOf(Enum.java:238)
        at org.apache.cassandra.schema.CompactionParams$TombstoneOption.valueOf(CompactionParams.java:59)
        at org.apache.cassandra.schema.CompactionParams.create(CompactionParams.java:98)
        at org.apache.cassandra.schema.CompactionParams.fromMap(CompactionParams.java:255)
        at org.apache.cassandra.cql3.statements.schema.TableAttributes.build(TableAttributes.java:98)
        at org.apache.cassandra.cql3.statements.schema.TableAttributes.validate(TableAttributes.java:58)
        at org.apache.cassandra.cql3.statements.schema.CreateTableStatement.builder(CreateTableStatement.java:145)
        at org.apache.cassandra.cql3.statements.schema.CreateTableStatement.apply(CreateTableStatement.java:104)
        at org.apache.cassandra.schema.Schema.transform(Schema.java:587)
        at org.apache.cassandra.schema.MigrationManager.lambda$announce$2(MigrationManager.java:221)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:748)

{code}",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16334,Replica failure causes timeout on multi-DC write,"Inserting a mutation larger than {{max_mutation_size_in_kb}} correctly throws a write error on a single DC keyspace with RF=3:
{noformat}
cassandra.WriteFailure: Error from server: code=1500 [Replica(s) failed to execute write] message=""Operation failed - received 0 responses and 3 failures: UNKNOWN from /127.0.0.3:7000, UNKNOWN from /127.0.0.2:7000, UNKNOWN from /127.0.0.1:7000"" info={'consistency': 'LOCAL_ONE', 'required_responses': 1, 'received_responses': 0, 'failures': 3}
{noformat}
The same insert wrongly causes a timeout on a keyspace with 2 dcs (RF=3 each):
{noformat}
cassandra.WriteTimeout: Error from server: code=1100 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 0 responses."" info={'consistency': 'LOCAL_ONE', 'required_responses': 1, 'received_responses': 0}
{noformat}
Reproduction steps:
{noformat}
# Setup cluster
ccm create -n 3:3 test
for i in {1..6}; do echo 'max_mutation_size_in_kb: 1000' >> ~/.ccm/test/node$i/conf/cassandra.yaml; done
ccm start

# Create schema
ccm node1 cqlsh
CREATE KEYSPACE test WITH replication = {'class': 'NetworkTopologyStrategy', 'dc1': 3, 'dc2': 3};
CREATE TABLE test.test (key int PRIMARY KEY, val blob);
exit;

# Insert data
python
from cassandra.cluster import Cluster
cluster = Cluster()
session = cluster.connect('test')
blob = f = open(""2mbBlob"", ""rb"").read().hex()
session.execute(""INSERT INTO test (key, val) VALUES (1, textAsBlob('"" + blob + ""'))"")
{noformat}
Reproduced in 3.0, 3.11, 4.0, trunk.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16311,Extend the exclusion of replica filtering protection to other indices instead of just SASI,"There was a check introduced in CASSANDRA-8272 telling if an index is a SASI index and if it is, replica filtering protection was not triggered.

There might be other custom index implementations which also do not support filtering protection and they do not have to be SASI indices neccessarily, however it is not possible to exclude them.

 

PR trunk

[https://github.com/apache/cassandra/pull/844]

PR 3.11

[https://github.com/apache/cassandra/pull/847]

PR 3.0

https://github.com/apache/cassandra/pull/848

 ",N/A,"3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16307,GROUP BY queries with paging can return deleted data,"{{GROUP BY}} queries using paging and CL>ONE/LOCAL_ONE. This dtest reproduces the problem:
{code:java}
try (Cluster cluster = init(Cluster.create(2)))
{
    cluster.schemaChange(withKeyspace(""CREATE TABLE %s.t (pk int, ck int, PRIMARY KEY (pk, ck))""));
    ICoordinator coordinator = cluster.coordinator(1);
    coordinator.execute(withKeyspace(""INSERT INTO %s.t (pk, ck) VALUES (0, 0)""), ConsistencyLevel.ALL);
    coordinator.execute(withKeyspace(""INSERT INTO %s.t (pk, ck) VALUES (1, 1)""), ConsistencyLevel.ALL);
    
    cluster.get(1).executeInternal(withKeyspace(""DELETE FROM %s.t WHERE pk=0 AND ck=0""));
    cluster.get(2).executeInternal(withKeyspace(""DELETE FROM %s.t WHERE pk=1 AND ck=1""));
    String query = withKeyspace(""SELECT * FROM %s.t GROUP BY pk"");
    Iterator<Object[]> rows = coordinator.executeWithPaging(query, ConsistencyLevel.ALL, 1);
    assertRows(Iterators.toArray(rows, Object[].class));
}
{code}
Using a 2-node cluster and RF=2, the test inserts two partitions in both nodes. Then it locally deletes each row in a separate node, so each node sees a different partition alive, but reconciliation should produce no alive partitions. However, a {{GROUP BY}} query using a page size of 1 wrongly returns one of the rows.

This has been detected during CASSANDRA-16180, and it is probably related to CASSANDRA-15459, which solved a similar problem for group-by queries with limit, instead of paging.",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16295,upgradesstables/scrub support for 2.x sstables,"This support is important for 2.0/3.0 cluster or 3.0/4.0 cluster that can/might contain 2.x sstables.

4.0 supports versions down to {{ma}}, and 3.0 supports versions down to {{jb}} (3.0 and 2.0.1 correspondingly). So by the time you're on 4.0, you should have no 2.0 sstables either way, and this needs to be fixed for 3.11

 

*NOTE: I just marked the ticket with both 3.11 and 4.0 rc versions even if the bug is presented only in 3.11 but to make it appear as a blocker on the 4.0 board!!*",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16294,Potential NPE in JVMStabilityInspector,"On either a FileNotFoundException or SocketException, JVMStabilityInspector checks the error message for the string ""Too many open files"". However, both of these exceptions have a constructor which sets a null message, which can lead to NPE if handled.",N/A,"3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16286,Make TokenMetadata's ring version increments atomic,"The update semantics of the ring version in {{TokenMetadata}} are not clear. The instance variable itself is {{volatile}}, but it is still incremented by a non-atomic check-and-set, and not all codepaths do that while holding the {{TokenMetadata}} write lock. We could make this more intelligible by forcing the external callers to use both the write when invalidating the ring and read lock when reading the current ring version. Most of the readers of the ring version (ex. compaction) don't need it to be fast, but it shouldn't be a problem even if they do. If we do this, we should be able to avoid a situation where concurrent invalidations don't produce two distinct version increments.",N/A,"2.2.20, 3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16261,Prevent unbounded number of flushing tasks,"The cleaner thread is not prevented from queueing an unbounded number of flushing tasks for memtables that are almost empty.

This patch adds a mechanism to track the number of pending flushing
tasks in the memtable cleaner. Above the maximum number (2x the flushing
threads by default), only memtables using at least MCT memory will be
flushed, where MCT stands for Memory Cleanup Threshold.

This patch also fixes a possible problem tracking the memory marked as
""reclaiming"" in the memtable allocators and pool. Writes that complete
only after a memtable has been scheduled for flushing, did not report
their memory as reclaiming. Normally this should be a small value of no
consequence, but if the flushing tasks are blocked for a long period,
and there is a sufficient number of writes, or these writes use
a sufficiently large quantity of memory, this would cause the memtable
cleaning algorithm to schedule repeated flushing tasks because the used
memory is always > reclaiming memory + MCT.",N/A,"3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16259,tablehistograms cause ArrayIndexOutOfBoundsException,"After upgrading some nodes in our cluster from 3.11.8 to 3.11.9 an error appeared on the upgraded nodes when trying to access *tablehistograms*. The same command run on our .8 nodes return as expected, only the upgraded .9 nodes fail. Not all tables fail when queried, but about 90% of them do.

We use Datastax MCAC which appears to query histograms every 30 seconds, this outputs to the system.log:
{noformat}
WARN  [insights-3-1] 2020-11-09 01:11:22,331 UnixSocketClient.java:830 - Error reporting:
java.lang.ArrayIndexOutOfBoundsException: 115
    at org.apache.cassandra.metrics.TableMetrics.combineHistograms(TableMetrics.java:261) ~[apache-cassandra-3.11.9.jar:3.11.9]
    at org.apache.cassandra.metrics.TableMetrics.access$000(TableMetrics.java:48) ~[apache-cassandra-3.11.9.jar:3.11.9]
    at org.apache.cassandra.metrics.TableMetrics$11.getValue(TableMetrics.java:376) ~[apache-cassandra-3.11.9.jar:3.11.9]
    at org.apache.cassandra.metrics.TableMetrics$11.getValue(TableMetrics.java:373) ~[apache-cassandra-3.11.9.jar:3.11.9]
    at com.datastax.mcac.UnixSocketClient.writeMetric(UnixSocketClient.java:839) [datastax-mcac-agent.jar:na]
    at com.datastax.mcac.UnixSocketClient.access$700(UnixSocketClient.java:78) [datastax-mcac-agent.jar:na]
    at com.datastax.mcac.UnixSocketClient$2.lambda$onGaugeAdded$0(UnixSocketClient.java:626) ~[datastax-mcac-agent.jar:na]
    at com.datastax.mcac.UnixSocketClient.writeGroup(UnixSocketClient.java:819) [datastax-mcac-agent.jar:na]
    at com.datastax.mcac.UnixSocketClient.lambda$restartMetricReporting$2(UnixSocketClient.java:798) [datastax-mcac-agent.jar:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_272]
    at io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:126) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:399) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:307) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_272]{noformat}
Manually trying a histogram from the CLI:
{noformat}
$ nodetool tablehistograms logdata log_height_index
error: 115
-- StackTrace --
java.lang.ArrayIndexOutOfBoundsException: 115
	at org.apache.cassandra.metrics.TableMetrics.combineHistograms(TableMetrics.java:261)
	at org.apache.cassandra.metrics.TableMetrics.access$000(TableMetrics.java:48)
	at org.apache.cassandra.metrics.TableMetrics$11.getValue(TableMetrics.java:376)
	at org.apache.cassandra.metrics.TableMetrics$11.getValue(TableMetrics.java:373)
	at org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxGauge.getValue(CassandraMetricsRegistry.java:250)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:276)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at com.sun.jmx.remote.security.MBeanServerAccessController.getAttribute(MBeanServerAccessController.java:320)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1445)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1408)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:639)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
	at sun.rmi.transport.Transport$1.run(Transport.java:200)
	at sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:834)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{noformat}",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16253,Fix documentation to reflect real defaults for commit logs,"About the documentation on commit logs:
 * Commit logs default is every 10 sec - *periodic*, not batch [https://github.com/apache/cassandra/blob/cassandra-3.11.8/conf/cassandra.yaml#L385-L386] (same in 3.11.9 or trunk). It can lead to data losses but still makes sense to me because fsync-ing every write is very expensive, and (even in batches) should be reserved for the super paranoid where handling multiple power-out (or sudden disk failures) simultaneously on multiple nodes needs to be handled (as [~mck] explained to me).

  * Nonetheless, apart from the theory behind it, in a more practical perspective, *the doc is wrong* and says *Default Value: batch* which mislead me and this could happen to more people: [https://cassandra.apache.org/doc/latest/architecture/storage_engine.html#commit-log]

 

I think this is important enough for us to quickly fix the documentation in trunk. As this information is fully missing in the 3.11 branch documentation, I suggest we add it as well on that branch (based on information about commit logs from trunk).",N/A,"3.11.9, 4.0-beta4, 4.0"
CASSANDRA-16228,TableMetrics are exposed before ColumnFamilyStore is fully initialized,"The {{ColumnFamilyStore}} exposes the {{TableMetrics}} before it is fully initialized, due to that it is possible to perform a call via the metrics that access uninitialized part of the {{ColumnFamilyStore}}. 

The following test can be added to ColumnFamilyMetricTest to show the issue:
{code:java}
@Test
public void testStartupRaceConditionOnMetricListeners()
{
 // Since the ColumnFamilyStore instance reference escapes during the construction
 // we have a race condition and listeners can see an instance that is in an unknown state.
 // This test just check that all callbacks can access the data without throwing any exception.
 registerMetricListener();
 SchemaLoader.createKeyspace(""Keyspace2"",
 KeyspaceParams.simple(1),
 SchemaLoader.standardCFMD(""Keyspace2"", ""Standard2""));
}

private void registerMetricListener()
{
 CassandraMetricsRegistry.Metrics.addListener(new MetricRegistryListener.Base()
 {
 @Override
 public void onGaugeAdded(String name, Gauge<?> gauge)
 {
 gauge.getValue();
 }

 @Override
 public void onGaugeRemoved(String name)
 {

 }

 @Override
 public void onCounterAdded(String name, Counter counter)
 {
 counter.getCount();
 }

 @Override
 public void onCounterRemoved(String name)
 {

 }

 @Override
 public void onHistogramAdded(String name, Histogram histogram)
 {
 histogram.getCount();
 }

 @Override
 public void onHistogramRemoved(String name)
 {

 }

 @Override
 public void onMeterAdded(String name, Meter meter)
 {
 meter.getCount();
 }

 @Override
 public void onMeterRemoved(String name)
 {

 }

 @Override
 public void onTimerAdded(String name, Timer timer)
 {
 timer.getCount();
 }

 @Override
 public void onTimerRemoved(String name)
 {

 }
 });{code}

While looking into that ticket we also discovered a problem with the used of {{Metered}} in {{CacheMetrics}}.
Metrics reporter looks for metrics classes that are instance of the standard codahale classes. Due to that, other Metered implementations are not be exposed through the reporter. This ticket will also address that issue.",N/A,"3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16226,COMPACT STORAGE SSTables created before 3.0 are not correctly skipped by timestamp due to missing primary key liveness info,"This was discovered while tracking down a spike in the number of  SSTables per read for a COMPACT STORAGE table after a 2.1 -> 3.0 upgrade. Before 3.0, there is no direct analog of 3.0's primary key liveness info. When we upgrade 2.1 COMPACT STORAGE SSTables to the mf format, we simply don't write row timestamps, even if the original mutations were INSERTs. On read, when we look at SSTables in order from newest to oldest max timestamp, we expect to have this primary key liveness information to determine whether we can skip older SSTables after finding completely populated rows.

ex. I have three SSTables in a COMPACT STORAGE table with max timestamps 1000, 2000, and 3000. There are many rows in a particular partition, making filtering on the min and max clustering effectively a no-op. All data is inserted, and there are no partial updates. A fully specified row with timestamp 2500 exists in the SSTable with a max timestamp of 3000. With a proper row timestamp in hand, we can easily ignore the SSTables w/ max timestamps of 1000 and 2000. Without it, we read 3 SSTables instead of 1, which likely means a significant performance regression. 

The following test illustrates this difference in behavior between 2.1 and 3.0:
https://github.com/maedhroz/cassandra/commit/84ce9242bedd735ca79d4f06007d127de6a82800

A solution here might be as simple as having {{SinglePartitionReadCommand#canRemoveRow()}} only inspect primary key liveness information for non-compact/CQL tables. Tombstones seem to be handled at a level above that anyway. (One potential problem with that is whether or not the distinction will continue to exist in 4.0, and dropping compact storage from a table doesn't magically make pk liveness information appear.)",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16225,Followup CASSANDRA-14554,"As per [~stefania]'s advice, additional synchronization should be added to   LogTransaction.java. Without synchronization, we could have corrupted txn log files with JBOD.",N/A,"3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16223,Reading dense table yields invalid results in case of row scan queries,"{{ThriftIntegrationTest}} is broken in the way that it does not actually test reads before and after flushing, because it does not do flush at all (see https://github.com/apache/cassandra/blob/cassandra-3.11/test/unit/org/apache/cassandra/cql3/validation/ThriftIntegrationTest.java#L939). After fixing that method so that it really flushes memtables to disk, we can see inconsistency in reads from dense table - the results returned from memtable differs from the results returned from sstable (the later are wrong, cell values are skipped unexpectedly).

{noformat}
java.lang.AssertionError: Invalid value for row 0 column 0 (value of type ascii), expected <value1> but got <>
{noformat}

In principle this problems is about skipping column values when doing row scan queries with explicitly selected columns (not wildcard), when the columns belong to a super column. This happens only when reading from sstables, it does not happen when reading from memtables.
",N/A,"3.0.23, 3.11.9"
CASSANDRA-16221,Jenkins doesn't run high resource python dtests with novonode so missing some tests,"In CASSANDRA-16220 it was found that a test is failing on trunk, and its because of the address to address with port changes; looking closer into it it was found that Jenkins ignores the novnode case which causes this test to get skipped.

We should enable novnode as well to match the rest of the pipeline, but also to handle some of the missing tests not currently run.",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16220,python dtest pending_range_test.py::TestPendingRangeMovements::test_pending_range (@pytest.mark.resource_intensive) fails on trunk,"The test has the following section

{code}
if cluster.version() >= '2.2':
  node2.watch_log_for('127.0.0.1 state moving', timeout=10, filename='debug.log’)
{code}

The issue is that in trunk we have the port attached to the log, so the log is now

{code}
DEBUG [GossipStage:1] 2020-10-21 00:48:20,104 StorageService.java:2452 - Node /127.0.0.1:7000 state MOVING, tokens [-9223372036854775808]
DEBUG [GossipStage:1] 2020-10-21 00:48:20,105 StorageService.java:2670 - Node /127.0.0.1:7000 state moving, new token -634023222112864484
{code}

Since the log now contains the port, this test always times out on trunk when it hits this

{code}
self = <pending_range_test.TestPendingRangeMovements object at 0x7fc5d35c85f8>
     @pytest.mark.resource_intensive
    def test_pending_range(self):
        """"""
            @jira_ticket CASSANDRA-10887
            """"""
        cluster = self.cluster
        # If we are on 2.1, we need to set the log level to debug or higher, as debug.log does not exist.
        if cluster.version() < '2.2':
            cluster.set_log_level('DEBUG')
   
        # Create 5 node cluster
        cluster.populate(5).start()
        node1, node2 = cluster.nodelist()[0:2]
   
        # Set up RF=3 keyspace
        session = self.patient_cql_connection(node1)
        create_ks(session, 'ks', 3)
   
        session.execute(""CREATE TABLE users (login text PRIMARY KEY, email text, name text, login_count int)"")
   
        # We use the partition key 'jdoe3' because it belongs to node1.
        # The key MUST belong to node1 to repro the bug.
        session.execute(""INSERT INTO users (login, email, name, login_count) VALUES ('jdoe3', 'jdoe@abc.com', 'Jane Doe', 1) IF NOT EXISTS;"")
   
        lwt_query = SimpleStatement(""UPDATE users SET email = 'janedoe@abc.com' WHERE login = 'jdoe3' IF email = 'jdoe@abc.com'"")
   
        # Show we can execute LWT no problem
        for i in range(1000):
            session.execute(lwt_query)
   
        token = '-634023222112864484'
   
        mark = node1.mark_log()
   
        # Move a node
        node1.nodetool('move {}'.format(token))
   
        # Watch the log so we know when the node is moving
        node1.watch_log_for('Moving .* to {}'.format(token), timeout=10, from_mark=mark)
        node1.watch_log_for('Sleeping 30000 ms before start streaming/fetching ranges', timeout=10, from_mark=mark)
   
        if cluster.version() >= '2.2':
>           node2.watch_log_for('127.0.0.1 state moving', timeout=10, filename='debug.log')
 pending_range_test.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 self = <ccmlib.node.Node object at 0x7fc5d3b51208>
exprs = '127.0.0.1 state moving', from_mark = None, timeout = 10, process = None
verbose = False, filename = 'debug.log'
     def watch_log_for(self, exprs, from_mark=None, timeout=600, process=None, verbose=False, filename='system.log'):
        """"""
            Watch the log until one or more (regular) expressions are found or timeouts (a
            TimeoutError is then raised). On successful completion, a list of pair (line matched,
            match object) is returned.
            """"""
        start = time.time()
        tofind = [exprs] if isinstance(exprs, string_types) else exprs
        tofind = [re.compile(e) for e in tofind]
        matchings = []
        reads = """"
        if len(tofind) == 0:
            return None
   
        log_file = os.path.join(self.log_directory(), filename)
        output_read = False
        while not os.path.exists(log_file):
            time.sleep(.5)
            if start + timeout < time.time():
                raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Timed out waiting for {} to be created."".format(log_file))
            if process and not output_read:
                process.poll()
                if process.returncode is not None:
                    self.print_process_output(self.name, process, verbose)
                    output_read = True
                    if process.returncode != 0:
                        raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
   
        with open(log_file) as f:
            if from_mark:
                f.seek(from_mark)
   
            while True:
                # First, if we have a process to check, then check it.
                # Skip on Windows - stdout/stderr is cassandra.bat
                if not common.is_win() and not output_read:
                    if process:
                        process.poll()
                        if process.returncode is not None:
                            self.print_process_output(self.name, process, verbose)
                            output_read = True
                            if process.returncode != 0:
                                raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
   
                line = f.readline()
                if line:
                    reads = reads + line
                    for e in tofind:
                        m = e.search(line)
                        if m:
                            matchings.append((line, m))
                            tofind.remove(e)
                            if len(tofind) == 0:
                                return matchings[0] if isinstance(exprs, string_types) else matchings
                else:
                    # yep, it's ugly
                    time.sleep(1)
                    if start + timeout < time.time():
>                       raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
E                       ccmlib.node.TimeoutError: 21 Oct 2020 00:50:26 [node2] Missing: ['127.0.0.1 state moving']:
E                       INFO  [main] 2020-10-21 00:49:15,878 YamlConfigura.....
E                       See debug.log for remainder
 /opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/ccmlib/node.py:550: TimeoutError
{code}",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16210,Synchronize Keyspace instance store/clear,DTest failure: dtest-large.repair_tests.repair_test.TestRepairDataSystemTable.test_repair_table (vnodes) - one random failure was reported which pointed to a race condition to be spotted. ,N/A,3.11.9
CASSANDRA-16207,NPE when calling broadcast address on unintialized node,"When trying to run upgrades, sometimes we’re calling broadcasts addrerss on an uninitialised new node:

{code}
java.lang.IllegalStateException: Can't use shut down instances, delegate is null
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.delegate(AbstractCluster.java:163)
	at org.apache.cassandra.distributed.impl.DelegatingInvokableInstance.broadcastAddress(DelegatingInvokableInstance.java:53) 
	at org.apache.cassandra.distributed.impl.Instance$2.allowIncomingMessage(Instance.java:278) 
	at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:1031) ~[dtest-3.0.19.jar:?]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:213) 
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:182) 
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93) 
{code}",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16206,Eliminate gen-doc template warning and unused (problematic) import,"1)

When running 'sphinx-build -b html -d build/doctrees source build/html'
on Linux we get the following warning:
{noformat}
     [exec] generating indices... genindex
     [exec] WARNING: Now base template defindex.html is deprecated.
     [exec] writing additional pages... index search{noformat}
On FreeBSD this causes gen-doc to fail:
{noformat}
     [exec] writing additional pages...  indexfailed
     [exec]
     [exec] Theme error:
     [exec] An error happened in rendering the page index.
     [exec] Reason: UndefinedError(""'warn' is undefined"")
     [exec] *** Error code 2{noformat}
2)

The patch to doc/source/_util/cql.py removes the unused iteritems import, preventing errors on versions of pygments 2.6.0+:
{noformat}
     [exec] Running Sphinx v3.2.1
     [exec]
     [exec] Exception occurred:
     [exec]   File ""/path/to/apache-cassandra-4.0-beta2-src/doc/source/_util/cql.py"", line 29, in <module>
     [exec]     from pygments.util import iteritems
     [exec] ImportError: cannot import name 'iteritems' from 'pygments.util' (/usr/local/lib/python3.7/site-packages/pygments/util.py){noformat}
The patch has been tested in the following environments:
 * FreeBSD 12.1-RELEASE-p7
 Python 3.7.9
 Sphinx 3.2.1
 pygments 2.7.1
 * Ubuntu 18.04.1
 Python 3.7.5
 Sphinx 1.6.7
 pygments 2.2.0
 * Ubuntu 18.04.1
 Python 2.7.17
 Sphinx 1.6.7
 pygments 2.5.2",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-16201,Reduce amount of allocations during batch statement execution,"In a Cas 2.1 / 3.0 / 3.11 / 4.0b2 comparison test with the same load profile, we see 4.0b2 going OOM from time to time. According to a heap dump, we have multiple NTR threads in a 3-digit MB range.

This is likely related to object array pre-allocations at the size of {{BatchUpdatesCollector.updatedRows}} per {{BTree}} although there is always only 1 {{BTreeRow}} in the {{BTree}}.
 !screenshot-1.png|width=100%! 

So it seems we have many, many 20K elemnts pre-allocated object arrays resulting in a shallow heap of 80K each, although there is only one element in the array.

This sort of pre-allocation is causing a lot of memory pressure.
",N/A,"3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16199,cassandra.logdir undefined when CASSANDRA_LOG_DIR,"When ${cassandra.logdir} is used in logback.xml nodetool doesn’t use the env variable CASSANDRA_LOG_DIR or the default value. and complains
{noformat}
03:07:27,387 |-ERROR in ch.qos.logback.core.rolling.RollingFileAppender[DEBUGLOG] - Failed to create parent directories for [/cassandra.logdir_IS_UNDEFINED/debug.log]03:07:27,387 |-ERROR in ch.qos.logback.core.rolling.RollingFileAppender[DEBUGLOG] - Failed to create parent directories for [/cassandra.logdir_IS_UNDEFINED/debug.log]03:07:27,388 |-ERROR in ch.qos.logback.core.rolling.RollingFileAppender[DEBUGLOG] - openFile(cassandra.logdir_IS_UNDEFINED/debug.log,true) call failed. java.io.FileNotFoundException: cassandra.logdir_IS_UNDEFINED/debug.log (No such file or directory) at java.io.FileNotFoundException: cassandra.logdir_IS_UNDEFINED/debug.log (No such file or directory)
...{noformat}
It’s different for cassandra for instance [https://github.com/apache/cassandra/blob/324267b3c0676ad31bd4f2fac0e2e673a9257a37/bin/cassandra#L186]. I feel like it should be added to [https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/bin/nodetool], or that it should call cassandra-env.sh

 

Seen on 3.11 and 4.0-beta1",N/A,"3.11.11, 4.0-rc2, 4.0"
CASSANDRA-16186,SEPExecutor does not release blocked threads as it should,"While adding some tests for the {{ThreadPoolMetrics}}, I discovered that the {{SEPExecutor}} does not release the blocked threads as it should.

If the number of tasks added to a SEPExecutor exceed the max queue size. The threads adding those task will be block until enough space become available for all the blocked tasks. At this point all the blocked threads will released at once. ",N/A,"3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16175,Avoid removing batch when it's not created during view replication,"When the base replica is also a view replica we don't write a local batchlog, but they are unnecessarily removed when the view write is successful, what creates (and persists) a tombstone into the system.batches table.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16166,Rename master branch to trunk in cassandra-dtest,,N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16163,Rename master branches to trunk in all repositories,"Applies to the following repositories
* cassandra-builds
* cassandra-website
* cassandra-dtest
* cassandra-sidecar
* cassandra-diff
* cassandra-in-jvm-dtest-api
* cassandra-harry

This was discussed in https://lists.apache.org/thread.html/r54db4cd870d2d665060d5fb50d925843be4b4d54dc64f3d21f04c367%40%3Cdev.cassandra.apache.org%3E

The general preference there was trunk over main, so to match the cassandra repository.",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16162,Improve empty hint file handling on startup,"Since CASSANDRA-14080 we handle empty/corrupt hint files on startup, we should remove empty files and rename corrupt ones to make sure we don't get the same exception on every startup",N/A,"3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-16161,Validation Compactions causing Java GC pressure,"Validation Compactions are not rate limited which can cause Java GC pressure and result in spikes in latency.

PR https://github.com/apache/cassandra/pull/814",N/A,3.11.10
CASSANDRA-16156,Decomissioned nodes are picked for gossip when unreachable nodes are considered for gossiping ,"After node is decommissioned, it is still considered for gossip via “unreachable” nodes, which results into following exceptions:
 
{code}
INFO  [node4_Messaging-EventLoop-3-3] node4 2020-09-29 16:37:37,527 NoSpamLogger.java:91 - /127.0.0.4:7012->/127.0.0.1:7012-URGENT_MESSAGES-[no-channel] failed to connect
io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /127.0.0.1:7012
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:702)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
 {code}

Trace of the method that attempts to establish connection:

{code} 
org.apache.cassandra.net.MessagingService.getOutbound(MessagingService.java:492)
	at org.apache.cassandra.net.MessagingService.doSend(MessagingService.java:335)
	at org.apache.cassandra.net.OutboundSink$Filtered.accept(OutboundSink.java:55)
	at org.apache.cassandra.net.OutboundSink.accept(OutboundSink.java:70)
	at org.apache.cassandra.net.MessagingService.send(MessagingService.java:327)
	at org.apache.cassandra.net.MessagingService.send(MessagingService.java:314)
	at org.apache.cassandra.gms.Gossiper.sendGossip(Gossiper.java:813)
	at org.apache.cassandra.gms.Gossiper.maybeGossipToUnreachableMember(Gossiper.java:840)
	at org.apache.cassandra.gms.Gossiper.access$400(Gossiper.java:86)
 {code}

LEFT and other nodes that are considered dead should not be picked for gossip with unreachable nodes.",N/A,"3.0.23, 3.11.9, 4.0"
CASSANDRA-16151,Package tools/bin scripts as executable,"The tools/bin scripts aren't packaged as executable in the source distributions, though in the repository the scripts have the right bits.

This causes, on 3.11.8 for example, the tests in org.apache.cassandra.cql3.EmptyValuesTest to fail:
{{java.io.IOException: Cannot run program ""tools/bin/sstabledump"": error=13, Permission denied}}

{{[junit-timeout] junit.framework.AssertionFailedError: java.io.IOException}}
{{[junit-timeout]         at org.apache.cassandra.cql3.EmptyValuesTest.verify(EmptyValuesTest.java:85)}}
{{[junit-timeout]         at org.apache.cassandra.cql3.EmptyValuesTest.verifyJsonInsert(EmptyValuesTest.java:112)}}

See attached patch of build.xml for the trunk and cassandra-3.11 branches.",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16146,Node state incorrectly set to NORMAL after nodetool disablegossip and enablegossip during bootstrap,"At high level, {{StorageService#setGossipTokens}} set the gossip state to {{NORMAL}} blindly. Therefore, re-enabling gossip (stop and start gossip) overrides the actual gossip state.
  
It could happen in the below scenario.
# Bootstrap failed. The gossip state remains in {{BOOT}} / {{JOINING}} and code execution exits StorageService#initServer.
# Operator runs nodetool to stop and re-start gossip. The gossip state gets flipped to {{NORMAL}}",N/A,"3.0.23, 3.11.10, 4.0-beta3, 4.0"
CASSANDRA-16136,Provide access to metrics from in-jvm dtests,"Current implementation requires of in-jvm dtests requires using callOnInstance in order to get metrics out of the in-jvm test node. Since many dtests require to check metrics, we need a version-agnostic mechanism that allows us to query the value of the metric by its published name instead of peeking into internals.",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-16128,"Jenkins: dsl for website build, logging repo SHAs, and using nightlies.a.o instead of archiving","Jenkins improvements

1. Add the cassandra-website job into cassandra_job_dsl.seed.groovy (so we don't lose it next time the Jenkins master is corrupted)

2. Print the SHAs of the different git repos used during the build process. Also store them in the .head files (so the pipeline can print them out too).

3. Instead of archiving artefacts, ssh them to https://nightlies.apache.org/cassandra/
(Disk usage on agents is largely under control, but disk usage on master was the new problem. The suspicion here is the Cassandra-*-artifact's artefacts was the disk usage culprit, though we have to evidence to support it.)",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16127,NullPointerException when calling nodetool enablethrift,"Having thrift disabled, it's impossible to enable it again without restarting the node:
{code}
$ nodetool statusthrift
not running
$ nodetool enablethrift
error: null
-- StackTrace --
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.startRPCServer(StorageService.java:392)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
	at sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
	at sun.rmi.transport.Transport$1.run(Transport.java:200)
	at sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:834)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
",N/A,"2.2.19, 3.0.23, 3.11.9"
CASSANDRA-16124,nodetool enablebinary throws exception,"I think there is a bug in 3.11.8, if you disable the Native port its not possible to enable it (without restarting Cassandra).
{quote}> nodetool statusbinary
 running
 > nodetool disablebinary
 > nodetool statusbinary
 not running
 > nodetool enablebinary
 error: Error starting native transport: setup() must be called first for CassandraDaemon
 – StackTrace –
 java.lang.RuntimeException: Error starting native transport: setup() must be called first for CassandraDaemon
 at org.apache.cassandra.service.StorageService.startNativeTransport(StorageService.java:429)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
 at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
 at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
 at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
 at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
 at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
 at sun.reflect.GeneratedMethodAccessor17.invoke(Unknown Source)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
 at sun.rmi.transport.Transport$1.run(Transport.java:200)
 at sun.rmi.transport.Transport$1.run(Transport.java:197)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
{quote}
I think this was introduced with CASSANDRA-15967. In {{CassandraDaemon.stopNativeTransport()}} {{nativeTransportService}} is set to {{null}} and when {{CassandraDaemon.startNativeTransport()}} is called the exception is thrown. By adding a call to {{CassandraDaemon.initializeNativeTransport()}} before calling {{CassandraDaemon.startNativeTransport()}} works but I'm not sure what the intention here is.",N/A,"2.2.19, 3.0.23, 3.11.9"
CASSANDRA-16114,Fix tests CQLTester.assertLastSchemaChange causes ClassCastException,"Build: https://app.circleci.com/pipelines/github/dcapwell/cassandra/494/workflows/b3765545-7b09-48dd-85ff-830c4f348329/jobs/2681

{code}
java.lang.ClassCastException: org.apache.cassandra.transport.messages.ResultMessage$Void cannot be cast to org.apache.cassandra.transport.messages.ResultMessage$SchemaChange
	at org.apache.cassandra.cql3.CQLTester.assertLastSchemaChange(CQLTester.java:916)
	at org.apache.cassandra.cql3.validation.entities.UFTest.testSchemaChange(UFTest.java:94)
{code}",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta4, 4.0"
CASSANDRA-16112,in-jvm dtests should validate Instance#serializeMessage serializeSize matches bytes written,In 3.0 sizeOf is an optimization but in 4.0 its used for message header and as such must be correct; this check is not done when mock messaging is used so may ignore mixed-mode issues.,N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16109,Don't adjust nodeCount when setting node id topology in in-jvm dtests,"We update the node count when setting the node id topology in in-jvm dtests, this should only happen if node count is smaller than the node id topology, otherwise bootstrap tests error out.",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16104,Wrong warning about data volumes capacity,"I see the following warning trying to run *nodetool upgradesstables*
{noformat}
WARN  16:09:24 Only 34988 MB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots
{noformat}
This warning is wrong because the wrong storage device capacity gets tested.

All my cassandra data paths are subdirectories of */data* mount point which has enough of space:
{noformat}
$ df -h /data
Filesystem      Size  Used Avail Mounted on
.................     1.2T 200G  1T  /data
{noformat}
However what Warning reports is a OS mount which has nothing to do with Cassandra configuration:
{noformat}
df -h /
Filesystem      Size  Used Avail Use% Mounted on
............        40G  5.7G   35G  15% /
{noformat}

I see this error running Cassandra 3.0.22",N/A,"3.0.26, 3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16094,Avoid marking shutting down nodes as up after receiving gossip shutdown message,"We have two recent failures for this test on trunk: 

1.) https://app.circleci.com/pipelines/github/maedhroz/cassandra/102/workflows/37ed8dab-9da4-4730-a883-20b7a99d88b4/jobs/518/tests (CASSANDRA-15909)
2.) https://app.circleci.com/pipelines/github/jolynch/cassandra/6/workflows/41e080e0-d7ff-4256-899e-b4010c6ef5ab/jobs/716/tests (CASSANDRA-15379)

The test expects there to be mismatches and then read repair executed on a following SELECT, but either those mismatches aren’t there, read repair isn’t happening, or both.",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16093,Cassandra website is building/including the wrong versioned nodetool docs,"For example
https://cassandra.apache.org/doc/3.11/tools/nodetool/enablefullquerylog.html

shouldn't be under the 3.11 documentation.",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16091,rpc server gets wrongly initialized with rpc_enabled:false,"After upgrading to Cassandra 3.11.8, Cassandra no longer starts. An exception is thrown:
{code:java}
 java.lang.RuntimeException: Client SSL is not supported for non-blocking sockets (hsha). Please remove client ssl from the configuration.
	at org.apache.cassandra.thrift.THsHaDisruptorServer$Factory.buildTServer(THsHaDisruptorServer.java:74)
	at org.apache.cassandra.thrift.TServerCustomFactory.buildTServer(TServerCustomFactory.java:55)
	at org.apache.cassandra.thrift.ThriftServer$ThriftServerThread.<init>(ThriftServer.java:128)
	at org.apache.cassandra.thrift.ThriftServer.start(ThriftServer.java:55)
	at org.apache.cassandra.service.CassandraDaemon.startNativeTransport(CassandraDaemon.java:713)
	at org.apache.cassandra.service.CassandraDaemon.start(CassandraDaemon.java:538)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:643)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:768)
{code}
No configuration changed between 3.11.7 and 3.11.8. rpc_enabled is false in both versions.

I created this Jira issue because clearly something changed between 3.11.7 and 3.11.8. Maybe intentional, maybe not. Changing `rpc_server_type` (which is not clearly documented to be about Thrift only) from `hsha` to `sync` does resolve the issue, as expected, but this does seem like a regression, hence the Jira issue.",N/A,"2.2.19, 3.0.23, 3.11.10"
CASSANDRA-16085,Cassandra fails to start using Java SE 8 Update 261,When using JRE 8u261 to start cassandra it fails and gives this error: \njava.lang.UnsatisfiedLinkError: WindowsTimer ,N/A,"3.0.26, 3.11.12"
CASSANDRA-16077,Only allow strings to be passed to JMX authentication,It doesn't make sense to allow other object types.,N/A,"2.1.22, 2.2.18, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-16072,Reduce thread contention in CommitLogSegment and HintsBuffer by rewriting CAS loops to atomic adds,"Follow up to CASSANDRA-15922

Both CommitLogSegment and HintsBuffer use AtomicIntegers for the current offset when allocating. Like in CASSANDRA\-15922 the loops on {{.compareAndSet(..)}} can be replaced with atomic adds using the {{. getAndAdd(..)}} method.

In highly contended environments the CAS failures can be high, starving writes in a running Cassandra node. On the same cluster CASSANDRA\-15922 was found, after CASSANDRA\-15922's fix was deployed, there was still problems around commit log flushing and hints. No flamegraph was collected that demonstrated the thread contention as clearly as was found in CASSANDRA\-15922, but the performance fix proposed here hopefully is obvious enough.",N/A,"3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-16071,max_compaction_flush_memory_in_mb is interpreted as bytes,"In CASSANDRA-12662, [~scottcarey] [reported|https://issues.apache.org/jira/browse/CASSANDRA-12662?focusedCommentId=17070055&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17070055] that the {{max_compaction_flush_memory_in_mb}} setting gets incorrectly interpreted in bytes rather than megabytes as its name implies.

{quote}
1.  the setting 'max_compaction_flush_memory_in_mb' is a misnomer, it is actually memory in BYTES.  If you take it at face value, and set it to say, '512' thinking that means 512MB,  you will produce a million temp files rather quickly in a large compaction, which will exhaust even large values of max_map_count rapidly, and get the OOM: Map Error issue above and possibly have a very difficult situation to get a cluster back into a place where nodes aren't crashing while initilaizing or soon after.  This issue is minor if you know about it in advance and set the value IN BYTES.
{quote}

",N/A,"3.11.8, 3.11.10, 4.0-beta2, 4.0-beta4, 4.0"
CASSANDRA-16066,Create tooling and update repository layout to render new website,"*We want to modernise the way the website is built*
 Rework the cassandra-website repository to generate a UI bundle containing resources that Antora will use when generating the Cassandra documents and website.

*The existing method is starting to become dated*
 The documentation and website templates are currently in markdown format. Sphinx is used to generate the Cassandra documentation and Jekyll generates the website content. One of the major issues with the existing website tooling is that the live preview server (render site as it is being updated) is really slow. There is a preview server that is really fast, however it is unable to detect changes to the content and render automatically.

*We are migrating the docs to be rendered with Antora*
 The work in CASSANDRA-16029 is converting the document templates to AsciiDoc format. Sphinx is being replaced by Antora to generate the documentation content. This change has two advantages:
 * More flexibility if the Apache Cassandra documentation look and feel needs to be updated or redesigned.
 * More modern look and feel to the documentation.

*We can use Antora to generate the website as well*
 Antora could also be used to generate the Cassandra website content. As suggested on the [mailing list|https://www.mail-archive.com/dev@cassandra.apache.org/msg15577.html] this would require the existing markdown templates to be converted to AsciiDoc as well.

*Antora needs a UI bundle to style content*
 For Antora to generate the document content and potentially the website content it requires a UI bundle (ui-bundle.zip). The UI bundle contains the HTML templates (layouts, partials, and helpers), CSS, JavaScript, fonts, and (site-wide) images. As such, it provides both the visual theme and user interactions for the documentation. Effectively the UI bundle is the templates and styling that are applied to the documentation and website content.

*The [cassandra-website|https://github.com/apache/cassandra-website] repository can be used to generate the UI bundle*
 All the resources associated with templating and styling the documentation and website can be placed in the [cassandra-website|https://github.com/apache/cassandra-website] repository. In this case the repository would serve two purposes;
 * Generation of the UI bundle resources.
 * Serve the production website content.

*The [cassandra|https://github.com/apache/cassandra] repository would contain the documentation material, while the rest of the non-versioned pages would contain that material*
 * All other material that is used to generate documentation would live in the [cassandra|https://github.com/apache/cassandra] repository. In this case Antora would run on the [cassandra/doc|https://github.com/apache/cassandra/doc] directory and pull in a UI bundle released on the GitHub [cassandra-website|https://github.com/apache/cassandra-website] repository. The content generated by Antora using the site.yml file located in this repo can be used to preview the docs for review.
 * All other non-versioned material, such as the blogs, development instructions, and third-party page would live in the GitHub [cassandra-website|https://github.com/apache/cassandra-website] repository. Antora will generate the full website using the site.yml located in this repo, using both as content sources the material located in both the [cassandra|https://github.com/apache/cassandra] and [cassandra-website|https://github.com/apache/cassandra-website] repositories.

*Design, content, and layout would remain the same*
 This proposal means the roles of each repository will change. In addition, the workflow to publish material will change as well. However, the website design, content, and layout will remain the same.

As an added bonus the tooling would allow for a live preview server that is fast and responsive. However, the time to build and generate the {{nodetool}} and Cassandra YAML AssciDoc files will still take in the order of minutes to complete.

*The [cassandra-website|https://github.com/apache/cassandra-website] repository would need to be modified*
 The following changes would need to be made to the [cassandra-website|https://github.com/apache/cassandra-website] repository:
||File/Directory||Action||Reason||
|[content/|https://github.com/apache/cassandra-website/content]|keep|Production site content served from this directory|
|[src/_data/|https://github.com/apache/cassandra-website/src/_data]|delete|_site.yml_ and _antora.yml_ include this info|
|[src/_includes/|https://github.com/apache/cassandra-website/src/_includes]|delete|Replace with UI bundle components|
|[src/_layout/|https://github.com/apache/cassandra-website/src/_layout]|delete|Replace with UI bundle components|
|[src/_plugins/|https://github.com/apache/cassandra-website/src/_plugins]|delete|Replace with UI bundle components|
|[src/_posts/|https://github.com/apache/cassandra-website/src/_posts]|move|Convert to AsciiDoc format and place in [cassandra|https://github.com/apache/cassandra] repository|
|[src/_sass/|https://github.com/apache/cassandra-website/src/_sass]|delete|Replace with UI bundle components|
|[src/_templates/|https://github.com/apache/cassandra-website/src/_templates]|move|Convert to AsciiDoc format (template format for blog posts) and place in [cassandra|https://github.com/apache/cassandra] repository|
|[src/blog/|https://github.com/apache/cassandra-website/src/blog]|delete|Replace with _index.adoc_ that will be the initial page for blogs|
|[src/css/|https://github.com/apache/cassandra-website/src/css]|delete|Replace with UI bundle components|
|[src/doc/|https://github.com/apache/cassandra-website/src/doc]|delete|Content already generated with Antora in [cassandra|https://github.com/apache/cassandra] repository|
|[src/icons/|https://github.com/apache/cassandra-website/src/icons]|keep|Probably needed for site generation but might be moved to a different folder|
|[src/img/|https://github.com/apache/cassandra-website/src/img]|keep|Probably needed for site generation but might be moved to a different folder|
|[src/js/|https://github.com/apache/cassandra-website/src/js/]|delete|Replace with UI bundle components|
|[src/Gemfile|https://github.com/apache/cassandra-website/src/Gemfile]|delete|Replace with node modules|
|[src/Gemfile.lock|https://github.com/apache/cassandra-website/src/Gemfile.lock]|delete|Replace with node modules|
|[src/Makefile|https://github.com/apache/cassandra-website/src/Makefile]|delete|Replace build mechanism with Gulp and Docker|
|[src/README|https://github.com/apache/cassandra-website/src/README]|delete|Place information and instructions in top level README|
|[src/_config.yml|https://github.com/apache/cassandra-website/src/_config.yml]|delete|Replaced by _site.yml_|
|[src/apachecon_cfp.md|https://github.com/apache/cassandra-website/src/apachecon_cfp.md]|delete??|This file is out of date|
|[src/community.md|https://github.com/apache/cassandra-website/src/community.md]|move|Convert to AsciiDoc and place in [cassandra|https://github.com/apache/cassandra] repository|
|[src/download.md|https://github.com/apache/cassandra-website/src/download.md]|move|Convert to AsciiDoc and place in [cassandra|https://github.com/apache/cassandra] repository|
|[src/index.html|https://github.com/apache/cassandra-website/src/index.html]|keep|Might need to convert to AsciiDoc and place in [cassandra|https://github.com/apache/cassandra] repository|
|[src/robot.txt|https://github.com/apache/cassandra-website/src/robot.txt]|keep|Needed by site|
|[src/third-party.md|https://github.com/apache/cassandra-website/src/third-party.md]|move|Convert to AsciiDoc and place in [cassandra|https://github.com/apache/cassandra] repository|",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-16063,Fix user experience when upgrading to 4.0 with compact tables,"The code to handle compact tables has been removed from 4.0, and the intended upgrade path to 4.0 for users having compact tables on 3.x is that they must execute {{ALTER ... DROP COMPACT STORAGE}} on all of their compact tables *before* attempting the upgrade.

Obviously, some users won't read the upgrade instructions (or miss a table) and may try upgrading despite still having compact tables. If they do so, the intent is that the node will _not_ start, with a message clearly indicating the pre-upgrade step the user has missed. The user will then downgrade back the node(s) to 3.x, run the proper {{ALTER ... DROP COMPACT STORAGE}}, and then upgrade again.

But while 4.0 does currently fail startup when finding any compact tables with a decent message, I believe the check is done too late during startup.

Namely, that check is done as we read the tables schema, so within [{{Schema.instance.loadFromDisk()}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/CassandraDaemon.java#L241].  But by then, we've _at least_ called {{SystemKeyspace.persistLocalMetadata()}}} and {{SystemKeyspaceMigrator40.migrate()}}, which will get into the commit log, and even possibly flush new {{na}} format sstables. As a results, a user might not be able to seemlessly restart the node on 3.x (to drop compact storage on the appropriate tables).

Basically, we should make sure the check for compact tables done at 4.0 startup is done as a {{StartupCheck}}, before the node does anything.

We should also add a test for this (checking that if you try upgrading to 4.0 with compact storage, you can downgrade back with no intervention whatsoever).
",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-16056,Remove jackson-mapper-asl-1.9.13 to mitigate CVE-2019-10172,"As a Cassandra consumer
 I want the jackson-mapper-asl removed
 So that I do not suffer risks that are published in that dependency

Swapping the codehause libraries over to jackson-databind resulted in CVE-2019-10172 being mitigated in 3.11. See CASSANDRA-15867;
{code:java}
Author: Stefan Miklosovic <stefan.miklosovic@instaclustr.com>  2020-06-13 16:09:00
Committer: Brandon Williams <brandonwilliams@apache.org>  2020-06-17 17:21:35
Parent: e49853914bd407827093cebf5151db0ebe2eba9e (Merge branch 'cassandra-3.0' into cassandra-3.11)
Child:  ac289270f2bb3bb7251319f7f71d6c66a4272db4 (Merge branch 'cassandra-3.0' into cassandra-3.11)
Branches: 3.11.7, cassandra-3.11, remotes/origin/cassandra-3.11, remotes/origin/trunk, trunk
Follows: cassandra-3.11.6
Precedes: cassandra-3.11.7

    update Jackson to 2.9.10
    
    Patch by Stefan Miklosovic, reviewed by brandonwilliams for
    CASSANDRA-15867

---------------------------------- build.xml ----------------------------------
index 0724dbb29c..25a47335b9 100644
@@ -406,8 +406,9 @@
           <dependency groupId=""org.slf4j"" artifactId=""jcl-over-slf4j"" version=""1.7.7"" />
           <dependency groupId=""ch.qos.logback"" artifactId=""logback-core"" version=""1.1.3""/>
           <dependency groupId=""ch.qos.logback"" artifactId=""logback-classic"" version=""1.1.3""/>
-          <dependency groupId=""org.codehaus.jackson"" artifactId=""jackson-core-asl"" version=""1.9.2""/>
-          <dependency groupId=""org.codehaus.jackson"" artifactId=""jackson-mapper-asl"" version=""1.9.2""/>
+          <dependency groupId=""com.fasterxml.jackson.core"" artifactId=""jackson-core"" version=""2.9.10""/>
+          <dependency groupId=""com.fasterxml.jackson.core"" artifactId=""jackson-databind"" version=""2.9.10.4""/>
+          <dependency groupId=""com.fasterxml.jackson.core"" artifactId=""jackson-annotations"" version=""2.9.10""/>
           <dependency groupId=""com.googlecode.json-simple"" artifactId=""json-simple"" version=""1.1""/>
           <dependency groupId=""com.boundary"" artifactId=""high-scale-lib"" version=""1.0.6""/>
           <dependency groupId=""com.github.jbellis"" artifactId=""jamm"" version=""0.3.0""/>
@@ -627,8 +628,9 @@
         <dependency groupId=""org.slf4j"" artifactId=""slf4j-api""/>
         <dependency groupId=""org.slf4j"" artifactId=""log4j-over-slf4j""/>
         <dependency groupId=""org.slf4j"" artifactId=""jcl-over-slf4j""/>
-        <dependency groupId=""org.codehaus.jackson"" artifactId=""jackson-core-asl""/>
-        <dependency groupId=""org.codehaus.jackson"" artifactId=""jackson-mapper-asl""/>
+        <dependency groupId=""com.fasterxml.jackson.core"" artifactId=""jackson-core""/>
+        <dependency groupId=""com.fasterxml.jackson.core"" artifactId=""jackson-databind""/>
+        <dependency groupId=""com.fasterxml.jackson.core"" artifactId=""jackson-annotations""/>
         <dependency groupId=""com.googlecode.json-simple"" artifactId=""json-simple""/>
         <dependency groupId=""com.boundary"" artifactId=""high-scale-lib""/>
         <dependency groupId=""org.yaml"" artifactId=""snakeyaml""/>
{code}
 ",N/A,"2.2.x, 3.0.26"
CASSANDRA-16054, Regression Test for Compact Storage Upgrades When Table Has Clustering and Value Column,Add a regression test that shows that dropping compact storage on tables that have clustering columns and a value column defined are safe after upgrading sstables in 3.0,N/A,"3.0.22, 3.11.8"
CASSANDRA-16050,Handle difference in timestamp precision between java8 and java11 in LogFIle.java,"https://bugs.openjdk.java.net/browse/JDK-8177809 made File.lastModified include milliseconds while java8 always has them as 000.

This causes problems in LogFile.java where we compare the timestamps with what we have in the logfile: [LogFile.java|https://github.com/apache/cassandra/blob/ffc8e407e087e942c4e5c40605743fe3b32d8fd5/src/java/org/apache/cassandra/db/lifecycle/LogFile.java#L234]. This means that we can throw errors when changing between java11 and 8",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-16031,Run in-jvm upgrade dtests in ci-cassandra,Add jvm-dtest-upgrade to ci-cassandra.a.o,N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-16004,When jvm dtest apis differ Circle CI's dtest_jars_build can fail to detect this and will use the jars from the older version,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/389/workflows/ea7776ac-5be0-4cb4-ab4c-61f524397c07/jobs/2019

{code}
build-test:
    [javac] Compiling 510 source files to /home/cassandra/cassandra/build/test/classes
    [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less than -source '1.8'
    [javac] /home/cassandra/cassandra/test/distributed/org/apache/cassandra/distributed/impl/RowUtil.java:49: error: no suitable constructor found for SimpleQueryResult(String[],Object[][],warnings =[...]nings)
    [javac]             return new SimpleQueryResult(names, results, warnings == null ? Collections.emptyList() : warnings);
    [javac]                    ^
    [javac]     constructor SimpleQueryResult.SimpleQueryResult(String[],Object[][]) is not applicable
    [javac]       (actual and formal argument lists differ in length)
    [javac]     constructor SimpleQueryResult.SimpleQueryResult(String[],Object[][],Predicate<Row>,int) is not applicable
    [javac]       (actual and formal argument lists differ in length)
    [javac] /home/cassandra/cassandra/test/distributed/org/apache/cassandra/distributed/test/ReplicaFilteringProtectionTest.java:212: error: cannot find symbol
    [javac]         List<String> futureWarnings = futureResult.warnings();
    [javac]                                                   ^
    [javac]   symbol:   method warnings()
    [javac]   location: variable futureResult of type SimpleQueryResult
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors
    [javac] 1 warning

BUILD FAILED
/home/cassandra/cassandra/build.xml:1176: Compile failed; see the compiler error output for details.
{code}

cassandra-3.0 compiled against the dtest jar provided by cassandra-2.2, so failed.",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-16002,jvm upgrade dtests fail on java 11 caused by bad initialization order of DatabaseDescriptor and FileUtils,"In FileUtils we check to see if we have access to some classes (specifically to set org.apache.cassandra.io.util.FileUtils#isCleanerAvailable), which can fail in java 11.  This is fine with CassandraDaemon as it will just log the failure, but in in-jvm dtests this can fail to startup an instance with the following

{code}
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.AssertionError: network topology must be assigned before using snitch
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.waitOn(IsolatedExecutor.java:209)
	at org.apache.cassandra.distributed.impl.IsolatedExecutor.lambda$sync$7(IsolatedExecutor.java:112)
	at org.apache.cassandra.distributed.impl.Instance.startup(Instance.java:592)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:209)
	at org.apache.cassandra.distributed.impl.AbstractCluster$Wrapper.startup(AbstractCluster.java:200)
	at org.apache.cassandra.distributed.upgrade.UpgradeTestBase$TestCase.run(UpgradeTestBase.java:179)
	at org.apache.cassandra.distributed.upgrade.UpgradeTest.upgradeTest(UpgradeTest.java:50)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Caused by: java.lang.RuntimeException: java.lang.AssertionError: network topology must be assigned before using snitch
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$7(Instance.java:590)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.AssertionError: network topology must be assigned before using snitch
	at org.apache.cassandra.distributed.impl.DistributedTestSnitch.getDatacenter(DistributedTestSnitch.java:90)
	at org.apache.cassandra.distributed.impl.DistributedTestSnitch.getDatacenter(DistributedTestSnitch.java:85)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:118)
	at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:488)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:137)
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:102)
	at org.apache.cassandra.utils.JVMStabilityInspector.inspectThrowable(JVMStabilityInspector.java:60)
	at org.apache.cassandra.io.util.FileUtils.<clinit>(FileUtils.java:78)
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$7(Instance.java:509)
{code}

The exception isn’t clear, but what is happening is the following

{code}
static
{
    boolean canClean = false;
    try
    {
        ByteBuffer buf = ByteBuffer.allocateDirect(1);
        ((DirectBuffer) buf).cleaner().clean();
        canClean = true;
    }
    catch (Throwable t)
    {
        JVMStabilityInspector.inspectThrowable(t);
        logger.info(""Cannot initialize un-mmaper.  (Are you using a non-Oracle JVM?)  Compacted data files will not be removed promptly.  Consider using an Oracle JVM or using standard disk access mode"");
    }
    canCleanDirectBuffers = canClean;
}
{code}

JVMStabilityInspector will check the throwable which will eventually call org.apache.cassandra.config.DatabaseDescriptor#getDiskFailurePolicy which will try to load the configs and fail",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15999,Cassandra 3.0.21 debian package is half available,"Hi,

Cassandra 3.0.21 seems to have been released in the debian package repository, as it is selected by an {{apt install cassandra}} however the {{.deb}} is not available in the repository (only the {{.dsc}}):

 
{code:java}
# apt-get install cassandra
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  cassandra-tools
The following packages will be upgraded:
  cassandra cassandra-tools
2 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.
Need to get 25.5 MB/25.5 MB of archives.
After this operation, 37.9 kB of additional disk space will be used.
Do you want to continue? [Y/n] y
Ign:1 https://dl.bintray.com/apache/cassandra 30x/main amd64 cassandra all 3.0.21
Err:1 http://www.apache.org/dist/cassandra/debian 30x/main amd64 cassandra all 3.0.21
  404  Not Found [IP: 52.58.94.94 443]
E: Failed to fetch http://www.apache.org/dist/cassandra/debian/pool/main/c/cassandra/cassandra_3.0.21_all.deb  404  Not Found [IP: 52.58.94.94 443]
E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
 {code}
 

 Indeed, there's no {{https://dl.bintray.com/apache/cassandra/dist/cassandra/debian/pool/main/c/cassandra/cassandra_3.0.21_all.deb}} file avaible. 

{{cassandra-tools}} 3.0.21 is available, and of course {{cassandra}} 3.0.20 is available though.

My current workaround is to use apt-pinning to pin to 3.0.20 until the problem is resolved.

 ",N/A,3.0.21
CASSANDRA-15994,Fix flaky python dtest test_simple_rebuild - rebuild_test.TestRebuild,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/360/workflows/8e93a655-b66e-4bf2-8866-5f9a46487763/jobs/1847

{code}
>       assert self.rebuild_errors == 1, \
            'rebuild errors should be 1, but found {}. Concurrent rebuild should not be allowed, but one rebuild command should have succeeded.'.format(self.rebuild_errors)
E       AssertionError: rebuild errors should be 1, but found 0. Concurrent rebuild should not be allowed, but one rebuild command should have succeeded.
E       assert 0 == 1
E        +  where 0 = <rebuild_test.TestRebuild object at 0x7f29fe243518>.rebuild_errors
{code}",N/A,3.0.26
CASSANDRA-15984,thrift_hsha_test.TestThriftHSHA test_closing_connections is broken on 3.0 and 3.11,"This test seems to have been broken on 3.0 for a while now; I ran Circle CI with HIGHER configs from ab6a87bf60174d9a6e7cd727702da3004c0dbeeb (from Jul 6 18:05:18 2020)  all the way to HEAD ebf9c74c4ea8aefb1262458664571fdb52b76102 (from Jul 24 18:47:39 2020).

Interestingly, when I run the test locally against latest 3.0 it passes.

This is not a flaky test, as it fails on no-vnode and vnode for every attempt (tried 9 times)

{code}
        if rc != 0:
>           raise ToolError(cmd_args, rc, out, err)
E           ccmlib.node.ToolError: Subprocess ['nodetool', '-h', 'localhost', '-p', '7100', 'enablethrift'] exited with non-zero status; exit status: 2; 
E           stderr: error: Could not create ServerSocket on address /127.0.0.1:9160.
E           -- StackTrace --
E           org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address /127.0.0.1:9160.
E           	at org.apache.thrift.transport.TNonblockingServerSocket.<init>(TNonblockingServerSocket.java:96)
E           	at org.apache.thrift.transport.TNonblockingServerSocket.<init>(TNonblockingServerSocket.java:79)
E           	at org.apache.thrift.transport.TNonblockingServerSocket.<init>(TNonblockingServerSocket.java:75)
E           	at org.apache.cassandra.thrift.TCustomNonblockingServerSocket.<init>(TCustomNonblockingServerSocket.java:39)
E           	at org.apache.cassandra.thrift.THsHaDisruptorServer$Factory.buildTServer(THsHaDisruptorServer.java:80)
E           	at org.apache.cassandra.thrift.TServerCustomFactory.buildTServer(TServerCustomFactory.java:55)
E           	at org.apache.cassandra.thrift.ThriftServer$ThriftServerThread.<init>(ThriftServer.java:128)
E           	at org.apache.cassandra.thrift.ThriftServer.start(ThriftServer.java:55)
E           	at org.apache.cassandra.service.StorageService.startRPCServer(StorageService.java:386)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E           	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.lang.reflect.Method.invoke(Method.java:498)
E           	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E           	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.lang.reflect.Method.invoke(Method.java:498)
E           	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
E           	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
E           	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
E           	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
E           	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
E           	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
E           	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
E           	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
E           	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
E           	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
E           	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
E           	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
E           	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
E           	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
E           	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
E           	at java.lang.reflect.Method.invoke(Method.java:498)
E           	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
E           	at sun.rmi.transport.Transport$1.run(Transport.java:200)
E           	at sun.rmi.transport.Transport$1.run(Transport.java:197)
E           	at java.security.AccessController.doPrivileged(Native Method)
E           	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
E           	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:573)
E           	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:834)
E           	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:688)
E           	at java.security.AccessController.doPrivileged(Native Method)
E           	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:687)
E           	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
E           	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
E           	at java.lang.Thread.run(Thread.java:748)

../env/src/ccm/ccmlib/node.py:2162: ToolError
{code}",N/A,"3.0.26, 3.11.12"
CASSANDRA-15983,DOC - Fix incorrect CREATE SCHEMA command in README.asc ,"It looks like no one has reviewed the contents of [{{README.asc}}|https://github.com/apache/cassandra/blob/cassandra-3.11.7/README.asc]. Instead of the {{CREATE KEYSPACE}} command, the example CQL reads:

{noformat}
cqlsh> CREATE SCHEMA schema1
       WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
{noformat}",N/A,"2.1.21, 2.2.18, 3.0.21, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15977,4.0 Quality: Read Repair Test Audit,"This is a subtask of CASSANDRA-15579 focusing on read repair.

[This document|https://docs.google.com/document/d/1-gldHcdLSMRbDhhI8ahs_tPeAZsjurjXr38xABVjWHE/edit?usp=sharing] lists and describes the existing functional tests for read repair, so we can have a broad view of what is currently covered. We can comment on this document and add ideas for new cases/tests, so it can gradually evolve to a more or less detailed test plan.",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15970,3.x fails to start if commit log has range tombstones from a column which is also deleted,"Cassandra crashes with the following exception

{code}
ERROR [node1_isolatedExecutor:1] node1 2020-07-21 18:59:39,048 JVMStabilityInspector.java:102 - Exiting due to error while processing commit log during initialization.
org.apache.cassandra.db.commitlog.CommitLogReplayer$CommitLogReplayException: Unexpected error deserializing mutation; saved to /var/folders/cm/08cddl2s25j7fq3jdb76gh4r0000gn/T/mutation6239873170066752296dat.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.handleReplayError(CommitLogReplayer.java:731) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayMutation(CommitLogReplayer.java:656) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.replaySyncSection(CommitLogReplayer.java:609) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:493) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:189) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:170) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:151) [dtest-3.0.21.jar:na]
	at org.apache.cassandra.distributed.impl.Instance.lambda$startup$7(Instance.java:535) [dtest-3.0.21.jar:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_242]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_242]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_242]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83) ~[dtest-3.0.21.jar:na]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_242]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.db.ClusteringComparator.validate(ClusteringComparator.java:206) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.partitions.PartitionUpdate.validate(PartitionUpdate.java:494) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayMutation(CommitLogReplayer.java:629) [dtest-3.0.21.jar:na]
	... 12 common frames omitted
{code}

If you drain in 2.2 before upgrade, you get the following

{code}
ERROR [SharedPool-Worker-1] node1 2020-07-21 22:17:25,661 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,node1]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2537) ~[dtest-3.0.21.jar:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_242]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [dtest-3.0.21.jar:na]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_242]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.db.ClusteringComparator.compare(ClusteringComparator.java:131) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer.compareNextTo(UnfilteredDeserializer.java:391) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.handlePreSliceData(SSTableIterator.java:105) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.hasNextInternal(SSTableIterator.java:164) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:336) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$1.hasNext(ClusteringIndexNamesFilter.java:157) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIterator.isEmpty(UnfilteredRowIterator.java:70) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndSSTablesInTimestampOrder(SinglePartitionReadCommand.java:952) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:679) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:656) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:491) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:418) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1793) ~[dtest-3.0.21.jar:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2533) ~[dtest-3.0.21.jar:na]
	... 4 common frames omitted
{code}

The issue found was as follows

* The schema had a collection type in 2.1
* a collection range tombstone happened in 2.1
* the row only had the RT, no other cells
* the collection was dropped in 2.1
* 3.0 detected the collection was deleted and ignored the cell
* 3.0 produced an empty row with a null clustering key (since we skipped the RT)",N/A,"3.0.22, 3.11.8"
CASSANDRA-15967,Add support in jvm dtest to test thrift,"In 2.2 and 3.x thrift is supported and can cause problems for the non thrift code.  To make sure we can test these interactions, need to add support for thrift in jvm dtest.",N/A,"2.2.18, 3.0.22, 3.11.8"
CASSANDRA-15962,Digest for some queries is different depending whether the data are retrieved from sstable or memtable,"Not sure into which category should I assign this ticket.

 

Basically when reading using certain column filters, the digest is different depending whether we read from sstable and memtable. This happens on {{trunk}} and {{cassandra-3.11}} branches. However it works properly on {{cassandra-3.0}} branch.

 

I'm attaching a simple test for trunk to demonstrate what I mean. 

 

Please verify my test and my conclusions

 ",N/A,"3.11.10, 4.0-rc1, 4.0"
CASSANDRA-15952,TestBootstrap::test_bootstrap_binary_disabled_resumable_bootstrap checks non-existent log messages for 3.0 and 3.11,"CASSANDRA-14525 added a bit of logging only in trunk that {{TestBootstrap::test_bootstrap_binary_disabled_resumable_bootstrap}} assumes is present in all versions. This should be as simple as making sure we only assert around that on 4.0+.

See https://app.circleci.com/pipelines/github/maedhroz/cassandra/57/workflows/b408ce8a-ee68-47bf-b9f1-eb9541e9827e/jobs/312",N/A,"3.0.22, 3.11.8"
CASSANDRA-15948,Multiple failures in cqlsh_tests.test_cqlsh.TestCqlsh,"At least the following tests in cqlsh_tests.test_cqlsh.TestCqlsh need to be investigated:

{{test_describe_types}}
 {{test_describe_functions}}
 {{test_pycodestyle_compliance}}

For a recent example of this, see [https://app.circleci.com/pipelines/github/maedhroz/cassandra/51/workflows/4fd639f6-6523-4520-961e-5b5c384a13b3/jobs/281]. These three tests fail locally and consistently against {{cassandra-3.0}}, but not on {{trunk}}.

 {{test_pycodestyle_compliance}} seems to fail only on 3.0 and 3.11.",N/A,"3.0.22, 3.11.8"
CASSANDRA-15938,Fix support for adding UDT fields to clustering keys,"Adding UDT fields to clustering keys is broken in all versions, however slightly differently.

In 4.0, there will be a brief moment while schema changes are propagated during which we won’t be able to decode and compare byte sequences. Unfortunately, it is unclear what we should do in such cases, since we can’t just come up with a comparator, and we can’t ignore non-null trailing values, since this will lead to cases where compare for tuples `a;1` and `a;2` would return 0, effectively making them equal, and we don’t know how to compare unknown trailing values. Probably we should reject such query since we can’t sort correctly, but we should make the error message more descriptive than just ""Index 1 out of bounds for length 1”. The only problem is that we get this exception only on flush right now, so data already propagates to the node by that time.

In 3.0, the problem is a bit worse than that, since in 3.0 we do not ignore trailing nulls, so some of the values, written before `ALTER TYPE .. ADD` become inaccessible. Both old values, and the new ones should always be accessible.
",N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15934,In-jvm dtests use -1 as timestamp for all writes that don't specify 'USING TIMESTAMP',,N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15933,Forbid adding new fields to UDTs used in partition key columns,,N/A,"3.0.21, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15925,Jenkins pipeline can copy wrong test report artefacts from stage builds,"Spotted in https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/196/console

Looks like copyArtifact will need to be specific to a build.",N/A,"2.2.18, 3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15924,Avoid emitting empty range tombstones from RangeTombstoneList,"In {{RangeTombstoneList#iterator}} there is a chance we emit empty range tombstones depending on the slice passed in. This can happen during read repair with either an empty slice or with paging and the final page being empty.

This creates problems in RTL if we try to insert a new range tombstone which covers the empty ones;
{code}
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:541)
	at org.apache.cassandra.db.RangeTombstoneList.addAll(RangeTombstoneList.java:217)
	at org.apache.cassandra.db.MutableDeletionInfo.add(MutableDeletionInfo.java:141)
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:137)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:254)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1210)
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:573)
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:582)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:572)
{code}",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15922,High CAS failures in NativeAllocator.Region.allocate(..) ,"h4. Problem

The method {{NativeAllocator.Region.allocate(..)}} uses an {{AtomicInteger}} for the current offset in the region. Allocations depends on a {{.compareAndSet(..)}} call.

In highly contended environments the CAS failures can be high, starving writes in a running Cassandra node.

h4. Example

It has been witnessed up to 33% of CPU time stuck in the {{NativeAllocator.Region.allocate(..)}} loop (due to the CAS failures) during a heavy spark analytics write load.

These nodes: 40 CPU cores and 256GB ram; have relevant settings
 - {{memtable_allocation_type: offheap_objects}}
 - {{memtable_offheap_space_in_mb: 5120}}
 - {{concurrent_writes: 160}}

Numerous  flamegraphs demonstrate the problem. See attached [^profile_pbdpc23zafsrh_20200702.svg].

h4. Suggestion: ThreadLocal Regions

One possible solution is to have separate Regions per thread.  
Code wise this is relatively easy to do, for example replacing NativeAllocator:59 
{code}private final AtomicReference<Region> currentRegion = new AtomicReference<>();{code}
with
{code}private final ThreadLocal<AtomicReference<Region>> currentRegion = new ThreadLocal<>() {...};{code}

But this approach substantially changes the allocation behaviour, with more than concurrent_writes number of Regions in use at any one time. For example with {{concurrent_writes: 160}} that's 160+ regions, each of 1MB. 

h4. Suggestion: Simple Contention Management Algorithm (Constant Backoff)

Another possible solution is to introduce a contention management algorithm that a) reduces CAS failures in high contention environments, b) doesn't impact normal environments, and c) keeps the allocation strategy of using one region at a time.

The research paper [arXiv:1305.5800|https://arxiv.org/abs/1305.5800] describes this contention CAS problem and demonstrates a number of algorithms to apply. The simplest of these algorithms is the Constant Backoff CAS Algorithm.

Applying the Constant Backoff CAS Algorithm involves adding one line of code to {{NativeAllocator.Region.allocate(..)}} to sleep for one (or some constant number) nanoseconds after a CAS failure occurs.
That is...
{code}
    // we raced and lost alloc, try again
    LockSupport.parkNanos(1);
{code}

h4. Constant Backoff CAS Algorithm Experiments

Using the code attached in NativeAllocatorRegionTest.java the concurrency and CAS failures of {{NativeAllocator.Region.allocate(..)}} can be demonstrated. 

In the attached [^NativeAllocatorRegionTest.java] class, which can be run standalone, the {{Region}} class: copied from {{NativeAllocator.Region}}; has also the {{casFailures}} field added. The following two screenshots are from data collected from this class on a 6 CPU (12 core) MBP, running the {{NativeAllocatorRegionTest.testRegionCAS}} method.

This attached screenshot shows the number of CAS failures during the life of a Region (over ~215 million allocations), using different threads and park times. This illustrates the improvement (reduction) of CAS failures from zero park time, through orders of magnitude, up to 10000000ns (10ms). The biggest improvement is from no algorithm to a park time of 1ns where CAS failures are ~two orders of magnitude lower. From a park time 10μs and higher there is a significant drop also at low contention rates.

 !Screen Shot 2020-07-05 at 13.16.10.png|width=500px! 

This attached screenshot shows the time it takes to fill a Region (~215 million allocations), using different threads and park times. The biggest improvement is from no algorithm to a park time of 1ns where performance is one order of magnitude faster. From a park time of 100μs and higher there is a even further significant drop, especially at low contention rates.

 !Screen Shot 2020-07-05 at 13.26.17.png|width=500px! 

Repeating the test run show reliably similar results:  [^Screen Shot 2020-07-05 at 13.37.01.png]  and  [^Screen Shot 2020-07-05 at 13.35.55.png].

h4. Region Per Thread Experiments

Implementing Region Per Thread: see the {{NativeAllocatorRegionTest.testRegionThreadLocal}} method; we can expect zero CAS failures of the life of a Region. For performance we see two orders of magnitude lower times to fill up the Region (~420ms).

 !Screen Shot 2020-07-05 at 13.48.16.png|width=200px! 

h4. Costs

Region per Thread is an unrealistic solution as it introduces many new issues and problems, from increased memory use to leaking memory and GC issues. It is better tackled as part of a TPC implementation.

The backoff approach is simple and elegant, and seems to improve throughput in all situations. It does introduce context switches which may impact throughput in some busy throughput scenarios, so this should to be tested further.",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15907,Operational Improvements & Hardening for Replica Filtering Protection,"CASSANDRA-8272 uses additional space on the heap to ensure correctness for 2i and filtering queries at consistency levels above ONE/LOCAL_ONE. There are a few things we should follow up on, however, to make life a bit easier for operators and generally de-risk usage:

(Note: Line numbers are based on {{trunk}} as of {{3cfe3c9f0dcf8ca8b25ad111800a21725bf152cb}}.)

*Minor Optimizations*

* {{ReplicaFilteringProtection:114}} - Given we size them up-front, we may be able to use simple arrays instead of lists for {{rowsToFetch}} and {{originalPartitions}}. Alternatively (or also), we may be able to null out references in these two collections more aggressively. (ex. Using {{ArrayList#set()}} instead of {{get()}} in {{queryProtectedPartitions()}}, assuming we pass {{toFetch}} as an argument to {{querySourceOnKey()}}.)
* {{ReplicaFilteringProtection:323}} - We may be able to use {{EncodingStats.merge()}} and remove the custom {{stats()}} method.
* {{DataResolver:111 & 228}} - Cache an instance of {{UnaryOperator#identity()}} instead of creating one on the fly.
* {{ReplicaFilteringProtection:217}} - We may be able to scatter/gather rather than serially querying every row that needs to be completed. This isn't a clear win perhaps, given it targets the latency of single queries and adds some complexity. (Certainly a decent candidate to kick even out of this issue.)

*Documentation and Intelligibility*

* There are a few places (CHANGES.txt, tracing output in {{ReplicaFilteringProtection}}, etc.) where we mention ""replica-side filtering protection"" (which makes it seem like the coordinator doesn't filter) rather than ""replica filtering protection"" (which sounds more like what we actually do, which is protect ourselves against incorrect replica filtering results). It's a minor fix, but would avoid confusion.
* The method call chain in {{DataResolver}} might be a bit simpler if we put the {{repairedDataTracker}} in {{ResolveContext}}.

*Testing*

* I want to bite the bullet and get some basic tests for RFP (including any guardrails we might add here) onto the in-JVM dtest framework.

*Guardrails*

* As it stands, we don't have a way to enforce an upper bound on the memory usage of {{ReplicaFilteringProtection}} which caches row responses from the first round of requests. (Remember, these are later used to merged with the second round of results to complete the data for filtering.) Operators will likely need a way to protect themselves, i.e. simply fail queries if they hit a particular threshold rather than GC nodes into oblivion. (Having control over limits and page sizes doesn't quite get us there, because stale results _expand_ the number of incomplete results we must cache.) The fun question is how we do this, with the primary axes being scope (per-query, global, etc.) and granularity (per-partition, per-row, per-cell, actual heap usage, etc.). My starting disposition   on the right trade-off between performance/complexity and accuracy is having something along the lines of cached rows per query. Prior art suggests this probably makes sense alongside things like {{tombstone_failure_threshold}} in {{cassandra.yaml}}.",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15906,Queries on KEYS 2i are broken by DROP COMPACT STORAGE on 3.0,"From 3.0 onwards, the declared columns of a thrift table are internally static columns. While the table is compact, this 

After DROP COMPACT STORAGE is used on a table that has a KEYS 2i, queries that uses that index will start failing with:
{noformat}
Queries using 2ndary indexes don't support selecting only static columns
{noformat}

In 3.0, we don't support index on static columns and have that validation that rejects 2i queries on static columns. But the declared columns of compact table are static under the hood, and while this specific validation is skipped while the table is compact, it isn't anymore after the DROP COMPACT STORAGE.

Note that internally, nothing changes with the DROP COMPACT STORAGE, and the 2i queries would still work as well as before, it is just that they are rejected.

Also not that this is only a problem in 3.0. In 3.11, static column indexes were added (CASSANDRA-8103) and thus this validation has been removed, and everything works as it should.

However, since DROP COMPACT STORAGE is a mandatory step for compact tables before upgrading to 4.0, fixing this annoying in 3.0 would avoid forcing users with KEYS 2i on 3.0 to upgrade to 3.11 before going to 4.0.
",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15905,cqlsh not able to fetch all rows when in batch mode,"The cqlsh in trunk only display the first page when running in the batch mode, i.e. using {{--execute}} or {{--file}} option. 
  
 It is a change of behavior. In 3.x branches, the cqlsh returns all rows. 
  
 It can be reproduced in 3 steps.
{code:java}
 1. ccm create trunk -v git:trunk -n1 && ccm start
 2. tools/bin/cassandra-stress write n=1k -schema keyspace=""keyspace1""   // write 1000 rows
 3. bin/cqlsh -e ""SELECT * FROM keyspace1.standard1;""                    // fetch all rows
{code}
 
 There are 1000 rows written. But the output in step 3 will only list 100 rows, which is the first page. 
{code:java}
➜ bin/cqlsh -e ""SELECT * FROM keyspace1.standard1"" | wc -l
     105{code}
 
 The related change was introduced in https://issues.apache.org/jira/browse/CASSANDRA-11534, where the cqlsh.py script no longer fetch all rows when not using tty in the print_result method. ",N/A,"3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15902,OOM because repair session thread not closed when terminating repair,"In our cluster, after a while some nodes running slowly out of memory. On that nodes we observed that Cassandra Reaper terminate repairs with a JMX call to {{StorageServiceMBean.forceTerminateAllRepairSessions()}} because reaching timeout of 30 min.

In the memory heap dump we see lot of instances of {{io.netty.util.concurrent.FastThreadLocalThread}} occupy most of the memory:
{noformat}
119 instances of ""io.netty.util.concurrent.FastThreadLocalThread"", loaded by ""sun.misc.Launcher$AppClassLoader @ 0x51a800000"" occupy 8.445.684.480 (93,96 %) bytes. {noformat}
In the thread dump we see lot of repair threads:
{noformat}
grep ""Repair#"" threaddump.txt | wc -l
      50 {noformat}
 

The repair jobs are waiting for the validation to finish:
{noformat}
""Repair#152:1"" #96170 daemon prio=5 os_prio=0 tid=0x0000000012fc5000 nid=0x542a waiting on condition [0x00007f81ee414000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007939bcfc8> (a com.google.common.util.concurrent.AbstractFuture$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:285)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:137)
        at com.google.common.util.concurrent.Futures.getUnchecked(Futures.java:1509)
        at org.apache.cassandra.repair.RepairJob.run(RepairJob.java:160)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$13/480490520.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:748) {noformat}
 

Thats the line where the threads stuck:
{noformat}
// Wait for validation to complete
Futures.getUnchecked(validations); {noformat}
 

The call to {{StorageServiceMBean.forceTerminateAllRepairSessions()}} stops the thread pool executor. It looks like that futures which are in progress will therefor never be completed and the repair thread waits forever and won't be finished.

 

Environment:

Cassandra version: 3.11.4 and 3.11.6

Cassandra Reaper: 1.4.0

JVM memory settings:
{noformat}
-Xms11771M -Xmx11771M -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+ParallelRefProcEnabled -XX:MaxMetaspaceSize=100M {noformat}
on another cluster with same issue:
{noformat}
-Xms31744M -Xmx31744M -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:+ParallelRefProcEnabled -XX:MaxMetaspaceSize=100M {noformat}
Java Runtime:
{noformat}
openjdk version ""1.8.0_212""
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_212-b03)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.212-b03, mixed mode) {noformat}
 

The same issue described in this comment: https://issues.apache.org/jira/browse/CASSANDRA-14355?focusedCommentId=16992973&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16992973

As suggested in the comments I created this new specific ticket.",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15899,Dropping a column can break queries until the schema is fully propagated,"With a table like:
{code}
CREATE TABLE ks.tbl (id int primary key, v1 int, v2 int, v3 int)
{code}

and we drop {{v2}}, we get this exception on the replicas which haven't seen the schema change:
{code}
ERROR [SharedPool-Worker-1] node2 2020-06-24 09:49:08,107 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,node2]
java.lang.IllegalStateException: [ColumnDefinition{name=v1, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, position=-1}, ColumnDefinition{name=v2, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, position=-1}, ColumnDefinition{name=v3, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, position=-1}] is not a subset of [v1 v3]
	at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:546) ~[main/:na]
	at org.apache.cassandra.db.Columns$Serializer.serializeSubset(Columns.java:478) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:184) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:114) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:102) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:132) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[main/:na]
...
{code}

Note that it doesn't matter if we {{SELECT *}} or {{SELECT id, v1}}",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15898,cassandra 3.11.4 deadlock,"We are running apache-cassandra-3.11.4, 10 node cluster with -Xms32G -Xmx32G -Xmn8G using CMS.
after running couple of days one of the node become unresponsive and threaddump (jstack -F) shows deadlock.


Found one Java-level deadlock:
=============================

""Native-Transport-Requests-144"": waiting to lock Monitor@0x00007cd5142e4d08 (Object@0x00007f6e00348268, a java/io/ExpiringCache),
 which is held by ""CompactionExecutor:115134""
""CompactionExecutor:115134"": waiting to lock Monitor@0x00007f6bcaf130f8 (Object@0x00007f6dff31faa0, a ch/qos/logback/core/joran/spi/ConfigurationWatchList),
 which is held by ""Native-Transport-Requests-144""

Found a total of 1 deadlock.

I have seen this couple of time now with different nodes with following in system.log

IndexSummaryRedistribution.java:77 - Redistributing index summaries
 NoSpamLogger.java:91 - Maximum memory usage reached (536870912), cannot allocate chunk of 1048576

also lookin in gc log there has not been a ParNew collection for last 10hrs, only CMS collections.

1739842.375: [GC (CMS Final Remark) [YG occupancy: 2712269 K (7549760 K)]
1739842.375: [Rescan (parallel) , 0.0614157 secs]
1739842.437: [weak refs processing, 0.0000994 secs]
1739842.437: [class unloading, 0.0231076 secs]
1739842.460: [scrub symbol table, 0.0061049 secs]
1739842.466: [scrub string table, 0.0043847 secs][1 CMS-remark: 17696837K(25165824K)] 20409107K(32715584K), 0.0953750 secs] [Times: user=2.95 sys=0.00, real=0.09 secs]
1739842.471: [CMS-concurrent-sweep-start]
1739848.572: [CMS-concurrent-sweep: 6.101/6.101 secs] [Times: user=6.13 sys=0.00, real=6.10 secs]
1739848.573: [CMS-concurrent-reset-start]
1739848.645: [CMS-concurrent-reset: 0.072/0.072 secs] [Times: user=0.08 sys=0.00, real=0.08 secs]
1739858.653: [GC (CMS Initial Mark) [1 CMS-initial-mark: 17696837K(25165824K)] 
20409111K(32715584K), 0.0584838 secs] [Times: user=2.68 sys=0.00, real=0.06 secs]
1739858.713: [CMS-concurrent-mark-start]
1739860.496: [CMS-concurrent-mark: 1.784/1.784 secs] [Times: user=84.77 sys=0.00, real=1.79 secs]
1739860.497: [CMS-concurrent-preclean-start]
1739860.566: [CMS-concurrent-preclean: 0.070/0.070 secs] [Times: user=0.07 sys=0.00, real=0.07 secs]
1739860.567: [CMS-concurrent-abortable-preclean-start]CMS: abort preclean due to time
1739866.333: [CMS-concurrent-abortable-preclean: 5.766/5.766 secs] [Times: user=5.80 sys=0.00, real=5.76 secs]

Java HotSpot(TM) 64-Bit Server VM (25.162-b12) for linux-amd64 JRE (1.8.0_162-b12)
Memory: 4k page, physical 792290076k(2780032k free), swap 16777212k(16693756k free)

CommandLine flags:
-XX:+AlwaysPreTouch
-XX:CICompilerCount=15
-XX:+CMSClassUnloadingEnabled
-XX:+CMSEdenChunksRecordAlways
-XX:CMSInitiatingOccupancyFraction=40
-XX:+CMSParallelInitialMarkEnabled
-XX:+CMSParallelRemarkEnabled
-XX:CMSWaitDuration=10000
-XX:ConcGCThreads=50
-XX:+CrashOnOutOfMemoryError
-XX:GCLogFileSize=10485760
-XX:+HeapDumpOnOutOfMemoryError
-XX:InitialHeapSize=34359738368
-XX:InitialTenuringThreshold=1
-XX:+ManagementServer
-XX:MaxHeapSize=34359738368
-XX:MaxNewSize=8589934592
-XX:MaxTenuringThreshold=1
-XX:MinHeapDeltaBytes=196608
-XX:NewSize=8589934592
-XX:NumberOfGCLogFiles=10
-XX:OldPLABSize=16
-XX:OldSize=25769803776
-XX:OnOutOfMemoryError=kill -9 %p
-XX:ParallelGCThreads=50
-XX:+PerfDisableSharedMem
-XX:+PrintGC
-XX:+PrintGCDetails
-XX:+PrintGCTimeStamps
-XX:+ResizeTLAB
-XX:StringTableSize=1000003
-XX:SurvivorRatio=8
-XX:ThreadPriorityPolicy=42
-XX:ThreadStackSize=256
-XX:-UseBiasedLocking
-XX:+UseCMSInitiatingOccupancyOnly
-XX:+UseConcMarkSweepGC
-XX:+UseCondCardMark
-XX:+UseFastUnorderedTimeStamps
-XX:+UseGCLogFileRotation
-XX:+UseNUMA
-XX:+UseNUMAInterleaving
-XX:+UseParNewGC
-XX:+UseTLAB
-XX:+UseThreadPriorities",N/A,3.11.11
CASSANDRA-15897,Dropping compact storage with 2.1-sstables on disk make them unreadable,Test reproducing: https://github.com/krummas/cassandra/commits/marcuse/dropcompactstorage,N/A,"3.0.25, 3.11.11, 4.0-rc1, 4.0"
CASSANDRA-15896,NullPointerException in SELECT JSON statement when a UUID field contains an empty string,"It seems that Cassandra accept empty strings """" ( FROM JSON string ) for UUID fields but crash when asking for JSON serialization of those fields.

 

Cassandra version 3.6.11.6 running in docker from official Dockerhub image.

Java driver:
{code:java}
<!-- https://mvnrepository.com/artifact/com.datastax.oss/java-driver-core -->
<dependency>
    <groupId>com.datastax.oss</groupId>
    <artifactId>java-driver-core</artifactId>
    <version>4.7.0</version>
</dependency>
{code}
The attached code is to allow bug reproducibility:
{code:java}
package com.foo.bar;
import com.datastax.oss.driver.api.core.CqlSession;
import com.datastax.oss.driver.api.core.CqlSessionBuilder;
import com.datastax.oss.driver.api.core.cql.PreparedStatement;
import com.datastax.oss.driver.api.core.cql.ResultSet;
import com.datastax.oss.driver.api.core.cql.Row;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.net.InetSocketAddress;
import java.net.URI;
import java.util.*;

import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotNull;

/**
 * @author Domenico Lupinetti <ostico@gmail.com> - 23/06/2020
 */
public class NullPointerExceptionTest {

    protected String uuid;
    protected CqlSession cqlSession;

    @Before
    public void setUp() throws Exception {

        URI node = new URI( ""tcp://localhost:9042"" );
        final CqlSessionBuilder builder = CqlSession.builder();

        cqlSession = builder.addContactPoint( new InetSocketAddress(
                node.getHost(),
                node.getPort()
        ) ).withLocalDatacenter( ""datacenter1"" ).build();

        cqlSession.execute( ""CREATE KEYSPACE IF NOT EXISTS test_suite WITH replication = {'class':'SimpleStrategy','replication_factor':1};"" );

        String sb = ""CREATE TABLE IF NOT EXISTS test_suite.test ( id uuid PRIMARY KEY, another_id uuid, subject text );"";

        cqlSession.execute( sb );
        PreparedStatement stm = cqlSession.prepare( ""INSERT INTO test_suite.test JSON :payload"" );

        this.uuid = UUID.randomUUID().toString();

        HashMap<String, String> payload = new HashMap<>();
        payload.put( ""id"", this.uuid );

        // ******* This exception do not happens if the field is set as NULL
        payload.put( ""another_id"", """" );  //<------ EMPTY STRING AS UUID
        payload.put( ""subject"", ""Alighieri, Dante. Divina Commedia"" );

        ObjectMapper objM = new ObjectMapper();
        cqlSession.execute(
                stm.bind().setString( ""payload"", objM.writeValueAsString( payload ) )
        );  //<------ serialize as JSON

    }

    @After
    public void tearDown() throws Exception {
        cqlSession.execute( ""DROP TABLE IF EXISTS test_suite.test;"" );
        cqlSession.execute( ""DROP KEYSPACE test_suite;"" );
        cqlSession.close();
    }

    @Test
    public void testNullPointer() {

        PreparedStatement stmt       = cqlSession.prepare( ""SELECT JSON id, another_id FROM test_suite.test where id = :id;"" );
        ResultSet         resultSet  = cqlSession.execute( stmt.bind().setUuid( ""id"", UUID.fromString( this.uuid ) ) ); // <------ EXCEPTION
        Row               r          = resultSet.one();

        assertNotNull( r );
        assertNotNull( r.getString( ""[json]"" ) );
        assertFalse( Objects.requireNonNull( r.getString( ""[json]"" ) ).isEmpty() );

    }

}


{code}
Client stack Trace:
{code:java}
com.datastax.oss.driver.api.core.servererrors.ServerError: java.lang.NullPointerExceptioncom.datastax.oss.driver.api.core.servererrors.ServerError: java.lang.NullPointerException
 at com.datastax.oss.driver.api.core.servererrors.ServerError.copy(ServerError.java:54) at com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149) at com.datastax.oss.driver.internal.core.cql.CqlRequestSyncProcessor.process(CqlRequestSyncProcessor.java:53) at com.datastax.oss.driver.internal.core.cql.CqlRequestSyncProcessor.process(CqlRequestSyncProcessor.java:30) at com.datastax.oss.driver.internal.core.session.DefaultSession.execute(DefaultSession.java:230) at com.datastax.oss.driver.api.core.cql.SyncCqlSession.execute(SyncCqlSession.java:53) at com.foo.bar.NullPointerExceptionTest.testNullPointer(NullPointerExceptionTest.java:74) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63) at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329) at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293) at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306) at org.junit.runners.ParentRunner.run(ParentRunner.java:413) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}
Cassandra stack Trace:

 
{code:java}
ERROR [Native-Transport-Requests-1] 2020-06-23 09:57:48,074 ErrorMessage.java:384 - Unexpected exception during requestERROR [Native-Transport-Requests-1] 2020-06-23 09:57:48,074 ErrorMessage.java:384 - Unexpected exception during requestjava.lang.NullPointerException: null at org.apache.cassandra.db.marshal.AbstractType.toJSONString(AbstractType.java:156) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.selection.Selection.rowToJson(Selection.java:343) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:494) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:477) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:794) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:438) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:416) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:289) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:117) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:225) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:532) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:509) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:146) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.transport.Message$Dispatcher.processRequest(Message.java:686) [apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.transport.Message$Dispatcher.lambda$channelRead0$0(Message.java:592) [apache-cassandra-3.11.6.jar:3.11.6] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_252] at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165) ~[apache-cassandra-3.11.6.jar:3.11.6] at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:113) ~[apache-cassandra-3.11.6.jar:3.11.6] at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_252]
{code}",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15890,Add token to tombstone warning and error log message,"If Cassandra scans too many tombstones while reading a partition, then it prints log messages with query based on warning/failure thresholds. The token is not printed in the log message. If tombstones are hurting the instance/replica set, then running force compaction for the partition (""nodetool compact"" using start and end tokens i.e. token -/+ some delta) is one of the actions taken to recover. In order to find out the token, someone has to manually connect to cluster and run SELECT TOKEN query. Printing token with the log message helps to avoid manual effort and execute force compaction quickly.",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15889,Debian package fails to download on Arm-based hosts,"Following the first three steps of the [Debian install process|https://cassandra.apache.org/download/], after an apt-get update you'll see this line:
{code:bash}
$ sudo apt-get update
...
N: Skipping acquire of configured file 'main/binary-arm64/Packages' as repository 'https://downloads.apache.org/cassandra/debian 311x InRelease' doesn't support architecture 'arm64'
{code}

Checking the [Debian repo|https://dl.bintray.com/apache/cassandra/dists/311x/main/] confirms there is no aarch64 variant available.

Should you then attempt to install Cassandra:
{code:bash}
$ sudo apt-get install cassandra
Reading package lists... Done
Building dependency tree
Reading state information... Done
Package cassandra is not available, but is referred to by another package.
This may mean that the package is missing, has been obsoleted, or
is only available from another source

E: Package 'cassandra' has no installation candidate
{code}

Note that there is a workaround available: if you specify ""amd64"" as the arch for the source, it downloads and runs on Arm without issue:
{code:bash}
echo ""deb [arch=amd64] https://downloads.apache.org/cassandra/debian 311x main"" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list
{code}

The Redhat RPM contains a ""noarch"" arch type, so it will download on any host. (Cassandra does not use separate binaries/releases for different architectures, so this seems to be the correct approach, but adding an aarch64 variant would also suffice.)",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-15881,Flaky unit test: SASIIndexTest.testInsertingIncorrectValuesIntoAgeIndex,"The unit test {{SASIIndexTest.testInsertingIncorrectValuesIntoAgeIndex}} seems to be flaky in 3.11, as it can be seen in [cassandra-ci|https://ci-cassandra.apache.org/view/Cassandra%203.11/job/Cassandra-3.11-test/42/testReport/org.apache.cassandra.index.sasi/SASIIndexTest/testInsertingIncorrectValuesIntoAgeIndex/] and also locally. Trunk doesn't seem to be affected.

{{SASIIndexTest.testIndexMemtableSwitching}} is [also flaky|https://ci-cassandra.apache.org/view/Cassandra%203.11/job/Cassandra-3.11-test/42/testReport/org.apache.cassandra.index.sasi/SASIIndexTest/testIndexMemtableSwitching/], although I haven't been able to reproduce it running it separately, but only when running the entire {{SASIIndexTest}}.

These could have been introduced when merging up CASSANDRA-15778, since they failed for their [CircleCI run|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra/17/workflows/0442eebe-c764-41c5-ba06-6617bcb9fc5f/jobs/2213], and they didn't seem to fail before that, or at least I cannot reproduce them locally with the previous commit.",N/A,3.11.12
CASSANDRA-15880,Memory leak in CompressedChunkReader,"CompressedChunkReader uses java.lang.ThreadLocal to reuse ByteBuffer for compressed data. ByteBuffers leak due to peculiar ThreadLocal quality.
ThreadLocals are stored in a map, where the key is a weak reference to a ThreadLocal and the value is the user's object (ByteBuffer in this case). When a last strong reference to a ThreadLocal is lost, weak reference to ThreadLocal (key) is removed but the value (ByteBuffer) is kept until cleaned by ThreadLocal heuristic expunge mechanism. See ThreadLocal's ""stale entries"" for details.

When a number of long-living threads is high enough this results in thousands of ByteBuffers stored as stale entries in ThreadLocals. In a not-so-lucky scenario we get OutOfMemoryException.",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15879,Flaky unit test: CorruptedSSTablesCompactionsTest,"CASSANDRA-14238 addressed the failure in {{BlacklistingCompactionsTest.testBlacklistingWithSizeTieredCompactionStrategy}}, but only on 2.2. While working on CASSANDRA-14888, we’ve reproduced [the failure|https://app.circleci.com/pipelines/github/dineshjoshi/cassandra/47/workflows/de5f7cdb-06b6-4869-9d19-81a145e79f3f/jobs/2516/tests] on trunk.

It looks like this should be a clean merge forward.",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15873,Update Netty 4.0.44 -> 4.1.50 (fix security/performance issues),"See https://issues.apache.org/jira/browse/CASSANDRA-15868 for the same issue on 4.0 / trunk. Attached is an OWASP dependency report for Netty 4.0.44, which identifies 3 of the same vulnerabilities as above.

 

Additionally, 4.1.50 contains aarch64 native libraries which can improve performance on ARM processors. 

 ",N/A,3.11.x
CASSANDRA-15870,When 3.0 reads 2.1 data with a regular column set<text> it expects the cellName to contain a element and fails if not true,"{code}
java.lang.AssertionError
	at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:48)
	at org.apache.cassandra.db.LegacyLayout$CellGrouper.addCell(LegacyLayout.java:1461)
	at org.apache.cassandra.db.LegacyLayout$CellGrouper.addAtom(LegacyLayout.java:1380)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.readRow(UnfilteredDeserializer.java:549)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.hasNext(UnfilteredDeserializer.java:523)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer.hasNext(UnfilteredDeserializer.java:336)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.readStaticRow(SSTableSimpleIterator.java:133)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:59)
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:364)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.isReverseOrder(LazilyInitializedUnfilteredRowIterator.java:65)
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:132)
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:123)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:207)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:160)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:174)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:93)
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:240)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:191)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:100)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:345)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83)
	at java.lang.Thread.run(Thread.java:748)
{code}

This exception is similar to other JIRA such as CASSANDRA-14113 but under root causing both exceptions, they only share the same symptom and not the same root cause; hence a new JIRA.

This was found when a frozen collection was found when a multi-cell collection was expected.  When this happened LegacyCellName#collectionElement comes back as null which eventually gets asserted against in BufferCell (complex cell needs a path).",N/A,"3.0.x, 3.11.x"
CASSANDRA-15867,Update Jackson version to 2.9.10.1 because there are security issues in 2.9.5,"Please see attached HTML report from OWASP dependency check for current 4.0-alpha5 trunk branch.

 

 ",N/A,"3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15863,Bootstrap resume and TestReplaceAddress fixes,This has been [broken|https://ci-cassandra.apache.org/job/Cassandra-trunk/159/testReport/dtest-large.replace_address_test/TestReplaceAddress/test_restart_failed_replace/history/] for ages,N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15862,"Use ""allow list"" or ""safe list"" instead of the term ""whitelist"" ","Language matters. I'd like to remove all references in Apache Airflow to whitelist or black list, and the Cassandra Python API has some that we can't easily remove.

The recent global events have made this even more relevant, but this has been on my radar for a while now. Here is a well written article for why I think it matters 

https://www.ncsc.gov.uk/blog-post/terminology-its-not-black-and-white

{quote}It's fairly common to say whitelisting and blacklisting to describe desirable and undesirable things in cyber security.

However, there's an issue with the terminology. It only makes sense if you equate white with 'good, permitted, safe' and black with 'bad, dangerous, forbidden'. There are some obvious problems with this. {quote}

My exposure to is via the Python API where there is the cassandra.pollicies.WhiteListRoundRobinPolicy class. I propose that this be renamed to AllowListRoundRobinPolicy instead. I do not know if there are other references.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15859,Avoid per-host hinted-handoff throttle being rounded to 0 in large cluster,"When ""hinted_handoff_throttle_in_kb"" is sufficiently small or num of nodes in the cluster is sufficiently large, the per-host throttle will be rounded to 0, aka. unthrottled.

 
{code:java|title=HintsDispatchExecutor.java}
int throttleInKB = DatabaseDescriptor.getHintedHandoffThrottleInKB() / nodesCount;
this.rateLimiter = RateLimiter.create(throttleInKB == 0 ? Double.MAX_VALUE : throttleInKB * 1024);
{code}
[trunk-patch|https://github.com/apache/cassandra/pull/616]",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15858,TestRepairDataSystemTable test_repair_parent_table and test_repair_table fail,They fail [consistently|https://ci-cassandra.apache.org/job/Cassandra-trunk/159/testReport/] and locally as well,N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15857,Frozen RawTuple is not annotated with frozen in the toString method,"All raw types (e.g. RawCollection, RawUT) that supports freezing wraps the type name with 'frozen<>' in the toString method, except RawTuple.

Therefore, the RawTuple::toString output misses the frozen wrapper.

Tuple is always frozen. However since CASSANDRA-15035, it throws when the inner tuple is not explicitly wrapped with frozen within a collection.

The method, CQL3Type.Raw::toString, is referenced at multiple places in the source. For example, referenced in CreateTypeStatement.Raw and involved in CQLSSTableWriter. Another example is that it is called to produce the SchemaChange at several AlterSchemaStatement implementations.

A test can prove that missing the frozen wrapper causes exception when building CQLSSTableWriter for user types defined like below. Note that the inner tuple is wrapped with frozen in the initial CQL statement.
{code:java}
CREATE TYPE ks.fooType ( f list<frozen<tuple<text, text>>> )
{code}
{code:java}
org.apache.cassandra.exceptions.InvalidRequestException: Non-frozen tuples are not allowed inside collections: list<tuple<text, text>>
	at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.throwNestedNonFrozenError(CQL3Type.java:710)
	at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepare(CQL3Type.java:669)
	at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepareInternal(CQL3Type.java:661)
	at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.lambda$prepare$1(Types.java:341)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.prepare(Types.java:342)
	at org.apache.cassandra.schema.Types$RawBuilder.build(Types.java:291)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.createTypes(CQLSSTableWriter.java:551)
	at org.apache.cassandra.io.sstable.CQLSSTableWriter$Builder.build(CQLSSTableWriter.java:527)
{code}",N/A,"3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15856,Security vulnerabilities with dependency jars  of Cassandra 3.11.6,"The latest release of Cassandra 3.11.6 has few dependency jars which have some security vulnerabilities.

 

Apache Thrift (org.apache.thrift:libthrift:0.9.2) has below mentioned security vulnerabilities reported
|+[https://nvd.nist.gov/vuln/detail/CVE-2016-5397]+|
|+[https://nvd.nist.gov/vuln/detail/CVE-2018-1320]+|
|+[https://nvd.nist.gov/vuln/detail/CVE-2019-0205]+|

 

Netty Project (io.netty:netty-all:4.0.44.Final) has below mentioned security vulnerabilities reported
|+[https://nvd.nist.gov/vuln/detail/CVE-2019-16869]+|
|+[https://nvd.nist.gov/vuln/detail/CVE-2019-20444]+|
|+[https://nvd.nist.gov/vuln/detail/CVE-2019-20445]+|

 

Is there a plan to upgrade these jars in any upcoming release?",N/A,3.11.12
CASSANDRA-15855,Mark test_populate_mv_after_insert_wide_rows as flaky,See CASSANDRA-15845. This test can still fail in a flaky way so we better mark it as such to avoid confusion and dup investigation efforts on failing tests,N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15853,@since missing/wrong for upgrade_internal_auth_test.TestAuthUpgrade.test_upgrade_to_22/33,@since missing/wrong for upgrade_internal_auth_test.TestAuthUpgrade.test_upgrade_to_22/33,N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15851,Add bytebuddy support for in-jvm dtests,"Old python dtests support byteman, but that is quite horrible to work with, [bytebuddy|https://bytebuddy.net/#/] is much better, so we should add support for that in the in-jvm dtests.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15834,Bloom filter false positive rate calculation does not take into account true negatives,"The bloom filter false positive ratio is [currently computed|https://github.com/apache/cassandra/blob/ded62076e7fdfd1cfdcf96447489ea607ca796a0/src/java/org/apache/cassandra/metrics/TableMetrics.java#L738] as:

{{bf_fp_ratio = false_positive_count / (false_positive_count + true_positive_count)}}

However, this calculation doesn't take into account true negatives (false negatives never happen on bloom filters).

In a situation where there are 1000 reads for non existing rows, and there are 10 false positives, the bloom filter false positive ratio will be wrongly calculated as 10/10 = 1.0, while it should be 10/1000 = 0.01.

We should update the calculation to:

{{bf_fp_ratio = false_positive_count / #bf_queries}}

Original jira by [~pauloricardomg]",N/A,"3.0.25, 3.11.11, 4.0-rc2, 4.0"
CASSANDRA-15833,Unresolvable false digest mismatch during upgrade due to CASSANDRA-10657,"CASSANDRA-10657 introduced changes in how the ColumnFilter is interpreted. This results in digest mismatch when querying incomplete set of columns from a table with consistency that requires reaching instances running pre CASSANDRA-10657 from nodes that include CASSANDRA-10657 (it was introduced in Cassandra 3.4). 

The fix is to bring back the previous behaviour until there are no instances running pre CASSANDRA-10657 version. 
",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15829,Upgrade to logback 1.2.3 to address CVE-2017-5929,"Recent scan results identified the following CVE that requires this upgrade to address

[https://nvd.nist.gov/vuln/detail/CVE-2017-5929]",N/A,"2.2.x, 3.0.26, 3.11.12"
CASSANDRA-15816,Transports are stopped in the wrong order,"Stopping gossip while native is running is almost always wrong, change the order of shutdown and log a warning when done manually",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15814,order by descending on frozen list not working,"By creating a table like the following:
{code:java}
CREATE TABLE IF NOT EXISTS software (
 name ascii,
 version frozen<list<int>>,
 data ascii,
 PRIMARY KEY(name,version)
)
{code}
It works and version is ordered in an ascending order. But when trying to order in descending order:
{code:java}
CREATE TABLE IF NOT EXISTS software (
    name ascii,
    version frozen<list<int>>,
    data ascii,
    PRIMARY KEY(name,version)
) WITH CLUSTERING ORDER BY (version DESC);
{code}
The table is created normally, but when trying to insert a row:
{code:java}
insert into software(name, version) values ('t1', [2,10,30,40,50]); 
{code}
Cassandra throws an error:
{code:java}
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid list literal for version of type frozen<list<int>>""
{code}
The goal here is that I would like to get the last version of a software.

 ",N/A,"2.2.18, 3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15811,Improve DROP COMPACT STORAGE,"DROP COMPACT STORAGE was introduced in CASSANDRA-10857 as one of the steps to deprecate Thrift. However, current semantics of dropping compact storage flags from tables reveal several columns that are usually empty (colum1 and value in non-dense case, value for dense columns, and a column with an empty name for super column families). Showing these columns  can confuse application developers, especially ones that have never used thrift and/or made writes that assumed presence of those fields, and used compact storage in 3.x because is has “compact” in the name.

There’s not much we can do in a super column family case, especially considering there’s no way to create a supercolumn family using CQL, but we can improve dense and non-dense cases. We can scan stables and make sure there are no signs of thrift writes in them, and if all sstables conform to this rule, we can not only drop the flag, but also drop columns that are supposed to be hidden. However, this is both not very user-friendly, and is probably not worth development effort. 

An alternative to scanning is to add {{FORCE DROP COMPACT}} syntax (or something similar) that would just drop columns unconditionally. It is likely that people who were using compact storage with thrift know they were doing that, so they'll usually use ""regular"" {{DROP COMPACT}}, withouot force, that will simply reveal the columns as it does right now.

Since for fixing CASSANDRA-15778, and to allow EmptyType column to actually have data[*] we had to remove empty type validation, properly handling compact storage starts making more sense, but we’ll solve it through not having columns, hence not caring about values instead, or keeping values _and_ data, not requiring validation in this case. EmptyType field will have to be handled differently though.

[*] as it is possible to end up with sstables upgraded from 2.x or written in 3.x before CASSANDRA-15373, which means not every 2.x upgraded or 3.x cluster is guaranteed to have empty values in this column, and this behaviour, even if undesired, might be used by people. 

Open question is: CASSANDRA-15373 adds validation to EmptyType that disallows any non-empty value to be written to it, but we already allow creating table via CQL, and still write data into it with thrift. It seems to have been unintended, but it might have become a feature people rely on. If we simply back port 15373 to 2.2 and 2.1, we’ll change and break behaviour. Given no-one complained in 3.0 and 3.11, this assumption is unlikely though. ",N/A,"3.0.24, 3.11.10"
CASSANDRA-15805,Potential duplicate rows on 2.X->3.X upgrade when multi-rows range tombstones interacts with collection tombstones,"The legacy reading code ({{LegacyLayout}} and {{UnfilteredDeserializer.OldFormatDeserializer}}) does not handle correctly the case where a range tombstone covering multiple rows interacts with a collection tombstone.

A simple example of this problem is if one runs on 2.X:
{noformat}
CREATE TABLE t (
  k int,
  c1 text,
  c2 text,
  a text,
  b set<text>,
  c text,
  PRIMARY KEY((k), c1, c2)
);

// Delete all rows where c1 is 'A'
DELETE FROM t USING TIMESTAMP 1 WHERE k = 0 AND c1 = 'A';
// Inserts a row covered by that previous range tombstone
INSERT INTO t(k, c1, c2, a, b, c) VALUES (0, 'A', 'X', 'foo', {'whatever'}, 'bar') USING TIMESTAMP 2;
// Delete the collection of that previously inserted row
DELETE b FROM t USING TIMESTAMP 3 WHERE k = 0 AND c1 = 'A' and c2 = 'X';
{noformat}

If the following is ran on 2.X (with everything either flushed in the same table or compacted together), then this will result in the inserted row being duplicated (one part containing the {{a}} column, the other the {{c}} one).

I will note that this is _not_ a duplicate of CASSANDRA-15789 and this reproduce even with the fix to {{LegacyLayout}} of this ticket. That said, the additional code added to CASSANDRA-15789 to force merging duplicated rows if they are produced _will_ end up fixing this as a consequence (assuming there is no variation of this problem that leads to other visible issues than duplicated rows). That said, I ""think"" we'd still rather fix the source of the issue.
",N/A,"3.0.21, 3.11.7"
CASSANDRA-15804,system_schema keyspace complain of schema mismatch during upgrade,"When upgrading from 3.11.4 to 3.11.6, we got the following error:

{code:Plain Text}
ERROR [MessagingService-Incoming-/10.20.11.59] 2020-05-07 13:53:52,627 CassandraDaemon.java:228 - Exception in thread Thread[MessagingService-Incoming-/10.20.11.59,5,main]
java.lang.RuntimeException: Unknown column kind during deserialization
    at org.apache.cassandra.db.Columns$Serializer.deserialize(Columns.java:464) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.db.SerializationHeader$Serializer.deserializeForMessaging(SerializationHeader.java:419) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.deserializeHeader(UnfilteredRowIteratorSerializer.java:195) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:851) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:839) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:425) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:434) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:675) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.service.MigrationManager$MigrationsSerializer.deserialize(MigrationManager.java:658) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.4.jar:3.11.4]
    at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.4.jar:3.11.4]
{code}

I've noticed that system_schema.dropped_columns has a new column called ""kind"".
No issues arise from this error message, and the error disappeared after upgrading all nodes. But it still caused concerns due to the ERROR logging level, although ""nodetool describecluster"" reported only one schema version.

It makes sense for the system keyspaces to not be included for the ""describecluster"" schema version check, but it seems to me that these internal schema mismatches should be ignored if the versions are different between the nodes.",N/A,"3.11.12, 4.0.1"
CASSANDRA-15790,EmptyType doesn't override writeValue so could attempt to write bytes when expected not to,"EmptyType.writeValues is defined here https://github.com/apache/cassandra/blob/e394dc0bb32f612a476269010930c617dd1ed3cb/src/java/org/apache/cassandra/db/marshal/AbstractType.java#L407-L414

{code}
public void writeValue(ByteBuffer value, DataOutputPlus out) throws IOException
    {
        assert value.hasRemaining();
        if (valueLengthIfFixed() >= 0)
            out.write(value);
        else
            ByteBufferUtil.writeWithVIntLength(value, out);
    }
{code}

This is fine when the value is empty as the write of empty no-ops (the readValue also noops since the length is 0), but if the value is not empty (possible during upgrades or random bugs) then this could silently cause corruption; ideally this should throw a exception if the ByteBuffer has data.

This was called from org.apache.cassandra.db.rows.BufferCell.Serializer#serialize, here we check to see if data is present or not and update the flags.  If data is present then and only then do we call type.writeValue (which requires bytes is not empty).  The problem is that EmptyType never expects writes to happen, but it still writes them; and does not read them (since it says it is fixed length of 0, so does read(buffer, 0)).",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15789,Rows can get duplicated in mixed major-version clusters and after full upgrade,"In a mixed 2.X/3.X major version cluster a sequence of row deletes, collection overwrites, paging, and read repair can cause 3.X nodes to split individual rows into several rows with identical clustering. This happens due to 2.X paging and RT semantics, and a 3.X {{LegacyLayout}} deficiency.

To reproduce, set up a 2-node mixed major version cluster with the following table:

{code}
CREATE TABLE distributed_test_keyspace.tlb (
    pk int,
    ck int,
    v map<text, text>,
    PRIMARY KEY (pk, ck)
);
{code}

1. Using either node as the coordinator, delete the row with ck=2 using timestamp 1

{code}
DELETE FROM tbl USING TIMESTAMP 1 WHERE pk = 1 AND ck = 2;
{code}

2. Using either node as the coordinator, insert the following 3 rows:

{code}
INSERT INTO tbl (pk, ck, v) VALUES (1, 1, {'e':'f'}) USING TIMESTAMP 3;
INSERT INTO tbl (pk, ck, v) VALUES (1, 2, {'g':'h'}) USING TIMESTAMP 3;
INSERT INTO tbl (pk, ck, v) VALUES (1, 3, {'i':'j'}) USING TIMESTAMP 3;
{code}

3. Flush the table on both nodes

4. Using the 2.2 node as the coordinator, force read repar by querying the table with page size = 2:
 
{code}
SELECT * FROM tbl;
{code}

5. Overwrite the row with ck=2 using timestamp 5:

{code}
INSERT INTO tbl (pk, ck, v) VALUES (1, 2, {'g':'h'}) USING TIMESTAMP 5;}}
{code}

6. Query the 3.0 node and observe the split row:

{code}
cqlsh> select * from distributed_test_keyspace.tlb ;

 pk | ck | v
----+----+------------
  1 |  1 | {'e': 'f'}
  1 |  2 | {'g': 'h'}
  1 |  2 | {'k': 'l'}
  1 |  3 | {'i': 'j'}
{code}

This happens because the read to query the second page ends up generating the following mutation for the 3.0 node:

{code}
ColumnFamily(tbl -{deletedAt=-9223372036854775808, localDeletion=2147483647,
             ranges=[2:v:_-2:v:!, deletedAt=2, localDeletion=1588588821]
                    [2:v:!-2:!,   deletedAt=1, localDeletion=1588588821]
                    [3:v:_-3:v:!, deletedAt=2, localDeletion=1588588821]}-
             [2:v:63:false:1@3,])
{code}

Which on 3.0 side gets incorrectly deserialized as

{code}
Mutation(keyspace='distributed_test_keyspace', key='00000001', modifications=[
  [distributed_test_keyspace.tbl] key=1 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | [v]]
    Row[info=[ts=-9223372036854775808] ]: ck=2 | del(v)=deletedAt=2, localDeletion=1588588821, [v[c]=d ts=3]
    Row[info=[ts=-9223372036854775808] del=deletedAt=1, localDeletion=1588588821 ]: ck=2 |
    Row[info=[ts=-9223372036854775808] ]: ck=3 | del(v)=deletedAt=2, localDeletion=1588588821
])
{code}

{{LegacyLayout}} correctly interprets a range tombstone whose start and finish {{collectionName}} values don't match as a wrapping fragment of a legacy row deletion that's being interrupted by a collection deletion (correctly) - see [code|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L1874-L1889]. Quoting the comment inline:

{code}
// Because of the way RangeTombstoneList work, we can have a tombstone where only one of
// the bound has a collectionName. That happens if we have a big tombstone A (spanning one
// or multiple rows) and a collection tombstone B. In that case, RangeTombstoneList will
// split this into 3 RTs: the first one from the beginning of A to the beginning of B,
// then B, then a third one from the end of B to the end of A. To make this simpler, if
 // we detect that case we transform the 1st and 3rd tombstone so they don't end in the middle
 // of a row (which is still correct).
{code}

{{LegacyLayout#addRowTombstone()}} method then chokes when it encounters such a tombstone in the middle of an existing row - having seen {{v[c]=d}} first, and mistakenly starts a new row, while in the middle of an existing one: (see [code|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L1500-L1501]).
",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15778,"CorruptSSTableException after a 2.1 SSTable is upgraded to 3.0, failing reads","Below is the exception with stack trace. This issue is consistently reproduce-able.
{code:java}
ERROR [SharedPool-Worker-1] 2020-05-01 14:57:57,661 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,main]ERROR [SharedPool-Worker-1] 2020-05-01 14:57:57,661 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,main]org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /mnt/data/cassandra/data/<ks>/<cf-fda511301fb311e7bd79fd24f2fcfb0d/md-10151-big-Data.db at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:349) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.hasNext(AbstractSSTableIterator.java:220) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.columniterator.SSTableIterator.hasNext(SSTableIterator.java:33) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:131) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:77) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:294) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:180) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:176) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:341) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:47) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_231] at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137) [nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [nf-cassandra-3.0.19.8.jar:3.0.19.8] at java.lang.Thread.run(Thread.java:748) [na:1.8.0_231]Caused by: java.lang.ArrayIndexOutOfBoundsException: 121 at org.apache.cassandra.db.ClusteringPrefix$Deserializer.prepare(ClusteringPrefix.java:425) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.UnfilteredDeserializer$CurrentDeserializer.prepareNext(UnfilteredDeserializer.java:170) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.UnfilteredDeserializer$CurrentDeserializer.hasNext(UnfilteredDeserializer.java:151) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.computeNext(SSTableIterator.java:140) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.hasNextInternal(SSTableIterator.java:172) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:336) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] ... 27 common frames omitted

Caused by: java.lang.ArrayIndexOutOfBoundsException: 121
    at org.apache.cassandra.db.ClusteringPrefix$Deserializer.prepare(ClusteringPrefix.java:425) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8]
    at org.apache.cassandra.db.UnfilteredDeserializer$CurrentDeserializer.prepareNext(UnfilteredDeserializer.java:170) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8]
    at org.apache.cassandra.db.UnfilteredDeserializer$CurrentDeserializer.hasNext(UnfilteredDeserializer.java:151) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8]
    at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.computeNext(SSTableIterator.java:140) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8]
    at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.hasNextInternal(SSTableIterator.java:172) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8]
    at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:336) ~[nf-cassandra-3.0.19.8.jar:3.0.19.8] ... 27 common frames omitted
{code}

Column family definition
{code:java}
CREATE TABLE <keyspace>.""<cf>"" (
 key text,
 value text,
 PRIMARY KEY (key, value)
 ) WITH COMPACT STORAGE
 AND CLUSTERING ORDER BY (value ASC)
 AND bloom_filter_fp_chance = 0.01
 AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
 AND comment = ''
 AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
 AND compression = {'enabled': 'false'}
 AND crc_check_chance = 1.0
 AND dclocal_read_repair_chance = 0.1
 AND default_time_to_live = 0
 AND gc_grace_seconds = 864000
 AND max_index_interval = 2048
 AND memtable_flush_period_in_ms = 0
 AND min_index_interval = 128
 AND read_repair_chance = 0.0
 AND speculative_retry = '99PERCENTILE';{code}",N/A,3.0.21
CASSANDRA-15777,"All test targets to set jvm arg ""java.io.tmpdir"" according to ${tmp.dir}","Make all test targets declare the temp directory (java.io.tmpdir) as defined by tmp.dir

This was originally done in CASSANDRA-7712 but has been eroded over time as the test targets evolved.

The attached patch moves the 
{code}<jvmarg value=""-Djava.io.tmpdir=${tmp.dir}""/>{code}
into the {{testmacrohelper}} macrodef.

Like CASSANDRA-7712, jenkins agents are filling up their root volumes because of what's left behind under {{""/tmp""}}

The build scripts have already been configured to define the temp directory to be within the jenkins job's workspace: https://github.com/apache/cassandra-builds/commit/33ba1e30ea196180f7d70f8e6ec47fdf6844f3f6#diff-91876f5f158ec50dab9a70cc06c06922",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15770,stop and restart fail in Debian 10 buster,"Debian 10 ""buster"" has an updated {{start-stop-daemon}} version that causes init script to fail to stop or restart Cassandra daemon. {{stop}} option do not stops first launched instance and depending of previous operations, {{restart}} and {{start}} could launch a second Cassandra daemon instance.

{{start-stop-daemon}} since version 1.19.3 fail to stop matching only a pidfile that can be writen by an unprivileged (non-root) user because it is considered a security risk.

Adding a second matching option (user) solved this.

This fix does not break compatibility with older versions and does not mess with pidfile's mode or permissions.

Related: CASSANDRA-15099

https://github.com/vice/cassandra/tree/debian-buster",N/A,"2.2.20, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15768,Tarball contains duplicate entries,"The tarball contains a lot of duplicate entries. One example is cassandra-stress.bat:
{code:sh}
tar -tvf /home/map/Downloads/apache-cassandra-3.11.6-bin.tar.gz |grep ""cassandra-stress.bat""
-rw-r--r-- 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat
-rwxr-xr-x 0/0            1097 2020-02-10 23:57 apache-cassandra-3.11.6/tools/bin/cassandra-stress.bat
{code}",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15767,/usr/bin/cassandra looking for wrong libjemalloc.so file,"In /usr/sbin/cassandra

Linux)
 if [ -z $CASSANDRA_LIBJEMALLOC ] ; then
 which ldconfig > /dev/null 2>&1
 if [ $? = 0 ] ; then
 # e.g. for CentOS
 dirs=""/lib64 /lib /usr/lib64 /usr/lib `ldconfig -v 2>/dev/null | grep -v '^\s' | sed 's/^\([^:]*\):.*$/\1/'`""
 else
 # e.g. for Debian, OpenSUSE
 dirs=""/lib64 /lib /usr/lib64 /usr/lib `cat /etc/ld.so.conf /etc/ld.so.conf.d/*.conf | grep '^/'`""
 fi
 dirs=`echo $dirs | tr "" "" "":""`
 CASSANDRA_LIBJEMALLOC=$(find_library '.*/libjemalloc\.so\(\.1\)*' $dirs)

 

However...

# find /usr/lib64 -name ""*jemalloc*""
/usr/lib64/libjemalloc.so.2

 

 ",N/A,"3.0.28, 3.11.14, 4.0.6, 4.1-beta1, 5.0-alpha1, 5.0"
CASSANDRA-15757,CustomIndexTest.indexBuildingPagesLargePartitions is flaky,"CustomIndexTest.indexBuildingPagesLargePartitions is flaky. Failed in CI and was able to reproduce failure inside IntelliJ by setting test Repeat to ‘Run Until Failure’. Failed after 459 iterations.

{code}
java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:86)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.junit.Assert.assertTrue(Assert.java:52)
	at org.apache.cassandra.index.CustomIndexTest.lambda$indexBuildingPagesLargePartitions$1(CustomIndexTest.java:687)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1540)
	at org.apache.cassandra.index.CustomIndexTest.indexBuildingPagesLargePartitions(CustomIndexTest.java:687)
	at jdk.internal.reflect.GeneratedMethodAccessor14.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:53)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15752,Range read concurrency factor didn't consider range merger,"During range read, coordinator computes concurrency factor which is the number of vnode ranges to contact in parallel for the next batch.

But in {{RangeCommandIterator}}, vnode ranges are merged by {{RangeMerger}} if vnode ranges share enough replicas to satisfy consistency level. eg. vnode range [a,b) has replica n1,n2,n3 and vnode range [b,c) has replica n2,n3,n4, so they can be merged as range [a,c) with replica n2, n3 for Quorum.

Currently it counts number of merged ranges towards concurrency factor. Coordinator may fetch more ranges than needed.

----

Another issue is that when executing range read on table with very small amount of data, concurrency factor can be bumped to {{size of total vnode ranges}}, eg. 10k, depending on the num of vnodes and cluster size. As a result, coordinator will send large number of concurrent range requests, potentially slowing down the cluster.. We should cap the max concurrency factor..",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15744,FBUtilitities.testWaitFirstFuture is flaky,"The unit test FBUtilitities.testWaitFirstFuture 

Example failure message: {{junit.framework.AssertionFailedError: expected:<40> but was:<10>}} ",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15733,jvm dtest builder should be provided to the factory and expose state,"Currently the builder is rather heavy and creates configs plus call the factory with specific fields only, this isn’t that flexible and makes it harder to have custom cluster definitions which require additional fields to be defined.  To solve this we should make the builder be sent to the factory and expose the state so the factory can get all the fields it needs, the factory should also be in charge of creating the configs",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15729,Jenkins Test Results Report in plaintext for ASF ML,"The Jenkins pipeline builds now aggregate all test reports.

For example: 
- https://ci-cassandra.apache.org/job/Cassandra-trunk/68/testReport/
- https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-trunk/detail/Cassandra-trunk/68/tests

But Jenkins can only keep a limited amount of build history, so those links are not permanent, can't be used as references, and don't help for bisecting and blame on regressions (and flakey tests) over a longer period of time.

The builds@ ML can provide a permanent record of test results. 

This was first brought up in these two threads: 
- https://lists.apache.org/thread.html/re8122e4fdd8629e7fbca2abf27d72054b3bc0e3690ece8b8e66f618b%40%3Cdev.cassandra.apache.org%3E
- https://lists.apache.org/thread.html/ra5f6aeea89546825fe7ccc4a80898c62f8ed57decabf709d81d9c720%40%3Cdev.cassandra.apache.org%3E

An example plaintext report, to demonstrate feasibility, is available here: https://lists.apache.org/thread.html/r80d13f7af706bf8dfbf2387fab46004c1fbd3917b7bc339c49e69aa8%40%3Cbuilds.cassandra.apache.org%3E

Hurdles:
 - the ASF mailing lists won't accept html, attachments, or any message body over 1MB.
 - packages are used as a differentiator in the final aggregated report. The cqlsh and dtests currently don't specify it. It needs to be added as a ""dot-separated"" prefix to the testsuite and testcase name.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15708,Fix in-jvm upgrade dtests,In-jvm upgrade dtests were broken by CASSANDRA-15539,N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15703,When CDC is disabled bootstrapping breaks,"Related to CASSANDRA-12697

There is an edge case left over.  If a cluster had enabled CDC on a table then subsequently set cdc=false, subsequent bootstraps break. 

 

This is because the cdc column is false on the existing nodes but null on the bootstrapping node, causing the schema sha to never match.

 

There are a couple possible fixes:

  1.  Since 12697 was only about upgrades we can serialize the cdc column IFF the cluster nodes are all on the same version.

  2.  We can force cdc=false on all tables where it's null.

 

I think #1 is probably simpler. #2 would probably cause more of the same problem if nodes are not all updated with the fix.

 

  ",N/A,"3.11.11, 4.0-rc1, 4.0"
CASSANDRA-15690,Single partition queries can mistakenly omit partition deletions and resurrect data,"We have logic that allows us to exclude sstables with partition deletions that are older than the minimum collected timestamp in a local request. However, it’s possible that another node could have rows that aren’t known to the local node that are in turn older than the excluded partition deletion. In such a scenario, those will be mistakenly resurrected, which is a correctness issue.",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15679,"cqlsh COPY FROM of map of blobs fails with parse error ""unhashable type: 'bytearray'""","h2. Background
A user was having issues loading CSV data with the {{COPY FROM}} command into a {{map}} column with {{blob}} values.
h2. Replication steps
I can easily replicate the problem with this simple table:

{noformat}
CREATE TABLE community.blobmaptable (
    id text PRIMARY KEY,
    blobmapcol map<int, blob>
)
{noformat}

I have this CSV file that contains just 1 row:

{noformat}
$ cat blobmap.csv 
c3,{3: 0x74776f}
{noformat}

And here's the error when I try to load it:

{noformat}
cqlsh:community> COPY blobmaptable (id, blobmapcol) FROM '~/blobmap.csv' ;
Using 1 child processes
Starting copy of community.blobmaptable with columns [id, blobmapcol].
Failed to import 1 rows: ParseError - Failed to parse {3: 0x74776f} : unhashable type: 'bytearray',  given up without retries
Failed to process 1 rows; failed rows written to import_community_blobmaptable.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       3 rows/s
1 rows imported from 1 files in 0.389 seconds (0 skipped).
{noformat}

I've also logged [PYTHON-1234|https://datastax-oss.atlassian.net/browse/PYTHON-1234] because I wasn't sure if it was a Python driver issue. Cheers!",N/A,"2.1.x, 2.2.17, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15669,LeveledCompactionStrategy compact last level throw an ArrayIndexOutOfBoundsException,"Cassandra will throw an ArrayIndexOutOfBoundsException when compact last level.

My test is as follows：
 # Create a table with LeveledCompactionStrategy and its params are 'enabled': 'true', 'fanout_size': '2', 'max_threshold': '32', 'min_threshold': '4', 'sstable_size_in_mb': '2'（fanout_size and sstable_size_in_mb are too small just to make it easier to reproduce the problem）;
 # Insert data into the table by stress;
 # Cassandra throw an ArrayIndexOutOfBoundsException when compact level9 sstables(this level score bigger than 1.001)

ERROR [CompactionExecutor:4] 2020-03-28 08:59:00,990 CassandraDaemon.java:442 - Exception in thread Thread[CompactionExecutor:4,1,main]
 java.lang.ArrayIndexOutOfBoundsException: 9
 at org.apache.cassandra.db.compaction.LeveledManifest.getLevel(LeveledManifest.java:814)
 at org.apache.cassandra.db.compaction.LeveledManifest.getCandidatesFor(LeveledManifest.java:746)
 at org.apache.cassandra.db.compaction.LeveledManifest.getCompactionCandidates(LeveledManifest.java:398)
 at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:131)
 at org.apache.cassandra.db.compaction.CompactionStrategyHolder.lambda$getBackgroundTaskSuppliers$0(CompactionStrategyHolder.java:109)
 at org.apache.cassandra.db.compaction.AbstractStrategyHolder$TaskSupplier.getTask(AbstractStrategyHolder.java:66)
 at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:214)
 at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:289)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
 at java.util.concurrent.FutureTask.run(FutureTask.java)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.lang.Thread.run(Thread.java:748)

I tested it on cassandra version 3.11.3 & 4.0-alpha3. The exception all happened.

once it triggers, level1- leveln compaction no longer works, level0 is still valid
 ",N/A,"3.11.11, 4.0-rc2, 4.0"
CASSANDRA-15668,Jenkins 'Cassandra' label applied to the declarative pipeline ,"On the new ci-cassandra.apache.org infrastructure agents are siloed to projects.

The declarative pipeline in the 2.2, 3.0, 3.11, and trunk branches do not restrict the builds to agents with the 'cassandra' label. Because these agents will not run jobs that don't specify this label, the pipeline task only ever runs on unlabelled agents, of which there are very few (and likely shouldn't exist, existing only from for now because of misconfigurations).

Example of the failure to run the pipeline tasks is
{noformat}
[Pipeline] Start of Pipeline
[Pipeline] node
Still waiting to schedule task
'cassandra10' is reserved for jobs with matching label expression; 'cassandra11' is reserved for jobs with matching label expression; 'cassandra12' is reserved for jobs with matching label expression; 'cassandra13' is reserved for jobs with matching label expression; 'cassandra14' is reserved for jobs with matching label expression; 'cassandra15' is reserved for jobs with matching label expression; 'cassandra16' is reserved for jobs with matching label expression; 'cassandra1' is reserved for jobs with matching label expression; 'cassandra2' is reserved for jobs with matching label expression; 'cassandra3' is reserved for jobs with matching label expression; 'cassandra4' is reserved for jobs with matching label expression; 'cassandra5' is reserved for jobs with matching label expression; 'cassandra6' is reserved for jobs with matching label expression; 'cassandra7' is reserved for jobs with matching label expression; 'cassandra8' is reserved for jobs with matching label expression; 'cassandra9' is reserved for jobs with matching label expression
{noformat}

Along with this change, we can improve the name of the {{*-test-jvm-dtest-forking}} stages.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15667,"StreamResultFuture check for completeness is inconsistent, leading to races","{{StreamResultFuture#maybeComplete()}} uses {{StreamCoordinator#hasActiveSessions()}} to determine if all sessions are completed, but then accesses each session state via {{StreamCoordinator#getAllSessionInfo()}}: this is inconsistent, as the former relies on the actual {{StreamSession}} state, while the latter on the {{SessionInfo}} state, and the two are concurrently updated with no coordination whatsoever.

This leads to races, i.e. apparent in some dtest spurious failures, such as {{TestBootstrap.resumable_bootstrap_test}} in CASSANDRA-15614 cc [~e.dimitrova].",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15663,DESCRIBE KEYSPACE does not properly quote table names,"How to reproduce (3.11.6) - cqlsh:

{code}
CREATE KEYSPACE test1 WITH replication = \{'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;
CREATE TABLE test1.""default"" (id text PRIMARY KEY, data text, etag text);
DESCRIBE KEYSPACE test1;
{code}

Output will be:

{code}
CREATE TABLE test1.default (
 id text PRIMARY KEY,
 data text,
 etag text
) WITH [..]
{code}

Output should be:

{code}
CREATE TABLE test1.""default"" (
 id text PRIMARY KEY,
 data text,
 etag text
) WITH [..]
{code}

 If you try to run {{CREATE TABLE test1.default [..]}} you will get an error 

SyntaxException: line 1:19 no viable alternative at input 'default' (CREATE TABLE test1.[default]...)

Oskar Liljeblad

 ",N/A,"3.11.11, 4.0"
CASSANDRA-15653,Disable JMX rebinding,JMX rebinding should be disabled for security reasons. More information provided to the PMC.,N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15651,Jenkins tests to use testclasslist where possible (like CircleCI),"Following up on CASSANDRA-15639
 make all the jenkins test jobs run in the same manner.

This standards the approach across test jobs and to CircleCI, and will make it easier to parallelise test runs later on.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15649,test.distributed.timeout no longer respected in CircleCI,"After switching jvm dtest over testclasslist (CASSANDRA-15508) we no longer respect the dtest timeout and instead use the unit test timeout (4m vs 6m).

This does not impact Jenkins as I made sure to check that before calling testclasslist; though this does impact 2.2, 3.0, and 3.11 as well.

||Config||Trunk||3.11||3.0||2.2||
|LOWER||[Circle CI|https://circleci.com/workflow-run/04f7fbe2-1919-4da0-bf72-ba41b41c3072]| [Circle CI|https://circleci.com/workflow-run/69c64062-80f5-4d08-8ab0-dbe88ce7b236] | [Circle CI|https://circleci.com/workflow-run/499d2fbe-a8c1-4a27-9430-a9ebb40aad53] | [Circle CI|https://circleci.com/workflow-run/fedbd5b8-683d-4c59-a9f6-3aad5a6ba41d] |
|HIGHER| [Circle CI|https://circleci.com/workflow-run/036bbad1-541a-49dc-a567-cef2300fa847] | TBD | TBD | TBD |

CI Failures were flaky tests, below are their links
* CASSANDRA-15630",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15623,"When running CQLSH with STDIN input, exit with error status code if script fails","Assuming CASSANDRA-6344 is in place for years and considering that scripts submitted with the `-e` option behave in a similar fashion, it is very surprising that scripts submitted to STDIN (i.e. piped in) always exit with a zero code, regardless of errors. I believe this should be fixed.",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15622,Unit tests throw UnknownHostException trying to use `InetAddress.getLocalHost()` instead of `FBUtilities.getLocalAddress()`,"Many of the unit tests in Jenkins fail because of the use of {{`InetAddress.getLocalHost()`}} in the test classes.

The Jenkins agents need a public ipaddress (and a hostname associated to it) so the Jenkins master can connect to them (agents can be hosted externally, by donating third-parties).

The call to {{`InetAddress.getLocalHost()`}} can resolve to a hostname that can't be looked up.
Not only can it not be listed in {{`/etc/hosts`}}, but we don't want it to be either (in case of accidental external port opening if the hostname points to the public ipaddress). (Which is also ASF policy on this infrastructure.)

The unit test code needs to replace these code occurrences with the call to {{`FBUtilities.getLocalAddress()`}}",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15595,"Many errors of ""java.lang.AssertionError: Illegal bounds""","Hi, i'm running cassandra 3.11.6 and getting on all hosts many errors of:
{code}
ERROR [ReadStage-6] 2020-02-24 13:53:34,528 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[ReadStage-6,5,main]
java.lang.AssertionError: Illegal bounds [-2102982480..-2102982472); size: 2761628520
        at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:345) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.util.Memory.getLong(Memory.java:254) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:234) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.util.CompressedChunkReader$Standard.readChunk(CompressedChunkReader.java:114) ~[apache-cassandra-3.11.6.ja
r:3.11.6]
        at org.apache.cassandra.cache.ChunkCache.load(ChunkCache.java:158) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.cache.ChunkCache.load(ChunkCache.java:39) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache.lambda$new$0(BoundedLocalCache.java:2949) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$15(BoundedLocalCache.java:1807) ~[caffeine-2.2.6.jar:na]
        at java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1853) ~[na:1.8.0-zing_19.12.102.0]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:1805) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:1788) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:97) ~[caffeine-2.2.6.jar:na]
        at com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:66) ~[caffeine-2.2.6.jar:na]
        at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.rebuffer(ChunkCache.java:236) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.rebuffer(ChunkCache.java:214) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.util.RandomAccessReader.reBufferAt(RandomAccessReader.java:65) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:207) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.util.FileHandle.createReader(FileHandle.java:150) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getFileDataInput(SSTableReader.java:1807) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.columniterator.AbstractSSTableIterator.<init>(AbstractSSTableIterator.java:103) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.columniterator.SSTableIterator.<init>(SSTableIterator.java:49) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:72) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.io.sstable.format.big.BigTableReader.iterator(BigTableReader.java:65) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.StorageHook$1.makeRowIterator(StorageHook.java:100) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndSSTablesInTimestampOrder(SinglePartitionReadCommand.java:982) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:693) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:670) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:504) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:423) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:48) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0-zing_19.12.102.0]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165) ~[apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137) [apache-cassandra-3.11.6.jar:3.11.6]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:113) [apache-cassandra-3.11.6.jar:3.11.6]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0-zing_19.12.102.0]
{code}

Someone familiar with that error?
",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15592,IllegalStateException in gossip after removing node,"In one of our test environments we encountered the following exception:
{noformat}
2020-02-02T10:50:13.276+0100 [GossipTasks:1] ERROR o.a.c.u.NoSpamLogger$NoSpamLogStatement:97 log java.lang.IllegalStateException: Attempting gossip state mutation from illegal thread: GossipTasks:1
 at org.apache.cassandra.gms.Gossiper.checkProperThreadForStateMutation(Gossiper.java:178)
 at org.apache.cassandra.gms.Gossiper.evictFromMembership(Gossiper.java:465)
 at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:895)
 at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:78)
 at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:240)
 at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84)
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
 at java.lang.Thread.run(Thread.java:748)
java.lang.IllegalStateException: Attempting gossip state mutation from illegal thread: GossipTasks:1
 at org.apache.cassandra.gms.Gossiper.checkProperThreadForStateMutation(Gossiper.java:178) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.gms.Gossiper.evictFromMembership(Gossiper.java:465) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:895) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:78) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:240) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) [apache-cassandra-3.11.5.jar:3.11.5]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_231]
 at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_231]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_231]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_231]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_231]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_231]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.1.42.Final.jar:4.1.42.Final]
 at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_231]
{noformat}
Since CASSANDRA-15059 we check that all state changes are performed in the GossipStage but it seems like it was still performed in the ""current"" thread [here|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/gms/Gossiper.java#L895]. It should be as simple as adding a
{code:java}
runInGossipStageBlocking(() ->)
{code}
for it.

I'll upload patches for 3.0, 3.11 and 4.0.
  ",N/A,"3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15567,Allow EXTRA_CLASSPATH to work in tarball/source installations,Both the debian and redhat packaging modify cassandra.in.sh to support the EXTRA_CLASSPATH variable.  It would be nice to just put this in cassandra.in.sh proper so people without a package installation can use it as well.,N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15565,Fix flaky test org.apache.cassandra.index.internal.CassandraIndexTest indexCorrectlyMarkedAsBuildAndRemoved,"{code}
junit.framework.AssertionFailedError: Got more rows than expected. Expected 1 but got 4.
	at org.apache.cassandra.cql3.CQLTester.assertRows(CQLTester.java:1098)
	at org.apache.cassandra.index.internal.CassandraIndexTest.indexCorrectlyMarkedAsBuildAndRemoved(CassandraIndexTest.java:499)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code}

The failure was seen on java 11.",N/A,"3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15563,Backport removal of OpenJDK warning log,"As requested on ASF Slack, creating this ticket for a backport of CASSANDRA-13916 for 3.0.",N/A,"2.2.20, 3.0.24"
CASSANDRA-15541,Add dch release version to debian packages,"Add to the prepare_release.sh script the `dch -r ` step that adds the version.

Currently the script will generate an UNRELEASED version.

Reference: https://lists.apache.org/thread.html/r7f8addef88553955b1057a61b29526cb974cc9037f20ce46fb26fb61%40%3Cdev.cassandra.apache.org%3E",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15539,Extract in-jvm API and tests out of Cassandra and into a separate repository,"Extract in-jvm DTest _API_ and tests into a separate repository that is shared between Cassandra branches. Tests themselves should be buildable using just API, which is not  the case now, since cluster creation relies on impl package, since we do not have factories / constructors in API.

Main goals we’re trying to achieve:
1. We should be able to fail a build on API incompatibility between versions 
2. Make it as easy as possible to detect break APIs between versions. 
3. Make development of _tests_ based on in-jvm framework simpler
4. Reduce surface area of impact when making modifications to tests 

Potentially, we’d also like to use a plugin to detect API incompatibilities between in-jvm DTest API and in-branch implementations, and start running tests using shared in-jvm test repository with each existing implementation in the branch. This entails both running tests for all branches whenever there’s a change in tests jar and running tests for a specific branch whenever the branch has changed.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15530,Update CHANGES.txt with Add cross-DC latency metrics (CASSANDRA-11569),"h1. Background
Question posed on ASF Slack on whether there is a metric for cross-DC latency. I then discovered that there's a typo in {{CHANGES.txt}}.

h1. Action
Replace:

{noformat}
 * Add cross-DC latency metrics (CASSANDRA-11596)
{noformat}

with:

{noformat}
 * Add cross-DC latency metrics (CASSANDRA-11569)
{noformat}",N/A,"3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15509,In-jvm upgrade dtest version parsing does not support 4.0 alpha/beta/rc builds,"for example:

https://circleci.com/gh/krummas/cassandra/2686",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15508,Failing jvm dtest: FailingRepairTest.testFailingMessage,"It seems we can't run parameterized unit tests with {{ant testsome}}:

{code}
$ ant testsome -Dtest.name=org.apache.cassandra.distributed.test.FailingRepairTest -Dtest.methods=testFailingMessage
....
[junit-timeout] Testcase: initializationError(org.junit.runner.manipulation.Filter):    Caused an ERROR
[junit-timeout] No tests found matching Method testFailingMessage(org.apache.cassandra.distributed.test.FailingRepairTest) from org.junit.internal.requests.ClassRequest@4d95d2a2
[junit-timeout] java.lang.Exception: No tests found matching Method testFailingMessage(org.apache.cassandra.distributed.test.FailingRepairTest) from org.junit.internal.requests.ClassRequest@4d95d2a2
[junit-timeout]         at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
{code}",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15506,Run in-jvm upgrade dtests in circleci,We should run the in-jvm upgrade dtests in circleci,N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15505,Add message interceptors to in-jvm dtests,Currently we only have means to filter messages in in-jvm tests. We need a facility to intercept and modify the messages between nodes for testing purposes.,N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha4, 4.0"
CASSANDRA-15503,Slow query log indicates opposite LTE when GTE operator,"Slow query log is indicating a '<=' when a "">="" operator was sent. This appears to be a logging only issue, but it threw off development for a day figuring this out. Please fix.

How to reproduce. Set slow query log timeout to 1 millisecond.

In cqlsh run

{noformat}
CREATE TABLE atable (
      .id text,
      timestamp timestamp,
      PRIMARY KEY ((id), timestamp)
 ) WITH CLUSTERING ORDER BY (timestamp DESC);
insert into atable (id, timestamp) VALUES ( '1',1);
insert into atable (id, timestamp) VALUES ( '2',2);
insert into atable (id, timestamp) VALUES ( '3',3);
insert into atable (id, timestamp) VALUES ( '4',4);
insert into atable (id, timestamp) VALUES ( '5',5);
insert into atable (id, timestamp) VALUES ( '6',6);
insert into atable (id, timestamp) VALUES ( '7',7);
insert into atable (id, timestamp) VALUES ( '8',8);
insert into atable (id, timestamp) VALUES ( '9',9);
insert into atable (id, timestamp) VALUES ( '10',10);
insert into atable (id, timestamp) VALUES ( '11',11);

select * from atable where timestamp >= '1970-01-01 00:00:00.006+0000' allow filtering;
{noformat}


In the logs it prints:

{noformat}
DEBUG 1 operations were slow in the last 5003 msecs:
<SELECT * FROM ks.atable WHERE timestamp <= 1970-01-01 00:00Z LIMIT 100>, time 7 msec - slow timeout 1 msec
{noformat}

But the query works appropriately and returns

{noformat}
 id | timestamp
----+---------------------------------
  6 | 1970-01-01 00:00:00.006000+0000
  7 | 1970-01-01 00:00:00.007000+0000
  9 | 1970-01-01 00:00:00.009000+0000
 10 | 1970-01-01 00:00:00.010000+0000
  8 | 1970-01-01 00:00:00.008000+0000
 11 | 1970-01-01 00:00:00.011000+0000

(6 rows)
{noformat}
",N/A,"3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15501,Duplicate results with DISTINCT queries in mixed mode 2.1/3.0,When a client switches coordinator from a 2.1 node to a 3.0 node it sends a 2.1 paging state to the 3.0 node. The 2.1 {{PagingState}} does not have {{remainingInPartition}} so on the 3.0 side we default this to Integer.MAX_VALUE. This value is then used to decide if the lastKey should be included in the result.,N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-15497,Implement node bootstrap in in-JVM tests,"Currently, we do not have an ability to add nodes to the running in-jvm cluster, either by bootstrap or replacement process. We need to add an ability to add nodes in inactive state, start them up, and bootstrap to test streaming, range movements, and operations that occur during these processes.",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha4, 4.0"
CASSANDRA-15496,Declarative Jenkins pipeline builds (and removing `ant test-all`),"*Declarative Jenkins pipeline*
Currently all the jenkins build jobs are generated by the DSL job from the cassandra-builds repository.
For a given branch, there are numerous builds: artifacts, unit tests, and dtests.
Using a (declarative) {{Jenkinsfile}} in-tree these builds can be put together into per-branch pipelines.

Per-branch pipelines will give a nicer UI (via blue ocean) and one place to look at test results for any branch and commit. From there we can post one complete test summary on commits back to the related jira ticket.

ref: https://lists.apache.org/thread.html/rc8d5a55142ea3bfb742e3347b8ea924946796bad03a1e089c8fb9ee7%40%3Cdev.cassandra.apache.org%3E

*Removing {{`ant test-all`}}*
Currently the test-all target does nothing but execute the test target.
This is because the test targets finishes with unit test failures.

As these jenkins jobs are automated here: https://github.com/apache/cassandra-builds/blob/master/jenkins-dsl/cassandra_job_dsl_seed.groovy#L44 ; it is easy enough to remove the ""test-all"" target, replacing it with separate jobs for 'stress-test', 'fqltool-test', and 'long-test'.",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15489, Allow dtest jar directory to be configurable,"In some circumstances, we may want to use a non-hard-coded directory as the source for dtest jars. We should allow for a system property to change the default `build` directory.",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15487,Reduce LongBTreeTest iteration count,"LongBTreeTest 2.2, 3.0, and 3.11 still takes 8+ hours to run because of the large iteration counts in it.

In trunk this has already fixed via CASSANDRA-9989

The iteration count should be reduced to something that is reasonable for both ASF Jenkins and CircleCI (ie less than an hour). For longer burns one can just go in at edit the iteration count value.",N/A,"2.2.16, 3.0.20, 3.11.6"
CASSANDRA-15471,Run Jenkins Cqlsh Tests using the in-tree cassandra-cqlsh-tests.sh,"Make the Jenkins Cqlsh test job to use the in-tree cassandra-cqlsh-tests.sh instead of the same script from the cassandra-builds repo.

",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15460,Fix missing call to enable RPC after native transport is started in in-jvm dtests,"When starting the native transport, the original patch missed the step of calling {{StorageService.instance.setRpcReady(true);}}. This appears to only be required for counter columns, but without it you can't update a counter value.

We should add this call after starting up the native transport, and set it to {{false}} during the shutdown sequence to mimic the production code.",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15459,Short read protection doesn't work on group-by queries,"[DTest to reproduce|https://github.com/apache/cassandra-dtest/compare/master...jasonstack:srp_group_by_trunk?expand=1]: it affects all versions..

{code}
In a two-node cluster with RF = 2

Execute only on Node1:
* Insert pk=1 and ck=1 with timestamp 9
* Delete pk=0 and ck=0 with timestamp 10
* Insert pk=2 and ck=2 with timestamp 9

Execute only on Node2:
* Delete pk=1 and ck=1 with timestamp 10
* Insert pk=0 and ck=0 with timestamp 9
* Delete pk=2 and ck=2 with timestamp 10

Query: ""SELECT pk, c FROM %s GROUP BY pk LIMIT 1""
* Expect no live data, but got [0, 0]
{code}


Note: for group-by queries, SRP should use ""group counted"" to calculate limits used for SRP query, rather than ""row counted"".",N/A,"3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15447,in-jvm dtest support for subnets doesn't change seed provider subnet,"When using the `withSubnet` function on AbstractCluster.Builder, the newly-selected subnet is never used when setting up the SeedProvider in the constructor of InstanceConfig, which is hard-coded to 127.0.0.1. Because of this, clusters with any subnet other than 0, and gossip enabled, cannot start up as they have no seed provider in their subnet and what should be the seed (instance 1) doesn't think it is the seed.
",N/A,"2.2.16, 3.0.20, 3.11.8, 4.0-alpha3, 4.0"
CASSANDRA-15441,"Bump generations and document changes to system_distributed and system_traces in 3.0, 3.11",We should document all the changes to distributed system keyspaces and assign unique generations to them. In 3.0 and 3.11 this is just a documentation issue.,N/A,"3.0.20, 3.11.6"
CASSANDRA-15435,SELECT JSON does not return the correct value for empty blobs,"In an attempt to fix the side effect of a problem CASSANDRA-13868 was committed in 2.2.11, 3.0.15, 3.11.1 and trunk.
This patch introduced an issue on how empty values were rendered by {{SELECT JSON}} queries.
Instead of returning the correct value for the type a null value was now returned.
A user detected that problem for text column and opened CASSANDRA-14245 to request a fix for that problem. Unfortunately, I misunderstood the problem and the fix did not solve the real problem. It only made the code return 'an empty string instead of null values.

The proper fix is to rollback the changes made for CASSANDRA-13868 and CASSANDRA-14245.
Some unit tests also need to be added to test the behavior.
",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15433,Pending ranges are not recalculated on keyspace creation,"When a node begins bootstrapping, Cassandra recalculates pending tokens for each keyspace that exists when the state change is observed (in StorageService:handleState*). When new keyspaces are created, we do not recalculate pending ranges (around Schema:merge). As a result, writes for new keyspaces are not received by nodes in BOOT or BOOT_REPLACE modes. When bootstrapping finishes, the node which just bootstrapped will not have data for the newly created keyspace.

Consider a ring with bootstrapped nodes A, B, and C. Node D is pending, and when it finishes bootstrapping, C will cede ownership of some ranges to D. A quorum write is acknowledged by C and A. B missed the write, and the coordinator didn't send it to D at all. When D finishes bootstrapping, the quorum B+D will not contain the mutation.

Steps to reproduce:
# Join a node in BOOT mode
# Create a keyspace
# Send writes to that keyspace
# On the joining node, observe that {{nodetool cfstats}} records zero writes to the new keyspace

I have observed this directly in Cassandra 3.0, and based on my reading the code, I believe it affects up through trunk.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-15432,"The ""read defragmentation"" optimization does not work","The so-called ""read defragmentation"" that has been added way back with CASSANDRA-2503 actually does not work, and never has. That is, the defragmentation writes do happen, but they only additional load on the nodes without helping anything, and are thus a clear negative.

The ""read defragmentation"" (which only impact so-called ""names queries"") kicks in when a read hits ""too many"" sstables (> 4 by default), and when it does, it writes down the result of that read. The assumption being that the next read for that data would only read the newly written data, which if not still in memtable would at least be in a single sstable, thus speeding that next read.

Unfortunately, this is not how this work. When we defrag and write the result of our original read, we do so with the timestamp of the data read (as we should, changing the timestamp would be plain wrong). And as a result, following reads will read that data first, but will have no way to tell that no more sstables should be read. Technically, the [{{reduceFilter}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L830] call will not return {{null}} because the {{currentMaxTs}} will be higher than at least some of the data in the result, and this until we've read from as many sstables than in the original read.

I see no easy way to fix this. It might be possible to make it work with additional per-sstable metadata, but nothing sufficiently simple and cheap to be worth it comes to mind. And I thus suggest simply removing that code.

For the record, I'll note that there is actually a 2nd problem with that code: currently, we ""defrag"" a read even if we didn't got data for everything that the query requests. This also is ""wrong"" even if we ignore the first issue: a following read that would read the defragmented data would also have no way to know to not read more sstables to try to get the missing parts. This problem would be fixeable, but is obviously overshadowed by the previous one anyway.

Anyway, as mentioned, I suggest to just remove the ""optimization"" (which again, never optimized anything) altogether, and happy to provide the simple patch.

The only question might be in which versions? This impact all versions, but this isn't a correction bug either, ""just"" a performance one. So do we want 4.0 only or is there appetite for earlier?
",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15430,Cassandra 3.0.18: BatchMessage.execute - 10x more on-heap allocations compared to 2.1.18,"In a 6 node loadtest cluster, we have been running with 2.1.18 a certain production-like workload constantly and sufficiently. After upgrading one node to 3.0.18 (remaining 5 still on 2.1.18 after we have seen that sort of regression described below), 3.0.18 is showing increased CPU usage, increase GC, high mutation stage pending tasks, dropped mutation messages ...

Some spec. All 6 nodes equally sized:
 * Bare metal, 32 physical cores, 512G RAM
 * Xmx31G, G1, max pause millis = 2000ms
 * cassandra.yaml basically unchanged, thus same settings in regard to number of threads, compaction throttling etc.

Following dashboard shows highlighted areas (CPU, suspension) with metrics for all 6 nodes and the one outlier being the node upgraded to Cassandra 3.0.18.
 !dashboard.png|width=1280!

Additionally we see a large increase on pending tasks in the mutation stage after the upgrade:
 !mutation_stage.png!

And dropped mutation messages, also confirmed in the Cassandra log:
{noformat}
INFO  [ScheduledTasks:1] 2019-11-15 08:24:24,780 MessagingService.java:1022 - MUTATION messages were dropped in last 5000 ms: 41552 for internal timeout and 0 for cross node timeout
INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,157 StatusLogger.java:52 - Pool Name                    Active   Pending      Completed   Blocked  All Time Blocked
INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,168 StatusLogger.java:56 - MutationStage                   256     81824     3360532756         0                 0

INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,168 StatusLogger.java:56 - ViewMutationStage                 0         0              0         0                 0

INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,168 StatusLogger.java:56 - ReadStage                         0         0       62862266         0                 0

INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,169 StatusLogger.java:56 - RequestResponseStage              0         0     2176659856         0                 0

INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,169 StatusLogger.java:56 - ReadRepairStage                   0         0              0         0                 0

INFO  [ScheduledTasks:1] 2019-11-15 08:24:25,169 StatusLogger.java:56 - CounterMutationStage              0         0              0         0                 0
...
{noformat}
Judging from a 15min JFR session for both, 3.0.18 and 2.1.18 on a different node, high-level, it looks like the code path underneath {{BatchMessage.execute}} is producing ~ 10x more on-heap allocations in 3.0.18 compared to 2.1.18.
 !jfr_allocations.png!

Left => 3.0.18
 Right => 2.1.18

JFRs zipped are exceeding the 60MB limit to directly attach to the ticket. I can upload them, if there is another destination available.",N/A,3.0.23
CASSANDRA-15426,Cassandra 3.11.5 fails to start on Windows ,"Cassandra 3.11.5 fails to start on Windows server 2012 R2. with following error trace.

Cassandra 3.11.4 doesn't fail on Windows 2012 R2. 

   

org.apache.cassandra.io.FSReadError: java.io.IOException: Invalid folder descriptor trying to create log replica C:\Users\Administrator\Downloads\apache-cassandra-3.11.5-bin.tar\apache-cassandra-3.11.5-bin\apache-cassandra-3.11.5\data\data\system\local-7ad54392bcdd35a684174e047860b377
 at org.apache.cassandra.db.lifecycle.LogReplica.create(LogReplica.java:58) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogReplicaSet.maybeCreateReplica(LogReplicaSet.java:86) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogFile.makeRecord(LogFile.java:311) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogFile.add(LogFile.java:283) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogTransaction.trackNew(LogTransaction.java:139) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LifecycleTransaction.trackNew(LifecycleTransaction.java:528) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:81) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:92) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:102) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.create(SimpleSSTableMultiWriter.java:119) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.createSSTableMultiWriter(AbstractCompactionStrategy.java:588) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.compaction.CompactionStrategyManager.createSSTableMultiWriter(CompactionStrategyManager.java:1027) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.ColumnFamilyStore.createSSTableMultiWriter(ColumnFamilyStore.java:532) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable$FlushRunnable.createFlushWriter(Memtable.java:504) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable$FlushRunnable.<init>(Memtable.java:443) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable$FlushRunnable.<init>(Memtable.java:420) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:307) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:298) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1153) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1118) [apache-cassandra-3.11.5.jar:3.11.5]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
 at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_161]
Caused by: java.io.IOException: Invalid folder descriptor trying to",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15424,CVE-2018-1320 (The libthrift component is vulnerable to Improper Access Control),"*Description :**Description :* *Severity :* CVE CVSS 3.0: 7.5Sonatype CVSS 3.0: 8.2
 
 *Weakness :* CVE CWE: 20
 
 *Source :* National Vulnerability Database
 
 *Categories :* Data 
 *Description from CVE :* Apache Thrift Java client library versions 0.5.0 through 0.11.0 can bypass SASL negotiation isComplete validation in the org.apache.thrift.transport.TSaslTransport class. An assert used to determine if the SASL handshake had successfully completed could be disabled in production settings making the validation incomplete.
 
 *Explanation :* The libthrift component is vulnerable to Improper Access Control. The open() method of the TSaslTransport class incorrectly uses an assertion to validate whether or not the SASL handshake has successfully completed. In some cases, such as production builds, the assertion functionality can be disabled rendering the validation incomplete. In such a case, an attacker can exploit this by being able to login without actually successfully completing the SASL handshake. 
 *Detection :* The application is vulnerable by using this component. 
 *Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue. 
 *Root Cause :* Cassandra-2.2.5.nupkgTSaslTransport.class : [0.5.0, 0.12.0)
 
 *Advisories :* Project: https://lists.apache.org/thread.html/da5234b5e78f1c99190407f...
 
 *CVSS Details :* CVE CVSS 3.0: 7.5
*Occurences (Paths) :* ["" apache-cassandra.zip/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/bin/cqlsh.bat"" ; "" apache-cassandra.zip/bin/debug-cql.bat"" ; "" apache-cassandra.zip/bin/source-conf.ps1"" ; "" apache-cassandra.zip/bin/sstableloader.bat"" ; "" apache-cassandra.zip/bin/sstablescrub.bat"" ; "" apache-cassandra.zip/bin/sstableupgrade.bat"" ; "" apache-cassandra.zip/bin/sstableverify.bat"" ; "" apache-cassandra.zip/bin/stop-server"" ; "" apache-cassandra.zip/bin/stop-server.bat"" ; "" apache-cassandra.zip/bin/stop-server.ps1"" ; "" apache-cassandra.zip/conf/README.txt"" ; "" apache-cassandra.zip/conf/cassandra-rackdc.properties"" ; "" apache-cassandra.zip/conf/cassandra-topology.properties"" ; "" apache-cassandra.zip/conf/commitlog_archiving.properties"" ; "" apache-cassandra.zip/conf/triggers/README.txt"" ; "" apache-cassandra.zip/lib/ST4-4.0.8.jar"" ; "" apache-cassandra.zip/lib/airline-0.6.jar"" ; "" apache-cassandra.zip/lib/antlr-runtime-3.5.2.jar"" ; "" apache-cassandra.zip/lib/commons-cli-1.1.jar"" ; "" apache-cassandra.zip/lib/commons-lang3-3.1.jar"" ; "" apache-cassandra.zip/lib/commons-math3-3.2.jar"" ; "" apache-cassandra.zip/lib/compress-lzf-0.8.4.jar"" ; "" apache-cassandra.zip/lib/concurrentlinkedhashmap-lru-1.4.jar"" ; "" apache-cassandra.zip/lib/disruptor-3.0.1.jar"" ; "" apache-cassandra.zip/lib/ecj-4.4.2.jar"" ; "" apache-cassandra.zip/lib/futures-2.1.6-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/high-scale-lib-1.0.6.jar"" ; "" apache-cassandra.zip/lib/jamm-0.3.0.jar"" ; "" apache-cassandra.zip/lib/javax.inject.jar"" ; "" apache-cassandra.zip/lib/jbcrypt-0.3m.jar"" ; "" apache-cassandra.zip/lib/jcl-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/joda-time-2.4.jar"" ; "" apache-cassandra.zip/lib/json-simple-1.1.jar"" ; "" apache-cassandra.zip/lib/libthrift-0.9.2.jar"" ; "" apache-cassandra.zip/lib/licenses/ST4-4.0.8.txt"" ; "" apache-cassandra.zip/lib/licenses/antlr-runtime-3.5.2.txt"" ; "" apache-cassandra.zip/lib/licenses/compress-lzf-0.8.4.txt"" ; "" apache-cassandra.zip/lib/licenses/concurrent-trees-2.4.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ecj-4.4.2.txt"" ; "" apache-cassandra.zip/lib/licenses/futures-2.1.6.txt"" ; "" apache-cassandra.zip/lib/licenses/high-scale-lib-1.0.6.txt"" ; "" apache-cassandra.zip/lib/licenses/jbcrypt-0.3m.txt"" ; "" apache-cassandra.zip/lib/licenses/jcl-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/jna-4.2.2.txt"" ; "" apache-cassandra.zip/lib/licenses/jstackjunit-0.0.1.txt"" ; "" apache-cassandra.zip/lib/licenses/log4j-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-classic-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-core-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/lz4-1.3.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-core-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-jvm-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ohc-0.4.4.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config-base-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config3-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/sigar-1.6.4.txt"" ; "" apache-cassandra.zip/lib/licenses/six-1.7.3.txt"" ; "" apache-cassandra.zip/lib/licenses/slf4j-api-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/stream-2.5.2.txt"" ; "" apache-cassandra.zip/lib/log4j-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/logback-classic-1.1.3.jar"" ; "" apache-cassandra.zip/lib/logback-core-1.1.3.jar"" ; "" apache-cassandra.zip/lib/lz4-1.3.0.jar"" ; "" apache-cassandra.zip/lib/metrics-core-3.1.0.jar"" ; "" apache-cassandra.zip/lib/metrics-logback-3.1.0.jar"" ; "" apache-cassandra.zip/lib/sigar-1.6.4.jar"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-pa-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-s390x-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal64-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-amd64-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.lib"" ; "" apache-cassandra.zip/lib/six-1.7.3-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/slf4j-api-1.7.7.jar"" ; "" apache-cassandra.zip/lib/snakeyaml-1.11.jar"" ; "" apache-cassandra.zip/lib/snappy-java-1.1.1.7.jar"" ; "" apache-cassandra.zip/lib/stream-2.5.2.jar"" ; "" apache-cassandra.zip/lib/thrift-server-0.3.7.jar"" ; "" apache-cassandra.zip/pylib/cqlshlib/__init__.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/saferscanner.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/sslhandling.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/ansi_colors.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/basecase.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cql_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_commands.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_invocation.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/winpty.py"" ; "" apache-cassandra.zip/tools/bin/cassandra-stress.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/tools/bin/sstableexpiredblockers.bat"" ; "" apache-cassandra.zip/tools/bin/sstablelevelreset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablemetadata.bat"" ; "" apache-cassandra.zip/tools/bin/sstableofflinerelevel.bat"" ; "" apache-cassandra.zip/tools/bin/sstablerepairedset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablesplit.bat""]
*CVE :* CVE-2018-1320
*URL :* http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-1320",N/A,"2.2.x, 3.0.26, 3.11.12"
CASSANDRA-15420,CVE-2019-0205(Apache Thrift all versions up to and including 0.12.0) on version Cassendra 3.11.4,"*Description :**Description :* *Severity :* CVE CVSS 3: 7.5Sonatype CVSS 3: 7.5
 
 *Weakness :* CVE CWE: 835
 
 *Source :* National Vulnerability Database
 
 *Categories :* Data 
 *Description from CVE :* In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.
 
 *Explanation :* This issue has undergone the Sonatype Fast-Track process. For more information, please see the Sonatype Knowledge Base Guide. 
 *Detection :* The application is vulnerable by using this component. 
 *Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue.Note: If this component is included as a bundled/transitive dependency of another component, there may not be an upgrade path. In this instance, we recommend contacting the maintainers who included the vulnerable package. Alternatively, we recommend investigating alternative components or a potential mitigating control. 
 *Advisories :* Project: http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mâ€¦
 
 *CVSS Details :* CVE CVSS 3: 7.5CVSS Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H
*Occurences (Paths) :* [""apache-cassandra.zip"" ; ""apache-cassandra.zip""]
*CVE :* CVE-2019-0205
*URL :* http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205
*Remediation :* This component does not have any non-vulnerable Version. Please contact the vendor to get this vulnerability fixed.",N/A,"3.0.26, 3.11.12"
CASSANDRA-15409,nodetool compactionstats showing extra pending task for TWCS,"Summary: nodetool compactionstats showing extra pending task for TWCS
-----------------
The output of {{nodetool compactionstats}}can show ""pending tasks: 1"" when there are actually none. This seems to be a consistent problem in testing C* trunk. In my testing, it looks like the {{nodetool compactionstats}} counter output is consistently off by 1 as compared to the table output of the tasks

 

testing with {{concurrent_compactors: 8}}

In 12 hours it never ended, always showing 1 pending job

 ",N/A,"3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15408,Cassandra throws SyntaxException for obsolete keywords that Thrift API permits,"In [this refactor|https://github.com/apache/cassandra/commit/b31845c4a7982358a7c5bfd9bcf572fda6c1bfa9#diff-826a67bf1ae2e45372a35a6a2a6f3f3cL74] of CFPropDefs to TableAttributes for CASSANDRA-9712, three obsolete keywords were removed:
{code:java}
        obsoleteKeywords.add(""index_interval"");
        obsoleteKeywords.add(""replicate_on_write"");
        obsoleteKeywords.add(""populate_io_cache_on_flush"");
{code}
 
The Thrift API continues to reference these keywords as deprecated, so it's not clear that they are actually unsupported.

Could we either add them back as obsoleteKeywords, or add a change log that statements with these properties will fail (There is already a changelog about ""index_interval"" but not the other two)?  I understand that the Thrift API is totally deprecated so I don't feel strongly about cleaning it up.",N/A,"3.0.20, 3.11.6"
CASSANDRA-15405,Mixed mode reads on compact storage tables can return incomplete results,"In mixed mode (2.1/3.0), when coordinating a read on a 2.1 node, reading data from 3.0 nodes, we [incorrectly trim|https://github.com/apache/cassandra/blob/53f604dc1789a800dbcbc3c8aee77f8f36b8b5db/src/java/org/apache/cassandra/db/LegacyLayout.java#L529] the result (if it has tombstones) when preparing it for the 2.1 node, this is then [interpreted by the 2.1 node|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/service/pager/AbstractQueryPager.java#L110] as the pager has been exhausted.",N/A,"3.0.20, 3.11.6"
CASSANDRA-15401,"nodetool stop help is missing ""ANTICOMPACTION""",The {{nodetool stop}} command can be used to stop certain activities including anti-compaction. While we can run {{nodetool stop ANTICOMPACTION}} on a given node the help menu does not list it.,N/A,"3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15400,Cassandra 3.0.18 went OOM several hours after joining a cluster,"We have been moving from Cassandra 2.1.18 to Cassandra 3.0.18 and have been facing an OOM two times with 3.0.18 on newly added nodes joining an existing cluster after several hours being successfully bootstrapped.

Running in AWS:
* m5.2xlarge, EBS SSD (gp2)
* Xms/Xmx12G, Xmn3G, CMS GC, OpenJDK8u222
* 4 compaction threads, throttling set to 32 MB/s

What we see is a steady increase in the OLD gen over many hours.
!cassandra_jvm_metrics.png!

* The node started to join / auto-bootstrap the cluster on Oct 30 ~ 12:00
* It basically finished joining the cluster (UJ => UN) ~ 19hrs later on Oct 31 ~ 07:00 also starting to be a member of serving client read requests
!cassandra_operationcount.png!

Memory-wise (on-heap) it didn't look that bad at that time, but old gen usage constantly increased.

We see a correlation in increased number of SSTables and pending compactions.
!cassandra_sstables_pending_compactions.png!

Until we reached the OOM somewhere in Nov 1 in the night. After a Cassandra startup (metric gap in the chart above), number of SSTables + pending compactions is still high, but without facing memory troubles since then.

This correlation is confirmed by the auto-generated heap dump with e.g. ~ 5K BigTableReader instances with ~ 8.7GByte retained heap in total.
!cassandra_hprof_dominator_classes.png!

Having a closer look on a single object instance, seems like each instance is ~ 2MByte in size.
!cassandra_hprof_bigtablereader_statsmetadata.png!
With 2 pre-allocated byte buffers (highlighted in the screen above) at 1 MByte each

We have been running with 2.1.18 for > 3 years and I can't remember dealing with such OOM in the context of extending a cluster.

While the MAT screens above are from our production cluster, we partly can reproduce this behavior in our loadtest environment (although not going full OOM there), thus I might be able to share a hprof from this non-prod environment if needed.

Thanks a lot.



",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15398,Fix system_traces creation timestamp; optimise system keyspace upgrades,"We have introduced changes to system_traces tables in 3.0 (removal of default_time_to_live, lowering of bloom_filter_fp_chance). We did not, however, bump the timestamp with which we add the tables to schema, still defaulting to 0. As a result, for clusters that upgraded from 2.1/2.2, on bounce we would always detect a mismatch between actual and desired table definitions, always try to reconcile it by issuing migration tasks, but have them never override the existing definitions in place.

Additionally, prior to 2.0.2 (CASSANDRA-6016) we’d use a ‘real’ timestamp, so for clusters that started on even earlier versions of C* (say, 1.2), a bump to the timestamp by 1 would be insufficient, and a larger generation is necessary (I picked Jan 1 2020 as cut-off date).

The patch also optimises the process of upgrading replicated system tables. Instead of issuing a migration task for every table that changed for every node, we batch all changes into a single schema migration task.",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15386,Use multiple data directories in the in-jvm dtests,We should default to using 3 data directories when running the in-jvm dtests.,N/A,"2.2.19, 3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15385,Ensure that tracing doesn't break connections in 3.x/4.0 mixed mode by default,,N/A,"3.0.20, 3.11.6"
CASSANDRA-15373,validate value sizes in LegacyLayout,"In 2.1, all values are serialized as variable length blobs, with a length prefix, followed by the actual value, even with fixed width types like int32. The 3.0 storage engine, on the other hand, omits the length prefix for fixed width types. Since the length of fixed width types are not validated on the 3.0 write path, writing data for a fixed width type from an incorrectly sized byte buffer will over or underflow the space allocated for it, corrupting the remainder of that partition or indexed region from being read. This is not discovered until we attempt to read the corrupted value. This patch updates LegacyLayout to throw a marshal exception if it encounters an unexpected value size for fixed size columns.",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15371,Incorrect messaging service version breaks in-JVM upgrade tests on trunk,"The in-JVM upgrade tests on trunk currently fail because the messaging
 version for internode messaging is selected as {{MessagingService.current_version}},
 a regression from the implementation in CASSANDRA-15078.",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15368,Failing to flush Memtable without terminating process results in permanent data loss,"{{Memtable}} do not contain records that cover a precise contiguous range of {{ReplayPosition}}, since there are only weak ordering constraints when rolling over to a new {{Memtable}} - the last operations for the old {{Memtable}} may obtain their {{ReplayPosition}} after the first operations for the new {{Memtable}}.

Unfortunately, we treat the {{Memtable}} range as contiguous, and invalidate the entire range on flush.  Ordinarily we only invalidate records when all prior {{Memtable}} have also successfully flushed.  However, in the event of a flush that does not terminate the process (either because of disk failure policy, or because it is a software error), the later flush is able to invalidate the region of the commit log that includes records that should have been flushed in the prior {{Memtable}}

More problematically, this can also occur on restart without any associated flush failure, as we use commit log boundaries written to our flushed sstables to filter {{ReplayPosition}} on recovery, which is meant to replicate our {{Memtable}} flush behaviour above.  However, we do not know that earlier flushes have completed, and they may complete successfully out-of-order.  So any flush that completes before the process terminates, but began after another flush that _doesn’t_ complete before the process terminates, has the potential to cause permanent data loss.
",N/A,"2.2.16, 3.0.20, 3.11.7, 4.0-alpha3, 4.0"
CASSANDRA-15367,Memtable memory allocations may deadlock,"* Under heavy contention, we guard modifications to a partition with a mutex, for the lifetime of the memtable.
* Memtables block for the completion of all {{OpOrder.Group}} started before their flush began
* Memtables permit operations from this cohort to fall-through to the following Memtable, in order to guarantee a precise commitLogUpperBound
* Memtable memory limits may be lifted for operations in the first cohort, since they block flush (and hence block future memory allocation)

With very unfortunate scheduling
* A contended partition may rapidly escalate to a mutex
* The system may reach memory limits that prevent allocations for the new Memtable’s cohort (C2) 
* An operation from C2 may hold the mutex when this occurs
* Operations from a prior Memtable’s cohort (C1), for a contended partition, may fall-through to the next Memtable
* The operations from C1 may execute after the above is encountered by those from C2
",N/A,"3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15365,Add primary key liveness info when skipping illegal cells,"In CASSANDRA-15086/CASSANDRA-15178 we started skipping the illegal legacy cells, problem is that if the row only contains illegal cells, we return a totally empty row which breaks stats collection: https://github.com/apache/cassandra/blob/93815db9853cb592edf13d82e91dc2e9d172f01f/src/java/org/apache/cassandra/db/rows/Rows.java#L70

If the row only has these invalid cells, we should add a primary key liveness info to it to match the 2.1 behaviour.",N/A,"3.0.20, 3.11.6"
CASSANDRA-15364,Avoid over scanning data directories in LogFile.verify(),"We currently list the data directory for every {{REMOVE}} record in the file in {{LogFile.verify()}} - this can get very expensive during startup when we call {{LogTransaction.removeUnfinishedLeftovers()}}. In {{LogRecord.getExistingFiles(Set<String> absoluteFilePaths)}} we also fully parse the file name of the sstables found, here we only need to prefix match.",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15363,Read repair in mixed mode between 2.1 and 3.0 on COMPACT STORAGE tables causes unreadable sstables after upgrade,"if we have a table like this:

{{CREATE TABLE tbl (pk ascii, b boolean, v blob, PRIMARY KEY (pk)) WITH COMPACT STORAGE}}

with a cluster where node1 is 2.1 and node2 is 3.0 (during upgrade):

* node2 coordinates a delete {{DELETE FROM tbl WHERE pk = 'something'}} which node1 does not get
* node1 coordinates a quorum read {{SELECT * FROM tbl WHERE id = 'something'}} which causes a read repair
* this makes node1 flush an sstable like this:
{code}
[
{""key"": ""something"",
 ""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1571388944364000,""localDeletionTime"":1571388944}},
 ""cells"": [[""b"",""b"",1571388944364000,""t"",1571388944],
           [""v"",""v"",1571388944364000,""t"",1571388944]]}
]
{code}
(It has range tombstones which are covered by the partition deletion)

Then, when we upgrade this node to 3.0 and try to read or run upgradesstables, we get this:
{code}
ERROR [node1_CompactionExecutor:1] node1 2019-10-18 10:44:11,325 DebuggableThreadPoolExecutor.java:242 - Error in ThreadPoolExecutor
java.lang.UnsupportedOperationException: null
	at org.apache.cassandra.db.LegacyLayout.extractStaticColumns(LegacyLayout.java:779) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.readStaticRow(SSTableSimpleIterator.java:120) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:57) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:362) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.isReverseOrder(LazilyInitializedUnfilteredRowIterator.java:65) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:103) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:94) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:442) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:144) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:227) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:190) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionManager$8.runMayThrow(CompactionManager.java:675) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[dtest-3.0.19.jar:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83) [dtest-3.0.19.jar:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{code}",N/A,"3.0.19, 3.11.5, 4.0-alpha2, 4.0"
CASSANDRA-15360,add mick's gpg key to project's KEYS file,"Currently only four individual's have their key in the project's KEYS file, and only one of these people are taking on the release manager role.

The patch adds my gpg public key to the project's KEYS file found at https://dist.apache.org/repos/dist/release/cassandra/KEYS

My gpg public key here has the fingerprint 
 ABCD 3108 336F 7CC6 567E  769F FDD3 B769 B21C 125C

References:
 - https://www.apache.org/dev/release-signing#keys-policy
 - http://www.apache.org/legal/release-policy.html
 - [dev ML thread ""Improving our frequency of (patch) releases, and letting committers make releases""|https://lists.apache.org/thread.html/660ed8c73e7b79afa610f3e45f37914ef43a4358a85a99c8b4b0288a@<dev.cassandra.apache.org>]",N/A,3.11.5
CASSANDRA-15347,Add client testing capabilities to in-jvm tests,Allow testing native transport code path using in-jvm tests.,N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15340,Resource leak in CompressedSequentialWriter,"In CompressedSequentialWriter, we reallocate the {{compressed}} buffer if the existing buffer is not large enough. These buffers are usually direct byte buffers, and we don't explicitly release their memory here, which delays release until the buffer is gc'd",N/A,"3.0.19, 3.11.5, 4.0-alpha2, 4.0"
CASSANDRA-15335,Node can corrupt gossip state and become unreplaceable,"In {{StorageService#prepareToJoin}}, a starting node first sends out an endpoint state without any tokens. Later, in {{StorageService#finishJoiningRing}} it sends out an endpoint state _with_ it’s tokens. If that node dies between these 2 events and cannot be restarted due to some unrecoverable error, the ring’s gossip state will be missing tokens for that node. This won’t cause any immediate data loss since TMD is populated from system.peers, but it will prevent a replacement node from associating that address with it’s tokens and replacing it. It could also cause data loss if other nodes are added to the ring and don’t see an owned token where there should be one.",N/A,"3.0.19, 3.11.5, 4.0-alpha2, 4.0"
CASSANDRA-15319,Add support for network topology and tracing to in-JVM dtests.,"While working on CASSANDRA-15318, testing it properly with an in-JVM test requires setting up the network topology and tracing requests to check which nodes performed forwarding.
  
In support of testing, make it possible to create in-JVM clusters with nodes appearing in different datacenter/racks and add support for executing queries with tracing enabled.",N/A,"2.2.15, 3.0.19, 3.11.8, 4.1-alpha1, 4.1"
CASSANDRA-15293,Static columns not include in mutation size calculation,"Patch to include any updates to static columns in the data size calculation of PartitionUpdate.


||Patch||
|[Trunk/3.11|https://github.com/vincewhite/cassandra/commits/include_update_to_static_columns_in_update_size]|

",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15292,Point-in-time recovery ignoring timestamp of static column updates,"During point-in-time recovery org.apache.cassandra.db.partitions.PartitionUpdate#maxTimestamp is checked to see if any write timestamps in the update exceed the recovery point. If any of the timestamps do exceed this point the the commit log replay is stopped.

Currently maxTimestamp only iterates over the regular rows in the update and doesn't check for any included updates to static columns. If a ParitionUpdate only contains updates to static columns then maxTimestamp will return Long.MIN_VALUE and always be replayed. 

This generally isn't much of an issue, except for non-dense compact storage tables which are implemented in the 3.x storage engine in large part with static columns. In this case the commit log will always continue applying updates to them past the recovery point until it hits an update to a different table with regular columns or reaches the end of the commit logs.

 
||Patch||
|[3.11|https://github.com/vincewhite/cassandra/commits/3_11_check_static_column_timestamps_commit_log_archive]|
|[Trunk|https://github.com/vincewhite/cassandra/commits/trunk_check_static_column_timestamps]|",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15289,bad merge reverted CASSANDRA-14993,,N/A,"3.0.19, 3.11.5, 4.0-alpha2, 4.0"
CASSANDRA-15273,cassandra does not start with new systemd version,"After update systemd with  fixed vulnerability https://access.redhat.com/security/cve/cve-2018-16888, the cassandra service does not start correctly.

Environment: RHEL 7, systemd-219-67.el7_7.1, cassandra-3.11.4-1 (https://www.apache.org/dist/cassandra/redhat/311x/cassandra-3.11.4-1.noarch.rpm)

---------------------------------------------------------------

systemctl status cassandra
● cassandra.service - LSB: distributed storage system for structured data
 Loaded: loaded (/etc/rc.d/init.d/cassandra; bad; vendor preset: disabled)
 Active: failed (Result: resources) since Fri 2019-08-09 17:20:26 MSK; 1s ago
 Docs: man:systemd-sysv-generator(8)
 Process: 2414 ExecStop=/etc/rc.d/init.d/cassandra stop (code=exited, status=0/SUCCESS)
 Process: 2463 ExecStart=/etc/rc.d/init.d/cassandra start (code=exited, status=0/SUCCESS)
 Main PID: 1884 (code=exited, status=143)

Aug 09 17:20:23 desktop43.example.com systemd[1]: Unit cassandra.service entered failed state.
Aug 09 17:20:23 desktop43.example.com systemd[1]: cassandra.service failed.
Aug 09 17:20:23 desktop43.example.com systemd[1]: Starting LSB: distributed storage system for structured data...
Aug 09 17:20:23 desktop43.example.com su[2473]: (to cassandra) root on none
Aug 09 17:20:26 desktop43.example.com cassandra[2463]: Starting Cassandra: OK
Aug 09 17:20:26 desktop43.example.com systemd[1]: New main PID 2545 does not belong to service, and PID file is not owned by root. Refusing.
Aug 09 17:20:26 desktop43.example.com systemd[1]: New main PID 2545 does not belong to service, and PID file is not owned by root. Refusing.
Aug 09 17:20:26 desktop43.example.com systemd[1]: Failed to start LSB: distributed storage system for structured data.
Aug 09 17:20:26 desktop43.example.com systemd[1]: Unit cassandra.service entered failed state.
Aug 09 17:20:26 desktop43.example.com systemd[1]: cassandra.service failed.",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-15266,java internal exception on attempt to UPDATE a row using CONTAINS operator,"kostja@atlas ~ % cqlsh -ucassandra -pcassandra
Connected to My Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> CREATE KEYSPACE t1 WITH replication = \{'class':'SimpleStrategy', 'replication_factor' : 1};
cassandra@cqlsh> use t1;
cassandra@cqlsh:t1> create table t (a int, b frozen<map<int, int>>, c int, primary key (a, b));
cassandra@cqlsh:t1> insert into t (a, b, c) values (1, \{1:1, 2:2}, 3);
cassandra@cqlsh:t1> update t set c=3 where a=1 and b contains 1;
ServerError: java.lang.UnsupportedOperationException

 

Server log file:

```

ERROR [Native-Transport-Requests-1] 2019-08-07 17:02:59,283 QueryMessage.java:129 - Unexpected error during query 
java.lang.UnsupportedOperationException: null
 at org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$ContainsRestriction.appendTo(SingleColumnRestriction.java:454) ~[a
pache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions.valuesAsClustering(ClusteringColumnRestrictions.java:109) ~[a
pache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getClusteringColumns(StatementRestrictions.java:770) ~[apache-cassan
dra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.createClustering(ModificationStatement.java:312) ~[apache-cassandra-3.
11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:677) ~[apache-cassandra-3.11.4.j
ar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:635) ~[apache-cassandra-3.11.4
.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:437) ~[apache-cassa
ndra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:425) ~[apache-cassandra-3.11.4.jar:
3.11.4]
 at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:225) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:256) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:241) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:566) [apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.4.jar:3.11.4]
 at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44
...

 ```
+Additional information for newcomers:+

{{CONTAINS}} and {{CONTAINS KEY}} restrictions are not supported for {{UPDATE}} or {{DELETE}} operations but they should be properly rejected with a proper error message.
To fix that problem a new check should be added in the {{StatementRestrictions}} constructor to thrown an {{InvalidRequestException}} if the relation operator is a {{CONTAINS}} or {{CONTAINS_KEY}} and the {{StatementType}} an {{UPDATE}} or a {{DELETION}}.
Some unit tests should be added to {{UpdateTest}} an {{DeleteTest}} to test the behavior.
",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-15265,Index summary redistribution can start even when compactions are paused,"When we pause autocompaction for upgradesstables/scrub/cleanup etc we pause all compaction strategies to make sure we can grab all sstables, index summary redistribution does not pause and this can cause us to fail the operation.",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15264,Make resumable bootstrap off by default,"Following the discussion in CASSANDRA-8838, we should make resumable bootstrap off by default and do it if the user requests it.",N/A,"3.0.24, 3.11.10, 4.1-alpha1, 4.1"
CASSANDRA-15259,Selecting Index by Lowest Mean Column Count Selects Random Index,"{{CassandraIndex}} uses [{{ColumnFamilyStore#getMeanColumns}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/index/internal/CassandraIndex.java#L273], average columns per partition, which always returns the same answer for index CFs because they contain no regular columns and clustering columns aren't included in the count in Cassandra 3.0+.

 

 ",N/A,"3.0.19, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-15252,Don't consider current keyspace in prepared statement id when the query is qualified,"{{QueryProcessor.computeId}} takes into account the session's current keyspace in the MD5 digest.
{code}
String toHash = keyspace == null ? queryString : keyspace + queryString;
{code}
This is desirable for unqualified queries, because switching to a different keyspace produces a different statement. However, for a qualified query, the current keyspace makes no difference, the prepared id should always be the same.

This can lead to an infinite reprepare loop on the client. Consider this example (Java driver 3.x):
{code}
    Cluster cluster = null;
    try {
      cluster = Cluster.builder().addContactPoint(""127.0.0.1"").build();
      Session session = cluster.connect();

      session.execute(
          ""CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
      session.execute(""CREATE TABLE IF NOT EXISTS test.foo(k int PRIMARY KEY)"");

      PreparedStatement pst = session.prepare(""SELECT * FROM test.foo WHERE k=?"");

      // Drop and recreate the table to invalidate the prepared statement server-side
      session.execute(""DROP TABLE test.foo"");
      session.execute(""CREATE TABLE test.foo(k int PRIMARY KEY)"");

      session.execute(""USE test"");

      // This will try to reprepare on the fly
      session.execute(pst.bind(0));
    } finally {
      if (cluster != null) cluster.close();
    }
{code}
When the driver goes to execute the bound statement (last line before the finally block), it will get an UNPREPARED response because the statement was evicted from the server cache (as a result of dropping the table earlier).
In those cases, the driver recovers transparently by sending another PREPARE message and retrying the bound statement.
However, that second PREPARE cached the statement under a different id, because we switched to another keyspace. Yet the driver is still using the original id (stored in {{pst}}) when it retries, so it will get UNPREPARED again, etc.

I would consider this low priority because issuing a {{USE}} statement after having prepared statements is a bad idea to begin with. And even if we fix the generated id for qualified query strings, the issue will remain for unqualified ones.

We'll add a check in the driver to fail fast and avoid the infinite loop if the id returned by the second PREPARE doesn't match the original one. That might be enough to cover this issue.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-15242,Race condition between flushing and compaction stalls compaction indefinitely,"Seen on Cassandra 3.11.4 with OpenJDK 8u212, although I've seen this a few times before, also on 3.11.3. It's a rare issue so I've not bothered with trying to trace it until now.
{noformat}
DEBUG [NativePoolCleaner] 2019-07-18 01:12:41,799 ColumnFamilyStore.java:1325 - Flushing largest CFS(Keyspace='keyspacename', ColumnFamily='tablename') to free up room. Used total: 0.10/0.33, live: 0.10/0.33, flushing: 0.00/0.00, this: 0.09/0.19
DEBUG [NativePoolCleaner] 2019-07-18 01:12:41,800 ColumnFamilyStore.java:935 - Enqueuing flush of tablename: 267.930MiB (9%) on-heap, 575.580MiB (19%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:204] 2019-07-18 01:12:42,480 Memtable.java:456 - Writing Memtable-tablename@498336646(520.721MiB serialized bytes, 870200 ops, 9%/19% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
INFO  [Service Thread] 2019-07-18 01:12:43,616 GCInspector.java:284 - G1 Young Generation GC in 227ms.  G1 Eden Space: 14713618432 -> 0; G1 Old Gen: 13240876928 -> 13259198848; G1 Survivor Space: 276824064 -> 268435456;
INFO  [Service Thread] 2019-07-18 01:12:56,251 GCInspector.java:284 - G1 Young Generation GC in 206ms.  G1 Eden Space: 14713618432 -> 0; G1 Old Gen: 13259198848 -> 13285123456; G1 Survivor Space: 268435456 -> 285212672;
DEBUG [PerDiskMemtableFlushWriter_0:204] 2019-07-18 01:12:56,693 Memtable.java:485 - Completed flushing /cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db (524.023MiB) for commitlog position CommitLogPosition(segmentId=1563386911266, position=32127822)
DEBUG [MemtableFlushWriter:204] 2019-07-18 01:12:57,620 ColumnFamilyStore.java:1233 - Flushed to [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] (1 sstables, 518.714MiB), biggest 518.714MiB, smallest 518.714MiB
WARN  [CompactionExecutor:1617] 2019-07-18 01:12:57,628 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
{noformat}
This final line then starts repeating about once per minute:
{noformat}
WARN  [CompactionExecutor:1610] 2019-07-18 01:13:18,898 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1611] 2019-07-18 01:14:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1622] 2019-07-18 01:15:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1436] 2019-07-18 01:16:15,073 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1618] 2019-07-18 01:16:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1611] 2019-07-18 01:17:18,900 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1606] 2019-07-18 01:18:18,900 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1630] 2019-07-18 01:19:18,902 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1627] 2019-07-18 01:20:18,904 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1638] 2019-07-18 01:21:18,904 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1631] 2019-07-18 01:22:18,905 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1636] 2019-07-18 01:22:58,220 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1625] 2019-07-18 01:23:18,905 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.{noformat}
It will keep going like this for days, until restarted, but compaction won't run until then, so sstables pile up.",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15230,Resizing window aborts cqlsh COPY: Interrupted system call,"When resizing a terminal window running cqlsh COPY, the Python program aborts immediately with:

{{<stdin>:1:(4, 'Interrupted system call')}}

This is very annoying, as COPY commands usually run for a long time, and e.g re-attaching to a screen session with a different terminal size aborts the command. This bug affects versions 2.1, 2.2, 3.0, 3.x, and trunk.",N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-15225,FileUtils.close() does not handle non-IOException,This can lead to {{close}} not being invoked on remaining items,N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15204,Toughen up column drop/recreate validations in 3.0/3.11,"After CASSANDRA-8099 it’s no longer possible to safely drop/add columns with incompatible types. In 4.0 we validate this correctly, but in 3.0 we don’t,
and that can result in unreadable sstables (corrupted serialization headers causing simple columns to be read as complex or vice versa).

This patch brings 3.0 in line with 4.0 restrictions, making such corruption impossible.",N/A,"3.0.19, 3.11.5"
CASSANDRA-15201,LegacyLayout does not handle paging states that cross a collection column,"{{LegacyLayout.decodeBound}} assumes there is only a single extra component, referring to a column name.  In fact an encoded page boundary may include a collection column, and this occurs as a matter of course when paging a table whose last column is a collection.",N/A,3.0.19
CASSANDRA-15198,Preventing RuntimeException when the username or password is empty," !empty_username_error.jpg! 

Although this does not affect the service, it's necessary to improve code robustness.",N/A,"3.0.19, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-15193,Add ability to cap max negotiable protocol version,"3.0 and native protocol V4 introduced a change to how PagingState is serialized. Unfortunately that can break requests during upgrades: since paging states are opaque, it's possible for a client to receive a paging state encoded as V3 on a 2.1 node, and then send it to a 3.0 node on a V4 session. The version of the current session will be used to deserialize the paging state, instead of the actual version used to serialize it, and the request will fail.

CASSANDRA-15176 solves half of this problem by enabling 3.0 nodes to serialize mis-versioned PagingStates. To address the other side of the issue, 2.1 nodes receiving V4 PagingStates, we can introduce a property to cap the max native protocol version that the 3.0 nodes will negotiate with clients. If we cap this to V3 during upgrades, no V4 connections will be established and so no incompatible PagingStates will be sent to clients.",N/A,"3.0.19, 3.11.5, 4.0-alpha2, 4.0"
CASSANDRA-15191,stop_paranoid disk failure policy is ignored on CorruptSSTableException after node is up,"There is a bug when disk_failure_policy is set to stop_paranoid and CorruptSSTableException is thrown after server is up. The problem is that this setting is ignored. Normally, it should stop gossip and transport but it just continues to serve requests and an exception is just logged.

 

This patch unifies the exception handling in JVMStabilityInspector and code is reworked in such way that this inspector acts as a central place where such exceptions are inspected. 

 

The core reason for ignoring that exception is that thrown exception in AbstractLocalAwareExecturorService is not CorruptSSTableException but it is RuntimeException and that exception is as its cause. Hence it is better if we handle this in JVMStabilityInspector which can recursively examine it, hence act accordingly.

Behaviour before:

stop_paranoid of disk_failure_policy is ignored when CorruptSSTableException is thrown, e.g. on a regular select statement

Behaviour after:

Gossip and transport (cql) is turned off, JVM is still up for further investigation e.g. by jmx.",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-15178,Skipping illegal legacy cells can break reverse iteration of indexed partitions,"The fix for CASSANDRA-15086 interacts badly with the accounting of bytes read from disk when indexed partitions are read in reverse. The skipped columns can cause the tracking of where CQL rows span index block boundaries to be incorrectly calculated, leading to rows being missing from read results.
",N/A,"3.0.19, 3.11.5"
CASSANDRA-15176,Fix PagingState deserialization when the state was serialized using protocol version different from current session's,"3.0 and native protocol V4 introduced a change to how {{PagingState}} is serialized. Unfortunately that can break requests during upgrades: since paging states are opaque, it's possible for a client to receive a paging state encoded as V3 on a 2.1 node, and then send it to a 3.0 node on a V4 session. The version of the current session will be used to deserialize the paging state, instead of the actual version used to serialize it, and the request will fail.

This is obviously sub-optimal, but also avoidable. This JIRA fixes one half of the problem: 3.0 failing to deserialize 'mislabeled' paging states. We can do this by inspecting the byte buffer to verify if it's been indeed serialized with the protocol version used by the session, and if not, use the other method of deserialization.

It should be noted that we list this as a 'known limitation' somewhere, but really this is an upgrade-blocking bug for some users of C*.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15172,LegacyLayout RangeTombstoneList throws IndexOutOfBoundsException,"Hi All,

This is the first time I open an issue, so apologies if I'm not following the rules properly.

 

After upgrading a node from version 2.1.21 to 3.11.4, we've started seeing a lot of AbstractLocalAwareExecutorService exceptions. This happened right after the node successfully started up with the new 3.11.4 binaries. 
{noformat}
INFO  [main] 2019-06-05 04:41:37,730 Gossiper.java:1715 - No gossip backlog; proceeding
INFO  [main] 2019-06-05 04:41:38,036 NativeTransportService.java:70 - Netty using native Epoll event loop
INFO  [main] 2019-06-05 04:41:38,117 Server.java:155 - Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
INFO  [main] 2019-06-05 04:41:38,118 Server.java:156 - Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
INFO  [main] 2019-06-05 04:41:38,179 CassandraDaemon.java:556 - Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it
INFO  [Native-Transport-Requests-21] 2019-06-05 04:41:39,145 AuthCache.java:161 - (Re)initializing PermissionsCache (validity period/update interval/max entries) (2000/2000/1000)
INFO  [OptionalTasks:1] 2019-06-05 04:41:39,729 CassandraAuthorizer.java:409 - Converting legacy permissions data
INFO  [HANDSHAKE-/10.10.10.8] 2019-06-05 04:41:39,808 OutboundTcpConnection.java:561 - Handshaking version with /10.10.10.8
INFO  [HANDSHAKE-/10.10.10.9] 2019-06-05 04:41:39,808 OutboundTcpConnection.java:561 - Handshaking version with /10.10.10.9
INFO  [HANDSHAKE-dc1_02/10.10.10.6] 2019-06-05 04:41:39,809 OutboundTcpConnection.java:561 - Handshaking version with dc1_02/10.10.10.6

WARN  [ReadStage-2] 2019-06-05 04:41:39,857 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.cassandra.db.AbstractBufferClusteringPrefix.get(AbstractBufferClusteringPrefix.java:55)
        at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.serializedSizeCompound(LegacyLayout.java:2545)
        at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.serializedSize(LegacyLayout.java:2522)
        at org.apache.cassandra.db.LegacyLayout.serializedSizeAsLegacyPartition(LegacyLayout.java:565)
        at org.apache.cassandra.db.ReadResponse$Serializer.serializedSize(ReadResponse.java:446)
        at org.apache.cassandra.db.ReadResponse$Serializer.serializedSize(ReadResponse.java:352)
        at org.apache.cassandra.net.MessageOut.payloadSize(MessageOut.java:171)
        at org.apache.cassandra.net.OutboundTcpConnectionPool.getConnection(OutboundTcpConnectionPool.java:77)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:802)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:953)
        at org.apache.cassandra.net.MessagingService.sendReply(MessagingService.java:929)
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:62)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:114)
        at java.lang.Thread.run(Thread.java:745)
 {noformat}

 

After several of the above warnings, the following warning appeared as well:

 {noformat}
WARN  [ReadStage-9] 2019-06-05 04:42:04,369 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-9,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null
WARN  [ReadStage-11] 2019-06-05 04:42:04,381 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-11,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null
WARN  [ReadStage-10] 2019-06-05 04:42:04,396 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-10,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null
WARN  [ReadStage-2] 2019-06-05 04:42:04,443 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null

 {noformat}
 

Then suddenly, Validation errors appeared although *no repair was running on any of the nodes*! Checked with {{ps -ef}} command and {{nodetool compactionstats}} on the entire cluster.

 

 {noformat}
ERROR [ValidationExecutor:2] 2019-06-05 04:42:47,979 Validator.java:268 - Failed creating a merkle tree for [repair #e54b4090-876d-11e9-a3f4-c33d22c45471 on ks1/table1, []], /
10.10.10.6 (see log for details)
ERROR [ValidationExecutor:2] 2019-06-05 04:42:47,979 CassandraDaemon.java:228 - Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1363)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager$13.call(CompactionManager.java:977)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
        at java.lang.Thread.run(Thread.java:745)
 {noformat}
 

Following those, client requests started to fail and NTR tasks started to pile up and get blocked and GC was impacted.

 {noformat}
INFO  [ScheduledTasks:1] 2019-06-05 04:43:11,660 StatusLogger.java:51 - Native-Transport-Requests       128       197         594810        65              2725
 {noformat}

 

FWIW, these are the warnings I found during startup: 

 {noformat}
-WARN in net.logstash.logback.encoder.LogstashEncoder@140e5a13 - Logback version is prior to 1.2.0.  Enabling backwards compatible encoding.  Logback 1.2.1 or greater is recommended.
 {noformat}

 

 {noformat}
WARN  [main] 2019-06-05 08:44:18,568 NativeLibrary.java:187 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
WARN  [main] 2019-06-05 08:44:18,569 StartupChecks.java:136 - jemalloc shared library could not be preloaded to speed up memory allocations

 

WARN  [main] 2019-06-05 08:44:20,225 Optional.java:159 - Legacy auth tables credentials, users, permissions in keyspace system_auth still exist and have not been properly migrated.

WARN  [MessagingService-Outgoing-dc1_03/10.10.10.4-Gossip] 2019-06-05 08:44:49,582 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [MessagingService-Outgoing-dc2_02/10.20.20.4-Gossip] 2019-06-05 08:44:49,620 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [MessagingService-Outgoing-dc2_01/10.20.20.1-Gossip] 2019-06-05 08:44:49,621 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [MessagingService-Outgoing-dc2_03/10.20.20.5-Gossip] 2019-06-05 08:44:49,621 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [GossipTasks:1] 2019-06-05 08:44:51,631 FailureDetector.java:278 - Not marking nodes down due to local pause of 30943606906 > 5000000000
 {noformat}

 

We've naturally stopped the upgrade but we still wish to upgrade from 2.1.21 and hopefully find the root cause of this matter. 
I'll be happy to provide additional details if needs be.

 

 ",N/A,"3.0.19, 3.11.5"
CASSANDRA-15169,SASI does not compare strings correctly,"In our scenario, we need to query with '>' conditions on string columns. So I created index with  is_literal = false. like the following:

 
{code:java}
CREATE TABLE test (id int primary key, t text);

CREATE CUSTOM INDEX ON test (t) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'is_literal': 'false'};
{code}
 I also inserted some records and query:

 
{code:java}
insert into test(id,t) values(1,'abc');
select * from test where t > 'ab';
{code}
At first ,it worked. But after flush, the query returned none record.

I have read the code of SASIIndex and found that it is because in the 
{code:java}
Expression.isLowerSatisfiedBy{code}
function,
{code:java}
term.compareTo{code}
was called with parameter checkFully=false, which cause the string 'abc' was only compared with its first 2 characters( length of expression value).

 

I have wrote a UT for this case and fixed it.",N/A,"3.11.5, 4.0-alpha2, 4.0"
CASSANDRA-15164,Overflowed Partition Cell Histograms Can Prevent Compactions from Executing,"Hi, we are running 6 node Cassandra cluster in production with 3 seed node but from last night one of our seed nodes is continuously throwing an error like this;-

cassandra.protocol.ServerError: <Error from server: code=0000 [Server error] message=""java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed"">

For a cluster to be up and running I Drained this node.

Can somebody help me out with this?

 

Any help or lead would be appreciated 

 

Note : We are using Cassandra version 3.7",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15160,Add flag to ignore unreplicated keyspaces during repair,"When a repair is triggered on a node in 'dc2' for a keyspace with replication factor {'dc1':3, 'dc2':0} we just ignore the repair in versions < 3. In 3.0+ we fail the repair to make sure the operator does not think the keyspace is fully repaired.

There might be tooling that relies on the old behaviour though, so we should add a flag to ignore those unreplicated keyspaces

 ",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-15158,Wait for schema agreement rather than in flight schema requests when bootstrapping,"Currently when a node is bootstrapping we use a set of latches (org.apache.cassandra.service.MigrationTask#inflightTasks) to keep track of in-flight schema pull requests, and we don't proceed with bootstrapping/stream until all the latches are released (or we timeout waiting for each one). One issue with this is that if we have a large schema, or the retrieval of the schema from the other nodes was unexpectedly slow then we have no explicit check in place to ensure we have actually received a schema before we proceed.

While it's possible to increase ""migration_task_wait_in_seconds"" to force the node to wait on each latche longer, there are cases where this doesn't help because the callbacks for the schema pull requests have expired off the messaging service's callback map (org.apache.cassandra.net.MessagingService#callbacks) after request_timeout_in_ms (default 10 seconds) before the other nodes were able to respond to the new node.

This patch checks for schema agreement between the bootstrapping node and the rest of the live nodes before proceeding with bootstrapping. It also adds a check to prevent the new node from flooding existing nodes with simultaneous schema pull requests as can happen in large clusters.

Removing the latch system should also prevent new nodes in large clusters getting stuck for extended amounts of time as they wait `migration_task_wait_in_seconds` on each of the latches left orphaned by the timed out callbacks.

 
||3.11||
|[PoC|https://github.com/apache/cassandra/compare/cassandra-3.11...vincewhite:check_for_schema]|
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...vincewhite:wait_for_schema_agreement]|

 ",N/A,"3.0.24, 3.11.10, 4.0-beta4, 4.0"
CASSANDRA-15145,Error in Cassandra database,"Dear Team,

We are facing the problem in the Cassandra database and below the error message and after this error application stopped working.

Please suggest why this issue occurred and what I have to do for resolving the issue?

 *Cassandra:- [cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4]*

 

v\:* \{behavior:url(#default#VML);} o\:* \{behavior:url(#default#VML);} w\:* \{behavior:url(#default#VML);} .shape \{behavior:url(#default#VML);}

2019-05-29 13:58:47.673  INFO 2003 — [ryBean_Worker-1] com.estel.common.MoneyMarkingJob         : Job for Leger Management is  strated at  2019-05-30T00:58:47.671Z time zone

org.springframework.dao.DataAccessResourceFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed); nested exception is com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)

        at org.springframework.cassandra.support.CassandraExceptionTranslator.translateExceptionIfPossible(CassandraExceptionTranslator.java:163)

        at org.springframework.cassandra.core.CqlTemplate.potentiallyConvertRuntimeException(CqlTemplate.java:946)

        at org.springframework.cassandra.core.CqlTemplate.translateExceptionIfPossible(CqlTemplate.java:930)

        at org.springframework.cassandra.core.CqlTemplate.translateExceptionIfPossible(CqlTemplate.java:912)

        at org.springframework.cassandra.core.CqlTemplate.doExecute(CqlTemplate.java:278)

        at org.springframework.cassandra.core.CqlTemplate.doExecuteQueryReturnResultSet(CqlTemplate.java:283)

        at org.springframework.data.cassandra.core.CassandraTemplate.select(CassandraTemplate.java:594)

        at org.springframework.data.cassandra.core.CassandraTemplate.select(CassandraTemplate.java:376)

        at org.springframework.data.cassandra.repository.query.CassandraQueryExecution$CollectionExecution.execute(CassandraQueryExecution.java:85)

        at org.springframework.data.cassandra.repository.query.CassandraQueryExecution$ResultProcessingExecution.execute(CassandraQueryExecution.java:143)

        at org.springframework.data.cassandra.repository.query.AbstractCassandraQuery.execute(AbstractCassandraQuery.java:113)

        at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.doInvoke(RepositoryFactorySupport.java:483)

        at org.springframework.data.repository.core.support.RepositoryFactorySupport$QueryExecutorMethodInterceptor.invoke(RepositoryFactorySupport.java:461)

        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)

        at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:56)

        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)

        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)

        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)

        at org.springframework.data.repository.core.support.SurroundingTransactionDetectorMethodInterceptor.invoke(SurroundingTransactionDetectorMethodInterceptor.java:57)

        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)

        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213)

        at com.sun.proxy.$Proxy66.findTransactionByDatenadStatus(Unknown Source)

        at com.estel.common.service.impl.TransactionServiceImpl.fingTransactionByDateAndStatus(TransactionServiceImpl.java:47)

        at com.estel.common.MoneyMarkingJob.execute(MoneyMarkingJob.java:101)

        at org.quartz.core.JobRunShell.run(JobRunShell.java:202)

        at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:573)

Caused by: com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)

        at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:85)

        at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:27)

        at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)

        at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)

        at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:68)

        at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:43)

        at org.springframework.cassandra.core.CqlTemplate$3.doInSession(CqlTemplate.java:286)

        at org.springframework.cassandra.core.CqlTemplate$3.doInSession(CqlTemplate.java:283)

        at org.springframework.cassandra.core.CqlTemplate.doExecute(CqlTemplate.java:276)

        ... 21 more

Caused by: com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)

        at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:95)

        at com.datastax.driver.core.Responses$Error.asException(Responses.java:128)

        at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:179)

        at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:177)

        at com.datastax.driver.core.RequestHandler.access$2500(RequestHandler.java:46)

        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:799)

        at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:633)

        at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1070)

        at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:993)

        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)

        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)

        at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)

        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)

        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)

        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)

        at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)

        at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:267)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)

        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:321)

        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1280)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:342)

        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:328)

        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:890)

        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)

        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:564)

        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:505)

        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:419)

        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:391)

        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112)

        at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:145)

        at java.lang.Thread.run(Thread.java:748)

Caused by: com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)

        at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:76)

        at com.datastax.driver.core.Responses$Error$1.decode(Responses.java:37)

        at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:289)

        at com.datastax.driver.core.Message$ProtocolDecoder.decode(Message.java:269)

        at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:88)

        ... 20 more

2019-05-29 13:58:47.802  INFO 2003 — [ryBean_Worker-1] com.estel.common.MoneyMarkingJob         : Transaction Dump is updated  in case of WALLETTOPUP process status!.Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed); nested exception is com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)

 

 

Regards,

Pradip

Mobile- +91 9873902729

 

 

 

 ",N/A,3.11.12
CASSANDRA-15137,Switch http to https URLs in build.xml,Switch to using https URLs wherever possible in build.xml.,N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15136,Incorrect error message in legacy reader,"Just fixes the order in the exception message.



||3.0.x||3.11.x||
|[Patch|https://github.com/vincewhite/cassandra/commits/readLegacyAtom30]|[Patch|https://github.com/vincewhite/cassandra/commits/readLegacyAtom]|",N/A,"3.0.26, 3.11.12"
CASSANDRA-15135,SASI tokenizer options not validated before being added to schema,"If you attempt to create a SASI index with an illegal argument combination the index will be added to the schema tables before trying instantiate the tokenizer which causes a RuntimeException. Since the index was written to the schema tables, cassandra will hit the same exception and fail to start when it tries to load the schema on boot.

 The branch below includes a unit test to reproduce the issue.
||3.11||
|[PoC|https://github.com/vincewhite/cassandra/commit/089547946d284ae3feb0d5620067b85b8fd66ebc]|

 ",N/A,"3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-15134,SASI index files not included in snapshots,"Newly written SASI index files are not being included in snapshots. This is because the SASI index files are not added to the components ({{org.apache.cassandra.io.sstable.SSTable#components}}) list of newly written sstables. 

Although I don't believe anything except snapshots ever tries to reference the SASI index files from this location, on startup Cassandra does add the SASI index files (if they are found on disk) of existing sstables in their components list. In that case sstables that existed on startup with SASI index files will have their SASI index files included in any snapshots.

 

This patch updates the components list of newly written sstable once the index is built.
||3.11||4.0||Trunk||
|[https://github.com/apache/cassandra/pull/1150]|[TBD]|[TBD]|

 ",N/A,"3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-15126,Resource leak when queries apply a RowFilter,"RowFilter.CQLFilter optionally removes those partitions that have no matching results, but fails to close the iterator representing that partition’s unfiltered results, leaking resources when this happens.",N/A,3.0.19
CASSANDRA-15123,Avoid keeping sstables marked compacting forever when user defined compaction gets interrupted,"When we have both repaired + unrepaired data on a node, we create multiple compaction tasks and run them serially. If one of those tasks gets interrupted or throws exception we will keep sstables in the other tasks as compacting forever.",N/A,"3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15120,Nodes that join the ring while another node is MOVING build an invalid view of the token ring,"Gossip only updates the token metadata for nodes in the NORMAL, SHUTDOWN or LEAVING* statuses.  MOVING and REMOVING_TOKEN nodes do not have their ring information updated (nor do others, but these other states _should_ only be taken by nodes that are not members of the ring).  

If a node missed the most recent token-modifying events because they were not a member of the ring when they happened (or because Gossip was delayed to them), they will retain an invalid view of the ring until the node enters the one of the NORMAL, SHUTDOWN or LEAVING states.

*LEAVING is populated differently, however, and in a probably unsafe manner that this work will also address.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15100,Improve no-op cleanup performance,We should filter sstables in `OneSSTableOperation#filterSSTables` instead of in the cleanup method to avoid creating unnecessary single-sstable transactions for sstables fully contained in the owned ranges.,N/A,"3.0.19, 3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-15097,Avoid updating unchanged gossip state,"The node might get unchanged gossip states, the state might be just updated after sending a GOSSIP_SYN, then it will get the state that is already up to date. If the heartbeat in the GOSSIP_ACK message is updated, it will unnecessary re-apply the same state again, which could be costly like updating token change.
It's very likely to happen for large cluster when a node startup, as the first gossip message will sync all endpoints tokens, it could take some time (in our case about 200 seconds), during that time, it keeps gossip with other node and get the full token states. Which causes lots of pending gossip tasks.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15090,Customize cassandra log directory,"Add a new variable CASSANDRA_LOG_DIR (default: $CASSANDRA_HOME/logs) so that we could customize log directory such as ‘/var/log/cassandra’ .

 

 ",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15086,Illegal column names make legacy sstables unreadable in 3.0/3.x,"CASSANDRA-10608 adds extra validation when decoding a bytebuffer representing a legacy cellname. If the table is not COMPACT and the column name component of the cellname refers to a primary key column, an IllegalArgumentException is thrown. It looks like the original intent of 10608 was to prevent Thrift writes from inserting these invalid cells, but the same code path is exercised on the read path. The problem is that this kind of cells may exist in pre-3.0 sstables, either due to Thrift writes or through side loading of externally generated SSTables. Following an upgrade to 3.0, these partitions become unreadable, breaking both the read and compaction paths (and so also upgradesstables). Scrub in 2.1 does not help here as it blindly reproduces the invalid cells.",N/A,"3.0.19, 3.11.5"
CASSANDRA-15084,failed compactions in specific cluster version 3.11.2,"We are using Cassandra version 3.11.2

In our biggest cluster, in the biggest table compactions keep failing.

The amount of data in every node is around 900GB.

After we run the compaction, we monitor the process using nodetool compactionstats and can see the progress continue until it reached 99.99% of the progress.

In this point the compaction fail with the error below

In addition, we increased index_summary_capacity_in_mb few times until the size of 4GB, but with no help.

 
ERROR [CompactionExecutor:26976] 2019-04-11 13:55:24,917 CassandraDaemon.java:228 - Exception in thread Thread[CompactionExecutor:26976,1,main]
java.lang.IllegalArgumentException: null
        at java.nio.Buffer.position(Buffer.java:244) ~[na:1.8.0_144]
        at org.apache.cassandra.io.util.SafeMemoryWriter.reallocate(SafeMemoryWriter.java:59) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.io.util.SafeMemoryWriter.setCapacity(SafeMemoryWriter.java:68) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.io.sstable.IndexSummaryBuilder.prepareToCommit(IndexSummaryBuilder.java:250) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.doPrepare(BigTableWriter.java:524) ~[apache-cassandra-3.11.2.jar:3.11.
2]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.2.
jar:3.11.2]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy.doPrepare(BigTableWriter.java:364) ~[apache-cassandra-3.11.2.ja
r:3.11.2]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.2.
jar:3.11.2]
        at org.apache.cassandra.io.sstable.format.SSTableWriter.prepareToCommit(SSTableWriter.java:281) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:379) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.2.
jar:3.11.2]
        at [org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:112|http://org.apache.cassandra.db.compaction.writers.compactionawarewriter.doprepare%28compactionawarewriter.java:112/]) ~[apache-cassandra-3.11.2.jar:3.1
1.2]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173) ~[apache-cassandra-3.11.2.
jar:3.11.2]
        at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184) ~[apache-cassandra-3.11.2.jar:3.11.
2]
        at [org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:122|http://org.apache.cassandra.db.compaction.writers.compactionawarewriter.finish%28compactionawarewriter.java:122/]) ~[apache-cassandra-3.11.2.jar:3.11.2
]
        at [org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220|http://org.apache.cassandra.db.compaction.compactiontask.runmaythrow%28compactiontask.java:220/]) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at [org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85|http://org.apache.cassandra.db.compaction.compactiontask.executeinternal%28compactiontask.java:85/]) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at [org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61|http://org.apache.cassandra.db.compaction.abstractcompactiontask.execute%28abstractcompactiontask.java:61/]) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at [org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:746|http://org.apache.cassandra.db.compaction.compactionmanager%2410.runmaythrow%28compactionmanager.java:746/]) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.2.jar:3.11.2]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.2.jar:3.11.2]",N/A,3.11.5
CASSANDRA-15081,LegacyLayout does not have same behavior as 2.x when handling unknown column names,"Due to a bug I haven't been able to reproduce the production cluster had unknown column names. To replicate the issue for this test I did the following:
{noformat}
$ ccm create -v 2.1.19 -n 1 -s bug
$ cat > schema.cql << 'EOF'
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;
CREATE TABLE test.unknowntest (id int primary key, payload text, ""paylo!d"" text);
EOF
$ ccm node1 cqlsh -f schema.cql
$ export CASSANDRA_INCLUDE=~/.ccm/bug/node1/bin/cassandra.in.sh
$ cat > bug.json << 'EOF'
[
{""key"": ""1"",
""cells"": [["""","""",1554432501209207],
[""paylo!d"",""hello world"",1554432501209207],
[""payload"",""hello world"",1554432501209207]]}
]
EOF
$ ~/.ccm/repository/2.1.19/tools/bin/json2sstable -K test -c unknowntest ~/bug.json ~/.ccm/bug/node1/data0/test/unknowntest-<cfid>/test-unknowntest-ka-1-Data.db{noformat}
Then test the behavior of unknown columns in 2.1:
{noformat}
$ ccm stop
$ ccm create -v 2.1.19 -n 1 -s bug2_1_19
$ cat > schema2.cql << 'EOF'
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;
CREATE TABLE test.unknowntest (id int primary key, payload text);
EOF
$ ccm node1 cqlsh -f schema2.cql
$ ccm stop
$ cp ~/.ccm/bug/node1/data0/test/unknowntest-<cfid>/test-unknowntest-ka-1-* ~/.ccm/bug2_1_19/node1/data0/test/unknowntest-<cfid>/
$ ccm start
$ ccm node1 cqlsh
Connected to bug2_1_19 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.19 | CQL spec 3.2.1 | Native protocol v3]
Use HELP for help.
cqlsh> select * from test.unknowntest where id = 1;

id | payload
----+-------------
1 | hello world

(1 rows){noformat}
Compared to 3.11.4 which did the following:
{noformat}
$ ccm stop
$ ccm create -v 3.11.4 -n 1 -s bug3_11_4
$ ccm node1 cqlsh -f schema2.cql
$ ccm stop
$ cp ~/.ccm/bug/node1/data0/test/unknowntest-<cfid>/test-unknowntest-ka-1-* ~/.ccm/bug3_11_4/node1/data0/test/unknowntest-<cfid>/
$ ccm start
$ ccm node1 cqlsh
Connected to bug3_11_4 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> select * from test.unknowntest where id = 1;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{noformat}
In the logs this resulted in an IllegalStateException from LegacyLayout line 1127

The expected behavior would be to ignore the column and return results the same as in 2.1",N/A,3.11.6
CASSANDRA-15078,Support cross version messaging in in-jvm upgrade dtests,,N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15075,SELECT JSON generates invalid JSON for the duration type,"Currently, Apache Cassandra generates invalid JSON for the ""duration"" type.

cqlsh> CREATE KEYSPACE ks1 WITH REPLICATION = \{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
 cqlsh> CREATE TABLE ks1.data (id int, d duration, PRIMARY KEY (id));

cqlsh> INSERT INTO ks1.data (id, d) VALUES (1, 6h40m);
 cqlsh> SELECT JSON d FROM ks1.data WHERE id = 1;

[json]
 --------------
 \{""d"": 6h40m}

That is, the duration is not quoted and is therefore invalid according to [https://jsonlint.com/,] for example.

 

Fix the problem by quoting the formatted duration type properly:

cqlsh> INSERT INTO ks1.data (id, d) VALUES (1, 6h40m);
 cqlsh> SELECT JSON d FROM ks1.data WHERE id = 1;

[json]
 ----------------
 \{""d"": ""6h40m""}

(1 rows)

 

The problem is fixed by the following patch:

[^0001-Fix-SELECT-JSON-formatting-for-the-duration-type.patch]",N/A,"3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-15072,Incomplete range results during 2.X -> 3.11.4 upgrade,"Hello

During an upgrade from 2.1.17 to 3.11.4, our application starting getting back incomplete results for range queries. When all nodes were upgraded (before upgrading sstables), we stopped getting incomplete results. I was able to reproduce it and listed steps below. It seems to require the random partitioner and compact storage to reproduce reliably. It also reproduces coming from 2.1.21 and 2.2.14. You seem to get the bad behavior when an old node is your coordinator and it has to talk to an upgraded replica.
{noformat}
ccm create test -v 2.1.17 -n 3
ccm updateconf 'partitioner: org.apache.cassandra.dht.RandomPartitioner'
ccm node1 updateconf 'initial_token: 0'
ccm node2 updateconf 'initial_token: 56713727820156410577229101238628035242'
ccm node3 updateconf 'initial_token: 113427455640312821154458202477256070484'
ccm start

ccm node1 cqlsh <<SCHEMA
CREATE KEYSPACE test WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3};
CREATE COLUMNFAMILY test.test (
  id text,
  foo text,
  bar text,
  PRIMARY KEY (id)
) WITH COMPACT STORAGE;
CONSISTENCY QUORUM;
INSERT INTO test.test (id, foo, bar) values ('1', 'hi', 'there');
INSERT INTO test.test (id, foo, bar) values ('2', 'hi', 'there');
SCHEMA

ccm node1 stop
ccm node1 setdir -v 3.11.4
ccm node1 start

ccm node2 stop
ccm node2 setdir -v 3.11.4
ccm node2 start

# here I use 3.X cqlsh to connect to 2.X node so I can lower the page size (to
# allow for simpler test setup)
cqlsh 127.0.0.3 <<QUERY
CONSISTENCY QUORUM;
PAGING 2;
select * from test.test;
QUERY
{noformat}
This results in:
{noformat}
Page size: 2

 id | bar   | foo
----+-------+-----
  2 | there |  hi

(1 rows)
{noformat}
Running it against the upgraded node (node1):
{noformat}
Page size: 2

 id | bar   | foo
----+-------+-----
  2 | there |  hi
  1 | there |  hi

(2 rows)
{noformat}",N/A,"3.0.19, 3.11.5"
CASSANDRA-15069,Tombstone/Partition not purged after gc_grace_seconds,"During a tombstone purge (reducing gc_grace_seconds to zero and running `nodetool garbagecollect`), I noticed that when doing a dump of the SSTable, sometimes, there are a few partitions that do not get completely purged, even with gc_grace_seconds set to zero. I was able to replicate this in a small test dataset, from which I have attached the SSTable files and the schema to this ticket so that you can verify this as well. 
Doing a dump of the mc-51-big-Data.db file, you'll notice the following partition:

{
    ""partition"" : {
      ""key"" : [ ""96"" ],
      ""position"" : 285,
      ""deletion_info"" : { ""marked_deleted"" : ""2019-03-14T21:31:55.244490Z"", ""local_delete_time"" : ""2019-03-14T21:31:55Z"" }
    },
    ""rows"" : [ ]
  },

As you can see, the rows were removed correctly by the garbagecollect, but the partition record, continues there, and never goes away.
From the client side, no data is returned, so it's good there. But regardless of that, this partition should not be present in the SSTable file.",N/A,3.11.5
CASSANDRA-15059,Gossiper#markAlive can race with Gossiper#markDead,"The Gossiper class is not threadsafe and assumes all state changes happen in a single thread (the gossip stage). Gossiper#convict, however, can be called from the GossipTasks thread. This creates a race where calls to Gossiper#markAlive and Gossiper#markDead can interleave, corrupting gossip state. Gossiper#assassinateEndpoint has a similar problem, being called from the mbean server thread.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15058,Avoid double counting read latencies for digest queries,We are closing the {{UnfilteredPartitionIterator}} wrapped with {{withMetricsRecording}} twice when we get digest requests - closing it calls {{onClose}} and that makes the metrics update twice.,N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15053,Fix handling FS errors on writing and reading flat files - LogTransaction and hints,We currently fail to handle and propagate IO errors when dealing with transaction log and hints.  It's trivial to fix this behaviour to ensure that disk failure policy is properly invoked in error scenarios.,N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15047,Slow Query Logger  in Cassandra 3.11.4,"As per
 https://issues.apache.org/jira/browse/CASSANDRA-12403

I tried to enable slow query logging with below steps on Cassandra 3.11.4 , but not able to log slow query's .seems its merged in 3.10 not sure if anything changed in 3.11.4 

there was dicussion in renaming 
{code:java}
 nodetool getlogginglevels
 Logger Name                                        Log Level
 ROOT                                                    INFO
 com.thinkaurelius.thrift                               ERROR
 org.apache.cassandra                                   DEBUG
 org.apache.cassandra.db                                DEBUG
 org.apache.cassandra.db.monitoring                     DEBUG
{code}
In cassandra.yaml
{code:java}
# can be identified. Set this value to zero to disable slow query logging. 
slow_query_log_timeout_in_ms: 500{code}
 
{code:java}
slow_query_log_timeout_in_ms & log_slow_query_log_timeout_in_ms{code}
Attached is the logback.xml i am using default [^logback.xml]",N/A,3.11.4
CASSANDRA-15045,Fix index summary redistribution compaction cancellation issues,"We can't cancel ongoing index summary redistributions currently due to {{CompactionInfo}} returning null for {{getTableMetadata/getCFMetaData}} [here|https://github.com/apache/cassandra/blob/67d613204fa4fb9584f11ec9886a0e7a0d622e92/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1814] for index summary redistributions

CASSANDRA-14935 also introduced a bug where we track the wrong sstables for index summary redistributions",N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15041,UncheckedExecutionException if authentication/authorization query fails,"If cache update for permissions/credentials/roles fails with UnavailableException this comes back to client as UncheckedExecutionException.

Stack trace on server side:
{noformat}
ERROR [Native-Transport-Requests-1] 2019-03-04 16:30:51,537 ErrorMessage.java:384 - Unexpected exception during request
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na]
        at org.apache.cassandra.auth.AuthCache.get(AuthCache.java:97) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.PermissionsCache.getPermissions(PermissionsCache.java:45) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthenticatedUser.getPermissions(AuthenticatedUser.java:104) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.authorize(ClientState.java:439) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.checkPermissionOnResourceChain(ClientState.java:368) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.ensureHasPermission(ClientState.java:345) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.hasAccess(ClientState.java:332) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:310) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.ModificationStatement.checkAccess(ModificationStatement.java:211) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:222) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:532) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:509) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:146) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:566) [apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.4.jar:3.11.4]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_181]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:114) [apache-cassandra-3.11.4.jar:3.11.4]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na]
        at org.apache.cassandra.auth.AuthCache.get(AuthCache.java:97) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.RolesCache.getRoles(RolesCache.java:44) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.Roles.hasSuperuserStatus(Roles.java:51) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthenticatedUser.isSuper(AuthenticatedUser.java:71) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraAuthorizer.authorize(CassandraAuthorizer.java:81) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.PermissionsCache.lambda$new$0(PermissionsCache.java:37) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthCache$1.load(AuthCache.java:172) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na]
        ... 26 common frames omitted
Caused by: java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:518) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRoles(CassandraRoleManager.java:283) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.RolesCache.lambda$new$0(RolesCache.java:36) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthCache$1.load(AuthCache.java:172) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na]
        ... 40 common frames omitted
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1766) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1728) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1671) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1586) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1209) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:315) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:285) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:526) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:508) ~[apache-cassandra-3.11.4.jar:3.11.4]
        ... 47 common frames omitted
{noformat}
Also, if {{x_validity_in_ms}} > {{x_update_interval_in_ms}}, then the background update thread will fail in a similar way:
{noformat}
ERROR [PermissionsCacheRefresh:1] 2019-03-04 16:30:43,541 CassandraDaemon.java:228 - Exception in thread Thread[PermissionsCacheRefresh:1,5,main]
java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:518) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.isSuper(CassandraRoleManager.java:307) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.Roles.hasSuperuserStatus(Roles.java:52) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthenticatedUser.isSuper(AuthenticatedUser.java:71) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraAuthorizer.authorize(CassandraAuthorizer.java:81) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.PermissionsCache.lambda$new$0(PermissionsCache.java:37) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthCache$1.lambda$reload$0(AuthCache.java:180) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.4.jar:3.11.4]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_181]
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1766) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1728) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1671) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1586) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1209) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:315) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:285) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:526) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:508) ~[apache-cassandra-3.11.4.jar:3.11.4]
        ... 11 common frames omitted
{noformat}
 ",N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15039,Documentation claims copyright for future years,"See attached patch for details and fix.

 
See also on this topic:
[https://stackoverflow.com/questions/2390230/do-copyright-dates-need-to-be-updated]
 ",N/A,"3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15035,C* 3.0 sstables w/ UDTs are corrupted in 3.11 + 4.0,"OSS C* 3.0 writes incorrect type information for UDTs into the serialization-header of each sstable.

In C* 3.0, both UDTs and tuple are always frozen. A frozen type must be enclosed in a {{frozen<...>}} via the {{CQL3Type}} hierarchy (resp {{org.apache.cassandra.db.marshal.FrozenType(...)}} via the {{AbstractType}} hierarchy) “bracket” in the schema and serialization-header.

Since CASSANDRA-7423 (committed to C* 3.6) UDTs can also be non-frozen (= multi-cell).

Unfortunately, C* 3.0 does not write the {{org.apache.cassandra.db.marshal.FrozenType(...)}} “bracket” for UDTs into the {{SerializationHeader.Component}} in the {{-Stats.db}} sstable component.

The order in which columns of a row are serialized depends on the concrete {{AbstractType}}. Columns with variable length types (frozen types belong to this category) are serialized before columns with multi-cell types (non-frozen types belong to that category).

If C* 3.6 (or any newer version) reads an sstable written by C* 3.0 (up to 3.5), it will read the type information “non-frozen UDT” from the serialization header, which is technically correct.

This means, that upgrades from C* 3.0 to C* 3.11 and 4.0, using a schema that uses UDTs, result in inaccessible data in those sstables. Reads against 3.0 sstables as well as attempts to scrub these sstables result in a wide variety of errors/exceptions ({{CorruptSSTableException}}, {{EOFExcepiton}}, {{OutOfMemoryError}}, etc etc), as usual in such cases.

Mitigation strategy in the proposed patch:
* Fix the broken serialization-headers automatically when an upgrade from C* 3.0 is detected.
* Enhance {{sstablescrub}} to verify the serialization-header against the schema and allow {{sstablescrub}} to fix the UDT types according to the information in the schema. This does not apply to ""online scrub"" (e.g. nodetool scrub). The behavior of {{sstablescrub}} has been changed to first inspect the serialization-header and verify the type information against the schema. 

Differences between the schema and the sstable serialization-headers cause {{sstablescrub}} to error out and stop - i.e. safety first (there’s a way to opt-out though).

A new class {{SSTableHeaderFix}} can inspect the serialization-header ({{SerializationHeader.Component}}) in the the {{-Statistics.db}} component and fix the type information in those sstables for UDTs according to the schema information.

This new class could be used during verify and before sstables are imported. But changes to “verify” and “import” are out of the scope of this ticket, as the patch is already bigger than I originally expected.

Another issue not tackled by this ticket is that the wrong ‘kind’ is written to the type information in {{system_schema.dropped_columns}} when a non-frozen UDT column is dropped. When a UDT column is dropped, the type of the dropped column is converted from the UDT definition to its “corresponding” tuple type definition. But all versions currently write {{frozen<tuple<...>>}}, but for non-frozen UDTs it should actually just be {{tuple<...>}}. Unfortunately, there is nothing that could be done in this ticket to fix (or even consider) the type information of a dropped column. But for correctness, the tuple type should be a multi-cell one (only accessible for dropped UDTs though - not as something that a user can create as a type).
",N/A,"3.11.6, 4.0-alpha4, 4.0"
CASSANDRA-15021,TestBootstrapAfterUpgrade.test_upgrade_with_range_tombstone_eoc_0 upgrade test is failing with TypeError,"While running upgrade tests for 3.11.4 candidate I noticed that {{upgrade_tests.storage_engine_upgrade_test.TestBootstrapAfterUpgrade.test_upgrade_with_range_tombstone_eoc_0}} is failing with TypeError.

Example run with error:
{code:java}
self = <upgrade_tests.storage_engine_upgrade_test.TestBootstrapAfterUpgrade object at 0x7f8db9908240>

    @since('3.0', max_version='3.99')
    def test_upgrade_with_range_tombstone_eoc_0(self):
        """"""
            Check sstable upgrading when the sstable contains a range tombstone with EOC=0.
    
            @jira_ticket CASSANDRA-12423
            """"""
        session = self._setup_cluster(cluster_options={'start_rpc': 'true'})
    
        session.execute(""CREATE TABLE rt (id INT, c1 TEXT, c2 TEXT, v INT, PRIMARY KEY (id, c1, c2)) ""
                        ""with compact storage and compression = {'sstable_compression': ''};"")
    
        range_delete = {
            i32(1): {
                'rt': [Mutation(deletion=Deletion(2470761440040513,
                                                  predicate=SlicePredicate(slice_range=SliceRange(
>                                                     start=composite('a', eoc='\x00'),
                                                      finish=composite('asd', eoc='\x00')))))]
            }
        }

upgrade_tests/storage_engine_upgrade_test.py:434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

item1 = b'a', item2 = None, eoc = '\x00'

    def composite(item1, item2=None, eoc=b'\x00'):
        if isinstance(item1, str):
            item1 = utf8encode(item1)
        if isinstance(item2, str):
            item2 = utf8encode(item2)
    
>       packed = _i16(len(item1)) + item1 + eoc
E       TypeError: can't concat str to bytes

thrift_test.py:153: TypeError
{code}
This TypeError is from Python3 migration. Python 3's standard string type is Unicode based, and Python 3 adds a dedicated bytes type, but critically, no automatic coercion between bytes and unicode strings is provided - [context|https://www.python.org/dev/peps/pep-0404/#strings-and-bytes]

This change in python3 is leading to ""TypeError: can't concat str to bytes"" while appending bytes to string.",N/A,3.11.4
CASSANDRA-15020,Invalid CQL in documentation," [http://cassandra.apache.org/doc/4.0/cql/security.html]

1.

Wrong:

CREATE USER *IF EXISTS* alice WITH PASSWORD 'password_a' SUPERUSER;
 CREATE ROLE *IF EXISTS* alice WITH PASSWORD = 'password_a' AND LOGIN = true AND SUPERUSER = true;

Correct:

CREATE USER IF *NOT* EXISTS alice WITH PASSWORD 'password_a' SUPERUSER;
CREATE ROLE IF *NOT* EXISTS alice WITH PASSWORD = 'password_a' AND LOGIN = true AND SUPERUSER = true; 

 

2. 

 Wrong:

CREATE ROLE alice WITH PASSWORD = 'password_a' *WITH* LOGIN = true;
CREATE ROLE alice WITH PASSWORD = 'password_a' *WITH* LOGIN = true;

Correct:

CREATE ROLE alice WITH PASSWORD = 'password_a' *AND* LOGIN = true;
CREATE ROLE alice WITH PASSWORD = 'password_a' *AND* LOGIN = true;
 ",N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15016,bootstrap_upgrade_test.py::test_simple_bootstrap_mixed_versions is failing on 3.11.4,"{{test_simple_bootstrap_mixed_versions}} is failing due to CASSANDRA-13004, which introduced ""cassandra.force_3_0_protocol_version"" for schema migrations during upgrades from 3.0.14 upwards. This flag is missing in `test_simple_bootstrap_mixed_versions` while we are adding/bootstrapping 3.11.4 node to an existing 3.5 version of C* node, which resulted in {{ks}} keyspace schema/data not being bootstrapped to the new node.

I debugged and confirmed that [MigrationManager::is30Compatible|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/service/MigrationManager.java#L181-L185] is returning false which is forcing [MigrationManager::shouldPullSchemaFrom|https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/service/MigrationManager.java#L168-L177] to return false as well.

*from debug logs:*
{code:java}
DEBUG [GossipStage:1] 2019-02-06 23:20:47,392 MigrationManager.java:115 - Not pulling schema because versions match or shouldPullSchemaFrom returned fal
{code}

*Failed upgrade tests: [https://circleci.com/gh/aweisberg/cassandra/2593#tests/containers/11]*

*From failed dtest:* 

{code:java}
self = <upgrade_tests.bootstrap_upgrade_test.TestBootstrapUpgrade object at 0x7f1bec192eb8>

    @pytest.mark.no_vnodes
    @since('3.10', max_version='3.99')
    def test_simple_bootstrap_mixed_versions(self):
>       self._base_bootstrap_test(bootstrap_from_version=""3.5"")

upgrade_tests/bootstrap_upgrade_test.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
bootstrap_test.py:114: in _base_bootstrap_test
    assert_almost_equal(size1, size2, error=0.3)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (429.36, 133.1), kwargs = {'error': 0.3}, error = 0.3, vmax = 429.36
vmin = 133.1, error_message = ''

    def assert_almost_equal(*args, **kwargs):
        """"""
        Assert variable number of arguments all fall within a margin of error.
        @params *args variable number of numerical arguments to check
        @params error Optional margin of error. Default 0.16
        @params error_message Optional error message to print. Default ''
    
        Examples:
        assert_almost_equal(sizes[2], init_size)
        assert_almost_equal(ttl_session1, ttl_session2[0][0], error=0.005)
        """"""
        error = kwargs['error'] if 'error' in kwargs else 0.16
        vmax = max(args)
        vmin = min(args)
        error_message = '' if 'error_message' not in kwargs else kwargs['error_message']
        assert vmin > vmax * (1.0 - error) or vmin == vmax, \
>           ""values not within {:.2f}% of the max: {} ({})"".format(error * 100, args, error_message)
E       AssertionError: values not within 30.00% of the max: (429.36, 133.1) ()

tools/assertions.py:206: AssertionError
{code}


 ",N/A,3.11.4
CASSANDRA-15014,Unit tests failure on trunk,"Currently org.apache.cassandra.distributed.test.DistributedReadWritePathTest is failing on trunk with the following error -
{code:java}
[junit-timeout] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest
[junit-timeout] Exception in thread ""main"" java.lang.OutOfMemoryError: Metaspace
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.distributed.test.DistributedReadWritePathTest:readWithSchemaDisagreement: Caused an ERROR
[junit-timeout] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout] at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout] at java.lang.Thread.run(Thread.java:748)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.test.DistributedReadWritePathTest FAILED (crashed)
[junitreport] Processing /tmp/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null1041131060
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 277ms
[junitreport] Deleting: /tmp/null1041131060{code}
I have noticed sporadic failures in the org.apache.cassandra.distributed.test.* suite.",N/A,"2.2.15, 3.0.19, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-15013,Prevent client requests from blocking on executor task queue,"This is a follow-up ticket out of CASSANDRA-14855, to make the Flusher queue bounded, since, in the current state, items get added to the queue without any checks on queue size, nor with any checks on netty outbound buffer to check the isWritable state.
We are seeing this issue hit our production 3.0 clusters quite often.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-15009,In-JVM Testing tooling for paging,Add distributed pager to in-jvm distributed tests to allow realistic pager tests.,N/A,"2.2.15, 3.0.19, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-15004,Anti-compaction briefly corrupts sstable state for reads,"Since we use multiple sstable rewriters in anticompaction, the first call to prepareToCommit will remove the original sstables from the tracker view before the other rewriters add their sstables. This creates a brief window where reads can miss data.",N/A,"3.0.18, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-14993,Catch CorruptSSTableExceptions and FSErrors in ALAExecutorService,"Actively handling CorruptSSTableExceptions and FSErrors currently only happens during opening of sstables and in the default exception handler. What's missing is to catch these in AbstractLocalAwareExecutorService as well. Therefor I propose to add calls to FileUtils.handleCorruptSSTable/handleFSError there, too, so we don't miss invoking the disk failure policy in that case.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14985, CircleCI docker image  should bake in more dependencies,"It's pretty frequent especially in the upgrade tests for either maven or github to fail. I think they are detecting the thundering herd of containers all pulling stuff at the same time and opting out.

We can reduce this especially for maven dependencies since most are hardly ever changing by downloading everything we can in advance and baking it into the image.

When building the docker image Initialize the local maven repository by running ant maven-ant-tasks-retrieve-build for 2.1-trunk and do the same with CCM and the Apache repository.",N/A,"2.2.14, 3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14979,cqlsh_tests/cqlsh_copy_tests.py::TestCqlshCopy::test_unusual_dates is failing,I think this may be due to a real bug. Looking at what happens I don't see how the code can handle the out of range dates.,N/A,"3.0.26, 3.11.12, 4.1-alpha1, 4.1"
CASSANDRA-14970,New releases must supply SHA-256 and/or SHA-512 checksums,"Release policy was updated around 9/2018 to state:

""For new releases, PMCs MUST supply SHA-256 and/or SHA-512; and SHOULD NOT supply MD5 or SHA-1. Existing releases do not need to be changed.""

build.xml needs to be updated from MD5 & SHA-1 to, at least, SHA-256 or both. cassandra-builds/cassandra-release scripts need to be updated to work with the new checksum files.

http://www.apache.org/dev/release-distribution#sigs-and-sums",N/A,"2.2.16, 3.0.20, 3.11.6, 4.0-beta1, 4.0"
CASSANDRA-14958,Counters fail to increment in 2.1/2.2 to 3.X mixed version clusters,"The upgrade test for this is failing
https://circleci.com/gh/aweisberg/cassandra/2362#tests/containers/1

I confirmed that this is occurring manually using cqlsh against the cluster constructed by the dtest.
{noformat}
cqlsh> describe schema;

CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE ks.clicks (
    userid int,
    url text,
    total counter,
    PRIMARY KEY (userid, url)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (url ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

cqlsh> use ks;
cqlsh:ks> UPDATE clicks SET total = total + 1 WHERE userid = 1 AND url = 'http://foo.com';
cqlsh:ks> SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'
      ... ;

 total
-------
     0

(1 rows)
{noformat}",N/A,"3.0.18, 3.11.4"
CASSANDRA-14954,"Website documentation for stable and latest, with stable as default linked","The website should link Documentation to the docs generated for our most recent stable release.

By providing directory listing (using {{`htaccess Indexes`}}) under /doc/, and having two symlinks {{latest}} and {{stable}}, we can by default link to {{stable}}.

The following patch
 - adds to the website Makefile the task {{add-stable-doc}}
 - changes the default documentation link to {{/doc/stable/}}
 - removes the html redirecting from {{doc/ --> doc/latest/}}
 - adds directory listing to {{/doc/}} for a simple view of versioned docs available",N/A,"3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-14952,NPE when using allocate_tokens_for_keyspace and add new DC,"Received following NPE while bootstrapping very first node in the new datacenter with {{allocate_tokens_for_keyspace}} yaml option
{code:java}
INFO  21:44:13 JOINING: getting bootstrap token
Exception (java.lang.NullPointerException) encountered during startup: null
java.lang.NullPointerException
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.getStrategy(TokenAllocation.java:208)
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.getStrategy(TokenAllocation.java:170)
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.allocateTokens(TokenAllocation.java:55)
	at org.apache.cassandra.dht.BootStrapper.allocateTokens(BootStrapper.java:206)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:173)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:854)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:579)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:351)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:586)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:714)

{code}
Please find reproducible steps here:
 1. Set {{allocate_tokens_for_keyspace}} property with {{Networktopologystrategy}} say {{{{Networktopologystrategy, 'dc1' : 1, 'dc2' : 1}}}}
 2. Start first node in {{dc1}}
 3. Now bootstrap second node in {{dc2,}} it will throw above exception.

RCA:
 [doAddEndpoint|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/locator/TokenMetadata.java#L1325] is invoked from the [bootstrap|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L1254] and at this time [local node's rack information|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/locator/TokenMetadata.java#L1276] is available

However with have {{allocate_tokens_for_keyspace}} option, daemon tries to access rack information even before calling [bootstrap|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L1241] function, at [this place|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L878] which results in NPE

Fix:
 Since this is applicable to only very first node for new dc, we can check for {{null}} as:
{code:java}
diff --git a/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java b/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
index 8d8a6ffeca..e162757d95 100644
--- a/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
+++ b/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
@@ -205,7 +205,11 @@ public class TokenAllocation
         final int replicas = rs.getReplicationFactor(dc);
 
         Topology topology = tokenMetadata.getTopology();
-        int racks = topology.getDatacenterRacks().get(dc).asMap().size();
+        int racks = 1;
+        if (topology.getDatacenterRacks().get(dc) != null)
+        {
+            racks = topology.getDatacenterRacks().get(dc).asMap().size();
+        }
 
         if (racks >= replicas)
         {
{code}
Let me know your comments.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14951,Add a script to make running the cqlsh tests in cassandra repo easier,Some CQLSH tests have not been running. They need to be reenabled and fixed before 4.0.,N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14950,Update comment link to mx4j in cassandra-env.sh ,"Seems this 

[http://wiki.apache.org/cassandra/Operations#Monitoring_with_MX4J]

has moved to http://cassandra.apache.org/doc/latest/operating/metrics.html#jmx

 

 ",N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14949,Fix documentation for default compressor used,Fix documentation at [http://cassandra.apache.org/doc/4.0/operating/compression.html] to correct default compressor to {{LZ4Compressor}}. (And has been since Cassandra-2.0),N/A,"3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14948,Backport dropped column checks to 3.0 and 3.11,"This is a follow on from CASSANDRA-14913 and CASSANDRA-14843 that introduced some fixes to prevent and mitigate data corruption caused by dropping a column then re-adding it with the same name but an incompatible type (e.g. simple int to a complex map<>) or different kind (regular/static). 

This patch backports the checks that now exist in trunk. This does include adding a column to the dropped_columns table to keep track of static columns like trunk, not sure it we are able to make that change in 3.11.x. 

Also not sure what our stance on backporting just the isValueCompatibleWith check to 3.0 is. I'd be for it since it prevents recreating a simple column as a map (or vice-versa) which will basically always lead to corruption.

||C* 3.11.x||
|[Patch|https://github.com/vincewhite/cassandra/commit/3986b53b8acaf1d3691f9b35fd098a40667c520f]|
",N/A,"3.0.19, 3.11.5"
CASSANDRA-14937,Multi-version In-JVM dtests,"In order to support more sophisticated upgrade tests, including complex fuzz tests that can span a sequence of version upgrades, we propose abstracting a cross-version API for the in-jvm dtests.  This will permit starting a node with an arbitrary compatible C* version, stopping the node, and restarting it with another C* version.",N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14934,Cassandra Too many open files .,"We have 4 Node with 2 DC . 

We are seeing 

Caused by: java.nio.file.FileSystemException: /apps/cassandra/data/commitlog/CommitLog-6-1544453835569.log: Too many open files .

We have increased the ulimit to 24 k. 

lsof | grep <processid> 

.3.3 million records . 

 

Found ~ .2.5 millions of records are of TCP connection with either Socket connection status as ESTABLISHED/LISTEN . Found that connections are of datasource on 9042 . 

 

Thanks

 ",N/A,3.10
CASSANDRA-14931,"Backport In-JVM dtests to 2.2, 3.0 and 3.11","The In-JVM dtests are of significant value, and much of the testing we are exploring with it can easily be utilised on all presently maintained versions.  We should backport the functionality to at least 3.0.x and 3.11.x - and perhaps even consider 2.2.",N/A,"2.2.14, 3.0.18, 3.11.4"
CASSANDRA-14928,MigrationManager attempts to pull schema from different major version nodes,"MigrationManager will do the version check against nodes it hasn't connected to yet so it doesn't know their messaging service version. We should also check the version in gossip as an additional layer of protection.

This causes many of the upgrade tests to fail.",N/A,"2.2.14, 3.0.18, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-14925,DecimalSerializer.toString() can be used as OOM attack ,"Currently, in {{DecimalSerializer.toString(value)}}, it uses {{BigDecimal.toPlainString()}} which generates huge string for large scale values.

 
{code:java}
BigDecimal d = new BigDecimal(""1e-"" + (Integer.MAX_VALUE - 6));
d.toPlainString(); // oom{code}
 

Propose to use {{BigDecimal.toString()}} when scale is larger than 100 which is configurable via {{-Dcassandra.decimal.maxscaleforstring}}

 
| patch | circle-ci |
|[trunk|https://github.com/jasonstack/cassandra/commits/decimal-tostring-trunk]|[unit|https://circleci.com/gh/jasonstack/cassandra/751?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link]|

The code should apply cleanly to 3.0+.
",N/A,"3.0.24, 3.11.10, 4.0-rc1, 4.0"
CASSANDRA-14921,thrift_test.py failures on 3.0 and 3.x branches,"{{putget_test::TestPutGet::test_wide_slice}} fails on CircleCI since the docker image was updated for CASSANDRA-14713. The reason for this is that the {{fastbinary}} extension used by {{TBinaryProtocolAccelerated}} is not compatible with thrift 0.10.0 (according to [this bug report against Pycassa|https://github.com/pycassa/pycassa/issues/245]). The offending binary is present in the filesystem of the [current docker image|https://hub.docker.com/r/spod/cassandra-testing-ubuntu18-java11/], but wasn't in [the previous image |https://hub.docker.com/r/kjellman/cassandra-test/], which meant that thrift would fallback to the standard protocol implementation (silently).

As this is the only test which uses {{TBinaryProtocolAccelerated}} it's easy enough to switch it to {{TBinaryProtocol}}, which also fixes things. We might want consider removing the binary next time the image is updated though (cc [~spodxx@gmail.com]).",N/A,"3.0.18, 3.11.4"
CASSANDRA-14919,Regression in paging queries in mixed version clusters ,"The changes to handling legacy bounds in CASSANDRA-14568/CASSANDRA-14749/CASSANDRA-14912 break paging queries where the coordinator is a legacy node and the replica is an upgraded node. 

The long-held assumption made by {{LegacyLayout::decodeBound}} that ""There can be more components than the clustering size only in the case this is the bound of a collection range tombstone."" is not true as serialized paged read commands may also include these type of bounds in their {{SliceQueryFilter}}. The additional checks the more recent tickets add cause such queries to error when processed by a 3.0 replica.",N/A,"3.0.18, 3.11.4"
CASSANDRA-14916,Add missing commands to nodetool_completion,"Since [CASSANDRA-6421|https://issues.apache.org/jira/browse/CASSANDRA-6421], the file nodetool_completion haven't been modified in order to add the new features of nodetool command.

I propose this patch to add those missing features.

I tried to follow the logic of the code, I hope I did not miss anything. [~cscetbon] , I would be happy if you have a look to the patch.",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14915,Handle ant-optional dependency,"CASSANDRA-13117 added a JUnit task which dumps threads on unit test timeout, and it depends on a class in {{org.apache.tools.ant.taskdefs.optional}} which seems to not always be present depending on how {{ant}} was installed. It can cause this error when building;
{code:java}
Throws: cassandra-trunk/build.xml:1134: taskdef A class needed by class org.krummas.junit.JStackJUnitTask cannot be found:
org/apache/tools/ant/taskdefs/optional/junit/JUnitTask  using the classloader
AntClassLoader[/.../cassandra-trunk/lib/jstackjunit-0.0.1.jar]
{code}",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14914,Deserialization Error," 
 I have a single cassandra, now this error appears when I start the server:
{code:java}
ERROR 11:18:45 Exiting due to error while processing commit log during initialization.
org.apache.cassandra.db.commitlog.CommitLogReadHandler$CommitLogReadException: Unexpected error deserializing mutation; saved to /tmp/mutation4787806670239768067dat.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception follows: org.apache.cassandra.serializers.MarshalException: A local deletion time should not be negative
{code}
If I delete all the commitlog and saved_cached files the server goes up, but the next day when I reboot the cassandra, the error occurs again.

The file mutationDDDDDDDDDDdat change name for each restart. I attachament a example mutation file .

What's wrong? How to make cassandra stable again?

 ",N/A,3.11.x
CASSANDRA-14912,LegacyLayout errors on collection tombstones from dropped columns,"When reading legacy sstables in 3.0, the dropped column records in table metadata are not checked when a collection tombstone is encountered. This means that if a collection column was dropped and a new column with the same name but a non-collection type subsequently added prior to upgrading to 3.0, reading any sstables containing the collection data will error. This includes reads done by upgradesstables, which makes recovery from this situation without losing data impossible. Scrub will clean the affected tables, but any valid data will also be discarded.",N/A,"3.0.18, 3.11.4"
CASSANDRA-14909,Netty IOExceptions caused by unclean client disconnects being logged at INFO instead of TRACE,"Observed spam logs on 3.0.17 cluster with redundant Netty IOExceptions caused due to client-side disconnections.

{code:java}
INFO  [epollEventLoopGroup-2-28] 2018-11-20 23:23:04,386 Message.java:619 - Unexpected exception during request; channel = [id: 0x12995bc1, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xxx.xxx:33754]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
	at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}


{code:java}
INFO  [epollEventLoopGroup-2-23] 2018-11-20 13:16:33,263 Message.java:619 - Unexpected exception during request; channel = [id: 0x98bd7c0e, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.xx.xx:33350]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
	at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

[CASSANDRA-7849|https://issues.apache.org/jira/browse/CASSANDRA-7849] addresses this for JAVA IO Exception like ""java.io.IOException: Connection reset by peer"", but not for Netty IOException since the exception message in Netty includes method name.",N/A,"3.0.18, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-14907,"cassandra-stress does not work with frozen collections: list, set","{code}
com.datastax.driver.core.exceptions.InvalidQueryException: Invalid operation (f_list = f_list + ?) for frozen collection column f_list
{code}
| patch | utest |
| [3.0|https://github.com/jasonstack/cassandra/commits/stress_frozen_collection_3.0] | [circle|https://circleci.com/gh/jasonstack/cassandra/745] |

This patch should apply cleanly.. {{Map}} is not supported yet..
",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14905,"If SizeEstimatesRecorder misses a 'onDropTable' notification, the size_estimates table will never be cleared for that table.","if a node is down when a keyspace/table is dropped, it will receive the schema notification before the size estimates listener is registered, so the entries for the dropped keyspace/table will never be cleaned from the table. ",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14904,SSTableloader doesn't understand listening for CQL connections on multiple ports,"sstableloader only searches the yaml for native_transport_port, so if native_transport_port_ssl is set and encryption is enabled sstableloader will fail to connect as it will use the non-SSL port for the connection.",N/A,"3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-14903,Nodetool cfstats prints index name twice,"{code:java}
CREATE TABLE test.test (
id int PRIMARY KEY,
data text
);
CREATE INDEX test_data_idx ON test.test (data);

ccm node1 nodetool cfstats test

Total number of tables: 40
----------------
Keyspace : test
Read Count: 0
Read Latency: NaN ms
Write Count: 0
Write Latency: NaN ms
Pending Flushes: 0
Table (index): test.test_data_idxtest.test_data_idx
{code}",N/A,"3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14900,DigestMismatchException log messages should be at TRACE,"DigestMismatchException log messages should probably be at TRACE. These log messages about normal digest mismatches that include scary stacktraces:

{noformat}
DEBUG [ReadRepairStage:40] 2017-10-24 19:45:50,349  ReadCallback.java:242 - Digest mismatch:
org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(-786225366477494582, 31302e33322e37382e31332d6765744469736b5574696c50657263656e742d736463) (943070f62d72259e3c25be0c6f76e489 vs f4c7c7675c803e0028992e11e0bbc5a0)
        at org.apache.cassandra.service.DigestResolver.compareResponses(DigestResolver.java:92) ~[cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.run(ReadCallback.java:233) ~[cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [cassandra-all-3.11.0.1855.jar:3.11.0.1855]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{noformat}",N/A,"3.0.25, 3.11.11"
CASSANDRA-14898,Key cache loading is very slow when there are many SSTables,"While dealing with a production issue today where some 3.0.17 nodes had close to ~8k sstables on disk due to excessive write pressure, we had a few nodes crash due to OOM and then they took close to 17 minutes to load the key cache and recover. This excessive key cache load significantly increased the duration of the outage (to mitigate we just removed the saved key cache files). For example here is one example taking 17 minutes to load 10k keys, or about 10 keys per second (which is ... very slow):
{noformat}
INFO  [pool-3-thread-1] 2018-11-15 21:50:21,885 AutoSavingCache.java:190 - reading saved cache /mnt/data/cassandra/saved_caches/KeyCache-d.db
INFO  [pool-3-thread-1] 2018-11-15 22:07:16,490 AutoSavingCache.java:166 - Completed loading (1014606 ms; 10103 keys) KeyCache cache
{noformat}
I've witnessed similar behavior in the past with large LCS clusters, and indeed it appears that any time the number of sstables is large, KeyCache loading takes a _really_ long time. Today I got a flame graph and I believe that I found the issue and I think it's reasonably easy to fix. From what I can tell the {{KeyCacheSerializer::deserialize}} [method |https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/service/CacheService.java#L445] which is called for every key is linear in the number of sstables due to the [call|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/service/CacheService.java#L459] to {{ColumnFamilyStore::getSSTables}} which ends up calling {{View::select}} [here|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/db/lifecycle/View.java#L139]. The {{View::select}} call is linear in the number of sstables and causes a _lot_ of {{HashSet}} [resizing|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/db/lifecycle/View.java#L139] when the number of sstables is much greater than 16 (the default size of the backing {{HashMap}}).

As we see in the attached flamegraph we spend 50% of our CPU time in these {{getSSTable}} calls, of which 36% is spent adding sstables to the HashSet in {{View::select}} and 17% is spent just iterating the sstables in the first place. A full 16% of CPU time is spent _just resizing the HashMap_. Then another 4% is spend calling {{CacheService::findDesc}} which does [a linear search|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/service/CacheService.java#L475] for the sstable generation.

I believe that this affects at least Cassandra 3.0.17 and trunk, and could be pretty easily fixed by either caching the getSSTables call or at the very least pre-sizing the {{HashSet}} in {{View::select}} to be the size of the sstables map.",N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-14894,RangeTombstoneList doesn't properly clean up mergeable or superseded rts in some cases,"There are a few scenarios RangeTombstoneList doesn't handle correctly.

If there are 2 overlapping range tombstones with identical timestamps, they should be merged. Instead, they're stored as 2 rts with congruent bounds and identical timestamps.

If a range tombstone supersedes multiple sequential range tombstones, instead of removing them, they cause the superseding rt to be split into multiple rts with congruent bounds and identical timestamps.

When converted to an UnfilteredRowIterator, these become extra boundary markers with the same timestamp on each side. Logically these are noops, but they do cause digest mismatches which will cause unneeded read repairs, and repair overstreaming (since they're also included in flushed sstables).

Also, not sure if this is reachable in practice, but querying RTL with an empty slice that covers a range tombstone causes an rt to be returned with an empty slice. If reachable this might cause extra read repairs as well.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14889,fix 14861 related test failures,CASSANDRA-14861 broke a few tests unfortunately,N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14884,"Move TWCS message ""No compaction necessary for bucket size"" to Trace level","When using TWCS, this message sometimes spams the debug logs:

DEBUG [CompactionExecutor:4993|https://datastax.jira.com/wiki/display/CompactionExecutor/4993] 2018-04-20 00:41:13,795 TimeWindowCompactionStrategy.java:304 - No compaction necessary for bucket size 1 , key 1521763200000, now 1524182400000

The similar message is already at trace level for LCS, so this patch changes the message from TWCS to trace as well.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14878,Race condition when setting bootstrap flags,"{{StorageService#bootstrap()}} is supposed to wait for bootstrap to finish, but Guava calls the future listeners [after|https://github.com/google/guava/blob/ec2dedebfa359991cbcc8750dc62003be63ec6d3/guava/src/com/google/common/util/concurrent/AbstractFuture.java#L890] unparking its waiters, which causes a race on when the {{bootstrapFinished()}} will be executed, making it non-deterministic.",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-14876,Snapshot name merges with keyspace name shown by nodetool listsnapshots for snapshots with long names,"If snapshot name is long enough, it will merge  keyspace name and the command output will be inconvenient to read for a {{nodetool}} user, e.g.

{noformat}
bin/nodetool listsnapshots
Snapshot Details:
Snapshot name       Keyspace name                Column family name           True size          Size on disk
1541670390886       system_distributed           parent_repair_history        0 bytes            13 bytes
1541670390886       system_distributed           repair_history               0 bytes            13 bytes
1541670390886       system_auth                  roles                        0 bytes            4.98 KB
1541670390886       system_auth                  role_members                 0 bytes            13 bytes
1541670390886       system_auth                  resource_role_permissons_index0 bytes            13 bytes
1541670390886       system_auth                  role_permissions             0 bytes            13 bytes
1541670390886       system_traces                sessions                     0 bytes            13 bytes
1541670390886       system_traces                events                       0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_distributed           parent_repair_history        0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_distributed           repair_history               0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_auth                  roles                        0 bytes            4.98 KB
39_characters_long_name_2017-09-05-11-Usystem_auth                  role_members                 0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_auth                  resource_role_permissons_index0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_auth                  role_permissions             0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_traces                sessions                     0 bytes            13 bytes
39_characters_long_name_2017-09-05-11-Usystem_traces                events                       0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_distributed           parent_repair_history        0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_distributed           repair_history               0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  roles                        0 bytes            4.98 KB
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  role_members                 0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  resource_role_permissons_index0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_auth                  role_permissions             0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_traces                sessions                     0 bytes            13 bytes
41_characters_long_name_2017-09-05-11-UTCsystem_traces                events                       0 bytes            13 bytes
{noformat}",N/A,3.0.19
CASSANDRA-14873,Fix missing rows when reading 2.1 SSTables with static columns in 3.0,"If a partition has a static row and is large enough to be indexed, then {{firstName}} of the first index block will be set to a static clustering. When deserializing the column index we then incorrectly deserialize the {{firstName}} as a regular, non-{{STATIC}} {{Clustering}} - a singleton array with an empty {{ByteBuffer}} to be exact. Depending on the clustering comparator, this can trip up binary search over {{IndexInfo}} list and cause an incorrect resultset to be returned.",N/A,"3.0.18, 3.11.4"
CASSANDRA-14871,"Severe concurrency issues in STCS,DTCS,TWCS,TMD.Topology,TypeParser","   There are a couple of places in the code base that do not respect that j.u.HashMap + related classes are not thread safe and some parts rely on internals of the implementation of HM, which can change.

We have observed failures like {{NullPointerException}} and  {{ConcurrentModificationException}} as well as wrong behavior.

Affected areas in the code base:
 * {{SizeTieredCompactionStrategy}}
 * {{DateTieredCompactionStrategy}}
 * {{TimeWindowCompactionStrategy}}
 * {{TokenMetadata.Topology}}
 * {{TypeParser}}
 * streaming / concurrent access to {{LifecycleTransaction}} (handled in CASSANDRA-14554)

While the patches for the compaction strategies + {{TypeParser}} are pretty straight forward, the patch for {{TokenMetadata.Topology}} requires it to be made immutable.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14870,The order of application of nodetool garbagecollect is broken,"{{nodetool garbagecollect}} was intended to work from oldest sstable to newest, so that the collection in newer tables can purge tombstones over data that has been deleted.

However, {{SSTableReader.maxTimestampComparator}} currently sorts in the opposite order (the order changed in CASSANDRA-13776 and then back in CASSANDRA-14010), which makes the garbage collection unable to purge any tombstones.",N/A,"3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14869,Range.subtractContained produces incorrect results when used on full ring,"Currently {{Range.subtractContained}} returns incorrect results if minuend range covers full ring and:
* subtrahend range wraps around. For example, {{(50, 50] - (10, 100]}} returns {{\{(50,10], (100,50]\}}} instead of {{(100,10]}}
* subtrahend range covers the full ring as well. For example {{(50, 50] - (0, 0]}} returns {{\{(0,50], (50,0]\}}} instead of {{\{\}}}",N/A,"3.0.18, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-14866,Issue a CQL native protocol warning if SASI indexes are enabled on a table,"If someone enables SASI indexes then we should return a native protocol warning that will be printed by cqlsh saying that they are beta quality still and you need to be careful with using them in production.

This is motivated not only by [the existing bugs and limitations|https://issues.apache.org/jira/browse/CASSANDRA-12674?jql=project%20%3D%20CASSANDRA%20AND%20status%20%3D%20Open%20AND%20component%20%3D%20sasi] but for the fact that they haven't been extensively tested yet.",N/A,"3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14865,"Cascading calls to read retries, system_auth, and read repairs","Roles validity and permission cache values are the default ones. Same thing for the read-repair chance.

We have a cluster with 3 data centers. We have noticed that in 2 of the data centers (NEC and CN) we have multiple calls to speculative read retries (rapid read protection), roles (instead of using cached values within the same tracing session), and multiple read repair messages.

We would expect calls to roles, within sequential read sessions, because the cache could be turned, but not so many calls within the same tracing session. Same thing for read-retries, and read repair messages. We are discarding blocking read repairs possibility because we consistently get the same results when doing the query several times.

It feels like something is cascading calls to these mechanisms regardless of conditions that would prevent them from being called (cached roles values for instance).

I have attached tracing files from the 3 data centers. Please let me know if more info is needed.",N/A,3.11.1
CASSANDRA-14861,sstable min/max metadata can cause data loss,"There’s a bug in the way we filter sstables in the read path that can cause sstables containing relevant range tombstones to be excluded from reads. This can cause data resurrection for an individual read, and if compaction timing is right, permanent resurrection via read repair. 

We track the min and max clustering values when writing an sstable so we can avoid reading from sstables that don’t contain the clustering values we’re looking for in a given read. The min max for each clustering column are updated for each row / RT marker we write. In the case of range tombstones markers though, we only update the min max for the clustering values they contain, which is almost never the full set of clustering values. This leaves a min/max that are above/below (respectively) the real ranges covered by the range tombstone contained in the sstable.

For instance, assume we’re writing an sstable for a table with 3 clustering values. The current min clustering is 5:6:7. We write an RT marker for a range tombstone that deletes any row with the value 4 in the first clustering value so the open marker is [4:]. This would make the new min clustering 4:6:7 when it should really be 4:. If we do a read for clustering values of 4:5 and lower, we’ll exclude this sstable and it’s range tombstone, resurrecting any data there that this tombstone would have deleted.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14855,"Message Flusher scheduling fell off the event loop, resulting in out of memory","We recently had a production issue where about 10 nodes in a 96 node cluster ran out of heap. 

From heap dump analysis, I believe there is enough evidence to indicate `queued` data member of the Flusher got too big, resulting in out of memory.
Below are specifics on what we found from the heap dump (relevant screenshots attached):
* non-empty ""queued"" data member of Flusher having retaining heap of 0.5GB, and multiple such instances.
* ""running"" data member of Flusher having ""true"" value
* Size of scheduledTasks on the eventloop was 0.

We suspect something (maybe an exception) caused the Flusher running state to continue to be true, but was not able to schedule itself with the event loop.
Could not find any ERROR in the system.log, except for following INFO logs around the incident time.


{code:java}
INFO [epollEventLoopGroup-2-4] 2018-xx-xx xx:xx:xx,592 Message.java:619 - Unexpected exception during request; channel = [id: 0x8d288811, L:/xxx.xx.xxx.xxx:7104 - R:/xxx.xx.x.xx:18886]
io.netty.channel.unix.Errors$NativeIoException: readAddress() failed: Connection timed out
 at io.netty.channel.unix.Errors.newIOException(Errors.java:117) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.Errors.ioResult(Errors.java:138) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.unix.FileDescriptor.readAddress(FileDescriptor.java:175) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollChannel.doReadBytes(AbstractEpollChannel.java:238) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:926) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:397) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:302) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131) [netty-all-4.0.44.Final.jar:4.0.44.Final]
 at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144) [netty-all-4.0.44.Final.jar:4.0.44.Final]
{code}

I would like to pursue the following proposals to fix this issue:
# ImmediateFlusher: Backport trunk's ImmediateFlusher ( [CASSANDRA-13651|https://issues.apache.org/jira/browse/CASSANDRA-13651] https://github.com/apache/cassandra/commit/96ef514917e5a4829dbe864104dbc08a7d0e0cec)  to 3.0.x and maybe to other versions as well, since ImmediateFlusher seems to be more robust than the existing Flusher as it does not depend on any running state/scheduling.
# Make ""queued"" data member of the Flusher bounded to avoid any potential of causing out of memory due to otherwise unbounded nature.


",N/A,3.0.18
CASSANDRA-14847,improvement of nodetool status -r,"Hello,

When using ""nodetool status -r"", I found a problem that the response time becomes longer depending on the number of vnodes.
 In my testing environment, when the num_token is 256 and the number of nodes is 6, the response takes about 60 seconds.

It turned out that the findMaxAddressLength method in status.java is causing the delay.
 Despite only obtaining the maximum length of the address by the number of vnodes, `tokenrange * vnode` times also loop processing, there is redundancy.

To prevent duplicate host names from being referenced every time, I modified to check with hash.
 In my environment, the response time has been reduced from 60 seconds to 2 seconds.

I attached the patch, so please check it.
 Thank you
{code:java}
[before]
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
-- Address Load Tokens Owns (effective) Host ID Rack
UN ******* 559.32 KB 256 48.7% 0555746a-60c2-4717-b042-94ba951ef679 *******
UN ******* 721.48 KB 256 51.4% 1af4acb6-e0a0-4bcb-8bba-76ae2e225cd5 *******
UN ******* 699.98 KB 256 48.3% 5215c728-9b80-4e3c-b46b-c5b8e5eb753f *******
UN ******* 691.65 KB 256 48.1% 57da4edf-4acb-474d-b26c-27f048c37bd6 *******
UN ******* 705.66 KB 256 52.8% 07520eab-47d2-4f5d-aeeb-f6e599c9b084 *******
UN ******* 610.87 KB 256 50.7% 6b39acaf-6ed6-42e4-a357-0d258bdf87b7 *******

time : 66s

[after]
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
-- Address Load Tokens Owns (effective) Host ID Rack
UN ******* 559.32 KB 256 48.7% 0555746a-60c2-4717-b042-94ba951ef679 *******
UN ******* 721.48 KB 256 51.4% 1af4acb6-e0a0-4bcb-8bba-76ae2e225cd5 *******
UN ******* 699.98 KB 256 48.3% 5215c728-9b80-4e3c-b46b-c5b8e5eb753f *******
UN ******* 691.65 KB 256 48.1% 57da4edf-4acb-474d-b26c-27f048c37bd6 *******
UN ******* 705.66 KB 256 52.8% 07520eab-47d2-4f5d-aeeb-f6e599c9b084 *******
UN ******* 610.87 KB 256 50.7% 6b39acaf-6ed6-42e4-a357-0d258bdf87b7 *******

time : 2s
{code}",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14843,Drop/add column name with different Kind can result in corruption,"While we have always imposed that the type of any column name remains consistent, we have not done the same for its kind.  If a column’s kind is changed via Drop/Add, the SerializationHeader’s column sets will be interpreted incorrectly, and may either lead to CorruptSSTableException or silent data corruption.

The problem occurs in SerializationHeader.Component.toHeader().  In this method, we lookup columns from the metadata only by name, ignoring the static status.  If a column of the same name but different kind to our dropped column exists in the schema, we will only add this column to the list of fields we expect.  So we will be missing a column from our set of expected static columns.  We will also add the regular variant to the set of regular columns present, which it may not be.

There are a number of ways this can play out:

1) There are no other static columns in the affected sstables.  In this case we will incorrectly treat the table as containing no static data, so we will not attempt to read any static row.  This leaves a static row to be read as the first regular row, and will throw the exception in the description.
1a) If for some reason the static row is absent - say, it has expired - then depending on lexicographical ordering, and if the sstable did not contain any instance of the regular variant, we may misattribute column data.  This will probably result in corruption exceptions, but if the columns were of the same type it could be undetectable.
2) There are other static columns on the table, and in the affected sstables.  In this case, the row will be correctly treated as a static row, but depending on the dropped column’s lexicographical position relative to these, it may result in column data being misattributed to other columns.  This *could* be undetectable corruption, if the column types were the same.
3) Either of these above scenario could be replayed with the static/regular relations replaced.

CASSANDRA-14591 would protect against 1a and 2.

Probably the safest and correct fix is to prevent adding a column back with a different kind to the original, however this would require storing more metadata about dropped columns.  This is probably viable for 4.0, but for 3.0 perhaps too invasive. 

It is quite easy to make this particular method safe against this particular corruption.  However it is hard to be certain there aren’t other places where assumptions were made about a name being of only one kind.  There are only a handful of places that *should* need to be of concern, specifically those places that invoke CFMetaData.getDroppedColumn, so we should be fairly confident that - if these call sites are safe - we are now insulated.  This might suffice for 3.0.  Thoughts?
",N/A,3.0.18
CASSANDRA-14838,Dropped columns can cause reverse sstable iteration to return prematurely,"CASSANDRA-14803 fixed an issue where reading legacy sstables in reverse could return early in certain cases. It's also possible to get into this state with current version sstables if there are 2 or more indexed blocks in a row that only contain data for a dropped column. Post 14803, this will throw an exception instead of returning an incomplete response, but it should just continue reading like it does for legacy sstables",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14829,Make stop-server.bat wait for Cassandra to terminate,"While administering a single node Cassandra on Windows, I noticed that the stop-server.bat script returns before the cassandra process has actually terminated. For use cases like creating a script ""shut down & create backup of data directory without having to worry about open files, then restart"", it would be good to make stop-server.bat wait for Cassandra to terminate.

All that is needed for that is to change in apache-cassandra-3.11.3\bin\stop-server.bat ""start /B powershell /file ..."" to ""start /WAIT /B powershell /file ..."" (additional /WAIT parameter).

Does this sound reasonable?

Here is the pull request: https://github.com/apache/cassandra/pull/287",N/A,"3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14826,cassandra spinning forever on 1 thread while initializing keyspace,"When starting cassandra 3.0.17 it takes a long time to initialize a keyspace.

top shows 1 thread spinning at 100% cpu. Thread dump shows:
{code:java}
""main"" - Thread t@1

    java.lang.Thread.State: RUNNABLE

         at java.util.TimSort.mergeHi(TimSort.java:850)

         at java.util.TimSort.mergeAt(TimSort.java:516)

         at java.util.TimSort.mergeCollapse(TimSort.java:441)

         at java.util.TimSort.sort(TimSort.java:245)

         at java.util.Arrays.sort(Arrays.java:1512)

         at java.util.ArrayList.sort(ArrayList.java:1454)

         at java.util.Collections.sort(Collections.java:175)

         at org.apache.cassandra.db.compaction.LeveledManifest.canAddSSTable(LeveledManifest.java:243)

         at org.apache.cassandra.db.compaction.LeveledManifest.add(LeveledManifest.java:146)

         - locked <c89b563> (a org.apache.cassandra.db.compaction.LeveledManifest)

         at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.addSSTable(LeveledCompactionStrategy.java:298)

         at org.apache.cassandra.db.compaction.CompactionStrategyManager.startup(CompactionStrategyManager.java:135)

         at org.apache.cassandra.db.compaction.CompactionStrategyManager.reload(CompactionStrategyManager.java:187)

         - locked <742bfce7> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)

         at org.apache.cassandra.db.compaction.CompactionStrategyManager.<init>(CompactionStrategyManager.java:75)

         at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:408)

         at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:363)

         at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:579)

         - locked <4e4f20d2> (a java.lang.Class)

         at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:556)

         at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:368)

         at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:305)

         at org.apache.cassandra.db.Keyspace.open(Keyspace.java:129)

         - locked <5318346c> (a java.lang.Class)

         at org.apache.cassandra.db.Keyspace.open(Keyspace.java:106)

         at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:262)

         at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)

         at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{code}",N/A,"3.0.23, 3.11.9, 4.1-alpha1, 4.1"
CASSANDRA-14824,Expand range tombstone validation checks to multiple interim request stages,"{{RTBoundValidator}} was originally only added to verify the end-game response iterator produced by {{ReadCommand}}.

However, turns out it's possible for sequencing errors in lower-level iterators to be silently erased by upstream iterators - as a result of merging two invalid iterators into one valid iterator, for example. CASSANDRA-14823 can create such a scenario.

The upcoming patch would attach the checker at the intermediate stages, ensuring that we can detect those otherwise silent corruptions.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14823,Legacy sstables with range tombstones spanning multiple index blocks create invalid bound sequences on 3.0+,"During upgrade from 2.1 to 3.0, reading old sstables in reverse order would generate invalid sequences of range tombstone bounds if their range tombstones spanned multiple column index blocks. The read fails in different ways depending on whether the 2.1 tables were produced by a flush or a compaction.",N/A,"3.0.18, 3.11.4"
CASSANDRA-14812,Multiget Thrift query returns null records after digest mismatch,"It seems that in Cassandra 3.0.0 a nasty bug was introduced in {{multiget}} Thrift query processing logic. When one tries to read data from several partitions with a single {{multiget}} query and {{DigestMismatch}} exception is raised during this query processing, request coordinator prematurely terminates response stream right at the point where the first \{{DigestMismatch}} error is occurring. This leads to situation where clients ""do not see"" some data contained in the database.

We managed to reproduce this bug in all versions of Cassandra starting with v3.0.0. The pre-release version 3.0.0-rc2 works correctly. It looks like [refactoring of iterator transformation hierarchy|https://github.com/apache/cassandra/commit/609497471441273367013c09a1e0e1c990726ec7] related to CASSANDRA-9975 triggers incorrect behaviour.

When concatenated iterator is returned from the [StorageProxy.fetchRows(...)|https://github.com/apache/cassandra/blob/a05785d82c621c9cd04d8a064c38fd2012ef981c/src/java/org/apache/cassandra/service/StorageProxy.java#L1770], Cassandra starts to consume this combined iterator. Because of {{DigestMismatch}} exception some elements of this combined iterator contain additional {{ThriftCounter}}, that was added during [DataResolver.resolve(...)|https://github.com/apache/cassandra/blob/ee9e06b5a75c0be954694b191ea4170456015b98/src/java/org/apache/cassandra/service/reads/DataResolver.java#L120] execution. While consuming iterator for many partitions Cassandra calls [BaseIterator.tryGetMoreContents(...)|https://github.com/apache/cassandra/blob/a05785d82c621c9cd04d8a064c38fd2012ef981c/src/java/org/apache/cassandra/db/transform/BaseIterator.java#L115] method that must switch from one partition iterator to another in case of exhaustion of the former. In this case all Transformations contained in the next iterator are applied to the combined BaseIterator that enumerates partitions sequence which is wrong. This behaviour causes BaseIterator to stop enumeration after it fully consumes partition with {{DigestMismatch}} error, because this partition iterator has additional {{ThriftCounter}} data limit.

The attachment contains the python2 script [^small_repro_script.py] that reproduces this bug within 3-nodes ccmlib controlled cluster. Also, there is an extended version of this script - [^repro_script.py] - that contains more logging information and provides the ability to test behavior for many Cassandra versions (to run all test cases from repro_script.py you can call {{python -m unittest2 -v repro_script.ThriftMultigetTestCase}}). All the necessary dependencies contained in the [^requirements.txt]

 
This bug is critical in our production environment because we can't permit any data skip.

Any ideas about a patch for this issue?",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14809,cluster initialization was aborted after timing out,"i have error ""cluster initialization was aborted after timing out"". It showed on UI.

We have ReleaseVersion: 3.11.1 and there is a lot of errors. Pls help.

in system log we see this errors:

INFO [epollEventLoopGroup-2-4] 2018-10-09 19:30:50,113 Message.java:623 - Unexpected exception during request; channel = [id: 0x295ce677, L:/192.168.xx.xxx:9042 - R:/192.168.xx.xxx:58644]
io.netty.channel.unix.Errors$NativeIoException: syscall:read(...)() failed: Connection reset by peer
at io.netty.channel.unix.FileDescriptor.readAddress(...)(Unknown Source) ~[netty-all-4.0.44.Final.jar:4.0.44.Final]

 ",N/A,3.11.1
CASSANDRA-14806,CircleCI workflow improvements and Java 11 support,"The current CircleCI config could use some cleanup and improvements. First of all, the config has been made more modular by using the new CircleCI 2.1 executors and command elements. Based on CASSANDRA-14713, there's now also a Java 11 executor that will allow running tests under Java 11. The {{build}} step will be done using Java 11 in all cases, so we can catch any regressions for that and also test the Java 11 multi-jar artifact during dtests, that we'd also create during the release process.

The job workflow has now also been changed to make use of the [manual job approval|https://circleci.com/docs/2.0/workflows/#holding-a-workflow-for-a-manual-approval] feature, which now allows running dtest jobs only on request and not automatically with every commit. The Java8 unit tests still do, but that could also be easily changed if needed. See [example workflow|https://circleci.com/workflow-run/be25579d-3cbb-4258-9e19-b1f571873850] with start_ jobs being triggers needed manual approval for starting the actual jobs.",N/A,"2.2.15, 3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14804,Running repair on multiple nodes in parallel could halt entire repair ,"Possible deadlock if we run repair on multiple nodes at the same time. We have come across a situation in production in which if we repair multiple nodes at the same time then repair hangs forever. Here are the details:

Time t1
 {{node-1}} has issued repair command to {{node-2}} but due to some reason {{node-2}} didn't receive request hence {{node-1}} is awaiting at [prepareForRepair |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/ActiveRepairService.java#L333] for 1 hour *with lock*

Time t2
 {{node-2}} sent prepare repair request to {{node-1}}, some exception occurred on {{node-1}} and it is trying to cleanup parent session [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/repair/RepairMessageVerbHandler.java#L172] but {{node-1}} cannot get lock as 1 hour of time has not yet elapsed (above one)

snippet of jstack on {{node-1}}
{quote}""Thread-888"" #262588 daemon prio=5 os_prio=0 waiting on condition
 java.lang.Thread.State: TIMED_WAITING (parking)
 at sun.misc.Unsafe.park(Native Method)
 - parking to wait for (a java.util.concurrent.CountDownLatch$Sync)
 at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
 at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
 at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
 at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
 - locked <> (a org.apache.cassandra.service.ActiveRepairService)
 at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:214)
 at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
 at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$9/864248990.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748)

""AntiEntropyStage:1"" #1789 daemon prio=5 os_prio=0 waiting for monitor entry []
 java.lang.Thread.State: BLOCKED (on object monitor)
 at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
 - waiting to lock <> (a org.apache.cassandra.service.ActiveRepairService)
 at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:172)
 at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at java.util.concurrent.FutureTask.run(FutureTask.java:266)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
 at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$9/864248990.run(Unknown Source)
 at java.lang.Thread.run(Thread.java:748){quote}
Time t3:
 {{node-2}}(and possibly other nodes {{node-3}}…) sent [prepare request |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/ActiveRepairService.java#L333] to {{node-1}}, but {{node-1}}’s AntiEntropyStage thread is busy awaiting for lock at {{ActiveRepairService.removeParentRepairSession}}, hence {{node-2}}, {{node-3}} (and possibly other nodes) will also go in 1 hour wait *with lock*. This rolling effect continues and stalls repair in entire ring.

If we totally stop triggering repair then system would recover slowly but here are the two major problems with this:
 1. Externally there is no way to decide whether to trigger new repair or wait for system to recover
 2. In this case system recovers eventually but it takes probably {{n}} hours where n = #of repair requests fired, only way to come out of this situation is either to do a rolling restart of entire ring or wait for {{n}} hours before triggering new repair request

Please let me know if my above analysis makes sense or not.",N/A,3.0.18
CASSANDRA-14803,Rows that cross index block boundaries can cause incomplete reverse reads in some cases.,"When we're reading 2.1 sstables in reverse, we skip the first row of an index block if it's split across index boundaries. The entire row will be read at the end of the next block. In some cases though, the only thing in this index block is the partial row, so we return an empty iterator. The empty iterator is then interpreted as the end of the row further down the call stack, so we return early without reading the rest of the data. This only affects 3.x during upgrades from 2.1",N/A,"3.0.18, 3.11.5"
CASSANDRA-14794,Avoid calling iter.next() in a loop when notifying indexers about range tombstones,In [SecondaryIndexManager|https://github.com/apache/cassandra/blob/914c66685c5bebe1624d827a9b4562b73a08c297/src/java/org/apache/cassandra/index/SecondaryIndexManager.java#L901-L902] - avoid calling {{.next()}} in the {{.forEach(..)}},N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14790,LongBufferPoolTest burn test fails assertion,"The LongBufferPoolTest from the burn tests fails with an assertion error.  I added a build target to run individual burn tests, and \{jasobrown} gave a fix for the uninitialized test setup (attached), however the test now fails on an assertion about recycling buffers.

To reproduce (with patch applied)

{{ant burn-testsome -Dtest.name=org.apache.cassandra.utils.memory.LongBufferPoolTest -Dtest.methods=testAllocate}}

Output

{{    [junit] Testcase: testAllocate(org.apache.cassandra.utils.memory.LongBufferPoolTest): FAILED}}

{{    [junit] null}}

{{    [junit] junit.framework.AssertionFailedError}}

{{    [junit] at org.apache.cassandra.utils.memory.BufferPool$Debug.check(BufferPool.java:204)}}

{{    [junit] at org.apache.cassandra.utils.memory.BufferPool.assertAllRecycled(BufferPool.java:181)}}

{{    [junit] at org.apache.cassandra.utils.memory.LongBufferPoolTest.testAllocate(LongBufferPoolTest.java:350)}}

{{    [junit] at org.apache.cassandra.utils.memory.LongBufferPoolTest.testAllocate(LongBufferPoolTest.java:54)}}

All major branches from 3.0 and later have issues, however the trunk branch also warns about references not being released before the reference is garbage collected.

{{[junit] ERROR [Reference-Reaper:1] 2018-09-25 13:59:54,089 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7f58d19a) to @623704362 was not released before the reference was garbage collected}}
{{ [junit] ERROR [Reference-Reaper:1] 2018-09-25 13:59:54,089 Ref.java:255 - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@7f58d19a:}}
{{ [junit] Thread[pool-2-thread-24,5,main]}}
{{ [junit] at java.lang.Thread.getStackTrace(Thread.java:1559)}}
{{ [junit] at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:245)}}
{{ [junit] at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:175)}}
{{ [junit] at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:97)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool$Chunk.setAttachment(BufferPool.java:663)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:803)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:793)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool$LocalPool.get(BufferPool.java:388)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool.maybeTakeFromPool(BufferPool.java:143)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool.takeFromPool(BufferPool.java:115)}}
{{ [junit] at org.apache.cassandra.utils.memory.BufferPool.get(BufferPool.java:85)}}
{{ [junit] at org.apache.cassandra.utils.memory.LongBufferPoolTest$3.allocate(LongBufferPoolTest.java:296)}}
{{ [junit] at org.apache.cassandra.utils.memory.LongBufferPoolTest$3.testOne(LongBufferPoolTest.java:246)}}
{{ [junit] at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:399)}}
{{ [junit] at org.apache.cassandra.utils.memory.LongBufferPoolTest$TestUntil.call(LongBufferPoolTest.java:379)}}
{{ [junit] at java.util.concurrent.FutureTask.run(FutureTask.java:266)}}
{{ [junit] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}
{{ [junit] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}
{{ [junit] at java.lang.Thread.run(Thread.java:748)}}

 

Perhaps the environment is not being set up correctly for the tests.
  ",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14766,DESC order reads can fail to return the last Unfiltered in the partition in a legacy sstable,"{{OldFormatDeserializer}}’s {{hasNext()}} method can and will consume two {{Unfiltered}} from the underlying iterator in some scenarios - intentionally.

But in doing that it’s losing intermediate state of {{lastConsumedPosition}}. If that last block, when iterating backwards, only has two {{Unfiltered}}, the first one will be returned, and the last one won’t as the reverse iterator would incorrectly things that the deserisalizer is past the index block, despite still having one {{Unfiltered}} unreturned.",N/A,"3.0.18, 3.11.4"
CASSANDRA-14752,serializers/BooleanSerializer.java is using static bytebuffers which may cause problem for subsequent operations,"[https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/serializers/BooleanSerializer.java#L26] It has two static Bytebuffer variables:-
{code:java}
private static final ByteBuffer TRUE = ByteBuffer.wrap(new byte[]{1});
private static final ByteBuffer FALSE = ByteBuffer.wrap(new byte[]{0});{code}
What will happen if the position of these Bytebuffers is being changed by some other operations? It'll affect other subsequent operations. -IMO Using static is not a good idea here.-

A potential place where it can become problematic: [https://github.com/apache/cassandra/blob/cassandra-2.1.13/src/java/org/apache/cassandra/db/marshal/AbstractCompositeType.java#L243] Since we are calling *`.remaining()`* It may give wrong results _i.e 0_ if these Bytebuffers have been used previously.

Solution: 
 [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/serializers/BooleanSerializer.java#L42] Every time we return new bytebuffer object. Please do let me know If there is a better way. I'd like to contribute. Thanks!!
{code:java}
public ByteBuffer serialize(Boolean value)
{
return (value == null) ? ByteBufferUtil.EMPTY_BYTE_BUFFER
: value ? ByteBuffer.wrap(new byte[] {1}) : ByteBuffer.wrap(new byte[] {0}); // false
}
{code}",N/A,"3.0.27, 3.11.13, 4.0.4, 4.1-alpha1, 4.1"
CASSANDRA-14749,Collection Deletions for Dropped Columns in 2.1/3.0 mixed-mode can delete rows,"Similar to CASSANDRA-14568, if a 2.1 node sends a response to a 3.0 node containing a deletion for a dropped collection column, instead of deleting the collection, we will delete the row containing the collection.

 

This is an admittedly unlikely cluster state but, during such a state, a great deal of data loss could happen.",N/A,3.0.18
CASSANDRA-14748,Recycler$WeakOrderQueue occupies Heap,"Heap constantly high on some of the nodes in the cluster, I dump the heap and open it through Eclipse Memory Analyzer, looks like Recycler$WeakOrderQueue occupies most of the heap. 

 
||Package||Retained Heap||Retained Heap, %||# Top Dominators||
|!/jira/icons/i5.gif! <all>|7,078,140,136|100.00%|379,627|
|io|5,665,035,800|80.04%|13,306|
|netty|5,665,035,800|80.04%|13,306|
|util|5,568,107,344|78.67%|2,965|
|Recycler$WeakOrderQueue|4,950,021,544|69.93%|2,169|",N/A,"3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-14721,"sstabledump displays incorrect value for ""position"" key","When partitions with multiple rows are displayed using sstabledump, the ""position"" value the first row of each partition is incorrect.

For example:
{code:java}
sstabledump mc-1-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""1"", ""24"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 66, 
        ""clustering"" : [ ""2013-12-10 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.290086Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 8 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.1 },
          { ""name"" : ""feelslike"", ""value"" : 8 },
          { ""name"" : ""humidity"", ""value"" : 0.76 },
          { ""name"" : ""wind"", ""value"" : 10.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 66, 
        ""clustering"" : [ ""2013-12-11 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.295369Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 4 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.3 },
          { ""name"" : ""feelslike"", ""value"" : 4 },
          { ""name"" : ""humidity"", ""value"" : 0.9 },
          { ""name"" : ""wind"", ""value"" : 12.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 105,
        ""clustering"" : [ ""2013-12-12 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.300841Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 3 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.2 },
          { ""name"" : ""feelslike"", ""value"" : 3 },
          { ""name"" : ""humidity"", ""value"" : 0.68 },
          { ""name"" : ""wind"", ""value"" : 6.0 }
        ]
      }
    ]
  }
]
{code}
 The expected output is:
{code:java}
[
  {
    ""partition"" : {
      ""key"" : [ ""1"", ""24"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 28,
        ""clustering"" : [ ""2013-12-10 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.290086Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 8 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.1 },
          { ""name"" : ""feelslike"", ""value"" : 8 },
          { ""name"" : ""humidity"", ""value"" : 0.76 },
          { ""name"" : ""wind"", ""value"" : 10.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 66,
        ""clustering"" : [ ""2013-12-11 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.295369Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 4 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.3 },
          { ""name"" : ""feelslike"", ""value"" : 4 },
          { ""name"" : ""humidity"", ""value"" : 0.9 },
          { ""name"" : ""wind"", ""value"" : 12.0 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 105,
        ""clustering"" : [ ""2013-12-12 00:00:00.000Z"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-09-12T05:01:09.300841Z"" },
        ""cells"" : [
          { ""name"" : ""centigrade"", ""value"" : 3 },
          { ""name"" : ""chanceofrain"", ""value"" : 0.2 },
          { ""name"" : ""feelslike"", ""value"" : 3 },
          { ""name"" : ""humidity"", ""value"" : 0.68 },
          { ""name"" : ""wind"", ""value"" : 6.0 }
        ]
      }
    ]
  }
]
{code}",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-14713,Update docker image used for testing,"Tests executed on builds.apache.org ({{docker/jenkins/jenkinscommand.sh}}) and circleCI ({{.circleci/config.yml}}) will currently use the same [cassandra-test|https://hub.docker.com/r/kjellman/cassandra-test/] docker image ([github|https://github.com/mkjellman/cassandra-test-docker]) by [~mkjellman].

We should manage this image on our own as part of cassandra-builds, to keep it updated. There's also a [Apache user|https://hub.docker.com/u/apache/?page=1] on docker hub for publishing images.",N/A,"2.2.14, 3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14701,Cleanup (and other) compaction type(s) not counted in compaction remaining time,"Opened a ticket, as discussed in user list.

Looks like compaction remaining time only includes compactions of type COMPACTION and other compaction types like cleanup etc. aren't part of the estimation calculation.

E.g. from one of our environments:
{noformat}
nodetool compactionstats -H

pending tasks: 1
   compaction type   keyspace           table   completed     total    unit   progress
           Cleanup        XXX             YYY   908.16 GB   1.13 TB   bytes     78.63%
Active compaction remaining time :   0h00m00s
{noformat}
",N/A,"3.0.25, 3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-14676,Use joins and aggregate functions in cassandra,"Team, 

 

I am using Apache Cassandra 3.11.2 version , apart from Spark , do we have any other alternative to use Joins and Aggregate functions in cassandra .

 

 

Thanks,

RK.",N/A,3.11.2
CASSANDRA-14672,"After deleting data in 3.11.3, reads fail with ""open marker and close marker have different deletion times""","We had 3.11.0, then we upgraded to 3.11.3 last week. We routinely perform deletions as the one described below. After upgrading we run the following deletion query:

 
{code:java}
DELETE FROM measurement_events_dbl WHERE measurement_source_id IN ( 9df798a2-6337-11e8-b52b-42010afa015a,  9df7717e-6337-11e8-b52b-42010afa015a, a08b8042-6337-11e8-b52b-42010afa015a, a08e52cc-6337-11e8-b52b-42010afa015a, a08e6654-6337-11e8-b52b-42010afa015a, a08e6104-6337-11e8-b52b-42010afa015a, a08e6c76-6337-11e8-b52b-42010afa015a, a08e5a9c-6337-11e8-b52b-42010afa015a, a08bcc50-6337-11e8-b52b-42010afa015a) AND year IN (2018) AND measurement_time >= '2018-07-19 04:00:00'{code}
 

Immediately after that, trying to read the last value produces an error:
{code:java}
select * FROM measurement_events_dbl WHERE measurement_source_id = a08b8042-6337-11e8-b52b-42010afa015a AND year IN (2018) order by measurement_time desc limit 1;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 2 failures"" info={'failures': 2, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}{code}
 

And the following exception: 
{noformat}
WARN [ReadStage-4] 2018-08-29 06:59:53,505 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-4,5,main]: {}
java.lang.RuntimeException: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times
 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2601) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_181]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
Caused by: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: open marker and close marker have different deletion times
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:103) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.applyToMarker(RTBoundValidator.java:81) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:148) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:136) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:92) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:79) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:308) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:180) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:176) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:352) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1889) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2597) ~[apache-cassandra-3.11.3.jar:3.11.3]
 ... 5 common frames omitted
 Suppressed: java.lang.IllegalStateException: UnfilteredRowIterator for pvpms_mevents.measurement_events_dbl has an illegal RT bounds sequence: expected all RTs to be closed, but the last one is open
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.ise(RTBoundValidator.java:103) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.RTBoundValidator$RowsTransformation.onPartitionClose(RTBoundValidator.java:96) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseRows.runOnClose(BaseRows.java:91) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.transform.BaseIterator.close(BaseIterator.java:86) ~[apache-cassandra-3.11.3.jar:3.11.3]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:309) ~[apache-cassandra-3.11.3.jar:3.11.3]
 ... 12 common frames omitted
 
{noformat}
 

We have 9 nodes ~2TB each, leveled compaction, repairs run daily in sequence.

Table definition is:
{noformat}
CREATE TABLE pvpms_mevents.measurement_events_dbl (
 measurement_source_id uuid,
 year int,
 measurement_time timestamp,
 event_reception_time timestamp,
 quality double,
 value double,
 PRIMARY KEY ((measurement_source_id, year), measurement_time)
) WITH CLUSTERING ORDER BY (measurement_time ASC)
 AND bloom_filter_fp_chance = 0.1
 AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
 AND comment = ''
 AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
 AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
 AND crc_check_chance = 1.0
 AND dclocal_read_repair_chance = 0.1
 AND default_time_to_live = 0
 AND gc_grace_seconds = 864000
 AND max_index_interval = 2048
 AND memtable_flush_period_in_ms = 0
 AND min_index_interval = 128
 AND read_repair_chance = 0.0
 AND speculative_retry = '99PERCENTILE';{noformat}
 

We host those on GCE and recreated all the nodes with disk snapshots, and we reproduced the error: after re-running the DELETE with all nodes up and no other queries running, the error was reproduced immediately.

 

We tried so far:

re-running repairs on all nodes and running nodetool garbagecollect with no success.

We downgraded to 3.11.2 for now, no issues so far.

This may be related to CASSANDRA-14515",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14660,Improve TokenMetaData cache populating performance for large cluster,"TokenMetaData#cachedOnlyTokenMap is a method C* used to get a consistent token and topology view on coordinations without paying read lock cost. Upon first read the method acquire a synchronize lock and generate a copy of major token meta data structures and cached it, and upon every token meta data changes(due to gossip changes), the cache get cleared and next read will taking care of cache population.

For small to medium size clusters this strategy works pretty well. But large clusters can actually be suffered from the locking since cache populating is much slower. On one of our largest cluster (~1000 nodes,  125k tokens, C* 3.0.15)  each cache population take about 500~700ms, and during that there are no requests can go through since synchronize lock was acquired. This caused waves of timeouts errors when there are large amount gossip messages propagating cross the cluster, such as in the case of cluster restarting.

Base on profiling we found that the cost mostly comes from copying tokenToEndpointMap. It is a SortedBiMultiValueMap made from a forward map use TreeMap and a reverse map use guava TreeMultiMap. There is an optimization in TreeMap helps reduce copying complexity from O(N*log(N)) to O(N) when copying from already ordered data. But guava's TreeMultiMap copying missed that optimization and make it ~10 times slower than it actually need to be on our size of cluster.

The patch attached to the issue replace the reverse TreeMultiMap<K, V> to a vanilla TreeMap<K, TreeSet<V>> in SortedBiMultiValueMap to make sure we can copy it O(N) time.

I also attached a benchmark script (TokenMetaDataBenchmark.java), which simulates a large cluster then measures average latency for TokenMetaData cache populating.

Benchmark result before and after that patch:
{code:java}
trunk: 
before 100ms, after 13ms
3.0.x: 
before 199ms, after 15ms
 {code}
(On 3.0.x even the forward TreeMap copying is slow, the O(N*log(N)) to O(N) optimization is not applied because the key comparator is dynamically created and TreeMap cannot determine the source and dest are in same order)",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14657,Handle failures in upgradesstables/cleanup/relocate,"If a compaction in {{parallelAllSSTableOperation}} throws exception, all current transactions are closed, this can make us close a transaction that has not yet finished (since we can run many of these compactions in parallel). This causes this error:
{code}
java.lang.IllegalStateException: Cannot prepare to commit unless IN_PROGRESS; state is ABORTED
{code}
and this can get the leveled manifest (if running LCS) in a bad state causing this error message:
{code}
Could not acquire references for compacting SSTables ...
{code}",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14649,Index summaries fail when their size gets > 2G and use more space than necessary,"After building a summary, {{IndexSummaryBuilder}} tries to trim the memory writers by calling {{SafeMemoryWriter.setCapacity(capacity())}}. Instead of trimming, this ends up allocating at least as much extra space and failing the {{Buffer.position()}} call when the size is greater than {{Integer.MAX_VALUE}}.",N/A,"2.2.14, 3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14642,Document lock handling in CompactionStrategyManager,"noticed that this comment is out-of-date since CASSANDRA-13948
{code}
        // we need a write lock here since we move sstables from one strategy instance to another
        readLock.lock();
{code}
we should update the comments and make sure it is actually correct to just grab a read lock here.",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-14638,Column result order can change in 'SELECT *' results when upgrading from 2.1 to 3.0 causing response corruption for queries using prepared statements when static columns are used,"When performing an upgrade from C* 2.1.20 to 3.0.17 I observed that the order of columns returned from a 'SELECT *' query changes, particularly when static columns are involved.

This may not seem like that much of a problem, however if using Prepared Statements, any clients that remain connected during the upgrade may encounter issues consuming results from these queries, as data is reordered and the client not aware of it.  The result definition is sent in the original prepared statement response, so if order changes the client has no way of knowing (until C* 4.0 via CASSANDRA-10786) without re-preparing, which is non-trivial as most client drivers cache prepared statements.

This could lead to reading the wrong values for columns, which could result in some kind of deserialization exception or if the data types of the switched columns are compatible, the wrong values.  This happens even if the client attempts to retrieve a column value by name (i.e. row.getInt(""colx"")).

Unfortunately I don't think there is an easy fix for this.  If the order was changed back to the previous format, you risk issues for users upgrading from older 3.0 version.  I think it would be nice to add a note in the NEWS file in the 3.0 upgrade section that describes this issue, and how to work around it (specify all column names of interest explicitly in query).

Example schema and code to reproduce:

 
{noformat}
create keyspace ks with replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

create table ks.tbl (p0 text,
  p1 text,
  m map<text, text> static,
  t text,
  u text static,
  primary key (p0, p1)
);

insert into ks.tbl (p0, p1, m, t, u) values ('p0', 'p1', { 'm0' : 'm1' }, 't', 'u');{noformat}
 

When querying with 2.1 you'll observe the following order via cqlsh:
{noformat}
 p0 | p1 | m            | u | t
----+----+--------------+---+---
 p0 | p1 | {'m0': 'm1'} | u | t{noformat}
 

With 3.0, observe that u and m are transposed:

 
{noformat}
 p0 | p1 | u | m            | t
----+----+---+--------------+---
 p0 | p1 | u | {'m0': 'm1'} | t{noformat}
 

 
{code:java}
import com.datastax.driver.core.BoundStatement;
import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.ColumnDefinitions;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.google.common.util.concurrent.Uninterruptibles;
import java.util.concurrent.TimeUnit;

public class LiveUpgradeTest {

  public static void main(String args[]) {
    Cluster cluster = Cluster.builder().addContactPoints(""127.0.0.1"").build();
    try {
      Session session = cluster.connect();
      PreparedStatement p = session.prepare(""SELECT * from ks.tbl"");

      BoundStatement bs = p.bind();

      // continually query every 30 seconds
      while (true) {
        try {
          ResultSet r = session.execute(bs);
          Row row = r.one();
          int i = 0;
          // iterate over the result metadata in order printing the
          // index, name, type, and length of the first row of data.
          for (ColumnDefinitions.Definition d : r.getColumnDefinitions()) {
            System.out.println(
                i++
                    + "": ""
                    + d.getName()
                    + "" -> ""
                    + d.getType()
                    + "" -> val = ""
                    + row.getBytesUnsafe(d.getName()).array().length);
          }
        } catch (Throwable t) {
          t.printStackTrace();
        } finally {
          Uninterruptibles.sleepUninterruptibly(30, TimeUnit.SECONDS);
        }
      }
    } finally {
      cluster.close();
    }
  }
}
{code}
To reproduce, set up a cluster, the schema, and run this script.  Then upgrade the cluster to 3.0.17 (with ccm, ccm stop; ccm node1 setdir -v 3.0.17; ccm start works) and observe after the client is able to reconnect that the results are in a different order.  i.e.:

 

With 2.x:

 
{noformat}
0: p0 -> varchar -> val = 2
1: p1 -> varchar -> val = 2
2: m -> map<varchar, varchar> -> val = 16
3: u -> varchar -> val = 1
4: t -> varchar -> val = 1{noformat}
 

With 3.x:

 
{noformat}
0: p0 -> varchar -> val = 2
1: p1 -> varchar -> val = 2
2: m -> map<varchar, varchar> -> val = 1
3: u -> varchar -> val = 16 (<-- the data for 'm' is now at index 3)
4: t -> varchar -> val = 1{noformat}
 

 

 

 ",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14616,cassandra-stress write hangs with default options,"Cassandra stress sits there for incredibly long time after connecting to JMX. To reproduce {code}./tools/bin/cassandra-stress write{code}

If you give it a -n its not as bad which is why dtests etc dont seem to be impacted. Does not occur in 3.0 branch but does in 3.11 and trunk",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14612,Please add OWASP Dependency Check to the build (pom.xml),"Please add OWASP Dependency Check to the build (pom.xml). OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.

OWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).

Also, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate

Generating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-14603,[dtest] read_repair_test.TestReadRepair,"tests {{test_alter_rf_and_run_read_repair}} and {{test_read_repair_chance}} consistently fail on 3.0; the latter also fails on 2.2. I suspect it's the same cause, as the output from pytest shows the same error in the same shared function ({{check_data_on_each_replica}}):

{noformat}
            res = rows_to_list(session.execute(stmt))
            logger.debug(""Actual result: "" + str(res))
            expected = [[1, 1, 1]] if expect_fully_repaired or n == initial_replica else [[1, 1, None]]
            if res != expected:
>               raise NotRepairedException()
E               read_repair_test.NotRepairedException

read_repair_test.py:204: NotRepairedException
{noformat}",N/A,"2.2.13, 3.0.17"
CASSANDRA-14591,Throw exception if Columns serialized subset encode more columns than possible,"When deserializing a \{{Columns}} subset via bitset membership, it is trivial to add a modest probability of detecting corruption, by simply testing that there are no higher bits set than the candidate \{{Columns}} permits.  This would help mitigate secondary problems arising from issues like CASSANDRA-14568.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14589,CommitLogReplayer.handleReplayError should print stack traces ,"handleReplayError does not accept an explicit Throwable parameter, so callers only integrate the exception’s message text into the log entry.  This means a loss of debug information for operators.

Note, this was fixed by CASSANDRA-8844 for 3.x+, only 3.0.x is affected.",N/A,3.0.18
CASSANDRA-14588,Unfiltered.isEmpty conflicts with Row extends AbstractCollection.isEmpty,"The isEmpty() method’s definition for a Row is incompatible with that for a Collection.  The former can return false even if there is no ColumnData for the row (i.e. the collection is of size 0).
 
This currently, by chance, doesn’t cause us any problems.  But if we ever pass a Row as a Collection to a method that invokes isEmpty() and then expects (for correctness) that the _collection_ portion is not empty, it will fail.
 
We should probably have an asCollection() method to obtain a collection from a Row, and not implement Collection directly.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14568,"Static collection deletions are corrupted in 3.0 -> 2.{1,2} messages","In 2.1 and 2.2, row and complex deletions were represented as range tombstones.  LegacyLayout is our compatibility layer, that translates the relevant RT patterns in 2.1/2.2 to row/complex deletions in 3.0, and vice versa.  Unfortunately, it does not handle the special case of static row deletions, they are treated as regular row deletions. Since static rows are themselves never directly deleted, the only issue is with collection deletions.

Collection deletions in 2.1/2.2 were encoded as a range tombstone, consisting of a sequence of the clustering keys’ data for the affected row, followed by the bytes representing the name of the collection column.  STATIC_CLUSTERING contains zero clusterings, so by treating the deletion as for a regular row, zero clusterings are written to precede the column name of the erased collection, so the column name is written at position zero.

This can exhibit itself in at least two ways:
 # If the type of your first clustering key is a variable width type, new deletes will begin appearing covering the clustering key represented by the column name.
 ** If you have multiple clustering keys, you will receive a RT covering all those rows with a matching first clustering key.
 ** This RT will be valid as far as the system is concerned, and go undetected unless there are outside data quality checks in place.
 # Otherwise, an invalid size of data will be written to the clustering and sent over the network to the 2.1 node.
 ** The 2.1/2.2 node will handle this just fine, even though the record is junk.  Since it is a deletion covering impossible data, there will be no user-API visible effect.  But if received as a write from a 3.0 node, it will dutifully persist the junk record.
 ** The 3.0 node that originally sent this junk, may later coordinate a read of the partition, and will notice a digest mismatch, read-repair and serialize the junk to disk
 ** The sstable containing this record is now corrupt; the deserialization expects fixed-width data, but it encounters too many (or too few) bytes, and is now at an incorrect position to read its structural information
 ** (Alternatively when the 2.1 node is upgraded this will occur on eventual compaction)",N/A,"3.0.17, 3.11.3"
CASSANDRA-14564, Adding regular column to COMPACT tables without clustering columns should trigger an InvalidRequestException,"I have upgraded my system from cassandra 2.1.16 to 3.11.2. We had some tables with COMPACT STORAGE enabled. We see some weird   behaviour of cassandra while adding a column into it.

Cassandra does not give any error while altering  however the added column is invisible. 

Same behaviour when we create a new table with compact storage and try to alter it. Below is the commands ran in sequence: 

 
{code:java}
x@cqlsh:xuser> CREATE TABLE xuser.employee(emp_id int PRIMARY KEY,emp_name text, emp_city text, emp_sal varint, emp_phone varint ) WITH  COMPACT STORAGE;
x@cqlsh:xuser> desc table xuser.employee ;

CREATE TABLE xuser.employee (
emp_id int PRIMARY KEY,
emp_city text,
emp_name text,
emp_phone varint,
emp_sal varint
) WITH COMPACT STORAGE
AND bloom_filter_fp_chance = 0.01
AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
AND comment = ''
AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
AND crc_check_chance = 1.0
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99PERCENTILE';{code}
Now altering the table by adding a new column:
  
{code:java}
x@cqlsh:xuser>  alter table employee add profile text;
x@cqlsh:xuser> desc table xuser.employee ;

CREATE TABLE xuser.employee (
    emp_id int PRIMARY KEY,
    emp_city text,
    emp_name text,
    emp_phone varint,
    emp_sal varint
) WITH COMPACT STORAGE
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}
notice that above desc table result does not have newly added column profile. However when i try to add it again it gives column already exist;
{code:java}
x@cqlsh:xuser>  alter table employee add profile text;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid column name profile because it conflicts with an existing column""
x@cqlsh:xuser> select emp_name,profile from employee;

 emp_name | profile
----------+---------

(0 rows)
x@cqlsh:xuser>
{code}
Inserting also behaves strange:
{code:java}
x@cqlsh:xuser> INSERT INTO employee (emp_id , emp_city , emp_name , emp_phone , emp_sal ,profile) VALUES ( 1, 'ggn', 'john', 123456, 50000, 'SE');
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Some clustering keys are missing: column1""
x@cqlsh:xuser> INSERT INTO employee (emp_id , emp_city , emp_name , emp_phone , emp_sal ,profile,column1) VALUES ( 1, 'ggn', 'john', 123456, 50000, 'SE',null);
x@cqlsh:xuser> select * from employee;

 emp_id | emp_city | emp_name | emp_phone | emp_sal
--------+----------+----------+-----------+---------

(0 rows)
{code}


*How to solve that ticket* ([~blerer])-------------------------------------------------------------------------------------- 
Adding regular columns to non-dense compact tables should be forbidden as it is the case for other column types. To do that {{AlterTableStatement}} should be modified to fire an {{InvalidRequestException}} when a user attempts to add a regular column to a  a COMPACT TABLE without clustering columns.
The fix should include a unit tests for that scenario  ",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-14559,Check for endpoint collision with hibernating nodes ,"I ran across an edge case when replacing a node with the same address. This issue results in the node(and its tokens) being unsafely removed from gossip.

Steps to replicate:

1. Create 3 node cluster.
2. Stop a node
3. Replace the stopped node with a node using the same address using the replace_address flag
4. Stop the node before it finishes bootstrapping
5. Remove the replace_address flag and restart the node to resume bootstrapping (if the data dir is also cleared at this point the node will also generate new tokens when it starts)
6. Stop the node before it finishes bootstrapping again
7. 30 Seconds later the node will be removed from gossip because it now matches the check for a FatClient

I think this is only an issue when replacing a node with the same address because other replacements now use STATUS_BOOTSTRAPPING_REPLACE and leave the dead node unchanged.

I believe the simplest fix for this is to add a check that prevents a non-bootstrapped node (without the replaces_address flag) starting if there is a gossip entry for the same address in the hibernate state. 

[3.11 PoC |https://github.com/apache/cassandra/compare/trunk...vincewhite:check_for_hibernate_on_start]


 ",N/A,"3.0.22, 3.11.8, 4.0-beta2, 4.0"
CASSANDRA-14554,Streaming needs to synchronise access to LifecycleTransaction,"When LifecycleTransaction is used in a multi-threaded context, we encounter this exception -
{quote}java.util.ConcurrentModificationException: null
 at java.util.LinkedHashMap$LinkedHashIterator.nextNode(LinkedHashMap.java:719)
 at java.util.LinkedHashMap$LinkedKeyIterator.next(LinkedHashMap.java:742)
 at java.lang.Iterable.forEach(Iterable.java:74)
 at org.apache.cassandra.db.lifecycle.LogReplicaSet.maybeCreateReplica(LogReplicaSet.java:78)
 at org.apache.cassandra.db.lifecycle.LogFile.makeRecord(LogFile.java:320)
 at org.apache.cassandra.db.lifecycle.LogFile.add(LogFile.java:285)
 at org.apache.cassandra.db.lifecycle.LogTransaction.trackNew(LogTransaction.java:136)
 at org.apache.cassandra.db.lifecycle.LifecycleTransaction.trackNew(LifecycleTransaction.java:529)
{quote}
During streaming we create a reference to a {{LifeCycleTransaction}} and share it between threads -

[https://github.com/apache/cassandra/blob/5cc68a87359dd02412bdb70a52dfcd718d44a5ba/src/java/org/apache/cassandra/db/streaming/CassandraStreamReader.java#L156]

This is used in a multi-threaded context inside {{CassandraIncomingFile}} which is an {{IncomingStreamMessage}}. This is being deserialized in parallel.

{{LifecycleTransaction}} is not meant to be used in a multi-threaded context and this leads to streaming failures due to object sharing. On trunk, this object is shared across all threads that transfer sstables in parallel for the given {{TableId}} in a {{StreamSession}}. There are two options to solve this - make {{LifecycleTransaction}} and the associated objects thread safe, scope the transaction to a single {{CassandraIncomingFile}}. The consequences of the latter option is that if we experience streaming failure we may have redundant SSTables on disk. This is ok as compaction should clean this up. A third option is we synchronize access in the streaming infrastructure.",N/A,"3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14541,Order of warning and custom payloads is unspecified in the protocol specification,"Section 2.2 of the protocol specification documents the types of tracing, warning, and custom payloads, but does not document their order in the body.",N/A,"2.2.20, 3.0.24, 3.11.10, 4.0"
CASSANDRA-14525,streaming failure during bootstrap makes new node into inconsistent state,"If bootstrap fails for newly joining node (most common reason is due to streaming failure) then Cassandra state remains in {{joining}} state which is fine but Cassandra also enables Native transport which makes overall state inconsistent. This further creates NullPointer exception if auth is enabled on the new node, please find reproducible steps here:

For example if bootstrap fails due to streaming errors like
{quote}java.util.concurrent.ExecutionException: org.apache.cassandra.streaming.StreamException: Stream failed
 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-18.0.jar:na]
 at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1256) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:894) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.initServer(StorageService.java:660) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.StorageService.initServer(StorageService.java:573) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:330) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:567) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:695) [apache-cassandra-3.0.16.jar:3.0.16]
 Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
 at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-18.0.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-18.0.jar:na]
 at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:211) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:187) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:440) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:540) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:307) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{quote}
then variable [StorageService.java::dataAvailable |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L892] will be {{false}}. Since {{dataAvailable}} is {{false}} hence it will not call [StorageService.java::finishJoiningRing |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L933] and as a result [StorageService.java::doAuthSetup|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L999] will not be invoked.

API [StorageService.java::joinTokenRing |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L763] returns without any problem. After this [CassandraDaemon.java::start|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/CassandraDaemon.java#L584] is invoked which starts native transport at 
 [CassandraDaemon.java::startNativeTransport |https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/CassandraDaemon.java#L478]

At this point daemon’s bootstrap is still not finished and transport is enabled. So client will connect to the node and will encounter {{java.lang.NullPointerException}} as following:
{quote}ERROR [SharedPool-Worker-2] Message.java:647 - Unexpected exception during request; channel = [id: 0x412a26b3, L:/a.b.c.d:9042 - R:/p.q.r.s:20121]
 java.lang.NullPointerException: null
 at org.apache.cassandra.auth.PasswordAuthenticator.doAuthenticate(PasswordAuthenticator.java:160) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator.authenticate(PasswordAuthenticator.java:82) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator.access$100(PasswordAuthenticator.java:54) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.auth.PasswordAuthenticator$PlainTextSaslAuthenticator.getAuthenticatedUser(PasswordAuthenticator.java:198) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:78) ~[apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:535) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:429) [apache-cassandra-3.0.16.jar:3.0.16]
 at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at io.netty.channel.ChannelHandlerInvokerUtil.invokeChannelReadNow(ChannelHandlerInvokerUtil.java:83) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at io.netty.channel.DefaultChannelHandlerInvoker$7.run(DefaultChannelHandlerInvoker.java:159) [netty-all-4.1.0.CR6.jar:4.1.0.CR6]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.16.jar:3.0.16]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.16.jar:3.0.16]
 at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{quote}
At this point if we run {{nodetool status}} then it will show this new node in {{UJ}} state, however clients can connect to this node over {{CQL}} and will receive {{java.lang.NullPointerException}}",N/A,"2.2.14, 3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14522,sstableloader should use discovered broadcast address to connect intra-cluster,"Currently, in the LoaderOptions for the BulkLoader, the user can give a list of initial host addresses.  That's to do the initial connection to the cluster but also to stream the sstables.  If you have two physical interfaces, one for rpc, the other for internode traffic, then bulk loader won't currently work.  It will throw an error such as:

{quote}
> sstableloader -v -u cassadmin -pw xxx -d 10.133.210.101,10.133.210.102,10.133.210.103,10.133.210.104 /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl
Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-1-big-Data.db /var/lib/cassandra/commitlog/backup_tmp/test_bkup/bkup_tbl/mc-2-big-Data.db  to [/10.133.210.101, /10.133.210.103, /10.133.210.102, /10.133.210.104]
progress: total: 100% 0  MB/s(avg: 0 MB/s)ERROR 10:16:05,311 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR 10:16:05,312 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
ERROR 10:16:05,312 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Streaming error occurred
java.net.ConnectException: Connection refused
        at sun.nio.ch.Net.connect0(Native Method) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:454) ~[na:1.8.0_101]
        at sun.nio.ch.Net.connect(Net.java:446) ~[na:1.8.0_101]
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648) ~[na:1.8.0_101]
        at java.nio.channels.SocketChannel.open(SocketChannel.java:189) ~[na:1.8.0_101]
        at org.apache.cassandra.tools.BulkLoadConnectionFactory.createConnection(BulkLoadConnectionFactory.java:60) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.createConnection(StreamSession.java:266) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.ConnectionHandler.initiate(ConnectionHandler.java:86) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:253) ~[cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [cassandra-all-3.0.15.2128.jar:3.0.15.2128]
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-all-4.0.54.Final.jar:4.0.54.Final]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_101]
progress: total: 100% 0  MB/s(avg: 0 MB/s)WARN  10:16:05,320 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
Streaming to the following hosts failed:
[/10.133.210.101, /10.133.210.103, /10.133.210.102]
java.util.concurrent.ExecutionException: org.apache.cassandra.streaming.StreamException: Stream failed
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:122)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at com.datastax.bdp.tools.ShellToolWrapper.main(ShellToolWrapper.java:34)
Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85)
        at com.google.common.util.concurrent.Futures$6.run(Futures.java:1310)
        at com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:457)
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156)
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145)
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202)
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:215)
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:191)
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:449)
        at org.apache.cassandra.streaming.StreamSession.onError(StreamSession.java:549)
        at org.apache.cassandra.streaming.StreamSession.start(StreamSession.java:259)
        at org.apache.cassandra.streaming.StreamCoordinator$StreamSessionConnector.run(StreamCoordinator.java:212)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
WARN  10:16:05,322 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
WARN  10:16:05,322 [Stream #9ed00130-6ff6-11e8-965c-93a78bf96e60] Stream failed
{quote}",N/A,"3.0.18, 3.11.4"
CASSANDRA-14515,Short read protection in presence of almost-purgeable range tombstones may cause permanent data loss,"Because read responses don't necessarily close their open RT bounds, it's possible to lose data during short read protection, if a closing bound is compacted away between two adjacent reads from a node.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14513,Reverse order queries in presence of range tombstones may cause permanent data loss,"Slice queries in descending sort order can create oversized artificial range tombstones. At CL > ONE, read repair can propagate these tombstones to all replicas, wiping out vast data ranges that they mistakenly cover.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14496,TWCS erroneously disabling tombstone compactions when unchecked_tombstone_compaction=true,"This code:
{code:java}
this.options = new TimeWindowCompactionStrategyOptions(options);
if (!options.containsKey(AbstractCompactionStrategy.TOMBSTONE_COMPACTION_INTERVAL_OPTION) && !options.containsKey(AbstractCompactionStrategy.TOMBSTONE_THRESHOLD_OPTION))
{
disableTombstoneCompactions = true;
logger.debug(""Disabling tombstone compactions for TWCS"");
}
else
logger.debug(""Enabling tombstone compactions for TWCS"");
}
{code}
... in TimeWindowCompactionStrategy.java disables tombstone compactions in TWCS if you have not *explicitly* set either tombstone_compaction_interval or tombstone_threshold.  Adding 'tombstone_compaction_interval': '86400' to the compaction stanza in a table definition has the (to me unexpected) side effect of enabling tombstone compactions. 

This is surprising and does not appear to be mentioned in the docs.

I would suggest that tombstone compactions should be run unless these options are both set to 0.

If the concern is that (as with DTCS in CASSANDRA-9234) we don't want to waste time on tombstone compactions when we expect the tables to eventually be expired away, perhaps we should also check unchecked_tombstone_compaction and still enable tombstone compactions if that's set to true.

May also make sense to set defaults for interval & threshold to 0 & disable if they're nonzero so that setting non-default values, rather than setting ANY value, is what determines whether tombstone compactions are enabled?",N/A,"3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-14477,The check of num_tokens against the length of inital_token in the yaml triggers unexpectedly,"In CASSANDRA-10120 we added a check that compares num_tokens against the number of tokens supplied in the yaml via initial_token. From my reading of CASSANDRA-10120 it was to prevent cassandra starting if the yaml contained contradictory values for num_tokens and initial_tokens which should help prevent misconfiguration via human error. The current behaviour appears to differ slightly in that it performs this comparison regardless of whether num_tokens is included in the yaml or not. Below are proposed patches to only perform the check if both options are present in the yaml.
||Branch||
|[3.0.x|https://github.com/apache/cassandra/compare/cassandra-3.0...vincewhite:num_tokens_30]|
|[3.x|https://github.com/apache/cassandra/compare/cassandra-3.11...vincewhite:num_tokens_test_1_311]|",N/A,"3.0.23, 3.11.9, 4.0-beta4, 4.0"
CASSANDRA-14475,"nodetool - Occasional high CPU on large, CPU capable machines","Periodically calling nodetool every 5 min results in increased CPU usage by nodetool only on a machine with 32 physical cores (64 vCPUs) according to our monitoring:
!nodetool_highcpu_gcthreads1_cassandra_JIRA.png|width=600!

Investigation and testing has shown that it is related to running with default number of parallel GC threads which is 43 on this particular machine. We see a System.gc() according to flight recorder but no real evidence from where it comes from. The nodetool call in question is simply gathering e.g. the version with ""nodetool version"".

After explicitly setting the number of parallel GC threads to 1, the high CPU is entirely gone (see chart above), without impacting nodetool being executed successfully. 1 parallel GC thread should be sufficient for nodetool anyway I think.",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14469,Cassandra Telnet Invalid/Unsupported Protocol,"I am trying to telnet into cassandra using the IP and Port number. I have configured it to for the network . But When I try to telnet into it using PuTTY it displays error 
Invalid or Unsupported Protocol.(197). Supported versions (3/v3, 4/v4 or 5/v5 beta)",N/A,3.11.2
CASSANDRA-14468,"""Unable to parse targets for index"" on upgrade to Cassandra 3.0.10-3.0.16","I am attempting to upgrade from Cassandra 2.2.10 to 3.0.16. I am getting this error:

{code}
org.apache.cassandra.exceptions.ConfigurationException: Unable to parse targets for index idx_foo (""666f6f"")
	at org.apache.cassandra.index.internal.CassandraIndex.parseTarget(CassandraIndex.java:800) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.index.internal.CassandraIndex.indexCfsMetadata(CassandraIndex.java:747) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:645) ~[apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:251) [apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [apache-cassandra-3.0.16.jar:3.0.16]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697) [apache-cassandra-3.0.16.jar:3.0.16]
{code}

It looks like this might be related to CASSANDRA-14104 that was just added to 3.0.16 ",N/A,"3.0.18, 3.11.4"
CASSANDRA-14463,Prevent the generation of new tokens when using replace_address flag,"This is a follow up to/replacement of CASSANDRA-14073.

The behaviour that I want to avoid is someone trying to replace a node with the replace_address flag and mistakenly having that node listed in its own seed list which causes the node to generate a new set of random tokens before joining the ring. 

Currently anytime an unbootstrapped node is listed in its own seed list and initial_token isn't set in the yaml, Cassandra will generate a new set of random tokens and join the ring regardless of whether it was replacing a previous node or not. 

We could simply check for this configuration and refuse to start but I it's probably better (particularly for 3.0.X) if it's handled in the same manner as skipping streaming with the allow_unsafe_replace flag that was introduced in 3.X . This would still allow 3.0.X users the ability to re-bootstrap nodes without needing to re-stream all the data to the node again, which can be useful. 

We currently handle replacing without streaming different;y between 3.0.X and 3.X. In 3.X we have the allow_unsafe_replace JVM flag to allow the use of auto_bootstrap: false in combination with the replace_address option.  But in 3.0.X to perform the replacement of a node with the same IP address without streaming I believe you need to:
 * Set replace_address (because the address is already in gossip)
 * Include the node in its own seed list (to skip bootstrapping/streaming)
 * Set the initial_token to the token/s owned by the previous node (to prevent it generating new tokens.

I believe if 3.0.X simply refused to start when a node has itself in its seed list and replace_address set this will completely block this operation. 

Example patches to fix this edge case using allow_unsafe_replace:

 
||Branch||
|[3.0.x|https://github.com/apache/cassandra/compare/trunk...vincewhite:30-no_clobber]|
|[3.x|https://github.com/apache/cassandra/compare/trunk...vincewhite:311-no_clobber]|",N/A,"3.0.25, 3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-14460,ERROR : java.lang.AssertionError: null,"When I tried to ADD column to a existing table, I am getting below error.

{code}
WARN [MutationStage-48] 2018-02-15 09:42:27,696 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[MutationStage-48,5,main]: {}
 java.lang.AssertionError: null
 at io.netty.util.Recycler$WeakOrderQueue.<init>(Recycler.java:225) ~[netty-all-4.0.39.Final.jar:4.0.39.Final]
 at io.netty.util.Recycler$DefaultHandle.recycle(Recycler.java:180) ~[netty-all-4.0.39.Final.jar:4.0.39.Final]
 at io.netty.util.Recycler.recycle(Recycler.java:141) ~[netty-all-4.0.39.Final.jar:4.0.39.Final]
 at org.apache.cassandra.utils.btree.BTree$Builder.recycle(BTree.java:839) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.utils.btree.BTree$Builder.build(BTree.java:1092) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.partitions.PartitionUpdate.build(PartitionUpdate.java:587) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.partitions.PartitionUpdate.maybeBuild(PartitionUpdate.java:577) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.partitions.PartitionUpdate.holder(PartitionUpdate.java:388) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.partitions.AbstractBTreePartition.unfilteredIterator(AbstractBTreePartition.java:177) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.partitions.AbstractBTreePartition.unfilteredIterator(AbstractBTreePartition.java:172) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:779) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:393) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:249) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:585) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:462) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.Mutation.apply(Mutation.java:232) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1416) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2640) ~[apache-cassandra-3.10.jar:3.10]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.10.jar:3.10]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.10.jar:3.10]
 at java.lang.Thread.run(Thread.java:745) 
 [na:1.8.0_121]
{code}
How to fix this issue? Why does this issue popped up? Any pointers / work around solution is appreciated!",N/A,3.10
CASSANDRA-14451,Infinity ms Commit Log Sync,"Its giving commit log sync warnings where there were apparently zero syncs and therefore gives ""Infinityms"" as the average duration

{code:java}
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:11:14,294 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 74.40ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:16:57,844 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 198.69ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:24:46,325 NoSpamLogger.java:94 - Out of 0 commit log syncs over the past 0.00s with average duration of Infinityms, 1 have exceeded the configured commit interval by an average of 264.11ms 
WARN [PERIODIC-COMMIT-LOG-SYNCER] 2018-05-16 21:29:46,393 NoSpamLogger.java:94 - Out of 32 commit log syncs over the past 268.84s with, average duration of 17.56ms, 1 have exceeded the configured commit interval by an average of 173.66ms{code}",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14450,DelimiterAnalyzer: IllegalArgumentException: The key argument was zero-length,"The [DelimiterAnalyzer|https://issues.apache.org/jira/browse/CASSANDRA-14247] can throw an IllegalArgumentException if there is no text between two delimiters. 

{noformat}
ERROR [MutationStage-1] 2018-05-17 13:55:09,734 StorageProxy.java:1417 - Failed to apply mutation locally : {}
java.lang.RuntimeException: The key argument was zero-length for ks: zipkin2, table: span
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1353) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:626) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:470) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:232) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1411) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2650) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]
Caused by: java.lang.IllegalArgumentException: The key argument was zero-length
	at com.googlecode.concurrenttrees.radix.ConcurrentRadixTree.putInternal(ConcurrentRadixTree.java:520) ~[concurrent-trees-2.4.0.jar:na]
	at com.googlecode.concurrenttrees.radix.ConcurrentRadixTree.putIfAbsent(ConcurrentRadixTree.java:123) ~[concurrent-trees-2.4.0.jar:na]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex$ConcurrentPrefixTrie.putIfAbsent(TrieMemIndex.java:178) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex$ConcurrentTrie.add(TrieMemIndex.java:123) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.TrieMemIndex.add(TrieMemIndex.java:94) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.memory.IndexMemtable.index(IndexMemtable.java:65) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.conf.ColumnIndex.index(ColumnIndex.java:104) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.sasi.SASIIndex$1.insertRow(SASIIndex.java:258) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:915) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:333) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:139) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:121) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:178) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:282) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1335) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
{noformat}",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14447,Cleanup StartupClusterConnectivityChecker and PING Verb,"This is a followup to CASSANDRA-13993. After an internal review, [~iamaleksey] had some concerns wrt to the VERB choices as was committed; this was discussed on CASSANDRA-13993, after commit. Further, he pointed out (and provided) some optimizations for {{StartupClusterConnectivityChecker}} itself. While testing some of the proposed changes, I also discovered a small bug where the timeout for waiting for the response to the PING message was dramtically shorter than the overall timeout that is configured for {{StartupClusterConnectivityChecker}}.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14444,Got NPE when querying Cassandra 3.11.2,"We just upgraded our Cassandra cluster from 2.2.6 to 3.11.2

After upgrading, we immediately got exceptions in Cassandra like this one: 

 
{code}
ERROR [Native-Transport-Requests-1] 2018-05-11 17:10:21,994 QueryMessage.java:129 - Unexpected error during query
java.lang.NullPointerException: null
at org.apache.cassandra.dht.RandomPartitioner.getToken(RandomPartitioner.java:248) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:92) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.config.CFMetaData.decorateKey(CFMetaData.java:666) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.service.pager.PartitionRangeQueryPager.<init>(PartitionRangeQueryPager.java:44) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.db.PartitionRangeReadCommand.getPager(PartitionRangeReadCommand.java:268) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.statements.SelectStatement.getPager(SelectStatement.java:475) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:288) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:118) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.2.jar:3.11.2]
at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.2.jar:3.11.2]
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.2.jar:3.11.2]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]
{code}
 

The table schema is like:
{code}
CREATE TABLE example.example_table (
 id bigint,
 hash text,
 json text,
 PRIMARY KEY (id, hash)
) WITH COMPACT STORAGE
{code}
 

The query is something like:
{code}
""select * from example.example_table;"" // (We do know this is bad practise, and we are trying to fix that right now)
{code}
with fetch-size as 200, using DataStax Java driver. 

This table contains about 20k rows. 

 

Actually, the fix is quite simple, 

 
{code}
--- a/src/java/org/apache/cassandra/service/pager/PagingState.java
+++ b/src/java/org/apache/cassandra/service/pager/PagingState.java
@@ -46,7 +46,7 @@ public class PagingState

public PagingState(ByteBuffer partitionKey, RowMark rowMark, int remaining, int remainingInPartition)
 {
- this.partitionKey = partitionKey;
+ this.partitionKey = partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;
 this.rowMark = rowMark;
 this.remaining = remaining;
 this.remainingInPartition = remainingInPartition;
{code}
 

""partitionKey == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : partitionKey;"" is in 2.2.6 and 2.2.8. But it was removed for some reason. 

The interesting part is that, we have: 
{code}
public final ByteBuffer partitionKey; // Can be null for single partition queries.
{code}
It seems ""partitionKey"" could be null.

Thanks a lot. 

 

 

 ",N/A,3.11.5
CASSANDRA-14441,Materialized view is not deleting/updating data when made changes in base table,"we have seen issue in mat view for 3.11.1 where mat view

1) we have inserted a row in test table and the same recored is in test_mat table, with Enabled = true,
 2) when I update the same record with Enabled = False, a new row is created in test_mat table(one with true and one with false) but in test table original record got updated to FALSE.
 3) when I delete the record using Feature UUID then only the record with Fales is getting deleted in both the tables. however I can see the TRUE record in test_mat table.

Issue is not reproducible in 3.11.2
 Steps

CREATE TABLE test ( 
 feature_uuid uuid, 
 namespace text, 
 feature_name text, 
 allocation_type text, 
 description text, 
 enabled boolean, 
 expiration_dt timestamp, 
 last_modified_dt timestamp, 
 last_modified_user text, 
 persist_allocations boolean, 
 rule text, 
 PRIMARY KEY (feature_uuid, namespace, feature_name, allocation_type) 
 ) WITH CLUSTERING ORDER BY (namespace ASC, feature_name ASC, allocation_type ASC) 
 AND bloom_filter_fp_chance = 0.01 
 AND caching = \{'keys': 'ALL', 'rows_per_partition': 'NONE'} 
 AND comment = '' 
 AND compaction = \{'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'} 
 AND compression = \{'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'} 
 AND crc_check_chance = 1.0 
 AND dclocal_read_repair_chance = 0.3 
 AND default_time_to_live = 63072000 
 AND gc_grace_seconds = 864000 
 AND max_index_interval = 2048 
 AND memtable_flush_period_in_ms = 0 
 AND min_index_interval = 128 
 AND read_repair_chance = 0.3 
 AND speculative_retry = '99PERCENTILE';

CREATE MATERIALIZED VIEW test_mat AS 
 SELECT allocation_type, enabled, feature_uuid, namespace, feature_name, last_modified_dt, last_modified_user, persist_allocations, rule 
 FROM test
 WHERE feature_uuid IS NOT NULL AND allocation_type IS NOT NULL AND namespace IS NOT NULL AND feature_name IS NOT NULL AND enabled IS NOT NULL 
 PRIMARY KEY (allocation_type, enabled, feature_uuid, namespace, feature_name) 
 WITH CLUSTERING ORDER BY (enabled ASC, feature_uuid ASC, namespace ASC, feature_name ASC) 
 AND bloom_filter_fp_chance = 0.01 
 AND caching = \{'keys': 'ALL', 'rows_per_partition': 'NONE'} 
 AND comment = '' 
 AND compaction = \{'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'} 
 AND compression = \{'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'} 
 AND crc_check_chance = 1.0 
 AND dclocal_read_repair_chance = 0.1 
 AND default_time_to_live = 0 
 AND gc_grace_seconds = 864000 
 AND max_index_interval = 2048 
 AND memtable_flush_period_in_ms = 0 
 AND min_index_interval = 128 
 AND read_repair_chance = 0.0 
 AND speculative_retry = '99PERCENTILE';

INSERT INTO test (feature_uuid, namespace, feature_name, allocation_type, description, enabled, expiration_dt, last_modified_dt, last_modified_user, persist_allocations,rule) VALUES (uuid(),'Service','NEW','preallocation','20newproduct',TRUE,'2019-10-02 05:05:05 -0500','2018-08-03 06:06:06 -0500','swapnil',TRUE,'NEW'); 
 UPDATE test SET enabled=FALSE WHERE feature_uuid=b2d5c245-e30e-4ea8-8609-d36b627dbb2a and namespace='Service' and feature_name='NEW' and allocation_type='preallocation' IF EXISTS ; 
 Delete from test where feature_uuid=98e6ebcc-cafd-4889-bf3d-774a746a3298;

 
 ",N/A,3.11.5
CASSANDRA-14438,Issue with FailureDetector.java:456 in the debug.log,"Team,

 

we are doing POC on open source cassandra for one of our critical application . we do see the below error hitting every second in debug.log , could you please help us to over come the error message .

Component : Apache cassandra 

Version : Cassandra 3.0.14

 

please let us if you need any additional information . 

 

DEBUG [GossipStage:1] 2018-05-07 11:37:37,645 FailureDetector.java:456 - Ignoring interval time of 3121506694 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:37,646 FailureDetector.java:456 - Ignoring interval time of 3043323658 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:37,646 FailureDetector.java:456 - Ignoring interval time of 2793436516 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:37,853 FailureDetector.java:456 - Ignoring interval time of 3000475949 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:37,853 FailureDetector.java:456 - Ignoring interval time of 3000661616 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:37,853 FailureDetector.java:456 - Ignoring interval time of 2000404942 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:38,372 FailureDetector.java:456 - Ignoring interval time of 2006601883 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:38,372 FailureDetector.java:456 - Ignoring interval time of 3520254134 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:38,373 FailureDetector.java:456 - Ignoring interval time of 2006610253 for /hostname
DEBUG [GossipStage:1] 2018-05-07 11:37:38,373 FailureDetector.java:456 - Ignoring interval time of 2006711620 for /hostname

 

Thanks,

RK.",N/A,3.0.17
CASSANDRA-14428,Run ant eclipse-warnings in circleci,We should run ant eclipse-warnings in circle-ci,N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14424,Gossip EchoMessages not being handled somewhere after node restart,"Noticing this behaviour on a brand new 3.11.2 ring:
 # Restart a random node in the ring.
 # When that node comes back up, around 30% of the time it sees a single other node down. No other node in the ring sees that node is down.
 # After 10-20 minutes, the DOWN node suddenly appears UP to the restarted node.

 

After digging through tracing logs, here's what I know:

 

The node seen as DOWN has not gone down, but simply hasn't been seen as UP yet. The restarted node is attempting to `markAlive()` the target node. Relevant logs from the restarted node's POV:

 

{{INFO [GossipStage:1] 2018-04-27 14:03:50,950 Gossiper.java:1053 - Node /10.0.225.147 has restarted, now UP}}
 {{INFO [GossipStage:1] 2018-04-27 14:03:50,969 StorageService.java:2292 - Node /10.0.225.147 state jump to NORMAL}}
 {{INFO [HANDSHAKE-/10.0.225.147] 2018-04-27 14:03:50,976 OutboundTcpConnection.java:560 - Handshaking version with /10.0.225.147}}
 {{INFO [GossipStage:1] 2018-04-27 14:03:50,977 TokenMetadata.java:479 - Updating topology for /10.0.225.147}}
 {{INFO [GossipStage:1] 2018-04-27 14:03:50,977 TokenMetadata.java:479 - Updating topology for /10.0.225.147}}

 

(note that despite the Gossip seeing the DOWN node as 'UP', nodetool status still shows it as 'DOWN', as markAlive has not completed, and will not actually be seen as 'UP' for 20 more minutes)

 

The restarted node is repeatedly sending Echo messages to the DOWN node as part of the `markAlive()` call. The DOWN node is receiving those, and claims to be sending a response. However, the restarted node is not marking the DOWN node as UP even after the DOWN node sends the Echo response.

 

Relevant logs from the restarted node's POV:

 

{{TRACE [GossipStage:1] 2018-04-27 14:11:28,792 MessagingService.java:945 - 10.0.103.45 sending ECHO to 99248@/10.0.225.147}}
{{TRACE [GossipTasks:1] 2018-04-27 14:11:29,792 MessagingService.java:945 - 10.0.103.45 sending GOSSIP_DIGEST_SYN to 99631@/10.0.225.147}}
{{TRACE [GossipStage:1] 2018-04-27 14:11:29,792 MessagingService.java:945 - 10.0.103.45 sending ECHO to 99632@/10.0.225.147}}
{{TRACE [GossipStage:1] 2018-04-27 14:11:29,793 MessagingService.java:945 - 10.0.103.45 sending GOSSIP_DIGEST_ACK2 to 99633@/10.0.225.147}}
{{TRACE [GossipStage:1] 2018-04-27 14:11:29,793 MessagingService.java:945 - 10.0.103.45 sending ECHO to 99635@/10.0.225.147}}
{{TRACE [GossipStage:1] 2018-04-27 14:11:31,794 MessagingService.java:945 - 10.0.103.45 sending ECHO to 100348@/10.0.225.147}}
{{TRACE [GossipStage:1] 2018-04-27 14:11:33,750 MessagingService.java:945 - 10.0.103.45 sending ECHO to 101157@/10.0.225.147}}
{{TRACE [GossipStage:1] 2018-04-27 14:11:35,412 MessagingService.java:945 - 10.0.103.45 sending ECHO to 101753@/10.0.225.147}}

 

 

Relevant logs from the DOWN node's POV:

 

{{TRACE [GossipStage:1] 2018-04-27 14:18:16,500 EchoVerbHandler.java:39 - Sending a EchoMessage reply /10.0.103.45}}
 {{TRACE [GossipStage:1] 2018-04-27 14:18:16,500 MessagingService.java:945 - 10.0.225.147 sending REQUEST_RESPONSE to 328389@/10.0.103.45}}
{{TRACE [GossipStage:1] 2018-04-27 14:18:17,679 EchoVerbHandler.java:39 - Sending a EchoMessage reply /10.0.103.45}}
 {{TRACE [GossipStage:1] 2018-04-27 14:18:17,679 MessagingService.java:945 - 10.0.225.147 sending REQUEST_RESPONSE to 329412@/10.0.103.45}}
{{TRACE [GossipStage:1] 2018-04-27 14:18:18,680 EchoVerbHandler.java:39 - Sending a EchoMessage reply /10.0.103.45}}
 {{TRACE [GossipStage:1] 2018-04-27 14:18:18,680 MessagingService.java:945 - 10.0.225.147 sending REQUEST_RESPONSE to 330185@/10.0.103.45}}

 

 

The metrics on the restarted node show that the MessagingService has a large number of TimeoutsPerHost for the DOWN node, and all other nodes have 0 timeouts.

 

 

Eventually, `realMarkAlive()` is called and the restarted node finally sees DOWN node as coming up, and it spams several UP messages when this happens:

 

 

{{INFO [RequestResponseStage-7] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-11] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-11] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-11] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-11] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-11] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-12] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-11] 2018-04-27 14:19:27,210 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}
 {{INFO [RequestResponseStage-12] 2018-04-27 14:19:27,211 Gossiper.java:1019 - InetAddress /10.0.225.147 is now UP}}

 

 

 

A tcpdump shows no packet loss or other oddities between these two hosts. The restarted node is sending Echo messages, and the DOWN node is ACKing them.

 

The fact that a burst comes through at the very end suggests to me that perhaps the Echo messages are getting queued up somewhere.

 

The issue happens roughly 30% of the time a given node in the ring is restarted.
",N/A,"3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-14423,SSTables stop being compacted,"So seeing a problem in 3.11.0 where SSTables are being lost from the view and not being included in compactions/as candidates for compaction. It seems to get progressively worse until there's only 1-2 SSTables in the view which happen to be the most recent SSTables and thus compactions completely stop for that table.

The SSTables seem to still be included in reads, just not compactions.

The issue can be fixed by restarting C*, as it will reload all SSTables into the view, but this is only a temporary fix. User defined/major compactions still work - not clear if they include the result back in the view but is not a good work around.

This also results in a discrepancy between SSTable count and SSTables in levels for any table using LCS.
{code:java}
Keyspace : xxx
Read Count: 57761088
Read Latency: 0.10527088681224288 ms.
Write Count: 2513164
Write Latency: 0.018211106398149903 ms.
Pending Flushes: 0
Table: xxx
SSTable count: 10
SSTables in each level: [2, 0, 0, 0, 0, 0, 0, 0, 0]
Space used (live): 894498746
Space used (total): 894498746
Space used by snapshots (total): 0
Off heap memory used (total): 11576197
SSTable Compression Ratio: 0.6956629530569777
Number of keys (estimate): 3562207
Memtable cell count: 0
Memtable data size: 0
Memtable off heap memory used: 0
Memtable switch count: 87
Local read count: 57761088
Local read latency: 0.108 ms
Local write count: 2513164
Local write latency: NaN ms
Pending flushes: 0
Percent repaired: 86.33
Bloom filter false positives: 43
Bloom filter false ratio: 0.00000
Bloom filter space used: 8046104
Bloom filter off heap memory used: 8046024
Index summary off heap memory used: 3449005
Compression metadata off heap memory used: 81168
Compacted partition minimum bytes: 104
Compacted partition maximum bytes: 5722
Compacted partition mean bytes: 175
Average live cells per slice (last five minutes): 1.0
Maximum live cells per slice (last five minutes): 1
Average tombstones per slice (last five minutes): 1.0
Maximum tombstones per slice (last five minutes): 1
Dropped Mutations: 0
{code}
Also for STCS we've confirmed that SSTable count will be different to the number of SSTables reported in the Compaction Bucket's. In the below example there's only 3 SSTables in a single bucket - no more are listed for this table. Compaction thresholds haven't been modified for this table and it's a very basic KV schema.
{code:java}
Keyspace : yyy
    Read Count: 30485
    Read Latency: 0.06708991307200263 ms.
    Write Count: 57044
    Write Latency: 0.02204061776873992 ms.
    Pending Flushes: 0
        Table: yyy
        SSTable count: 19
        Space used (live): 18195482
        Space used (total): 18195482
        Space used by snapshots (total): 0
        Off heap memory used (total): 747376
        SSTable Compression Ratio: 0.7607394576769735
        Number of keys (estimate): 116074
        Memtable cell count: 0
        Memtable data size: 0
        Memtable off heap memory used: 0
        Memtable switch count: 39
        Local read count: 30485
        Local read latency: NaN ms
        Local write count: 57044
        Local write latency: NaN ms
        Pending flushes: 0
        Percent repaired: 79.76
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 690912
        Bloom filter off heap memory used: 690760
        Index summary off heap memory used: 54736
        Compression metadata off heap memory used: 1880
        Compacted partition minimum bytes: 73
        Compacted partition maximum bytes: 124
        Compacted partition mean bytes: 96
        Average live cells per slice (last five minutes): NaN
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): NaN
        Maximum tombstones per slice (last five minutes): 0
        Dropped Mutations: 0 
{code}
{code:java}
Apr 27 03:10:39 cassandra[9263]: TRACE o.a.c.d.c.SizeTieredCompactionStrategy Compaction buckets are [[BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67168-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67167-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67166-big-Data.db')]]
{code}
Also for every LCS table we're seeing the following warning being spammed (seems to be in line with anticompaction spam):
{code:java}
Apr 26 21:30:09 cassandra[9263]: WARN  o.a.c.d.c.LeveledCompactionStrategy Live sstable /var/lib/cassandra/data/xxx/xxx-8c3ef9e0e3fc11e6868a8fd39a64fd59/mc-79024-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.{code}
This is a vnodes cluster with 256 tokens per node, and the only thing that seems like it could be causing issues is anticompactions.

CASSANDRA-14079 might be related but doesn't quite describe the same issue, and in this case we're using only a single disk for data. Have yet to reproduce but figured worth reporting here first.",N/A,"2.2.13, 3.0.17, 3.11.3"
CASSANDRA-14422,Missing dependencies airline and ohc-core-j8 for pom-all,"I found two missing dependencies for pom-all (cassandra-all):
 * airline
 * ohc-core-j8

 

This doesn't affect current build scheme because their jars are hardcoded in the lib directory. However, if we depend on cassandra-all in our downstream projects to resolve and fetch dependencies (instead of using the official tarball), Cassandra will have problems, e.g. airline is required by nodetool, and it will fail our dtests.

I will attach the patch shortly",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14419,Resume compresed hints delivery broken,"We are using Cassandra 3.0.15 and are using compressed hints, but if hint delivery is interrupted resuming hint delivery is failing.

{code}

2018-04-04T13:27:48.948+0200 ERROR [HintsDispatcher:14] CassandraDaemon.java:207 Exception in thread Thread[HintsDispatcher:14,1,main]
java.lang.IllegalArgumentException: Unable to seek to position 1789149057 in /var/lib/cassandra/hints/9592c860-1054-4c60-b3b8-faa9adc6d769-1522838912649-1.hints (118259682 bytes) in read-only mode
        at org.apache.cassandra.io.util.RandomAccessReader.seek(RandomAccessReader.java:287) ~[apache-cassandra-clientutil-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsReader.seek(HintsReader.java:114) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatcher.seek(HintsDispatcher.java:83) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:263) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:248) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:226) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:205) ~[apache-cassandra-3.0.15.jar:3.0.15]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_152]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_152]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_152]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_152]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [apache-cassandra-3.0.15.jar:3.0.15]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_152]

{code}

 I think the problem is similar to CASSANDRA-11960.",N/A,3.0.17
CASSANDRA-14418,Cassandra not starting when using enhanced startup scripts in windows,"I am using Apache Cassandra 3.11.2 with my application. 

My application is getting installed under C:/Program Files/My Application/Some Folder/.

And cassandra C:/Program Files/My Application/Some Folder/cassandra.

So when I am using enhanced startup scripts cassandra not getting up and running and I am getting below error:

""Error: Could not find or load main class Files\My""

One of the solution I got is moving cassandra to another location where location path does not contain spaces. But this is not good way of getting this problem resolved.

After doing detailed analysis of all the scripts I found the solution below:

Inside file cassandra-env.ps1 at line number 380:

Replace line:

$env:JVM_OPTS = ""$env:JVM_OPTS -XX:CompileCommandFile=$env:CASSANDRA_CONF\hotspot_compiler""

with line

$env:JVM_OPTS = ""$env:JVM_OPTS -XX:CompileCommandFile=""""$env:CASSANDRA_CONF\hotspot_compiler""""""

Fix here is the double quotes added before $env:CASSANDRA_CONF and at the end.

At other places this case is well handled. But missed at this place.

 ",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14416,Remove string formatting lines from hot path,"In the BufferPool, we have a {{NoSpamLogger}} that gets called in some conditions (like when we're at or near the cap in a given pool), which has some string formatting on it. In situations where that log line is invoked, that string formatting gets called before the rate limiting, which becomes expensive. ",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14415,Performance regression in queries for distinct keys,"Running Cassandra 3.0.16, we observed a major performance regression affecting {{SELECT DISTINCT keys}}-style queries against certain tables.  Based on some investigation (guided by some helpful feedback from Benjamin on the dev list), we tracked the regression down to two problems.
 * One is that Cassandra was reading more data from disk than was necessary to satisfy the query.  This was fixed under CASSANDRA-10657 in a later 3.x release.
 * If the fix for CASSANDRA-10657 is incorporated, the other is this code snippet in {{RebufferingInputStream}}:
{code:java}
    @Override
    public int skipBytes(int n) throws IOException
    {
        if (n < 0)
            return 0;
        int requested = n;
        int position = buffer.position(), limit = buffer.limit(), remaining;
        while ((remaining = limit - position) < n)
        {
            n -= remaining;
            buffer.position(limit);
            reBuffer();
            position = buffer.position();
            limit = buffer.limit();
            if (position == limit)
                return requested - n;
        }
        buffer.position(position + n);
        return requested;
    }
{code}
The gist of it is that to skip bytes, the stream needs to read those bytes into memory then throw them away.  In our tests, we were spending a lot of time in this method, so it looked like the chief drag on performance.

We noticed that the subclass of {{RebufferingInputStream}} in use for our queries, {{RandomAccessReader}} (over compressed sstables), implements a {{seek()}} method.  Overriding {{skipBytes()}} in it to use {{seek()}} instead was sufficient to fix the performance regression.

The performance difference is significant for tables with large values.  It's straightforward to evaluate with very simple key-value tables, e.g.:

{{CREATE TABLE testtable (key TEXT PRIMARY KEY, value BLOB);}}

We did some basic experimentation with the following variations (all in a single-node 3.11.2 cluster with off-the-shelf settings running on a dev workstation):
 * small values (1 KB, 100,000 entries), somewhat larger values (25 KB, 10,000 entries), and much larger values (1 MB, 10,000 entries);
 * compressible data (a single byte repeated) and uncompressible data (output from {{openssl rand $bytes}}); and
 * with and without sstable compression.  (With compression, we use Cassandra's defaults.)

The difference is most conspicuous for tables with large, uncompressible data and sstable decompression (which happens to describe the use case that triggered our investigation).  It is smaller but still readily apparent for tables with effective compression.  For uncompressible data without compression enabled, there is no appreciable difference.

Here's what the performance looks like without our patch for the 1-MB entries (times in seconds, five consecutive runs for each data set, all exhausting the results from a {{SELECT DISTINCT key FROM ...}} query with a page size of 24):
{noformat}
working on compressible
5.21180510521
5.10270500183
5.22311806679
4.6732840538
4.84219098091
working on uncompressible_uncompressed
55.0423607826
0.769015073776
0.850513935089
0.713396072388
0.62596988678
working on uncompressible
413.292617083
231.345913887
449.524993896
425.135111094
243.469946861
{noformat}
and with the fix:
{noformat}
working on compressible
2.86733293533
1.24895811081
1.108907938
1.12742400169
1.04647302628
working on uncompressible_uncompressed
56.4146180153
0.895509958267
0.922824144363
0.772884130478
0.731923818588
working on uncompressible
64.4587619305
1.81325793266
1.52577018738
1.41769099236
1.60442209244
{noformat}
The long initial runs for the uncompressible data presumably come from repeatedly hitting the disk.  In contrast to the runs without the fix, the initial runs seem to be effective at warming the page cache (as lots of data is skipped, so the data that's read can fit in memory), so subsequent runs are faster.

For smaller data sets, {{RandomAccessReader.seek()}} and {{RebufferingInputStream.skipBytes()}} are approximately equivalent in their behavior (reducing to changing the position pointer of an in-memory buffer most of the time), so there isn't much difference.  Here's before the fix for the 1-KB entries:
{noformat}
working on small_compressible
8.34115099907
8.57280993462
8.3534219265
8.55130696297
8.17362189293
working on small_uncompressible_uncompressed
7.85155582428
7.54075288773
7.50106596947
7.39202189445
7.95735621452
working on small_uncompressible
7.89256501198
7.88875198364
7.9013261795
7.76551413536
7.84927678108
{noformat}
and after:
{noformat}
working on small_compressible
8.29225707054
7.57822394371
8.10092878342
8.21332192421
8.19347810745
working on small_uncompressible_uncompressed
7.74823594093
7.81218004227
7.68660092354
7.95432114601
7.77612304688
working on small_uncompressible
8.18260502815
8.21010804176
8.1233921051
7.31543707848
7.91079998016
{noformat}
The effect is similar for the 25-KB entries, which might enjoy a slight performance benefit from the patch (perhaps because they're larger than the default buffer size defined in {{RandomAccessReader}}).  Before:
{noformat}
working on medium_compressible
0.988080978394
1.02464294434
0.977658033371
1.02553391457
0.769363880157
working on medium_uncompressible_uncompressed
1.07718396187
1.08547902107
1.12398791313
1.10300898552
1.08757281303
working on medium_uncompressible
0.940990209579
0.917474985123
0.768013954163
0.871683835983
0.814841985703
{noformat}
and after:
{noformat}
working on medium_compressible
0.829009056091
0.705173015594
0.603646993637
0.820069074631
0.873830080032
working on medium_uncompressible_uncompressed
0.785156965256
0.808106184006
0.848286151886
0.857885837555
0.825689077377
working on medium_uncompressible
0.845101118088
0.913790941238
0.824147939682
0.849114894867
0.85981798172
{noformat}
In short, this looks like a pretty straightforward performance win with negligible cost.  (It's worth noting that for our use case, disabling sstable compression is clearly the _best_ solution, but there's still reasonably clear benefit from this minor fix for data sets with larger, compressible values, as well as presumably data sets with a mix of compressible and uncompressible values in environments where storage is limited.)",N/A,"3.11.11, 4.0.1"
CASSANDRA-14411,Use Bounds instead of Range to represent sstable first/last token when checking how to anticompact sstables,"There is currently a chance of missing marking a token as repaired due to the fact that we use Range which are (a, b] to represent first/last token in sstables instead of Bounds which are [a, b].",N/A,"2.2.13, 3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14399,unable to start cassandra in version 3.11.21 and getting eror too many files,"unable to start Cassandra on three node cluster and getting errors in error log as Too Many files open. Getting the error as ERROR [SSTableBatchOpen:1] 2018-04-18 17:57:16,617 JVMStabilityInspector.java:162 - JVM state determined to be unstable.  Exiting forcefully due to:
java.io.FileNotFoundException: /fs2/cassandra/data/system_auth/role_permissions-3afbe79f219431a7add7f5ab90d8ec9c/mc-38-big-CompressionInfo.db (Too many open files)",N/A,3.11.2
CASSANDRA-14396,Error about JNA on Startup ," 

Hi, all.

I got some error on startup.

this is my own backup server which can't use network.

I just extracted 'apache-cassandra-3.11.2-bin.tar.gz' file to /usr/local/cassandra.

and then ran like this 'cassandra -f'.

but log displayed below's error.

 

I found some way to solve. but it's not working.

after JNA library download, make JNA library symbolic link - not working

can anyone advise to me about this issue?(I attached full logging file)

 

ERROR [main] 2018-04-18 10:44:59,536 NativeLibraryLinux.java:62 - Failed to link the C library against JNA. Native methods will be unavailable.
 java.lang.UnsatisfiedLinkError: /tmp/jna-3506402/jna5682737284440877593.tmp: /tmp/jna-3506402/jna5682737284440877593.tmp: failed to map segment from shared object: Operation not permitted
 at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[na:1.8.0_152]
 at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[na:1.8.0_152]
 at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[na:1.8.0_152]
 at java.lang.Runtime.load0(Runtime.java:809) ~[na:1.8.0_152]
 at java.lang.System.load(System.java:1086) ~[na:1.8.0_152]
 at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:851) ~[jna-4.2.2.jar:4.2.2 (b0)]
 at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:826) ~[jna-4.2.2.jar:4.2.2 (b0)]
 at com.sun.jna.Native.<clinit>(Native.java:140) ~[jna-4.2.2.jar:4.2.2 (b0)]
 at org.apache.cassandra.utils.NativeLibraryLinux.<clinit>(NativeLibraryLinux.java:53) ~[apache-cassandra-3.11.2.jar:3.11.2]
 at org.apache.cassandra.utils.NativeLibrary.<clinit>(NativeLibrary.java:93) [apache-cassandra-3.11.2.jar:3.11.2]
 at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:196) [apache-cassandra-3.11.2.jar:3.11.2]
 at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:602) [apache-cassandra-3.11.2.jar:3.11.2]
 at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:691) [apache-cassandra-3.11.2.jar:3.11.2]
 WARN [main] 2018-04-18 10:44:59,537 StartupChecks.java:136 - jemalloc shared library could not be preloaded to speed up memory allocations
 WARN [main] 2018-04-18 10:44:59,537 StartupChecks.java:169 - JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.
 ERROR [main] 2018-04-18 10:44:59,539 CassandraDaemon.java:708 - The native library could not be initialized properly.",N/A,3.11.10
CASSANDRA-14387,SSTableReaderTest#testOpeningSSTable fails on macOS,"I ran into an issue with {{SSTableReaderTest#testOpeningSSTable}} test failure on macOS. The reason for failure seems that on macOS, the file modification timestamps are at a second granularity (See: https://stackoverflow.com/questions/18403588/how-to-return-millisecond-information-for-file-access-on-mac-os-x-in-java and https://developer.apple.com/legacy/library/technotes/tn/tn1150.html#HFSPlusDates). The fix is simple - bumping up the sleep time to 1 second instead of 10ms.


{noformat}
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testOpeningSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	FAILED
    [junit] Bloomfilter was not recreated
    [junit] junit.framework.AssertionFailedError: Bloomfilter was not recreated
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testOpeningSSTable(SSTableReaderTest.java:421)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.io.sstable.SSTableReaderTest FAILED
{noformat}

Related issue: CASSANDRA-11163",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14384,If fsync fails it's always an issue and continuing execution is suspect,"We can't catch fsync errors and continue so we shouldn't have code that does that in C*. There was a Postgres bug where fsync returned an error and the FS lost data, but subsequent fsyncs succeeded.

The [LastErrorException code in NativeLibrary.trySync|https://github.com/apache/cassandra/commit/be313935e54be450d9aaabda7965a2f266e922c9#diff-4258621cdf765f0fea6770db5d40038fR307] looks a little janky. What's up with that? When would trySync be something we would merely try? If try is good enough why do it at all considering try is the default behavior of a series of unsynced filesystem operations.

Also when we fsync in FD it's not just fsyncing that file the FS is potentially fsyncing other data and the error code we get could be related to that other data so we can't safely ignore it. The filesystem could be internally inconsistent as well. This happens because the FS journaling may force the FS to flush other data as well to preserve the ordering requirements of journaled metadata.

If we ignore fsync errors it needs to be for whitelisted reasons such as a bad FD.

I know we have FSErrorHandler and it makes sense for reads, but I'm not sold on it being the right answer for writes. We don't retry flushing a memtable or writing to the commit log to my knowledge. We could go read only and I need to check if that is w",N/A,"2.1.x, 3.0.17, 3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-14379,Better handling of missing partition columns in system_schema.columns during startup,"Follow up for CASSANDRA-13180, during table deletion/creation, we saw one table having partially deleted columns (no partition column, only regular column). It's blocking node from startup:
{noformat}
java.lang.AssertionError: null
        at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:103) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.config.CFMetaData.rebuild(CFMetaData.java:308) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:288) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.config.CFMetaData.create(CFMetaData.java:363) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchTable(SchemaKeyspace.java:1028) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchTables(SchemaKeyspace.java:987) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:945) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:922) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:910) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:138) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:128) ~[apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:241) [apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:567) [apache-cassandra-3.0.14.x.jar:3.0.14.x]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:695) [apache-cassandra-3.0.14.x.jar:3.0.14.x]
{noformat}

As partition column is mandatory, it should throw [{{MissingColumns}}|https://github.com/apache/cassandra/blob/60563f4e8910fb59af141fd24f1fc1f98f34f705/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L1351], the same as CASSANDRA-13180, so the user is able to cleanup the schema.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14377,Returning invalid JSON for NaN and Infinity float values,"After inserting special float values like NaN and Infinity into a table:

{{CREATE TABLE testme (t1 bigint, t2 float, t3 float, PRIMARY KEY (t1));}}
{{INSERT INTO testme (t1, t2, t3) VALUES (7, NaN, Infinity);}}

and returning them as JSON...

{{cqlsh:demodb> select json * from testme;}}
{{ [json]}}
{{--------------------------------------}}
{{ \{""t1"": 7, ""t2"": NaN, ""t3"": Infinity}}}

 

... the result will not be validated (e.g. with [https://jsonlint.com/|https://jsonlint.com/)] ) because neither NaN nor Infinity is a valid JSON value. The consensus seems to be returning JSON's `null` in these cases, based on this article [https://stackoverflow.com/questions/1423081/json-left-out-infinity-and-nan-json-status-in-ecmascript] and other similar ones.",N/A,"2.2.14, 3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14370,Reduce level of log from debug to trace in CommitLogSegmentManager.java,"[{{AbstractCommitLogSegmentManager.java:112}}|https://github.com/apache/cassandra/blob/2402acd47e3bb514981cde742b7330666c564869/src/java/org/apache/cassandra/db/commitlog/AbstractCommitLogSegmentManager.java#L112]
It's changed to trace() in cassandra-3.0 with CASSANDRA-10241:https://github.com/pauloricardomg/cassandra/commit/3ef1b18fa76dce7cd65b73977fc30e51301f3fed#diff-d07279710c482983e537aed26df80400

but not in cassandra-3.11 and trunk. I think it makes sense to make them consistent and downgrade to {{trace()}}.",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14365,Commit log replay failure for static columns with collections in clustering keys,"In the old storage engine, static cells with a collection as part of the clustering key fail to validate because a 0 byte collection (like in the cell name of a static cell) isn't valid.

To reproduce:

1.
{code:java}
CREATE TABLE test.x (
    id int,
    id2 frozen<map<text, text>>,
    st int static,
    PRIMARY KEY (id, id2)
);

INSERT INTO test.x (id, st) VALUES (1, 2);
{code}
2.
 Kill the cassandra process

3.
 Restart cassandra to replay the commitlog

Outcome:
{noformat}
ERROR [main] 2018-04-05 04:58:23,741 JVMStabilityInspector.java:99 - Exiting due to error while processing commit log during initialization.
org.apache.cassandra.db.commitlog.CommitLogReplayer$CommitLogReplayException: Unexpected error deserializing mutation; saved to /tmp/mutation3825739904516830950dat.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception follows: org.apache.cassandra.serializers.MarshalException: Not enough bytes to read a set
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.handleReplayError(CommitLogReplayer.java:638) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.replayMutation(CommitLogReplayer.java:565) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.replaySyncSection(CommitLogReplayer.java:517) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:397) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:143) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:181) [main/:na]
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:161) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:284) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:533) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:642) [main/:na]


{noformat}
I haven't investigated if there are other more subtle issues caused by these cells failing to validate other places in the code, but I believe the fix for this is to check for 0 byte length collections and accept them as valid as we do with other types.

I haven't had a chance for any extensive testing but this naive patch seems to have the desired affect. 


||Patch||
|[2.2 PoC|https://github.com/vincewhite/cassandra/commits/zero_length_collection]|
",N/A,"2.2.17, 3.0.21, 3.11.7, 4.0-alpha4, 4.0"
CASSANDRA-14359,"CREATE TABLE fails if there is a column called ""default"" with Cassandra 3.11.2","My project is upgrading from Cassandra 2.1 to 3.11. We have a table whose column name is ""default"". The Cassandra 3.11.2 is rejecting it. I don't see ""default"" as a keyword in the CQL spec. 

To reproduce, try adding the following:
{code:java}
CREATE TABLE simple (
    simplekey text PRIMARY KEY,
    default text // THIS IS REJECTED
);
{code}
I get this error:
{code:java}
SyntaxException: line 3:4 mismatched input 'default' expecting ')' (...    simplekey text PRIMARY KEY,    [default]...)
{code}",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14349,Untracked CDC segment files are not deleted after replay,"When CDC is enabled, a hard link to each commit log file will be created in cdc_raw directory. Those commit logs with CDC mutations will also have cdc index files created along with the hard links; these are intended for the consumer to handle and clean them up.

However, if we don't produce any CDC traffic, those hard links in cdc_raw will be never cleaned up (because hard links will still be created, without the index files), whereas the real original commit logs are correctly deleted after replay during process startup. This will results in many untracked hard links in cdc_raw if we restart the cassandra process many times. I am able to use CCM to reproduce it in trunk version which has the CASSANDRA-12148 changes.

This seems a bug in handleReplayedSegment of the commit log segment manager which neglects to take care of CDC commit logs. I will attach a patch here.",N/A,"3.11.10, 4.0-rc2"
CASSANDRA-14332,Fix unbounded validation compactions on repair,"After CASSANDRA-13797 it's possible to cause unbounded, simultaneous validation compactions as we no longer wait for validations to finish. Potential fix is to have a sane default for the # of concurrent validation compactions by backporting CASSANDRA-13521 and setting a sane default.",N/A,"3.0.17, 3.11.3"
CASSANDRA-14330,Handle repeat open bound from SRP in read repair,"If there is an open range tombstone in an iterator, a short read protection request for it will include a repeat open bound. Currently, {{DataResolver}} doesn't expect this, and will raise an assertion, timing out the request:
{code:java}
java.lang.AssertionError: Error merging RTs on test.test: merged=null, versions=[Marker EXCL_START_BOUND(0)@0, null], sources={[/127.0.0.1, /127.0.0.2]}, responses:
    /127.0.0.1 => [test.test] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
       Row[info=[ts=1] ]: ck=0 | ,
   /127.0.0.2 => [test.test] key=0 partition_deletion=deletedAt=-9223372036854775808, localDeletion=2147483647 columns=[[] | []]
       Row[info=[ts=-9223372036854775808] del=deletedAt=1, localDeletion=1521572669 ]: ck=0 |
       Row[info=[ts=1] ]: ck=1 | 
{code}
As this is a completely normal/common scenario, we should allow for this, and relax the assertion.

Additionally, the linked branch makes the re-throwing {{AssertionError}} more detailed and more correct: the responses are now printed out in the correct order, respecting {{isReversed}}, the command causing the assertion is now logged, as is {{isReversed}} itself, and local deletion times for RTs.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14325,Java executable check succeeds despite no java on PATH,"The check -z $JAVA on line 102 of bin/cassandra currently always succeeds if JAVA_HOME is not set since in this case JAVA gets set directly to 'java'. The error message ""_Unable to find java executable. Check JAVA_HOME and PATH environment variables._"" will never be echoed on a PATH misconfiguration. If java isn't on the PATH the failure will instead occur on line 95 of cassandra-env.sh at the java version check.

It would be better to check consistently for the java executable in one place in bin/cassandra. Also we don't want users to mistakenly think they have a java version problem when they in fact have a PATH problem.

See proposed patch.",N/A,"3.0.25, 3.11.11, 4.0.1"
CASSANDRA-14319,nodetool rebuild from DC lets you pass invalid datacenters ,"If you pass an invalid datacenter to nodetool rebuild, you'll get an error like this:

{code}
Unable to find sufficient sources for streaming range (3074457345618258602,-9223372036854775808] in keyspace system_distributed
{code}

Unfortunately, this is a rabbit hole of frustration if you are using caps for your DC names and you pass in a lowercase DC name, or you just typo the DC.  

Let's do the following:

# Check the DC name that's passed in against the list of DCs we know about
# If we don't find it, let's output a reasonable error, and list all the DCs someone could put in.
# Ideally we indicate which keyspaces are set to replicate to this DC and which aren't",N/A,"3.0.29, 3.11.16, 4.0.10, 4.1.2, 5.0-alpha1, 5.0"
CASSANDRA-14310,Don't allow nodetool refresh before cfs is opened,There is a potential deadlock in during startup if nodetool refresh is called while sstables are being opened. We should not allow refresh to be called before everything is initialized.,N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14306,Single config variable to specify logs path,"Motivation: All configuration should take place in bin/cassandra.in.sh (for non-Windows) and the various conf/ files. In particular, bin/cassandra should not need to be modified upon installation. In many installs, $CASSANDRA_HOME is not a writable location, the yaml setting 'data_file_directories' is being set to a non-default location, etc. It would be good to have a single variable in an explicit conf file to specify where logs should be written.

For non-Windows installs, there are currently two places where the log directory is set: in conf/cassandra-env.sh and in bin/cassandra. The defaults for these are both $CASSANDRA_HOME/logs. These can be unified to a single variable CASSANDRA_LOGS that is set in conf/cassandra-env.sh, with the intention that it would be modified once there (if not set in the environment) by a user running a custom installation. Then include a check in bin/cassandra that CASSANDRA_LOGS is set in case conf/cassandra-env.sh doesn't get sourced on startup, and provide a default value if not. For the scenario that a user would prefer different paths for the logback logs and the GC logs, they can still go into bin/cassandra to set the second path, just as they would do currently. See ""unified_logs_dir.patch"" for a proposed patch. 

No change seems necessary for the Windows scripts. The two uses of $CASSANDRA_HOME/logs are in the same script conf/cassandra-env.ps1 within scrolling distance of each other (lines 278-301). They haven't been combined I suppose because of the different path separators in the two usages.",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-14305,Use $CASSANDRA_CONF not $CASSANDRA_HOME/conf in cassandra-env.sh ,"CASSANDRA_CONF should be used uniformly in conf/cassandra-env.sh to reference the configuration path. Currently, jaas users will have to modify the default path provided for cassandra-jaas.config if their $CASSANDRA_CONF differs from $CASSANDRA_HOME/conf.",N/A,"3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14302,Log when sstables are deleted,This was removed in 3.0 and is super helpful for debugging issues in prod,N/A,"3.0.17, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-14299,cqlsh: ssl setting not read from cqlshrc in 3.11,"With CASSANDRA-10458 an option was added to read the {{--ssl}} flag from cqlshrc, however the commit seems to have been incorrectly merged or the changes were dropped somehow.

Currently adding the following has no effect:
{code:java}
[connection]
ssl = true{code}
When looking at the current tree it's obvious that the flag is not read: [https://github.com/apache/cassandra/blame/cassandra-3.11/bin/cqlsh.py#L2247]

However it should have been added with [https://github.com/apache/cassandra/commit/70649a8d65825144fcdbde136d9b6354ef1fb911]

The values like {{DEFAULT_SSL = False}}  are present, but the {{option_with_default()}} call is missing.

Git blame also shows no change to that line which would have reverted the change.",N/A,3.11.3
CASSANDRA-14292,Fix batch commitlog sync regression,"Prior to CASSANDRA-13987, in batch commitlog mode, commitlog will be synced to disk right after mutation comes.
 * haveWork semaphore is released in BatchCommitLogService.maybeWaitForSync
 * AbstractCommitlogService will continue and sync to disk

After C-13987, it makes a branch for chain maker flush more frequently in periodic mode. To make sure in batch mode CL still flushes immediately, it added {{syncRequested}} flag.
 Unfortunately, in 3.0 branch, this flag is not being set to true when mutation is waiting.

So in AbstractCommitlogService, it will not execute the CL sync branch until it reaches sync window(2ms)..
{code:java|title=AbstractCommitLogService.java}
if (lastSyncedAt + syncIntervalMillis <= pollStarted || shutdown || syncRequested)
{
    // in this branch, we want to flush the commit log to disk
    syncRequested = false;
    commitLog.sync(shutdown, true);
    lastSyncedAt = pollStarted;
    syncComplete.signalAll();
}
else
{
    // in this branch, just update the commit log sync headers
    commitLog.sync(false, false);
}
{code}",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14286,IndexOutOfBoundsException with SELECT JSON using IN and ORDER BY,"When running the following code:

{code}
public class CassandraJsonOrderingBug {
    public static void main(String[] args) {
        Session session = CassandraFactory.getSession();

        session.execute(""CREATE TABLE thebug ( PRIMARY KEY (a, b), a INT, b INT)"");
        try {
            session.execute(""INSERT INTO thebug (a, b) VALUES (20, 30)"");
            session.execute(""INSERT INTO thebug (a, b) VALUES (100, 200)"");
            Statement statement = new SimpleStatement(""SELECT JSON a, b FROM thebug WHERE a IN (20, 100) ORDER BY b"");
            statement.setFetchSize(Integer.MAX_VALUE);
            for (Row w: session.execute(statement)) {
                System.out.println(w.toString());
            }
        } finally {
            session.execute(""DROP TABLE thebug"");
        }
    }
}
{code}

The following exception is thrown server-side:

{noformat}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.Collections$SingletonList.get(Collections.java:4815) ~[na:1.8.0_151]
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1297) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.cql3.statements.SelectStatement$SingleColumnComparator.compare(SelectStatement.java:1284) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355) ~[na:1.8.0_151]
	at java.util.TimSort.sort(TimSort.java:220) ~[na:1.8.0_151]
	at java.util.Arrays.sort(Arrays.java:1512) ~[na:1.8.0_151]
	at java.util.ArrayList.sort(ArrayList.java:1460) ~[na:1.8.0_151]
	at java.util.Collections.sort(Collections.java:175) ~[na:1.8.0_151]
{noformat}

(full traceback attached)

The accessed index is the index of the sorted column in the SELECT JSON fields list.
Similarly, if the select clause is changed to

SELECT JSON b, a FROM thebug WHERE a IN (20, 100) ORDER BY b

then the query finishes, but the output is sorted incorrectly (by textual JSON representation):

{noformat}
Row[{""b"": 200, ""a"": 100}]
Row[{""b"": 30, ""a"": 20}]
{noformat}",N/A,"2.2.13, 3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14284,Chunk checksum test needs to occur before uncompress to avoid JVM crash,"While checksums are (generally) performed on compressed data, the checksum test when reading is currently (in all variants of C* 2.x, 3.x I've looked at) done [on the compressed data] only after the uncompress operation has completed. 

The issue here is that LZ4_decompress_fast (as documented in e.g. [https://github.com/lz4/lz4/blob/dev/lib/lz4.h#L214)] can result in memory overruns when provided with malformed source data. This in turn can (and does, e.g. in CASSANDRA-13757) lead to JVM crashes during the uncompress of corrupted chunks. The checksum operation would obviously detect the issue, but we'd never get to it if the JVM crashes first.

Moving the checksum test of the compressed data to before the uncompress operation (in cases where the checksum is done on compressed data) will resolve this issue.

-----------------------------

The check-only-after-doing-the-decompress logic appears to be in all current releases.

Here are some samples at different evolution points :

3.11.2:

[https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L146]

https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L207

 

3.5:

 [https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L135]

[https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L196]

2.1.17:

 [https://github.com/apache/cassandra/blob/cassandra-2.1.17/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L122]",N/A,"2.1.21, 2.2.13, 3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14251,View replica is not written to pending endpoint when base replica is also view replica,"From the [dev list|https://www.mail-archive.com/dev@cassandra.apache.org/msg12084.html]:

bq. There's an optimization that when we're lucky enough that the paired view replica is the same as this base replica, mutateMV doesn't use the normal view-mutation-sending code (wrapViewBatchResponseHandler) and just writes the mutation locally. In particular, in this case we do NOT write to the pending node (unless I'm missing something). But, sometimes all replicas will be paired with themselves - this can happen for example when number of nodes is equal to RF, or when the base and view table have the same partition keys (but different clustering keys). In this case, it seems the pending node will not be written at all...

This was a regression from CASSANDRA-13069 and the original behavior should be restored.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14250,ERROR 1815 (HY000) at line 28: Internal error: TimedOutException: Default TException.,"Hello,

Good day,

 

Please I need your support for solve this problem and I thank you for all the help you can give.

I'm facing a very strange problem with cassandradb.

I installed cassandra version 3.11.1 on a virtual machine with linux CentOS, where I had already converted a mysql database to MariaDb (Server version: 10.0.33-MariaDB MariaDB Server).

 

In MariaDB I created the following table:

 

CREATE TABLE `tbl_paciente_crm_anexo` (

  `id` int (11) NOT NULL PRIMARY KEY,

  `id_crm` int (11) DEFAULT NULL,

  `patient_id` int (11) DEFAULT NULL,

  `int_organization` int (11) NOT NULL DEFAULT '0',

  `id_pasta_pai` int (11) DEFAULT '0',

  `file` longblob,

  `file_file` longblob,

  `subtitle` varchar (255) DEFAULT NULL,

  `key words` text,

  `filename` varchar (100) DEFAULT NULL,

  `filename` varchar (20) DEFAULT NULL,

  `char_type` char (1) DEFAULT NULL,

  blob data_foto,

  `date_inclusion` blob,

  `data_alteracao` blob,

  `data_ult_access` blob,

  `log_user` varchar (10) DEFAULT NULL,

  `sign` char (1) DEFAULT NULL,

  `login_id` int (11) DEFAULT NULL,

  `xml` blob,

  `hash_xml` varchar (64) DEFAULT NULL,

  `hash_verif` varchar (64) DEFAULT NULL,

  `status_assinatura` varchar (2) DEFAULT NULL

) ENGINE = CASSANDRA thrift_host = `localhost` keyspace =` md_paciente` column_family = `cf_crm_anexo`;

 

 

I'm trying to load 147Gb of data into this table, and there are 2 blob fields in this table .

 
 * +During the loading, and after inserting several records, I receive the following error:+

 

[root@srvmeddbh01 dbbkp]# mysql -p -u root –pxxxxxxxxxx     medicina_intramed<anexo_30276_40275.sql

*ERROR 1815 (HY000) at line 28: Internal error: TimedOutException: Default TException*.

[root@srvmeddbh01 dbbkp]#

 

*+Script with insert command and blob field converted to hex:+*

 

INSERT INTO `tbl_paciente_crm_anexo` (`id`, `assinar`, `data_alteracao` , `arquivo` , `arquivo_imagem`, `data_foto`, `data_inclusao`, `data_ult_acesso`, `hash_verif`, `

hash_xml`, `id_assinatura`, `id_crm`, `id_organizacao`, `id_paciente`, `id_pasta_pai`, `legenda`, `log_usuario`, `nome_arquivo`, `palavras_chaves`, `status_assinatura`,

`tamanhoarquivo`, `tipo_arquivo`, `xml` ) VALUES (30276,' ',' ',x'FFD8FFE000104A46494600010100004800480000FFE1004C4578696600004D4D002A000000080002011200030000000100010

000876900040000000100000026000000000002A002000400000001000001E0A0030004000000010000028000000000FFED003850686F746F73686F7020332E30003842494D04040000000000003842494D04250

00000000010D41D8CD98F00B204E9800998ECF8427EFFC0001108028001E00301220002110...);

 ",N/A,3.11.3
CASSANDRA-14247,SASI tokenizer for simple delimiter based entries,"Currently SASI offers only two tokenizer options:
 - NonTokenizerAnalyser
 - StandardAnalyzer

The latter is built upon Snowball, powerful for human languages but overkill for simple tokenization.

A simple tokenizer is proposed here. The need for this arose as a workaround of CASSANDRA-11182, and to avoid the disk usage explosion when having to resort to {{CONTAINS}}. See https://github.com/openzipkin/zipkin/issues/1861

Example use of this would be:
{code}
CREATE CUSTOM INDEX span_annotation_query_idx 
    ON zipkin2.span (annotation_query) USING 'org.apache.cassandra.index.sasi.SASIIndex' 
    WITH OPTIONS = {
        'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.DelimiterAnalyzer', 
        'delimiter': '░',
        'case_sensitive': 'true', 
        'mode': 'prefix', 
        'analyzed': 'true'};
{code}

Original credit for this work goes to https://github.com/zuochangan",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14245,SELECT JSON prints null on empty strings,"SELECT JSON reports an empty string as null.

 

Example:
{code:java}
cqlsh:unittest> create table test(id INT, name TEXT, PRIMARY KEY(id));
cqlsh:unittest> insert into test (id, name) VALUES (1, 'Foo');
cqlsh:unittest> insert into test (id, name) VALUES (2, '');
cqlsh:unittest> insert into test (id, name) VALUES (3, null);

cqlsh:unittest> select * from test;

id | name
----+------
  1 |  Foo
  2 |     
  3 | null

(3 rows)

cqlsh:unittest> select JSON * from test;

[json]
--------------------------
{""id"": 1, ""name"": ""Foo""}
{""id"": 2, ""name"": null}
{""id"": 3, ""name"": null}

(3 rows){code}
 

This even happens, if the string is part of the Primary Key, which makes the generated string not insertable.

 
{code:java}
cqlsh:unittest> create table test2 (id INT, name TEXT, age INT, PRIMARY KEY(id, name));
cqlsh:unittest> insert into test2 (id, name, age) VALUES (1, '', 42);
cqlsh:unittest> select JSON * from test2;

[json]
------------------------------------
{""id"": 1, ""name"": null, ""age"": 42}

(1 rows)

cqlsh:unittest> insert into test2 JSON '{""id"": 1, ""name"": null, ""age"": 42}';
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid null value in condition for column name""{code}
 

On an older version of Cassandra (3.0.8) does not have this problem.",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14242,Indexed static column returns inconsistent results,"I am using Cassandra 3.11.2, and the Java driver 3.4.0

I have a table that has a static column, where the static column has a secondary index.
When querying the table I get incomplete or duplicated results, depending on the fetch size.

e.g.
{code:java}
CREATE KEYSPACE hack WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE hack.stuff (id int, kind text, chunk int static, val1 int, PRIMARY KEY (id, kind));
CREATE INDEX stuff_chunk_index ON hack.stuff (chunk);{code}

-- repeat with thousands of values for id =>
{code:java}
  INSERT INTO hack.stuff (id, chunk, kind, val1 ) VALUES (${id}, 777, 'A', 123);{code}


Querying from Java:
{code:java}
    final SimpleStatement statement = new SimpleStatement(""SELECT id, kind, val1 FROM hack.stuff WHERE chunk = "" + chunk); 
    statement.setFetchSize(fetchSize);
    statement.setConsistencyLevel(ConsistencyLevel.ALL);
    final ResultSet resultSet = connection.getSession().execute(statement);
    for (Row row : resultSet) {
        final int id = row.getInt(""id"");
    }{code}

*The number of results returned depends on the fetch-size.*

e.g. For 30k values inserted, I get the following:
||fetch-size||result-size||
|40000|30000|
|20000|30001|
|5000|30006|
|100|30303|



In production, I have a much larger table where the correct result size for a specific chunk is 20019, but some fetch sizes will return _significantly fewer_ results.
||fetch-size||result-size|| ||
|25000|20019| |
|5000|9999|*<== this one is has far fewer results*|
|5001|20026| |

(so far been unable to reproduce this with the simpler test table)

Thanks,
Ross",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-14240,Backport circleci yaml,Backport the circleci yaml (sha {{d6e508f33c1a7274b5826ad9d5ce814d719bd848}}) to earlier branches,N/A,"2.2.13, 3.0.17, 3.11.3"
CASSANDRA-14234,ReadCommandTest::testCountWithNoDeletedRow broken,"{code}junit.framework.AssertionFailedError: expected:<1> but was:<5>
	at org.apache.cassandra.db.ReadCommandTest.testCountWithNoDeletedRow(ReadCommandTest.java:336){code}",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14233,nodetool tablestats/cfstats output has inconsistent formatting for latency,"Latencies are reported at keyspace level with `ms.` and at table level with `ms`.

There should be no trailing `.` as it is not a sentence and `.` is not part of the abbreviation.

This is also present in 2.x with `nodetool cfstats`.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14231,After ddl cluster.getMetadata().checkSchemaAgreement() returns true but schema is not in agreement,"We run a set of ddl statements and after each of them wait until schema come to agreement. However periodically after Cluster says that schema is in agreement and we run next DDL statement, it fails as previous DDL did not work properly. 

The most typical scenario when this problem happens almost always: first DDL drops a table, and next DDL creates it (and cassandra says that this table already exists).

 

This is our code to wait:

private void waitForSchemaAgreement()
 {
while (!icluster.getMetadata().checkSchemaAgreement()) {
try {
 Thread.sleep(waitTime*1000);
 } catch (Exception e){

// ignore  } } }

 ",N/A,3.10
CASSANDRA-14219,Change to AlterTableStatement logging breaks MView tests,"looks like [~dbrosius]'s ninja commit {{7df36056b12a13b60097b7a9a4f8155a1d02ff62}} to improve the logging of {{AlterTableStatement}} breaks some MView tests that check the exception message. I see about six failed tests from {{ViewComplexTest}} that have messages similar to this:
{noformat}
junit.framework.AssertionFailedError: Expected error message to contain 'Cannot drop column m on base table with materialized views', but got 'Cannot drop column m on base table table_6 with materialized views.'{noformat}",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14217,nodetool verify needs to use the correct digest file and reload sstable metadata,{{nodetool verify}} tries to use the wrong digest file when verifying old version sstables and it also needs to reload the sstable metadata and notify compaction strategies when it mutates the repaired at field,N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14215,Cassandra does not respect hint window for CAS,"On Cassandra 3.0.9, it was observed that Cassandra continues to write hints even though a node remains down (and does not come up) for longer than the default 3 hour window.

 

After doing ""nodetool setlogginglevel org.apache.cassandra TRACE"", we see the following log line in cassandra (debug) logs:
 StorageProxy.java:2625 - Adding hints for [/10.0.100.84]

 

One possible code path seems to be:

cas -> commitPaxos(proposal, consistencyForCommit, true); -> submitHint (in StorageProxy.java)

 

The ""true"" parameter above explicitly states that a hint should be recorded and ignores the time window calculation performed by the shouldHint method invoked in other code paths. Is there a reason for this behavior?

 

Edit: There are actually two stacks that seem to be producing hints, the ""cas"" and ""syncWriteBatchedMutations"" methods. I have posted them below.

 

A third issue seems to be that Cassandra seems to reset the timer which counts how long a node has been down after a restart. Thus if Cassandra is restarted on a good node, it continues to accumulate hints for a down node over the next three hours.

 

{code:java}
WARN [SharedPool-Worker-14] 2018-02-06 22:15:51,136 StorageProxy.java:2636 - Adding hints for [/10.0.100.84] with stack trace: java.lang.Throwable: at org.apache.cassandra.service.StorageProxy.stackTrace(StorageProxy.java:2608) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2617) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2603) at org.apache.cassandra.service.StorageProxy.commitPaxos(StorageProxy.java:540) at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:282) at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:432) at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:407) at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) at java.lang.Thread.run(Thread.java:748) WARN
{code}

{code:java}
[SharedPool-Worker-8] 2018-02-06 22:15:51,153 StorageProxy.java:2636 - Adding hints for [/10.0.100.84] with stack trace: java.lang.Throwable: at org.apache.cassandra.service.StorageProxy.stackTrace(StorageProxy.java:2608) at org.apache.cassandra.service.StorageProxy.submitHint(StorageProxy.java:2617) at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:1247) at org.apache.cassandra.service.StorageProxy.syncWriteBatchedMutations(StorageProxy.java:1014) at org.apache.cassandra.service.StorageProxy.mutateAtomically(StorageProxy.java:899) at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:834) at org.apache.cassandra.cql3.statements.BatchStatement.executeWithoutConditions(BatchStatement.java:365) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:343) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:329) at org.apache.cassandra.cql3.statements.BatchStatement.execute(BatchStatement.java:324) at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) at java.lang.Thread.run(Thread.java:748)
{code}

 

 ",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14212,Back port CASSANDRA-13080 to 3.11.2 (Use new token allocation for non bootstrap case as well),"Backport CASSANDRA-13080 to 3.11.x

 

The patch applies without conflict to the {{cassandra-3.11}} and equally concerns to users of Cassandra-3.11.1

 ",N/A,3.11.2
CASSANDRA-14210,Optimize SSTables upgrade task scheduling,"When starting the SSTable-rewrite process by running {{nodetool upgradesstables --jobs N}}, with N > 1, not all of the provided N slots are used.

For example, we were testing with {{concurrent_compactors=5}} and {{N=4}}.  What we observed both for version 2.2 and 3.0, is that initially all 4 provided slots are used for ""Upgrade sstables"" compactions, but later when some of the 4 tasks are finished, no new tasks are scheduled immediately.  It takes the last of the 4 tasks to finish before new 4 tasks would be scheduled.  This happens on every node we've observed.

This doesn't utilize available resources to the full extent allowed by the --jobs N parameter.  In the field, on a cluster of 12 nodes with 4-5 TiB data each, we've seen that the whole process was taking more than 7 days, instead of estimated 1.5-2 days (provided there would be close to full N slots utilization).

Instead, new tasks should be scheduled as soon as there is a free compaction slot.
Additionally, starting from the biggest SSTables could further reduce the total time required for the whole process to finish on any given node.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14209,group by select queries query results differ when using select * vs select fields,"{{I get two different out with these 2 queries.  The only difference between the 2 queries is that one does ‘select *’ and other does ‘select specific fields’ without any aggregate functions.}}

{{I am using Apache Cassandra 3.10.}}


{{Consistency level set to LOCAL_QUORUM.}}
{{cassandra@cqlsh> select * from wp.position where account_id = 'user_1';}}

{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}
{{------------+-------------+---------+----------------+------------------+----------+----------------+---------------------------------}}
{{ user_1 | AMZN | 2 | 1239.2 | 0 | 1011 | null | 2018-01-25 17:18:07.158000+0000}}
{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}

{{(2 rows)}}
{{cassandra@cqlsh> select * from wp.position where account_id = 'user_1' group by security_id;}}

{{ account_id | security_id | counter | avg_exec_price | pending_quantity | quantity | transaction_id | update_time}}
{{------------+-------------+---------+----------------+------------------+----------+----------------+---------------------------------}}
{{ user_1 | AMZN | 1 | 1239.2 | 0 | 1010 | null | 2018-01-25 17:18:07.158000+0000}}

{{(1 rows)}}
{{cassandra@cqlsh> select account_id,security_id, counter, avg_exec_price,quantity, update_time from wp.position where account_id = 'user_1' group by security_id ;}}

{{ account_id | security_id | counter | avg_exec_price | quantity | update_time}}
{{------------+-------------+---------+----------------+----------+---------------------------------}}
{{ user_1 | AMZN | 2 | 1239.2 | 1011 | 2018-01-25 17:18:07.158000+0000}}

{{(1 rows)}}


{{Table Description:}}
{{CREATE TABLE wp.position (}}
{{ account_id text,}}
{{ security_id text,}}
{{ counter bigint,}}
{{ avg_exec_price double,}}
{{ pending_quantity double,}}
{{ quantity double,}}
{{ transaction_id uuid,}}
{{ update_time timestamp,}}
{{ PRIMARY KEY (account_id, security_id, counter)}}
{{) WITH CLUSTERING ORDER BY (security_id ASC, counter DESC)}}{{ }}",N/A,3.11.3
CASSANDRA-14208,space is 100 percent full on one node and other nodes we have free space ,We have 3 node cluster. On one node we are filled with 100 precent. on other 2 nodes we have littlt bit space ]. how to reclaim the space on node3 which is having only 1GB free out of 300 GB.,N/A,3.0.16
CASSANDRA-14205,ReservedKeywords class is missing some reserved CQL keywords,"The CQL keywords {{DEFAULT}}, {{UNSET}}, {{MBEAN}} and {{MBEANS}} (introduced by CASSANDRA-11424 and CASSANDRA-10091) are neither considered [unreserved keywords|https://github.com/apache/cassandra/blob/trunk/src/antlr/Parser.g#L1788-L1846] by the ANTLR parser, nor included in the [{{ReservedKeywords}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/ReservedKeywords.java] class.

The current parser behaviour is considering them as reserved keywords, in the sense that they can't be used as keyspace/table/column names, which seems right:
{code:java}
cassandra@cqlsh> CREATE KEYSPACE unset WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
SyntaxException: line 1:16 no viable alternative at input 'unset' (CREATE KEYSPACE [unset]...)
{code}
I think we should keep considering these keywords as reserved and add them to {{ReservedKeywords}} class.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14204,Remove unrepaired SSTables from garbage collection when only_purge_repaired_tombstones is true to avoid AssertionError in nodetool garbagecollect,"When manually running a garbage collection compaction across a table with unrepaired sstables and only_purge_repaired_tombstones set to true an assertion error is thrown. This is because the unrepaired sstables aren't being removed from the transaction as they are filtered out in filterSSTables().
||3.11||trunk||
|[branch|https://github.com/vincewhite/cassandra/commit/e13c822736edd3df3403c02e8ef90816f158cde2]|[branch|https://github.com/vincewhite/cassandra/commit/cc8828576404e72504d9b334be85f84c90e77aa7]|

The stacktrace:
{noformat}
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.CompactionManager.parallelAllSSTableOperation(CompactionManager.java:339)
	at org.apache.cassandra.db.compaction.CompactionManager.performGarbageCollection(CompactionManager.java:476)
	at org.apache.cassandra.db.ColumnFamilyStore.garbageCollect(ColumnFamilyStore.java:1579)
	at org.apache.cassandra.service.StorageService.garbageCollect(StorageService.java:3069)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1468)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1309)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1401)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:829)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:357)
	at sun.rmi.transport.Transport$1.run(Transport.java:200)
	at sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


{noformat}",N/A,"3.11.16, 4.0.11, 4.1.3, 5.0-alpha1, 5.0"
CASSANDRA-14202,Assertion error on sstable open during startup should invoke disk failure policy,We should catch all exceptions when opening sstables on startup and invoke the disk failure policy,N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14200,NullPointerException when dumping sstable with null value for timestamp column,"We have an sstable whose schema has a column of type timestamp and it's not part of primary key. When dumping the sstable using sstabledump there is NPE like this:
{code:java}
Exception in thread ""main"" java.lang.NullPointerException
at java.util.Calendar.setTime(Calendar.java:1770)
at java.text.SimpleDateFormat.format(SimpleDateFormat.java:943)
at java.text.SimpleDateFormat.format(SimpleDateFormat.java:936)
at java.text.DateFormat.format(DateFormat.java:345)
at org.apache.cassandra.db.marshal.TimestampType.toJSONString(TimestampType.java:93)
at org.apache.cassandra.tools.JsonTransformer.serializeCell(JsonTransformer.java:442)
at org.apache.cassandra.tools.JsonTransformer.serializeColumnData(JsonTransformer.java:376)
at org.apache.cassandra.tools.JsonTransformer.serializeRow(JsonTransformer.java:280)
at org.apache.cassandra.tools.JsonTransformer.serializePartition(JsonTransformer.java:215)
at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
at java.util.Iterator.forEachRemaining(Iterator.java:116)
at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
at org.apache.cassandra.tools.JsonTransformer.toJson(JsonTransformer.java:104)
at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:242){code}

The reason is that we use a null Date when there is no value for this column:
{code}
    public Date deserialize(ByteBuffer bytes)
    {
        return bytes.remaining() == 0 ? null : new Date(ByteBufferUtil.toLong(bytes));
    }
{code}

It seems that we should not deserialize columns with null values.",N/A,3.0.22
CASSANDRA-14199,exception when dumping sstable with frozen collection of UUID,"When dumping (sstabledump) sstable with frozen collection of UUID, there is exception like this:
{code:java}
Exception in thread ""main"" org.apache.cassandra.serializers.MarshalException: UUID should be 16 or 0 bytes (24)
        at org.apache.cassandra.serializers.UUIDSerializer.validate(UUIDSerializer.java:43)
        at org.apache.cassandra.db.marshal.AbstractType.getString(AbstractType.java:128)
        at org.apache.cassandra.tools.JsonTransformer.serializeCell(JsonTransformer.java:440)
        at org.apache.cassandra.tools.JsonTransformer.serializeColumnData(JsonTransformer.java:374)
        at org.apache.cassandra.tools.JsonTransformer.serializeRow(JsonTransformer.java:278)
        at org.apache.cassandra.tools.JsonTransformer.serializePartition(JsonTransformer.java:213)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
        at java.util.Iterator.forEachRemaining(Iterator.java:116)
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801)
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)
        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
        at org.apache.cassandra.tools.JsonTransformer.toJson(JsonTransformer.java:102)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:242){code}
 

*Steps to reproduce:*
{code:java}
cqlsh> create TABLE stresscql.sstabledump_test(userid text PRIMARY KEY, c1 list<uuid>, c2 frozen<list<uuid>>, c3 set<text>, c4 frozen<set<text>>, c5 map<text,text>, c6 frozen<map<text,text>>);
cqlsh> insert INTO stresscql.sstabledump_test (userid, c1, c2, c3, c4, c5, c6) VALUES ( 'id', [6947e8c0-02fa-11e8-87e1-fb0d0e20b5c4], [6947e8c0-02fa-11e8-87e1-fb0d0e20b5c4], {'set', 'user'}, {'view', 'over'}, {'good': 'hello', 'root': 'text'}, {'driver': 'java', 'note': 'new'});{code}
 

*Root cause:*

Frozen collection is treated as simple column and it's the client's responsibility to parse the data from ByteBuffer. We have this logic in different drivers but sstabledump doesn't have the logic in place. It just treat the whole collection as a single UUID.",N/A,3.0.25
CASSANDRA-14196,replace_address_test:TestReplaceAddress.test_multi_dc_replace_with_rf1 fails without vnodes,"Skipping it for now, but it probably should pass without vnodes.
https://circleci.com/gh/aweisberg/cassandra/871#tests/containers/15",N/A,"3.0.26, 3.11.12, 4.0.2, 4.1-alpha1, 4.1"
CASSANDRA-14194,Chain commit log marker potential performance regression in batch commit mode,"CASSANDRA-13987 modified how the commit log sync thread works. I noticed that cql3.ViewTest and ViewBuilderTaskTest have been timing out, but only in CircleCI. When I jstack in CircleCI what I see is that the commit log writer thread is parked and the threads trying to append to the commit log are blocked waiting on it.

I tested the commit before 13987 and it passed in CircleCI and then I tested with 13987 and it timed out. I suspect this may be a general performance regression and not just a CircleCI issue.

And this is with write barriers disabled (sync thread doesn't actually sync) so it wasn't blocked in the filesystem.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14183,CVE-2017-5929 Security vulnerability and redefine default log rotation policy,"Cassandra 3.11.1 is patched with logback 1.1.3, which contains the security vulnerability described here. [https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2017-5929]

Also update to logback allows a simple date and size rotation policy to
replace the default fixed policy, which is broken by design.",N/A,"2.1.21, 2.2.13, 3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14181,RPM package has too many executable files,"When installing using the RPM files:
In /etc/cassandra/conf, the files should not be execuable, as they are either
 * properties-like files
 * readme-like files, or
 * files to be sourced by shell scripts

I'm adding a patch (cassandra-permissions-fix.patch) to the cassandra.spec file which fixes this.",N/A,"2.1.20, 2.2.12, 3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14180,cassandra.spec needs to require ant-junit,"I tried rebuilding cassandra-3.11.1-1.src.rpm on a Centos 7 host which had ant installed, but not the ""ant-junit"" package; that failed with a somewhat cryptic error message. It turnout out I needed to have the ""ant-junit"" package installed, as well. So for the cassandra.spec file, I suggest that the following line be added just below the existing BuildRequires line:

{{BuildRequires: ant-junit >= 1.9}}",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14175,Incorrect documentation about CASSANDRA_INCLUDE priority,"In _bin/cassandra_ the comments say:
{quote}The lowest priority search path is the same directory as the startup script...
{quote}
However the ""same directory"" currently has the *highest* priority:
{code:java}
    # Locations (in order) to use when searching for an include file.
    for include in ""`dirname ""$0""`/cassandra.in.sh"" \
                   ""$HOME/.cassandra.in.sh"" \
                   /usr/share/cassandra/cassandra.in.sh \
                   /usr/local/share/cassandra/cassandra.in.sh \
                   /opt/cassandra/cassandra.in.sh; do
        if [ -r ""$include"" ]; then
            . ""$include""
            break
        fi
    done
{code}
It looks like around the release of v 2.0.0 the order was changed but the comments stayed the same.",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14173,JDK 8u161 breaks JMX integration,"{\{org.apache.cassandra.utils.JMXServerUtils}} which is used to programatically configure the JMX server and RMI registry (CASSANDRA-2967, CASSANDRA-10091) depends on some JDK internal classes/interfaces. A change to one of these, introduced in Oracle JDK 1.8.0_162 is incompatible, which means we cannot build using that JDK version. Upgrading the JVM on a node running 3.6+ will result in Cassandra being unable to start.
{noformat}
ERROR [main] 2018-01-18 07:33:18,804 CassandraDaemon.java:706 - Exception encountered during startup
java.lang.AbstractMethodError: org.apache.cassandra.utils.JMXServerUtils$Exporter.exportObject(Ljava/rmi/Remote;ILjava/rmi/server/RMIClientSocketFactory;Ljava/rmi/server/RMIServerSocketFactory;Lsun/misc/ObjectInputFilter;)Ljava/rmi/Remote;
        at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:150) ~[na:1.8.0_162]
        at javax.management.remote.rmi.RMIJRMPServerImpl.export(RMIJRMPServerImpl.java:135) ~[na:1.8.0_162]
        at javax.management.remote.rmi.RMIConnectorServer.start(RMIConnectorServer.java:405) ~[na:1.8.0_162]
        at org.apache.cassandra.utils.JMXServerUtils.createJMXServer(JMXServerUtils.java:104) ~[apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.maybeInitJmx(CassandraDaemon.java:143) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:600) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:689) [apache-cassandra-3.11.2-SNAPSHOT.jar:3.11.2-SNAPSHOT]{noformat}

This is also a problem for CASSANDRA-9608, as the internals are completely re-organised in JDK9, so a more stable solution that can be applied to both JDK8 & JDK9 is required.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14170,Loss of digits when doing CAST from varint/bigint to decimal,"Cast functions from numeric types to decimal type are implemented as conversion to double first and then from double to decimal: [https://github.com/apache/cassandra/compare/trunk...blerer:10310-3.0#diff-6aa4a8f76df6c30c5bb4026b8c9251eeR80].

This can cause loss of digits for big values stored in varint or bigint. It is probably unexpected because decimal can store such values precisely.

Examples:

{{cqlsh> CREATE TABLE cast_bigint_test(k int PRIMARY KEY, bigint_clmn bigint);}}
 {{cqlsh> INSERT INTO cast_bigint_test(k, decimal_clmn) VALUES(2, 9223372036854775807);}}
 {{cqlsh> SELECT CAST(bigint_clmn AS decimal) FROM cast_bigint_test;}}
 {{cast(bigint_clmn as decimal)}}
 {{------------------------------}}
 {{9.223372036854776E+18}}
 {{(1 rows)}}

{{cqlsh> CREATE TABLE cast_varint_test (k int PRIMARY KEY, varint_clmn varint);}}
 {{cqlsh> INSERT INTO cast_varint_test(k, varint_clmn) values(2, 1234567890123456789);}}
 {{cqlsh> SELECT CAST(varint_clmn AS decimal) FROM cast_varint_test;}}
 {{cast(varint_clmn as decimal)}}
 {{------------------------------}}
1.23456789012345677E+18
 {{(1 rows)}}

 ",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14167,IndexOutOfBoundsException when selecting column counter and consistency quorum,"This morning I upgraded my cluster from 3.11.0 to 3.11.1 and it appears when I perform a query on a counter specifying the column name cassandra throws the following exception:
{code:java}
WARN [ReadStage-1] 2018-01-15 10:58:30,121 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.IndexOutOfBoundsException: null
java.nio.Buffer.checkIndex(Buffer.java:546) ~[na:1.8.0_144]
java.nio.HeapByteBuffer.getShort(HeapByteBuffer.java:314) ~[na:1.8.0_144]
org.apache.cassandra.db.context.CounterContext.headerLength(CounterContext.java:173) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.context.CounterContext.updateDigest(CounterContext.java:696) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.AbstractCell.digest(AbstractCell.java:126) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.AbstractRow.digest(AbstractRow.java:73) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:181) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:120) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadResponse.createDigestResponse(ReadResponse.java:87) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:345) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.1.jar:3.11.1]
org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
java.lang.Thread.run(Thread.java:748) [na:1.8.0_144]
{code}

Query works completely find on consistency level ONE but not on QUORUM. 
Is this possibly related to CASSANDRA-11726?",N/A,"3.0.17, 3.11.3"
CASSANDRA-14166,sstabledump tries to delete a file,"Directory /var/lib/cassandra/data/<keyspace>/<table> has cassandra:cassandra owner.
An error happens when sstabledump executed on file in that directory by regular user:


{code:java}
$ sstabledump mc-56801-big-Data.db
Exception in thread ""main"" FSWriteError in /var/lib/cassandra/data/<keyspace>/<table>/mc-56801-big-Summary.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:142)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:159)
        at org.apache.cassandra.io.sstable.format.SSTableReader.saveSummary(SSTableReader.java:935)
        at org.apache.cassandra.io.sstable.format.SSTableReader.saveSummary(SSTableReader.java:920)
        at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:788)
        at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:731)
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:516)
        at org.apache.cassandra.io.sstable.format.SSTableReader.openNoValidation(SSTableReader.java:396)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:191)
Caused by: java.nio.file.AccessDeniedException: /var/lib/cassandra/data/<keyspace>/<table>/mc-56801-big-Summary.db
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244)
        at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103)
        at java.nio.file.Files.delete(Files.java:1126)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:136)
        ... 8 more

{code}

I have changed bloom_filter_fp_chance for that table couple months ago, so I believe that's the reason why SSTableReader wants to recreate summary file. But when used in sstabledump it should not try to modify / delete any files.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14163,RateBasedBackPressure unnecessarily invokes a lock on the Guava RateLimiter,"{{o.a.c.net.RateBasedBackPressure#apply}} calls {{getRate()}} on the underlying Guava {{RateLimiter}} several times in quick succession (including as the argument to a non level checked log statement). 

Said {{getRate()}} method acquires a lock within {{RateLimiter}}, so just getting a local variable reference will remove several calls and thus several lock acquisitions. ",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14162,Backport 7950 (Output of nodetool compactionstats and compactionhistory does not work well with long keyspace and column family names),Colleagues have had issues with output of listsnapshots/compactionstats because of things with really long names. Mostly cosmetic but I see no reason we shouldn't backport CASSANDRA-7950 to 3.0. It's practically a bugfix. I've attached a patch and a bunch of images to show the relevant commands working as intended after applying the patch.,N/A,3.0.25
CASSANDRA-14154,`ant javadoc` task broken due to UTF-8 characters in multiple source files,"Several source files contain UTF-8 characters in String literals. When building the {{javadoc}} target with ant ({{ant javadoc}}), these will trip up javadoc, which defaults to ASCII encoding. See the {{build.log}} for what I did and the resulting output.

I created a patch that will fix the problem ({{javadoc-encoding.patch}}), which is attached.

I encountered this problem in 3.11.1, but I haven't checked whether other versions are affected as well.",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14151,"[TRUNK] TestRepair.test_dead_sync_initiator failed due to ERROR in logs ""SSTableTidier ran with no existing data file for an sstable that was not new""","TestRepair.test_dead_sync_initiator failed due to finding the following unexpected error in the node's logs:

{code}
ERROR [NonPeriodicTasks:1] 2018-01-06 03:38:50,229 LogTransaction.java:347 - SSTableTidier ran with no existing data file for an sstable that was not new
{code}

If this is ""okay/expected"" behavior we should change the log level to something different (which will fix the test) or if it's an actual bug use this JIRA to fix it. I've attached all of the logs from all 3 instances from the dtest run that hit this failure.",N/A,"3.11.9, 4.1-alpha1, 4.1"
CASSANDRA-14143,CommitLogStressTest timeout in 3.11,"[~jasobrown] fixed the CommitLogStressTest timeout issue as part of CASSANDRA-13530, but it's only in trunk, it would be better to backport the unittest change to 3.11.
",N/A,3.11.2
CASSANDRA-14141,Enable CDC unittest,"Follow up for CASSANDRA-14066
2 CDC unittests are skipped in the normal test run, it has to be {{$ ant test-cdc}} to run the cdc test.

The fix enables them in normal {{$ ant test}}",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14140,Add unittest for Schema migration change (CASSANDRA-14109),"It's a fairly big change, would be better to have a few unittest.",N/A,3.11.2
CASSANDRA-14139,Acquire read lock before accessing CompactionStrategyManager fields,"There are a few methods in {{CompactionStrategyManager}} accessing the repaired/unrepaired compaction strategy lists without using the read lock, what could cause issues like the one below:

{noformat}
ERROR [CompactionExecutor:1] 2017-12-22 12:17:12,320 CassandraDaemon.java:141 - Exception in thread Thread[CompactionExecutor:1,5,main]
java.lang.IndexOutOfBoundsException: Index: 0, Size: 1
    at java.util.ArrayList.rangeCheck(ArrayList.java:657)
    at java.util.ArrayList.get(ArrayList.java:433)
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.supportsEarlyOpen(CompactionStrategyManager.java:1262)
    at org.apache.cassandra.db.ColumnFamilyStore.supportsEarlyOpen(ColumnFamilyStore.java:558)
    at org.apache.cassandra.io.sstable.SSTableRewriter.construct(SSTableRewriter.java:119)
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.<init>(CompactionAwareWriter.java:91)
    at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:57)
    at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:293)
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:200)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:90)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:101)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:310)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
    at java.lang.Thread.run(Thread.java:748)
{noformat}",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14137,Cassandra crashes on startup.  Crash Problematic frame: # C  [sigar-amd64-winnt.dll+0x14ed4] using JRE version: Java(TM) SE Runtime Environment (9.0+11),"Startup of Cassandra crashes in sigar-amd64-winnt.dll+0x14ed4.
Short term work around: change from Java 9 back to Java 8.

#
# A fatal error has been detected by the Java Runtime Environment:
#
#  EXCEPTION_ACCESS_VIOLATION (0xc0000005) at pc=0x0000000010014ed4, pid=7112, tid=1764
#
# JRE version: Java(TM) SE Runtime Environment (9.0+11) (build 9.0.1+11)Problematic frame:
# C  [sigar-amd64-winnt.dll+0x14ed4]
# Java VM: Java HotSpot(TM) 64-Bit Server VM (9.0.1+11, mixed mode, tiered, compressed oops, concurrent mark sweep gc, windows-amd64)
# Problematic frame:
# C  [sigar-amd64-winnt.dll+0x14ed4]
#
# No core dump will be written. Minidumps are not enabled by default on client versions of Windows
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
# ",N/A,"3.0.15, 3.0.16"
CASSANDRA-14133,Log file names of files streamed in to a node,We should log the file names of sstables streamed in to a node,N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14113,AssertionError while trying to upgrade 2.2.11 -> 3.11.1,"We're trying to upgrade a test cluster from Cassandra 2.2.11 to Cassandra 3.11.1. The tables have been created using thrift and have supercolumns. When I try to run {{nodetool upgradesstables}} I get the following:
{noformat}error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.rows.BufferCell.<init>(BufferCell.java:42)
	at org.apache.cassandra.db.LegacyLayout$CellGrouper.addCell(LegacyLayout.java:1242)
	at org.apache.cassandra.db.LegacyLayout$CellGrouper.addAtom(LegacyLayout.java:1185)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.readRow(UnfilteredDeserializer.java:498)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer$UnfilteredIterator.hasNext(UnfilteredDeserializer.java:472)
	at org.apache.cassandra.db.UnfilteredDeserializer$OldFormatDeserializer.hasNext(UnfilteredDeserializer.java:306)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.computeNext(SSTableSimpleIterator.java:188)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.computeNext(SSTableSimpleIterator.java:140)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:122)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:100)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.utils.MergeIterator$TrivialOneToOne.computeNext(MergeIterator.java:484)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:499)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:359)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:133)
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:74)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:75)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:233)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:196)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:428)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:315)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:748)
{noformat}

We also tried to upgrade to 3.0.15 instead and had a different error:
{noformat}
ERROR 11:00:40 Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IllegalStateException: [ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.BytesType, kind=PARTITION_KEY, position=0}, ColumnDefinition{name=, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.BytesType,org.apache.cassandra.db.marshal.BytesType), kind=REGULAR, position=-1}] is not a subset of []
    at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:532) ~[main/:na]
    at org.apache.cassandra.db.Columns$Serializer.serializedSubsetSize(Columns.java:484) ~[main/:na]
    at org.apache.cassandra.db.rows.UnfilteredSerializer.serializedRowBodySize(UnfilteredSerializer.java:290) ~[main/:na]
    at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:169) ~[main/:na]
    at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:114) ~[main/:na]
    at org.apache.cassandra.db.ColumnIndex$Builder.add(ColumnIndex.java:144) ~[main/:na]
    at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:112) ~[main/:na]
    at org.apache.cassandra.db.ColumnIndex.writeAndBuildIndex(ColumnIndex.java:52) ~[main/:na]
    at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:149) ~[main/:na]
    at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125) ~[main/:na]
    at org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter.realAppend(MaxSSTableSizeWriter.java:88) ~[main/:na]
    at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195) ~[main/:na]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89) ~[main/:na]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:424) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:311) ~[main/:na]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_151]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_151]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_151]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
    at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_151]
{noformat}

Attached are a set of sstables that reproduce the issue.",N/A,3.11.14
CASSANDRA-14112,The inspectJvmOptions startup check can trigger some Exception on some JRE versions,"[~adelapena] pointed out that the Startup check added by CASSANDRA-13006 can cause some Exception if Cassandra is run on a non GA version.
After investigation it seems that it can also be the case for major versions or some JRE 9 versions. ",N/A,"2.2.12, 3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14109,Prevent continuous schema exchange between 3.0 and 3.11 nodes,"Continuous schema migrations can happen during an upgrade from 3.0.x to 3.x even with versions having the patches for CASSANDRA-13441 and CASSANDRA-13559.

The root cause is the {{cdc}} column, which is included in schema version calculation in {{RowIterators.digest()}} via {{SchemaKeyspace.calculateSchemaDigest()}}.

It is possible to make the schema-version calculation between 3.0 and 3.11 compatible. The idea here is: 3.11 accepts both 3.0 compatible and 3.11 ""native"" schema versions. As long as there is one 3.0 node in the cluster, 3.11 announces a 3.0 compatible schema version (without the {{cdc}} column). When there are no (more) 3.0 nodes in the cluster, announce the ""real"" 3.11 schema version (including the {{cdc}} column). ""Announce"" means announcing via Gossip and storing in {{system.local}}.

The change itself is against 3.11 only. A couple of log messages have been improved and some code regarding schema version checks has been moved into the {{Schema}} class. Those ""side changes"" are carried to trunk. Because of that, the 3.11 and trunk branches are different. The ""real"" change is in the 3.11 branch.

{{NEWS.txt}} for 3.11(only) contains upgrade notes.

||OSS 3.11|[branch|https://github.com/apache/cassandra/compare/cassandra-3.11...snazy:schema-migration-upgrade-bug-3.11?expand=1]
||OSS trunk|[branch|https://github.com/apache/cassandra/compare/trunk...snazy:schema-migration-upgrade-bug-trunk?expand=1]
||OSS dtest|[branch|https://github.com/riptano/cassandra-dtest/compare/master...snazy:schema-migration-upgrade-bug?expand=1]

We've verified the functionality of the patch by usual CI tests and extensive tests using the new upgrade dtest.",N/A,3.11.2
CASSANDRA-14108,Improve commit log chain marker updating,"CASSANDRA-13987 addressed the commit log behavior change that was introduced with CASSANDRA-3578. After that patch was committed, [~aweisberg] did his own review and found a bug as well as having some concerns about the configuration. He and I discussed offline, and agreed on some improvements. 

Instead of requiring users to configure a deep, dark implementation detail like the commit log chained markers (via {{commitlog_marker_period_in_ms}} in the yaml), we decided it is best to eliminate thew configuration and always update the chained markers (when in periodic mode). 

The bug [~aweisberg] found was when the chained marker update is not a value that evenly divides into the periodic sync mode value, we would not sync in an expected manner. For example if the marker interval is 9 seconds, and the sync interval is 10 seconds, we would update the markers at time9, but we would then sleep for another 9 seconds, and when we wake up at time18, it is then that we flush - 8 seconds later than we should have. 
",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14104,Index target doesn't correctly recognise non-UTF column names after COMPACT STORAGE drop,"Creating a compact storage table with dynamic composite type, then running {{ALTER TALBE ... DROP COMPACT STORAGE}} and then restarting the node will crash Cassandra node, since the Index Target is fetched using hashmap / strict equality. We need to fall back to linear search when index target can't be found (which should not be happening often).",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14103,Fix potential race during compaction strategy reload,"When the compaction strategies are reloaded after disk boundary changes (CASSANDRA-13948), it's possible that a recently finished SSTable is added twice to the compaction strategy: once when the compaction strategies are reloaded due to the disk boundary change ({{maybeReloadDiskBoundarie}}), and another when the {{CompactionStrategyManager}} is processing the {{SSTableAddedNotification}}.

This should be quite unlikely because a compaction must finish as soon as the disk boundary changes, and even if it happens most compaction strategies would not be affected by it since they deduplicate sstables internally, but we should protect against such scenario. 

For more context see [this comment|https://issues.apache.org/jira/browse/CASSANDRA-13948?focusedCommentId=16280448&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16280448] from Marcus.",N/A,"3.11.9, 4.0-beta3, 4.0"
CASSANDRA-14099,LCS ordering of sstables by timestamp is inverted,"In CASSANDRA-14010 we discovered that CASSANDRA-13776 broke sstable ordering by timestamp (inverted it accidentally). Investigating that revealed that the comparator was expecting newest-to-oldest for read command, but LCS expects oldest-to-newest.

",N/A,"3.11.4, 4.0-alpha2, 4.0"
CASSANDRA-14096,Cassandra 3.11.1 Repair Causes Out of Memory,"Number of nodes: 9
System resources: 8 Core, 16GB RAM
Replication factor: 3
Number of vnodes: 256

We get out of memory errors while repairing (incremental or full) our keyspace. I had tried to increase node's memory from 16GB to 32GB but result did not change. Repairing tables one by one in our keyspace was not completed successfully for all tables too. 

Only subrange repair with cassandra-reaper worked for me.

Here is the output of heap utils before oom:

{code}

ERROR [MessagingService-Incoming-/192.168.199.121] 2017-12-05 11:38:08,121 JVMStabilityInspector.java:142 - JVM state determined to be unstable.  Exiting forcefully due to:
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.gms.GossipDigestSerializationHelper.deserialize(GossipDigestSyn.java:66) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:95) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.gms.GossipDigestSynSerializer.deserialize(GossipDigestSyn.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:192) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:180) ~[apache-cassandra-3.11.1.jar:3.11.1]
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:94) ~[apache-cassandra-3.11.1.jar:3.11.1]
{code}

{code}
 num     #instances         #bytes  class name
----------------------------------------------
   1:      31105265     1493052720  org.apache.cassandra.utils.MerkleTree$Inner
   2:      31134570      996306240  org.apache.cassandra.utils.MerkleTree$Leaf
   3:      31195121      748682904  org.apache.cassandra.dht.Murmur3Partitioner$LongToken
   4:      22885384      667447608  [B
   5:        214550       18357360  [C
   6:        364637       17502576  java.nio.HeapByteBuffer
   7:         46525        9566496  [J
   8:        111024        5306976  [Ljava.lang.Object;
   9:        132674        5306960  org.apache.cassandra.db.rows.BufferCell
  10:        210309        5047416  java.lang.String
  11:         59984        3838976  org.apache.cassandra.utils.btree.BTreeSearchIterator
  12:        101181        3237792  java.util.HashMap$Node
  13:         27158        2719216  [I
  14:         60181        2407240  java.util.TreeMap$Entry
  15:         65998        2111936  org.apache.cassandra.db.rows.BTreeRow
  16:         62387        2023784  [Ljava.nio.ByteBuffer;
  17:         19086        1750464  [Ljava.util.HashMap$Node;
  18:         63466        1523184  javax.management.ObjectName$Property
  19:         61553        1477272  org.apache.cassandra.db.BufferClustering
  20:         29274        1405152  org.apache.cassandra.utils.MerkleTree
  21:         34602        1384080  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$100/78247817
  22:         40972        1311104  java.util.concurrent.ConcurrentHashMap$Node
  23:         39172        1253504  java.util.RandomAccessSubList
  24:         51657        1239768  org.apache.cassandra.db.LivenessInfo
  25:         19013        1216832  java.nio.DirectByteBuffer
  26:         28178        1127120  org.apache.cassandra.db.PreHashedDecoratedKey
  27:         32407        1033120  [Ljavax.management.ObjectName$Property;
  28:         42090        1010160  java.util.EnumMap$EntryIterator$Entry
  29:         40878         981072  java.util.Arrays$ArrayList
  30:         19721         946608  java.util.HashMap
  31:          8359         932600  java.lang.Class
  32:         37277         894648  org.apache.cassandra.dht.Range
  33:         26897         860704  org.apache.cassandra.db.rows.EncodingStats
  34:         19958         798320  org.apache.cassandra.utils.MergeIterator$Candidate
  35:         31281         750744  java.util.ArrayList
  36:         23291         745312  org.apache.cassandra.utils.MerkleTree$TreeRange
  37:         21650         692800  java.util.AbstractList$ListItr
  38:         27675         664200  java.lang.Long
  39:         16204         648160  javax.management.ObjectName
  40:         36873         589968  org.apache.cassandra.utils.WrappedInt
  41:          4100         557600  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference
  42:         21651         519624  java.util.SubList$1
  43:         12275         491000  java.math.BigInteger
  44:          8657         484792  org.apache.cassandra.utils.memory.BufferPool$Chunk
  45:         14732         471424  java.util.ArrayList$Itr
  46:          5371         429680  java.lang.reflect.Constructor
  47:         12640         404480  com.codahale.metrics.LongAdder
  48:         16156         387744  com.sun.jmx.mbeanserver.NamedObject
  49:         16133         387192  com.sun.jmx.mbeanserver.StandardMBeanSupport
  50:          9536         381440  org.apache.cassandra.db.EmptyIterators$EmptyUnfilteredRowIterator
  51:          6035         337960  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator
  52:          6031         337736  org.apache.cassandra.db.transform.UnfilteredRows
  53:          8298         331920  org.apache.cassandra.db.rows.BTreeRow$Builder
  54:          5182         331648  sun.security.provider.SHA2$SHA256
  55:         10356         331392  org.apache.cassandra.utils.btree.BTree$$Lambda$192/259279152
  56:          8145         325800  org.apache.cassandra.db.rows.SerializationHelper
  57:          8144         325760  org.apache.cassandra.io.sstable.SSTableIdentityIterator
  58:          8144         325760  org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator
  59:           176         319536  [Ljava.util.concurrent.ConcurrentHashMap$Node;
  60:          9716         310912  java.net.InetAddress$InetAddressHolder
  61:          7770         310800  com.github.benmanes.caffeine.cache.NodeFactory$SStMW
  62:         18470         295520  org.apache.cassandra.db.rows.CellPath$SingleItemCellPath
  63:          2505         276784  [S
  64:          5646         271008  com.codahale.metrics.EWMA
  65:         11258         270192  java.util.concurrent.ConcurrentLinkedDeque$Node
  66:          8248         263936  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1
  67:         10618         254832  java.lang.Double
  68:          7921         253472  org.apache.cassandra.cache.ChunkCache$Buffer
  69:          7773         248736  org.apache.cassandra.cache.ChunkCache$Key
  70:         10296         247104  org.apache.cassandra.dht.Token$KeyBound
  71:          6096         243816  [Lorg.apache.cassandra.db.transform.Transformation;
  72:          6035         241400  org.apache.cassandra.db.rows.Row$Merger
  73:          6034         241360  org.apache.cassandra.db.rows.RangeTombstoneMarker$Merger
  74:          6034         241360  org.apache.cassandra.db.rows.Row$Merger$ColumnDataReducer
  75:          9969         239256  org.apache.cassandra.db.RowIndexEntry
  76:          9699         232776  java.net.Inet4Address
  77:          5750         230000  org.apache.cassandra.utils.concurrent.Ref$State
  78:         13690         219040  java.util.concurrent.atomic.AtomicInteger
  79:          9091         218184  org.apache.cassandra.gms.GossipDigest
  80:         12392         216040  [Ljava.lang.Class;
  81:          5289         211560  org.apache.cassandra.utils.MergeIterator$ManyToOne
  82:         13079         209264  java.lang.Object
  83:          5183         207320  org.apache.cassandra.repair.Validator$CountingDigest
  84:          8157         195768  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxGauge
  85:          6035         193120  org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator$MergeReducer
  86:          6023         192736  org.apache.cassandra.db.LivenessInfo$ExpiringLivenessInfo
  87:          5745         183840  com.google.common.collect.RegularImmutableList
  88:          6035         180640  [Lorg.apache.cassandra.db.rows.Row;
  89:          6034         180600  [Lorg.apache.cassandra.db.rows.RangeTombstoneMarker;
  90:          6033         180576  [Lorg.apache.cassandra.db.DeletionTime;
  91:          7464         179136  org.apache.cassandra.db.rows.BTreeRow$$Lambda$109/2102075500
  92:          5288         171488  [Lorg.apache.cassandra.utils.MergeIterator$Candidate;
  93:          5331         170592  com.google.common.collect.Iterators$11
  94:          5183         165856  java.security.MessageDigest$Delegate
  95:          5178         165696  com.google.common.collect.Iterators$7
  96:          5157         165024  org.apache.cassandra.utils.MerkleTree$RowHash
  97:           169         163280  [Lio.netty.util.Recycler$DefaultHandle;
  98:          2304         147456  io.netty.buffer.PoolSubpage
  99:          4608         147456  java.util.EnumMap$EntryIterator
 100:          6034         144816  org.apache.cassandra.db.rows.Row$Merger$CellReducer
 101:          1595         140360  java.lang.reflect.Method
 102:          2893         138864  java.util.TreeMap
 103:          5750         138000  org.apache.cassandra.utils.concurrent.Ref
 104:          8453         135248  org.apache.cassandra.db.rows.BTreeRow$Builder$CellResolver
 105:          5613         134712  java.util.concurrent.atomic.AtomicLong
 106:          5509         132216  org.apache.cassandra.utils.btree.BTree$FiltrationTracker
 107:          5179         124296  com.google.common.collect.Iterables$6
 108:          5179         124296  com.google.common.collect.Iterables$8
 109:          5179         124296  com.google.common.collect.Iterators$5
 110:          5179         124296  com.google.common.collect.Iterators$8
 111:          5177         124248  com.google.common.collect.Iterables$2
 112:          5159         123816  sun.security.jca.GetInstance$Instance
 113:          2577         123696  java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync
 114:          2399         115152  org.apache.cassandra.metrics.DecayingEstimatedHistogramReservoir
 115:          4643         111432  org.apache.cassandra.db.DeletionTime
 116:          4490         107760  org.apache.cassandra.db.Columns
 117:          2673         106920  java.util.EnumMap
 118:          4202         100848  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxCounter
 119:          6095          97520  org.apache.cassandra.db.transform.BaseIterator$Stop
 120:          4041          96984  java.util.concurrent.ConcurrentLinkedDeque
 121:          4033          96792  org.apache.cassandra.utils.concurrent.Ref$GlobalState
 122:          1882          90336  com.codahale.metrics.Meter
 123:          5596          89536  java.util.concurrent.atomic.AtomicLongArray
 124:          1845          88560  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxTimer
 125:          5179          82864  com.google.common.collect.Iterables$3
 126:          2050          82000  org.apache.cassandra.utils.btree.BTree$Builder
 127:          1111          71104  java.nio.DirectByteBufferR
 128:          1713          68520  java.util.LinkedHashMap$Entry
 129:          2115          67680  io.netty.util.Recycler$DefaultHandle
 130:          1687          67480  java.lang.ref.SoftReference
 131:          1519          66968  [Ljava.lang.String;
 132:          2724          65376  org.apache.cassandra.db.PartitionColumns
 133:          1598          63920  org.apache.cassandra.io.util.MmappedRegions$State
 134:          2572          61728  java.util.concurrent.locks.ReentrantReadWriteLock
 135:          3736          59776  java.util.concurrent.atomic.AtomicBoolean
 136:           154          59136  io.netty.util.concurrent.FastThreadLocalThread
 137:          1835          58720  org.apache.cassandra.utils.MergeIterator$TrivialOneToOne
 138:          1794          57408  org.apache.cassandra.gms.EndpointState
 139:           896          57344  org.apache.cassandra.config.ColumnDefinition
 140:          1385          55400  sun.misc.Cleaner
 141:          2302          55248  org.apache.cassandra.db.commitlog.CommitLogPosition
 142:          1713          54816  java.io.FileDescriptor
 143:           802          51328  sun.nio.ch.FileChannelImpl
 144:          2137          51288  org.apache.cassandra.db.rows.Row$Deletion
 145:           400          51200  org.apache.cassandra.io.sstable.format.big.BigTableReader
 146:          1584          50688  java.lang.StackTraceElement
 147:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node
 148:          1583          50656  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
 149:          1579          50528  java.lang.ref.WeakReference
 150:          1563          50016  org.apache.cassandra.io.util.Memory
 151:          1559          49888  java.util.concurrent.locks.ReentrantLock$NonfairSync
 152:            60          48760  [D
 153:           867          48552  java.lang.invoke.MemberName
 154:          1176          47040  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper
 155:          1176          47040  org.apache.cassandra.net.MessageIn
 156:          1938          46512  org.apache.cassandra.db.rows.ComplexColumnData
 157:          1157          46280  com.google.common.util.concurrent.AbstractFuture$Sync
 158:          1893          45432  java.util.concurrent.Executors$RunnableAdapter
 159:           400          44800  org.apache.cassandra.io.sstable.metadata.StatsMetadata
 160:           605          43560  java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask
 161:          2713          43408  com.codahale.metrics.Counter
 162:          1794          43056  org.apache.cassandra.gms.HeartBeatState
 163:          1033          41320  org.apache.cassandra.db.rows.BTreeRow$Builder$ComplexColumnDeletion
 164:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock
 165:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$ThreadLocalHoldCounter
 166:          2581          41296  java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock
 167:           616          39424  com.google.common.collect.MapMakerInternalMap$Segment
 168:          1611          38664  com.codahale.metrics.Histogram
 169:          1611          38664  com.codahale.metrics.Timer
 170:          2410          38560  java.util.concurrent.atomic.AtomicReference
 171:           601          38464  java.util.concurrent.ConcurrentHashMap
 172:          1601          38424  org.apache.cassandra.io.util.ChannelProxy
 173:          1587          38088  org.apache.cassandra.cache.KeyCacheKey
 174:          1583          37992  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
 175:           945          37800  org.apache.cassandra.metrics.LatencyMetrics
 176:          1557          37368  org.apache.cassandra.gms.VersionedValue
 177:          1157          37024  java.lang.ThreadLocal$ThreadLocalMap$Entry
 178:          1540          36960  java.util.concurrent.LinkedBlockingQueue$Node
 179:          1525          36600  org.apache.cassandra.repair.NodePair
 180:           151          36240  org.apache.cassandra.metrics.TableMetrics
 181:          1490          35760  java.util.concurrent.ConcurrentLinkedQueue$Node
 182:          2213          35408  java.util.TreeMap$KeySet
 183:           868          34720  java.util.HashMap$ValueIterator
 184:           863          34520  java.lang.invoke.MethodType
 185:           710          34080  org.apache.cassandra.metrics.RestorableMeter$RestorableEWMA
 186:           418          33696  [Ljava.lang.ThreadLocal$ThreadLocalMap$Entry;
 187:           809          32360  sun.nio.ch.FileChannelImpl$Unmapper
 188:          1344          32256  com.google.common.util.concurrent.ExecutionList
 189:          1342          32208  org.apache.cassandra.utils.Pair
 190:          2012          32192  java.lang.Integer
 191:           800          32000  org.apache.cassandra.io.util.FileHandle
 192:          1333          31992  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxHistogram
 193:          1324          31776  [Lorg.apache.cassandra.dht.Range;
 194:           948          30336  org.apache.cassandra.db.partitions.AbstractBTreePartition$Holder
 195:          1223          29352  java.lang.StringBuilder
 196:           898          28736  sun.security.util.DerInputBuffer
 197:           898          28736  sun.security.util.DerValue
 198:          1196          28704  javax.management.openmbean.CompositeDataSupport
 199:          1176          28224  org.apache.cassandra.concurrent.ExecutorLocals
 200:          1176          28224  org.apache.cassandra.net.MessageDeliveryTask
 201:           866          27712  java.lang.invoke.MethodType$ConcurrentWeakInternSet$WeakEntry
 202:          1143          27432  org.apache.cassandra.repair.SyncStat
 203:           685          27400  org.apache.cassandra.io.sstable.IndexInfo
 204:          1109          26616  org.apache.cassandra.utils.Interval
 205:           828          26496  org.apache.cassandra.utils.MergeIterator$OneToOne
 206:           816          26112  java.lang.ref.ReferenceQueue
 207:           800          25600  org.apache.cassandra.io.util.FileHandle$Cleanup
 208:           982          23568  java.util.Collections$UnmodifiableRandomAccessList
 209:           716          22912  org.apache.cassandra.db.context.CounterContext$ContextState
 210:           941          22584  org.apache.cassandra.utils.MerkleTrees
 211:           400          22400  org.apache.cassandra.io.compress.CompressionMetadata
 212:           400          22400  org.apache.cassandra.io.sstable.IndexSummary
 213:           400          22400  org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier
 214:           553          22120  org.apache.cassandra.db.SerializationHeader
 215:           389          21784  sun.nio.cs.UTF_8$Encoder
 216:           160          21760  io.netty.util.internal.InternalThreadLocalMap
 217:           898          21552  sun.security.util.DerInputStream
 218:           445          21360  org.apache.cassandra.repair.RepairJob
 219:           885          21240  [Lsun.security.x509.AVA;
 220:           885          21240  sun.security.x509.AVA
 221:           885          21240  sun.security.x509.RDN
 222:           878          21072  org.apache.cassandra.repair.TreeResponse
 223:           855          20520  java.util.concurrent.ConcurrentSkipListMap$Node
 224:           628          20096  java.util.Hashtable$Entry
 225:           349          20024  [Z
 226:           621          19872  java.io.File
 227:          1233          19728  java.util.TreeMap$Values
 228:          1212          19392  java.util.Optional
 229:           404          19392  org.apache.cassandra.io.sstable.Descriptor
 230:           604          19328  [Lcom.codahale.metrics.Histogram;
 231:           802          19248  sun.nio.ch.NativeThreadSet
 232:           801          19224  org.apache.cassandra.io.util.MmappedRegions
 233:           399          19152  org.apache.cassandra.io.sstable.format.big.BigFormat$BigVersion
 234:           798          19152  org.apache.cassandra.io.util.ChannelProxy$Cleanup
 235:           798          19152  org.apache.cassandra.utils.EstimatedHistogram
 236:           788          18912  org.apache.cassandra.metrics.ClearableHistogram
 237:           766          18384  com.google.common.collect.SingletonImmutableList
 238:           762          18288  org.apache.cassandra.gms.GossipDigestSyn
 239:           569          18208  java.nio.DirectByteBuffer$Deallocator
 240:           569          18208  org.apache.cassandra.db.filter.ColumnFilter
 241:           300          18000  [Ljava.lang.ref.SoftReference;
 242:           160          17920  org.apache.cassandra.config.CFMetaData
 243:           744          17856  java.util.concurrent.CopyOnWriteArrayList
 244:           442          17680  java.util.HashMap$EntryIterator
 245:           221          17680  org.apache.cassandra.io.sstable.format.big.BigTableScanner
 246:           225          17464  [Ljava.lang.StackTraceElement;
 247:          1084          17344  java.util.EnumMap$EntrySet
 248:           424          16960  org.apache.cassandra.utils.btree.NodeCursor
 249:            32          16896  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
 250:           300          16800  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
 251:             1          16400  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$Node;
 252:           512          16384  org.apache.cassandra.repair.RepairJobDesc
 253:           154          16016  com.google.common.collect.MapMakerInternalMap
 254:           500          16000  java.lang.invoke.DirectMethodHandle
 255:           400          16000  org.apache.cassandra.io.sstable.BloomFilterTracker
 256:           998          15968  org.antlr.runtime.BitSet
 257:           664          15936  com.google.common.collect.ImmutableMapEntry$TerminalEntry
 258:           398          15920  java.util.WeakHashMap$Entry
 259:           392          15680  java.lang.ref.Finalizer
 260:           325          15600  java.util.concurrent.ConcurrentSkipListMap
 261:           487          15584  org.apache.cassandra.schema.CompressionParams
 262:           485          15520  sun.security.util.ObjectIdentifier
 263:           483          15456  org.apache.cassandra.db.partitions.AtomicBTreePartition
 264:           161          15456  org.apache.cassandra.schema.TableParams
 265:           170          15440  [Ljava.util.WeakHashMap$Entry;
 266:           384          15360  io.netty.buffer.PoolChunkList
 267:           382          15280  org.apache.cassandra.repair.RemoteSyncTask
 268:           941          15056  org.apache.cassandra.utils.MerkleTrees$TokenRangeComparator
 269:           622          14928  java.util.Collections$1
 270:           622          14928  org.apache.cassandra.db.RowIndexEntry$Serializer
 271:           930          14880  java.util.concurrent.locks.ReentrantLock
 272:           464          14848  org.apache.cassandra.cql3.ColumnIdentifier
 273:           925          14800  java.util.HashSet
 274:           264          14784  java.util.LinkedHashMap
 275:           151          14496  org.apache.cassandra.db.ColumnFamilyStore
 276:           604          14496  org.apache.cassandra.metrics.TableMetrics$TableHistogram
 277:           301          14448  ch.qos.logback.classic.Logger
 278:           355          14200  org.apache.cassandra.metrics.RestorableMeter
 279:           442          14144  org.apache.cassandra.io.util.RandomAccessReader
 280:           430          14056  [Lcom.google.common.collect.ImmutableMapEntry;
 281:           433          13856  com.google.common.collect.MapMakerInternalMap$StrongEntry
 282:           433          13856  com.google.common.collect.MapMakerInternalMap$WeakValueReference
 283:           855          13680  java.nio.channels.spi.AbstractInterruptibleChannel$1
 284:            34          13600  org.apache.cassandra.net.IncomingTcpConnection
 285:           333          13320  com.google.common.collect.RegularImmutableSortedMap
 286:           818          13088  java.lang.ref.ReferenceQueue$Lock
 287:           201          12864  java.net.URL
 288:           803          12848  sun.nio.ch.FileDispatcherImpl
 289:           401          12832  org.apache.cassandra.utils.BloomFilter
 290:           200          12800  java.util.regex.Matcher
 291:           400          12800  org.apache.cassandra.cache.ChunkCache$CachingRebufferer
 292:           400          12800  org.apache.cassandra.io.util.CompressedChunkReader$Mmap
 293:           400          12800  org.apache.cassandra.io.util.MmapRebufferer
 294:           799          12784  org.apache.cassandra.io.util.MmappedRegions$Tidier
 295:           799          12784  org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy
 296:           399          12768  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy
 297:           797          12752  java.util.Collections$SingletonSet
 298:           396          12672  java.util.UUID
 299:           784          12544  java.util.HashMap$KeySet
 300:           521          12504  java.util.concurrent.ConcurrentLinkedQueue
 301:           154          12320  org.apache.cassandra.db.rows.RowAndDeletionMergeIterator
 302:           170          12240  java.lang.reflect.Field
 303:           507          12168  org.apache.cassandra.db.BufferDecoratedKey
 304:           151          12080  org.apache.cassandra.db.Memtable
 305:           302          12080  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategyOptions
 306:           376          12032  java.lang.invoke.LambdaForm$Name
 307:           213          11928  sun.security.ssl.CipherSuite
 308:            27          11880  org.apache.cassandra.net.OutboundTcpConnection
 309:           738          11808  java.util.HashMap$Values
 310:           208          11648  java.lang.Package
 311:           242          11616  org.apache.cassandra.utils.IntervalTree$IntervalNode
 312:           128          11264  [Lio.netty.buffer.PoolSubpage;
 313:           699          11184  java.util.HashMap$EntrySet
 314:           155          11160  org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater
 315:           344          11008  java.util.concurrent.ConcurrentSkipListMap$HeadIndex
 316:           341          10912  sun.misc.FDBigInteger
 317:           227          10896  sun.security.x509.X500Name
 318:           453          10872  org.apache.cassandra.utils.DefaultValue
 319:           333          10656  com.google.common.collect.RegularImmutableSortedSet
 320:           265          10600  java.util.Formatter$FormatSpecifier
 321:           263          10520  [Ljava.util.Formatter$Flags;
 322:           433          10392  org.apache.cassandra.cql3.ColumnIdentifier$InternedKey
 323:            72          10368  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong
 324:           324          10368  sun.security.x509.AlgorithmId
 325:           320          10240  io.netty.util.internal.chmv8.LongAdderV8
 326:           633          10128  java.util.concurrent.atomic.AtomicReferenceArray
 327:           180          10080  java.lang.invoke.MethodTypeForm
 328:           156           9984  io.netty.util.Recycler$Stack
 329:           416           9984  java.lang.ThreadLocal$ThreadLocalMap
 330:           622           9952  org.apache.cassandra.dht.Range$1
 331:           154           9856  org.apache.cassandra.cql3.UpdateParameters
 332:           244           9760  java.util.HashMap$KeyIterator
 333:           304           9728  java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
 334:           302           9664  org.apache.cassandra.metrics.TableMetrics$TableMetricNameFactory
 335:           302           9664  org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator
 336:           400           9600  [Lorg.apache.cassandra.io.util.Memory;
 337:           400           9600  org.apache.cassandra.utils.StreamingHistogram
 338:           399           9576  [Ljava.lang.AutoCloseable;
 339:            25           9400  java.lang.Thread
 340:           195           9360  org.apache.cassandra.net.MessageOut
 341:           292           9344  [Lcom.codahale.metrics.Timer;
 342:            16           9216  io.netty.util.internal.shaded.org.jctools.queues.MpscChunkedArrayQueue
 343:           381           9144  org.apache.cassandra.repair.RepairResult
 344:           362           8688  com.google.common.util.concurrent.ExecutionList$RunnableExecutorPair
 345:            68           8680  [Ljava.util.Hashtable$Entry;
 346:           271           8672  org.apache.cassandra.metrics.CassandraMetricsRegistry$JmxMeter
 347:           108           8640  sun.security.x509.X509CertImpl
 348:           269           8608  javax.management.MBeanAttributeInfo
 349:           215           8600  com.google.common.collect.RegularImmutableMap
 350:           215           8600  org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator
 351:           151           8456  org.apache.cassandra.db.compaction.CompactionStrategyManager
 352:           260           8320  javax.management.MBeanParameterInfo
 353:           142           7952  java.beans.MethodDescriptor
 354:           331           7944  java.util.Collections$SingletonList
 355:           494           7904  com.google.common.base.Present
 356:           164           7872  java.util.WeakHashMap
 357:           227           7768  [Lsun.security.x509.RDN;
 358:           483           7728  org.apache.cassandra.utils.CounterId
 359:           318           7632  java.util.Collections$SetFromMap
 360:           318           7632  java.util.Formatter$FixedString
 361:           156           7488  org.apache.cassandra.utils.concurrent.OpOrder$Group
 362:           187           7480  com.google.common.util.concurrent.ListenableFutureTask
 363:           308           7392  org.apache.cassandra.utils.btree.BTreeSet
 364:           306           7344  java.beans.MethodRef
 365:           304           7296  org.apache.cassandra.io.util.MmappedRegions$Region
 366:           302           7248  org.apache.cassandra.utils.TopKSampler
 367:           151           7248  org.apache.cassandra.utils.memory.SlabAllocator
 368:           148           7104  java.lang.invoke.LambdaForm
 369:           292           7008  org.apache.cassandra.metrics.TableMetrics$TableTimer
 370:           155           6904  [Ljava.lang.invoke.LambdaForm$Name;
 371:           121           6776  jdk.internal.org.objectweb.asm.Item
 372:           169           6760  java.security.AccessControlContext
 373:           280           6720  java.util.Date
 374:           168           6720  java.util.IdentityHashMap
 375:           209           6688  org.apache.cassandra.db.ClusteringComparator
 376:           278           6672  com.google.common.collect.ImmutableSortedAsList
 377:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet
 378:           278           6672  com.google.common.collect.RegularImmutableSortedMap$EntrySet$1
 379:           404           6464  java.util.concurrent.CopyOnWriteArraySet
 380:           200           6400  java.util.Formatter
 381:           400           6400  org.apache.cassandra.io.sstable.format.SSTableReader$UniqueIdentifier
 382:           399           6384  org.apache.cassandra.utils.obs.OffHeapBitSet
 383:            23           6368  [[S
 384:           394           6304  org.apache.cassandra.db.commitlog.IntervalSet
 385:           262           6288  java.util.concurrent.CopyOnWriteArrayList$COWIterator
 386:           156           6240  org.apache.cassandra.cql3.QueryOptions$DefaultQueryOptions
 387:           111           6216  sun.security.util.MemoryCache$SoftCacheEntry
 388:           155           6200  javax.management.MBeanOperationInfo
 389:           155           6200  org.apache.cassandra.db.Mutation
 390:           155           6200  org.apache.cassandra.db.partitions.PartitionUpdate
 391:           155           6200  org.apache.cassandra.utils.memory.AbstractAllocator$CloningBTreeRowBuilder
 392:           193           6176  org.apache.cassandra.net.OutboundTcpConnection$QueuedMessage
 393:           200           6160  [Ljava.util.Formatter$FormatString;
 394:           154           6160  java.util.Collections$SingletonMap
 395:           154           6160  org.apache.cassandra.db.rows.BTreeRow$$Lambda$122/418553968
 396:           154           6160  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$125/1196438970
 397:           152           6080  org.apache.cassandra.db.lifecycle.View
 398:           253           6072  java.util.concurrent.ConcurrentSkipListMap$Index
 399:           189           6048  org.apache.cassandra.repair.ValidationTask
 400:           108           6048  sun.security.x509.X509CertInfo
 401:           251           6024  javax.management.ImmutableDescriptor
 402:            62           5952  java.util.jar.JarFile$JarFileEntry
 403:            82           5904  java.beans.PropertyDescriptor
 404:           244           5856  org.apache.cassandra.db.rows.ComplexColumnData$$Lambda$111/177399658
 405:           243           5832  org.apache.cassandra.cql3.functions.FunctionName
 406:            52           5824  sun.nio.ch.SocketChannelImpl
 407:            90           5760  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$99/328488350
 408:           240           5736  [Lorg.apache.cassandra.db.marshal.AbstractType;
 409:           179           5728  org.apache.cassandra.auth.DataResource
 410:            89           5696  org.apache.cassandra.utils.btree.NodeBuilder
 411:           355           5680  org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1
 412:           229           5496  org.apache.cassandra.db.MutableDeletionInfo
 413:           227           5448  java.security.Provider$ServiceKey
 414:           224           5376  com.google.common.collect.SingletonImmutableSet
 415:            74           5328  ch.qos.logback.classic.spi.LoggingEvent
 416:            95           5320  java.security.Provider$Service
 417:           165           5280  java.lang.invoke.BoundMethodHandle$Species_L
 418:           106           5272  [Ljavax.management.MBeanAttributeInfo;
 419:           109           5232  java.util.concurrent.ThreadPoolExecutor$Worker
 420:           325           5200  org.apache.cassandra.utils.concurrent.WaitQueue
 421:           108           5184  javax.management.MBeanInfo
 422:           210           5040  com.google.common.collect.RegularImmutableAsList
 423:           210           5040  com.google.common.collect.RegularImmutableMap$EntrySet
 424:           208           4992  java.util.concurrent.ConcurrentHashMap$KeySetView
 425:           155           4960  org.apache.cassandra.db.commitlog.CommitLogSegment$Allocation
 426:           154           4928  [Lcom.google.common.collect.MapMakerInternalMap$Segment;
 427:           308           4928  org.apache.cassandra.db.Columns$$Lambda$121/617875913
 428:           154           4928  org.apache.cassandra.db.rows.EncodingStats$Collector
 429:           154           4928  org.apache.cassandra.io.util.DataOutputBufferFixed
 430:           102           4896  java.util.TimSort
 431:           152           4864  org.apache.cassandra.db.lifecycle.Tracker
 432:           202           4848  org.apache.cassandra.db.lifecycle.SSTableIntervalTree
 433:           121           4840  java.io.ObjectStreamField
 434:           151           4832  org.apache.cassandra.db.compaction.CompactionLogger
 435:            99           4752  javax.management.Notification
 436:           198           4752  org.apache.cassandra.db.ClusteringBound
 437:           198           4752  org.apache.cassandra.db.rows.ComplexColumnData$Builder
 438:           180           4744  [Ljava.security.ProtectionDomain;
 439:            63           4536  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterator
 440:            40           4480  java.net.SocksSocketImpl
 441:           275           4400  java.util.Formatter$Flags
 442:           273           4368  java.lang.Byte
 443:            32           4352  io.netty.buffer.PoolArena$DirectArena
 444:            32           4352  io.netty.buffer.PoolArena$HeapArena
 445:           181           4344  java.lang.invoke.LambdaForm$NamedFunction
 446:             6           4320  [Ljdk.internal.org.objectweb.asm.Item;
 447:            90           4320  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$313/480779282
 448:           108           4320  org.apache.cassandra.db.CachedHashDecoratedKey
 449:           178           4272  org.apache.cassandra.gms.GossipDigestAck
 450:           177           4248  java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
 451:           131           4192  com.sun.jmx.mbeanserver.ConvertingMethod
 452:           128           4096  java.lang.NoSuchMethodException
 453:           256           4096  java.lang.Short
 454:            70           3920  sun.misc.URLClassPath$JarLoader
 455:            60           3840  java.util.jar.JarFile
 456:            80           3840  java.util.logging.LogManager$LoggerWeakRef
 457:           160           3840  org.apache.cassandra.db.Serializers
 458:           160           3840  org.apache.cassandra.db.Serializers$NewFormatSerializer
 459:           160           3840  org.apache.cassandra.io.sstable.IndexInfo$Serializer
 460:           160           3840  org.apache.cassandra.schema.Indexes
 461:            53           3816  java.util.regex.Pattern
 462:            95           3800  sun.security.rsa.RSAPublicKeyImpl
 463:           158           3792  com.sun.jmx.mbeanserver.PerInterface$MethodAndSig
 464:            59           3776  java.text.DateFormatSymbols
 465:           155           3720  org.apache.cassandra.utils.memory.ContextAllocator
 466:           154           3696  [Lorg.apache.cassandra.db.Directories$DataDirectory;
 467:           154           3696  com.google.common.collect.Collections2$TransformedCollection
 468:           154           3696  org.apache.cassandra.cql3.statements.UpdatesCollector
 469:           154           3696  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter
 470:           154           3696  org.apache.cassandra.db.rows.Rows$$Lambda$120/877468788
 471:           151           3624  [Ljava.io.File;
 472:           151           3624  org.apache.cassandra.db.Directories
 473:           151           3624  org.apache.cassandra.db.Memtable$ColumnsCollector
 474:           151           3624  org.apache.cassandra.index.SecondaryIndexManager
 475:           151           3624  org.apache.cassandra.metrics.TableMetrics$10
 476:           151           3624  org.apache.cassandra.metrics.TableMetrics$11
 477:           151           3624  org.apache.cassandra.metrics.TableMetrics$12
 478:           151           3624  org.apache.cassandra.metrics.TableMetrics$14
 479:           151           3624  org.apache.cassandra.metrics.TableMetrics$15
 480:           151           3624  org.apache.cassandra.metrics.TableMetrics$16
 481:           151           3624  org.apache.cassandra.metrics.TableMetrics$17
 482:           151           3624  org.apache.cassandra.metrics.TableMetrics$19
 483:           151           3624  org.apache.cassandra.metrics.TableMetrics$2
 484:           151           3624  org.apache.cassandra.metrics.TableMetrics$21
 485:           151           3624  org.apache.cassandra.metrics.TableMetrics$23
 486:           151           3624  org.apache.cassandra.metrics.TableMetrics$24
 487:           151           3624  org.apache.cassandra.metrics.TableMetrics$25
 488:           151           3624  org.apache.cassandra.metrics.TableMetrics$27
 489:           151           3624  org.apache.cassandra.metrics.TableMetrics$29
 490:           151           3624  org.apache.cassandra.metrics.TableMetrics$3
 491:           151           3624  org.apache.cassandra.metrics.TableMetrics$30
 492:           151           3624  org.apache.cassandra.metrics.TableMetrics$31
 493:           151           3624  org.apache.cassandra.metrics.TableMetrics$32
 494:           151           3624  org.apache.cassandra.metrics.TableMetrics$33
 495:           151           3624  org.apache.cassandra.metrics.TableMetrics$34
 496:           151           3624  org.apache.cassandra.metrics.TableMetrics$4
 497:           151           3624  org.apache.cassandra.metrics.TableMetrics$5
 498:           151           3624  org.apache.cassandra.metrics.TableMetrics$6
 499:           151           3624  org.apache.cassandra.metrics.TableMetrics$7
 500:           151           3624  org.apache.cassandra.metrics.TableMetrics$8
 501:           151           3624  org.apache.cassandra.metrics.TableMetrics$9
 502:           113           3616  [Lorg.apache.cassandra.utils.memory.BufferPool$Chunk;
 503:           113           3616  org.apache.cassandra.utils.memory.BufferPool$LocalPoolRef
 504:           225           3600  org.apache.cassandra.cql3.FieldIdentifier
 505:           149           3576  org.apache.cassandra.cql3.restrictions.RestrictionSet
 506:           221           3536  java.util.zip.CRC32
 507:            63           3528  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController
 508:            63           3528  org.apache.cassandra.repair.Validator
 509:            12           3480  [Ljava.util.concurrent.RunnableScheduledFuture;
 510:           108           3456  java.util.Collections$SynchronizedMap
 511:           143           3432  com.google.common.util.concurrent.Futures$CombinedFuture$2
 512:           143           3432  java.util.LinkedList$Node
 513:           107           3424  java.io.IOException
 514:            37           3384  [Lorg.apache.cassandra.io.sstable.IndexInfo;
 515:            60           3360  org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
 516:           122           3344  [Ljavax.management.MBeanParameterInfo;
 517:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$31/1914108708
 518:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$32/1889757798
 519:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$33/1166106620
 520:           209           3344  org.apache.cassandra.db.ClusteringComparator$$Lambda$34/221861886
 521:            41           3328  [Ljava.lang.invoke.MethodHandle;
 522:            32           3328  java.io.ObjectStreamClass
 523:           208           3328  org.apache.cassandra.utils.concurrent.Refs
 524:            69           3312  com.google.common.util.concurrent.Futures$CombinedFuture
 525:           103           3296  org.apache.cassandra.schema.CompactionParams
 526:           137           3288  java.util.ArrayDeque
 527:            24           3264  com.codahale.metrics.Striped64$Cell
 528:           203           3248  org.apache.cassandra.io.util.DataOutputBuffer$GrowingChannel
 529:           135           3240  com.sun.jmx.remote.internal.ArrayNotificationBuffer$NamedNotification
 530:           101           3232  java.util.Vector
 531:           101           3232  org.apache.cassandra.schema.SpeculativeRetryParam
 532:           132           3168  org.apache.cassandra.db.view.TableViews
 533:            79           3160  com.google.common.collect.SingletonImmutableBiMap
 534:            98           3136  org.xml.sax.helpers.LocatorImpl
 535:            98           3136  sun.security.x509.BasicConstraintsExtension
 536:            78           3120  java.security.ProtectionDomain
 537:           129           3096  com.google.common.collect.RegularImmutableMap$NonTerminalMapEntry
 538:            77           3080  sun.nio.cs.UTF_8$Decoder
 539:            64           3072  org.apache.cassandra.db.compaction.CompactionIterator$Purger
 540:            64           3072  org.apache.cassandra.db.transform.UnfilteredPartitions
 541:            96           3072  sun.security.x509.SubjectKeyIdentifierExtension
 542:            24           3032  [Ljava.beans.MethodDescriptor;
 543:            92           3024  [Ljavax.management.MBeanOperationInfo;
 544:            94           3008  java.util.AbstractList$Itr
 545:            91           2912  com.codahale.metrics.Timer$Context
 546:           121           2904  org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate
 547:            60           2880  java.util.zip.Inflater
 548:            45           2880  javax.management.openmbean.OpenMBeanAttributeInfoSupport
 549:           118           2832  java.util.regex.Pattern$1
 550:           118           2832  sun.reflect.generics.tree.SimpleClassTypeSignature
 551:            88           2816  sun.security.x509.KeyUsageExtension
 552:           175           2800  org.apache.cassandra.gms.GossipDigestAck2
 553:           113           2712  org.apache.cassandra.utils.memory.BufferPool$LocalPool
 554:            37           2664  java.util.logging.Logger
 555:           111           2664  sun.security.util.Cache$EqualByteArray
 556:            55           2640  java.util.Hashtable
 557:           163           2608  java.util.IdentityHashMap$KeySet
 558:           162           2592  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable
 559:           108           2592  org.apache.cassandra.dht.LocalPartitioner$LocalToken
 560:            18           2592  sun.reflect.MethodAccessorGenerator
 561:           108           2592  sun.security.util.BitArray
 562:           108           2592  sun.security.x509.CertificateValidity
 563:           138           2584  [Lcom.sun.jmx.mbeanserver.MXBeanMapping;
 564:           107           2568  java.net.InetSocketAddress$InetSocketAddressHolder
 565:            64           2560  com.google.common.collect.Multimaps$UnmodifiableMultimap
 566:            64           2560  java.util.ArrayList$SubList
 567:            64           2560  java.util.ArrayList$SubList$1
 568:            64           2560  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1
 569:           160           2560  org.apache.cassandra.schema.Triggers
 570:            64           2560  org.apache.cassandra.utils.OverlapIterator
 571:            53           2544  java.util.concurrent.LinkedBlockingQueue
 572:           155           2480  org.apache.cassandra.utils.btree.UpdateFunction$Simple
 573:           155           2480  org.apache.cassandra.utils.concurrent.OpOrder
 574:            44           2464  java.lang.Class$ReflectionData
 575:           154           2464  java.util.concurrent.ConcurrentSkipListSet
 576:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$117/1004624941
 577:           154           2464  org.apache.cassandra.db.partitions.PartitionUpdate$$Lambda$119/1364111969
 578:           154           2464  org.apache.cassandra.utils.WrappedBoolean
 579:           102           2448  org.apache.cassandra.schema.CachingParams
 580:            76           2432  java.security.CodeSource
 581:           151           2416  org.apache.cassandra.db.Memtable$StatsCollector
 582:           151           2416  org.apache.cassandra.utils.memory.EnsureOnHeap$NoOp
 583:            75           2400  java.util.LinkedList
 584:            50           2400  org.apache.cassandra.cql3.restrictions.StatementRestrictions
 585:            99           2376  sun.security.x509.CertificateExtensions
 586:            74           2368  java.io.ObjectStreamClass$WeakClassKey
 587:            98           2352  java.lang.Class$AnnotationData
 588:           147           2352  java.util.concurrent.ConcurrentHashMap$ValuesView
 589:            98           2352  java.util.jar.Attributes$Name
 590:            73           2336  java.util.regex.Pattern$Curly
 591:            97           2328  com.google.common.collect.ImmutableMapKeySet
 592:            48           2304  com.google.common.collect.HashMultimap
 593:            96           2304  com.google.common.collect.ImmutableMapKeySet$1
 594:            16           2304  io.netty.channel.epoll.EpollEventLoop
 595:           144           2304  org.apache.cassandra.db.ColumnFamilyStore$3
 596:            96           2304  org.apache.cassandra.metrics.KeyspaceMetrics$17
 597:            72           2304  sun.reflect.ClassFileAssembler
 598:            70           2240  java.util.concurrent.ConcurrentHashMap$ReservationNode
 599:            70           2240  java.util.logging.LogManager$LogNode
 600:            70           2240  org.apache.cassandra.utils.MerkleTree$TreeRangeIterator
 601:            91           2200  [Lcom.github.benmanes.caffeine.cache.RemovalCause;
 602:            91           2184  com.github.benmanes.caffeine.SingleConsumerQueue$Node
 603:            39           2184  org.apache.cassandra.db.marshal.UserType
 604:            90           2160  [Lcom.github.benmanes.caffeine.cache.Node;
 605:           118           2160  [Lsun.reflect.generics.tree.TypeArgument;
 606:            90           2160  com.github.benmanes.caffeine.cache.BoundedLocalCache$AddTask
 607:            90           2160  java.lang.StringBuffer
 608:            67           2144  java.util.TreeMap$ValueIterator
 609:            89           2136  java.lang.RuntimePermission
 610:            89           2136  org.apache.cassandra.io.compress.CompressionMetadata$Chunk
 611:            53           2120  sun.security.ec.NamedCurve
 612:            66           2112  java.io.FilePermission
 613:            66           2112  java.util.zip.ZipCoder
 614:            52           2080  sun.nio.ch.SocketAdaptor
 615:            37           2072  javax.management.MBeanServerNotification
 616:            37           2072  org.apache.cassandra.db.RowIndexEntry$IndexedEntry
 617:            86           2064  javax.management.openmbean.TabularDataSupport
 618:           129           2064  sun.security.x509.KeyIdentifier
 619:            64           2048  com.google.common.util.concurrent.Futures$ChainingListenableFuture
 620:           128           2048  java.lang.Character
 621:            64           2048  org.apache.cassandra.db.partitions.PurgeFunction$$Lambda$104/2021147872
 622:            64           2048  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2
 623:            64           2048  sun.misc.FloatingDecimal$ASCIIToBinaryBuffer
 624:            84           2016  java.security.Provider$UString
 625:            18           2016  java.util.GregorianCalendar
 626:            62           1984  org.apache.cassandra.utils.MerkleTrees$TreeRangeIterator
 627:            27           1944  sun.reflect.DelegatingClassLoader
 628:           120           1920  com.codahale.metrics.Striped64$HashCode
 629:            80           1920  java.util.regex.Pattern$GroupTail
 630:            34           1904  org.apache.cassandra.cql3.statements.SelectStatement
 631:            79           1896  com.google.common.collect.ImmutableList$1
 632:            79           1896  java.util.regex.Pattern$GroupHead
 633:            59           1888  java.util.RegularEnumSet
 634:           118           1888  sun.reflect.generics.tree.ClassTypeSignature
 635:           118           1888  sun.security.x509.SerialNumber
 636:            13           1872  java.text.DecimalFormat
 637:            39           1872  sun.util.locale.LocaleObjectCache$CacheEntry
 638:            10           1832  [[B
 639:            57           1824  org.apache.cassandra.cql3.functions.CastFcts$JavaFunctionWrapper
 640:            75           1800  java.util.regex.Pattern$Single
 641:            56           1792  java.lang.Throwable
 642:             8           1792  jdk.internal.org.objectweb.asm.MethodWriter
 643:            74           1776  com.google.common.util.concurrent.Futures$6
 644:           111           1776  java.util.LinkedHashMap$LinkedValues
 645:            44           1760  java.io.ObjectStreamClass$FieldReflectorKey
 646:            36           1728  org.apache.cassandra.concurrent.SEPWorker
 647:            72           1728  sun.reflect.ByteVectorImpl
 648:           108           1728  sun.security.x509.CertificateAlgorithmId
 649:           108           1728  sun.security.x509.CertificateSerialNumber
 650:           108           1728  sun.security.x509.CertificateVersion
 651:           108           1728  sun.security.x509.CertificateX509Key
 652:            18           1728  sun.util.calendar.Gregorian$Date
 653:           107           1712  java.net.InetSocketAddress
 654:             4           1696  [Ljava.lang.Thread;
 655:            53           1696  java.security.spec.EllipticCurve
 656:            30           1688  [Ljava.lang.reflect.Method;
 657:             6           1680  java.util.concurrent.ConcurrentHashMap$CounterCell
 658:            52           1664  java.lang.invoke.DirectMethodHandle$Special
 659:            52           1664  sun.nio.ch.SocketAdaptor$SocketInputStream
 660:            68           1632  org.apache.cassandra.cql3.Constants$Marker
 661:            68           1632  sun.reflect.NativeConstructorAccessorImpl
 662:           101           1616  org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/673586830
 663:            40           1600  ch.qos.logback.core.joran.event.StartEvent
 664:            40           1600  com.sun.jmx.mbeanserver.PerInterface
 665:            40           1600  sun.management.DiagnosticCommandArgumentInfo
 666:            99           1584  org.apache.cassandra.db.marshal.AbstractType$$Lambda$4/495702238
 667:            49           1568  java.io.DataOutputStream
 668:            49           1568  java.nio.channels.Channels$1
 669:            65           1560  java.security.spec.ECPoint
 670:            39           1560  org.apache.cassandra.io.util.SafeMemory
 671:            65           1560  org.apache.cassandra.utils.btree.TreeBuilder
 672:            64           1536  org.apache.cassandra.db.compaction.CompactionIterator$GarbageSkipper
 673:            63           1512  com.google.common.util.concurrent.Futures$1
 674:            63           1512  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$EQRestriction
 675:            63           1512  org.apache.cassandra.db.compaction.CompactionManager$13
 676:            47           1504  org.apache.cassandra.cql3.statements.ParsedStatement$Prepared
 677:            47           1504  org.apache.cassandra.io.util.DataOutputBuffer$1$1
 678:            93           1488  java.util.Collections$UnmodifiableSet
 679:            61           1464  java.util.regex.Pattern$Slice
 680:            60           1440  java.util.zip.ZStreamRef
 681:            51           1408  [Ljava.io.ObjectStreamField;
 682:            16           1392  [Ljava.lang.Byte;
 683:             1           1376  [Lsun.misc.FDBigInteger;
 684:            43           1376  java.util.regex.Pattern$Branch
 685:            43           1376  org.apache.cassandra.concurrent.NamedThreadFactory
 686:            34           1360  ch.qos.logback.core.status.InfoStatus
 687:            17           1360  java.net.URI
 688:            34           1360  org.apache.cassandra.cql3.selection.Selection$SimpleSelection
 689:            61           1352  [Ljava.lang.reflect.Type;
 690:            24           1344  java.util.ResourceBundle$CacheKey
 691:            24           1344  javax.management.openmbean.CompositeType
 692:            72           1336  [Ljavax.management.openmbean.CompositeData;
 693:            33           1320  sun.security.x509.AuthorityKeyIdentifierExtension
 694:            79           1312  [Ljava.security.Principal;
 695:            54           1296  ch.qos.logback.classic.spi.StackTraceElementProxy
 696:            23           1288  java.net.SocketPermission
 697:            39           1280  [Ljava.math.BigInteger;
 698:            40           1280  ch.qos.logback.core.joran.event.EndEvent
 699:            16           1280  com.google.common.cache.LocalCache$Segment
 700:            20           1280  org.apache.cassandra.db.RowIndexEntry$ShallowIndexedEntry
 701:            43           1272  [Ljava.util.regex.Pattern$Node;
 702:            53           1272  sun.nio.ch.Util$BufferCache
 703:            79           1264  java.security.ProtectionDomain$Key
 704:            39           1248  java.lang.Thread$WeakClassKey
 705:            38           1240  [Ljava.lang.reflect.Field;
 706:            14           1232  org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor
 707:            38           1216  java.security.Permissions
 708:            50           1200  org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions
 709:            50           1200  org.apache.cassandra.cql3.restrictions.IndexRestrictions
 710:            25           1200  org.apache.cassandra.metrics.ClientRequestMetrics
 711:             2           1184  [Lcom.github.benmanes.caffeine.cache.NodeFactory;
 712:            37           1184  java.net.Socket
 713:            49           1176  org.apache.cassandra.cql3.restrictions.PartitionKeySingleRestrictionSet
 714:            21           1176  sun.util.calendar.ZoneInfo
 715:            52           1168  [Lorg.apache.cassandra.cql3.ColumnSpecification;
 716:            24           1152  java.beans.BeanDescriptor
 717:            24           1152  java.lang.management.MemoryUsage
 718:            72           1152  org.apache.cassandra.db.ColumnFamilyStore$1
 719:            36           1152  org.apache.cassandra.io.util.SafeMemory$MemoryTidy
 720:            24           1152  org.hyperic.sigar.FileSystem
 721:            36           1152  sun.reflect.generics.repository.ClassRepository
 722:            20           1120  javax.management.openmbean.ArrayType
 723:            35           1120  org.apache.cassandra.cql3.ResultSet$ResultMetadata
 724:            69           1104  com.google.common.util.concurrent.Futures$8
 725:            69           1104  com.google.common.util.concurrent.Futures$CombinedFuture$1
 726:            46           1104  org.apache.cassandra.metrics.DefaultNameFactory
 727:            69           1104  sun.reflect.DelegatingConstructorAccessorImpl
 728:             3           1080  [Ljava.lang.Integer;
 729:            27           1080  com.google.common.collect.HashBiMap$BiEntry
 730:            27           1080  org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy
 731:            45           1080  sun.reflect.generics.factory.CoreReflectionFactory
 732:            24           1064  [Ljava.beans.PropertyDescriptor;
 733:             2           1056  [Ljava.lang.Long;
 734:             2           1056  [Ljava.lang.Short;
 735:            26           1040  java.math.BigDecimal
 736:            43           1032  io.netty.channel.ChannelOption
 737:            43           1032  java.io.ExpiringCache$Entry
 738:            64           1024  org.apache.cassandra.db.compaction.AbstractCompactionStrategy$ScannerList
 739:            64           1024  org.apache.cassandra.db.compaction.CompactionIterator$1
 740:            64           1024  org.apache.cassandra.repair.RepairJob$3
 741:            63           1008  org.apache.cassandra.repair.RepairJob$2
 742:            12            960  [Lcom.google.common.collect.HashBiMap$BiEntry;
 743:            24            960  java.beans.GenericBeanInfo
 744:            30            960  java.security.Provider$EngineDescription
 745:            40            960  java.util.regex.Pattern$BitClass
 746:            20            960  org.antlr.runtime.CommonToken
 747:            30            960  org.apache.cassandra.cql3.ColumnSpecification
 748:            40            960  org.apache.cassandra.cql3.statements.SelectStatement$Parameters
 749:            60            960  org.cliffc.high_scale_lib.Counter
 750:            20            960  org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
 751:            40            960  org.codehaus.jackson.map.type.ClassKey
 752:            40            960  org.xml.sax.helpers.AttributesImpl
 753:            46            944  [Lsun.reflect.generics.tree.FormalTypeParameter;
 754:            39            936  java.util.regex.Pattern$5
 755:             8            928  [Lorg.apache.cassandra.db.ClusteringBound;
 756:            29            928  java.security.BasicPermissionCollection
 757:            29            928  org.apache.cassandra.io.util.DataInputPlus$DataInputStreamPlus
 758:            23            920  org.codehaus.jackson.map.type.SimpleType
 759:            19            912  sun.management.DiagnosticCommandInfo
 760:            28            896  java.io.DataInputStream
 761:            18            864  net.jpountz.lz4.LZ4BlockOutputStream
 762:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$26/843299092
 763:            54            864  org.apache.cassandra.config.ColumnDefinition$$Lambda$27/605982374
 764:            54            864  org.apache.cassandra.config.ColumnDefinition$1
 765:            18            864  org.apache.cassandra.utils.SlidingTimeRate
 766:            36            864  sun.reflect.Label$PatchInfo
 767:            27            864  sun.reflect.generics.reflectiveObjects.TypeVariableImpl
 768:            36            864  sun.reflect.generics.tree.ClassSignature
 769:            44            856  [Ljavax.management.MBeanConstructorInfo;
 770:            21            840  com.sun.jmx.mbeanserver.MXBeanSupport
 771:            35            840  net.jpountz.xxhash.StreamingXXHash32JNI
 772:            35            840  sun.reflect.generics.scope.ClassScope
 773:            21            840  sun.util.locale.BaseLocale$Key
 774:             2            832  [Lorg.antlr.runtime.BitSet;
 775:            13            832  com.google.common.util.concurrent.SmoothRateLimiter$SmoothBursty
 776:            13            832  java.text.DecimalFormatSymbols
 777:            38            824  [Lsun.reflect.generics.tree.FieldTypeSignature;
 778:            34            816  org.apache.cassandra.cql3.selection.SelectionColumnMapping
 779:             6            816  org.apache.cassandra.metrics.KeyspaceMetrics
 780:            25            800  java.util.PropertyPermission
 781:            20            800  org.cliffc.high_scale_lib.NonBlockingHashMap
 782:            14            784  java.util.HashMap$TreeNode
 783:            14            784  org.apache.cassandra.cql3.statements.UpdateStatement
 784:            32            768  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$IdentityMapping
 785:            32            768  io.netty.channel.unix.FileDescriptor
 786:            16            768  java.util.ResourceBundle$BundleReference
 787:            24            768  java.util.ResourceBundle$LoaderReference
 788:            16            768  net.jpountz.lz4.LZ4BlockInputStream
 789:            32            768  org.apache.cassandra.cql3.functions.CastFcts$CastAsTextFunction
 790:            32            768  sun.reflect.generics.reflectiveObjects.ParameterizedTypeImpl
 791:            24            768  sun.security.x509.OIDMap$OIDInfo
 792:            23            736  javax.management.MBeanConstructorInfo
 793:            23            736  sun.management.MappedMXBeanType$BasicMXBeanType
 794:            30            720  com.google.common.collect.ImmutableEntry
 795:            30            720  java.io.ObjectStreamClass$EntryFuture
 796:            15            720  java.lang.management.PlatformComponent
 797:             9            720  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor
 798:             9            720  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
 799:             1            720  org.apache.cassandra.config.Config
 800:            18            720  org.apache.cassandra.metrics.ThreadPoolMetrics
 801:            22            704  com.sun.jmx.mbeanserver.WeakIdentityHashMap$IdentityWeakReference
 802:            11            704  java.text.SimpleDateFormat
 803:            29            696  org.apache.cassandra.net.MessagingService$Verb
 804:            36            688  [Lsun.reflect.generics.tree.ClassTypeSignature;
 805:            43            688  java.util.regex.Pattern$BranchConn
 806:            17            680  sun.reflect.UnsafeQualifiedStaticLongFieldAccessorImpl
 807:            29            672  [Ljava.lang.reflect.TypeVariable;
 808:            28            672  ch.qos.logback.core.spi.ContextAwareBase
 809:            28            672  java.util.regex.Pattern$Ctype
 810:            28            672  java.util.regex.Pattern$Start
 811:             4            672  jdk.internal.org.objectweb.asm.ClassWriter
 812:            42            672  org.apache.cassandra.config.ColumnDefinition$Raw$Literal
 813:            42            672  org.apache.cassandra.io.sstable.format.big.BigTableScanner$EmptySSTableScanner
 814:            28            672  sun.nio.ch.SocketOptionRegistry$RegistryKey
 815:            12            672  sun.security.ssl.CipherSuite$BulkCipher
 816:            41            656  ch.qos.logback.core.joran.spi.ElementPath
 817:            27            648  java.io.FilePermissionCollection
 818:            27            648  org.apache.cassandra.cql3.selection.RawSelector
 819:            27            648  sun.reflect.generics.tree.FormalTypeParameter
 820:            16            640  io.netty.util.collection.IntObjectHashMap
 821:             8            640  java.util.concurrent.ThreadPoolExecutor
 822:            40            640  java.util.jar.Attributes
 823:             8            640  java.util.zip.ZipEntry
 824:            10            640  jdk.internal.org.objectweb.asm.Label
 825:            20            640  org.apache.cassandra.cql3.functions.BytesConversionFcts$2
 826:            20            640  org.apache.cassandra.db.compaction.OperationType
 827:             3            624  [Ljava.lang.invoke.LambdaForm;
 828:            13            624  java.nio.HeapCharBuffer
 829:            26            624  java.security.spec.ECFieldF2m
 830:            26            624  java.util.regex.Pattern$Ques
 831:            39            624  org.apache.cassandra.serializers.TupleSerializer
 832:            39            624  org.apache.cassandra.serializers.UserTypeSerializer
 833:            27            616  [Ljava.lang.reflect.Constructor;
 834:            19            608  java.io.FileInputStream
 835:            19            608  java.rmi.server.UID
 836:            19            608  java.util.Locale
 837:            19            608  org.apache.cassandra.schema.IndexMetadata
 838:            19            608  sun.management.DiagnosticCommandImpl$Wrapper
 839:            19            608  sun.util.locale.BaseLocale
 840:            15            600  java.lang.ClassNotFoundException
 841:            25            600  java.lang.invoke.Invokers
 842:            25            600  java.util.concurrent.locks.ReentrantReadWriteLock$Sync$HoldCounter
 843:            25            600  org.apache.cassandra.gms.ApplicationState
 844:            25            600  sun.reflect.NativeMethodAccessorImpl
 845:            25            600  sun.reflect.annotation.AnnotationInvocationHandler
 846:            18            576  ch.qos.logback.core.joran.event.BodyEvent
 847:            12            576  java.io.ObjectInputStream$FilterValues
 848:            24            576  jdk.internal.org.objectweb.asm.ByteVector
 849:            12            576  org.apache.cassandra.db.marshal.MapType
 850:             9            576  org.apache.cassandra.metrics.ConnectionMetrics
 851:            24            576  org.apache.cassandra.metrics.ThreadPoolMetricNameFactory
 852:            35            560  ch.qos.logback.core.joran.spi.ElementSelector
 853:            14            560  io.netty.util.Recycler$WeakOrderQueue
 854:            10            560  java.util.zip.ZipFile$ZipFileInflaterInputStream
 855:            10            560  java.util.zip.ZipFile$ZipFileInputStream
 856:            14            560  javax.management.openmbean.SimpleType
 857:            10            560  sun.invoke.util.Wrapper
 858:            23            552  [Ljava.net.InetAddress;
 859:             3            552  [Lorg.apache.cassandra.net.MessagingService$Verb;
 860:            23            552  ch.qos.logback.core.pattern.LiteralConverter
 861:            23            552  io.netty.util.internal.logging.Slf4JLogger
 862:            23            552  org.codehaus.jackson.map.SerializationConfig$Feature
 863:             2            544  [Ljava.lang.Character;
 864:            17            544  io.netty.util.concurrent.DefaultPromise
 865:            34            544  java.io.FilePermission$1
 866:            17            544  java.nio.channels.ClosedChannelException
 867:            17            544  java.util.concurrent.atomic.AtomicIntegerFieldUpdater$AtomicIntegerFieldUpdaterImpl
 868:            34            544  net.jpountz.xxhash.StreamingXXHash32$1
 869:            17            544  org.apache.cassandra.transport.Message$Type
 870:            17            544  sun.reflect.MethodAccessorGenerator$1
 871:            17            544  sun.security.x509.DistributionPoint
 872:            17            544  sun.security.x509.URIName
 873:            22            528  java.net.URLClassLoader$1
 874:            22            528  org.apache.cassandra.cql3.CQL3Type$Native
 875:            33            528  sun.reflect.DelegatingMethodAccessorImpl
 876:            13            520  com.google.common.base.Stopwatch
 877:            13            520  io.netty.channel.unix.Errors$NativeIoException
 878:            13            520  java.lang.invoke.MethodHandleImpl$IntrinsicMethodHandle
 879:            13            520  java.text.DigitList
 880:             4            512  com.google.common.cache.LocalCache
 881:            16            512  io.netty.channel.epoll.IovArray
 882:            16            512  java.lang.NoSuchFieldException
 883:            32            512  java.util.TreeSet
 884:            16            512  java.util.concurrent.Semaphore$NonfairSync
 885:            16            512  sun.security.ssl.CipherSuite$KeyExchange
 886:            21            504  java.util.Locale$LocaleKey
 887:             9            504  java.util.concurrent.ConcurrentHashMap$ValueIterator
 888:            21            504  org.apache.cassandra.cql3.functions.AggregateFcts$24
 889:             9            504  org.apache.cassandra.net.RateBasedBackPressureState
 890:            21            504  sun.security.x509.AVAKeyword
 891:            31            496  sun.security.x509.GeneralName
 892:            19            488  [Lsun.management.DiagnosticCommandArgumentInfo;
 893:            20            480  java.io.ObjectStreamClass$2
 894:            12            480  java.lang.UNIXProcess$ProcessPipeInputStream
 895:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$22
 896:            20            480  org.apache.cassandra.cql3.functions.AggregateFcts$23
 897:            20            480  org.apache.cassandra.cql3.functions.BytesConversionFcts$1
 898:            20            480  org.apache.cassandra.dht.LocalPartitioner
 899:            15            480  org.apache.cassandra.index.internal.composites.RegularColumnIndex
 900:             6            480  org.apache.cassandra.repair.RepairSession
 901:            20            480  org.yaml.snakeyaml.tokens.Token$ID
 902:             6            480  sun.net.www.protocol.jar.URLJarFile
 903:            30            480  sun.security.x509.GeneralNames
 904:             6            456  [Lsun.invoke.util.Wrapper;
 905:            19            456  ch.qos.logback.classic.spi.ClassPackagingData
 906:            19            456  java.lang.Class$1
 907:            19            456  java.util.regex.Pattern$Dollar
 908:             5            448  [[Ljava.lang.Object;
 909:             7            448  java.security.SecureRandom
 910:            28            448  java.util.LinkedHashSet
 911:             8            448  javax.management.openmbean.OpenMBeanParameterInfoSupport
 912:             8            448  jdk.internal.org.objectweb.asm.AnnotationWriter
 913:            14            448  jdk.internal.org.objectweb.asm.Type
 914:            14            448  sun.security.x509.CRLDistributionPointsExtension
 915:            11            440  java.lang.ClassLoader$NativeLibrary
 916:            11            440  sun.security.ec.ECPublicKeyImpl
 917:             9            432  com.sun.jna.Function
 918:            27            432  java.security.spec.ECFieldFp
 919:            18            432  java.text.DateFormat$Field
 920:            18            432  java.util.Collections$UnmodifiableCollection$1
 921:            18            432  org.apache.cassandra.exceptions.ExceptionCode
 922:            18            432  org.apache.cassandra.io.util.WrappedDataOutputStreamPlus
 923:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$1
 924:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$2
 925:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$3
 926:            18            432  org.apache.cassandra.metrics.ThreadPoolMetrics$4
 927:             9            432  org.apache.cassandra.net.OutboundTcpConnectionPool
 928:            18            432  org.cliffc.high_scale_lib.NonBlockingHashMap$NBHMEntry
 929:            13            416  io.netty.util.Recycler$WeakOrderQueue$Link
 930:            13            416  java.lang.invoke.SimpleMethodHandle
 931:            13            416  java.security.AlgorithmParameters
 932:            13            416  java.util.Stack
 933:             4            416  sun.net.www.protocol.file.FileURLConnection
 934:            17            408  org.apache.cassandra.utils.IntegerInterval
 935:            17            408  org.codehaus.jackson.map.DeserializationConfig$Feature
 936:            10            400  java.io.ObjectStreamClass$FieldReflector
 937:            10            400  java.lang.invoke.DirectMethodHandle$Accessor
 938:            10            400  javax.crypto.CryptoPermission
 939:            10            400  sun.reflect.generics.repository.MethodRepository
 940:             7            392  java.util.Calendar$Builder
 941:             1            392  org.apache.cassandra.utils.memory.MemtableCleanerThread
 942:             8            384  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicLong;
 943:             1            384  ch.qos.logback.core.AsyncAppenderBase$Worker
 944:             4            384  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap
 945:            16            384  io.netty.channel.epoll.EpollEventArray
 946:            12            384  java.io.EOFException
 947:             1            384  java.lang.ref.Finalizer$FinalizerThread
 948:             8            384  java.net.SocketInputStream
 949:             8            384  java.net.SocketOutputStream
 950:            12            384  java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue
 951:            12            384  java.util.concurrent.atomic.AtomicLongFieldUpdater$CASUpdater
 952:             1            384  java.util.logging.LogManager$Cleaner
 953:            16            384  javax.management.StandardMBean
 954:            16            384  org.apache.cassandra.cql3.Attributes
 955:            16            384  org.apache.cassandra.cql3.Constants$Setter
 956:            16            384  org.apache.cassandra.cql3.Operations
 957:            12            384  org.apache.cassandra.cql3.SingleColumnRelation
 958:             8            384  org.apache.cassandra.hints.HintsStore
 959:            16            384  org.apache.cassandra.metrics.TableMetrics$35
 960:             1            384  org.apache.cassandra.net.MessagingService$SocketThread
 961:            16            384  org.apache.cassandra.schema.TableParams$Option
 962:             1            384  org.apache.cassandra.thrift.ThriftServer$ThriftServerThread
 963:            16            384  sun.misc.MetaIndex
 964:            16            384  sun.nio.ch.OptionKey
 965:             3            384  sun.nio.fs.UnixFileAttributes
 966:            12            384  sun.nio.fs.UnixPath
 967:             1            376  java.lang.ref.Reference$ReferenceHandler
 968:            16            368  [Ljava.security.cert.Certificate;
 969:            17            368  [Ljavax.management.MBeanNotificationInfo;
 970:            23            368  java.lang.ThreadLocal
 971:             3            360  [Lorg.apache.cassandra.gms.ApplicationState;
 972:            15            360  com.sun.jmx.remote.util.ClassLogger
 973:             9            360  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit
 974:             9            360  java.io.BufferedInputStream
 975:            15            360  java.io.ObjectStreamClass$ClassDataSlot
 976:            15            360  java.net.InetAddress
 977:             9            360  org.apache.cassandra.db.marshal.SetType
 978:            15            360  org.apache.cassandra.utils.memory.SlabAllocator$Region
 979:            11            352  java.lang.ClassLoader$1
 980:            11            352  java.util.concurrent.SynchronousQueue
 981:            11            352  org.apache.cassandra.db.ConsistencyLevel
 982:             4            352  sun.rmi.transport.ConnectionInputStream
 983:             7            336  [Ljavax.management.openmbean.OpenType;
 984:             7            336  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CompositeMapping
 985:            14            336  java.lang.invoke.LambdaFormEditor$Transform$Kind
 986:             6            336  java.nio.DirectLongBufferU
 987:            21            336  java.util.Collections$UnmodifiableCollection
 988:             7            336  java.util.Properties
 989:             6            336  org.apache.cassandra.concurrent.SEPExecutor
 990:             6            336  sun.management.MemoryPoolImpl
 991:             5            328  [Ljava.io.ObjectInputStream$HandleTable$HandleList;
 992:            16            328  [Ljava.lang.management.PlatformComponent;
 993:             4            320  [Lio.netty.buffer.PoolArena;
 994:            10            320  [Ljava.lang.invoke.LambdaForm$BasicType;
 995:            10            320  java.io.FileOutputStream
 996:             8            320  java.io.ObjectOutputStream$HandleTable
 997:            10            320  java.lang.OutOfMemoryError
 998:            10            320  java.lang.StringCoding$StringEncoder
 999:            10            320  java.lang.reflect.WeakCache$CacheValue
1000:            10            320  java.security.cert.PolicyQualifierInfo
1001:             8            320  org.apache.cassandra.db.marshal.ListType
1002:            20            320  org.apache.cassandra.dht.LocalPartitioner$1
1003:             8            320  org.apache.cassandra.gms.ArrayBackedBoundedStats
1004:             8            320  org.apache.cassandra.gms.ArrivalWindow
1005:            10            320  sun.reflect.generics.tree.MethodTypeSignature
1006:             8            320  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler
1007:            10            320  sun.security.util.DisabledAlgorithmConstraints$KeySizeConstraint
1008:            13            312  [Ljava.net.InetSocketAddress;
1009:            13            312  com.sun.jna.Pointer
1010:            13            312  java.lang.management.ManagementPermission
1011:            19            304  sun.reflect.BootstrapConstructorAccessorImpl
1012:             1            296  com.github.benmanes.caffeine.SingleConsumerQueue
1013:             1            296  com.github.benmanes.caffeine.cache.BoundedBuffer$RingBuffer
1014:             4            288  [Lch.qos.logback.classic.spi.StackTraceElementProxy;
1015:            12            288  [Lcom.codahale.metrics.Striped64$Cell;
1016:            12            288  ch.qos.logback.core.joran.spi.HostClassAndPropertyDouble
1017:             1            288  com.github.benmanes.caffeine.cache.LocalCacheFactory$SSLiMW
1018:             6            288  com.google.common.collect.HashBiMap
1019:             9            288  com.google.common.collect.RegularImmutableSet
1020:             4            288  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8
1021:            12            288  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ListenerWrapper
1022:             6            288  java.io.BufferedReader
1023:            12            288  java.lang.ProcessEnvironment$Variable
1024:             9            288  java.lang.reflect.Proxy$Key1
1025:             9            288  java.util.concurrent.CountDownLatch$Sync
1026:             9            288  java.util.concurrent.SynchronousQueue$TransferStack$SNode
1027:             9            288  java.util.logging.Level
1028:            18            288  java.util.regex.Pattern$Begin
1029:            12            288  org.apache.cassandra.concurrent.Stage
1030:            18            288  org.apache.cassandra.io.util.DataOutputStreamPlus$2
1031:             4            288  org.apache.cassandra.locator.TokenMetadata
1032:             9            288  org.apache.commons.lang3.JavaVersion
1033:             6            288  sun.nio.cs.StreamDecoder
1034:            18            288  sun.reflect.Label
1035:             4            288  sun.rmi.transport.ConnectionOutputStream
1036:             9            288  sun.security.jca.ProviderConfig
1037:             7            280  java.net.SocketTimeoutException
1038:             7            280  org.apache.cassandra.streaming.messages.StreamMessage$Type
1039:             7            280  org.apache.thrift.transport.TTransportException
1040:             7            280  sun.misc.FloatingDecimal$BinaryToASCIIBuffer
1041:             7            280  sun.rmi.transport.tcp.TCPEndpoint
1042:             1            272  [Lorg.codehaus.jackson.sym.Name;
1043:            17            272  com.sun.proxy.$Proxy3
1044:            17            272  net.jpountz.lz4.LZ4HCJNICompressor
1045:            17            272  org.apache.cassandra.cql3.Constants$Value
1046:            17            272  sun.reflect.ClassDefiner$1
1047:            17            272  sun.security.x509.DNSName
1048:             3            264  [[D
1049:            11            264  com.google.common.collect.ImmutableMapValues
1050:            11            264  java.net.StandardSocketOptions$StdSocketOption
1051:            11            264  java.rmi.server.ObjID
1052:            11            264  java.util.regex.Pattern$SliceI
1053:            11            264  org.apache.cassandra.io.sstable.Component
1054:            11            264  org.apache.cassandra.io.sstable.Component$Type
1055:            11            264  org.apache.cassandra.metrics.DroppedMessageMetrics
1056:            11            264  org.apache.cassandra.metrics.TableMetrics$36
1057:            11            264  org.apache.cassandra.net.MessagingService$DroppedMessages
1058:            11            264  sun.rmi.transport.ObjectEndpoint
1059:            11            264  sun.security.util.DisabledAlgorithmConstraints$DisabledConstraint
1060:            10            256  [Ljava.io.ObjectStreamClass$ClassDataSlot;
1061:             8            256  com.google.common.cache.LocalCache$StrongEntry
1062:            16            256  io.netty.channel.epoll.EpollEventLoop$1
1063:            16            256  io.netty.channel.epoll.EpollEventLoop$2
1064:            16            256  io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator
1065:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$2
1066:            16            256  io.netty.util.concurrent.SingleThreadEventExecutor$DefaultThreadProperties
1067:             8            256  java.util.Collections$UnmodifiableMap
1068:            16            256  java.util.concurrent.Semaphore
1069:             8            256  javax.management.MBeanNotificationInfo
1070:             8            256  org.apache.cassandra.cql3.functions.CastFcts$JavaCounterFunctionWrapper
1071:             8            256  org.apache.cassandra.db.ClusteringPrefix$Kind
1072:             8            256  org.apache.cassandra.repair.messages.RepairMessage$Type
1073:             8            256  sun.management.NotificationEmitterSupport$ListenerInfo
1074:             8            256  sun.misc.ProxyGenerator$PrimitiveTypeInfo
1075:             8            256  sun.misc.URLClassPath$JarLoader$2
1076:             8            256  sun.security.x509.CertificatePoliciesExtension
1077:             6            240  [Ljava.lang.invoke.BoundMethodHandle$SpeciesData;
1078:            10            240  com.sun.org.apache.xerces.internal.impl.XMLScanner$NameType
1079:            10            240  java.io.BufferedOutputStream
1080:             6            240  java.lang.UNIXProcess
1081:            10            240  java.nio.file.StandardOpenOption
1082:            10            240  java.security.CryptoPrimitive
1083:             3            240  java.util.concurrent.ScheduledThreadPoolExecutor
1084:            15            240  java.util.regex.Pattern$Dot
1085:            10            240  org.apache.cassandra.auth.Permission
1086:             5            240  org.apache.cassandra.config.ViewDefinition
1087:             5            240  org.apache.cassandra.db.lifecycle.LogRecord
1088:             5            240  org.apache.cassandra.db.view.View
1089:             6            240  org.apache.cassandra.metrics.SEPMetrics
1090:             6            240  org.apache.cassandra.schema.KeyspaceMetadata
1091:            10            240  org.codehaus.jackson.JsonParser$Feature
1092:            10            240  org.yaml.snakeyaml.events.Event$ID
1093:            15            240  org.yaml.snakeyaml.nodes.Tag
1094:             6            240  sun.management.MemoryPoolImpl$CollectionSensor
1095:             6            240  sun.management.MemoryPoolImpl$PoolSensor
1096:             5            240  sun.misc.URLClassPath
1097:            10            240  sun.reflect.generics.scope.MethodScope
1098:            15            240  sun.reflect.generics.tree.TypeVariableSignature
1099:            10            240  sun.rmi.runtime.Log$LoggerLog
1100:            10            240  sun.security.x509.Extension
1101:             5            240  sun.util.locale.provider.LocaleResources$ResourceReference
1102:             8            232  [Ljava.lang.Boolean;
1103:             2            224  [Lorg.codehaus.jackson.map.SerializationConfig$Feature;
1104:             7            224  [Lsun.nio.fs.NativeBuffer;
1105:             7            224  com.google.common.util.concurrent.MoreExecutors$DirectExecutorService
1106:             4            224  java.io.ObjectInputStream$BlockDataInputStream
1107:            14            224  java.rmi.server.Operation
1108:             7            224  java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
1109:             7            224  java.util.regex.Pattern$BnM
1110:             7            224  org.codehaus.jackson.JsonGenerator$Feature
1111:             4            224  org.codehaus.jackson.map.introspect.AnnotatedClass
1112:             4            224  org.codehaus.jackson.map.introspect.BasicBeanDescription
1113:             7            224  sun.nio.fs.NativeBuffer
1114:             7            224  sun.reflect.annotation.AnnotationType
1115:             4            224  sun.rmi.transport.Target
1116:             7            224  sun.security.x509.NetscapeCertTypeExtension
1117:             9            216  java.lang.ProcessEnvironment$Value
1118:             9            216  java.util.Collections$SynchronizedSet
1119:             9            216  java.util.logging.Level$KnownLevel
1120:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$1
1121:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$2
1122:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$3
1123:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$4
1124:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$5
1125:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$6
1126:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$7
1127:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$8
1128:             9            216  org.apache.cassandra.metrics.ConnectionMetrics$9
1129:             3            216  sun.security.provider.NativePRNG$RandomIO
1130:             9            216  sun.util.logging.PlatformLogger$Level
1131:             7            208  [Ljava.lang.invoke.LambdaForm$NamedFunction;
1132:             2            208  [Lorg.apache.cassandra.cql3.CQL3Type$Native;
1133:            13            208  com.google.common.util.concurrent.RateLimiter$SleepingStopwatch$1
1134:             2            208  java.lang.invoke.InnerClassLambdaMetafactory
1135:            13            208  sun.nio.ch.SocketAdaptor$2
1136:             2            200  [Ljava.text.DateFormat$Field;
1137:             5            200  io.netty.channel.group.DefaultChannelGroup
1138:             5            200  java.lang.invoke.BoundMethodHandle$SpeciesData
1139:             5            200  java.lang.invoke.DirectMethodHandle$Constructor
1140:             5            200  java.util.stream.StreamOpFlag
1141:             5            200  org.apache.cassandra.cql3.statements.SelectStatement$RawStatement
1142:             5            200  org.apache.cassandra.db.view.ViewBuilder
1143:             5            200  sun.rmi.transport.WeakRef
1144:             6            192  [Ljava.rmi.server.Operation;
1145:             3            192  [Lorg.apache.cassandra.db.ConsistencyLevel;
1146:             4            192  [[Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$PaddedAtomicReference;
1147:             3            192  ch.qos.logback.classic.PatternLayout
1148:             6            192  ch.qos.logback.core.util.CachingDateFormatter
1149:             8            192  com.google.common.cache.LocalCache$AccessQueue$1
1150:             4            192  com.google.common.collect.TreeMultimap
1151:             6            192  java.lang.ProcessBuilder
1152:             6            192  java.lang.invoke.LambdaForm$BasicType
1153:             8            192  java.lang.invoke.MethodHandleImpl$Intrinsic
1154:             8            192  java.math.RoundingMode
1155:            12            192  java.util.concurrent.ConcurrentSkipListMap$EntrySet
1156:             4            192  java.util.concurrent.locks.ReentrantReadWriteLock$FairSync
1157:             8            192  java.util.regex.Pattern$7
1158:             8            192  javax.crypto.CryptoPermissionCollection
1159:             4            192  javax.management.openmbean.TabularType
1160:             3            192  jdk.internal.org.objectweb.asm.FieldWriter
1161:             4            192  jdk.internal.org.objectweb.asm.Frame
1162:             8            192  jdk.net.SocketFlow$Status
1163:             6            192  org.apache.cassandra.db.Keyspace
1164:             4            192  org.apache.cassandra.db.RangeTombstoneList
1165:             8            192  org.apache.cassandra.db.WriteType
1166:             8            192  org.apache.cassandra.serializers.MapSerializer
1167:             8            192  org.apache.cassandra.serializers.SetSerializer
1168:             8            192  org.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State
1169:             8            192  org.apache.cassandra.service.StorageService$Mode
1170:             3            192  org.apache.cassandra.utils.MerkleTree$TreeDifference
1171:             6            192  org.apache.commons.lang3.text.StrBuilder
1172:             8            192  org.yaml.snakeyaml.scanner.Constant
1173:            12            192  sun.nio.ch.SocketAdaptor$1
1174:             6            192  sun.rmi.runtime.NewThreadAction
1175:             2            192  sun.security.provider.Sun
1176:             6            192  sun.security.util.MemoryCache
1177:             8            192  sun.security.x509.PolicyInformation
1178:             2            176  [Lorg.apache.cassandra.transport.Message$Type;
1179:             2            176  [Lorg.codehaus.jackson.map.DeserializationConfig$Feature;
1180:            10            176  [Lsun.reflect.generics.tree.TypeSignature;
1181:            11            176  java.text.NumberFormat$Field
1182:            11            176  java.util.LinkedHashMap$LinkedEntrySet
1183:            11            176  java.util.concurrent.SynchronousQueue$TransferStack
1184:             2            176  javax.management.remote.rmi.NoCallStackClassLoader
1185:             2            176  org.apache.cassandra.db.commitlog.MemoryMappedSegment
1186:            11            176  sun.security.ec.ECParameters
1187:             1            168  [[Ljava.math.BigInteger;
1188:             7            168  ch.qos.logback.classic.Level
1189:             3            168  ch.qos.logback.classic.encoder.PatternLayoutEncoder
1190:             7            168  com.google.common.collect.ImmutableEnumSet
1191:             7            168  com.sun.management.VMOption$Origin
1192:             7            168  com.sun.org.apache.xerces.internal.util.FeatureState
1193:             7            168  java.lang.invoke.MethodHandles$Lookup
1194:             7            168  java.net.NetPermission
1195:             7            168  java.util.BitSet
1196:             3            168  javax.management.openmbean.OpenMBeanOperationInfoSupport
1197:             7            168  javax.security.auth.AuthPermission
1198:             7            168  org.apache.cassandra.cql3.Constants$Type
1199:             7            168  org.apache.cassandra.db.Directories$FileAction
1200:             7            168  org.apache.cassandra.utils.concurrent.SimpleCondition
1201:             7            168  org.apache.cassandra.utils.progress.ProgressEventType
1202:             7            168  org.codehaus.jackson.annotate.JsonMethod
1203:             7            168  sun.nio.fs.NativeBuffer$Deallocator
1204:             7            168  sun.rmi.server.LoaderHandler$LoaderKey
1205:             3            168  sun.rmi.transport.tcp.TCPChannel
1206:             3            168  sun.rmi.transport.tcp.TCPConnection
1207:             3            168  sun.security.provider.SHA
1208:             7            168  sun.security.x509.NetscapeCertTypeExtension$MapEntry
1209:             4            160  [F
1210:             2            160  ch.qos.logback.core.rolling.RollingFileAppender
1211:            10            160  io.netty.util.internal.ConcurrentSet
1212:             4            160  java.io.ObjectOutputStream$BlockDataOutputStream
1213:             5            160  java.io.SerializablePermission
1214:             5            160  java.lang.StringCoding$StringDecoder
1215:             5            160  javax.management.StandardEmitterMBean
1216:             5            160  org.apache.cassandra.db.marshal.CompositeType
1217:             5            160  org.apache.cassandra.repair.RepairRunnable$1
1218:             5            160  org.apache.cassandra.transport.ProtocolVersion
1219:             5            160  org.apache.cassandra.transport.messages.ResultMessage$Kind
1220:             5            160  org.apache.cassandra.utils.CassandraVersion
1221:             4            160  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotV
1222:             5            160  sun.rmi.transport.StreamRemoteCall
1223:             5            160  sun.security.ssl.CipherSuite$MacAlg
1224:            10            160  sun.security.x509.CertificatePolicyId
1225:             5            160  sun.util.locale.provider.LocaleProviderAdapter$Type
1226:             6            144  [Ljava.io.Closeable;
1227:             2            144  [Ljava.math.BigDecimal;
1228:             1            144  [Ljava.util.concurrent.ForkJoinTask$ExceptionNode;
1229:             1            144  [Lorg.codehaus.jackson.sym.CharsToNameCanonicalizer$Bucket;
1230:             3            144  ch.qos.logback.classic.pattern.DateConverter
1231:             3            144  ch.qos.logback.classic.pattern.ExtendedThrowableProxyConverter
1232:             3            144  ch.qos.logback.classic.spi.ThrowableProxy
1233:             6            144  com.google.common.collect.AbstractMultimap$EntrySet
1234:             6            144  com.sun.org.apache.xerces.internal.util.Status
1235:             6            144  java.io.InputStreamReader
1236:             3            144  java.lang.ThreadGroup
1237:             6            144  java.lang.UNIXProcess$$Lambda$15/1221027335
1238:             6            144  java.lang.UNIXProcess$ProcessPipeOutputStream
1239:             9            144  java.util.concurrent.CountDownLatch
1240:             6            144  java.util.regex.Pattern$CharProperty$1
1241:             2            144  org.antlr.runtime.RecognizerSharedState
1242:             6            144  org.apache.cassandra.cql3.CFName
1243:             6            144  org.apache.cassandra.cql3.WhereClause
1244:             6            144  org.apache.cassandra.db.filter.DataLimits$Kind
1245:             6            144  org.apache.cassandra.db.view.ViewManager
1246:             3            144  org.apache.cassandra.locator.SimpleStrategy
1247:             3            144  org.apache.cassandra.metrics.CacheMetrics
1248:             6            144  org.apache.cassandra.metrics.SEPMetrics$1
1249:             6            144  org.apache.cassandra.metrics.SEPMetrics$2
1250:             6            144  org.apache.cassandra.metrics.SEPMetrics$3
1251:             6            144  org.apache.cassandra.metrics.SEPMetrics$4
1252:             6            144  org.apache.cassandra.schema.KeyspaceParams
1253:             6            144  org.apache.cassandra.schema.ReplicationParams
1254:             6            144  org.apache.cassandra.service.ActiveRepairService$1
1255:             6            144  org.apache.cassandra.service.ActiveRepairService$2
1256:             6            144  org.apache.cassandra.streaming.StreamSession$State
1257:             6            144  org.codehaus.jackson.annotate.JsonAutoDetect$Visibility
1258:             6            144  org.github.jamm.MemoryMeter$Guess
1259:             6            144  sun.misc.PerfCounter
1260:             6            144  sun.security.ssl.ProtocolVersion
1261:             6            144  sun.security.util.DisabledAlgorithmConstraints$Constraint$Operator
1262:             4            128  [Lcom.google.common.cache.LocalCache$Segment;
1263:             4            128  [Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
1264:             2            128  [Lorg.apache.cassandra.concurrent.Stage;
1265:             2            128  [Lorg.apache.cassandra.io.sstable.Component$Type;
1266:             2            128  ch.qos.logback.core.rolling.FixedWindowRollingPolicy
1267:             4            128  ch.qos.logback.core.rolling.helper.FileNamePattern
1268:             8            128  com.google.common.cache.LocalCache$AccessQueue
1269:             8            128  com.google.common.cache.LocalCache$StrongValueReference
1270:             4            128  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$ArrayMapping
1271:             2            128  java.io.ExpiringCache$1
1272:             4            128  java.io.ObjectInputStream$HandleTable
1273:             4            128  java.io.ObjectInputStream$PeekInputStream
1274:             4            128  java.lang.UNIXProcess$Platform
1275:             2            128  java.lang.invoke.InvokerBytecodeGenerator
1276:             4            128  java.util.Random
1277:             4            128  java.util.concurrent.ExecutionException
1278:             4            128  net.jpountz.util.Native$OS
1279:             4            128  org.apache.cassandra.cql3.functions.CastFcts$CassandraFunctionWrapper
1280:             4            128  org.apache.cassandra.db.marshal.ReversedType
1281:             1            128  org.apache.cassandra.io.compress.CompressedSequentialWriter
1282:             1            128  org.apache.cassandra.io.sstable.format.big.BigTableWriter
1283:             4            128  org.apache.cassandra.io.util.SequentialWriterOption
1284:             4            128  org.apache.cassandra.locator.PendingRangeMaps
1285:             2            128  org.apache.cassandra.metrics.CASClientRequestMetrics
1286:             8            128  org.apache.cassandra.serializers.MapSerializer$$Lambda$24/2072313080
1287:             8            128  sun.net.www.ParseUtil
1288:             4            128  sun.rmi.transport.LiveRef
1289:             8            128  sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$292/1509453068
1290:             4            128  sun.security.ssl.CipherSuite$PRF
1291:             4            128  sun.security.x509.ExtendedKeyUsageExtension
1292:             3            120  [Lorg.codehaus.jackson.annotate.JsonMethod;
1293:             1            120  [[Ljava.lang.String;
1294:             5            120  ch.qos.logback.core.pattern.parser.TokenStream$TokenizerState
1295:             5            120  ch.qos.logback.core.subst.Token$Type
1296:             5            120  ch.qos.logback.core.util.AggregationType
1297:             3            120  com.google.common.collect.AbstractMapBasedMultimap$AsMap
1298:             5            120  com.sun.org.apache.xerces.internal.util.PropertyState
1299:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State
1300:             5            120  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State
1301:             3            120  java.lang.invoke.BoundMethodHandle$Species_LL
1302:             3            120  java.lang.invoke.MethodHandleImpl$AsVarargsCollector
1303:             5            120  java.util.stream.StreamOpFlag$Type
1304:             3            120  org.apache.cassandra.cache.AutoSavingCache
1305:             5            120  org.apache.cassandra.config.Config$DiskFailurePolicy
1306:             5            120  org.apache.cassandra.cql3.VariableSpecifications
1307:             5            120  org.apache.cassandra.cql3.statements.IndexTarget$Type
1308:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Status
1309:             5            120  org.apache.cassandra.db.lifecycle.LogRecord$Type
1310:             3            120  org.apache.cassandra.db.lifecycle.LogTransaction$SSTableTidier
1311:             3            120  org.apache.cassandra.index.internal.composites.ClusteringColumnIndex
1312:             5            120  org.apache.cassandra.schema.CompactionParams$Option
1313:             1            120  org.apache.cassandra.service.StorageService
1314:             5            120  org.apache.cassandra.utils.NativeLibrary$OSType
1315:             5            120  org.yaml.snakeyaml.DumperOptions$ScalarStyle
1316:             5            120  sun.misc.FloatingDecimal$PreparedASCIIToBinaryBuffer
1317:             5            120  sun.security.jca.ServiceId
1318:             5            120  sun.security.util.DisabledAlgorithmConstraints
1319:             2            112  [Ljava.lang.invoke.MethodType;
1320:             2            112  [Ljava.security.CryptoPrimitive;
1321:             2            112  [Ljava.util.List;
1322:             2            112  [Lorg.apache.cassandra.auth.Permission;
1323:             2            112  [Lorg.apache.cassandra.db.PartitionPosition;
1324:             3            112  [Lorg.apache.cassandra.transport.ProtocolVersion;
1325:             7            112  com.google.common.util.concurrent.MoreExecutors$ListeningDecorator
1326:             2            112  com.sun.management.GcInfo
1327:             2            112  io.netty.buffer.PooledByteBufAllocator
1328:             7            112  java.util.concurrent.ConcurrentHashMap$EntrySetView
1329:             2            112  org.apache.cassandra.cql3.statements.DeleteStatement
1330:             2            112  org.apache.cassandra.db.compaction.LeveledCompactionStrategy
1331:             2            112  org.apache.cassandra.repair.LocalSyncTask
1332:             7            112  org.apache.cassandra.serializers.ListSerializer
1333:             2            112  org.apache.cassandra.utils.memory.MemtablePool$SubPool
1334:             7            112  sun.security.provider.NativePRNG
1335:             1            104  com.codahale.metrics.ThreadLocalRandom
1336:             1            104  io.netty.channel.epoll.EpollServerSocketChannel
1337:             1            104  org.apache.cassandra.db.ColumnIndex
1338:             1            104  sun.rmi.server.LoaderHandler$Loader
1339:             2             96  [Lcom.google.common.cache.LocalCache$EntryFactory;
1340:             6             96  [Ljava.io.ObjectStreamClass$MemberSignature;
1341:             2             96  [Ljava.util.concurrent.TimeUnit;
1342:             1             96  [Lorg.apache.cassandra.db.compaction.OperationType;
1343:             2             96  [Lorg.apache.cassandra.repair.messages.RepairMessage$Type;
1344:             1             96  [Lorg.yaml.snakeyaml.tokens.Token$ID;
1345:             1             96  [[J
1346:             1             96  ch.qos.logback.classic.LoggerContext
1347:             3             96  ch.qos.logback.classic.pattern.FileOfCallerConverter
1348:             3             96  ch.qos.logback.classic.pattern.LevelConverter
1349:             3             96  ch.qos.logback.classic.pattern.LineOfCallerConverter
1350:             3             96  ch.qos.logback.classic.pattern.LineSeparatorConverter
1351:             3             96  ch.qos.logback.classic.pattern.MessageConverter
1352:             3             96  ch.qos.logback.classic.pattern.ThreadConverter
1353:             3             96  ch.qos.logback.core.joran.action.AppenderRefAction
1354:             4             96  ch.qos.logback.core.pattern.parser.Token
1355:             2             96  ch.qos.logback.core.recovery.ResilientFileOutputStream
1356:             2             96  ch.qos.logback.core.rolling.helper.DateTokenConverter
1357:             4             96  ch.qos.logback.core.subst.Token
1358:             2             96  ch.qos.logback.core.util.InvocationGate
1359:             4             96  com.google.common.cache.LocalCache$WriteQueue$1
1360:             4             96  com.google.common.collect.AbstractIterator$State
1361:             4             96  com.google.common.collect.Iterators$12
1362:             4             96  com.googlecode.concurrentlinkedhashmap.LinkedDeque
1363:             3             96  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$EnumMapping
1364:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$MBeanInfoMap
1365:             2             96  com.sun.jmx.mbeanserver.MBeanIntrospector$PerInterfaceMap
1366:             1             96  com.sun.net.ssl.internal.ssl.Provider
1367:             3             96  com.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap
1368:             3             96  io.netty.buffer.EmptyByteBuf
1369:             3             96  java.io.ByteArrayInputStream
1370:             6             96  java.io.FileInputStream$1
1371:             4             96  java.io.ObjectOutputStream$ReplaceTable
1372:             6             96  java.lang.UNIXProcess$$Lambda$16/1801942731
1373:             6             96  java.net.Socket$2
1374:             6             96  java.net.Socket$3
1375:             4             96  java.net.URLClassLoader$2
1376:             4             96  java.nio.file.FileVisitResult
1377:             4             96  java.text.Normalizer$Form
1378:             6             96  java.util.LinkedHashMap$LinkedKeySet
1379:             2             96  java.util.concurrent.ArrayBlockingQueue
1380:             3             96  java.util.concurrent.ConcurrentHashMap$ForwardingNode
1381:             3             96  java.util.concurrent.locks.ReentrantLock$FairSync
1382:             4             96  java.util.stream.StreamShape
1383:             4             96  javax.management.NotificationBroadcasterSupport$ListenerInfo
1384:             4             96  org.apache.cassandra.auth.IRoleManager$Option
1385:             4             96  org.apache.cassandra.config.CFMetaData$Flag
1386:             4             96  org.apache.cassandra.config.ColumnDefinition$Kind
1387:             4             96  org.apache.cassandra.config.Config$CommitFailurePolicy
1388:             4             96  org.apache.cassandra.config.Config$DiskAccessMode
1389:             4             96  org.apache.cassandra.config.Config$MemtableAllocationType
1390:             4             96  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption
1391:             1             96  org.apache.cassandra.cql3.Cql_Parser
1392:             4             96  org.apache.cassandra.db.SystemKeyspace$BootstrapState
1393:             2             96  org.apache.cassandra.db.compaction.LeveledManifest
1394:             4             96  org.apache.cassandra.db.context.CounterContext$Relationship
1395:             4             96  org.apache.cassandra.db.lifecycle.LogTransaction$Obsoletion
1396:             4             96  org.apache.cassandra.dht.Bounds
1397:             4             96  org.apache.cassandra.hints.HintsDispatcher$Callback$Outcome
1398:             4             96  org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys
1399:             4             96  org.apache.cassandra.io.sstable.format.SSTableReader$OpenReason
1400:             4             96  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason
1401:             4             96  org.apache.cassandra.io.sstable.metadata.MetadataType
1402:             2             96  org.apache.cassandra.io.util.FileHandle$Builder
1403:             2             96  org.apache.cassandra.locator.LocalStrategy
1404:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$1
1405:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$10
1406:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$11
1407:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$12
1408:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$13
1409:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$14
1410:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$15
1411:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$16
1412:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$2
1413:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$3
1414:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$4
1415:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$5
1416:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$6
1417:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$7
1418:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$8
1419:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$9
1420:             6             96  org.apache.cassandra.metrics.KeyspaceMetrics$KeyspaceMetricNameFactory
1421:             6             96  org.apache.cassandra.schema.Functions
1422:             4             96  org.apache.cassandra.schema.SpeculativeRetryParam$Kind
1423:             6             96  org.apache.cassandra.schema.Tables
1424:             6             96  org.apache.cassandra.schema.Views
1425:             4             96  org.apache.cassandra.transport.Event$Type
1426:             1             96  org.apache.cassandra.triggers.CustomClassLoader
1427:             4             96  org.apache.cassandra.utils.AbstractIterator$State
1428:             4             96  org.apache.cassandra.utils.AsymmetricOrdering$Op
1429:             3             96  org.apache.cassandra.utils.NoSpamLogger
1430:             4             96  org.apache.cassandra.utils.SortedBiMultiValMap
1431:             4             96  org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State
1432:             2             96  org.codehaus.jackson.map.MapperConfig$Base
1433:             4             96  org.yaml.snakeyaml.nodes.NodeId
1434:             2             96  sun.management.GarbageCollectorImpl
1435:             2             96  sun.management.GcInfoBuilder
1436:             4             96  sun.misc.FormattedFloatingDecimal$Form
1437:             1             96  sun.misc.Launcher$AppClassLoader
1438:             4             96  sun.net.www.MessageHeader
1439:             1             96  sun.nio.ch.ServerSocketChannelImpl
1440:             2             96  sun.nio.cs.StreamEncoder
1441:             6             96  sun.rmi.transport.Transport$$Lambda$295/399097450
1442:             3             96  sun.rmi.transport.Transport$1
1443:             1             96  sun.security.ec.SunEC
1444:             1             96  sun.security.jca.ProviderList$1
1445:             1             96  sun.security.rsa.SunRsaSign
1446:             3             96  sun.security.ssl.ProtocolList
1447:             4             88  [Ljava.util.Map$Entry;
1448:             1             88  [Lnet.jpountz.lz4.LZ4Compressor;
1449:             1             88  [Lorg.apache.cassandra.exceptions.ExceptionCode;
1450:             1             88  [Lsun.security.util.ObjectIdentifier;
1451:             1             88  [[Ljava.lang.Byte;
1452:             1             88  java.util.jar.JarVerifier
1453:             1             88  org.apache.cassandra.concurrent.JMXConfigurableThreadPoolExecutor
1454:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CacheCleanupExecutor
1455:             1             88  org.apache.cassandra.db.compaction.CompactionManager$CompactionExecutor
1456:             1             88  org.apache.cassandra.db.compaction.CompactionManager$ValidationExecutor
1457:             1             88  org.apache.cassandra.gms.Gossiper
1458:             1             88  org.apache.cassandra.io.sstable.IndexSummaryBuilder
1459:             1             88  org.apache.cassandra.io.sstable.metadata.MetadataCollector
1460:             1             88  sun.misc.Launcher$ExtClassLoader
1461:             1             80  [Lio.netty.util.concurrent.SingleThreadEventExecutor;
1462:             2             80  [Ljava.lang.management.MemoryUsage;
1463:             2             80  [Ljava.util.stream.StreamOpFlag$Type;
1464:             5             80  [Lorg.apache.cassandra.config.ColumnDefinition;
1465:             2             80  [Lorg.apache.cassandra.config.Config$DiskFailurePolicy;
1466:             1             80  [Lorg.apache.cassandra.cql3.Operator;
1467:             1             80  [Lorg.apache.cassandra.schema.TableParams$Option;
1468:             2             80  [Lorg.apache.cassandra.transport.messages.ResultMessage$Kind;
1469:             2             80  [Lorg.codehaus.jackson.annotate.JsonAutoDetect$Visibility;
1470:             1             80  [Lsun.security.ssl.CipherSuite$KeyExchange;
1471:             1             80  ch.qos.logback.classic.AsyncAppender
1472:             2             80  ch.qos.logback.classic.filter.ThresholdFilter
1473:             1             80  ch.qos.logback.classic.turbo.ReconfigureOnChangeFilter
1474:             2             80  ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy
1475:             2             80  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$TabularMapping
1476:             1             80  com.sun.jmx.remote.util.ClassLoaderWithRepository
1477:             5             80  com.sun.proxy.$Proxy1
1478:             5             80  io.netty.channel.group.DefaultChannelGroup$1
1479:             2             80  io.netty.channel.unix.Errors$NativeConnectException
1480:             2             80  io.netty.util.Signal
1481:             2             80  java.io.ExpiringCache
1482:             2             80  java.util.Locale$Category
1483:             5             80  java.util.logging.SimpleFormatter
1484:             2             80  java.util.regex.Pattern$Loop
1485:             5             80  javax.security.auth.x500.X500Principal
1486:             1             80  org.apache.cassandra.concurrent.StageManager$TracingExecutor
1487:             1             80  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$SMAwareReconfigureOnChangeFilter
1488:             1             80  org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter
1489:             1             80  org.apache.cassandra.io.sstable.SSTableRewriter
1490:             5             80  org.apache.cassandra.repair.RepairSession$1
1491:             2             80  org.codehaus.jackson.sym.CharsToNameCanonicalizer
1492:             2             80  sun.management.MemoryManagerImpl
1493:             2             80  sun.reflect.UnsafeQualifiedStaticObjectFieldAccessorImpl
1494:             1             80  sun.reflect.misc.MethodUtil
1495:             2             80  sun.rmi.server.LoaderHandler$LoaderEntry
1496:             2             80  sun.rmi.server.UnicastServerRef
1497:             2             80  sun.rmi.server.UnicastServerRef2
1498:             2             80  sun.security.provider.DSAPublicKeyImpl
1499:             5             80  sun.security.util.DisabledAlgorithmConstraints$Constraints
1500:             2             80  sun.util.logging.resources.logging
1501:             1             72  [Ljava.lang.invoke.LambdaFormEditor$Transform$Kind;
1502:             4             72  [Ljava.nio.file.LinkOption;
1503:             3             72  [Ljava.util.concurrent.ConcurrentHashMap$CounterCell;
1504:             1             72  [Ljavax.management.openmbean.SimpleType;
1505:             2             72  [Lsun.security.jca.ProviderConfig;
1506:             1             72  ch.qos.logback.core.ConsoleAppender
1507:             3             72  ch.qos.logback.core.joran.action.NOPAction
1508:             3             72  ch.qos.logback.core.joran.action.PropertyAction
1509:             3             72  ch.qos.logback.core.pattern.FormatInfo
1510:             3             72  ch.qos.logback.core.rolling.helper.CompressionMode
1511:             3             72  ch.qos.logback.core.spi.FilterReply
1512:             3             72  ch.qos.logback.core.subst.Tokenizer$TokenizerState
1513:             3             72  com.github.benmanes.caffeine.cache.AccessOrderDeque
1514:             3             72  com.github.benmanes.caffeine.cache.Caffeine$Strength
1515:             3             72  com.google.common.base.CharMatcher$13
1516:             3             72  com.google.common.base.CharMatcher$RangesMatcher
1517:             3             72  com.google.common.collect.AbstractMapBasedMultimap$KeySet
1518:             1             72  io.netty.channel.DefaultChannelHandlerContext
1519:             1             72  io.netty.channel.DefaultChannelPipeline$HeadContext
1520:             1             72  io.netty.channel.DefaultChannelPipeline$TailContext
1521:             1             72  io.netty.channel.epoll.EpollServerSocketChannelConfig
1522:             3             72  java.io.ObjectStreamClass$ExceptionInfo
1523:             3             72  java.lang.UNIXProcess$LaunchMechanism
1524:             3             72  java.lang.annotation.RetentionPolicy
1525:             3             72  java.nio.file.FileTreeWalker$EventType
1526:             3             72  java.rmi.dgc.VMID
1527:             3             72  java.security.SecurityPermission
1528:             3             72  java.util.Base64$Encoder
1529:             1             72  java.util.ResourceBundle$RBClassLoader
1530:             3             72  java.util.concurrent.atomic.AtomicMarkableReference$Pair
1531:             3             72  java.util.jar.Manifest
1532:             1             72  java.util.logging.LogManager$RootLogger
1533:             1             72  java.util.logging.LogRecord
1534:             3             72  java.util.stream.Collector$Characteristics
1535:             3             72  java.util.stream.MatchOps$MatchKind
1536:             3             72  javax.crypto.CryptoPermissions
1537:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader
1538:             1             72  javax.management.remote.rmi.RMIConnectionImpl$CombinedClassLoader$ClassLoaderWrapper
1539:             3             72  javax.security.auth.Subject$SecureSet
1540:             3             72  org.apache.cassandra.auth.DataResource$Level
1541:             3             72  org.apache.cassandra.config.ColumnDefinition$ClusteringOrder
1542:             3             72  org.apache.cassandra.config.Config$InternodeCompression
1543:             3             72  org.apache.cassandra.config.Config$UserFunctionTimeoutPolicy
1544:             3             72  org.apache.cassandra.config.ReadRepairDecision
1545:             3             72  org.apache.cassandra.cql3.AssignmentTestable$TestResult
1546:             1             72  org.apache.cassandra.cql3.Cql_Lexer
1547:             3             72  org.apache.cassandra.cql3.ResultSet$Flag
1548:             3             72  org.apache.cassandra.db.Conflicts$Resolution
1549:             3             72  org.apache.cassandra.db.Directories$FileType
1550:             3             72  org.apache.cassandra.db.commitlog.CommitLogSegment$CDCState
1551:             1             72  org.apache.cassandra.db.compaction.CompactionIterator
1552:             3             72  org.apache.cassandra.db.lifecycle.SSTableSet
1553:             3             72  org.apache.cassandra.db.marshal.AbstractType$ComparisonType
1554:             3             72  org.apache.cassandra.db.monitoring.MonitoringState
1555:             3             72  org.apache.cassandra.db.rows.SerializationHelper$Flag
1556:             1             72  org.apache.cassandra.io.util.SequentialWriter
1557:             3             72  org.apache.cassandra.locator.TokenMetadata$Topology
1558:             3             72  org.apache.cassandra.metrics.CacheMetrics$1
1559:             3             72  org.apache.cassandra.metrics.CacheMetrics$6
1560:             3             72  org.apache.cassandra.metrics.CacheMetrics$7
1561:             3             72  org.apache.cassandra.metrics.StreamingMetrics
1562:             3             72  org.apache.cassandra.repair.RepairParallelism
1563:             3             72  org.apache.cassandra.repair.SystemDistributedKeyspace$RepairState
1564:             3             72  org.apache.cassandra.repair.messages.ValidationComplete
1565:             3             72  org.apache.cassandra.schema.CompactionParams$TombstoneOption
1566:             3             72  org.apache.cassandra.schema.IndexMetadata$Kind
1567:             3             72  org.apache.cassandra.service.CacheService$CacheType
1568:             3             72  org.apache.cassandra.streaming.StreamEvent$Type
1569:             3             72  org.apache.cassandra.transport.Server$LatestEvent
1570:             3             72  org.apache.cassandra.utils.BiMultiValMap
1571:             3             72  org.apache.cassandra.utils.NoSpamLogger$Level
1572:             3             72  org.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle
1573:             1             72  org.apache.commons.lang3.builder.ToStringStyle$DefaultToStringStyle
1574:             1             72  org.apache.commons.lang3.builder.ToStringStyle$MultiLineToStringStyle
1575:             1             72  org.apache.commons.lang3.builder.ToStringStyle$NoFieldNameToStringStyle
1576:             1             72  org.apache.commons.lang3.builder.ToStringStyle$ShortPrefixToStringStyle
1577:             1             72  org.apache.commons.lang3.builder.ToStringStyle$SimpleToStringStyle
1578:             1             72  org.apache.thrift.server.TThreadPoolServer$Args
1579:             3             72  org.yaml.snakeyaml.DumperOptions$FlowStyle
1580:             3             72  org.yaml.snakeyaml.DumperOptions$LineBreak
1581:             3             72  org.yaml.snakeyaml.introspector.BeanAccess
1582:             3             72  sun.misc.FloatingDecimal$ExceptionalBinaryToASCIIBuffer
1583:             3             72  sun.misc.ObjectInputFilter$Status
1584:             3             72  sun.misc.Signal
1585:             3             72  sun.nio.fs.UnixFileAttributeViews$Basic
1586:             3             72  sun.rmi.transport.SequenceEntry
1587:             3             72  sun.security.provider.NativePRNG$Variant
1588:             3             72  sun.security.ssl.CipherSuite$CipherType
1589:             3             72  sun.security.ssl.CipherSuiteList
1590:             1             72  sun.util.locale.provider.JRELocaleProviderAdapter
1591:             3             72  sun.util.resources.ParallelListResourceBundle$KeySet
1592:             2             64  [Ljava.lang.UNIXProcess$LaunchMechanism;
1593:             2             64  [Ljava.lang.annotation.RetentionPolicy;
1594:             3             64  [Ljava.security.CodeSigner;
1595:             3             64  [Ljava.security.cert.X509Certificate;
1596:             2             64  [Ljava.util.stream.Collector$Characteristics;
1597:             2             64  [Lorg.apache.cassandra.config.CFMetaData$Flag;
1598:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$ClusteringOrder;
1599:             2             64  [Lorg.apache.cassandra.config.ColumnDefinition$Kind;
1600:             2             64  [Lorg.apache.cassandra.config.Config$CommitFailurePolicy;
1601:             2             64  [Lorg.apache.cassandra.config.Config$InternodeCompression;
1602:             2             64  [Lorg.apache.cassandra.config.Config$MemtableAllocationType;
1603:             2             64  [Lorg.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions$InternodeEncryption;
1604:             2             64  [Lorg.apache.cassandra.cql3.ResultSet$Flag;
1605:             2             64  [Lorg.apache.cassandra.db.SystemKeyspace$BootstrapState;
1606:             2             64  [Lorg.apache.cassandra.io.sstable.metadata.MetadataType;
1607:             2             64  [Lorg.apache.cassandra.schema.CompactionParams$TombstoneOption;
1608:             2             64  [Lorg.apache.cassandra.schema.IndexMetadata$Kind;
1609:             2             64  [Lorg.apache.cassandra.transport.Event$Type;
1610:             2             64  [Lorg.yaml.snakeyaml.nodes.NodeId;
1611:             2             64  ch.qos.logback.classic.joran.action.LevelAction
1612:             2             64  ch.qos.logback.core.joran.spi.ConsoleTarget
1613:             2             64  ch.qos.logback.core.rolling.helper.Compressor
1614:             2             64  ch.qos.logback.core.rolling.helper.IntegerTokenConverter
1615:             4             64  ch.qos.logback.core.spi.FilterAttachableImpl
1616:             1             64  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus
1617:             2             64  com.github.benmanes.caffeine.cache.References$WeakKeyReference
1618:             1             64  com.github.benmanes.caffeine.cache.stats.CacheStats
1619:             1             64  com.google.common.cache.CacheStats
1620:             4             64  com.google.common.cache.LocalCache$WriteQueue
1621:             2             64  com.google.common.util.concurrent.Striped$LargeLazyStriped
1622:             4             64  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$BoundedEntryWeigher
1623:             2             64  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$CollectionMapping
1624:             1             64  com.sun.jmx.remote.internal.ArrayNotificationBuffer
1625:             2             64  com.sun.management.GarbageCollectionNotificationInfo
1626:             2             64  com.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property
1627:             1             64  io.netty.channel.ChannelOutboundBuffer
1628:             4             64  io.netty.util.concurrent.FastThreadLocal
1629:             4             64  java.io.ObjectInputStream$ValidationList
1630:             2             64  java.io.PrintStream
1631:             2             64  java.lang.ClassValue$Entry
1632:             2             64  java.lang.NoSuchMethodError
1633:             2             64  java.lang.VirtualMachineError
1634:             2             64  java.lang.ref.ReferenceQueue$Null
1635:             2             64  java.net.Inet6Address
1636:             2             64  java.net.Inet6Address$Inet6AddressHolder
1637:             2             64  java.util.ResourceBundle$Control$1
1638:             2             64  java.util.concurrent.ConcurrentLinkedQueue$Itr
1639:             2             64  java.util.jar.Manifest$FastInputStream
1640:             1             64  javax.management.remote.rmi.RMIConnectionImpl
1641:             1             64  javax.management.remote.rmi.RMIConnectorServer
1642:             4             64  javax.security.auth.login.AppConfigurationEntry$LoginModuleControlFlag
1643:             4             64  org.apache.cassandra.concurrent.SEPWorker$Work
1644:             2             64  org.apache.cassandra.cql3.functions.TokenFct
1645:             2             64  org.apache.cassandra.db.commitlog.CommitLogDescriptor
1646:             2             64  org.apache.cassandra.db.lifecycle.LogFile
1647:             2             64  org.apache.cassandra.db.lifecycle.LogTransaction
1648:             2             64  org.apache.cassandra.io.sstable.format.SSTableFormat$Type
1649:             2             64  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxIntTracker
1650:             2             64  org.apache.cassandra.io.util.SafeMemoryWriter
1651:             1             64  org.apache.cassandra.locator.DynamicEndpointSnitch
1652:             1             64  org.apache.cassandra.metrics.ViewWriteMetrics
1653:             1             64  org.apache.cassandra.net.MessagingService
1654:             2             64  org.apache.cassandra.service.ClientState
1655:             2             64  org.apache.cassandra.service.GCInspector$GCState
1656:             1             64  org.apache.cassandra.service.GCInspector$State
1657:             1             64  org.apache.cassandra.thrift.CustomTThreadPoolServer
1658:             1             64  org.apache.cassandra.utils.SigarLibrary
1659:             4             64  org.apache.cassandra.utils.SortedBiMultiValMap$1
1660:             4             64  org.codehaus.jackson.map.introspect.AnnotationMap
1661:             4             64  sun.net.www.protocol.jar.Handler
1662:             4             64  sun.rmi.server.MarshalOutputStream$1
1663:             2             64  sun.rmi.transport.DGCImpl$LeaseInfo
1664:             2             64  sun.rmi.transport.tcp.TCPTransport
1665:             2             64  sun.security.ssl.EphemeralKeyManager$EphemeralKeyPair
1666:             2             64  sun.security.ssl.SSLSessionContextImpl
1667:             2             64  sun.security.x509.PrivateKeyUsageExtension
1668:             2             64  sun.security.x509.SubjectAlternativeNameExtension
1669:             2             64  sun.util.locale.provider.LocaleServiceProviderPool
1670:             1             56  [Lcom.sun.org.apache.xerces.internal.impl.XMLScanner$NameType;
1671:             1             56  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$Limit;
1672:             1             56  [Ljava.lang.Runnable;
1673:             1             56  [Ljava.nio.file.StandardOpenOption;
1674:             2             56  [Ljdk.internal.org.objectweb.asm.Type;
1675:             1             56  [Lorg.apache.commons.lang3.JavaVersion;
1676:             1             56  [Lorg.codehaus.jackson.JsonParser$Feature;
1677:             1             56  [Lorg.yaml.snakeyaml.events.Event$ID;
1678:             1             56  [Lsun.util.logging.PlatformLogger$Level;
1679:             1             56  [[I
1680:             1             56  com.sun.jmx.remote.internal.ServerNotifForwarder
1681:             1             56  io.netty.util.concurrent.ScheduledFutureTask
1682:             1             56  java.lang.invoke.LambdaFormEditor$Transform
1683:             1             56  java.util.concurrent.ConcurrentHashMap$KeyIterator
1684:             1             56  java.util.logging.ConsoleHandler
1685:             1             56  java.util.logging.LogManager
1686:             1             56  javax.management.remote.JMXConnectionNotification
1687:             1             56  javax.management.remote.rmi.RMIJRMPServerImpl
1688:             1             56  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache
1689:             1             56  org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
1690:             1             56  org.apache.cassandra.config.EncryptionOptions$ServerEncryptionOptions
1691:             1             56  org.apache.cassandra.cql3.CqlLexer$DFA1
1692:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA14
1693:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA22
1694:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA24
1695:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA28
1696:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA30
1697:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA37
1698:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA44
1699:             1             56  org.apache.cassandra.cql3.Cql_Lexer$DFA9
1700:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA1
1701:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA15
1702:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA153
1703:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA154
1704:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA172
1705:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA174
1706:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA176
1707:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA178
1708:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA181
1709:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA189
1710:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA194
1711:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA195
1712:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA204
1713:             1             56  org.apache.cassandra.cql3.Cql_Parser$DFA44
1714:             1             56  org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard
1715:             1             56  org.apache.cassandra.db.commitlog.PeriodicCommitLogService
1716:             1             56  org.apache.cassandra.db.compaction.CompactionController
1717:             1             56  org.apache.cassandra.db.lifecycle.LifecycleTransaction
1718:             1             56  org.apache.cassandra.io.compress.CompressionMetadata$Writer
1719:             1             56  org.apache.cassandra.metrics.CacheMissMetrics
1720:             1             56  org.codehaus.jackson.map.ObjectMapper
1721:             1             56  org.codehaus.jackson.map.ser.StdSerializerProvider
1722:             1             56  org.codehaus.jackson.sym.BytesToNameCanonicalizer
1723:             1             56  org.hyperic.sigar.SigarLoader
1724:             1             56  sun.rmi.runtime.Log$InternalStreamHandler
1725:             1             48  [Lcom.sun.beans.util.Cache$CacheEntry;
1726:             1             48  [Lcom.sun.management.VMOption$Origin;
1727:             1             48  [Ljava.beans.WeakIdentityMap$Entry;
1728:             3             48  [Ljava.lang.annotation.Annotation;
1729:             1             48  [Ljava.lang.invoke.MethodHandleImpl$Intrinsic;
1730:             1             48  [Ljava.math.RoundingMode;
1731:             2             48  [Ljava.nio.file.FileVisitOption;
1732:             1             48  [Ljdk.net.SocketFlow$Status;
1733:             2             48  [Lorg.apache.cassandra.config.Config$CommitLogSync;
1734:             1             48  [Lorg.apache.cassandra.cql3.Constants$Type;
1735:             1             48  [Lorg.apache.cassandra.db.ClusteringPrefix$Kind;
1736:             1             48  [Lorg.apache.cassandra.db.Directories$FileAction;
1737:             1             48  [Lorg.apache.cassandra.db.WriteType;
1738:             2             48  [Lorg.apache.cassandra.exceptions.RequestFailureReason;
1739:             2             48  [Lorg.apache.cassandra.net.RateBasedBackPressure$Flow;
1740:             1             48  [Lorg.apache.cassandra.serializers.UTF8Serializer$UTF8Validator$State;
1741:             1             48  [Lorg.apache.cassandra.service.StorageService$Mode;
1742:             1             48  [Lorg.apache.cassandra.streaming.messages.StreamMessage$Type;
1743:             1             48  [Lorg.apache.cassandra.utils.progress.ProgressEventType;
1744:             1             48  [Lorg.codehaus.jackson.JsonGenerator$Feature;
1745:             1             48  [Lsun.security.x509.NetscapeCertTypeExtension$MapEntry;
1746:             1             48  ch.qos.logback.classic.jmx.JMXConfigurator
1747:             3             48  ch.qos.logback.classic.pattern.EnsureExceptionHandling
1748:             3             48  ch.qos.logback.classic.spi.PackagingDataCalculator
1749:             1             48  ch.qos.logback.core.joran.action.DefinePropertyAction
1750:             1             48  ch.qos.logback.core.joran.spi.InterpretationContext
1751:             1             48  ch.qos.logback.core.joran.spi.Interpreter
1752:             2             48  ch.qos.logback.core.rolling.helper.RenameUtil
1753:             3             48  ch.qos.logback.core.spi.LogbackLock
1754:             2             48  ch.qos.logback.core.subst.Node$Type
1755:             2             48  ch.qos.logback.core.util.FileSize
1756:             2             48  com.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format
1757:             3             48  com.google.common.cache.LocalCache$LocalLoadingCache
1758:             1             48  com.google.common.collect.EmptyImmutableListMultimap
1759:             2             48  com.google.common.collect.HashBiMap$Inverse
1760:             1             48  com.google.common.collect.ImmutableListMultimap
1761:             2             48  com.google.common.collect.ImmutableMultimap$Values
1762:             2             48  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry
1763:             1             48  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory$Mappings
1764:             2             48  com.sun.jmx.mbeanserver.WeakIdentityHashMap
1765:             2             48  com.sun.jmx.remote.internal.ServerNotifForwarder$IdAndFilter
1766:             1             48  com.sun.jna.NativeLibrary
1767:             3             48  com.sun.org.apache.xerces.internal.impl.dv.dtd.ListDatatypeValidator
1768:             2             48  io.netty.buffer.PooledByteBufAllocator$PoolThreadLocalCache
1769:             2             48  io.netty.channel.VoidChannelPromise
1770:             2             48  io.netty.util.Recycler$2
1771:             2             48  io.netty.util.UniqueName
1772:             1             48  io.netty.util.concurrent.GlobalEventExecutor
1773:             3             48  io.netty.util.internal.TypeParameterMatcher$ReflectiveMatcher
1774:             2             48  java.io.ByteArrayOutputStream
1775:             2             48  java.io.File$PathStatus
1776:             3             48  java.io.FileOutputStream$1
1777:             2             48  java.io.OutputStreamWriter
1778:             2             48  java.io.SerialCallbackContext
1779:             3             48  java.lang.Boolean
1780:             3             48  java.lang.Float
1781:             3             48  java.lang.InheritableThreadLocal
1782:             1             48  java.lang.invoke.BoundMethodHandle$Species_L4
1783:             2             48  java.lang.invoke.ConstantCallSite
1784:             2             48  java.lang.invoke.InfoFromMemberName
1785:             2             48  java.lang.invoke.InnerClassLambdaMetafactory$ForwardingMethodGenerator
1786:             2             48  java.lang.management.MemoryType
1787:             2             48  java.lang.reflect.ReflectPermission
1788:             2             48  java.net.InetAddress$Cache
1789:             2             48  java.net.InetAddress$Cache$Type
1790:             2             48  java.net.InetAddress$CacheEntry
1791:             1             48  java.net.NetworkInterface
1792:             2             48  java.net.ServerSocket
1793:             2             48  java.net.SocketPermissionCollection
1794:             2             48  java.net.StandardProtocolFamily
1795:             3             48  java.nio.channels.FileChannel$MapMode
1796:             2             48  java.nio.charset.CoderResult
1797:             3             48  java.nio.charset.CodingErrorAction
1798:             2             48  java.rmi.dgc.Lease
1799:             2             48  java.security.AllPermissionCollection
1800:             3             48  java.text.AttributedCharacterIterator$Attribute
1801:             3             48  java.util.Base64$Decoder
1802:             2             48  java.util.PropertyPermissionCollection
1803:             3             48  java.util.TreeMap$EntrySet
1804:             2             48  java.util.concurrent.Executors$DefaultThreadFactory
1805:             3             48  java.util.concurrent.atomic.AtomicMarkableReference
1806:             2             48  java.util.logging.Logger$LoggerBundle
1807:             1             48  java.util.regex.Pattern$GroupCurly
1808:             2             48  java.util.regex.Pattern$Prolog
1809:             2             48  javax.management.MBeanServerInvocationHandler
1810:             1             48  javax.management.remote.rmi.RMIConnectionImpl$RMIServerCommunicatorAdmin
1811:             1             48  javax.security.auth.SubjectDomainCombiner$WeakKeyValueMap
1812:             1             48  org.antlr.runtime.ANTLRStringStream
1813:             2             48  org.apache.cassandra.cache.AutoSavingCache$2
1814:             2             48  org.apache.cassandra.config.Config$CommitLogSync
1815:             2             48  org.apache.cassandra.config.Config$DiskOptimizationStrategy
1816:             2             48  org.apache.cassandra.config.ParameterizedClass
1817:             2             48  org.apache.cassandra.cql3.Sets$Marker
1818:             2             48  org.apache.cassandra.cql3.Sets$Setter
1819:             2             48  org.apache.cassandra.cql3.functions.FunctionCall
1820:             2             48  org.apache.cassandra.cql3.statements.Bound
1821:             2             48  org.apache.cassandra.db.Directories$OnTxnErr
1822:             2             48  org.apache.cassandra.db.Memtable$LastCommitLogPosition
1823:             2             48  org.apache.cassandra.db.ReadCommand$Kind
1824:             2             48  org.apache.cassandra.db.aggregation.AggregationSpecification$Kind
1825:             1             48  org.apache.cassandra.db.commitlog.CommitLogArchiver
1826:             1             48  org.apache.cassandra.db.compaction.CompactionInfo
1827:             2             48  org.apache.cassandra.db.filter.ClusteringIndexFilter$Kind
1828:             2             48  org.apache.cassandra.db.lifecycle.LifecycleTransaction$State
1829:             2             48  org.apache.cassandra.db.lifecycle.LogReplica
1830:             2             48  org.apache.cassandra.db.rows.Unfiltered$Kind
1831:             2             48  org.apache.cassandra.exceptions.RequestFailureReason
1832:             1             48  org.apache.cassandra.gms.FailureDetector
1833:             2             48  org.apache.cassandra.hints.HintsDispatcher$Action
1834:             1             48  org.apache.cassandra.hints.HintsService
1835:             2             48  org.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason
1836:             1             48  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter
1837:             1             48  org.apache.cassandra.io.sstable.metadata.MetadataCollector$MinMaxLongTracker
1838:             2             48  org.apache.cassandra.io.util.NIODataInputStream
1839:             1             48  org.apache.cassandra.locator.NetworkTopologyStrategy
1840:             3             48  org.apache.cassandra.metrics.CacheMetrics$2
1841:             3             48  org.apache.cassandra.metrics.CacheMetrics$3
1842:             3             48  org.apache.cassandra.metrics.CacheMetrics$4
1843:             3             48  org.apache.cassandra.metrics.CacheMetrics$5
1844:             2             48  org.apache.cassandra.metrics.TableMetrics$Sampler
1845:             1             48  org.apache.cassandra.net.MessagingService$2
1846:             1             48  org.apache.cassandra.net.RateBasedBackPressure
1847:             2             48  org.apache.cassandra.net.RateBasedBackPressure$Flow
1848:             1             48  org.apache.cassandra.repair.messages.RepairOption
1849:             2             48  org.apache.cassandra.schema.CachingParams$Option
1850:             2             48  org.apache.cassandra.schema.KeyspaceParams$Option
1851:             1             48  org.apache.cassandra.service.ActiveRepairService$ParentRepairSession
1852:             2             48  org.apache.cassandra.streaming.ProgressInfo$Direction
1853:             2             48  org.apache.cassandra.transport.Event$StatusChange$Status
1854:             2             48  org.apache.cassandra.transport.Message$Direction
1855:             2             48  org.apache.cassandra.utils.ChecksumType$3
1856:             2             48  org.apache.cassandra.utils.Throwables$FileOpType
1857:             2             48  org.apache.cassandra.utils.btree.BTree$Dir
1858:             2             48  org.apache.cassandra.utils.concurrent.WaitQueue$RegisteredSignal
1859:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotE
1860:             2             48  org.cliffc.high_scale_lib.NonBlockingHashMap$SnapshotK
1861:             1             48  org.codehaus.jackson.JsonFactory
1862:             1             48  org.codehaus.jackson.map.DeserializationConfig
1863:             1             48  org.codehaus.jackson.map.SerializationConfig
1864:             2             48  org.codehaus.jackson.map.deser.std.CalendarDeserializer
1865:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$BooleanDeserializer
1866:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ByteDeserializer
1867:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$CharacterDeserializer
1868:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$DoubleDeserializer
1869:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$FloatDeserializer
1870:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$IntegerDeserializer
1871:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$LongDeserializer
1872:             2             48  org.codehaus.jackson.map.deser.std.StdDeserializer$ShortDeserializer
1873:             2             48  org.codehaus.jackson.map.ser.StdSerializers$BooleanSerializer
1874:             1             48  org.hyperic.sigar.FileSystemMap
1875:             1             48  org.hyperic.sigar.Sigar
1876:             2             48  sun.management.ManagementFactoryHelper$1
1877:             2             48  sun.misc.NativeSignalHandler
1878:             2             48  sun.misc.URLClassPath$FileLoader
1879:             3             48  sun.nio.fs.UnixFileAttributes$UnixAsBasicFileAttributes
1880:             2             48  sun.rmi.server.UnicastServerRef$1
1881:             3             48  sun.rmi.server.WeakClassHashMap$ValueCell
1882:             2             48  sun.security.jca.ProviderList
1883:             2             48  sun.security.jca.ProviderList$3
1884:             2             48  sun.security.provider.DSAParameters
1885:             2             48  sun.security.ssl.SSLAlgorithmConstraints
1886:             3             48  sun.security.util.AlgorithmDecomposer
1887:             2             48  sun.security.util.DisabledAlgorithmConstraints$UsageConstraint
1888:             2             48  sun.security.util.DisabledAlgorithmConstraints$jdkCAConstraint
1889:             3             48  sun.security.x509.RFC822Name
1890:             3             48  sun.text.normalizer.NormalizerBase$QuickCheckResult
1891:             1             48  sun.text.resources.FormatData
1892:             1             48  sun.text.resources.en.FormatData_en
1893:             1             48  sun.text.resources.en.FormatData_en_US
1894:             1             40  [Lch.qos.logback.core.pattern.parser.TokenStream$TokenizerState;
1895:             1             40  [Lch.qos.logback.core.subst.Token$Type;
1896:             1             40  [Lch.qos.logback.core.util.AggregationType;
1897:             1             40  [Lcom.google.common.collect.SortedLists$KeyPresentBehavior;
1898:             2             40  [Lcom.sun.jmx.mbeanserver.ClassLoaderRepositorySupport$LoaderEntry;
1899:             1             40  [Lcom.sun.org.apache.xerces.internal.util.Status;
1900:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$State;
1901:             1             40  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$State;
1902:             1             40  [Ljava.lang.management.MemoryPoolMXBean;
1903:             2             40  [Ljava.util.logging.Handler;
1904:             1             40  [Ljava.util.stream.StreamOpFlag;
1905:             1             40  [Lorg.apache.cassandra.cql3.statements.IndexTarget$Type;
1906:             1             40  [Lorg.apache.cassandra.db.filter.DataLimits$Kind;
1907:             1             40  [Lorg.apache.cassandra.db.lifecycle.LogRecord$Type;
1908:             1             40  [Lorg.apache.cassandra.schema.CompactionParams$Option;
1909:             1             40  [Lorg.apache.cassandra.streaming.StreamSession$State;
1910:             1             40  [Lorg.apache.cassandra.utils.NativeLibrary$OSType;
1911:             1             40  [Lorg.github.jamm.MemoryMeter$Guess;
1912:             1             40  [Lorg.yaml.snakeyaml.DumperOptions$ScalarStyle;
1913:             1             40  [Lsun.security.jca.ServiceId;
1914:             1             40  [Lsun.security.util.DisabledAlgorithmConstraints$Constraint$Operator;
1915:             1             40  [Lsun.util.locale.provider.LocaleProviderAdapter$Type;
1916:             1             40  [[Ljava.lang.invoke.LambdaForm$Name;
1917:             1             40  ch.qos.logback.core.BasicStatusManager
1918:             1             40  ch.qos.logback.core.joran.spi.ConfigurationWatchList
1919:             1             40  com.google.common.collect.AbstractMapBasedMultimap$2
1920:             1             40  com.google.common.collect.AbstractMapBasedMultimap$WrappedSet
1921:             1             40  com.google.common.collect.EmptyImmutableSortedMap
1922:             1             40  com.sun.beans.finder.MethodFinder$1
1923:             1             40  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor
1924:             1             40  com.sun.jmx.mbeanserver.JmxMBeanServer
1925:             1             40  com.sun.jmx.mbeanserver.MBeanServerDelegateImpl
1926:             1             40  io.netty.channel.AbstractChannel$CloseFuture
1927:             1             40  io.netty.channel.DefaultChannelPipeline
1928:             1             40  io.netty.channel.epoll.AbstractEpollServerChannel$EpollServerSocketUnsafe
1929:             1             40  java.beans.WeakIdentityMap$Entry
1930:             1             40  java.lang.reflect.Proxy$Key2
1931:             1             40  java.rmi.NoSuchObjectException
1932:             1             40  java.util.ResourceBundle$1
1933:             1             40  javax.crypto.CryptoAllPermission
1934:             1             40  net.jpountz.lz4.LZ4Factory
1935:             1             40  org.antlr.runtime.CommonTokenStream
1936:             1             40  org.apache.cassandra.concurrent.SharedExecutorPool
1937:             1             40  org.apache.cassandra.config.TransparentDataEncryptionOptions
1938:             1             40  org.apache.cassandra.cql3.CqlLexer
1939:             1             40  org.apache.cassandra.db.commitlog.CommitLog
1940:             1             40  org.apache.cassandra.db.compaction.CompactionTask
1941:             1             40  org.apache.cassandra.exceptions.RepairException
1942:             1             40  org.apache.cassandra.io.sstable.format.big.BigTableWriter$TransactionalProxy
1943:             1             40  org.apache.cassandra.locator.GossipingPropertyFileSnitch
1944:             1             40  org.apache.cassandra.net.MessagingService$1
1945:             1             40  org.apache.cassandra.net.MessagingService$3
1946:             1             40  org.apache.cassandra.streaming.management.StreamEventJMXNotifier
1947:             1             40  org.apache.cassandra.transport.Server
1948:             1             40  org.apache.cassandra.utils.NoSpamLogger$NoSpamLogStatement
1949:             1             40  org.apache.cassandra.utils.memory.SlabPool
1950:             1             40  org.codehaus.jackson.map.util.StdDateFormat
1951:             1             40  sun.management.DiagnosticCommandImpl
1952:             1             40  sun.management.MappedMXBeanType$CompositeDataMXBeanType
1953:             1             40  sun.management.MappedMXBeanType$MapMXBeanType
1954:             1             40  sun.nio.cs.StandardCharsets$Aliases
1955:             1             40  sun.nio.cs.StandardCharsets$Cache
1956:             1             40  sun.nio.cs.StandardCharsets$Classes
1957:             1             40  sun.security.ssl.SSLContextImpl$DefaultSSLContext
1958:             1             32  [Lch.qos.logback.core.rolling.helper.CompressionMode;
1959:             1             32  [Lch.qos.logback.core.spi.FilterReply;
1960:             1             32  [Lch.qos.logback.core.subst.Tokenizer$TokenizerState;
1961:             1             32  [Lcom.github.benmanes.caffeine.cache.Caffeine$Strength;
1962:             1             32  [Lcom.google.common.base.Predicates$ObjectPredicate;
1963:             1             32  [Lcom.google.common.cache.LocalCache$Strength;
1964:             1             32  [Lcom.google.common.collect.AbstractIterator$State;
1965:             1             32  [Lcom.google.common.collect.MapMakerInternalMap$Strength;
1966:             1             32  [Lcom.google.common.collect.SortedLists$KeyAbsentBehavior;
1967:             1             32  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus;
1968:             1             32  [Lcom.sun.beans.util.Cache$Kind;
1969:             1             32  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityManager$NameMap;
1970:             2             32  [Ljava.lang.Enum;
1971:             1             32  [Ljava.lang.OutOfMemoryError;
1972:             1             32  [Ljava.lang.ThreadGroup;
1973:             1             32  [Ljava.lang.UNIXProcess$Platform;
1974:             1             32  [Ljava.lang.management.MemoryManagerMXBean;
1975:             1             32  [Ljava.nio.file.FileTreeWalker$EventType;
1976:             1             32  [Ljava.nio.file.FileVisitResult;
1977:             1             32  [Ljava.text.Normalizer$Form;
1978:             1             32  [Ljava.util.concurrent.atomic.AtomicReference;
1979:             1             32  [Ljava.util.stream.MatchOps$MatchKind;
1980:             1             32  [Ljava.util.stream.StreamShape;
1981:             1             32  [Lnet.jpountz.util.Native$OS;
1982:             1             32  [Lorg.apache.cassandra.auth.DataResource$Level;
1983:             1             32  [Lorg.apache.cassandra.auth.IRoleManager$Option;
1984:             1             32  [Lorg.apache.cassandra.config.Config$DiskAccessMode;
1985:             1             32  [Lorg.apache.cassandra.config.Config$UserFunctionTimeoutPolicy;
1986:             1             32  [Lorg.apache.cassandra.config.ReadRepairDecision;
1987:             1             32  [Lorg.apache.cassandra.cql3.AssignmentTestable$TestResult;
1988:             1             32  [Lorg.apache.cassandra.cql3.statements.StatementType;
1989:             1             32  [Lorg.apache.cassandra.db.Conflicts$Resolution;
1990:             1             32  [Lorg.apache.cassandra.db.Directories$FileType;
1991:             1             32  [Lorg.apache.cassandra.db.commitlog.CommitLogSegment$CDCState;
1992:             1             32  [Lorg.apache.cassandra.db.context.CounterContext$Relationship;
1993:             1             32  [Lorg.apache.cassandra.db.lifecycle.SSTableSet;
1994:             1             32  [Lorg.apache.cassandra.db.marshal.AbstractType$ComparisonType;
1995:             1             32  [Lorg.apache.cassandra.db.marshal.CollectionType$Kind;
1996:             1             32  [Lorg.apache.cassandra.db.monitoring.MonitoringState;
1997:             1             32  [Lorg.apache.cassandra.db.rows.SerializationHelper$Flag;
1998:             1             32  [Lorg.apache.cassandra.hints.HintsDispatcher$Callback$Outcome;
1999:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReader$OpenReason;
2000:             1             32  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SkippingReason;
2001:             1             32  [Lorg.apache.cassandra.repair.RepairParallelism;
2002:             1             32  [Lorg.apache.cassandra.repair.SystemDistributedKeyspace$RepairState;
2003:             1             32  [Lorg.apache.cassandra.schema.SpeculativeRetryParam$Kind;
2004:             1             32  [Lorg.apache.cassandra.service.CacheService$CacheType;
2005:             1             32  [Lorg.apache.cassandra.streaming.StreamEvent$Type;
2006:             1             32  [Lorg.apache.cassandra.utils.AbstractIterator$State;
2007:             1             32  [Lorg.apache.cassandra.utils.AsymmetricOrdering$Op;
2008:             1             32  [Lorg.apache.cassandra.utils.NoSpamLogger$Level;
2009:             1             32  [Lorg.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional$State;
2010:             1             32  [Lorg.apache.cassandra.utils.memory.MemtableAllocator$LifeCycle;
2011:             2             32  [Lorg.codehaus.jackson.type.JavaType;
2012:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$FlowStyle;
2013:             1             32  [Lorg.yaml.snakeyaml.DumperOptions$LineBreak;
2014:             1             32  [Lorg.yaml.snakeyaml.introspector.BeanAccess;
2015:             1             32  [Lsun.misc.FormattedFloatingDecimal$Form;
2016:             1             32  [Lsun.misc.ObjectInputFilter$Status;
2017:             1             32  [Lsun.security.provider.NativePRNG$Variant;
2018:             1             32  [Lsun.security.ssl.CipherSuite$CipherType;
2019:             1             32  [Lsun.security.ssl.CipherSuite$PRF;
2020:             1             32  [[Lcom.google.common.collect.MapMakerInternalMap$EntryFactory;
2021:             1             32  ch.qos.logback.classic.joran.JoranConfigurator
2022:             1             32  ch.qos.logback.classic.joran.action.ConfigurationAction
2023:             1             32  ch.qos.logback.classic.joran.action.EvaluatorAction
2024:             1             32  ch.qos.logback.classic.joran.action.LoggerAction
2025:             1             32  ch.qos.logback.classic.joran.action.LoggerContextListenerAction
2026:             1             32  ch.qos.logback.classic.joran.action.ReceiverAction
2027:             1             32  ch.qos.logback.classic.joran.action.RootLoggerAction
2028:             1             32  ch.qos.logback.classic.sift.SiftAction
2029:             1             32  ch.qos.logback.classic.spi.LoggerContextVO
2030:             1             32  ch.qos.logback.core.helpers.CyclicBuffer
2031:             1             32  ch.qos.logback.core.joran.action.AppenderAction
2032:             1             32  ch.qos.logback.core.joran.action.ConversionRuleAction
2033:             1             32  ch.qos.logback.core.joran.action.IncludeAction
2034:             1             32  ch.qos.logback.core.joran.action.NestedBasicPropertyIA
2035:             1             32  ch.qos.logback.core.joran.action.NestedComplexPropertyIA
2036:             1             32  ch.qos.logback.core.joran.action.NewRuleAction
2037:             1             32  ch.qos.logback.core.joran.action.ParamAction
2038:             1             32  ch.qos.logback.core.joran.action.ShutdownHookAction
2039:             1             32  ch.qos.logback.core.joran.action.StatusListenerAction
2040:             1             32  ch.qos.logback.core.joran.action.TimestampAction
2041:             1             32  ch.qos.logback.core.joran.conditional.ElseAction
2042:             1             32  ch.qos.logback.core.joran.conditional.IfAction
2043:             1             32  ch.qos.logback.core.joran.conditional.ThenAction
2044:             1             32  ch.qos.logback.core.joran.spi.SimpleRuleStore
2045:             2             32  ch.qos.logback.core.spi.AppenderAttachableImpl
2046:             1             32  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache
2047:             1             32  com.github.benmanes.caffeine.cache.FrequencySketch
2048:             2             32  com.google.common.base.Joiner
2049:             2             32  com.google.common.base.Predicates$InPredicate
2050:             1             32  com.google.common.collect.AbstractMapBasedMultimap$NavigableKeySet
2051:             1             32  com.google.common.collect.EmptyImmutableBiMap
2052:             2             32  com.google.common.util.concurrent.Striped$2
2053:             2             32  com.sun.beans.WeakCache
2054:             1             32  com.sun.beans.finder.BeanInfoFinder
2055:             1             32  com.sun.jmx.mbeanserver.Repository
2056:             1             32  com.sun.jmx.remote.internal.ArrayQueue
2057:             1             32  com.sun.jmx.remote.security.JMXSubjectDomainCombiner
2058:             1             32  com.sun.org.apache.xerces.internal.impl.XMLEntityScanner$1
2059:             2             32  com.sun.org.apache.xerces.internal.impl.dv.dtd.ENTITYDatatypeValidator
2060:             2             32  com.sun.proxy.$Proxy5
2061:             1             32  io.netty.bootstrap.ServerBootstrap$ServerBootstrapAcceptor
2062:             1             32  io.netty.channel.epoll.EpollEventLoopGroup
2063:             2             32  io.netty.channel.group.ChannelMatchers$ClassMatcher
2064:             1             32  io.netty.util.concurrent.DefaultThreadFactory
2065:             2             32  io.netty.util.internal.logging.Slf4JLoggerFactory
2066:             1             32  java.beans.ThreadGroupContext
2067:             1             32  java.beans.ThreadGroupContext$1
2068:             2             32  java.io.ObjectStreamClass$1
2069:             2             32  java.io.ObjectStreamClass$3
2070:             2             32  java.io.ObjectStreamClass$4
2071:             2             32  java.io.ObjectStreamClass$5
2072:             1             32  java.io.UnixFileSystem
2073:             1             32  java.lang.ArithmeticException
2074:             1             32  java.lang.ArrayIndexOutOfBoundsException
2075:             1             32  java.lang.ClassCastException
2076:             1             32  java.lang.Exception
2077:             1             32  java.lang.NullPointerException
2078:             2             32  java.lang.Shutdown$Lock
2079:             1             32  java.lang.UnsupportedOperationException
2080:             1             32  java.lang.reflect.WeakCache
2081:             1             32  java.lang.reflect.WeakCache$CacheKey
2082:             2             32  java.nio.ByteOrder
2083:             1             32  java.nio.channels.NotYetConnectedException
2084:             1             32  java.text.DontCareFieldPosition
2085:             2             32  java.util.Hashtable$EntrySet
2086:             1             32  java.util.PriorityQueue
2087:             1             32  java.util.TreeMap$EntryIterator
2088:             1             32  java.util.TreeMap$KeyIterator
2089:             1             32  java.util.concurrent.CancellationException
2090:             1             32  java.util.concurrent.ConcurrentSkipListMap$KeyIterator
2091:             2             32  java.util.concurrent.ConcurrentSkipListMap$KeySet
2092:             1             32  java.util.concurrent.FutureTask
2093:             1             32  java.util.concurrent.ThreadLocalRandom
2094:             2             32  java.util.logging.ErrorManager
2095:             1             32  java.util.logging.LogManager$SystemLoggerContext
2096:             1             32  java.util.regex.Pattern$3
2097:             1             32  javax.crypto.spec.RC5ParameterSpec
2098:             2             32  javax.management.NotificationFilterSupport
2099:             1             32  javax.management.remote.JMXServiceURL
2100:             1             32  javax.security.auth.Subject
2101:             1             32  net.jpountz.xxhash.XXHashFactory
2102:             1             32  org.apache.cassandra.auth.CassandraRoleManager
2103:             1             32  org.apache.cassandra.batchlog.BatchlogManager
2104:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache
2105:             2             32  org.apache.cassandra.cache.ConcurrentLinkedHashCache$1
2106:             1             32  org.apache.cassandra.config.Schema
2107:             1             32  org.apache.cassandra.cql3.QueryOptions$SpecificOptions
2108:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$4
2109:             1             32  org.apache.cassandra.cql3.functions.TimeFcts$5
2110:             2             32  org.apache.cassandra.db.RangeSliceVerbHandler
2111:             1             32  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool
2112:             1             32  org.apache.cassandra.db.compaction.CompactionManager
2113:             2             32  org.apache.cassandra.db.lifecycle.LogReplicaSet
2114:             2             32  org.apache.cassandra.db.lifecycle.LogTransaction$TransactionTidier
2115:             1             32  org.apache.cassandra.db.marshal.AsciiType
2116:             1             32  org.apache.cassandra.db.marshal.PartitionerDefinedOrder
2117:             2             32  org.apache.cassandra.db.rows.CellPath$EmptyCellPath
2118:             2             32  org.apache.cassandra.dht.AbstractBounds$AbstractBoundsSerializer
2119:             1             32  org.apache.cassandra.hints.HintsBuffer
2120:             1             32  org.apache.cassandra.hints.HintsBufferPool
2121:             1             32  org.apache.cassandra.hints.HintsDispatchExecutor
2122:             1             32  org.apache.cassandra.hints.HintsDispatchTrigger
2123:             1             32  org.apache.cassandra.index.internal.composites.CollectionKeyIndex
2124:             1             32  org.apache.cassandra.io.compress.CompressedSequentialWriter$TransactionalProxy
2125:             1             32  org.apache.cassandra.io.compress.LZ4Compressor
2126:             1             32  org.apache.cassandra.io.sstable.IndexSummaryManager
2127:             1             32  org.apache.cassandra.metrics.CQLMetrics
2128:             2             32  org.apache.cassandra.metrics.ClientMetrics$$Lambda$278/1979648826
2129:             1             32  org.apache.cassandra.metrics.CommitLogMetrics
2130:             1             32  org.apache.cassandra.metrics.CompactionMetrics
2131:             2             32  org.apache.cassandra.metrics.TableMetrics$AllTableMetricNameFactory
2132:             2             32  org.apache.cassandra.net.ResponseVerbHandler
2133:             1             32  org.apache.cassandra.repair.RepairRunnable
2134:             2             32  org.apache.cassandra.schema.Types
2135:             1             32  org.apache.cassandra.security.EncryptionContext
2136:             1             32  org.apache.cassandra.service.ActiveRepairService
2137:             1             32  org.apache.cassandra.service.CassandraDaemon
2138:             1             32  org.apache.cassandra.service.NativeTransportService
2139:             1             32  org.apache.cassandra.thrift.TCustomServerSocket
2140:             1             32  org.apache.cassandra.thrift.ThriftServer
2141:             1             32  org.apache.cassandra.utils.ExpiringMap
2142:             2             32  org.apache.cassandra.utils.IntegerInterval$Set
2143:             1             32  org.apache.cassandra.utils.ResourceWatcher$WatchedResource
2144:             1             32  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder
2145:             1             32  org.apache.cassandra.utils.btree.BTree$1
2146:             1             32  org.apache.cassandra.utils.btree.TreeBuilder$1
2147:             1             32  org.apache.cassandra.utils.concurrent.WaitQueue$TimedSignal
2148:             1             32  org.apache.cassandra.utils.memory.BufferPool$GlobalPool
2149:             1             32  org.apache.thrift.protocol.TBinaryProtocol$Factory
2150:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$2
2151:             2             32  org.cliffc.high_scale_lib.NonBlockingHashMap$3
2152:             1             32  org.codehaus.jackson.map.deser.BeanDeserializerFactory$ConfigImpl
2153:             1             32  org.codehaus.jackson.map.deser.StdDeserializerProvider
2154:             1             32  org.codehaus.jackson.map.introspect.VisibilityChecker$Std
2155:             2             32  org.codehaus.jackson.map.ser.StdSerializers$NumberSerializer
2156:             2             32  org.codehaus.jackson.map.ser.std.StdKeySerializer
2157:             1             32  org.codehaus.jackson.map.type.TypeFactory
2158:             2             32  org.codehaus.jackson.map.util.RootNameLookup
2159:             1             32  org.github.jamm.MemoryMeter
2160:             1             32  sun.instrument.InstrumentationImpl
2161:             1             32  sun.management.GcInfoCompositeData
2162:             1             32  sun.management.MappedMXBeanType$InProgress
2163:             1             32  sun.nio.ch.ServerSocketAdaptor
2164:             2             32  sun.nio.ch.SocketDispatcher
2165:             1             32  sun.nio.cs.StandardCharsets
2166:             1             32  sun.nio.fs.LinuxFileSystem
2167:             1             32  sun.reflect.UnsafeIntegerFieldAccessorImpl
2168:             1             32  sun.reflect.UnsafeQualifiedObjectFieldAccessorImpl
2169:             2             32  sun.rmi.server.UnicastRef
2170:             2             32  sun.rmi.server.UnicastRef2
2171:             2             32  sun.rmi.transport.DGCImpl$1
2172:             1             32  sun.rmi.transport.proxy.RMIMasterSocketFactory
2173:             1             32  sun.rmi.transport.tcp.TCPTransport$AcceptLoop
2174:             1             32  sun.security.provider.SecureRandom
2175:             2             32  sun.security.ssl.SSLAlgorithmDecomposer
2176:             1             32  sun.security.ssl.X509TrustManagerImpl
2177:             1             32  sun.security.validator.SimpleValidator
2178:             1             32  sun.security.x509.AuthorityInfoAccessExtension
2179:             1             32  sun.security.x509.IssuerAlternativeNameExtension
2180:             1             32  sun.security.x509.PolicyMappingsExtension
2181:             1             32  sun.util.locale.provider.LocaleResources
2182:             1             24  [Lch.qos.logback.core.joran.spi.ConsoleTarget;
2183:             1             24  [Lch.qos.logback.core.subst.Node$Type;
2184:             1             24  [Lcom.clearspring.analytics.stream.cardinality.HyperLogLogPlus$Format;
2185:             1             24  [Lcom.github.benmanes.caffeine.cache.Buffer;
2186:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledTicker;
2187:             1             24  [Lcom.github.benmanes.caffeine.cache.DisabledWriter;
2188:             1             24  [Lcom.github.benmanes.caffeine.cache.SingletonWeigher;
2189:             1             24  [Lcom.github.benmanes.caffeine.cache.stats.DisabledStatsCounter;
2190:             1             24  [Lcom.google.common.base.Functions$IdentityFunction;
2191:             1             24  [Lcom.google.common.cache.CacheBuilder$NullListener;
2192:             1             24  [Lcom.google.common.cache.CacheBuilder$OneWeigher;
2193:             1             24  [Lcom.google.common.collect.GenericMapMaker$NullListener;
2194:             1             24  [Lcom.google.common.collect.Maps$EntryFunction;
2195:             1             24  [Lcom.google.common.util.concurrent.MoreExecutors$DirectExecutor;
2196:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener;
2197:             1             24  [Lcom.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher;
2198:             1             24  [Lcom.sun.org.apache.xerces.internal.utils.XMLSecurityPropertyManager$Property;
2199:             1             24  [Ljava.io.File$PathStatus;
2200:             1             24  [Ljava.lang.ClassValue$Entry;
2201:             1             24  [Ljava.lang.management.MemoryType;
2202:             1             24  [Ljava.net.InetAddress$Cache$Type;
2203:             1             24  [Ljava.net.InterfaceAddress;
2204:             1             24  [Ljava.net.StandardProtocolFamily;
2205:             1             24  [Ljava.rmi.server.ObjID;
2206:             1             24  [Ljava.util.Comparators$NaturalOrderComparator;
2207:             1             24  [Ljava.util.Locale$Category;
2208:             1             24  [Ljava.util.concurrent.ExecutorService;
2209:             1             24  [Ljava.util.concurrent.ThreadPoolExecutor;
2210:             1             24  [Ljavax.net.ssl.KeyManager;
2211:             1             24  [Ljavax.net.ssl.TrustManager;
2212:             1             24  [Lorg.apache.cassandra.concurrent.ExecutorLocal;
2213:             1             24  [Lorg.apache.cassandra.config.Config$DiskOptimizationStrategy;
2214:             1             24  [Lorg.apache.cassandra.config.Config$RequestSchedulerId;
2215:             1             24  [Lorg.apache.cassandra.cql3.QueryProcessor$InternalStateInstance;
2216:             1             24  [Lorg.apache.cassandra.cql3.Term;
2217:             1             24  [Lorg.apache.cassandra.cql3.statements.Bound;
2218:             1             24  [Lorg.apache.cassandra.db.Directories$OnTxnErr;
2219:             1             24  [Lorg.apache.cassandra.db.ReadCommand$Kind;
2220:             1             24  [Lorg.apache.cassandra.db.aggregation.AggregationSpecification$Kind;
2221:             1             24  [Lorg.apache.cassandra.db.filter.ClusteringIndexFilter$Kind;
2222:             1             24  [Lorg.apache.cassandra.db.rows.Unfiltered$Kind;
2223:             1             24  [Lorg.apache.cassandra.hints.HintsDispatcher$Action;
2224:             1             24  [Lorg.apache.cassandra.io.compress.BufferType;
2225:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableFormat$Type;
2226:             1             24  [Lorg.apache.cassandra.io.sstable.format.SSTableReadsListener$SelectionReason;
2227:             1             24  [Lorg.apache.cassandra.metrics.TableMetrics$Sampler;
2228:             1             24  [Lorg.apache.cassandra.schema.CachingParams$Option;
2229:             1             24  [Lorg.apache.cassandra.schema.KeyspaceParams$Option;
2230:             1             24  [Lorg.apache.cassandra.streaming.ProgressInfo$Direction;
2231:             1             24  [Lorg.apache.cassandra.transport.Event$StatusChange$Status;
2232:             1             24  [Lorg.apache.cassandra.transport.Message$Direction;
2233:             1             24  [Lorg.apache.cassandra.utils.ChecksumType;
2234:             1             24  [Lorg.apache.cassandra.utils.Throwables$FileOpType;
2235:             1             24  [Lorg.apache.cassandra.utils.btree.BTree$Dir;
2236:             1             24  [Lsun.launcher.LauncherHelper;
2237:             1             24  [Lsun.security.ssl.EphemeralKeyManager$EphemeralKeyPair;
2238:             1             24  ch.qos.logback.classic.joran.action.ConsolePluginAction
2239:             1             24  ch.qos.logback.classic.joran.action.ContextNameAction
2240:             1             24  ch.qos.logback.classic.joran.action.InsertFromJNDIAction
2241:             1             24  ch.qos.logback.classic.joran.action.JMXConfiguratorAction
2242:             1             24  ch.qos.logback.classic.spi.TurboFilterList
2243:             1             24  ch.qos.logback.classic.util.ContextSelectorStaticBinder
2244:             1             24  ch.qos.logback.classic.util.LogbackMDCAdapter
2245:             1             24  ch.qos.logback.core.joran.action.ContextPropertyAction
2246:             1             24  ch.qos.logback.core.joran.spi.CAI_WithLocatorSupport
2247:             1             24  ch.qos.logback.core.joran.spi.EventPlayer
2248:             1             24  com.clearspring.analytics.stream.cardinality.RegisterSet
2249:             1             24  com.codahale.metrics.MetricRegistry
2250:             1             24  com.github.benmanes.caffeine.cache.BoundedBuffer
2251:             1             24  com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask
2252:             1             24  com.github.benmanes.caffeine.cache.DisabledTicker
2253:             1             24  com.github.benmanes.caffeine.cache.DisabledWriter
2254:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$1
2255:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$10
2256:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$100
2257:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$101
2258:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$102
2259:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$103
2260:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$104
2261:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$105
2262:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$106
2263:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$107
2264:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$108
2265:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$109
2266:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$11
2267:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$110
2268:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$111
2269:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$112
2270:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$113
2271:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$114
2272:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$115
2273:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$116
2274:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$117
2275:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$118
2276:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$119
2277:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$12
2278:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$120
2279:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$121
2280:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$122
2281:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$123
2282:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$124
2283:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$125
2284:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$126
2285:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$127
2286:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$128
2287:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$129
2288:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$13
2289:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$130
2290:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$131
2291:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$132
2292:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$133
2293:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$134
2294:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$135
2295:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$136
2296:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$137
2297:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$138
2298:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$139
2299:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$14
2300:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$140
2301:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$141
2302:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$142
2303:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$143
2304:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$144
2305:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$15
2306:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$16
2307:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$17
2308:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$18
2309:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$19
2310:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$2
2311:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$20
2312:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$21
2313:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$22
2314:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$23
2315:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$24
2316:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$25
2317:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$26
2318:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$27
2319:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$28
2320:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$29
2321:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$3
2322:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$30
2323:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$31
2324:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$32
2325:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$33
2326:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$34
2327:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$35
2328:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$36
2329:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$37
2330:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$38
2331:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$39
2332:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$4
2333:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$40
2334:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$41
2335:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$42
2336:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$43
2337:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$44
2338:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$45
2339:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$46
2340:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$47
2341:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$48
2342:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$49
2343:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$5
2344:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$50
2345:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$51
2346:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$52
2347:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$53
2348:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$54
2349:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$55
2350:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$56
2351:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$57
2352:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$58
2353:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$59
2354:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$6
2355:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$60
2356:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$61
2357:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$62
2358:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$63
2359:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$64
2360:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$65
2361:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$66
2362:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$67
2363:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$68
2364:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$69
2365:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$7
2366:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$70
2367:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$71
2368:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$72
2369:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$73
2370:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$74
2371:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$75
2372:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$76
2373:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$77
2374:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$78
2375:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$79
2376:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$8
2377:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$80
2378:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$81
2379:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$82
2380:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$83
2381:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$84
2382:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$85
2383:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$86
2384:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$87
2385:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$88
2386:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$89
2387:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$9
2388:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$90
2389:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$91
2390:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$92
2391:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$93
2392:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$94
2393:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$95
2394:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$96
2395:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$97
2396:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$98
2397:             1             24  com.github.benmanes.caffeine.cache.NodeFactory$99
2398:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$1
2399:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$2
2400:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$3
2401:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$4
2402:             1             24  com.github.benmanes.caffeine.cache.RemovalCause$5
2403:             1             24  com.github.benmanes.caffeine.cache.SingletonWeigher
2404:             1             24  com.github.benmanes.caffeine.cache.stats.DisabledStatsCounter
2405:             1             24  com.google.common.base.CharMatcher$Or
2406:             1             24  com.google.common.base.Functions$IdentityFunction
2407:             1             24  com.google.common.base.Joiner$1
2408:             1             24  com.google.common.base.Joiner$MapJoiner
2409:             1             24  com.google.common.base.Predicates$ObjectPredicate$1
2410:             1             24  com.google.common.base.Predicates$ObjectPredicate$2
2411:             1             24  com.google.common.base.Predicates$ObjectPredicate$3
2412:             1             24  com.google.common.base.Predicates$ObjectPredicate$4
2413:             1             24  com.google.common.cache.CacheBuilder$NullListener
2414:             1             24  com.google.common.cache.CacheBuilder$OneWeigher
2415:             1             24  com.google.common.cache.LocalCache$EntryFactory$1
2416:             1             24  com.google.common.cache.LocalCache$EntryFactory$2
2417:             1             24  com.google.common.cache.LocalCache$EntryFactory$3
2418:             1             24  com.google.common.cache.LocalCache$EntryFactory$4
2419:             1             24  com.google.common.cache.LocalCache$EntryFactory$5
2420:             1             24  com.google.common.cache.LocalCache$EntryFactory$6
2421:             1             24  com.google.common.cache.LocalCache$EntryFactory$7
2422:             1             24  com.google.common.cache.LocalCache$EntryFactory$8
2423:             1             24  com.google.common.cache.LocalCache$Strength$1
2424:             1             24  com.google.common.cache.LocalCache$Strength$2
2425:             1             24  com.google.common.cache.LocalCache$Strength$3
2426:             1             24  com.google.common.collect.ByFunctionOrdering
2427:             1             24  com.google.common.collect.ConcurrentHashMultiset
2428:             1             24  com.google.common.collect.EmptyImmutableSortedSet
2429:             1             24  com.google.common.collect.GenericMapMaker$NullListener
2430:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$1
2431:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$2
2432:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$3
2433:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$4
2434:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$5
2435:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$6
2436:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$7
2437:             1             24  com.google.common.collect.MapMakerInternalMap$EntryFactory$8
2438:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$1
2439:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$2
2440:             1             24  com.google.common.collect.MapMakerInternalMap$Strength$3
2441:             1             24  com.google.common.collect.Maps$EntryFunction$1
2442:             1             24  com.google.common.collect.Maps$EntryFunction$2
2443:             1             24  com.google.common.collect.Sets$3
2444:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$1
2445:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$2
2446:             1             24  com.google.common.collect.SortedLists$KeyAbsentBehavior$3
2447:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$1
2448:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$2
2449:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$3
2450:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$4
2451:             1             24  com.google.common.collect.SortedLists$KeyPresentBehavior$5
2452:             1             24  com.google.common.util.concurrent.Futures$1$1
2453:             1             24  com.google.common.util.concurrent.Futures$ChainingListenableFuture$1
2454:             1             24  com.google.common.util.concurrent.MoreExecutors$DirectExecutor
2455:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentHashMapV8$KeySetView
2456:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingListener
2457:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$1
2458:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$2
2459:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DrainStatus$3
2460:             1             24  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$KeySet
2461:             1             24  com.googlecode.concurrentlinkedhashmap.Weighers$SingletonEntryWeigher
2462:             1             24  com.sun.beans.util.Cache$Kind$1
2463:             1             24  com.sun.beans.util.Cache$Kind$2
2464:             1             24  com.sun.beans.util.Cache$Kind$3
2465:             1             24  com.sun.jmx.mbeanserver.ClassLoaderRepositorySupport
2466:             1             24  com.sun.jmx.mbeanserver.MXBeanLookup
2467:             1             24  com.sun.jmx.remote.internal.ArrayNotificationBuffer$ShareBuffer
2468:             1             24  com.sun.jna.Structure$3
2469:             1             24  com.sun.org.apache.xerces.internal.impl.Constants$ArrayEnumeration
2470:             1             24  io.netty.buffer.UnpooledByteBufAllocator
2471:             1             24  io.netty.channel.AdaptiveRecvByteBufAllocator
2472:             1             24  io.netty.channel.SucceededChannelFuture
2473:             1             24  io.netty.channel.unix.Socket
2474:             1             24  io.netty.util.concurrent.FailedFuture
2475:             1             24  java.lang.ClassValue$Version
2476:             1             24  java.lang.Package$1
2477:             1             24  java.lang.ProcessEnvironment$StringEnvironment
2478:             1             24  java.lang.invoke.MethodHandleImpl$4
2479:             1             24  java.lang.invoke.MethodType$ConcurrentWeakInternSet
2480:             1             24  java.math.MutableBigInteger
2481:             1             24  java.net.Inet4AddressImpl
2482:             1             24  java.net.InterfaceAddress
2483:             1             24  java.nio.file.FileVisitOption
2484:             1             24  java.nio.file.LinkOption
2485:             1             24  java.security.CodeSigner
2486:             1             24  java.security.Policy$PolicyInfo
2487:             1             24  java.security.Policy$UnsupportedEmptyCollection
2488:             1             24  java.util.Collections$EmptyMap
2489:             1             24  java.util.Collections$UnmodifiableList
2490:             1             24  java.util.Comparators$NaturalOrderComparator
2491:             1             24  java.util.Currency
2492:             1             24  java.util.Locale$Cache
2493:             1             24  java.util.OptionalLong
2494:             1             24  java.util.ResourceBundle$Control$CandidateListCache
2495:             1             24  java.util.Vector$1
2496:             1             24  java.util.concurrent.Executors$DelegatedScheduledExecutorService
2497:             1             24  java.util.concurrent.TimeUnit$1
2498:             1             24  java.util.concurrent.TimeUnit$2
2499:             1             24  java.util.concurrent.TimeUnit$3
2500:             1             24  java.util.concurrent.TimeUnit$4
2501:             1             24  java.util.concurrent.TimeUnit$5
2502:             1             24  java.util.concurrent.TimeUnit$6
2503:             1             24  java.util.concurrent.TimeUnit$7
2504:             1             24  java.util.logging.LogManager$5
2505:             1             24  java.util.logging.LogManager$LoggerContext
2506:             1             24  java.util.logging.LoggingPermission
2507:             1             24  java.util.regex.Pattern$SingleI
2508:             1             24  javax.crypto.spec.RC2ParameterSpec
2509:             1             24  javax.management.NotificationBroadcasterSupport
2510:             1             24  javax.net.ssl.SSLContext
2511:             1             24  org.antlr.runtime.CharStreamState
2512:             1             24  org.apache.cassandra.auth.CassandraAuthorizer
2513:             1             24  org.apache.cassandra.auth.CassandraRoleManager$Role
2514:             1             24  org.apache.cassandra.auth.PasswordAuthenticator
2515:             1             24  org.apache.cassandra.cache.ChunkCache
2516:             1             24  org.apache.cassandra.config.Config$1
2517:             1             24  org.apache.cassandra.config.Config$RequestSchedulerId
2518:             1             24  org.apache.cassandra.config.RequestSchedulerOptions
2519:             1             24  org.apache.cassandra.cql3.Attributes$Raw
2520:             1             24  org.apache.cassandra.cql3.ColumnConditions
2521:             1             24  org.apache.cassandra.cql3.CqlParser
2522:             1             24  org.apache.cassandra.cql3.ErrorCollector
2523:             1             24  org.apache.cassandra.cql3.Lists$Marker
2524:             1             24  org.apache.cassandra.cql3.Maps$DiscarderByKey
2525:             1             24  org.apache.cassandra.cql3.Maps$Marker
2526:             1             24  org.apache.cassandra.cql3.Maps$Setter
2527:             1             24  org.apache.cassandra.cql3.Operator$1
2528:             1             24  org.apache.cassandra.cql3.Operator$10
2529:             1             24  org.apache.cassandra.cql3.Operator$11
2530:             1             24  org.apache.cassandra.cql3.Operator$12
2531:             1             24  org.apache.cassandra.cql3.Operator$13
2532:             1             24  org.apache.cassandra.cql3.Operator$14
2533:             1             24  org.apache.cassandra.cql3.Operator$15
2534:             1             24  org.apache.cassandra.cql3.Operator$2
2535:             1             24  org.apache.cassandra.cql3.Operator$3
2536:             1             24  org.apache.cassandra.cql3.Operator$4
2537:             1             24  org.apache.cassandra.cql3.Operator$5
2538:             1             24  org.apache.cassandra.cql3.Operator$6
2539:             1             24  org.apache.cassandra.cql3.Operator$7
2540:             1             24  org.apache.cassandra.cql3.Operator$8
2541:             1             24  org.apache.cassandra.cql3.Operator$9
2542:             1             24  org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance
2543:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$1
2544:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$10
2545:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$11
2546:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$12
2547:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$13
2548:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$14
2549:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$15
2550:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$16
2551:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$17
2552:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$18
2553:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$19
2554:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$2
2555:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$20
2556:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$21
2557:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$3
2558:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$4
2559:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$5
2560:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$6
2561:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$7
2562:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$8
2563:             1             24  org.apache.cassandra.cql3.functions.AggregateFcts$9
2564:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$3
2565:             1             24  org.apache.cassandra.cql3.functions.BytesConversionFcts$4
2566:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$1
2567:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$10
2568:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$11
2569:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$12
2570:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$2
2571:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$3
2572:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$6
2573:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$7
2574:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$8
2575:             1             24  org.apache.cassandra.cql3.functions.TimeFcts$9
2576:             1             24  org.apache.cassandra.cql3.functions.UuidFcts$1
2577:             1             24  org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$InRestrictionWithMarker
2578:             1             24  org.apache.cassandra.cql3.restrictions.TermSlice
2579:             1             24  org.apache.cassandra.cql3.restrictions.TokenRestriction$SliceRestriction
2580:             1             24  org.apache.cassandra.cql3.statements.StatementType$1
2581:             1             24  org.apache.cassandra.cql3.statements.StatementType$2
2582:             1             24  org.apache.cassandra.cql3.statements.StatementType$3
2583:             1             24  org.apache.cassandra.cql3.statements.StatementType$4
2584:             1             24  org.apache.cassandra.db.BlacklistedDirectories
2585:             1             24  org.apache.cassandra.db.Clustering$1
2586:             1             24  org.apache.cassandra.db.Clustering$2
2587:             1             24  org.apache.cassandra.db.Slice$1
2588:             1             24  org.apache.cassandra.db.commitlog.CommitLog$Configuration
2589:             1             24  org.apache.cassandra.db.compaction.CompactionLogger$CompactionLogSerializer
2590:             1             24  org.apache.cassandra.db.filter.DataLimits$1
2591:             1             24  org.apache.cassandra.db.filter.DataLimits$CQLLimits
2592:             1             24  org.apache.cassandra.db.marshal.AsciiType$1
2593:             1             24  org.apache.cassandra.db.marshal.BooleanType
2594:             1             24  org.apache.cassandra.db.marshal.ByteType
2595:             1             24  org.apache.cassandra.db.marshal.BytesType
2596:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$1
2597:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$2
2598:             1             24  org.apache.cassandra.db.marshal.CollectionType$Kind$3
2599:             1             24  org.apache.cassandra.db.marshal.CounterColumnType
2600:             1             24  org.apache.cassandra.db.marshal.DecimalType
2601:             1             24  org.apache.cassandra.db.marshal.DoubleType
2602:             1             24  org.apache.cassandra.db.marshal.DurationType
2603:             1             24  org.apache.cassandra.db.marshal.EmptyType
2604:             1             24  org.apache.cassandra.db.marshal.FloatType
2605:             1             24  org.apache.cassandra.db.marshal.InetAddressType
2606:             1             24  org.apache.cassandra.db.marshal.Int32Type
2607:             1             24  org.apache.cassandra.db.marshal.IntegerType
2608:             1             24  org.apache.cassandra.db.marshal.LongType
2609:             1             24  org.apache.cassandra.db.marshal.ShortType
2610:             1             24  org.apache.cassandra.db.marshal.SimpleDateType
2611:             1             24  org.apache.cassandra.db.marshal.TimeType
2612:             1             24  org.apache.cassandra.db.marshal.TimeUUIDType
2613:             1             24  org.apache.cassandra.db.marshal.TimestampType
2614:             1             24  org.apache.cassandra.db.marshal.TypeParser
2615:             1             24  org.apache.cassandra.db.marshal.UTF8Type
2616:             1             24  org.apache.cassandra.db.marshal.UUIDType
2617:             1             24  org.apache.cassandra.db.transform.Stack
2618:             1             24  org.apache.cassandra.dht.Murmur3Partitioner
2619:             1             24  org.apache.cassandra.dht.Murmur3Partitioner$1
2620:             1             24  org.apache.cassandra.hints.HintsCatalog
2621:             1             24  org.apache.cassandra.hints.HintsWriteExecutor
2622:             1             24  org.apache.cassandra.io.compress.BufferType$1
2623:             1             24  org.apache.cassandra.io.compress.BufferType$2
2624:             1             24  org.apache.cassandra.io.util.ChecksumWriter
2625:             1             24  org.apache.cassandra.io.util.SequentialWriter$TransactionalProxy
2626:             1             24  org.apache.cassandra.io.util.SsdDiskOptimizationStrategy
2627:             1             24  org.apache.cassandra.locator.ReconnectableSnitchHelper
2628:             1             24  org.apache.cassandra.metrics.AuthMetrics
2629:             1             24  org.apache.cassandra.metrics.BufferPoolMetrics
2630:             1             24  org.apache.cassandra.metrics.CassandraMetricsRegistry
2631:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$1
2632:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$2
2633:             1             24  org.apache.cassandra.metrics.CommitLogMetrics$3
2634:             1             24  org.apache.cassandra.metrics.CompactionMetrics$3
2635:             1             24  org.apache.cassandra.metrics.HintedHandoffMetrics
2636:             1             24  org.apache.cassandra.metrics.MessagingMetrics
2637:             1             24  org.apache.cassandra.net.MessagingService$Verb$1
2638:             1             24  org.apache.cassandra.net.MessagingService$Verb$10
2639:             1             24  org.apache.cassandra.net.MessagingService$Verb$11
2640:             1             24  org.apache.cassandra.net.MessagingService$Verb$12
2641:             1             24  org.apache.cassandra.net.MessagingService$Verb$13
2642:             1             24  org.apache.cassandra.net.MessagingService$Verb$2
2643:             1             24  org.apache.cassandra.net.MessagingService$Verb$3
2644:             1             24  org.apache.cassandra.net.MessagingService$Verb$4
2645:             1             24  org.apache.cassandra.net.MessagingService$Verb$5
2646:             1             24  org.apache.cassandra.net.MessagingService$Verb$6
2647:             1             24  org.apache.cassandra.net.MessagingService$Verb$7
2648:             1             24  org.apache.cassandra.net.MessagingService$Verb$8
2649:             1             24  org.apache.cassandra.net.MessagingService$Verb$9
2650:             1             24  org.apache.cassandra.service.CacheService
2651:             1             24  org.apache.cassandra.service.GCInspector
2652:             1             24  org.apache.cassandra.service.PendingRangeCalculatorService
2653:             1             24  org.apache.cassandra.service.QueryState
2654:             1             24  org.apache.cassandra.service.StartupChecks
2655:             1             24  org.apache.cassandra.service.StartupChecks$8
2656:             1             24  org.apache.cassandra.streaming.StreamManager
2657:             1             24  org.apache.cassandra.thrift.Cassandra$Processor
2658:             1             24  org.apache.cassandra.tracing.TracingImpl
2659:             1             24  org.apache.cassandra.transport.ConnectionLimitHandler
2660:             1             24  org.apache.cassandra.transport.Frame$Compressor
2661:             1             24  org.apache.cassandra.transport.Frame$Decompressor
2662:             1             24  org.apache.cassandra.transport.Frame$Encoder
2663:             1             24  org.apache.cassandra.transport.Message$Dispatcher
2664:             1             24  org.apache.cassandra.transport.Message$ProtocolDecoder
2665:             1             24  org.apache.cassandra.transport.Message$ProtocolEncoder
2666:             1             24  org.apache.cassandra.transport.RequestThreadPoolExecutor
2667:             1             24  org.apache.cassandra.transport.Server$ConnectionTracker
2668:             1             24  org.apache.cassandra.transport.Server$EventNotifier
2669:             1             24  org.apache.cassandra.transport.Server$Initializer
2670:             1             24  org.apache.cassandra.triggers.TriggerExecutor
2671:             1             24  org.apache.cassandra.utils.ChecksumType$1
2672:             1             24  org.apache.cassandra.utils.ChecksumType$2
2673:             1             24  org.apache.cassandra.utils.ConcurrentBiMap
2674:             1             24  org.apache.cassandra.utils.ExpiringMap$1
2675:             1             24  org.apache.cassandra.utils.HistogramBuilder
2676:             1             24  org.apache.cassandra.utils.IntervalTree
2677:             1             24  org.apache.cassandra.utils.JMXServerUtils$Registry
2678:             1             24  org.apache.cassandra.utils.concurrent.OpOrder$Barrier
2679:             1             24  org.apache.cassandra.utils.memory.BufferPool$Debug
2680:             1             24  org.apache.cassandra.utils.progress.jmx.JMXProgressSupport
2681:             1             24  org.apache.cassandra.utils.progress.jmx.LegacyJMXProgressSupport
2682:             1             24  org.codehaus.jackson.map.deser.BeanDeserializerFactory
2683:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory
2684:             1             24  org.codehaus.jackson.map.ser.BeanSerializerFactory$ConfigImpl
2685:             1             24  org.codehaus.jackson.map.ser.impl.FailingSerializer
2686:             1             24  org.codehaus.jackson.map.ser.impl.SerializerCache
2687:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$BooleanArraySerializer
2688:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$DoubleArraySerializer
2689:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$FloatArraySerializer
2690:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$IntArraySerializer
2691:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$LongArraySerializer
2692:             1             24  org.codehaus.jackson.map.ser.std.StdArraySerializers$ShortArraySerializer
2693:             1             24  org.slf4j.helpers.FormattingTuple
2694:             1             24  org.slf4j.impl.StaticLoggerBinder
2695:             1             24  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.PercentEscaper
2696:             1             24  sun.instrument.TransformerManager
2697:             1             24  sun.launcher.LauncherHelper
2698:             1             24  sun.management.CompilationImpl
2699:             1             24  sun.management.GarbageCollectionNotifInfoCompositeData
2700:             1             24  sun.management.MemoryImpl
2701:             1             24  sun.management.OperatingSystemImpl
2702:             1             24  sun.management.RuntimeImpl
2703:             1             24  sun.management.ThreadImpl
2704:             1             24  sun.management.VMManagementImpl
2705:             1             24  sun.misc.JarIndex
2706:             1             24  sun.net.ProgressMonitor
2707:             1             24  sun.net.sdp.SdpProvider
2708:             1             24  sun.net.www.protocol.http.Handler
2709:             1             24  sun.nio.cs.ISO_8859_1
2710:             1             24  sun.nio.cs.US_ASCII
2711:             1             24  sun.nio.cs.UTF_16
2712:             1             24  sun.nio.cs.UTF_16BE
2713:             1             24  sun.nio.cs.UTF_16LE
2714:             1             24  sun.nio.cs.UTF_8
2715:             1             24  sun.rmi.runtime.RuntimeUtil$1
2716:             1             24  sun.rmi.server.LoaderHandler$1
2717:             1             24  sun.rmi.transport.DGCImpl
2718:             1             24  sun.rmi.transport.Target$$Lambda$338/684260999
2719:             1             24  sun.security.provider.certpath.X509CertPath
2720:             1             24  sun.security.ssl.SunX509KeyManagerImpl
2721:             1             24  sun.security.validator.EndEntityChecker
2722:             1             24  sun.security.x509.AccessDescription
2723:             1             24  sun.security.x509.CertificatePolicyMap
2724:             1             24  sun.util.locale.BaseLocale$Cache
2725:             1             24  sun.util.locale.provider.CalendarDataProviderImpl
2726:             1             24  sun.util.locale.provider.CalendarProviderImpl
2727:             1             24  sun.util.locale.provider.CurrencyNameProviderImpl
2728:             1             24  sun.util.locale.provider.DateFormatSymbolsProviderImpl
2729:             1             24  sun.util.locale.provider.DecimalFormatSymbolsProviderImpl
2730:             1             24  sun.util.locale.provider.NumberFormatProviderImpl
2731:             1             24  sun.util.logging.PlatformLogger
2732:             1             24  sun.util.logging.PlatformLogger$JavaLoggerProxy
2733:             1             24  sun.util.resources.LocaleData$1
2734:             1             16  [Lch.qos.logback.classic.spi.ThrowableProxy;
2735:             1             16  [Ljava.beans.EventSetDescriptor;
2736:             1             16  [Ljava.lang.Double;
2737:             1             16  [Ljava.lang.Float;
2738:             1             16  [Ljava.lang.Throwable;
2739:             1             16  [Ljava.net.NetworkInterface;
2740:             1             16  [Ljava.net.URL;
2741:             1             16  [Ljava.nio.file.attribute.FileAttribute;
2742:             1             16  [Ljava.security.Provider;
2743:             1             16  [Ljava.text.FieldPosition;
2744:             1             16  [Ljavax.security.cert.X509Certificate;
2745:             1             16  [Lnet.jpountz.lz4.LZ4JNI;
2746:             1             16  [Lnet.jpountz.lz4.LZ4Utils;
2747:             1             16  [Lnet.jpountz.util.ByteBufferUtils;
2748:             1             16  [Lnet.jpountz.util.Native;
2749:             1             16  [Lnet.jpountz.util.SafeUtils;
2750:             1             16  [Lnet.jpountz.xxhash.XXHashJNI;
2751:             1             16  [Lorg.apache.cassandra.db.rows.Cell;
2752:             1             16  [Lorg.apache.cassandra.db.transform.Stack$MoreContentsHolder;
2753:             1             16  [Lorg.codehaus.jackson.map.AbstractTypeResolver;
2754:             1             16  [Lorg.codehaus.jackson.map.Deserializers;
2755:             1             16  [Lorg.codehaus.jackson.map.KeyDeserializers;
2756:             1             16  [Lorg.codehaus.jackson.map.Serializers;
2757:             1             16  [Lorg.codehaus.jackson.map.deser.BeanDeserializerModifier;
2758:             1             16  [Lorg.codehaus.jackson.map.deser.ValueInstantiators;
2759:             1             16  [Lorg.codehaus.jackson.map.introspect.AnnotationMap;
2760:             1             16  [Lorg.codehaus.jackson.map.ser.BeanSerializerModifier;
2761:             1             16  [Lsun.instrument.TransformerManager$TransformerInfo;
2762:             1             16  ch.qos.logback.classic.selector.DefaultContextSelector
2763:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$1
2764:             1             16  ch.qos.logback.core.joran.spi.ConsoleTarget$2
2765:             1             16  ch.qos.logback.core.joran.spi.DefaultNestedComponentRegistry
2766:             1             16  ch.qos.logback.core.joran.util.ConfigurationWatchListUtil
2767:             1             16  com.codahale.metrics.Clock$UserTimeClock
2768:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$1
2769:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$2
2770:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$3
2771:             1             16  com.codahale.metrics.MetricRegistry$MetricBuilder$4
2772:             1             16  com.codahale.metrics.Striped64$ThreadHashCode
2773:             1             16  com.codahale.metrics.ThreadLocalRandom$1
2774:             1             16  com.github.benmanes.caffeine.SingleConsumerQueue$$Lambda$80/692511295
2775:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$79/608770405
2776:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$BoundedLocalLoadingCache$$Lambda$81/1858886571
2777:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$EntrySetView
2778:             1             16  com.github.benmanes.caffeine.cache.BoundedLocalCache$KeySetView
2779:             1             16  com.github.benmanes.caffeine.cache.BoundedWeigher
2780:             1             16  com.github.benmanes.caffeine.cache.Caffeine$$Lambda$77/2064869182
2781:             1             16  com.google.common.base.Absent
2782:             1             16  com.google.common.base.CharMatcher$1
2783:             1             16  com.google.common.base.CharMatcher$15
2784:             1             16  com.google.common.base.CharMatcher$2
2785:             1             16  com.google.common.base.CharMatcher$3
2786:             1             16  com.google.common.base.CharMatcher$4
2787:             1             16  com.google.common.base.CharMatcher$5
2788:             1             16  com.google.common.base.CharMatcher$6
2789:             1             16  com.google.common.base.CharMatcher$7
2790:             1             16  com.google.common.base.CharMatcher$8
2791:             1             16  com.google.common.base.Equivalence$Equals
2792:             1             16  com.google.common.base.Equivalence$Identity
2793:             1             16  com.google.common.base.Predicates$NotPredicate
2794:             1             16  com.google.common.base.Predicates$OrPredicate
2795:             1             16  com.google.common.base.Suppliers$SupplierOfInstance
2796:             1             16  com.google.common.base.Ticker$1
2797:             1             16  com.google.common.cache.CacheBuilder$1
2798:             1             16  com.google.common.cache.CacheBuilder$2
2799:             1             16  com.google.common.cache.CacheBuilder$3
2800:             1             16  com.google.common.cache.LocalCache$1
2801:             1             16  com.google.common.cache.LocalCache$2
2802:             1             16  com.google.common.cache.LocalCache$LocalManualCache
2803:             1             16  com.google.common.collect.ComparatorOrdering
2804:             1             16  com.google.common.collect.EmptyImmutableSet
2805:             1             16  com.google.common.collect.Iterators$1
2806:             1             16  com.google.common.collect.Iterators$2
2807:             1             16  com.google.common.collect.MapMakerInternalMap$1
2808:             1             16  com.google.common.collect.MapMakerInternalMap$2
2809:             1             16  com.google.common.collect.Multisets$5
2810:             1             16  com.google.common.collect.NaturalOrdering
2811:             1             16  com.google.common.collect.ReverseOrdering
2812:             1             16  com.google.common.io.ByteStreams$1
2813:             1             16  com.google.common.util.concurrent.Futures$4
2814:             1             16  com.google.common.util.concurrent.Futures$7
2815:             1             16  com.google.common.util.concurrent.Runnables$1
2816:             1             16  com.google.common.util.concurrent.Striped$5
2817:             1             16  com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$DiscardingQueue
2818:             1             16  com.sun.jmx.interceptor.DefaultMBeanServerInterceptor$ResourceContext$1
2819:             1             16  com.sun.jmx.mbeanserver.DefaultMXBeanMappingFactory
2820:             1             16  com.sun.jmx.mbeanserver.DescriptorCache
2821:             1             16  com.sun.jmx.mbeanserver.MBeanAnalyzer$MethodOrder
2822:             1             16  com.sun.jmx.mbeanserver.MBeanInstantiator
2823:             1             16  com.sun.jmx.mbeanserver.MXBeanIntrospector
2824:             1             16  com.sun.jmx.mbeanserver.SecureClassLoaderRepository
2825:             1             16  com.sun.jmx.mbeanserver.StandardMBeanIntrospector
2826:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$5
2827:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BroadcasterQuery
2828:             1             16  com.sun.jmx.remote.internal.ArrayNotificationBuffer$BufferListener
2829:             1             16  com.sun.jmx.remote.internal.ServerCommunicatorAdmin$Timeout
2830:             1             16  com.sun.jmx.remote.internal.ServerNotifForwarder$NotifForwarderBufferFilter
2831:             1             16  com.sun.jmx.remote.protocol.iiop.IIOPProxyImpl
2832:             1             16  com.sun.jmx.remote.security.SubjectDelegator
2833:             1             16  com.sun.jna.Native$1
2834:             1             16  com.sun.jna.Native$2
2835:             1             16  com.sun.jna.Native$7
2836:             1             16  com.sun.jna.Structure$1
2837:             1             16  com.sun.jna.Structure$2
2838:             1             16  com.sun.jna.VarArgsChecker$RealVarArgsChecker
2839:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDDatatypeValidator
2840:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.IDREFDatatypeValidator
2841:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NMTOKENDatatypeValidator
2842:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.NOTATIONDatatypeValidator
2843:             1             16  com.sun.org.apache.xerces.internal.impl.dv.dtd.StringDatatypeValidator
2844:             1             16  com.sun.org.apache.xerces.internal.utils.SecuritySupport
2845:             1             16  com.sun.proxy.$Proxy2
2846:             1             16  com.sun.proxy.$Proxy4
2847:             1             16  com.sun.proxy.$Proxy7
2848:             1             16  io.netty.buffer.ByteBufUtil$1
2849:             1             16  io.netty.buffer.ByteBufUtil$2
2850:             1             16  io.netty.channel.ChannelFutureListener$1
2851:             1             16  io.netty.channel.ChannelFutureListener$2
2852:             1             16  io.netty.channel.ChannelFutureListener$3
2853:             1             16  io.netty.channel.ChannelMetadata
2854:             1             16  io.netty.channel.ChannelOutboundBuffer$1
2855:             1             16  io.netty.channel.DefaultChannelPipeline$1
2856:             1             16  io.netty.channel.DefaultMessageSizeEstimator
2857:             1             16  io.netty.channel.DefaultMessageSizeEstimator$HandleImpl
2858:             1             16  io.netty.channel.DefaultSelectStrategy
2859:             1             16  io.netty.channel.DefaultSelectStrategyFactory
2860:             1             16  io.netty.channel.group.ChannelMatchers$1
2861:             1             16  io.netty.channel.group.ChannelMatchers$InvertMatcher
2862:             1             16  io.netty.util.Recycler$1
2863:             1             16  io.netty.util.Recycler$3
2864:             1             16  io.netty.util.concurrent.DefaultPromise$CauseHolder
2865:             1             16  io.netty.util.concurrent.GlobalEventExecutor$1
2866:             1             16  io.netty.util.concurrent.GlobalEventExecutor$TaskRunner
2867:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$1
2868:             1             16  io.netty.util.concurrent.MultithreadEventExecutorGroup$PowerOfTwoEventExecutorChooser
2869:             1             16  io.netty.util.concurrent.RejectedExecutionHandlers$1
2870:             1             16  io.netty.util.concurrent.SingleThreadEventExecutor$1
2871:             1             16  io.netty.util.internal.NoOpTypeParameterMatcher
2872:             1             16  java.io.DeleteOnExitHook$1
2873:             1             16  java.io.FileDescriptor$1
2874:             1             16  java.io.ObjectInputStream$$Lambda$293/697818519
2875:             1             16  java.io.ObjectInputStream$1
2876:             1             16  java.lang.ApplicationShutdownHooks$1
2877:             1             16  java.lang.CharacterDataLatin1
2878:             1             16  java.lang.ClassValue$Identity
2879:             1             16  java.lang.ProcessBuilder$NullInputStream
2880:             1             16  java.lang.ProcessBuilder$NullOutputStream
2881:             1             16  java.lang.Runtime
2882:             1             16  java.lang.String$CaseInsensitiveComparator
2883:             1             16  java.lang.System$2
2884:             1             16  java.lang.Terminator$1
2885:             1             16  java.lang.UNIXProcess$$Lambda$13/1784131088
2886:             1             16  java.lang.UNIXProcess$$Lambda$14/2143582219
2887:             1             16  java.lang.UNIXProcess$Platform$$Lambda$10/616881582
2888:             1             16  java.lang.invoke.MemberName$Factory
2889:             1             16  java.lang.invoke.MethodHandleImpl$2
2890:             1             16  java.lang.invoke.MethodHandleImpl$3
2891:             1             16  java.lang.management.PlatformComponent$1
2892:             1             16  java.lang.management.PlatformComponent$10
2893:             1             16  java.lang.management.PlatformComponent$11
2894:             1             16  java.lang.management.PlatformComponent$12
2895:             1             16  java.lang.management.PlatformComponent$13
2896:             1             16  java.lang.management.PlatformComponent$14
2897:             1             16  java.lang.management.PlatformComponent$15
2898:             1             16  java.lang.management.PlatformComponent$2
2899:             1             16  java.lang.management.PlatformComponent$3
2900:             1             16  java.lang.management.PlatformComponent$4
2901:             1             16  java.lang.management.PlatformComponent$5
2902:             1             16  java.lang.management.PlatformComponent$6
2903:             1             16  java.lang.management.PlatformComponent$7
2904:             1             16  java.lang.management.PlatformComponent$8
2905:             1             16  java.lang.management.PlatformComponent$9
2906:             1             16  java.lang.ref.Reference$1
2907:             1             16  java.lang.ref.Reference$Lock
2908:             1             16  java.lang.reflect.Proxy$KeyFactory
2909:             1             16  java.lang.reflect.Proxy$ProxyClassFactory
2910:             1             16  java.lang.reflect.ReflectAccess
2911:             1             16  java.math.BigDecimal$1
2912:             1             16  java.net.InetAddress$2
2913:             1             16  java.net.URLClassLoader$7
2914:             1             16  java.nio.Bits$1
2915:             1             16  java.nio.Bits$1$1
2916:             1             16  java.nio.charset.CoderResult$1
2917:             1             16  java.nio.charset.CoderResult$2
2918:             1             16  java.nio.file.Files$AcceptAllFilter
2919:             1             16  java.rmi.server.RMIClassLoader$2
2920:             1             16  java.security.AllPermission
2921:             1             16  java.security.ProtectionDomain$2
2922:             1             16  java.security.ProtectionDomain$JavaSecurityAccessImpl
2923:             1             16  java.text.DontCareFieldPosition$1
2924:             1             16  java.util.Collections$EmptyEnumeration
2925:             1             16  java.util.Collections$EmptyIterator
2926:             1             16  java.util.Collections$EmptyList
2927:             1             16  java.util.Collections$EmptySet
2928:             1             16  java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet
2929:             1             16  java.util.Currency$CurrencyNameGetter
2930:             1             16  java.util.EnumMap$1
2931:             1             16  java.util.ResourceBundle$Control
2932:             1             16  java.util.Spliterators$EmptySpliterator$OfDouble
2933:             1             16  java.util.Spliterators$EmptySpliterator$OfInt
2934:             1             16  java.util.Spliterators$EmptySpliterator$OfLong
2935:             1             16  java.util.Spliterators$EmptySpliterator$OfRef
2936:             1             16  java.util.TreeMap$EntrySpliterator$$Lambda$68/1819038759
2937:             1             16  java.util.WeakHashMap$KeySet
2938:             1             16  java.util.concurrent.Executors$FinalizableDelegatedExecutorService
2939:             1             16  java.util.concurrent.ThreadPoolExecutor$AbortPolicy
2940:             1             16  java.util.jar.JarVerifier$3
2941:             1             16  java.util.jar.JavaUtilJarAccessImpl
2942:             1             16  java.util.logging.LoggingProxyImpl
2943:             1             16  java.util.regex.Pattern$4
2944:             1             16  java.util.regex.Pattern$LastNode
2945:             1             16  java.util.regex.Pattern$Node
2946:             1             16  java.util.stream.Collectors$$Lambda$178/1708585783
2947:             1             16  java.util.stream.Collectors$$Lambda$179/2048467502
2948:             1             16  java.util.stream.Collectors$$Lambda$180/1269763229
2949:             1             16  java.util.stream.Collectors$$Lambda$221/1489469437
2950:             1             16  java.util.stream.Collectors$$Lambda$222/431613642
2951:             1             16  java.util.stream.Collectors$$Lambda$223/1098744211
2952:             1             16  java.util.stream.Collectors$$Lambda$247/1746129463
2953:             1             16  java.util.stream.Collectors$$Lambda$60/1724814719
2954:             1             16  java.util.stream.Collectors$$Lambda$61/1718322084
2955:             1             16  java.util.stream.Collectors$$Lambda$62/24039137
2956:             1             16  java.util.stream.Collectors$$Lambda$63/992086987
2957:             1             16  java.util.stream.LongPipeline$$Lambda$189/1888591113
2958:             1             16  java.util.stream.LongPipeline$$Lambda$325/1014276638
2959:             1             16  java.util.zip.ZipFile$1
2960:             1             16  javax.crypto.JceSecurityManager
2961:             1             16  javax.management.JMX
2962:             1             16  javax.management.MBeanServerBuilder
2963:             1             16  javax.management.NotificationBroadcasterSupport$1
2964:             1             16  javax.management.remote.JMXPrincipal
2965:             1             16  javax.management.remote.rmi.RMIConnectionImpl_Stub
2966:             1             16  javax.management.remote.rmi.RMIServerImpl_Stub
2967:             1             16  javax.xml.parsers.SecuritySupport
2968:             1             16  net.jpountz.lz4.LZ4JNICompressor
2969:             1             16  net.jpountz.lz4.LZ4JNIFastDecompressor
2970:             1             16  net.jpountz.lz4.LZ4JNISafeDecompressor
2971:             1             16  net.jpountz.xxhash.StreamingXXHash32JNI$Factory
2972:             1             16  net.jpountz.xxhash.StreamingXXHash64JNI$Factory
2973:             1             16  net.jpountz.xxhash.XXHash32JNI
2974:             1             16  net.jpountz.xxhash.XXHash64JNI
2975:             1             16  org.apache.cassandra.auth.AllowAllAuthenticator$Negotiator
2976:             1             16  org.apache.cassandra.auth.AllowAllInternodeAuthenticator
2977:             1             16  org.apache.cassandra.auth.AuthCache$1
2978:             1             16  org.apache.cassandra.auth.AuthMigrationListener
2979:             1             16  org.apache.cassandra.auth.CassandraRoleManager$$Lambda$264/195066780
2980:             1             16  org.apache.cassandra.auth.CassandraRoleManager$1
2981:             1             16  org.apache.cassandra.auth.CassandraRoleManager$2
2982:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$265/385180766
2983:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$266/694021194
2984:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$267/767298601
2985:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$268/274090580
2986:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$269/1588510401
2987:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$270/331234425
2988:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$271/996989596
2989:             1             16  org.apache.cassandra.auth.PasswordAuthenticator$CredentialsCache$$Lambda$272/1507030140
2990:             1             16  org.apache.cassandra.batchlog.Batch$Serializer
2991:             1             16  org.apache.cassandra.batchlog.BatchRemoveVerbHandler
2992:             1             16  org.apache.cassandra.batchlog.BatchStoreVerbHandler
2993:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$258/2042553130
2994:             1             16  org.apache.cassandra.batchlog.BatchlogManager$$Lambda$290/1638031626
2995:             1             16  org.apache.cassandra.cache.AutoSavingCache$1
2996:             1             16  org.apache.cassandra.cache.ChunkCache$$Lambda$78/420307438
2997:             1             16  org.apache.cassandra.cache.NopCacheProvider$NopCache
2998:             1             16  org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$1
2999:             1             16  org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1
3000:             1             16  org.apache.cassandra.concurrent.StageManager$1
3001:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$213/1328645530
3002:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$214/2107098463
3003:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$232/1529326426
3004:             1             16  org.apache.cassandra.config.CFMetaData$$Lambda$233/570714518
3005:             1             16  org.apache.cassandra.config.CFMetaData$Builder$$Lambda$30/671596011
3006:             1             16  org.apache.cassandra.config.CFMetaData$Serializer
3007:             1             16  org.apache.cassandra.config.ColumnDefinition$$Lambda$25/207471778
3008:             1             16  org.apache.cassandra.config.DatabaseDescriptor$1
3009:             1             16  org.apache.cassandra.config.Schema$$Lambda$262/956354740
3010:             1             16  org.apache.cassandra.config.Schema$$Lambda$263/2080528880
3011:             1             16  org.apache.cassandra.cql3.ColumnConditions$$Lambda$116/841977955
3012:             1             16  org.apache.cassandra.cql3.Constants$1
3013:             1             16  org.apache.cassandra.cql3.Constants$NullLiteral
3014:             1             16  org.apache.cassandra.cql3.Constants$UnsetLiteral
3015:             1             16  org.apache.cassandra.cql3.Cql_Parser$1
3016:             1             16  org.apache.cassandra.cql3.IfExistsCondition
3017:             1             16  org.apache.cassandra.cql3.IfNotExistsCondition
3018:             1             16  org.apache.cassandra.cql3.QueryOptions$Codec
3019:             1             16  org.apache.cassandra.cql3.QueryProcessor
3020:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$17/951221468
3021:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$18/1046545660
3022:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$19/1545827753
3023:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$20/1611832218
3024:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$21/2027317551
3025:             1             16  org.apache.cassandra.cql3.QueryProcessor$$Lambda$22/273077527
3026:             1             16  org.apache.cassandra.cql3.QueryProcessor$MigrationSubscriber
3027:             1             16  org.apache.cassandra.cql3.ResultSet$Codec
3028:             1             16  org.apache.cassandra.cql3.ResultSet$ResultMetadata$Codec
3029:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$41/1614133563
3030:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$42/839771540
3031:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$43/1751403001
3032:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$44/1756819670
3033:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$45/178604517
3034:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$46/1543518287
3035:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$47/464872674
3036:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$48/1659286984
3037:             1             16  org.apache.cassandra.cql3.functions.CastFcts$$Lambda$49/1793899405
3038:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager
3039:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$1
3040:             1             16  org.apache.cassandra.cql3.functions.ThreadAwareSecurityManager$2
3041:             1             16  org.apache.cassandra.cql3.restrictions.RestrictionSet$1
3042:             1             16  org.apache.cassandra.cql3.selection.Selection$1
3043:             1             16  org.apache.cassandra.cql3.statements.CreateTableStatement$$Lambda$23/1470868839
3044:             1             16  org.apache.cassandra.db.CBuilder$1
3045:             1             16  org.apache.cassandra.db.Clustering$Serializer
3046:             1             16  org.apache.cassandra.db.ClusteringBoundOrBoundary$Serializer
3047:             1             16  org.apache.cassandra.db.ClusteringPrefix$Serializer
3048:             1             16  org.apache.cassandra.db.ColumnFamilyStore$$Lambda$190/1269783694
3049:             1             16  org.apache.cassandra.db.ColumnFamilyStore$2
3050:             1             16  org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily
3051:             1             16  org.apache.cassandra.db.Columns$$Lambda$205/2092785251
3052:             1             16  org.apache.cassandra.db.Columns$Serializer
3053:             1             16  org.apache.cassandra.db.CounterMutation$CounterMutationSerializer
3054:             1             16  org.apache.cassandra.db.CounterMutationVerbHandler
3055:             1             16  org.apache.cassandra.db.DataRange$Serializer
3056:             1             16  org.apache.cassandra.db.DecoratedKey$1
3057:             1             16  org.apache.cassandra.db.DefinitionsUpdateVerbHandler
3058:             1             16  org.apache.cassandra.db.DeletionPurger$$Lambda$105/2116697030
3059:             1             16  org.apache.cassandra.db.DeletionTime$Serializer
3060:             1             16  org.apache.cassandra.db.Directories$3
3061:             1             16  org.apache.cassandra.db.Directories$DataDirectory
3062:             1             16  org.apache.cassandra.db.EmptyIterators$EmptyPartitionIterator
3063:             1             16  org.apache.cassandra.db.HintedHandOffManager
3064:             1             16  org.apache.cassandra.db.Keyspace$1
3065:             1             16  org.apache.cassandra.db.MigrationRequestVerbHandler
3066:             1             16  org.apache.cassandra.db.Mutation$MutationSerializer
3067:             1             16  org.apache.cassandra.db.MutationVerbHandler
3068:             1             16  org.apache.cassandra.db.PartitionPosition$RowPositionSerializer
3069:             1             16  org.apache.cassandra.db.PartitionRangeReadCommand$Deserializer
3070:             1             16  org.apache.cassandra.db.ReadCommand$1
3071:             1             16  org.apache.cassandra.db.ReadCommand$1WithoutPurgeableTombstones$$Lambda$110/208106294
3072:             1             16  org.apache.cassandra.db.ReadCommand$2
3073:             1             16  org.apache.cassandra.db.ReadCommand$3
3074:             1             16  org.apache.cassandra.db.ReadCommand$LegacyPagedRangeCommandSerializer
3075:             1             16  org.apache.cassandra.db.ReadCommand$LegacyRangeSliceCommandSerializer
3076:             1             16  org.apache.cassandra.db.ReadCommand$LegacyReadCommandSerializer
3077:             1             16  org.apache.cassandra.db.ReadCommand$Serializer
3078:             1             16  org.apache.cassandra.db.ReadCommandVerbHandler
3079:             1             16  org.apache.cassandra.db.ReadQuery$1
3080:             1             16  org.apache.cassandra.db.ReadRepairVerbHandler
3081:             1             16  org.apache.cassandra.db.ReadResponse$1
3082:             1             16  org.apache.cassandra.db.ReadResponse$LegacyRangeSliceReplySerializer
3083:             1             16  org.apache.cassandra.db.ReadResponse$Serializer
3084:             1             16  org.apache.cassandra.db.SchemaCheckVerbHandler
3085:             1             16  org.apache.cassandra.db.SerializationHeader$Serializer
3086:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Deserializer
3087:             1             16  org.apache.cassandra.db.SinglePartitionReadCommand$Group$$Lambda$106/1952605049
3088:             1             16  org.apache.cassandra.db.SizeEstimatesRecorder
3089:             1             16  org.apache.cassandra.db.Slice$Serializer
3090:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices
3091:             1             16  org.apache.cassandra.db.Slices$SelectAllSlices$1
3092:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices
3093:             1             16  org.apache.cassandra.db.Slices$SelectNoSlices$1
3094:             1             16  org.apache.cassandra.db.Slices$Serializer
3095:             1             16  org.apache.cassandra.db.SnapshotCommandSerializer
3096:             1             16  org.apache.cassandra.db.StorageHook$1
3097:             1             16  org.apache.cassandra.db.SystemKeyspace$$Lambda$186/1473888912
3098:             1             16  org.apache.cassandra.db.TruncateResponse$TruncateResponseSerializer
3099:             1             16  org.apache.cassandra.db.TruncateVerbHandler
3100:             1             16  org.apache.cassandra.db.TruncationSerializer
3101:             1             16  org.apache.cassandra.db.WriteResponse
3102:             1             16  org.apache.cassandra.db.WriteResponse$Serializer
3103:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$1
3104:             1             16  org.apache.cassandra.db.aggregation.AggregationSpecification$Serializer
3105:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$$Lambda$72/500233312
3106:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1
3107:             1             16  org.apache.cassandra.db.commitlog.AbstractCommitLogService$1
3108:             1             16  org.apache.cassandra.db.commitlog.CommitLog$$Lambda$227/2024217158
3109:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$1
3110:             1             16  org.apache.cassandra.db.commitlog.CommitLogPosition$CommitLogPositionSerializer
3111:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$$Lambda$228/1186545861
3112:             1             16  org.apache.cassandra.db.commitlog.CommitLogReplayer$MutationInitiator
3113:             1             16  org.apache.cassandra.db.commitlog.CommitLogSegment$$Lambda$175/1833918497
3114:             1             16  org.apache.cassandra.db.commitlog.IntervalSet$1
3115:             1             16  org.apache.cassandra.db.commitlog.SimpleCachedBufferPool$1
3116:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$184/889018651
3117:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$185/638825183
3118:             1             16  org.apache.cassandra.db.compaction.CompactionController$$Lambda$242/1509719872
3119:             1             16  org.apache.cassandra.db.compaction.CompactionManager$1
3120:             1             16  org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionController$$Lambda$307/363853319
3121:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$133/1728760599
3122:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$134/703363283
3123:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$172/1546684896
3124:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$85/654029265
3125:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$86/2030162789
3126:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$87/1306548322
3127:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$88/973942848
3128:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$89/558033602
3129:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$90/1361733480
3130:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$91/999951331
3131:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$92/1918201666
3132:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$93/1181004273
3133:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$95/1423931162
3134:             1             16  org.apache.cassandra.db.compaction.CompactionStrategyManager$$Lambda$96/1090942546
3135:             1             16  org.apache.cassandra.db.compaction.LeveledManifest$1
3136:             1             16  org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy$1
3137:             1             16  org.apache.cassandra.db.context.CounterContext
3138:             1             16  org.apache.cassandra.db.filter.AbstractClusteringIndexFilter$FilterSerializer
3139:             1             16  org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$NamesDeserializer
3140:             1             16  org.apache.cassandra.db.filter.ClusteringIndexSliceFilter$SliceDeserializer
3141:             1             16  org.apache.cassandra.db.filter.ColumnFilter$Serializer
3142:             1             16  org.apache.cassandra.db.filter.DataLimits$Serializer
3143:             1             16  org.apache.cassandra.db.filter.RowFilter$CQLFilter
3144:             1             16  org.apache.cassandra.db.filter.RowFilter$Serializer
3145:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$58/435914790
3146:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$59/1273958371
3147:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$64/731243659
3148:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$66/1037955032
3149:             1             16  org.apache.cassandra.db.lifecycle.LogAwareFileLister$$Lambda$70/331596257
3150:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$165/1814072734
3151:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$203/2022031193
3152:             1             16  org.apache.cassandra.db.lifecycle.LogFile$$Lambda$204/1336053009
3153:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$140/1142908098
3154:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$141/423008343
3155:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$142/88843440
3156:             1             16  org.apache.cassandra.db.lifecycle.LogRecord$$Lambda$177/1035048662
3157:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$162/1676168006
3158:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$166/1882192501
3159:             1             16  org.apache.cassandra.db.lifecycle.LogReplicaSet$$Lambda$168/700891016
3160:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$52/894421232
3161:             1             16  org.apache.cassandra.db.lifecycle.LogTransaction$LogFilesByName$$Lambda$54/276869158
3162:             1             16  org.apache.cassandra.db.lifecycle.Tracker$$Lambda$170/1786214274
3163:             1             16  org.apache.cassandra.db.marshal.CollectionType$CollectionPathSerializer
3164:             1             16  org.apache.cassandra.db.monitoring.ApproximateTime$$Lambda$108/2001863314
3165:             1             16  org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer
3166:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$$Lambda$107/2345640
3167:             1             16  org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer
3168:             1             16  org.apache.cassandra.db.rows.AbstractTypeVersionComparator
3169:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$118/474868079
3170:             1             16  org.apache.cassandra.db.rows.BTreeRow$$Lambda$123/164389557
3171:             1             16  org.apache.cassandra.db.rows.Cell$$Lambda$101/1913147328
3172:             1             16  org.apache.cassandra.db.rows.Cell$Serializer
3173:             1             16  org.apache.cassandra.db.rows.ColumnData$$Lambda$28/494077446
3174:             1             16  org.apache.cassandra.db.rows.EncodingStats$Serializer
3175:             1             16  org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer
3176:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer
3177:             1             16  org.apache.cassandra.db.rows.UnfilteredSerializer$$Lambda$194/5263871
3178:             1             16  org.apache.cassandra.db.view.View$$Lambda$219/1557380482
3179:             1             16  org.apache.cassandra.dht.BootStrapper$StringSerializer
3180:             1             16  org.apache.cassandra.dht.Murmur3Partitioner$2
3181:             1             16  org.apache.cassandra.dht.StreamStateStore
3182:             1             16  org.apache.cassandra.dht.Token$TokenSerializer
3183:             1             16  org.apache.cassandra.gms.EchoMessage
3184:             1             16  org.apache.cassandra.gms.EchoMessage$EchoMessageSerializer
3185:             1             16  org.apache.cassandra.gms.EndpointStateSerializer
3186:             1             16  org.apache.cassandra.gms.GossipDigestAck2Serializer
3187:             1             16  org.apache.cassandra.gms.GossipDigestAck2VerbHandler
3188:             1             16  org.apache.cassandra.gms.GossipDigestAckSerializer
3189:             1             16  org.apache.cassandra.gms.GossipDigestAckVerbHandler
3190:             1             16  org.apache.cassandra.gms.GossipDigestSerializer
3191:             1             16  org.apache.cassandra.gms.GossipDigestSynSerializer
3192:             1             16  org.apache.cassandra.gms.GossipDigestSynVerbHandler
3193:             1             16  org.apache.cassandra.gms.GossipShutdownVerbHandler
3194:             1             16  org.apache.cassandra.gms.Gossiper$1
3195:             1             16  org.apache.cassandra.gms.Gossiper$GossipTask
3196:             1             16  org.apache.cassandra.gms.HeartBeatStateSerializer
3197:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueFactory
3198:             1             16  org.apache.cassandra.gms.VersionedValue$VersionedValueSerializer
3199:             1             16  org.apache.cassandra.hints.EncodedHintMessage$Serializer
3200:             1             16  org.apache.cassandra.hints.Hint$Serializer
3201:             1             16  org.apache.cassandra.hints.HintMessage$Serializer
3202:             1             16  org.apache.cassandra.hints.HintResponse
3203:             1             16  org.apache.cassandra.hints.HintResponse$Serializer
3204:             1             16  org.apache.cassandra.hints.HintVerbHandler
3205:             1             16  org.apache.cassandra.hints.HintsBuffer$$Lambda$327/1070755303
3206:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$244/955891688
3207:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$245/1579667951
3208:             1             16  org.apache.cassandra.hints.HintsCatalog$$Lambda$246/2099786968
3209:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$282/2033605821
3210:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$283/1986677941
3211:             1             16  org.apache.cassandra.hints.HintsDispatchTrigger$$Lambda$284/355640298
3212:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$250/1791992279
3213:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$251/1557383930
3214:             1             16  org.apache.cassandra.hints.HintsService$$Lambda$252/763495689
3215:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$318/991892116
3216:             1             16  org.apache.cassandra.hints.HintsStore$$Lambda$322/1059094831
3217:             1             16  org.apache.cassandra.hints.HintsWriteExecutor$FsyncWritersTask$$Lambda$289/2053564305
3218:             1             16  org.apache.cassandra.index.Index$CollatedViewIndexBuildingSupport
3219:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$152/111521464
3220:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$153/118079547
3221:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$182/992085984
3222:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$188/887656608
3223:             1             16  org.apache.cassandra.index.SecondaryIndexManager$$Lambda$312/1070341018
3224:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$1
3225:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$2
3226:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$3
3227:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$4
3228:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$5
3229:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$6
3230:             1             16  org.apache.cassandra.index.internal.CassandraIndexFunctions$7
3231:             1             16  org.apache.cassandra.index.transactions.UpdateTransaction$1
3232:             1             16  org.apache.cassandra.io.compress.CompressionMetadata$ChunkSerializer
3233:             1             16  org.apache.cassandra.io.compress.SnappyCompressor
3234:             1             16  org.apache.cassandra.io.sstable.Descriptor$$Lambda$71/999647352
3235:             1             16  org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer
3236:             1             16  org.apache.cassandra.io.sstable.IndexSummaryManager$1
3237:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$73/1687768728
3238:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$74/15478307
3239:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$$Lambda$75/1394837936
3240:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$1
3241:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$Equals
3242:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThan
3243:             1             16  org.apache.cassandra.io.sstable.format.SSTableReader$Operator$GreaterThanOrEqualTo
3244:             1             16  org.apache.cassandra.io.sstable.format.SSTableReadsListener$1
3245:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$160/1520196427
3246:             1             16  org.apache.cassandra.io.sstable.format.SSTableWriter$$Lambda$311/1357900831
3247:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat
3248:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory
3249:             1             16  org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory
3250:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$150/504911193
3251:             1             16  org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter$$Lambda$151/451889382
3252:             1             16  org.apache.cassandra.io.sstable.metadata.CompactionMetadata$CompactionMetadataSerializer
3253:             1             16  org.apache.cassandra.io.sstable.metadata.StatsMetadata$StatsMetadataSerializer
3254:             1             16  org.apache.cassandra.io.sstable.metadata.ValidationMetadata$ValidationMetadataSerializer
3255:             1             16  org.apache.cassandra.io.util.DataOutputBuffer$1
3256:             1             16  org.apache.cassandra.io.util.DataOutputStreamPlus$1
3257:             1             16  org.apache.cassandra.io.util.FileHandle$$Lambda$158/795408782
3258:             1             16  org.apache.cassandra.io.util.MmappedRegions$State$$Lambda$197/1396226930
3259:             1             16  org.apache.cassandra.io.util.Rebufferer$1
3260:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$1
3261:             1             16  org.apache.cassandra.locator.DynamicEndpointSnitch$2
3262:             1             16  org.apache.cassandra.locator.EndpointSnitchInfo
3263:             1             16  org.apache.cassandra.locator.PendingRangeMaps$1
3264:             1             16  org.apache.cassandra.locator.PendingRangeMaps$2
3265:             1             16  org.apache.cassandra.locator.PendingRangeMaps$3
3266:             1             16  org.apache.cassandra.locator.PendingRangeMaps$4
3267:             1             16  org.apache.cassandra.locator.PropertyFileSnitch
3268:             1             16  org.apache.cassandra.locator.PropertyFileSnitch$1
3269:             1             16  org.apache.cassandra.locator.SimpleSeedProvider
3270:             1             16  org.apache.cassandra.locator.TokenMetadata$1
3271:             1             16  org.apache.cassandra.metrics.BufferPoolMetrics$1
3272:             1             16  org.apache.cassandra.metrics.CQLMetrics$1
3273:             1             16  org.apache.cassandra.metrics.CQLMetrics$2
3274:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$82/1609657810
3275:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$83/2101898459
3276:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$$Lambda$84/342161168
3277:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$1
3278:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$2
3279:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$3
3280:             1             16  org.apache.cassandra.metrics.CacheMissMetrics$4
3281:             1             16  org.apache.cassandra.metrics.ClientMetrics
3282:             1             16  org.apache.cassandra.metrics.CompactionMetrics$1
3283:             1             16  org.apache.cassandra.metrics.CompactionMetrics$2
3284:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$1
3285:             1             16  org.apache.cassandra.metrics.HintedHandoffMetrics$2
3286:             1             16  org.apache.cassandra.metrics.TableMetrics$1
3287:             1             16  org.apache.cassandra.metrics.TableMetrics$13
3288:             1             16  org.apache.cassandra.metrics.TableMetrics$18
3289:             1             16  org.apache.cassandra.metrics.TableMetrics$20
3290:             1             16  org.apache.cassandra.metrics.TableMetrics$22
3291:             1             16  org.apache.cassandra.metrics.TableMetrics$26
3292:             1             16  org.apache.cassandra.metrics.TableMetrics$28
3293:             1             16  org.apache.cassandra.metrics.ViewWriteMetrics$1
3294:             1             16  org.apache.cassandra.net.IAsyncCallback$1
3295:             1             16  org.apache.cassandra.net.MessagingService$4
3296:             1             16  org.apache.cassandra.net.MessagingService$5
3297:             1             16  org.apache.cassandra.net.MessagingService$CallbackDeterminedSerializer
3298:             1             16  org.apache.cassandra.notifications.SSTableDeletingNotification
3299:             1             16  org.apache.cassandra.repair.NodePair$NodePairSerializer
3300:             1             16  org.apache.cassandra.repair.RepairJobDesc$RepairJobDescSerializer
3301:             1             16  org.apache.cassandra.repair.RepairMessageVerbHandler
3302:             1             16  org.apache.cassandra.repair.messages.AnticompactionRequest$AnticompactionRequestSerializer
3303:             1             16  org.apache.cassandra.repair.messages.CleanupMessage$CleanupMessageSerializer
3304:             1             16  org.apache.cassandra.repair.messages.PrepareMessage$PrepareMessageSerializer
3305:             1             16  org.apache.cassandra.repair.messages.RepairMessage$RepairMessageSerializer
3306:             1             16  org.apache.cassandra.repair.messages.SnapshotMessage$SnapshotMessageSerializer
3307:             1             16  org.apache.cassandra.repair.messages.SyncComplete$SyncCompleteSerializer
3308:             1             16  org.apache.cassandra.repair.messages.SyncRequest$SyncRequestSerializer
3309:             1             16  org.apache.cassandra.repair.messages.ValidationComplete$ValidationCompleteSerializer
3310:             1             16  org.apache.cassandra.repair.messages.ValidationRequest$ValidationRequestSerializer
3311:             1             16  org.apache.cassandra.scheduler.NoScheduler
3312:             1             16  org.apache.cassandra.schema.CQLTypeParser$$Lambda$207/2843617
3313:             1             16  org.apache.cassandra.schema.CompressionParams$Serializer
3314:             1             16  org.apache.cassandra.schema.Functions$$Lambda$236/1017996482
3315:             1             16  org.apache.cassandra.schema.Functions$$Lambda$237/2135117754
3316:             1             16  org.apache.cassandra.schema.Functions$$Lambda$239/854637578
3317:             1             16  org.apache.cassandra.schema.Functions$$Lambda$240/305461269
3318:             1             16  org.apache.cassandra.schema.Functions$Builder$$Lambda$36/146874094
3319:             1             16  org.apache.cassandra.schema.IndexMetadata$Serializer
3320:             1             16  org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$132/399524457
3321:             1             16  org.apache.cassandra.schema.SchemaKeyspace$$Lambda$216/2137640552
3322:             1             16  org.apache.cassandra.schema.Types$RawBuilder$$Lambda$206/1399449613
3323:             1             16  org.apache.cassandra.schema.Types$RawBuilder$RawUDT$$Lambda$210/2069170964
3324:             1             16  org.apache.cassandra.schema.Views$$Lambda$50/1348115836
3325:             1             16  org.apache.cassandra.serializers.BooleanSerializer
3326:             1             16  org.apache.cassandra.serializers.ByteSerializer
3327:             1             16  org.apache.cassandra.serializers.BytesSerializer
3328:             1             16  org.apache.cassandra.serializers.DecimalSerializer
3329:             1             16  org.apache.cassandra.serializers.DoubleSerializer
3330:             1             16  org.apache.cassandra.serializers.InetAddressSerializer
3331:             1             16  org.apache.cassandra.serializers.Int32Serializer
3332:             1             16  org.apache.cassandra.serializers.LongSerializer
3333:             1             16  org.apache.cassandra.serializers.TimeUUIDSerializer
3334:             1             16  org.apache.cassandra.serializers.TimestampSerializer
3335:             1             16  org.apache.cassandra.serializers.TimestampSerializer$1
3336:             1             16  org.apache.cassandra.serializers.TimestampSerializer$2
3337:             1             16  org.apache.cassandra.serializers.TimestampSerializer$3
3338:             1             16  org.apache.cassandra.serializers.UTF8Serializer
3339:             1             16  org.apache.cassandra.serializers.UUIDSerializer
3340:             1             16  org.apache.cassandra.service.CacheService$CounterCacheSerializer
3341:             1             16  org.apache.cassandra.service.CacheService$KeyCacheSerializer
3342:             1             16  org.apache.cassandra.service.CacheService$RowCacheSerializer
3343:             1             16  org.apache.cassandra.service.CassandraDaemon$$Lambda$273/1244026033
3344:             1             16  org.apache.cassandra.service.CassandraDaemon$1
3345:             1             16  org.apache.cassandra.service.CassandraDaemon$2
3346:             1             16  org.apache.cassandra.service.CassandraDaemon$NativeAccess
3347:             1             16  org.apache.cassandra.service.ClientState$$Lambda$97/466481125
3348:             1             16  org.apache.cassandra.service.ClientWarn
3349:             1             16  org.apache.cassandra.service.DefaultFSErrorHandler
3350:             1             16  org.apache.cassandra.service.EchoVerbHandler
3351:             1             16  org.apache.cassandra.service.LoadBroadcaster
3352:             1             16  org.apache.cassandra.service.LoadBroadcaster$1
3353:             1             16  org.apache.cassandra.service.MigrationManager
3354:             1             16  org.apache.cassandra.service.MigrationManager$MigrationsSerializer
3355:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$277/794251840
3356:             1             16  org.apache.cassandra.service.NativeTransportService$$Lambda$279/1246696592
3357:             1             16  org.apache.cassandra.service.PendingRangeCalculatorService$1
3358:             1             16  org.apache.cassandra.service.SnapshotVerbHandler
3359:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$1/1204167249
3360:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$114/1819989346
3361:             1             16  org.apache.cassandra.service.StartupChecks$$Lambda$2/1615780336
3362:             1             16  org.apache.cassandra.service.StartupChecks$1
3363:             1             16  org.apache.cassandra.service.StartupChecks$10
3364:             1             16  org.apache.cassandra.service.StartupChecks$11
3365:             1             16  org.apache.cassandra.service.StartupChecks$12
3366:             1             16  org.apache.cassandra.service.StartupChecks$2
3367:             1             16  org.apache.cassandra.service.StartupChecks$3
3368:             1             16  org.apache.cassandra.service.StartupChecks$4
3369:             1             16  org.apache.cassandra.service.StartupChecks$5
3370:             1             16  org.apache.cassandra.service.StartupChecks$6
3371:             1             16  org.apache.cassandra.service.StartupChecks$7
3372:             1             16  org.apache.cassandra.service.StartupChecks$9
3373:             1             16  org.apache.cassandra.service.StorageProxy
3374:             1             16  org.apache.cassandra.service.StorageProxy$1
3375:             1             16  org.apache.cassandra.service.StorageProxy$2
3376:             1             16  org.apache.cassandra.service.StorageProxy$3
3377:             1             16  org.apache.cassandra.service.StorageProxy$4
3378:             1             16  org.apache.cassandra.service.StorageService$$Lambda$259/1361973748
3379:             1             16  org.apache.cassandra.service.StorageService$1
3380:             1             16  org.apache.cassandra.service.paxos.Commit$CommitSerializer
3381:             1             16  org.apache.cassandra.service.paxos.CommitVerbHandler
3382:             1             16  org.apache.cassandra.service.paxos.PrepareResponse$PrepareResponseSerializer
3383:             1             16  org.apache.cassandra.service.paxos.PrepareVerbHandler
3384:             1             16  org.apache.cassandra.service.paxos.ProposeVerbHandler
3385:             1             16  org.apache.cassandra.streaming.ReplicationFinishedVerbHandler
3386:             1             16  org.apache.cassandra.streaming.StreamHook$1
3387:             1             16  org.apache.cassandra.streaming.StreamRequest$StreamRequestSerializer
3388:             1             16  org.apache.cassandra.streaming.StreamSummary$StreamSummarySerializer
3389:             1             16  org.apache.cassandra.streaming.compress.CompressionInfo$CompressionInfoSerializer
3390:             1             16  org.apache.cassandra.streaming.messages.CompleteMessage$1
3391:             1             16  org.apache.cassandra.streaming.messages.FileMessageHeader$FileMessageHeaderSerializer
3392:             1             16  org.apache.cassandra.streaming.messages.IncomingFileMessage$1
3393:             1             16  org.apache.cassandra.streaming.messages.KeepAliveMessage$1
3394:             1             16  org.apache.cassandra.streaming.messages.OutgoingFileMessage$1
3395:             1             16  org.apache.cassandra.streaming.messages.PrepareMessage$1
3396:             1             16  org.apache.cassandra.streaming.messages.ReceivedMessage$1
3397:             1             16  org.apache.cassandra.streaming.messages.RetryMessage$1
3398:             1             16  org.apache.cassandra.streaming.messages.SessionFailedMessage$1
3399:             1             16  org.apache.cassandra.streaming.messages.StreamInitMessage$StreamInitMessageSerializer
3400:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$add
3401:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$atomic_batch_mutate
3402:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate
3403:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$cas
3404:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_cluster_name
3405:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspace
3406:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_keyspaces
3407:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_local_ring
3408:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_partitioner
3409:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_ring
3410:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_schema_versions
3411:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_snitch
3412:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits
3413:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_splits_ex
3414:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_token_map
3415:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$describe_version
3416:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query
3417:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query
3418:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query
3419:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql_query
3420:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get
3421:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_count
3422:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices
3423:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_multi_slice
3424:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_paged_slice
3425:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_range_slices
3426:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$get_slice
3427:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$insert
3428:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$login
3429:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_count
3430:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$multiget_slice
3431:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql3_query
3432:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$prepare_cql_query
3433:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove
3434:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$remove_counter
3435:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_cql_version
3436:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$set_keyspace
3437:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_column_family
3438:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_add_keyspace
3439:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family
3440:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace
3441:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family
3442:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$system_update_keyspace
3443:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$trace_next_query
3444:             1             16  org.apache.cassandra.thrift.Cassandra$Processor$truncate
3445:             1             16  org.apache.cassandra.thrift.CassandraServer
3446:             1             16  org.apache.cassandra.thrift.CassandraServer$1
3447:             1             16  org.apache.cassandra.transport.CBUtil$1
3448:             1             16  org.apache.cassandra.transport.Message$ExceptionHandler
3449:             1             16  org.apache.cassandra.transport.Server$1
3450:             1             16  org.apache.cassandra.transport.messages.AuthChallenge$1
3451:             1             16  org.apache.cassandra.transport.messages.AuthResponse$1
3452:             1             16  org.apache.cassandra.transport.messages.AuthSuccess$1
3453:             1             16  org.apache.cassandra.transport.messages.AuthenticateMessage$1
3454:             1             16  org.apache.cassandra.transport.messages.BatchMessage$1
3455:             1             16  org.apache.cassandra.transport.messages.CredentialsMessage$1
3456:             1             16  org.apache.cassandra.transport.messages.ErrorMessage$1
3457:             1             16  org.apache.cassandra.transport.messages.EventMessage$1
3458:             1             16  org.apache.cassandra.transport.messages.ExecuteMessage$1
3459:             1             16  org.apache.cassandra.transport.messages.OptionsMessage$1
3460:             1             16  org.apache.cassandra.transport.messages.PrepareMessage$1
3461:             1             16  org.apache.cassandra.transport.messages.QueryMessage$1
3462:             1             16  org.apache.cassandra.transport.messages.ReadyMessage$1
3463:             1             16  org.apache.cassandra.transport.messages.RegisterMessage$1
3464:             1             16  org.apache.cassandra.transport.messages.ResultMessage$1
3465:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Prepared$1
3466:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Rows$1
3467:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SchemaChange$1
3468:             1             16  org.apache.cassandra.transport.messages.ResultMessage$SetKeyspace$1
3469:             1             16  org.apache.cassandra.transport.messages.ResultMessage$Void$1
3470:             1             16  org.apache.cassandra.transport.messages.StartupMessage$1
3471:             1             16  org.apache.cassandra.transport.messages.SupportedMessage$1
3472:             1             16  org.apache.cassandra.utils.AlwaysPresentFilter
3473:             1             16  org.apache.cassandra.utils.AsymmetricOrdering$Reversed
3474:             1             16  org.apache.cassandra.utils.BloomFilter$1
3475:             1             16  org.apache.cassandra.utils.BooleanSerializer
3476:             1             16  org.apache.cassandra.utils.Clock
3477:             1             16  org.apache.cassandra.utils.CoalescingStrategies$1
3478:             1             16  org.apache.cassandra.utils.CoalescingStrategies$2
3479:             1             16  org.apache.cassandra.utils.EstimatedHistogram$EstimatedHistogramSerializer
3480:             1             16  org.apache.cassandra.utils.FBUtilities$1
3481:             1             16  org.apache.cassandra.utils.FastByteOperations$UnsafeOperations
3482:             1             16  org.apache.cassandra.utils.Interval$1
3483:             1             16  org.apache.cassandra.utils.Interval$2
3484:             1             16  org.apache.cassandra.utils.JMXServerUtils$Exporter
3485:             1             16  org.apache.cassandra.utils.JMXServerUtils$JMXPluggableAuthenticatorWrapper
3486:             1             16  org.apache.cassandra.utils.JVMStabilityInspector$Killer
3487:             1             16  org.apache.cassandra.utils.MerkleTree$Hashable$HashableSerializer
3488:             1             16  org.apache.cassandra.utils.MerkleTree$Inner$InnerSerializer
3489:             1             16  org.apache.cassandra.utils.MerkleTree$Leaf$LeafSerializer
3490:             1             16  org.apache.cassandra.utils.MerkleTree$MerkleTreeSerializer
3491:             1             16  org.apache.cassandra.utils.MerkleTrees$MerkleTreesSerializer
3492:             1             16  org.apache.cassandra.utils.NanoTimeToCurrentTimeMillis$$Lambda$255/703776031
3493:             1             16  org.apache.cassandra.utils.NativeLibraryLinux
3494:             1             16  org.apache.cassandra.utils.NoSpamLogger$1
3495:             1             16  org.apache.cassandra.utils.StreamingHistogram$$Lambda$76/244613162
3496:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$136/1321552491
3497:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramBuilder$$Lambda$137/732447846
3498:             1             16  org.apache.cassandra.utils.StreamingHistogram$StreamingHistogramSerializer
3499:             1             16  org.apache.cassandra.utils.SystemTimeSource
3500:             1             16  org.apache.cassandra.utils.UUIDGen
3501:             1             16  org.apache.cassandra.utils.UUIDSerializer
3502:             1             16  org.apache.cassandra.utils.btree.BTree$$Lambda$193/1448037571
3503:             1             16  org.apache.cassandra.utils.btree.UpdateFunction$$Lambda$29/24650043
3504:             1             16  org.apache.cassandra.utils.concurrent.Ref$ReferenceReaper
3505:             1             16  org.apache.cassandra.utils.memory.BufferPool$1
3506:             1             16  org.apache.cassandra.utils.memory.BufferPool$2
3507:             1             16  org.apache.cassandra.utils.memory.HeapAllocator
3508:             1             16  org.apache.cassandra.utils.vint.VIntCoding$1
3509:             1             16  org.apache.thrift.TProcessorFactory
3510:             1             16  org.apache.thrift.transport.TFramedTransport$Factory
3511:             1             16  org.cliffc.high_scale_lib.NonBlockingHashMap$Prime
3512:             1             16  org.cliffc.high_scale_lib.NonBlockingHashSet
3513:             1             16  org.codehaus.jackson.map.deser.std.AtomicBooleanDeserializer
3514:             1             16  org.codehaus.jackson.map.deser.std.ClassDeserializer
3515:             1             16  org.codehaus.jackson.map.deser.std.DateDeserializer
3516:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$CurrencyDeserializer
3517:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$InetAddressDeserializer
3518:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$LocaleDeserializer
3519:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$PatternDeserializer
3520:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$TimeZoneDeserializer
3521:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URIDeserializer
3522:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$URLDeserializer
3523:             1             16  org.codehaus.jackson.map.deser.std.FromStringDeserializer$UUIDDeserializer
3524:             1             16  org.codehaus.jackson.map.deser.std.JavaTypeDeserializer
3525:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers
3526:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$BooleanDeser
3527:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ByteDeser
3528:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$CharDeser
3529:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$DoubleDeser
3530:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$FloatDeser
3531:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$IntDeser
3532:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$LongDeser
3533:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$ShortDeser
3534:             1             16  org.codehaus.jackson.map.deser.std.PrimitiveArrayDeserializers$StringDeser
3535:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigDecimalDeserializer
3536:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$BigIntegerDeserializer
3537:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$NumberDeserializer
3538:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$SqlDateDeserializer
3539:             1             16  org.codehaus.jackson.map.deser.std.StdDeserializer$StackTraceElementDeserializer
3540:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$BoolKD
3541:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$ByteKD
3542:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$CharKD
3543:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$DoubleKD
3544:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$FloatKD
3545:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$IntKD
3546:             1             16  org.codehaus.jackson.map.deser.std.StdKeyDeserializer$LongKD
3547:             1             16  org.codehaus.jackson.map.deser.std.StringDeserializer
3548:             1             16  org.codehaus.jackson.map.deser.std.TimestampDeserializer
3549:             1             16  org.codehaus.jackson.map.deser.std.TokenBufferDeserializer
3550:             1             16  org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer
3551:             1             16  org.codehaus.jackson.map.ext.OptionalHandlerFactory
3552:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector
3553:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$GetterMethodFilter
3554:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$MinimalMethodFilter
3555:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterAndGetterMethodFilter
3556:             1             16  org.codehaus.jackson.map.introspect.BasicClassIntrospector$SetterMethodFilter
3557:             1             16  org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector
3558:             1             16  org.codehaus.jackson.map.ser.StdSerializers$DoubleSerializer
3559:             1             16  org.codehaus.jackson.map.ser.StdSerializers$FloatSerializer
3560:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntLikeSerializer
3561:             1             16  org.codehaus.jackson.map.ser.StdSerializers$IntegerSerializer
3562:             1             16  org.codehaus.jackson.map.ser.StdSerializers$LongSerializer
3563:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlDateSerializer
3564:             1             16  org.codehaus.jackson.map.ser.StdSerializers$SqlTimeSerializer
3565:             1             16  org.codehaus.jackson.map.ser.impl.UnknownSerializer
3566:             1             16  org.codehaus.jackson.map.ser.std.CalendarSerializer
3567:             1             16  org.codehaus.jackson.map.ser.std.DateSerializer
3568:             1             16  org.codehaus.jackson.map.ser.std.NullSerializer
3569:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$ByteArraySerializer
3570:             1             16  org.codehaus.jackson.map.ser.std.StdArraySerializers$CharArraySerializer
3571:             1             16  org.codehaus.jackson.map.ser.std.StringSerializer
3572:             1             16  org.codehaus.jackson.map.ser.std.ToStringSerializer
3573:             1             16  org.codehaus.jackson.map.type.TypeParser
3574:             1             16  org.codehaus.jackson.node.JsonNodeFactory
3575:             1             16  org.github.jamm.MemoryLayoutSpecification$2
3576:             1             16  org.github.jamm.MemoryMeter$1
3577:             1             16  org.github.jamm.NoopMemoryMeterListener
3578:             1             16  org.github.jamm.NoopMemoryMeterListener$1
3579:             1             16  org.slf4j.helpers.NOPLoggerFactory
3580:             1             16  org.slf4j.helpers.SubstituteLoggerFactory
3581:             1             16  org.slf4j.impl.StaticMDCBinder
3582:             1             16  org.xerial.snappy.SnappyNative
3583:             1             16  org.yaml.snakeyaml.constructor.SafeConstructor$ConstructUndefined
3584:             1             16  org.yaml.snakeyaml.external.com.google.gdata.util.common.base.UnicodeEscaper$2
3585:             1             16  sun.management.ClassLoadingImpl
3586:             1             16  sun.management.HotSpotDiagnostic
3587:             1             16  sun.management.ManagementFactoryHelper$PlatformLoggingImpl
3588:             1             16  sun.misc.ASCIICaseInsensitiveComparator
3589:             1             16  sun.misc.FloatingDecimal$1
3590:             1             16  sun.misc.FormattedFloatingDecimal$1
3591:             1             16  sun.misc.Launcher
3592:             1             16  sun.misc.Launcher$Factory
3593:             1             16  sun.misc.ObjectInputFilter$Config$$Lambda$294/1344368391
3594:             1             16  sun.misc.Perf
3595:             1             16  sun.misc.Unsafe
3596:             1             16  sun.net.DefaultProgressMeteringPolicy
3597:             1             16  sun.net.ExtendedOptionsImpl$$Lambda$253/1943122657
3598:             1             16  sun.net.www.protocol.file.Handler
3599:             1             16  sun.net.www.protocol.jar.JarFileFactory
3600:             1             16  sun.nio.ch.EPollSelectorProvider
3601:             1             16  sun.nio.ch.ExtendedSocketOption$1
3602:             1             16  sun.nio.ch.FileChannelImpl$1
3603:             1             16  sun.nio.ch.Net$1
3604:             1             16  sun.nio.ch.Util$1
3605:             1             16  sun.nio.fs.LinuxFileSystemProvider
3606:             1             16  sun.reflect.GeneratedConstructorAccessor12
3607:             1             16  sun.reflect.GeneratedConstructorAccessor18
3608:             1             16  sun.reflect.GeneratedMethodAccessor10
3609:             1             16  sun.reflect.GeneratedMethodAccessor11
3610:             1             16  sun.reflect.GeneratedMethodAccessor12
3611:             1             16  sun.reflect.GeneratedMethodAccessor13
3612:             1             16  sun.reflect.GeneratedMethodAccessor14
3613:             1             16  sun.reflect.GeneratedMethodAccessor15
3614:             1             16  sun.reflect.GeneratedMethodAccessor6
3615:             1             16  sun.reflect.GeneratedMethodAccessor7
3616:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor36
3617:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor37
3618:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor38
3619:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor39
3620:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor40
3621:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor41
3622:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor42
3623:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor43
3624:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor44
3625:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor45
3626:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor46
3627:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor47
3628:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor49
3629:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor50
3630:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor51
3631:             1             16  sun.reflect.GeneratedSerializationConstructorAccessor52
3632:             1             16  sun.reflect.ReflectionFactory
3633:             1             16  sun.reflect.generics.tree.BooleanSignature
3634:             1             16  sun.reflect.generics.tree.BottomSignature
3635:             1             16  sun.reflect.generics.tree.VoidDescriptor
3636:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$8/817299424
3637:             1             16  sun.rmi.registry.RegistryImpl$$Lambda$9/2031951755
3638:             1             16  sun.rmi.registry.RegistryImpl_Skel
3639:             1             16  sun.rmi.registry.RegistryImpl_Stub
3640:             1             16  sun.rmi.runtime.Log$LoggerLogFactory
3641:             1             16  sun.rmi.runtime.RuntimeUtil
3642:             1             16  sun.rmi.server.LoaderHandler$2
3643:             1             16  sun.rmi.server.UnicastServerRef$HashToMethod_Maps
3644:             1             16  sun.rmi.transport.DGCImpl$$Lambda$6/516537656
3645:             1             16  sun.rmi.transport.DGCImpl$2$$Lambda$7/1023268896
3646:             1             16  sun.rmi.transport.DGCImpl_Skel
3647:             1             16  sun.rmi.transport.DGCImpl_Stub
3648:             1             16  sun.rmi.transport.Target$$Lambda$339/2000963151
3649:             1             16  sun.rmi.transport.proxy.RMIDirectSocketFactory
3650:             1             16  sun.rmi.transport.tcp.TCPTransport$1
3651:             1             16  sun.security.rsa.RSAKeyFactory
3652:             1             16  sun.security.ssl.EphemeralKeyManager
3653:             1             16  sun.security.util.ByteArrayLexOrder
3654:             1             16  sun.security.util.ByteArrayTagOrder
3655:             1             16  sun.text.normalizer.NormalizerBase$Mode
3656:             1             16  sun.text.normalizer.NormalizerBase$NFCMode
3657:             1             16  sun.text.normalizer.NormalizerBase$NFDMode
3658:             1             16  sun.text.normalizer.NormalizerBase$NFKCMode
3659:             1             16  sun.text.normalizer.NormalizerBase$NFKDMode
3660:             1             16  sun.util.calendar.Gregorian
3661:             1             16  sun.util.locale.provider.AuxLocaleProviderAdapter$NullProvider
3662:             1             16  sun.util.locale.provider.CalendarDataUtility$CalendarWeekParameterGetter
3663:             1             16  sun.util.locale.provider.SPILocaleProviderAdapter
3664:             1             16  sun.util.resources.LocaleData
3665:             1             16  sun.util.resources.LocaleData$LocaleDataResourceBundleControl
Total     119374210     4034601936
{code}

",N/A,"3.0.19, 3.11.5, 4.0-alpha1, 4.0"
CASSANDRA-14094,Avoid pointless calls to ThreadLocalRandom,"In the compression paths, we probabilistically validate the checksum. In cases where the chance is 100%, we don’t need to call {{ThreadLocalRandom}} .
",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14092,Max ttl of 20 years will overflow localDeletionTime,"CASSANDRA-4771 added a max value of 20 years for ttl to protect against [year 2038 overflow bug|https://en.wikipedia.org/wiki/Year_2038_problem] for {{localDeletionTime}}.

It turns out that next year the {{localDeletionTime}} will start overflowing with the maximum ttl of 20 years ({{System.currentTimeMillis() + ttl(20 years) > Integer.MAX_VALUE}}), so we should remove this limitation.",N/A,"2.1.20, 2.2.12, 3.0.16, 3.11.2"
CASSANDRA-14091,DynamicSnitch creates a lot of garbage,"The ExponentiallyDecayingReservoir snapshots we take during score updates generate a lot of garbage, and we call getSnapshot twice per endpoint when we only need to call it once.",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14088,Forward slash in role name breaks CassandraAuthorizer,"The standard system authorizer ({{org.apache.cassandra.auth.CassandraAuthorizer}}) stores the permissions granted to each user for a given resource in {{system_auth.role_permissions}}.

A resource like the {{my_keyspace.items}} table is stored as {{""data/my_keyspace/items""}} (note the {{/}} delimiter).

Similarly, role resources (like the {{joe}} role) are stored as {{""roles/joe""}}.

The problem is that roles can be created with {{/}} in their names, which confuses the authorizer when the table is queried.

For example,

{code}
$ bin/cqlsh -u cassandra -p cassandra
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 4.0-SNAPSHOT | CQL spec 3.4.5 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> CREATE ROLE emperor;
cassandra@cqlsh> CREATE ROLE ""ki/ng"";
cassandra@cqlsh> GRANT ALTER ON ROLE ""ki/ng"" TO emperor;
cassandra@cqlsh> LIST ROLES;

 role      | super | login | options
-----------+-------+-------+---------
 cassandra |  True |  True |        {}
   emperor | False | False |        {}
     ki/ng | False | False |        {}

(3 rows)
cassandra@cqlsh> SELECT * FROM system_auth.role_permissions;

 role      | resource      | permissions
-----------+---------------+--------------------------------
   emperor |   roles/ki/ng |                      {'ALTER'}
 cassandra | roles/emperor | {'ALTER', 'AUTHORIZE', 'DROP'}
 cassandra |   roles/ki/ng | {'ALTER', 'AUTHORIZE', 'DROP'}

(3 rows)
cassandra@cqlsh> LIST ALL PERMISSIONS OF emperor;
ServerError: java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
{code}

Here's the backtrace from the server process:

{code}
ERROR [Native-Transport-Requests-1] 2017-12-01 11:07:52,811 QueryMessage.java:129 - Unexpected error during query
java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
        at org.apache.cassandra.auth.RoleResource.fromName(RoleResource.java:101) ~[main/:na]
        at org.apache.cassandra.auth.Resources.fromName(Resources.java:56) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.listPermissionsForRole(CassandraAuthorizer.java:283) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.list(CassandraAuthorizer.java:263) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.list(ListPermissionsStatement.java:108) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.execute(ListPermissionsStatement.java:96) ~[main/:na]
        at org.apache.cassandra.cql3.statements.AuthorizationStatement.execute(AuthorizationStatement.java:48) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:207) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:238) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
ERROR [Native-Transport-Requests-1] 2017-12-01 11:07:52,812 ErrorMessage.java:389 - Unexpected exception during request
java.lang.IllegalArgumentException: roles/ki/ng is not a valid role resource name
        at org.apache.cassandra.auth.RoleResource.fromName(RoleResource.java:101) ~[main/:na]
        at org.apache.cassandra.auth.Resources.fromName(Resources.java:56) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.listPermissionsForRole(CassandraAuthorizer.java:283) ~[main/:na]
        at org.apache.cassandra.auth.CassandraAuthorizer.list(CassandraAuthorizer.java:263) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.list(ListPermissionsStatement.java:108) ~[main/:na]
        at org.apache.cassandra.cql3.statements.ListPermissionsStatement.execute(ListPermissionsStatement.java:96) ~[main/:na]
        at org.apache.cassandra.cql3.statements.AuthorizationStatement.execute(AuthorizationStatement.java:48) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:207) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:238) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:38) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:353) [netty-all-4.1.14.Final.jar:4.1.14.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14087,NPE when CAS encounters empty frozen collection,"When a compare-and-set operation specifying an equality criterion with a non-{{null}} value encounters an empty collection ({{null}} cell), the server throws a {{NullPointerException}} and the query fails.

This does not happen for non-frozen collections.

There's a self-contained test case at [github|https://github.com/incub8/cassandra-npe-in-cas].

The stack trace for 3.11.0 is:

{code}
ERROR [Native-Transport-Requests-1] 2017-11-27 12:59:26,924 QueryMessage.java:129 - Unexpected error during query
java.lang.NullPointerException: null
        at org.apache.cassandra.cql3.ColumnCondition$CollectionBound.appliesTo(ColumnCondition.java:546) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest$ColumnsConditions.appliesTo(CQL3CasRequest.java:324) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.CQL3CasRequest.appliesTo(CQL3CasRequest.java:210) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.StorageProxy.cas(StorageProxy.java:265) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithCondition(ModificationStatement.java:441) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:416) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_151]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_151]
{code}
",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14085,Excessive update of ReadLatency metric in digest calculation,"We noticed an increase in read latency after upgrading to 3.x, specifically for requests with CL>ONE. It turns out the read latency metric is being doubly updated for digest calculations. This code (https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/partitions/UnfilteredPartitionIterators.java#L243) makes an improper copy of an iterator that's wrapped by MetricRecording, whose onClose() records the latency of the execution.",N/A,"3.0.26, 3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-14084,Disks can be imbalanced during replace of same address when using JBOD,"While investigating CASSANDRA-14083, I noticed that [we use the pending ranges to calculate the disk boundaries|https://github.com/apache/cassandra/blob/41904684bb5509595d11f008d0851c7ce625e020/src/java/org/apache/cassandra/db/DiskBoundaryManager.java#L91] when the node is bootstrapping.

The problem is that when the node is replacing a node with the same address, it [sets itself as normal locally|https://github.com/apache/cassandra/blob/41904684bb5509595d11f008d0851c7ce625e020/src/java/org/apache/cassandra/service/StorageService.java#L1449] (for other unrelated reasons), so the local ranges will be null and consequently the disk boundaries will be null. This will cause the sstables to be randomly spread across disks potentially causing imbalance.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14083,Avoid invalidating disk boundaries unnecessarily,"We currently invalidate disk boundaries whenever [instantiating a new replication strategy|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/Keyspace.java#L359], but this is done whenever [updating keyspace settings|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/Keyspace.java#L187]. 

Computing new boundaries is expensive and unnecessarily invalidating them will cause {{DiskBoundaries}} consumers to also invalidate their work unnecessarily. For instance, after CASSANDRA-13948 the {{CompactionStrategyManager}} will reload all compaction strategies when the boundaries are invalidated.

In this case, we should only invalidate the disk boundaries when the replication settings change to avoid doing unnecessary work.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14082,Do not expose compaction strategy index publicly,"Before CASSANDRA-13215 we used the compaction strategy index to decide which disk to place a given sstable, but now we can get this directly from the disk boundary manager and keep the compaction strategy index internal only.

This will ensure external consumers will use a consistent {{DiskBoundaries}} object to perform operations on multiple disks, rather than risking getting inconsistent indexes if the compaction strategy indexes change between successive calls to {{CSM.getCompactionStrategyIndex}}.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14080,Handle incompletely written hint descriptors during startup,"Continuation of CASSANDRA-12728 bug.

Problem: Cassandra didn't start due to 0 size hints files

Log form v3.0.14:
{code:java}
INFO  [main] 2017-11-28 19:10:13,554 StorageService.java:575 - Cassandra version: 3.0.14
INFO  [main] 2017-11-28 19:10:13,555 StorageService.java:576 - Thrift API version: 20.1.0
INFO  [main] 2017-11-28 19:10:13,555 StorageService.java:577 - CQL supported versions: 3.4.0 (default: 3.4.0)
ERROR [main] 2017-11-28 19:10:13,592 CassandraDaemon.java:710 - Exception encountered during startup
org.apache.cassandra.io.FSReadError: java.io.EOFException
        at org.apache.cassandra.hints.HintsDescriptor.readFromFile(HintsDescriptor.java:142) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[na:1.8.0_141]
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ~[na:1.8.0_141]
        at java.util.Iterator.forEachRemaining(Iterator.java:116) ~[na:1.8.0_141]
        at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_141]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[na:1.8.0_141]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_141]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[na:1.8.0_141]
        at org.apache.cassandra.hints.HintsCatalog.load(HintsCatalog.java:65) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsService.<init>(HintsService.java:88) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsService.<clinit>(HintsService.java:63) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:121) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at java.lang.Class.forName0(Native Method) ~[na:1.8.0_141]
        at java.lang.Class.forName(Class.java:264) ~[na:1.8.0_141]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:585) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:570) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:346) [apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569) [apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697) [apache-cassandra-3.0.14.jar:3.0.14]
Caused by: java.io.EOFException: null
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:803) ~[na:1.8.0_141]
        at org.apache.cassandra.hints.HintsDescriptor.deserialize(HintsDescriptor.java:237) ~[apache-cassandra-3.0.14.jar:3.0.14]
        at org.apache.cassandra.hints.HintsDescriptor.readFromFile(HintsDescriptor.java:138) ~[apache-cassandra-3.0.14.jar:3.0.14]
        ... 20 common frames omitted
{code}



After several 0 size hints files deletion Cassandra started successfully.

Jeff Jirsa added a comment - Yesterday
Aleksandr Ivanov can you open a new JIRA and link it back to this one? It's possible that the original patch didn't consider 0 byte files (I don't have time to go back and look at the commit, and it was long enough ago that I've forgotten) - were all of your files 0 bytes?

Not all, 8..10 hints files were with 0 size.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14079,Prevent compaction strategies from looping indefinitely,"As a result of CASSANDRA-13948, LCS was looping indefinitely trying to generate the same candidates for SSTables which were not on the tracker.

We should add a protection on compaction strategies against looping indefinitely to avoid similar bugs in the future.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14071,Materialized view on table with TTL issue,"Materialized views that cluster by a column that is not part of table's PK and are created from tables that have *default_time_to_live* seems to malfunction.

Having this table
{code:java}
CREATE TABLE sbutnariu.test_bug (
    field1 smallint,
    field2 smallint,
    date timestamp,
    PRIMARY KEY ((field1), field2)
) WITH default_time_to_live = 1000;
{code}

and the materialized view
{code:java}
CREATE MATERIALIZED VIEW sbutnariu.test_bug_by_date AS SELECT * FROM sbutnariu.test_bug WHERE field1 IS NOT NULL AND field2 IS NOT NULL AND date IS NOT NULL PRIMARY KEY ((field1), date, field2) WITH CLUSTERING ORDER BY (date desc, field2 asc);
{code}

After inserting 3 rows with same PK (should upsert), the materialized view will have 3 rows.
{code:java}
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));
insert into sbutnariu.test_bug(field1, field2, date) values (1, 2, toTimestamp(now()));

select * from sbutnariu.test_bug; /*1 row*/
select * from sbutnariu.test_bug_by_date;/*3 rows*/
{code}

If I remove the ttl and try again, it works as expected:
{code:java}
truncate sbutnariu.test_bug;
alter table sbutnariu.test_bug with default_time_to_live = 0;

select * from sbutnariu.test_bug; /*1 row*/
select * from sbutnariu.test_bug_by_date;/*1 row*/
{code}

I've tested on versions 3.0.14 and 3.0.15. The bug was introduced in 3.0.15, as in 3.0.14 it works as expected.",N/A,"3.0.16, 3.11.2"
CASSANDRA-14065,Docs: Fix page width exceeding the viewport,"Ticket for [#175|https://github.com/apache/cassandra/pull/175] / [#176|https://github.com/apache/cassandra/pull/176].

The layout seems to adapt more natural after applying the patch with less overlapping content. Seems to fix a real issue with our template.

However, I'm not really sure about the extra.css changes, as the compile website (build via jekyll) doesn't seem to reference the css file anywhere..
",N/A,"3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-14057,The size of a byte is not 2,"{{DataLimits}} serializer uses {{TypeSizes.sizeof((byte)limits.kind().ordinal())}} in it's {{serializedSize}} method, but {{TypeSizes}} does not (on 3.0/3.11) have a {{sizeof(byte)}} override, so {{sizeof(short)}} is picked and it returns 2, which is wrong.

This is actually fixed on trunk, [~jasobrown] committed the fix as part of CASSANDRA-8457, but it wasn't committed in 3.0/3.11 and that still feel dodgy to leave as is.

To be clear, I don't think it's a problem as of now: it does break the size computed for read commands, and thus the payload size of such message, but said payload size is not use (for read requests that is).

Still, not reason to leave something wrong like this and risk a future bug when the fix is trivial.",N/A,"3.0.16, 3.11.2"
CASSANDRA-14055,Index redistribution breaks SASI index,"During index redistribution process, a new view is created.
During this creation, old indexes should be released.

But, new indexes are ""attached"" to the same SSTable as the old indexes.

This leads to the deletion of the last SASI index file and breaks the index.

The issue is in this function : [https://github.com/apache/cassandra/blob/9ee44db49b13d4b4c91c9d6332ce06a6e2abf944/src/java/org/apache/cassandra/index/sasi/conf/view/View.java#L62]



",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-14021,test_pycodestyle_compliance - cqlsh_tests.cqlsh_tests.TestCqlsh code style errors,"Once we commit CASSANDRA-14020, we'll need to cleanup all of the errors that pycodestyle has found to get the test passing",N/A,"2.1.20, 2.2.12, 3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14010,Fix SStable ordering by max timestamp in SinglePartitionReadCommand,"We have a test environment were we drop and create keyspaces and tables several times within a short time frame. Since upgrading from 3.11.0 to 3.11.1, we are seeing a lot of create statements failing. See the logs below:
{code:java}
2017-11-13T14:29:20.037986449Z WARN Directory /tmp/ramdisk/commitlog doesn't exist
2017-11-13T14:29:20.038009590Z WARN Directory /tmp/ramdisk/saved_caches doesn't exist
2017-11-13T14:29:20.094337265Z INFO Initialized prepared statement caches with 10 MB (native) and 10 MB (Thrift)
2017-11-13T14:29:20.805946340Z INFO Initializing system.IndexInfo
2017-11-13T14:29:21.934686905Z INFO Initializing system.batches
2017-11-13T14:29:21.973914733Z INFO Initializing system.paxos
2017-11-13T14:29:21.994550268Z INFO Initializing system.local
2017-11-13T14:29:22.014097194Z INFO Initializing system.peers
2017-11-13T14:29:22.124211254Z INFO Initializing system.peer_events
2017-11-13T14:29:22.153966833Z INFO Initializing system.range_xfers
2017-11-13T14:29:22.174097334Z INFO Initializing system.compaction_history
2017-11-13T14:29:22.194259920Z INFO Initializing system.sstable_activity
2017-11-13T14:29:22.210178271Z INFO Initializing system.size_estimates
2017-11-13T14:29:22.223836992Z INFO Initializing system.available_ranges
2017-11-13T14:29:22.237854207Z INFO Initializing system.transferred_ranges
2017-11-13T14:29:22.253995621Z INFO Initializing system.views_builds_in_progress
2017-11-13T14:29:22.264052481Z INFO Initializing system.built_views
2017-11-13T14:29:22.283334779Z INFO Initializing system.hints
2017-11-13T14:29:22.304110311Z INFO Initializing system.batchlog
2017-11-13T14:29:22.318031950Z INFO Initializing system.prepared_statements
2017-11-13T14:29:22.326547917Z INFO Initializing system.schema_keyspaces
2017-11-13T14:29:22.337097407Z INFO Initializing system.schema_columnfamilies
2017-11-13T14:29:22.354082675Z INFO Initializing system.schema_columns
2017-11-13T14:29:22.384179063Z INFO Initializing system.schema_triggers
2017-11-13T14:29:22.394222027Z INFO Initializing system.schema_usertypes
2017-11-13T14:29:22.414199833Z INFO Initializing system.schema_functions
2017-11-13T14:29:22.427205182Z INFO Initializing system.schema_aggregates
2017-11-13T14:29:22.427228345Z INFO Not submitting build tasks for views in keyspace system as storage service is not initialized
2017-11-13T14:29:22.652838866Z INFO Scheduling approximate time-check task with a precision of 10 milliseconds
2017-11-13T14:29:22.732862906Z INFO Initializing system_schema.keyspaces
2017-11-13T14:29:22.746598744Z INFO Initializing system_schema.tables
2017-11-13T14:29:22.759649011Z INFO Initializing system_schema.columns
2017-11-13T14:29:22.766245435Z INFO Initializing system_schema.triggers
2017-11-13T14:29:22.778716809Z INFO Initializing system_schema.dropped_columns
2017-11-13T14:29:22.791369819Z INFO Initializing system_schema.views
2017-11-13T14:29:22.839141724Z INFO Initializing system_schema.types
2017-11-13T14:29:22.852911976Z INFO Initializing system_schema.functions
2017-11-13T14:29:22.852938112Z INFO Initializing system_schema.aggregates
2017-11-13T14:29:22.869348526Z INFO Initializing system_schema.indexes
2017-11-13T14:29:22.874178682Z INFO Not submitting build tasks for views in keyspace system_schema as storage service is not initialized
2017-11-13T14:29:23.700250435Z INFO Initializing key cache with capacity of 25 MBs.
2017-11-13T14:29:23.724357053Z INFO Initializing row cache with capacity of 0 MBs
2017-11-13T14:29:23.724383599Z INFO Initializing counter cache with capacity of 12 MBs
2017-11-13T14:29:23.724386906Z INFO Scheduling counter cache save to every 7200 seconds (going to save all keys).
2017-11-13T14:29:23.984408710Z INFO Populating token metadata from system tables
2017-11-13T14:29:24.032687075Z INFO Global buffer pool is enabled, when pool is exhausted (max is 125.000MiB) it will allocate on heap
2017-11-13T14:29:24.214123695Z INFO Token metadata:
2017-11-13T14:29:24.304218769Z INFO Completed loading (14 ms; 8 keys) KeyCache cache
2017-11-13T14:29:24.363978406Z INFO No commitlog files found; skipping replay
2017-11-13T14:29:24.364005238Z INFO Populating token metadata from system tables
2017-11-13T14:29:24.394408476Z INFO Token metadata:
2017-11-13T14:29:24.709411652Z INFO Preloaded 0 prepared statements
2017-11-13T14:29:24.719332880Z INFO Cassandra version: 3.11.1
2017-11-13T14:29:24.719355969Z INFO Thrift API version: 20.1.0
2017-11-13T14:29:24.719359443Z INFO CQL supported versions: 3.4.4 (default: 3.4.4)
2017-11-13T14:29:24.719362103Z INFO Native protocol supported versions: 3/v3, 4/v4, 5/v5-beta (default: 4/v4)
2017-11-13T14:29:24.766102400Z INFO Initializing index summary manager with a memory pool size of 25 MB and a resize interval of 60 minutes
2017-11-13T14:29:24.778800183Z INFO Starting Messaging Service on /172.17.0.2:7000 (eth0)
2017-11-13T14:29:24.783832188Z WARN No host ID found, created 62452b7c-33ae-40e6-859c-1d7c803aaea8 (Note: This should happen exactly once per node).
2017-11-13T14:29:24.897281778Z INFO Loading persisted ring state
2017-11-13T14:29:24.904217782Z INFO Starting up server gossip
2017-11-13T14:29:25.003802973Z INFO This node will not auto bootstrap because it is configured to be a seed node.
2017-11-13T14:29:25.047674499Z INFO Generated random tokens. tokens are [-6736304773851341012, 3437071596424929702, 4372058337604769145, -306854781937968525, -4419476154597297006, 4339837665480866486, 2052026232731139893, -5761537575805252593, -4477540978357776290, 6263754683045286998, 3670054894619378302, -4326549778810780939, 7187409938161102814, 7030537377703307755, -2757270254308154659, -1953637968902719055, -7235425703069930259, 7123794193321014835, 349308827967095711, 997472983569031481, 992257140226393205, -4045122629441468253, 4149955653388319941, -3690032393349188278, 3528068129562283633, -5057394127379238561, -4944743272177354946, 1371473468273321389, -2771267888257678908, -2379074055482922854, 8800628062632970014, 6016352719444925532, -6458243637210081043, -7131512441131507433, -6135681286390467242, -7886878247827491401, -3964432859204941604, -7124853795154335905, 4536647221115220987, 4518363137218750861, -3945920538919881061, -8569890499152898728, -2228677668104169495, -4004623128783039030, -6849460601197629451, -1787645289665343374, -9004089114738085395, -8444847561386064840, -7719025430480017932, -5020575591450775929, -3535144847803187721, 7252524597471726426, -2582131369519057623, 3737595811793840609, -7248797595897252845, -7065188032269288840, -6731826791431802176, -2970075663731571587, -2619987499373344925, -2698285069650269138, -8589822844420136511, 2658120945314344720, -3710290429036098141, 134530136452862749, 3703742438909992913, 3460544540911930621, 8673891706698173777, 2853177281247015813, 13977464647778584, 2404057737490125388, -6759648287860184451, 744453319830059045, -688104893800828924, 3356383003502762348, 9054641886966810357, 2317130729058165506, -5810663910204725460, 2577132949237273515, 6326216055185945365, 1376570278575995967, 8758101809469842945, -2892126907778256351, -1716283861287440286, 3040640159143123724, 4243935966006505554, -6827972097309863039, 3055912546894309570, -3992773844369808712, -4717007910267923035, -846198401308205724, -3924870907185309086, 1746803312676010060, 6821355560067598541, -5786385588878319458, 3085551110635941848, 7832310180114101987, -9149254679798945822, 3124836728424468300, -100875121723899324, -7606007094353527325, 270256410769436649, -3016541299722946307, 6864985654287583845, 8465468836551135602, 7372808321676939792, -2815261206329145311, -2044219183173664775, -5342853768228072396, 3636940711408324184, -2772742494800447004, -8420993393273439531, -1530882172522252534, 8236427746033013128, -8939749738449264357, -571957476330656311, 6462994120934510138, -2744633996286755268, 1001793370994802364, 6170004027360887596, 383603396273760626, 184737756504479596, -4799447088893889554, 1038205033737034383, 2078124248957773983, -5177819727898656480, 1588469358432181111, 2476693400197902714, 246839957213783595, -7804622995667946321, 3516202677463047183, 7649126752776473673, -3286662198144050257, 2592926684883421936, 6953901594207876325, 8920684239689152479, -2427878301857439455, -6527468054932471540, -4117125961852289967, -2833593154725933249, 2548273043767381234, -814886098184093796, -1113961241682560435, -8364806058670744019, -86067309810855914, -7325813350040495905, -2651532619332818109, -3028501296208600216, 2638649530375347897, -3870517833780069551, 3770751443844709295, -7272035856681375921, -6750394828506790417, 3368553496734537183, 8516129492713951191, 4435960977618718666, 638690551817702460, -7462842134093200053, -7312636473795422279, 3825550639500258186, -490674188267611204, 8488259904981422083, 4436678791994058329, 5971819389544487212, 5777643219857256454, 6295906877222880293, -6635403410495817577, -7125973103119231247, 2275471188158109929, -6554337501188391642, -4759608795508681126, -7655250005358224912, 9106670136441382451, -9080117178764089351, 5094764588972879219, -3599769156391426161, 6116955962236377408, -1734768840951819839, 7826627278264825770, -2624139016757063818, -4122417151587476614, -6757251857390630385, 2099124804383862824, -3162332634454027278, 4826222794133551270, 9122652158513265055, 1734656138981660315, 972980826344778639, -1746779194020635548, -3426944282250211269, -3857828063692993065, 1895243495321867610, -8828035583443240909, -4705856469629722102, -8519546521146945353, -2150150551733933931, 8281585304878501119, -2775028105733898661, 2087277989579187052, -4016777313261130077, 2747128117959922334, -1398884803916585873, 7188260080368469340, -3880993098463994199, 3574665846011083154, 5260683239918360122, 5817587463499837044, 38978473621576635, 2680910834841463710, 6083561971466189055, 7236937177408808074, -3600112532662592989, -4559800196660261967, 8276688045060113438, 5496539762676760591, -2999626688519766687, 8917068693185637310, 2348378561310644717, 7605443413072783308, 5729359499569394810, -782345069306605591, 1165004403533704355, -8301882560002322767, 2008499890787626408, -6211027251975593898, 7406423735628820605, -3204398339633370684, -7917412446164112725, -106645076087724250, -1186720400780396653, -8676089669972641821, -1970508303671183113, -7283082875075535628, -3469652138221449481, -3310949358194646693, 6449384223770405185, -3602652844861890703, -7845236015467185307, -4548809972889727666, -8898627491921139823, 5187965699546741544, 295363921125698104, -8013235493809339368, -6747271362503076577, 1102625310233591704, -2543233385033476145, -6197912327393001665, 118165474822979356, -4838870266722406438, -5797141823778124932, -1506683916229985698, 9139710449103348665, -1571612701117454805, 8031141543284728427, 8472337544063987034, 3222463867738580103, 8210687258187437204]
2017-11-13T14:29:25.092248590Z INFO Create new Keyspace: KeyspaceMetadata{name=system_traces, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}, tables=[org.apache.cassandra.config.CFMetaData@3bc5ed95[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,flags=[COMPOUND],params=TableParams{comment=tracing sessions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [client command coordinator duration request started_at parameters]],partitionKeyColumns=[session_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[client, command, session_id, coordinator, request, started_at, duration, parameters],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@1a296ffd[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,flags=[COMPOUND],params=TableParams{comment=tracing events, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=0, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [activity source source_elapsed thread]],partitionKeyColumns=[session_id],clusteringColumns=[event_id],keyValidator=org.apache.cassandra.db.marshal.UUIDType,columnMetadata=[activity, event_id, session_id, source, thread, source_elapsed],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.394141160Z INFO Not submitting build tasks for views in keyspace system_traces as storage service is not initialized
2017-11-13T14:29:25.408584506Z INFO Initializing system_traces.events
2017-11-13T14:29:25.424314845Z INFO Initializing system_traces.sessions
2017-11-13T14:29:25.483133136Z INFO Create new Keyspace: KeyspaceMetadata{name=system_distributed, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}, tables=[org.apache.cassandra.config.CFMetaData@2884b38b[cfId=759fffad-624b-3181-80ee-fa9a52d1f627,ksName=system_distributed,cfName=repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.TimeUUIDType),partitionColumns=[[] | [coordinator exception_message exception_stacktrace finished_at parent_id range_begin range_end started_at status participants]],partitionKeyColumns=[keyspace_name, columnfamily_name],clusteringColumns=[id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[status, id, coordinator, finished_at, participants, exception_stacktrace, parent_id, range_end, range_begin, exception_message, keyspace_name, started_at, columnfamily_name],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7fcc80b2[cfId=deabd734-b99d-3b9c-92e5-fd92eb5abf14,ksName=system_distributed,cfName=parent_repair_history,flags=[COMPOUND],params=TableParams{comment=Repair history, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [exception_message exception_stacktrace finished_at keyspace_name started_at columnfamily_names options requested_ranges successful_ranges]],partitionKeyColumns=[parent_id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,columnMetadata=[requested_ranges, exception_message, keyspace_name, successful_ranges, started_at, finished_at, options, exception_stacktrace, parent_id, columnfamily_names],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7e500004[cfId=5582b59f-8e4e-35e1-b913-3acada51eb04,ksName=system_distributed,cfName=view_build_status,flags=[COMPOUND],params=TableParams{comment=Materialized View build status, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UUIDType),partitionColumns=[[] | [status]],partitionKeyColumns=[keyspace_name, view_name],clusteringColumns=[host_id],keyValidator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type),columnMetadata=[view_name, status, keyspace_name, host_id],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.598604284Z INFO Not submitting build tasks for views in keyspace system_distributed as storage service is not initialized
2017-11-13T14:29:25.602132560Z INFO Initializing system_distributed.parent_repair_history
2017-11-13T14:29:25.624580018Z INFO Initializing system_distributed.repair_history
2017-11-13T14:29:25.624605811Z INFO Initializing system_distributed.view_build_status
2017-11-13T14:29:25.682205208Z INFO JOINING: Finish joining ring
2017-11-13T14:29:25.808448539Z INFO Create new Keyspace: KeyspaceMetadata{name=system_auth, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[org.apache.cassandra.config.CFMetaData@3c28c0da[cfId=5bc52802-de25-35ed-aeab-188eecebb090,ksName=system_auth,cfName=roles,flags=[COMPOUND],params=TableParams{comment=role definitions, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [can_login is_superuser salted_hash member_of]],partitionKeyColumns=[role],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[salted_hash, member_of, role, can_login, is_superuser],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@2e0f771e[cfId=0ecdaa87-f8fb-3e60-88d1-74fb36fe5c0d,ksName=system_auth,cfName=role_members,flags=[COMPOUND],params=TableParams{comment=role memberships lookup table, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[role],clusteringColumns=[member],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, member],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@4fabdebb[cfId=3afbe79f-2194-31a7-add7-f5ab90d8ec9c,ksName=system_auth,cfName=role_permissions,flags=[COMPOUND],params=TableParams{comment=permissions granted to db roles, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | [permissions]],partitionKeyColumns=[role],clusteringColumns=[resource],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[role, resource, permissions],droppedColumns={},triggers=[],indexes=[]], org.apache.cassandra.config.CFMetaData@7103b8de[cfId=5f2fbdad-91f1-3946-bd25-d5da3a5c35ec,ksName=system_auth,cfName=resource_role_permissons_index,flags=[COMPOUND],params=TableParams{comment=index of db roles with permissions granted on a resource, read_repair_chance=0.0, dclocal_read_repair_chance=0.0, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=7776000, default_time_to_live=0, memtable_flush_period_in_ms=3600000, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(org.apache.cassandra.db.marshal.UTF8Type),partitionColumns=[[] | []],partitionKeyColumns=[resource],clusteringColumns=[role],keyValidator=org.apache.cassandra.db.marshal.UTF8Type,columnMetadata=[resource, role],droppedColumns={},triggers=[],indexes=[]]], views=[], functions=[], types=[]}
2017-11-13T14:29:25.934019252Z INFO Not submitting build tasks for views in keyspace system_auth as storage service is not initialized
2017-11-13T14:29:25.953887674Z INFO Initializing system_auth.resource_role_permissons_index
2017-11-13T14:29:25.957358898Z INFO Initializing system_auth.role_members
2017-11-13T14:29:25.967935061Z INFO Initializing system_auth.role_permissions
2017-11-13T14:29:25.995449692Z INFO Initializing system_auth.roles
2017-11-13T14:29:26.193856408Z INFO Netty using native Epoll event loop
2017-11-13T14:29:26.247676724Z INFO Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
2017-11-13T14:29:26.247705469Z INFO Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
2017-11-13T14:29:26.309591159Z INFO Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it
2017-11-13T14:29:36.275846037Z INFO Created default superuser role 'cassandra'
2017-11-13T14:29:40.333918591Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:40.434399612Z INFO Create new table: org.apache.cassandra.config.CFMetaData@c74a94b[cfId=1572b410-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:40.566922871Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:42.719380089Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:43.124510221Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:43.243928493Z INFO Create new table: org.apache.cassandra.config.CFMetaData@1a0616e9[cfId=171e8f50-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:43.284700491Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:44.706916652Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:44.924446999Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:44.993983743Z INFO Create new table: org.apache.cassandra.config.CFMetaData@7338ccab[cfId=182996b0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:45.078407254Z INFO Initializing my_keyspace.schema_version
2017-11-13T14:29:46.244137923Z INFO Drop Keyspace 'my_keyspace'
2017-11-13T14:29:46.500351100Z INFO Create new Keyspace: KeyspaceMetadata{name=my_keyspace, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=1}}, tables=[], views=[], functions=[], types=[]}
2017-11-13T14:29:46.575419551Z INFO Create new table: org.apache.cassandra.config.CFMetaData@229f3694[cfId=191b97d0-c87f-11e7-9db1-6d2c86545d91,ksName=my_keyspace,cfName=schema_version,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4c3448a7, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [migration_lock version]],partitionKeyColumns=[id],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[migration_lock, version, id],droppedColumns={},triggers=[],indexes=[]]
2017-11-13T14:29:46.617101680Z ERROR Unexpected error during query
2017-11-13T14:29:46.617126436Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.617130194Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617133358Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617135966Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617138576Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617141018Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617143454Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617145953Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617148372Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617150806Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617153201Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617155595Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617157962Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617160377Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617162787Z at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617166295Z at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617168898Z at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617171389Z at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.617173808Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.617184008Z at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617186971Z at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617189340Z at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
2017-11-13T14:29:46.617191666Z Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.617193951Z at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
2017-11-13T14:29:46.617196258Z at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
2017-11-13T14:29:46.617198553Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:400) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617200927Z ... 20 common frames omitted
2017-11-13T14:29:46.617203114Z Caused by: java.lang.NullPointerException: null
2017-11-13T14:29:46.617205382Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617207766Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617210107Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617212462Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617214868Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617217261Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617220404Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617222948Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617225287Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.617227589Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.617229894Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.617232175Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
2017-11-13T14:29:46.617234514Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.617236990Z ... 1 common frames omitted
2017-11-13T14:29:46.621331936Z ERROR Exception in thread Thread[MigrationStage:1,5,main]
2017-11-13T14:29:46.621360645Z java.lang.NullPointerException: null
2017-11-13T14:29:46.621364339Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621373614Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621376363Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621378927Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621381395Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621384992Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621387567Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621390255Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621392722Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
2017-11-13T14:29:46.621395153Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.621397502Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.621399919Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
2017-11-13T14:29:46.621402347Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.621404867Z at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
2017-11-13T14:29:46.626625652Z ERROR Unexpected exception during request
2017-11-13T14:29:46.626650886Z java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.626654840Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:404) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626658003Z at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:549) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626660570Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:356) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626663155Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:341) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626665745Z at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:321) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626676412Z at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:89) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626679497Z at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626682051Z at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626684610Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626687059Z at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626689495Z at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626691956Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626694391Z at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626696869Z at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626700811Z at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626703433Z at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626705926Z at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
2017-11-13T14:29:46.626708464Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.626710858Z at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626713448Z at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626715868Z at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
2017-11-13T14:29:46.626718281Z Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
2017-11-13T14:29:46.626720647Z at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
2017-11-13T14:29:46.626723006Z at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
2017-11-13T14:29:46.626725392Z at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:400) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626727820Z ... 20 common frames omitted
2017-11-13T14:29:46.626730100Z Caused by: java.lang.NullPointerException: null
2017-11-13T14:29:46.626735106Z at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626737800Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspaceParams(SchemaKeyspace.java:956) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626740362Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:943) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626742804Z at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesOnly(SchemaKeyspace.java:937) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626745273Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1363) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626747719Z at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1342) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626750759Z at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:567) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626753445Z at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626755900Z at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
2017-11-13T14:29:46.626758684Z at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
2017-11-13T14:29:46.626761055Z at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
2017-11-13T14:29:46.626763436Z at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
2017-11-13T14:29:46.626765871Z at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) ~[apache-cassandra-3.11.1.jar:3.11.1]
2017-11-13T14:29:46.626768418Z ... 1 common frames omitted{code}

Steps to reproduce:
1. Start cassandra
2. Start cqlsh and paste the following in quick succession:
{code:java}
USE system;
DROP KEYSPACE IF EXISTS my_keyspace;
CREATE KEYSPACE my_keyspace WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};
USE my_keyspace;
CREATE TABLE schema_version (id int primary key, version int, migration_lock text);
INSERT INTO schema_version (id, version) values (1, 0);{code}
3. Once fourth time or so , we'll see:
{code:java}
cqlsh:system> CREATE KEYSPACE my_keyspace WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1};
ServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException{code}
or
{code:java}
cqlsh:my_keyspace> CREATE TABLE schema_version (id int primary key, version int, migration_lock text);
ServerError: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException{code}",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-14008,RTs at index boundaries in 2.x sstables can create unexpected CQL row in 3.x,"In 2.1/2.2, it is possible for a range tombstone that isn't a row deletion and isn't a complex deletion to appear between two cells with the same clustering. The 8099 legacy code incorrectly treats the two (non-RT) cells as two distinct CQL rows, despite having the same clustering prefix.",N/A,"3.0.16, 3.11.2"
CASSANDRA-14003,Correct logger message formatting in SSTableLoader,"Correct logger message formatting inside SSTableLoader.java

-        outputHandler.output(String.format(""Streaming relevant part of %sto %s"", names(sstables), endpointToRanges.keySet()));
+        outputHandler.output(String.format(""Streaming relevant part of %s to %s"", names(sstables), endpointToRanges.keySet()));
 
",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13999,Segfault during memtable flush,"We are getting segfaults on a production Cassandra cluster, apparently caused by Memtable flushes to disk.
{code}
Current thread (0x000000000cd77920):  JavaThread ""PerDiskMemtableFlushWriter_0:140"" daemon [_thread_in_Java, id=28952, stack(0x00007f8b7aa53000,0x00007f8b7aa94000)]
{code}

Stack
{code}
Stack: [0x00007f8b7aa53000,0x00007f8b7aa94000],  sp=0x00007f8b7aa924a0,  free space=253k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J 21889 C2 org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(Lorg/apache/cassandra/db/rows/UnfilteredRowIterator;)Lorg/apache/cassandra/db/RowIndexEntry; (361 bytes) @ 0x00007f8e9fcf75ac [0x00007f8e9fcf42c0+0x32ec]
J 22464 C2 org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents()V (383 bytes) @ 0x00007f8e9f17b988 [0x00007f8e9f17b5c0+0x3c8]
j  org.apache.cassandra.db.Memtable$FlushRunnable.call()Lorg/apache/cassandra/io/sstable/SSTableMultiWriter;+1
j  org.apache.cassandra.db.Memtable$FlushRunnable.call()Ljava/lang/Object;+1
J 18865 C2 java.util.concurrent.FutureTask.run()V (126 bytes) @ 0x00007f8e9d3c9540 [0x00007f8e9d3c93a0+0x1a0]
J 21832 C2 java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (225 bytes) @ 0x00007f8e9f16856c [0x00007f8e9f168400+0x16c]
J 6720 C1 java.util.concurrent.ThreadPoolExecutor$Worker.run()V (9 bytes) @ 0x00007f8e9def73c4 [0x00007f8e9def72c0+0x104]
J 22079 C2 java.lang.Thread.run()V (17 bytes) @ 0x00007f8e9e67c4ac [0x00007f8e9e67c460+0x4c]
v  ~StubRoutines::call_stub
V  [libjvm.so+0x691d16]  JavaCalls::call_helper(JavaValue*, methodHandle*, JavaCallArguments*, Thread*)+0x1056
V  [libjvm.so+0x692221]  JavaCalls::call_virtual(JavaValue*, KlassHandle, Symbol*, Symbol*, JavaCallArguments*, Thread*)+0x321
V  [libjvm.so+0x6926c7]  JavaCalls::call_virtual(JavaValue*, Handle, KlassHandle, Symbol*, Symbol*, Thread*)+0x47
V  [libjvm.so+0x72da50]  thread_entry(JavaThread*, Thread*)+0xa0
V  [libjvm.so+0xa76833]  JavaThread::thread_main_inner()+0x103
V  [libjvm.so+0xa7697c]  JavaThread::run()+0x11c
V  [libjvm.so+0x927568]  java_start(Thread*)+0x108
C  [libpthread.so.0+0x7de5]  start_thread+0xc5
{code}

For further details, we attached:
* JVM error file with all details
* cassandra config file (we are using offheap_buffers as memtable_allocation_method)
* some lines printed in debug.log when the JVM error file was created and process died

h5. Reproducing the issue
So far we have been unable to reproduce it. It happens once/twice a week on single nodes. It happens either during high load or low load times. We have seen that when we replace EC2 instances and bootstrap new ones, due to compactions happening on source nodes before stream starts, sometimes more than a single node was affected by this, letting us with 2 out of 3 replicas out and UnavailableExceptions in the cluster.

This issue might have relation with CASSANDRA-12590 (Segfault reading secondary index) even this is the write path. Can someone confirm if both issues could be related? 

h5. Specifics of our scenario:
* Cassandra 3.9 on Amazon Linux (previous to this, we were running Cassandra 2.0.9 and there are no records of this also happening, even I was not working on Cassandra)
* 12 x i3.2xlarge EC2 instances (8 core, 64GB RAM)
* a total of 176 keyspaces (there is a per-customer pattern)
** Some keyspaces have a single table, while others have 2 or 5 tables
** There is a table that uses standard Secondary Indexes (""emailindex"" on ""user_info"" table)
* It happens on both Oracle JDK 1.8.0_112 and 1.8.0_131
* It happens in both kernel 4.9.43-17.38.amzn1.x86_64 and 3.14.35-28.38.amzn1.x86_64


h5. Possible workarounds/solutions that we have in mind (to be validated yet)
* switching to heap_buffers (in case offheap_buffers triggers the bug), even we are still pending to measure performance degradation under that scenario.
* removing secondary indexes in favour of Materialized Views for this specific case, even we are concerned too about the fact that using MVs introduces new issues that may be present in our current Cassandra 3.9
* Upgrading to 3.11.1 is an option, but we are trying to keep it as last resort given that the cost of migrating is big and we don't have any guarantee that new bugs that affects nodes availability are not introduced.",N/A,3.10
CASSANDRA-13987,Multithreaded commitlog subtly changed durability,"When multithreaded commitlog was introduced in CASSANDRA-3578, we subtly changed the way that commitlog durability worked. Everything still gets written to an mmap file. However, not everything is replayable from the mmaped file after a process crash, in periodic mode.

In brief, the reason this changesd is due to the chained markers that are required for the multithreaded commit log. At each msync, we wait for outstanding mutations to serialize into the commitlog, and update a marker before and after the commits that have accumluated since the last sync. With those markers, we can safely replay that section of the commitlog. Without the markers, we have no guarantee that the commits in that section were successfully written, thus we abandon those commits on replay.

If you have correlated process failures of multiple nodes at ""nearly"" the same time (see [""There Is No Now""|http://queue.acm.org/detail.cfm?id=2745385]), it is possible to have data loss if none of the nodes msync the commitlog. For example, with RF=3, if quorum write succeeds on two nodes (and we acknowledge the write back to the client), and then the process on both nodes OOMs (say, due to reading the index for a 100GB partition), the write will be lost if neither process msync'ed the commitlog. More exactly, the commitlog cannot be fully replayed. The reason why this data is silently lost is due to the chained markers that were introduced with CASSANDRA-3578.

The problem we are addressing with this ticket is incrementally improving 'durability' due to process crash, not host crash. (Note: operators should use batch mode to ensure greater durability, but batch mode in it's current implementation is a) borked, and b) will burn through, *very* rapidly, SSDs that don't have a non-volatile write cache sitting in front.) 

The current default for {{commitlog_sync_period_in_ms}} is 10 seconds, which means that a node could lose up to ten seconds of data due to process crash. The unfortunate thing is that the data is still avaialble, in the mmap file, but we can't replay it due to incomplete chained markers.

ftr, I don't believe we've ever had a stated policy about commitlog durability wrt process crash. Pre-2.0 we naturally piggy-backed off the memory mapped file and the fact that every mutation was acquired a lock and wrote into the mmap buffer, and the ability to replay everything out of it came for free. With CASSANDRA-3578, that was subtly changed. 

Something [~jjirsa] pointed out to me is that [MySQL provides a way to adjust the durability guarantees|https://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_flush_log_at_trx_commit] of each commit in innodb via the {{innodb_flush_log_at_trx_commit}}. I'm using that idea as a loose springboard for what to do here.",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13975,Add a workaround for overly large read repair mutations,"It's currently possible for {{DataResolver}} to accumulate more changes to read repair that would fit in a single serialized mutation. If that happens, the node receiving the mutation would fail, and the read would time out, and won't be able to proceed until the operator runs repair or manually drops the affected partitions.

Ideally we should either read repair iteratively, or at least split the resulting mutation into smaller chunks in the end. In the meantime, for 3.0.x, I suggest we add logging to catch this, and a -D flag to allow proceeding with the requests as is when the mutation is too large, without read repair.",N/A,"3.0.16, 3.11.2"
CASSANDRA-13974,Bad prefix matching when figuring out data directory for an sstable,"We do a ""startsWith"" check when getting data directory for an sstable, we should match including File.separator",N/A,"3.0.20, 3.11.6, 4.0-alpha3, 4.0"
CASSANDRA-13964,Tracing interferes with digest requests when using RandomPartitioner,"A {{ThreadLocal<MessageDigest>}} is used to generate the MD5 digest when a replica serves a read command and the {{isDigestQuery}} flag is set. The same threadlocal is also used by {{RandomPartitioner}} to decorate partition keys. So in a cluster with RP, if tracing is enabled the data digest is corrupted by the partitioner making tokens for the tracing mutations. This causes a digest mismatch on the coordinator, triggering a full data read on every read where CL > 1 (or speculative execution/read repair kick in).
",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13959,"Add yaml flag for disabling MVs, log warnings on creation","As discussed on dev@, we should give operators the option to disable materialized view creation, and log warnings when they're created.

Update - Adding link for posterity: https://lists.apache.org/thread.html/d81a61da48e1b872d7599df4edfa8e244d34cbd591a18539f724796f@%3Cdev.cassandra.apache.org%3E",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13956,Cannot restrict columns on materialized view,"After upgrading from 3.11.0 to 3.11.1 I cannot define materialized views restricted on non primary key columns.

I get: InvalidRequest: Error from server: code=2200 [Invalid query] message=""Non-primary key columns cannot be restricted in the SELECT statement used for materialized view creation (got restrictions on: d_destination_set)""

In my 3.11.0 cluster:

$ cqlsh ******
Connected to Test Cluster at ********:9042.
[cqlsh 5.0.1 | Cassandra 3.11.0 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh>   CREATE TABLE phoenix.test_table (
   ...     datetime_bucket         ascii primary key,
   ...     d_destination_set       boolean,
   ...     d_destination_timestamp timestamp
   ...   );
cqlsh> CREATE MATERIALIZED VIEW phoenix.test_view AS
   ...   SELECT datetime_bucket,
   ...          d_destination_set
   ...     FROM phoenix.test_table
   ...     WHERE datetime_bucket IS NOT NULL
   ...       AND d_destination_set = FALSE
   ... PRIMARY KEY (datetime_bucket, d_destination_set);
cqlsh> 

No errors shown here.

In my 3.11.1 cluster:

cqlsh>   CREATE TABLE phoenix.test_table (
   ...     datetime_bucket         ascii primary key,
   ...     d_destination_set       boolean,
   ...     d_destination_timestamp timestamp
   ...   );
cqlsh> CREATE MATERIALIZED VIEW phoenix.test_view AS
   ...   SELECT datetime_bucket,
   ...          d_destination_set
   ...     FROM phoenix.test_table
   ...     WHERE datetime_bucket IS NOT NULL
   ...       AND d_destination_set = FALSE
   ... PRIMARY KEY (datetime_bucket, d_destination_set);
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Non-primary key columns cannot be restricted in the SELECT statement used for materialized view creation (got restrictions on: d_destination_set)""

",N/A,3.11.1
CASSANDRA-13954,Provide a JMX call to sync schema with local storage,"As discussed in CASSANDRA-13813 comments, we need such a call / nodetool command to enable the workaround for CASSANDRA-12701.

This is technically a new feature and shouldn't go into 3.0.x, however in practical terms it's part of a solution to CASSANDRA-12701, which is a bug, and an pre-requisite for CASSANDRA-13813 - which is also a bug.",N/A,"3.0.16, 3.11.2"
CASSANDRA-13949,java.lang.ArrayIndexOutOfBoundsException while executing query,"While executing a query on a  table contaninig a field with a (escaped) json, the following exception occurs:

java.lang.ArrayIndexOutOfBoundsException: null
        at org.codehaus.jackson.io.JsonStringEncoder.quoteAsString(JsonStringEncoder.java:141) ~[jackson-core-asl-1.9.2.jar:1.9.2]
        at org.apache.cassandra.cql3.Json.quoteAsJsonString(Json.java:45) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.db.marshal.UTF8Type.toJSONString(UTF8Type.java:66) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection.rowToJson(Selection.java:291) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:431) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:417) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:763) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:378) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:251) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.0.jar:3.11.0]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_131]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.0.jar:3.11.0]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]

Find attached the schema of the table, the insertion query with the data provoking the failure, and the failing query.
 ",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13948,Reload compaction strategies when JBOD disk boundary changes,"The thread dump below shows a race between an sstable replacement by the {{IndexSummaryRedistribution}} and {{AbstractCompactionTask.getNextBackgroundTask}}:

{noformat}
Thread 94580: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.util.concurrent.locks.AbstractQueuedSynchronizer$Node, int) @bci=67, line=870 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(int) @bci=17, line=1199 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock() @bci=5, line=943 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleListChangedNotification(java.lang.Iterable, java.lang.Iterable) @bci=359, line=483 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(org.apache.cassandra.notifications.INotification, java.lang.Object) @bci=53, line=555 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.Tracker.notifySSTablesChanged(java.util.Collection, java.util.Collection, org.apache.cassandra.db.compaction.OperationType, java.lang.Throwable) @bci=50, line=409 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.LifecycleTransaction.doCommit(java.lang.Throwable) @bci=157, line=227 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit(java.lang.Throwable) @bci=61, line=116 (Compiled frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit() @bci=2, line=200 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish() @bci=5, line=185 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryRedistribution.redistributeSummaries() @bci=559, line=130 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager.runIndexSummaryRedistribution(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=9, line=1420 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=4, line=250 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries() @bci=30, line=228 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager$1.runMayThrow() @bci=4, line=125 (Interpreted frame)
 - org.apache.cassandra.utils.WrappedRunnable.run() @bci=1, line=28 (Interpreted frame)
 - org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run() @bci=4, line=118 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}

{noformat}
Thread 94573: (state = IN_JAVA)
 - java.util.HashMap$HashIterator.nextNode() @bci=95, line=1441 (Compiled frame; information may be imprecise)
 - java.util.HashMap$KeyIterator.next() @bci=1, line=1461 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(org.apache.cassandra.db.lifecycle.View) @bci=20, line=268 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(java.lang.Object) @bci=5, line=265 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.apply(com.google.common.base.Predicate, com.google.common.base.Function) @bci=13, line=133 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.tryModify(java.lang.Iterable, org.apache.cassandra.db.compaction.OperationType) @bci=31, line=99 (Compiled frame)
 - org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(int) @bci=84, line=139 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(int) @bci=105, line=119 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run() @bci=84, line=265 (Interpreted frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
{noformat}

This particular node remain in this state forever, indicating {{LeveledCompactionStrategyTask.getNextBackgroundTask}} was looping indefinitely.

What happened is that sstable references were replaced on the tracker by the {{IndexSummaryRedistribution}} thread, so the {{AbstractCompactionStrategy.getNextBackgroundTask}} could not create the transaction with the old references, and the {{IndexSummaryRedistribution}} could not update the sstable reference in the compaction strategy because {{AbstractCompactionStrategy.getNextBackgroundTask}} was holding the {{CompactionStrategyManager}} lock.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13939,Mishandling of cells for removed/dropped columns when reading legacy files,"The tl;dr is that there is a bug in reading legacy files that can manifests itself with a trace looking like this:
{noformat}
Exception (java.lang.IllegalStateException) encountered during startup: One row required, 2 found
java.lang.IllegalStateException: One row required, 2 found
    at org.apache.cassandra.cql3.UntypedResultSet$FromResultSet.one(UntypedResultSet.java:84)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTableTimestamp(LegacySchemaMigrator.java:254)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTable(LegacySchemaMigrator.java:244)
    at org.apache.cassandra.schema.LegacySchemaMigrator.lambda$readTables$7(LegacySchemaMigrator.java:238)
    at org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$126/591203139.accept(Unknown Source)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readTables(LegacySchemaMigrator.java:238)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readKeyspace(LegacySchemaMigrator.java:187)
    at org.apache.cassandra.schema.LegacySchemaMigrator.lambda$readSchema$4(LegacySchemaMigrator.java:178)
    at org.apache.cassandra.schema.LegacySchemaMigrator$$Lambda$123/1612073393.accept(Unknown Source)
    at java.util.ArrayList.forEach(ArrayList.java:1249)
    at org.apache.cassandra.schema.LegacySchemaMigrator.readSchema(LegacySchemaMigrator.java:178)
{noformat}

The reason this can happen has to do with the handling of legacy files. Legacy files are cell based while the 3.0 storage engine is primarily row based, so we group those cells into rows early in the deserialization process (in {{UnfilteredDeserializer.OldFormatDeserializer}}), but in doing so, we can only consider a row finished when we've either reach the end of the partition/file, or when we've read a cell that doesn't belong to that row.  That second case means that when the deserializer returns a given row, the underlying file pointer may actually not positioned at the end of that row, but rather it may be past the first cell of the next row (which the deserializer remembers for future use). Long story short, when we try to detect if we're logically past our current index block in {{AbstractIterator.IndexState#isPastCurrentBlock(}}), we can't simply rely on the file pointer, which again may be a bit more advanced that we logically are, and that's the reason for the ""correction"" in that method. That correction is really just the amount of bytes remembered but not yet used in the deserializer.

That ""correction"" is sometimes wrong however and that's due to the fact that in {{LegacyLayout#readLegacyAtom}}, if we get a cell for an dropped or removed cell, we ignore that cell (which, in itself, is fine). Problem is that this skipping is done within the {{LegacyLayout#readLegacyAtom}} method but without {{UnfilteredDeserializer.OldFormatDeserializer}} knowing about it. As such, the size of the skipped cell ends up being accounted in the ""correction"" bytes for the next cell we read. Lo and behold, if such cell for a removed/dropped column is both the last cell of a CQL row and just before an index boundary (pretty unlikely in general btw, but definitively possible), then the deserializer will count its size with the first cell of the next row, which happens to also be the first cell of the next index block.  And when the code then tries to figure out if it crossed an index boundary, it will over-correct. That is, the {{indexState.updateBlock()}} call at the start of {{SSTableIterator.ForwardIndexedReader#computeNext}} will not work correctly.  This can then make the code return a row that is after the requested slice end (and should thus not be returned) because it doesn't compare that row to said requested end due to thinking it's not on the last index block to read, even though it genuinely is.

Anyway, the whole explanation is a tad complex, but the fix isn't: we need to move the skipping of cells for removed/dropped column a level up so the deserializer knows about it and don't silently count their size in the next atom size.",N/A,"3.0.16, 3.11.2"
CASSANDRA-13935,Indexes and UDTs creation should have IF NOT EXISTS on its String representation,"I came across something that bothers me a lot. I'm using snapshots to backup data from my Cassandra cluster in case something really bad happens (like dropping a table or a keyspace).

Exercising the recovery actions from those backups, I discover that the schema put on the file ""schema.cql"" as a result of the snapshot has the ""CREATE IF NOT EXISTS"" for the table, but not for the indexes.

When restoring from snapshots, and relying on the execution of these schemas to build up the table structure, everything seems fine for tables without secondary indexes, but for the ones that make use of them, the execution of these statements fail miserably.

Here I paste a generated schema.cql content for a table with indexes:

CREATE TABLE IF NOT EXISTS keyspace1.table1 (
	id text PRIMARY KEY,
	content text,
	last_update_date date,
	last_update_date_time timestamp)
	WITH ID = f1045fc0-2f59-11e7-95ec-295c3c064920
	AND bloom_filter_fp_chance = 0.01
	AND dclocal_read_repair_chance = 0.1
	AND crc_check_chance = 1.0
	AND default_time_to_live = 8640000
	AND gc_grace_seconds = 864000
	AND min_index_interval = 128
	AND max_index_interval = 2048
	AND memtable_flush_period_in_ms = 0
	AND read_repair_chance = 0.0
	AND speculative_retry = '99PERCENTILE'
	AND caching = { 'keys': 'NONE', 'rows_per_partition': 'NONE' }
	AND compaction = { 'max_threshold': '32', 'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy' }
	AND compression = { 'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor' }
	AND cdc = false
	AND extensions = {  };
CREATE INDEX table1_last_update_date_idx ON keyspace1.table1 (last_update_date);

I think the last part should be:

CREATE INDEX IF NOT EXISTS table1_last_update_date_idx ON keyspace1.table1 (last_update_date);

// edit by Stefan Miklosovic

PR: https://github.com/apache/cassandra/pull/731

I have added UDTs as part of this patch as well.",N/A,"3.0.23, 3.11.9, 4.0-beta3, 4.0"
CASSANDRA-13933,Handle mutateRepaired failure in nodetool verify,See comment here: https://issues.apache.org/jira/browse/CASSANDRA-13922?focusedCommentId=16189875&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16189875,N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13930,Avoid grabbing the read lock when checking LCS fanout and if compaction strategy should do defragmentation,"We grab the read lock when checking whether the compaction strategy benefits from defragmentation, avoid that.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13929,BTree$Builder / io.netty.util.Recycler$Stack leaking memory,"Different to CASSANDRA-13754, there seems to be another memory leak in 3.11.0+ in BTree$Builder / io.netty.util.Recycler$Stack.

* heap utilization increase after upgrading to 3.11.0 => cassandra_3.11.0_min_memory_utilization.jpg
* No difference after upgrading to 3.11.1 (snapshot build) => cassandra_3.11.1_snapshot_heaputilization.png; thus most likely after fixing CASSANDRA-13754, more visible now
* MAT shows io.netty.util.Recycler$Stack as top contributing class => cassandra_3.11.1_mat_dominator_classes.png
* With -Xmx8G (CMS) and our load pattern, we have to do a rolling restart after ~ 72 hours

Verified the following fix, namely explicitly unreferencing the _recycleHandle_ member (making it non-final). In _org.apache.cassandra.utils.btree.BTree.Builder.recycle()_
{code}
        public void recycle()
        {
            if (recycleHandle != null)
            {
                this.cleanup();
                builderRecycler.recycle(this, recycleHandle);
                recycleHandle = null; // ADDED
            }
        }
{code}

Patched a single node in our loadtest cluster with this change and after ~ 10 hours uptime, no sign of the previously offending class in MAT anymore => cassandra_3.11.1_mat_dominator_classes_FIXED.png

Can' say if this has any other side effects etc., but I doubt.",N/A,3.11.3
CASSANDRA-13922,nodetool verify should also verify sstable metadata,"nodetool verify should also try to deserialize the sstable metadata (and once CASSANDRA-13321 makes it in, verify the checksums)",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13918,Header only commit logs should be filtered before recovery,"Commit log recovery will tolerate commit log truncation in the most recent log file found on disk, but will abort startup if problems are detected in others. 

Since we allocate commit log segments before they're used though, it's possible to get into a state where the last commit log file actually written to is not the same file that was most recently allocated, preventing startup for what should otherwise be allowable incomplete final segments.

Excluding header only files on recovery should prevent this from happening.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13917,COMPACT STORAGE queries on dense static tables accept hidden column1 and value columns,"Test for the issue:

{code}
    @Test
    public void testCompactStorage() throws Throwable
    {
        createTable(""CREATE TABLE %s (a int PRIMARY KEY, b int, c int) WITH COMPACT STORAGE"");
        assertInvalid(""INSERT INTO %s (a, b, c, column1) VALUES (?, ?, ?, ?)"", 1, 1, 1, ByteBufferUtil.bytes('a'));
        // This one fails with Some clustering keys are missing: column1, which is still wrong
        assertInvalid(""INSERT INTO %s (a, b, c, value) VALUES (?, ?, ?, ?)"", 1, 1, 1, ByteBufferUtil.bytes('a'));       
        assertInvalid(""INSERT INTO %s (a, b, c, column1, value) VALUES (?, ?, ?, ?, ?)"", 1, 1, 1, ByteBufferUtil.bytes('a'), ByteBufferUtil.bytes('b'));
        assertEmpty(execute(""SELECT * FROM %s""));
    }
{code}

Gladly, these writes are no-op, even though they succeed.

{{value}} and {{column1}} should be completely hidden. Fixing this one should be as easy as just adding validations.",N/A,"3.0.21, 3.11.7"
CASSANDRA-13916,Remove OpenJDK log warning,"The following warning message will appear in the logs when using OpenJDK
{noformat}
WARN  [main] ... OpenJDK is not recommended. Please upgrade to the newest Oracle Java release
{noformat}

The above warning dates back to when OpenJDK 6 was released and there were some issues in early releases of this version. The OpenJDK implementation is used as a reference for the OracleJDK which means the implementations are very close. In addition, most users have moved off Java 6 so we can probably remove this warning message.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13911,IllegalStateException thrown by UPI.Serializer.hasNext() for some SELECT queries,"Certain combinations of rows, in presence of per partition limit (set explicitly in 3.6+ or implicitly to 1 via DISTINCT) cause {{UnfilteredPartitionIterators.Serializer.hasNext()}} to throw {{IllegalStateException}} .

Relevant code snippet:

{code}
// We can't answer this until the previously returned iterator has been fully consumed,
// so complain if that's not the case.
if (next != null && next.hasNext())
    throw new IllegalStateException(""Cannot call hasNext() until the previous iterator has been fully consumed"");
{code}

Since {{UnfilteredPartitionIterators.Serializer}} and {{UnfilteredRowIteratorSerializer.serializer}} deserialize partitions/rows lazily, it is required for correct operation of the partition iterator to have the previous partition fully consumed, so that deserializing the next one can start from the correct position in the byte buffer. However, that condition won’t always be satisfied, as there are legitimate combinations of rows that do not consume every row in every partition.

For example, look at [this dtest|https://github.com/iamaleksey/cassandra-dtest/commits/13911].

In case we end up with a following pattern of rows:

{code}
node1, partition 0 | 0
node2, partition 0 |   x x
{code}

, where {{x}} and {{x}} a row tombstones for rows 1 and 2, it’s sufficient for {{MergeIterator}} to only look at row 0 in partition from node1 and at row tombstone 1 from node2 to satisfy the per partition limit of 1. The stopping merge result counter will stop iteration right there, leaving row tombstone 2 from node2 unvisited and not deseiralized. Switching to the next partition will in turn trigger the {{IllegalStateException}} because we aren’t done yet.

The stopping counter is behaving correctly, so is the {{MergeIterator}}. I’ll note that simply removing that condition is not enough to fix the problem properly - it’d just cause us to deseiralize garbage, trying to deserialize a new partition from a position in the bytebuffer that precedes remaining rows in the previous partition.",N/A,"3.0.15, 3.11.1"
CASSANDRA-13910,Remove read_repair_chance/dclocal_read_repair_chance,"First, let me clarify so this is not misunderstood that I'm not *at all* suggesting to remove the read-repair mechanism of detecting and repairing inconsistencies between read responses: that mechanism is imo fine and useful.  But the {{read_repair_chance}} and {{dclocal_read_repair_chance}} have never been about _enabling_ that mechanism, they are about querying all replicas (even when this is not required by the consistency level) for the sole purpose of maybe read-repairing some of the replica that wouldn't have been queried otherwise. Which btw, bring me to reason 1 for considering their removal: their naming/behavior is super confusing. Over the years, I've seen countless users (and not only newbies) misunderstanding what those options do, and as a consequence misunderstand when read-repair itself was happening.

But my 2nd reason for suggesting this is that I suspect {{read_repair_chance}}/{{dclocal_read_repair_chance}} are, especially nowadays, more harmful than anything else when enabled. When those option kick in, what you trade-off is additional resources consumption (all nodes have to execute the read) for a _fairly remote chance_ of having some inconsistencies repaired on _some_ replica _a bit faster_ than they would otherwise be. To justify that last part, let's recall that:
# most inconsistencies are actually fixed by hints in practice; and in the case where a node stay dead for a long time so that hints ends up timing-out, you really should repair the node when it comes back (if not simply re-bootstrapping it).  Read-repair probably don't fix _that_ much stuff in the first place.
# again, read-repair do happen without those options kicking in. If you do reads at {{QUORUM}}, inconsistencies will eventually get read-repaired all the same.  Just a tiny bit less quickly.
# I suspect almost everyone use a low ""chance"" for those options at best (because the extra resources consumption is real), so at the end of the day, it's up to chance how much faster this fixes inconsistencies.

Overall, I'm having a hard time imagining real cases where that trade-off really make sense. Don't get me wrong, those options had their places a long time ago when hints weren't working all that well, but I think they bring more confusion than benefits now.

And I think it's sane to reconsider stuffs every once in a while, and to clean up anything that may not make all that much sense anymore, which I think is the case here.

Tl;dr, I feel the benefits brought by those options are very slim at best and well overshadowed by the confusion they bring, and not worth maintaining the code that supports them (which, to be fair, isn't huge, but getting rid of {{ReadCallback.AsyncRepairRunner}} wouldn't hurt for instance).

Lastly, if the consensus here ends up being that they can have their use in weird case and that we fill supporting those cases is worth confusing everyone else and maintaining that code, I would still suggest disabling them totally by default.
",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13909,Improve TRUNCATE performance with many sstables,"Truncate is very slow in 3.0, mostly due to {{LogRecord.make}} listing all files in the directory for every sstable.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13907,Versioned documentation on cassandra.apache.org,"Services like https://readthedocs.org and http://www.javadoc.io/ make it easy to browse the documentation for a particular version or commit of various open source projects. It would be nice to be able to browse the docs for a particular release on http://cassandra.apache.org/doc/.

Currently it seems only CQL has this at http://cassandra.apache.org/doc/old/.",N/A,"3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-13901,Linux Script for stopping running cassandra and cqlsh,The script for stopping Cassandra and cqlsh if running on any Linux machine.,N/A,3.11.12
CASSANDRA-13897,"nodetool compact and flush fail with ""error: null""","{{nodetool flush}} and {{nodetool compact}} return an error message that is not clear. This could probably be improved. Both of my two nodes return this error.

{{nodetool flush}} Will return this error the first 2-3 times you invoke it, then the error temporarily disappears. {{nodetool compress}} always returns this error message no matter how many times you invoke it.

I have tried deleting saved_caches, commit logs, doing nodetool compact/rebuild/scrub, and nothing seems to remove the error. 

{noformat}
cass@s5:~/apache-cassandra-3.11.0$ nodetool compact
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.cache.ChunkCache$CachingRebufferer.<init>(ChunkCache.java:222)
	at org.apache.cassandra.cache.ChunkCache.wrap(ChunkCache.java:175)
	at org.apache.cassandra.io.util.FileHandle$Builder.maybeCached(FileHandle.java:412)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:381)
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:331)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinal(BigTableWriter.java:333)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openFinalEarly(BigTableWriter.java:318)
	at org.apache.cassandra.io.sstable.SSTableRewriter.switchWriter(SSTableRewriter.java:322)
	at org.apache.cassandra.io.sstable.SSTableRewriter.doPrepare(SSTableRewriter.java:370)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.doPrepare(CompactionAwareWriter.java:111)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.prepareToCommit(Transactional.java:173)
	at org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish(Transactional.java:184)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.finish(CompactionAwareWriter.java:121)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:220)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:733)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
	at java.lang.Thread.run(Thread.java:748)

{noformat}
",N/A,3.11.2
CASSANDRA-13894,TriggerExecutor ignored original PartitionUpdate,"Since 3.0, the [TriggerExecutor.execute(PartitionUpdate)|https://github.com/jasonstack/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/triggers/TriggerExecutor.java#L82-L89] will only return augmented mutation, ignoring original update..

[Test|https://github.com/jasonstack/cassandra/commit/eb28844035242c4cb73c5148254f34672f2325da]to reproduce.


",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13891,fromJson(null) throws java.lang.NullPointerException on Cassandra,"Basically, {{fromJson}} throws a {{java.lang.NullPointerException}} when NULL is passed, instead of just returning a NULL itself. Say I create a UDT and a table as follows:
{code:java}
create type type1
(
id int,
name text
);

create table table1
(
id int,
t FROZEN<type1>,

primary key (id)
);{code}
And then try and insert a row as such:

{{insert into table1 (id, t) VALUES (1, fromJson(null));}}

I get the error: {{java.lang.NullPointerException}}

This works as expected: {{insert into table1 (id, t) VALUES (1, null);}}

Programmatically, one does not always know when a UDT will be null, hence me expecting {{fromJson}} to just return NULL.",N/A,"2.2.13, 3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13883,StrictLiveness for view row is not handled in AbstractRow,"In {{AbstractRow.hasLiveData(nowInSecond)}}, it doesn't handle {{strictLiveness}} introduced in CASSANDRA-11500. The {{DataLimits}} counts the expired view row as live data and then the expired view row is purged in {{Row.purge()}}. When query with limit, we will get less data.

{code:title=test to reproduce}
    @Test
    public void testRegularColumnTimestampUpdates() throws Throwable
    {
        createTable(""CREATE TABLE %s ("" +
                    ""k int PRIMARY KEY, "" +
                    ""c int, "" +
                    ""val int)"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""mv_rctstest"", ""CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE k IS NOT NULL AND c IS NOT NULL PRIMARY KEY (k,c)"");

        updateView(""UPDATE %s SET c = ?, val = ? WHERE k = ?"", 0, 0, 0);
        updateView(""UPDATE %s SET val = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s SET c = ? WHERE k = ?"", 1, 0);
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest""), row(1, 0, 1));

        updateView(""TRUNCATE %s"");

        updateView(""UPDATE %s USING TIMESTAMP 1 SET c = ?, val = ? WHERE k = ?"", 0, 0, 0);
        updateView(""UPDATE %s USING TIMESTAMP 3 SET c = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s USING TIMESTAMP 2 SET val = ? WHERE k = ?"", 1, 0);
        updateView(""UPDATE %s USING TIMESTAMP 4 SET c = ? WHERE k = ?"", 2, 0);
        updateView(""UPDATE %s USING TIMESTAMP 3 SET val = ? WHERE k = ?"", 2, 0);

        // FIXME no rows return
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest limit 1""), row(2, 0, 2));
        assertRows(execute(""SELECT c, k, val FROM mv_rctstest""), row(2, 0, 2));
    }
{code}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13880,Fix short read protection for tables with no clustering columns,"CASSANDRA-12872 fixed counting replica rows, so that we do now fetch more than one extra row if necessary.

Fixing the issue caused consistency_test.py:TestConsistency.test_13747 to start failing, by exposing a bug in the way we handle empty clusterings.

When {{moreContents()}} asks for another row and {{lastClustering}} is {{EMPTY}}, the response again (and again) contains the row with {{EMPTY}} clustering.

SRP assumes it’s a new row, counts it as one, gets confused and keeps asking for more, in a loop, again and again.

Arguably, a response to a read command with the following non-inclusive {{ClusteringIndexFilter}}:

{code}
command.clusteringIndexFilter(partitionKey).forPaging(metadata.comparator, Clustering.EMPTY, false);
{code}

... should return nothing at all rather than a row with an empty clustering.

Also arguably, SRP should not even attempt to fetch more rows if {{lastClustering == Clustering.EMPTY}}. In a partition key only column
we shouldn’t expect any more rows.

This JIRA is to fix the latter issue on SRP side - to modify SRP logic to short-circuit execution if {{lastClustering}} was an {{EMPTY}} one instead of querying pointlessly for non-existent extra rows.",N/A,"3.0.15, 3.11.1"
CASSANDRA-13874,nodetool setcachecapacity behaves oddly when cache disabled,"If a node has row cache disabled, trying to turn it on via setcachecapacity doesn't issue an error, and doesn't turn it on, it just silently doesn't work.",N/A,"3.11.12, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-13873,Ref bug in Scrub,"I'm hitting a Ref bug when many scrubs run against a node.  This doesn't happen on 3.0.X.  I'm not sure if/if not this happens with compactions too but I suspect it does.

I'm not seeing any Ref leaks or double frees.

To Reproduce:

{quote}
./tools/bin/cassandra-stress write n=10m -rate threads=100
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
#Ctrl-C
./bin/nodetool scrub
{quote}

Eventually in the logs you get:
WARN  [RMI TCP Connection(4)-127.0.0.1] 2017-09-14 15:51:26,722 NoSpamLogger.java:97 - Spinning trying to capture readers [BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-5-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-32-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-31-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-29-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-27-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-26-big-Data.db'), BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-20-big-Data.db')],
*released: [BigTableReader(path='/home/jake/workspace/cassandra2/data/data/keyspace1/standard1-2eb5c780998311e79e09311efffdcd17/mc-5-big-Data.db')],* 

This released table has a selfRef of 0 but is in the Tracker
",N/A,"2.2.12, 3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13869,AbstractTokenTreeBuilder#serializedSize returns wrong value when there is a single leaf and overflow collisions,In the extremely rare case where a small token tree (< 248 values) has overflow collisions the size returned by AbstractTokenTreeBuilder#serializedSize is incorrect because it fails to account for the overflow collisions. ,N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13868,Safely handle empty buffers when outputting to JSON,"As discussed in CASSANDRA-13734, when {{Selection#rowToJSON}} encounters an empty {{ByteBuffer}}, a {{BufferUnderflowException}} can be thrown. At a minimum let's be defensive and not throw an error.",N/A,"2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13866,Clock-dependent integer overflow in tests CellTest and RowsTest,"These tests create timestamps from Unix time, but this is done as int math with the result stored in a long. This means that if the test is run at certain times, like 1505177731, corresponding to Tuesday, September 12, 2017, 12:55:31, the test can have two timestamps separated by a single second that reverse their ordering when multiplied by 1000000, such as 1505177731 -> 2147149504 and 1505177732 -> -2146817792. This causes a variety of test failures, since it changes the reconciliation order of these cells.

Note that I've tagged this as trivial because the problem is in the manual construction of timestamps in the test; I know of nowhere  that we make this mistake with real data.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13851,Allow existing nodes to use all peers in shadow round,"In CASSANDRA-10134 we made collision checks necessary on every startup. A side-effect was introduced that then requires a nodes seeds to be contacted on every startup. Prior to this change an existing node could start up regardless whether it could contact a seed node or not (because checkForEndpointCollision() was only called for bootstrapping nodes). 

Now if a nodes seeds are removed/deleted/fail it will no longer be able to start up until live seeds are configured (or itself is made a seed), even though it already knows about the rest of the ring. This is inconvenient for operators and has the potential to cause some nasty surprises and increase downtime.

One solution would be to use all a nodes existing peers as seeds in the shadow round. Not a Gossip guru though so not sure of implications.

",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13849,GossipStage blocks because of race in ActiveRepairService,"Bad luck caused a kernel panic in a cluster, and that took another node with it because GossipStage stopped responding.

I think it's pretty obvious what's happening, here are the relevant excerpts from the stack traces :

{noformat}
""Thread-24004"" #393781 daemon prio=5 os_prio=0 tid=0x00007efca9647400 nid=0xe75c waiting on condition [0x00007efaa47fe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x000000052b63a7e8> (a java.util.concurrent.CountDownLatch$Sync)
    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
    at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
    at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:332)
    - locked <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:211)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)                                                                                                           at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipTasks:1"" #367 daemon prio=5 os_prio=0 tid=0x00007efc5e971000 nid=0x700b waiting for monitor entry [0x00007dfb839fe000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)
    at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:306)
    at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:775)                                                                                                                at org.apache.cassandra.gms.Gossiper.access$800(Gossiper.java:67)
    at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:187)
    at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:748)

""GossipStage:1"" #320 daemon prio=5 os_prio=0 tid=0x00007efc5b9f2c00 nid=0x6fcd waiting for monitor entry [0x00007e260186a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.ActiveRepairService.removeParentRepairSession(ActiveRepairService.java:421)
    - waiting to lock <0x00000002e6bc99f0> (a org.apache.cassandra.service.ActiveRepairService)                                                                                          at org.apache.cassandra.service.ActiveRepairService.convict(ActiveRepairService.java:776)
    at org.apache.cassandra.service.ActiveRepairService.onRestart(ActiveRepairService.java:744)
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1049)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1143)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$3/1498438472.run(Unknown Source)                                                                                       at java.lang.Thread.run(Thread.java:748)
{noformat}

iow, org.apache.cassandra.service.ActiveRepairService.prepareForRepair holds a lock until the repair is prepared, which means waiting for other nodes to respond, which may die at exactly that moment, so they won't complete. Gossip will at the same time try to mark the node as down, but it requires that same lock :)",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13843,Debian init shadows CASSANDRA_HEAPDUMP_DIR,The debian init script sets the heap dump file directly using the cassandra users home directory and the -H flag to the cassandra process[1|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/debian/init#L76]. The cassandra heap dump location can also be set in the cassandra-env.sh file using CASSANDRA_HEAPDUMP_DIR. Unfortunately the debian init heap dump location is based off the home directory of the cassandra user and cannot easily be changed. Also unfortunately if you do `ps aux | grep casandra` you can clearly see that the -H flag takes precedent over the value found in cassandra-env.sh. This makes it difficult to change the heap dump location for cassandra and is non-intuitive when the value is set in cassandra-env.sh why the heap dump does not actually end up in the correct place.,N/A,"3.0.26, 3.11.12, 4.0.2"
CASSANDRA-13833,Failed compaction is not captured,"Follow up for CASSANDRA-13785, when the compaction failed, it fails silently. No error message is logged and exceptions metric is not updated. Basically, it's unable to get the exception: [CompactionManager.java:1491|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1491]

Here is the call stack:
{noformat}
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at java.lang.Thread.run(Thread.java:745)
{noformat}
There're 2 {{FutureTask}} in the call stack, for example {{FutureTask1(FutureTask2))}}, If the call thrown an exception, {{FutureTask2}} sets the status, save the exception and return. But FutureTask1 doesn't get any exception, then set the status to normal. So we're unable to get the exception in:
[CompactionManager.java:1491|https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1491]

2.1.x is working fine, here is the call stack:
{noformat}
    at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:177) ~[main/:na]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73) ~[main/:na]
    at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[main/:na]
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:264) ~[main/:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_141]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_141]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_141]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_141]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_141]
{noformat}",N/A,"2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13822,Add example of using cache option to documentation,"We should include an example of configuring cache in the docs.

{code}
CREATE TABLE simple (
id int,
key text,
value text,
primary key(key, value)
) WITH caching = {'keys': 'ALL', 'rows_per_partition': 10};
{code}",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13817,cassandra allow filtering bug,"I have one bug about cassandra cql, when I use  select * from table where name = 'myName' alllow filtering,  sometimes can be found, but sometimes can not found. I am very sure the row data existed. my data was very small, just 2000 rows.and only one node.   i use cassandra 3.10, ubuntu 14.",N/A,3.11.12
CASSANDRA-13813,Don't let user drop (or generally break) tables in system_distributed,"There is not currently no particular restrictions on schema modifications to tables of the {{system_distributed}} keyspace. This does mean you can drop those tables, or even alter them in wrong ways like dropping or renaming columns. All of which is guaranteed to break stuffs (that is, repair if you mess up with on of it's table, or MVs if you mess up with {{view_build_status}}).

I'm pretty sure this was never intended and is an oversight of the condition on {{ALTERABLE_SYSTEM_KEYSPACES}} in [ClientState|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/ClientState.java#L397]. That condition is such that any keyspace not listed in {{ALTERABLE_SYSTEM_KEYSPACES}} (which happens to be the case for {{system_distributed}}) has no specific restrictions whatsoever, while given the naming it's fair to assume the intention that exactly the opposite.",N/A,"3.0.16, 3.11.2"
CASSANDRA-13808,Integer overflows with Amazon Elastic File System (EFS),"Integer overflow issue was fixed for cassandra 2.2, but in 3.8 new property was introduced in config that also derives from disk size  {{cdc_total_space_in_mb}}, see CASSANDRA-8844

It should be updated too https://github.com/apache/cassandra/blob/6b7d73a49695c0ceb78bc7a003ace606a806c13a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L484",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13807,CircleCI fix - only collect the xml file from containers where it exists,"Followup from CASSANDRA-13775 - my fix with {{ant eclipse-warnings}} obviously does not work since it doesn't generate any xml files

Push a new fix here: https://github.com/krummas/cassandra/commits/marcuse/fix_circle_3.0 which only collects the xml file from the first 3 containers
Test running here: https://circleci.com/gh/krummas/cassandra/86",N/A,"2.1.19, 2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13801,CompactionManager sometimes wrongly determines that a background compaction is running for a particular table,"Sometimes after writing different rows to a table, then doing a blocking flush, if you alter the compaction strategy, then run background compaction and wait for it to finish, {{CompactionManager}} may decide that there's an ongoing compaction for that same table.
This may happen even though logs don't indicate that to be the case (compaction may still be running for system_schema tables).",N/A,"2.2.12, 3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13798,Disallow filtering on non-primary-key base column for MV,"We should probably consider disallow filtering conditions on non-primary-key base column for Materialized View which is introduced in CASSANDRA-10368.

The main problem is that the liveness of view row is now depending on multiple base columns (multiple filtered non-pk base column + base column used in view pk) and this semantic could not be properly support without drastic storage format changes. (SEE CASSANDRA-11500, [background|https://issues.apache.org/jira/browse/CASSANDRA-11500?focusedCommentId=16119823&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16119823])

We should step back and re-consider the non-primary-key filtering feature together with supporting multiple non-PK cols in MV clustering key in CASSANDRA-10226.
",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13797,RepairJob blocks on syncTasks,"The thread running {{RepairJob}} blocks while it waits for the validations it starts to complete ([see here|https://github.com/bdeggleston/cassandra/blob/9fdec0a82851f5c35cd21d02e8c4da8fc685edb2/src/java/org/apache/cassandra/repair/RepairJob.java#L185]). However, the downstream callbacks (ie: the post-repair cleanup stuff) aren't waiting for {{RepairJob#run}} to return, they're waiting for a result to be set on RepairJob the future, which happens after the sync tasks have completed. This post repair cleanup stuff also immediately shuts down the executor {{RepairJob#run}} is running in. So in noop repair sessions, where there's nothing to stream, I'm seeing the callbacks sometimes fire before {{RepairJob#run}} wakes up, and causing an {{InterruptedException}} is thrown.

I'm pretty sure this can just be removed, but I'd like a second opinion. This appears to just be a holdover from before repair coordination became async. I thought it might be doing some throttling by blocking, but each repair session gets it's own executor, and validation is  throttled by the fixed size executors doing the actual work of validation, so I don't think we need to keep this around.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13794,Fix short read protection logic for querying more rows,"Discovered by [~benedict] while reviewing CASSANDRA-13747:

{quote}
While reviewing I got a little suspicious of the modified line {{DataResolver}} :479, as it seemed that n and x were the wrong way around... and, reading the comment of intent directly above, and reproducing the calculation, they are indeed.

This is probably a significant enough bug that it warrants its own ticket for record keeping, though I'm fairly agnostic on that decision.

I'm a little concerned about our current short read behaviour, as right now it seems we should be requesting exactly one row, for any size of under-read, which could mean extremely poor performance in case of large under-reads.

I would suggest that the outer unconditional {{Math.max}} is a bad idea, has been (poorly) insulating us from this error, and that we should first be asserting that the calculation yields a value >= 0 before setting to 1.
{quote}",N/A,"3.0.15, 3.11.1"
CASSANDRA-13793,"Regression in 3.0, breaking the fix from CASSANDRA-6069","The goal of the fix of CASSANDRA-6069 was to make sure that collection tombstones in an update in CAS were using {{t-1}} because at least in {{INSERT}} collection tombstones are used to delete data prior to the update but shouldn't delete the newly inserted data itself. Because in 2.x the collection tombstones are normal range tombstones and thus part of the {{DeletionInfo}}, we went with the easy solution of using {{t-1}} for all of {{DeletionInfo}}.

When moving that code to 3.0, this was migrated too literally however and only the {{DeletionInfo}} got the {{t -1}}. But in 3.0, range tombstones are not part of {{DeletionInfo}} anymore, and so this is broken.

Thanks to [~aweisberg] for noticing this.",N/A,"3.0.15, 3.11.5"
CASSANDRA-13792,unable to connect apache-cassandra-3.11.0 after place 4.2.2 JNA jar ,"After apply work around below solution, installled the pache-cassandra-3.11 but i am not connect server status and cqlsh. please find attached log for more information  and let us know the instatllion completed successfully

""Jeff Jirsa resolved CASSANDRA-13791.
------------------------------------
    Resolution: Duplicate

This was caused (and resolved) by CASSANDRA-13072 , where JNA was upgraded to {{4.4.0}}. That 4.4.0 JNA library was linked against a different version of glibc ( {{GLIBC_2.14}} ), which causes this error. This is fixed in {{3.11.1}}, but you can work around the issue by downloading the 4.2.2 JNA jar from [here|https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_apache_cassandra_raw_00a777ec8ab701b843172e23a6cbdc4d6cf48f8d_lib_jna-2D4.2.2.jar&d=DwICaQ&c=eIGjsITfXP_y-DLLX0uEHXJvU8nOHrUK8IrwNKOtkVU&r=zSydaPm_PKES-H2mB46-9X-iWMDIIymvQCfY02eRW-Q&m=cSmoTlLQJTt32Gb257uuaz6_WHE6UeXOV5XjO7GTcxk&s=yieYJ2V_ND3saGBjGH3O-DEwQdfkA44B9yBqtjIU8bE&e= ] and placing it into the classpath ({{lib/}}), removing {{jna-4.4.0.jar}} in the process.


> unable to install apache-cassandra-3.11.0 in linux  box
> -------------------------------------------------------
""

[cassdb@alsc_dev_db bin]$ nodetool status
-bash: nodetool: command not found
[cassdb@alsc_dev_db bin]$ pwd
/u01/Cassandra_home/apache-cassandra-3.11.0/bin
[cassdb@alsc_dev_db bin]$ sh /u01/Cassandra_home/apache-cassandra-3.11.0/bin/cqlsh
No appropriate python interpreter found.
[cassdb@alsc_dev_db bin]$ service cassandra status
",N/A,3.11.0
CASSANDRA-13791,unable to install apache-cassandra-3.11.0 in linux  box,"While  installing the Cassandra in linux serverr,  I am getting below error . could you please look int it and provide suggestions. PFA atttached log for more information.

[cassdb@alsc_dev_db bin]$ sh /u01/Cassandra_home/apache-cassandra-3.11.0/bin/cassandra

Error:

ERROR [main] 2017-08-23 09:48:21,467 NativeLibraryLinux.java:62 - Failed to link the C library against JNA. Native methods will be unavailable.
java.lang.UnsatisfiedLinkError: /tmp/jna--1367560132/jna4859101025087222330.tmp: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by /tmp/jna--1367560132/jna4859101025087222330.tmp)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[na:1.8.0_71]
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1938) ~[na:1.8.0_71]
       at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1821) ~[na:1.8.0_71]
        at java.lang.Runtime.load0(Runtime.java:809) ~[na:1.8.0_71]
        at java.lang.System.load(System.java:1086) ~[na:1.8.0_71]
        at com.sun.jna.Native.loadNativeDispatchLibraryFromClasspath(Native.java:947) ~[jna-4.4.0.jar:4.4.0 (b0)]
        at com.sun.jna.Native.loadNativeDispatchLibrary(Native.java:922) ~[jna-4.4.0.jar:4.4.0 (b0)]
        at com.sun.jna.Native.<clinit>(Native.java:190) ~[jna-4.4.0.jar:4.4.0 (b0)]
        at org.apache.cassandra.utils.NativeLibraryLinux.<clinit>(NativeLibraryLinux.java:53) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.utils.NativeLibrary.<clinit>(NativeLibrary.java:93) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:196) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:600) [apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:689) [apache-cassandra-3.11.0.jar:3.11.0]
WARN  [main] 2017-08-23 09:48:21,468 StartupChecks.java:127 - jemalloc shared library could not be preloaded to speed up memory allocations
WARN  [main] 2017-08-23 09:48:21,469 StartupChecks.java:160 - JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.
ERROR [main] 2017-08-23 09:48:21,470 CassandraDaemon.java:706 - The native library could not be initialized properly
",N/A,3.11.0
CASSANDRA-13787,RangeTombstoneMarker and PartitionDeletion is not properly included in MV,"Found two problems related to MV tombstone. 

1. Range-tombstone-Marker being ignored after shadowing first row, subsequent base rows are not shadowed in TableViews.

    If the range tombstone was not flushed, it was used as deleted row to shadow new updates. It works correctly.
    After range tombstone was flushed, it was used as RangeTombstoneMarker and being skipped after shadowing first update. The bound of RangeTombstoneMarker seems wrong, it contained full clustering, but it should contain range or it should be multiple RangeTombstoneMarkers for multiple slices(aka. new updates)

-2. Partition tombstone is not used when no existing live data, it will resurrect deleted cells. It was found in 11500 and included in that patch.- (Merged in CASSANDRA-11500)


In order not to make 11500 patch more complicated, I will try fix range/partition tombstone issue here.


{code:title=Tests to reproduce}
    @Test
    public void testExistingRangeTombstoneWithFlush() throws Throwable
    {
        testExistingRangeTombstone(true);
    }

    @Test
    public void testExistingRangeTombstoneWithoutFlush() throws Throwable
    {
        testExistingRangeTombstone(false);
    }

    public void testExistingRangeTombstone(boolean flush) throws Throwable
    {
        createTable(""CREATE TABLE %s (k1 int, c1 int, c2 int, v1 int, v2 int, PRIMARY KEY (k1, c1, c2))"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""view1"",
                   ""CREATE MATERIALIZED VIEW view1 AS SELECT * FROM %%s WHERE k1 IS NOT NULL AND c1 IS NOT NULL AND c2 IS NOT NULL PRIMARY KEY (k1, c2, c1)"");

        updateView(""DELETE FROM %s USING TIMESTAMP 10 WHERE k1 = 1 and c1=1"");


        if (flush)
            Keyspace.open(keyspace()).getColumnFamilyStore(currentTable()).forceBlockingFlush();

        String table = KEYSPACE + ""."" + currentTable();
        updateView(""BEGIN BATCH "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 0, 0, 0, 0) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 0, 1, 0, 1) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 0, 1, 0) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 1, 1, 1) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 2, 1, 2) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 1, 3, 1, 3) USING TIMESTAMP 5; "" +
                ""INSERT INTO "" + table + "" (k1, c1, c2, v1, v2) VALUES (1, 2, 0, 2, 0) USING TIMESTAMP 5; "" +
                ""APPLY BATCH"");

        assertRowsIgnoringOrder(execute(""select * from %s""),
                                row(1, 0, 0, 0, 0),
                                row(1, 0, 1, 0, 1),
                                row(1, 2, 0, 2, 0));
        assertRowsIgnoringOrder(execute(""select k1,c1,c2,v1,v2 from view1""),
                                row(1, 0, 0, 0, 0),
                                row(1, 0, 1, 0, 1),
                                row(1, 2, 0, 2, 0));
    }

    @Test
    public void testExistingParitionDeletionWithFlush() throws Throwable
    {
        testExistingParitionDeletion(true);
    }

    @Test
    public void testExistingParitionDeletionWithoutFlush() throws Throwable
    {
        testExistingParitionDeletion(false);
    }

    public void testExistingParitionDeletion(boolean flush) throws Throwable
    {
        // for partition range deletion, need to know that existing row is shadowed instead of not existed.
        createTable(""CREATE TABLE %s (a int, b int, c int, d int, PRIMARY KEY (a))"");

        execute(""USE "" + keyspace());
        executeNet(protocolVersion, ""USE "" + keyspace());

        createView(""mv_test1"",
                   ""CREATE MATERIALIZED VIEW %s AS SELECT * FROM %%s WHERE a IS NOT NULL AND b IS NOT NULL PRIMARY KEY (a, b)"");

        Keyspace ks = Keyspace.open(keyspace());
        ks.getColumnFamilyStore(""mv_test1"").disableAutoCompaction();

        execute(""INSERT INTO %s (a, b, c, d) VALUES (?, ?, ?, ?) using timestamp 0"", 1, 1, 1, 1);
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""), row(1, 1, 1, 1));

        // remove view row
        updateView(""UPDATE %s using timestamp 1 set b = null WHERE a=1"");
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""));
        // remove base row, no view updated generated.
        updateView(""DELETE FROM %s using timestamp 2 where a=1"");
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""));

        // restor view row with b,c column. d is still tombstone
        updateView(""UPDATE %s using timestamp 3 set b = 1,c = 1 where a=1""); // upsert
        if (flush)
            FBUtilities.waitOnFutures(ks.flush());

        assertRowsIgnoringOrder(execute(""SELECT * FROM mv_test1""), row(1, 1, 1, null));
    }
{code}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13782,Cassandra RPM has wrong owner for /usr/share directories,"Some Cassandra RPM directories are owned by cassandra user against the fedora package guidelines.

Offending lines: https://github.com/apache/cassandra/blob/trunk/redhat/cassandra.spec#L135-L136

""Permissions on files MUST be set properly. Inside of /usr, files should be owned by root:root unless a more specific user or group is needed for security.""
- https://fedoraproject.org/wiki/Packaging:Guidelines?rd=Packaging/Guidelines#File_Permissions",N/A,"2.1.19, 2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13776,Adding a field to an UDT can corrupte the tables using it,"Adding a field to an UDT which is used as a {{Set}} element or as a {{Map}} element can corrupt the table.
The problem can be reproduced using the following test case:
{code}
    @Test
    public void testReadAfterAlteringUserTypeNestedWithinSet() throws Throwable
    {
        String ut1 = createType(""CREATE TYPE %s (a int)"");
        String columnType = KEYSPACE + ""."" + ut1;

        try
        {
            createTable(""CREATE TABLE %s (x int PRIMARY KEY, y set<frozen<"" + columnType + "">>)"");
            disableCompaction();

            execute(""INSERT INTO %s (x, y) VALUES(1, ?)"", set(userType(1), userType(2)));
            assertRows(execute(""SELECT * FROM %s""), row(1, set(userType(1), userType(2))));
            flush();

            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                       row(1, set(userType(1), userType(2))));

            execute(""ALTER TYPE "" + KEYSPACE + ""."" + ut1 + "" ADD b int"");
            execute(""UPDATE %s SET y = y + ? WHERE x = 1"",
                    set(userType(1, 1), userType(1, 2), userType(2, 1)));

            flush();
            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                           row(1, set(userType(1),
                                      userType(1, 1),
                                      userType(1, 2),
                                      userType(2),
                                      userType(2, 1))));

            compact();

            assertRows(execute(""SELECT * FROM %s WHERE x = 1""),
                       row(1, set(userType(1),
                                  userType(1, 1),
                                  userType(1, 2),
                                  userType(2),
                                  userType(2, 1))));
        }
        finally
        {
            enableCompaction();
        }
    }
{code} 

There are in fact 2 problems:
# When the {{sets}} from the 2 versions are merged the {{ColumnDefinition}} being picked up can be the older one. In which case when the tuples are sorted it my lead to an {{IndexOutOfBoundsException}}.
# During compaction, the old column definition can be the one being kept for the SSTable metadata. If it is the case the SSTable will not be readable any more and will be marked as {{corrupted}}.

If one of the tables using the type has a Materialized View attached to it, the MV updates can also fail with {{IndexOutOfBoundsException}}.

This problem can be reproduced using the following test:
{code}
    @Test
    public void testAlteringUserTypeNestedWithinSetWithView() throws Throwable
    {
        String columnType = typeWithKs(createType(""CREATE TYPE %s (a int)""));

        createTable(""CREATE TABLE %s (pk int, c int, v int, s set<frozen<"" + columnType + "">>, PRIMARY KEY (pk, c))"");
        execute(""CREATE MATERIALIZED VIEW "" + keyspace() + "".view1 AS SELECT c, pk, v FROM %s WHERE pk IS NOT NULL AND c IS NOT NULL AND v IS NOT NULL PRIMARY KEY (c, pk)"");

        execute(""INSERT INTO %s (pk, c, v, s) VALUES(?, ?, ?, ?)"", 1, 1, 1, set(userType(1), userType(2)));
        flush();

        execute(""ALTER TYPE "" + columnType + "" ADD b int"");
        execute(""UPDATE %s SET s = s + ?, v = ? WHERE pk = ? AND c = ?"",
                set(userType(1, 1), userType(1, 2), userType(2, 1)), 2, 1, 1);


        assertRows(execute(""SELECT * FROM %s WHERE pk = ? AND c = ?"", 1, 1),
                       row(1, 1, 2, set(userType(1),
                                        userType(1, 1),
                                        userType(1, 2),
                                        userType(2),
                                        userType(2, 1))));
    }
{code}      ",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13775,CircleCI tests fail because *stress-test* isn't a valid target,"*stress-test* was added to CircleCI in CASSANDRA-13413 (2.1+) but the target itself got introduced in CASSANDRA-11638 (3.10).
",N/A,"2.1.19, 2.2.11, 3.0.15"
CASSANDRA-13773,cassandra-stress writes even data when n=0,"This is very unintuitive as
{code}
cassandra-stress write n=0 -rate threads=1
{code}
will do inserts even with *n=0*. I guess most people won't ever run with *n=0* but this is a nice shortcut for creating some schema without using *cqlsh*

This is happening because we're writing *50k* rows of warmup data as can be seen below:
{code}
cqlsh> select count(*) from keyspace1.standard1 ;

 count
-------
 50000

(1 rows)
{code}

We can avoid writing warmup data using 
{code}
cassandra-stress write n=0 no-warmup -rate threads=1
{code}

but I would still expect to have *0* rows written when specifying *n=0*.

",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13772,Add a skip read validation flag to cassandra-stress,"When running cassandra-stress with read operations, you must make sure all the data was populated beforehand or else you will get the following errors:
java.io.IOException: Operation x0 on key(s) [4d31314e32314b395030]: Data returned was not validated

	at org.apache.cassandra.stress.Operation.error(Operation.java:127)
	at org.apache.cassandra.stress.Operation.timeWithRetry(Operation.java:105)
	at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:91)
	at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:99)
	at org.apache.cassandra.stress.operations.predefined.CqlOperation.run(CqlOperation.java:245)
	at org.apache.cassandra.stress.StressAction$Consumer.run(StressAction.java:453)
java.lang.RuntimeException: Failed to execute warmup
	at org.apache.cassandra.stress.StressAction.warmup(StressAction.java:117)
	at org.apache.cassandra.stress.StressAction.run(StressAction.java:62)
	at org.apache.cassandra.stress.Stress.run(Stress.java:143)
	at org.apache.cassandra.stress.Stress.main(Stress.java:62)

Even if you use the ""-errors ignore"" flag you'll get a lot of the following messages which will both slow the stress and prevent it from displaying the metrics:
Operation x0 on key(s) [4b3539393831374e3431]: Data returned was not validated
Operation x0 on key(s) [4f4b3936363233375030]: Data returned was not validated
Operation x0 on key(s) [4d304c4b32384f4b3031]: Data returned was not validated

What I propose is to add a flag to skip the read validation, such as:
-errors skip-read-validations

This way when needed you can run a mixed workload and ignore validation of unpopulated data.",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13767,update a row which was inserted with 'IF NOT EXISTS' key word will fail siently,"First, create keyspace and a table using the following

{code:java}
CREATE KEYSPACE scheduler WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE scheduler.job_info (
    id timeuuid PRIMARY KEY,
    create_time int,
    cur_retry int,
    cur_run_times int,
    expire_time int,
    max_retry int,
    max_run_times int,
    payload text,
    period int,
    retry_interval int,
    status tinyint,
    topic text,
    type text,
    update_time int
) with caching = {'keys':'ALL', 'rows_per_partition':'NONE'};
{code}


then, execute the following cql:


{code:java}
insert into job_info (id, create_time) values (5be224c6-8231-11e7-9619-9801b2a97471, 0) IF NOT EXISTS;
insert into job_info (id, create_time) values (5be224c6-8231-11e7-9619-9801b2a97471, 1);
select * from job_info;
{code}

You will find that create_time is still 0, it is not updated.

but, if you remove the IF NOT EXISTS keyword in the first cql, the update will success.
",N/A,3.11.9
CASSANDRA-13764,SelectTest.testMixedTTLOnColumnsWide is flaky,"{{org.apache.cassandra.cql3.validation.operations.SelectTest.testMixedTTLOnColumnsWide}} is flaky. This is because it inserts rows and then asserts their contents using {{ttl()}} in the select, but if the test is sufficiently slow, the remaining ttl may change by the time the select is run. Anecdotally, {{testSelectWithAlias}} in the same class uses a fudge factor of 1 second that would fix all the failures I've seen, but it might make more sense to measure the elapsed time in the test and calculate the acceptable variation from that time.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13756,StreamingHistogram is not thread safe,"When we test C*3 in shadow cluster, we notice after a period of time, several data node suddenly run into 100% cpu and stop process query anymore.

After investigation, we found that threads are stuck on the sum() in streaminghistogram class. Those are jmx threads that working on expose getTombStoneRatio metrics (since jmx is kicked off every 3 seconds, there is a chance that multiple jmx thread is access streaminghistogram at the same time).  

After further investigation, we find that the optimization in CASSANDRA-13038 led to a spool flush every time when we call sum(). Since TreeMap is not thread safe, threads will be stuck when multiple threads visit sum() at the same time.

There are two approaches to solve this issue. 

The first one is to add a lock to the flush in sum() which will introduce some extra overhead to streaminghistogram.

The second one is to avoid streaminghistogram to be access by multiple threads. For our specific case, is to remove the metrics we added.  ",N/A,"3.0.15, 3.11.1"
CASSANDRA-13754,BTree.Builder memory leak,"After a chronic bout of {{OutOfMemoryError}} in our development environment, a heap analysis is showing that more than 10G of our 12G heaps are consumed by the {{threadLocals}} members (instances of {{java.lang.ThreadLocalMap}}) of various {{io.netty.util.concurrent.FastThreadLocalThread}} instances.  Reverting [cecbe17|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=cecbe17e3eafc052acc13950494f7dddf026aa54] fixes the issue.",N/A,3.11.1
CASSANDRA-13752,Corrupted SSTables created in 3.11,"We have discovered issues with corrupted SSTables. 

{code}
ERROR [SSTableBatchOpen:22] 2017-08-03 20:19:53,195 SSTableReader.java:577 - Cannot read sstable /cassandra/data/mykeyspace/mytable-7a4992800d5611e7b782cb90016f2d17/mc-35556-big=[Data.db, Statistics.db, Summary.db, Digest.crc32, CompressionInfo.db, TOC.txt, Index.db, Filter.db]; other IO error, skipping table
java.io.EOFException: EOF after 1898 bytes out of 21093
        at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:68) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.util.RebufferingInputStream.readFully(RebufferingInputStream.java:60) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:402) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:377) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.metadata.StatsMetadata$StatsMetadataSerializer.deserialize(StatsMetadata.java:325) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.metadata.StatsMetadata$StatsMetadataSerializer.deserialize(StatsMetadata.java:231) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:122) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:93) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:488) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:396) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at org.apache.cassandra.io.sstable.format.SSTableReader$5.run(SSTableReader.java:561) ~[apache-cassandra-3.11.0.jar:3.11.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_111]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_111]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.0.jar:3.11.0]
{code}

Files look like this:
{code}
-rw-r--r--. 1 cassandra cassandra     3899251 Aug  7 08:37 mc-6166-big-CompressionInfo.db
-rw-r--r--. 1 cassandra cassandra 16874421686 Aug  7 08:37 mc-6166-big-Data.db
-rw-r--r--. 1 cassandra cassandra          10 Aug  7 08:37 mc-6166-big-Digest.crc32
-rw-r--r--. 1 cassandra cassandra     2930904 Aug  7 08:37 mc-6166-big-Filter.db
-rw-r--r--. 1 cassandra cassandra   111175880 Aug  7 08:37 mc-6166-big-Index.db
-rw-r--r--. 1 cassandra cassandra       13762 Aug  7 08:37 mc-6166-big-Statistics.db
-rw-r--r--. 1 cassandra cassandra      882008 Aug  7 08:37 mc-6166-big-Summary.db
-rw-r--r--. 1 cassandra cassandra          92 Aug  7 08:37 mc-6166-big-TOC.txt
{code}",N/A,3.11.1
CASSANDRA-13750,Counter digests include local data,"In 3.x+, the raw counter value bytes are used when hashing counters for reads and repair, including local shard data, which is removed when streamed. This leads to constant digest mismatches and repair overstreaming.",N/A,"3.0.15, 3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-13747,Fix AssertionError in short read protection,"{{ShortReadRowProtection.moreContents()}} expects that by the time we get to that method, the global post-reconciliation counter was already applied to the current partition. However, sometimes it won’t get applied, and the global counter continues counting with {{rowInCurrentPartition}} value not reset from previous partition, which in the most obvious case would trigger the assertion we are observing - {{assert !postReconciliationCounter.isDoneForPartition();}}. In other cases it’s possible because of this lack of reset to query a node for too few extra rows, causing unnecessary SRP data requests.

Why is the counter not always applied to the current partition?

The merged {{PartitionIterator}} returned from {{DataResolver.resolve()}} has two transformations applied to it, in the following order:
{{Filter}} - to purge non-live data from partitions, and to discard empty partitions altogether (except for Thrift)
{{Counter}}, to count and stop iteration

Problem is, {{Filter}} ’s {{applyToPartition()}} code that discards empty partitions ({{closeIfEmpty()}} method) would sometimes consume the iterator, triggering short read protection *before* {{Counter}} ’s {{applyToPartition()}} gets called and resets its {{rowInCurrentPartition}} sub-counter.

We should not be consuming iterators until all transformations are applied to them. For transformations it means that they cannot consume iterators unless they are the last transformation on the stack.

The linked branch fixes the problem by splitting {{Filter}} into two transformations. The original - {{Filter}} - that does filtering within partitions - and a separate {{EmptyPartitionsDiscarder}}, that discards empty partitions from {{PartitionIterators}}. Thus {{DataResolve.resolve()}}, when constructing its {{PartitionIterator}}, now does merge first, then applies {{Filter}}, then {{Counter}}, and only then, as its last (third) transformation - the {{EmptyPartitionsDiscarder}}. Being the last one applied, it’s legal for it to consume the iterator, and triggering {{moreContents()}} is now no longer a problem.

Fixes: [3.0|https://github.com/iamaleksey/cassandra/commits/13747-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/13747-3.11], [4.0|https://github.com/iamaleksey/cassandra/commits/13747-4.0]. dtest [here|https://github.com/iamaleksey/cassandra-dtest/commits/13747].",N/A,"3.0.15, 3.11.1"
CASSANDRA-13744,Better bootstrap failure message when blocked by (potential) range movement,"The UnsupportedOperationException thrown from {{StorageService.joinTokenRing(..)}} when it's detected that other nodes are bootstrapping|leaving|moving offers no information as to which are those other nodes.

In a large cluster this might not be obvious nor easy to discover, gossipinfo can hold information that takes a bit of effort to uncover. Even when it is easily seen it's helpful to have it confirmed.

Attached is the patch that provides a more thorough exception message to the failed bootstrap attempt.

",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13740,Orphan hint file gets created while node is being removed from cluster,"I have found this new issue during my test, whenever node is being removed then hint file for that node gets written and stays inside the hint directory forever. I debugged the code and found that it is due to the race condition between [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L195] and [HintsWriteExecutor.java::closeWriter | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L106]
. 
 
*Time t1* Node is down, as a result Hints are being written by [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L195]
*Time t2* Node is removed from cluster as a result it calls [HintsService.java-exciseStore | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L327] which removes hint files for the node being removed
*Time t3* Mutation stage keeps pumping Hints through [HintService.java::write | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L145] which again calls [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L215] and new orphan file gets created

I was writing a new dtest for {CASSANDRA-13562, CASSANDRA-13308} and that helped me reproduce this new bug. I will submit patch for this new dtest later.

I also tried following to check how this orphan hint file responds:
1. I tried {{nodetool truncatehints <node>}} but it fails as node is no longer part of the ring
2. I then tried {{nodetool truncatehints}}, that still doesn’t remove hint file because it is not yet included in the [dispatchDequeue | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsStore.java#L53]


Reproducible steps:
Please find dTest python file {{gossip_hang_test.py}} attached which reproduces this bug.

Solution:
This is due to race condition as mentioned above. Since {{HintsWriteExecutor.java}} creates thread pool with only 1 worker, so solution becomes little simple. Whenever we [HintService.java::excise | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsService.java#L303] a host, just store it in-memory, and check for already evicted host inside [HintsWriteExecutor.java::flush | https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/hints/HintsWriteExecutor.java#L215]. If already evicted host is found then ignore hints.

Jaydeep",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13739,Cassandra can't start because of unknown type <usertype> exception,"OS: CentOS Linux release 7.2.1511 (Core)
Kernel: 3.10.0-327.36.1.el7.x86_64
Cassandra: v 3.7

Issue:

Cassandra is not able to start after restart of the node with following error in system.log:

ERROR [main] 2017-08-03 12:10:28,633 CassandraDaemon.java:731 - Exception encountered during startup
org.apache.cassandra.exceptions.InvalidRequestException: Unknown type <name of user type>
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawUT.prepare(CQL3Type.java:751) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepare(CQL3Type.java:667) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.cql3.CQL3Type$Raw$RawCollection.prepareInternal(CQL3Type.java:644) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.lambda$prepare$34(Types.java:313) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[na:1.8.0_102]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[na:1.8.0_102]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[na:1.8.0_102]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[na:1.8.0_102]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[na:1.8.0_102]
        at org.apache.cassandra.schema.Types$RawBuilder$RawUDT.prepare(Types.java:314) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.Types$RawBuilder.build(Types.java:263) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchTypes(SchemaKeyspace.java:920) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspace(SchemaKeyspace.java:891) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchKeyspacesWithout(SchemaKeyspace.java:869) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.schema.SchemaKeyspace.fetchNonSystemKeyspaces(SchemaKeyspace.java:857) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:136) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.config.Schema.loadFromDisk(Schema.java:126) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:251) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:585) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:714) [apache-cassandra-3.7.0.jar:3.7.0]

Thank you for your help!",N/A,3.7
CASSANDRA-13738,Load is over calculated after each IndexSummaryRedistribution," For example, here is one of our cluster with about 500GB per node, but {{nodetool status}} shows far more load than it actually is and keeps increasing, restarting the process will reset the load, but keeps increasing afterwards:
{noformat}
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  IP1*****       13.52 TB   256          100.0%            c4c31e0a-3f01-49f7-8a22-33043737975d  rac1
UN  IP2*****       14.25 TB   256          100.0%            efec4980-ec9e-4424-8a21-ce7ddaf80aa0  rac1
UN  IP3*****       13.52 TB   256          100.0%            7dbcfdfc-9c07-4b1a-a4b9-970b715ebed8  rac1
UN  IP4*****       22.13 TB   256          100.0%            8879e6c4-93e3-4cc5-b957-f999c6b9b563  rac1
UN  IP5*****       18.02 TB   256          100.0%            4a1eaf22-4a83-4736-9e1c-12f898d685fa  rac1
UN  IP6*****       11.68 TB   256          100.0%            d633c591-28af-42cc-bc5e-47d1c8bcf50f  rac1
{noformat}

!sizeIssue.png|test!

The root cause is if the SSTable index summary is redistributed (typically executes hourly), the updated SSTable size is added again.",N/A,"2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13737,Node start can fail if the base table of a materialized view is not found,"Node start can fail if the base table of a materialized view is not found, which is something that can happen under certain circumstances. There is a dtest reproducing the problem:
{code}
cluster = self.cluster
cluster.populate(3)
cluster.start()
node1, node2, node3 = self.cluster.nodelist()
session = self.patient_cql_connection(node1, consistency_level=ConsistencyLevel.QUORUM)
create_ks(session, 'ks', 3)

session.execute('CREATE TABLE users (username varchar PRIMARY KEY, state varchar)')

node3.stop(wait_other_notice=True)

# create a materialized view only in nodes 1 and 2
session.execute(('CREATE MATERIALIZED VIEW users_by_state AS '
                 'SELECT * FROM users WHERE state IS NOT NULL AND username IS NOT NULL '
                 'PRIMARY KEY (state, username)'))

node1.stop(wait_other_notice=True)
node2.stop(wait_other_notice=True)

# drop the base table only in node 3
node3.start(wait_for_binary_proto=True)
session = self.patient_cql_connection(node3, consistency_level=ConsistencyLevel.QUORUM)
session.execute('DROP TABLE ks.users')

cluster.stop()
cluster.start()  # Fails
{code}
This is the error during node start:
{code}
java.lang.IllegalArgumentException: Unknown CF 958ebc30-76e4-11e7-869a-9d8367a71c76
	at org.apache.cassandra.db.Keyspace.getColumnFamilyStore(Keyspace.java:215) ~[main/:na]
	at org.apache.cassandra.db.view.ViewManager.addView(ViewManager.java:143) ~[main/:na]
	at org.apache.cassandra.db.view.ViewManager.reload(ViewManager.java:113) ~[main/:na]
	at org.apache.cassandra.schema.Schema.alterKeyspace(Schema.java:618) ~[main/:na]
	at org.apache.cassandra.schema.Schema.lambda$merge$18(Schema.java:591) ~[main/:na]
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.lambda$entryConsumer$0(Collections.java:1575) ~[na:1.8.0_131]
	at java.util.HashMap$EntrySet.forEach(HashMap.java:1043) ~[na:1.8.0_131]
	at java.util.Collections$UnmodifiableMap$UnmodifiableEntrySet.forEach(Collections.java:1580) ~[na:1.8.0_131]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:591) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationTask$1.response(MigrationTask.java:89) ~[main/:na]
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[main/:na]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [main/:na]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
{code}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13730,Dropping a table doesn't drop its dropped columns,"I'm not sure if this is intended or not, but currently a table's dropped columns are not dropped when the table itself is dropped:

{noformat}
cqlsh> create keyspace ks WITH replication={ 'class' : 'SimpleStrategy', 'replication_factor' : 1 } ;
cqlsh> use ks;
cqlsh:ks> create table  test (pk text primary key, c1 int);
cqlsh:ks> alter table test drop c1;
cqlsh:ks> drop table test;
cqlsh:ks> select * from system_schema.dropped_columns where keyspace_name = 'ks' and table_name = 'test';

 keyspace_name | table_name | column_name | dropped_time                    | kind    | type
---------------+------------+-------------+---------------------------------+---------+------
            ks |       test |          c1 | 2017-07-25 17:53:47.651000+0000 | regular |  int

(1 rows)
{noformat}

This can have surprising consequences when creating another table with the same name.",N/A,"3.0.15, 3.11.1"
CASSANDRA-13722,"""Number of keys (estimate)"" in the output of the `nodetool tablestats` should be ""Number of parititions (estimate)""","The output of the nodetool tablestats command is ambiguous in regards to the number of keys/partitions. It shows:

{code}
Number of keys (estimate): 6
{code}

And based on that it is not clear if it is number of primary keys (rows) or number of partition keys.

The fix is trivial.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13721,"""ignore"" option is ignored in sstableloader","If ignore option is set on the command line sstableloader still streams to the nodes excluded.

I believe the issue is in the [https://github.com/apache/cassandra/blob/dfb90b1458ac6ee427f9e329b45c764a3a0a0c06/src/java/org/apache/cassandra/tools/LoaderOptions.java] - the LoaderOptions constructor does not set the ""ignores"" field from the the ""builder.ignores""",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13719,Potential AssertionError during ReadRepair of range tombstone and partition deletions,"When reconciling range tombstones for read repair in {{DataResolver.RepairMergeListener.MergeListener}}, when we check if there is ongoing deletion repair for a source, we don't look for partition level deletions which throw off the logic and can throw an AssertionError.",N/A,"3.0.15, 3.11.1"
CASSANDRA-13717,INSERT statement fails when Tuple type is used as clustering column with default DESC order,"When a column family is created and a Tuple is used on clustering column with default clustering order DESC, then the INSERT statement fails. 

For example, the following table will make the INSERT statement fail with error message ""Invalid tuple type literal for tdemo of type frozen<tuple<timestamp, text>>"" , although the INSERT statement is correct (works as expected when the default order is ASC)

{noformat}
create table test_table (
	id int,
	tdemo tuple<timestamp, varchar>,
	primary key (id, tdemo)
) with clustering order by (tdemo desc);
{noformat}
",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13711,Invalid writetime for null columns in cqlsh,"From the user list:

https://lists.apache.org/thread.html/448731c029eee72e499fc6acd44d257d1671193f850a68521c2c6681@%3Cuser.cassandra.apache.org%3E

{code}
(oss-ccm) MacBook-Pro:~ jjirsa$ ccm create test -n 1 -s -v 3.0.10
Current cluster is now: test
(oss-ccm) MacBook-Pro:~ jjirsa$ ccm node1 cqlsh
Connected to test at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.10 | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor': 1};
cqlsh> CREATE TABLE test.t ( a text primary key, b text );
cqlsh> insert into test.t(a) values('z');
cqlsh> insert into test.t(a) values('w');
cqlsh> insert into test.t(a) values('e');
cqlsh> insert into test.t(a) values('r');
cqlsh> insert into test.t(a) values('t');
cqlsh> select a,b, writetime (b) from test.t;

a | b | writetime(b)
---+------+--------------
z | null | null
e | null | null
r | null | null
w | null | null
t | null | null

(5 rows)
cqlsh>
cqlsh> insert into test.t(a,b) values('t','x');
cqlsh> insert into test.t(a) values('b');
cqlsh> select a,b, writetime (b) from test.t;

 a | b    | writetime(b)
---+------+------------------
 z | null |             null
 e | null |             null
 r | null |             null
 w | null |             null
 t |    x | 1500565131354883
 b | null | 1500565131354883

(6 rows)
{code}

Data on disk:

{code}
MacBook-Pro:~ jjirsa$ ~/.ccm/repository/3.0.14/tools/bin/sstabledump /Users/jjirsa/.ccm/test/node1/data0/test/t-bed196006d0511e7904be9daad294861/mc-1-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""z"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 20,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:41:54.818118Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""e"" ],
      ""position"" : 21
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 44,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:42:04.288547Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""r"" ],
      ""position"" : 45
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 68,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:42:08.991417Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""w"" ],
      ""position"" : 69
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 92,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T04:41:59.005382Z"" },
        ""cells"" : [ ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""t"" ],
      ""position"" : 93
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 120,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T15:38:51.354883Z"" },
        ""cells"" : [
          { ""name"" : ""b"", ""value"" : ""x"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""b"" ],
      ""position"" : 121
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 146,
        ""liveness_info"" : { ""tstamp"" : ""2017-07-20T15:39:03.631297Z"" },
        ""cells"" : [ ]
      }
    ]
  }
]MacBook-Pro:~ jjirsa$
{code}
",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13700,Heartbeats can cause gossip information to go permanently missing on certain nodes,"In {{Gossiper.getStateForVersionBiggerThan}}, we add the {{HeartBeatState}} from the corresponding {{EndpointState}} to the {{EndpointState}} to send. When we're getting state for ourselves, this means that we add a reference to the local {{HeartBeatState}}. Then, once we've built a message (in either the Syn or Ack handler), we send it through the {{MessagingService}}. In the case that the {{MessagingService}} is sufficiently slow, the {{GossipTask}} may run before serialization of the Syn or Ack. This means that when the {{GossipTask}} acquires the gossip {{taskLock}}, it may increment the {{HeartBeatState}} version of the local node as stored in the endpoint state map. Then, when we finally serialize the Syn or Ack, we'll follow the reference to the {{HeartBeatState}} and serialize it with a higher version than we saw when constructing the Ack or Ack2.

Consider the case where we see {{HeartBeatState}} with version 4 when constructing an Ack and send it through the {{MessagingService}}. Then, we add some piece of state with version 5 to our local {{EndpointState}}. If {{GossipTask}} runs and increases the {{HeartBeatState}} version to 6 before the {{MessageOut}} containing the Ack is serialized, the node receiving the Ack will believe it is current to version 6, despite the fact that it has never received a message containing the {{ApplicationState}} tagged with version 5.

I've reproduced in this in several versions; so far, I believe this is possible in all versions.",N/A,"2.1.19, 2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13698,Reinstate or get rid of unit tests with multiple compaction strategies,"At some point there were (anti-)compaction tests with multiple compaction strategy classes, but now it's only tested with {{STCS}}:
* [AnticompactionTest|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/test/unit/org/apache/cassandra/db/compaction/AntiCompactionTest.java#L247]
* [CompactionsTest|https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java#L85]

We should either reinstate these tests or decide they are not important and remove the unused parameter.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13697,CDC and VIEW writeType missing from spec for write_timeout / write_failure,"In cassandra 3.0 a new {{WriteType}} {{VIEW}} was added which appears to be used when raising a {{WriteTimeoutException}} when the local view lock for a key cannot be acquired within timeout.

In cassandra 3.8 {{CDC}} {{WriteType}} was added for when {{cdc_total_space_in_mb}} is exceeded when doing a write to data tracked by cdc.

The [v4 spec|https://github.com/apache/cassandra/blob/cassandra-3.11.0/doc/native_protocol_v4.spec#L1051-L1066] currently doesn't cover these two write types.   While the protocol allows for a free form string for write type, it would be nice to document that types are available since some drivers (java, cpp, python) attempt to deserialize write type into an enum and may not handle it well.",N/A,"3.0.17, 3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13696,Digest mismatch Exception if hints file has UnknownColumnFamily,"{noformat}
WARN  [HintsDispatcher:2] 2017-07-16 22:00:32,579 HintsReader.java:235 - Failed to read a hint for /127.0.0.2: a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0 - table with id 3882bbb0-6a71-11e7-9bca-2759083e3964 is unknown in file a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0-1500242103097-1.hints
ERROR [HintsDispatcher:2] 2017-07-16 22:00:32,580 HintsDispatchExecutor.java:234 - Failed to dispatch hints file a2b7daf1-a6a4-4dfc-89de-32d12d2d48b0-1500242103097-1.hints: file is corrupted ({})
org.apache.cassandra.io.FSReadError: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:199) ~[main/:na]
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:164) ~[main/:na]
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.sendHints(HintsDispatcher.java:157) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.sendHintsAndAwait(HintsDispatcher.java:139) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:123) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatcher.dispatch(HintsDispatcher.java:95) ~[main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.deliver(HintsDispatchExecutor.java:268) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:251) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.dispatch(HintsDispatchExecutor.java:229) [main/:na]
    at org.apache.cassandra.hints.HintsDispatchExecutor$DispatchHintsTask.run(HintsDispatchExecutor.java:208) [main/:na]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_111]
    at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_111]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_111]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_111]
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_111]
Caused by: java.io.IOException: Digest mismatch exception
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNextInternal(HintsReader.java:216) ~[main/:na]
    at org.apache.cassandra.hints.HintsReader$HintsIterator.computeNext(HintsReader.java:190) ~[main/:na]
    ... 16 common frames omitted
{noformat}

It causes multiple cassandra nodes stop [by default|https://github.com/apache/cassandra/blob/cassandra-3.0/conf/cassandra.yaml#L188].

Here is the reproduce steps on a 3 nodes cluster, RF=3:
1. stop node1
2. send some data with quorum (or one), it will generate hints file on node2/node3
3. drop the table
4. start node1

node2/node3 will report ""corrupted hints file"" and stop. The impact is very bad for a large cluster, when it happens, almost all the nodes are down at the same time and we have to remove all the hints files (which contain the dropped table) to bring the node back.
",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13694,sstabledump does not show full precision of timestamp columns,"Create a table:

CREATE TABLE test_table (
    unit_no bigint,
    event_code text,
    active_time timestamp,
    ack_time timestamp,
    PRIMARY KEY ((unit_no, event_code), active_time)
) WITH CLUSTERING ORDER BY (active_time DESC)

Insert a row:

INSERT INTO test_table (unit_no, event_code, active_time, ack_time)
                  VALUES (1234, 'TEST EVENT', toTimestamp(now()), toTimestamp(now()));

Verify that it is in the database with a full timestamp:

cqlsh:pentaho> select * from test_table;

 unit_no | event_code | active_time                     | ack_time
---------+------------+---------------------------------+---------------------------------
    1234 | TEST EVENT | 2017-07-14 14:52:39.919000+0000 | 2017-07-14 14:52:39.919000+0000

(1 rows)


Write file:

nodetool flush
nodetool compact pentaho

Use sstabledump:

treeves@ubuntu:~$ sstabledump /var/lib/cassandra/data/pentaho/test_table-99ba228068a311e7ac30953b79ac2c3e/mb-2-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""1234"", ""TEST EVENT"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 38,
        ""clustering"" : [ ""2017-07-14 15:52+0100"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-07-14T14:52:39.888701Z"" },
        ""cells"" : [
          { ""name"" : ""ack_time"", ""value"" : ""2017-07-14 15:52+0100"" }
        ]
      }
    ]
  }
]

treeves@ubuntu:~$ 

The timestamp in the cluster key, and the regular column, are both truncated to the minute.
",N/A,3.11.11
CASSANDRA-13691,Fix incorrect [2.1 <— 3.0] serialization of counter cells with pre-2.1 local shards,"We stopped generating local shards in C* 2.1, after CASSANDRA-6504 (Counters 2.0). But it’s still possible to have counter cell values
around, remaining from 2.0 times, on 2.1, 3.0, 3.11, and even trunk nodes, if they’ve never been overwritten.

In 2.1, we used two classes for two kinds of counter columns:
{{CounterCell}} class to store counters - internally as collections of {{CounterContext}} blobs, encoding collections of (host id, count, clock) tuples
{{CounterUpdateCell}} class to represent unapplied increments - essentially a single long value; this class was never written to commit log, memtables, or sstables, and was only used inside {{Mutation}} object graph - in memory, and marshalled over network in cases when counter write coordinator and counter write leader were different nodes
3.0 got rid of {{CounterCell}} and {{CounterUpdateCell}}, among other {{Cell}} classes. In order to represent these unapplied increments - equivalents of 2.1 {{CounterUpdateCell}} - in 3.0 we encode them as regular counter columns, with a ‘special’ {{CounterContext}} value. I.e. a counter context with a single local shard. We do that so that we can reuse local shard reconcile logic (summing up) to seamlessly support counters with same names collapsing to single increments in batches. See {{UpdateParameters.addCounter()}} method comments [here|https://github.com/apache/cassandra/blob/cassandra-3.0.14/src/java/org/apache/cassandra/cql3/UpdateParameters.java#L157-L171] for details. It also assumes that nothing else can generate a counter with local shards.

It works fine in pure 3.0 clusters, and in mixed 2.1/3.0 clusters, assuming that there are no counters with legacy local shards remaining from 2.0 era. It breaks down badly if there are.

{{LegacyLayout.serializeAsLegacyPartition()}} and consequently {{LegacyCell.isCounterUpdate()}} - classes responsible for serializing and deserialising in 2.1 format for compatibility - use the following logic to tell if a cell of {{COUNTER}} kind is a regular final counter or an unapplied increment:

{code}
private boolean isCounterUpdate()
{
    // See UpdateParameters.addCounter() for more details on this
    return isCounter() && CounterContext.instance().isLocal(value);
}
{code}

{{CounterContext.isLocal()}} method here looks at the first shard of the collection of tuples and returns true if it’s a local one.

This method would correctly identify a cell generated by {{UpdateParameters.addCounter()}} as a counter update and serialize it correctly as a 2.1 {{CounterUpdateCell}}. However, it would also incorrectly flag any regular counter cell that just so happens to have a local shard as the first tuple of the counter context as a counter update. If a 2.1 node as a coordinator of a read requests fetches such a value from a 3.0 node, during a rolling upgrade, instead of the expected {{CounterCell}} object it will receive a {{CounterUpdateCell}}, breaking all the things. In the best case scenario it will cause an assert in {{AbstractCell.reconcileCounter()}} to be raised.

To fix the problem we must find an unambiguous way, without false positives or false negatives, to represent and identify unapplied counter updates on 3.0 side. ",N/A,"3.0.15, 3.11.1"
CASSANDRA-13686,Fix documentation typo,"Fix documentation typo under {quote}doc/html/cql/definitions.html#constants{quote}
and
{quote}doc/html/cql/ddl.html#the-clustering-columns{quote}",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13669,Error when starting cassandra: Unable to make UUID from 'aa' (SASI index),"Recently I experienced a problem that prevents me to restart cassandra.
I narrowed it down to SASI Index when added on uuid field.



Steps to reproduce:
1. start cassandra (./bin/cassandra -f)
2. create keyspace, table, index and add data:

{noformat}
CREATE KEYSPACE testkeyspace
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} 
           AND durable_writes = true;

use testkeyspace ;

CREATE TABLE testtable (
   col1 uuid,
   col2 uuid,
   ts timeuuid,
   col3 uuid,
   PRIMARY KEY((col1, col2), ts) ) with clustering order by (ts desc);

CREATE CUSTOM INDEX col3_testtable_idx ON testtable(col3)
USING 'org.apache.cassandra.index.sasi.SASIIndex'
WITH OPTIONS = {'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer', 'mode': 'PREFIX'};

INSERT INTO testtable(col1, col2, ts, col3)
VALUES(898e0014-6161-11e7-b9b7-238ea83bd70b,
               898e0014-6161-11e7-b9b7-238ea83bd70b,
               now(), 898e0014-6161-11e7-b9b7-238ea83bd70b);
{noformat}

3. restart cassandra

It crashes with an error (sorry it's huge):
{noformat}
DEBUG 09:09:20 Writing Memtable-testtable@1005362073(0.075KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
ERROR 09:09:20 Exception in thread Thread[PerDiskMemtableFlushWriter_0:1,5,main]
org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.Collections$SingletonSet.forEach(Collections.java:4767) ~[na:1.8.0_131]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_131]
	at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]
Exception (java.lang.RuntimeException) encountered during startup: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)
ERROR 09:09:20 Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:188) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:167) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:323) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730) [apache-cassandra-3.9.jar:3.9]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384) ~[apache-cassandra-3.9.jar:3.9]
	... 6 common frames omitted
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1122) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1084) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_131]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_131]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_131]
	at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384) ~[apache-cassandra-3.9.jar:3.9]
	... 5 common frames omitted
Caused by: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
	at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.Collections$SingletonSet.forEach(Collections.java:4767) ~[na:1.8.0_131]
	at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_131]
	at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	... 3 common frames omitted
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391)
at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:168)
at org.apache.cassandra.db.commitlog.CommitLog.recoverFiles(CommitLog.java:188)
at org.apache.cassandra.db.commitlog.CommitLog.recoverSegmentsOnDisk(CommitLog.java:167)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:323)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:601)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:730)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384)
... 6 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:391)
at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1122)
at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1084)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:192)
at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:384)
... 5 more
Caused by: org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118)
at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132)
at java.util.Collections$SingletonSet.forEach(Collections.java:4767)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119)
at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233)
at java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233)
at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107)
at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169)
at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
... 3 more
{noformat}

When I do ""nodetool flush"" I also get:
{noformat}
$  ./bin/nodetool flush
objc[35941]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/bin/java (0x1052a34c0) and /Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/libinstrument.dylib (0x10536b4e0). One of the two will be used. Which one is undefined.
error: Unable to make UUID from 'aa'
-- StackTrace --
org.apache.cassandra.serializers.MarshalException: Unable to make UUID from 'aa'
at org.apache.cassandra.db.marshal.UUIDType.fromString(UUIDType.java:118)
at org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer.hasNext(StandardAnalyzer.java:168)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter$Index.add(PerSSTableIndexWriter.java:208)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.lambda$nextUnfilteredCluster$0(PerSSTableIndexWriter.java:132)
at java.util.Collections$SingletonSet.forEach(Collections.java:4767)
at org.apache.cassandra.index.sasi.disk.PerSSTableIndexWriter.nextUnfilteredCluster(PerSSTableIndexWriter.java:119)
at org.apache.cassandra.db.ColumnIndex.lambda$add$1(ColumnIndex.java:233)
at java.lang.Iterable.forEach(Iterable.java:75)
at org.apache.cassandra.db.ColumnIndex.add(ColumnIndex.java:233)
at org.apache.cassandra.db.ColumnIndex.buildRowIndex(ColumnIndex.java:107)
at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:169)
at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:48)
at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:458)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:493)
at org.apache.cassandra.db.Memtable$FlushRunnable.call(Memtable.java:380)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:748)
{noformat}

Any ideas how to solve it?
I can keep col3 as text, I figured it out, but I already have bunch of data on production and I basically can't do anything with any of nodes, because I won't be able to start them again.

Thanks,
Lukasz",N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13666,Secondary index query on partition key columns might not return partitions with only static data,"The problem can be reproduced with the following test in {{3.0}}:
{code}
   @Test
    public void testIndexOnPartitionKeyWithPartitionWithoutRows() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk1 int, pk2 int, c int, s int static, v int, PRIMARY KEY((pk1, pk2), c))"");
        createIndex(""CREATE INDEX ON %s (pk2)"");

        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 2, 9, 2);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 3, 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 4, 1, 1, 9, 1);
        flush();

        assertRows(execute(""SELECT * FROM %s WHERE pk2 = ?"", 1),
                   row(1, 1, 1, 9, 1),
                   row(1, 1, 2, 9, 2),
                   row(3, 1, 1, 9, 1),
                   row(4, 1, 1, 9, 1));

        execute(""DELETE FROM %s WHERE pk1 = ? AND pk2 = ? AND c = ?"", 3, 1, 1);

        assertRows(execute(""SELECT * FROM %s WHERE pk2 = ?"", 1),
                   row(1, 1, 1, 9, 1),
                   row(1, 1, 2, 9, 2),
                   row(3, 1, null, 9, null),  // This row will not be returned
                   row(4, 1, 1, 9, 1));
    }
{code}

The problem seems to be that the index entries for the static data are inserted with an empty clustering key. When the first {{SELECT}} is executed those entries are removed by {{CompositesSearcher::filterStaleEntries}} which consider that those entries are stales. When the second {{SELECT}} is executed the index ignore the (3, 1) partition as there is not entry for it anymore.",N/A,"3.0.21, 3.11.7, 4.0-beta1, 4.0"
CASSANDRA-13655,Range deletes in a CAS batch are ignored,Range deletes in a CAS batch are ignored ,N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13652,Deadlock in AbstractCommitLogSegmentManager,"AbstractCommitLogManager uses LockSupport.(un)park incorreclty. It invokes unpark without checking if manager thread was parked in approriate place. 
For example, logging frameworks uses queues and queues uses ReadWriteLock's that uses LockSupport. Therefore AbstractCommitLogManager.wakeManager can wake thread inside Lock and manager thread will sleep forever at park() method (because unpark permit was already consumed inside lock).

For examle stack traces:
{code}
""MigrationStage:1"" id=412 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.awaitAvailableSegment(AbstractCommitLogSegmentManager.java:263)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.advanceAllocatingFrom(AbstractCommitLogSegmentManager.java:237)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager.forceRecycleAll(AbstractCommitLogSegmentManager.java:279)
    at org.apache.cassandra.db.commitlog.CommitLog.forceRecycleAllSegments(CommitLog.java:210)
    at org.apache.cassandra.config.Schema.dropView(Schema.java:708)
    at org.apache.cassandra.schema.SchemaKeyspace.lambda$updateKeyspace$23(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$382/1123232162.accept(Unknown Source)
    at java.util.LinkedHashMap$LinkedValues.forEach(LinkedHashMap.java:608)
    at java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1080)
    at org.apache.cassandra.schema.SchemaKeyspace.updateKeyspace(SchemaKeyspace.java:1361)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1332)
    at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1282)
      - locked java.lang.Class@cc38904
    at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$LocalSessionWrapper.run(DebuggableThreadPoolExecutor.java:322)
    at com.ringcentral.concurrent.executors.MonitoredRunnable.run(MonitoredRunnable.java:36)
    at MON_R_MigrationStage.run(NamedRunnableFactory.java:67)
    at com.ringcentral.concurrent.executors.MonitoredThreadPoolExecutor$MdcAwareRunnable.run(MonitoredThreadPoolExecutor.java:114)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)

""COMMIT-LOG-ALLOCATOR:1"" id=80 state=WAITING
    at sun.misc.Unsafe.park(Native Method)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
    at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1.runMayThrow(AbstractCommitLogSegmentManager.java:128)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
    at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$61/1733339045.run(Unknown Source)
    at java.lang.Thread.run(Thread.java:745)
{code}

Solution is to use Semaphore instead of low-level LockSupport.",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13646,Bind parameters of collection types are not properly validated,"It looks like C* is not validating properly the bind parameters for collection types. If an element of the collection is invalid the value will not be rejected and might cause an Exception later on.
The problem can be reproduced with the following test:
{code}
    @Test
    public void testInvalidQueries() throws Throwable
    {
        createTable(""CREATE TABLE %s (k int PRIMARY KEY, s frozen<set<tuple<int, text, double>>>)"");
        execute(""INSERT INTO %s (k, s) VALUES (0, ?)"", set(tuple(1,""1"",1.0,1), tuple(2,""2"",2.0,2)));
    }
{code}

The invalid Tuple will cause an ""IndexOutOfBoundsException: Index: 3, Size: 3""",N/A,"2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13643,converting expired ttl cells to tombstones causing unnecessary digest mismatches,"In [{{AbstractCell#purge}}|https://github.com/apache/cassandra/blob/26e025804c6777a0d124dbc257747cba85b18f37/src/java/org/apache/cassandra/db/rows/AbstractCell.java#L77]  , we convert expired ttl'd cells to tombstones, and set the the local deletion time to the cell's expiration time, less the ttl time. Depending on the timing of the purge, this can cause purge to generate tombstones that are otherwise purgeable. If compaction for a row with ttls isn't at the same state between replicas, this will then cause digest mismatches between logically identical rows, leading to unnecessary repair streaming and read repairs.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13641,Properly evict pstmts from prepared statements cache,Prepared statements that are evicted from the prepared statements cache are not removed from the underlying table {{system.prepared_statements}}. This can lead to issues during startup.,N/A,3.11.1
CASSANDRA-13640,CQLSH error when using 'login' to switch users,"Using {{PasswordAuthenticator}} and {{CassandraAuthorizer}}:

{code}
bin/cqlsh -u cassandra -p cassandra
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.14-SNAPSHOT | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> create role super with superuser = true and password = 'p' and login = true;
cassandra@cqlsh> login super;
Password:
super@cqlsh> list roles;

'Row' object has no attribute 'values'
{code}

When we initialize the Shell, we configure certain settings on the session object such as
{code}
self.session.default_timeout = request_timeout
self.session.row_factory = ordered_dict_factory
self.session.default_consistency_level = cassandra.ConsistencyLevel.ONE
{code}
However, once we perform a LOGIN cmd, which calls do_login(..), we create a new cluster/session object but actually never set those settings on the new session.

It isn't failing on 3.x. 

As a workaround, it is possible to logout and log back in and things work correctly.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13632,Digest mismatch if row is empty,"This issue is similar to CASSANDRA-12090. Quorum read queries that include a column selector (non-wildcard) result in digest mismatch when the row is empty (key does not exist). It seems the data serialization path checks if rowIterator.isEmpty() and if so ignores column names (by setting IS_EMPTY flag). However, the digest serialization path does not perform this check and includes column names. The digest comparison results in a mismatch. The mismatch does not end up issuing a read repair mutation since the underlying data is the same.

The mismatch on the read path ends up doubling our p99 read latency. We discovered this issue while testing a 2.2.5 to 3.0.13 upgrade.

One thing to note is that we're using thrift, which ends up handling the ColumnFilter differently than the CQL path. 

As with CASSANDRA-12090, fixing the digest seems sensible.",N/A,3.0.25
CASSANDRA-13627,Index queries are rejected on COMPACT tables,"Since {{3.0}}, {{compact}} tables are using under the hood {{static}} columns. Due to that {{SELECT}} queries using secondary indexes get rejected with the following error:
{{Queries using 2ndary indexes don't support selecting only static columns}}.

This problem can be reproduced using the following unit test:
{code}    @Test
    public void testIndicesOnCompactTable() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk int PRIMARY KEY, v int) WITH COMPACT STORAGE"");
        createIndex(""CREATE INDEX ON %s(v)"");

        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 1, 1);
        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 2, 1);
        execute(""INSERT INTO %S (pk, v) VALUES (?, ?)"", 3, 3);

        assertRows(execute(""SELECT pk, v FROM %s WHERE v = 1""),
                   row(1, 1),
                   row(2, 1));

    }{code}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13626,Check hashed password matches expected bcrypt hash format before checking,"We use {{Bcrypt.checkpw}} in the auth subsystem, but do a reasonably poor job of guaranteeing that the hashed password we send to it is really a hashed password, and {{checkpw}} does an even worse job of failing nicely. We should at least sanity check the hash complies with the expected format prior to validating.
",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13622,Better config validation/documentation,"There are a number of properties in the yaml that are ""in_mb"", however resolve to bytes when calculated in {{DatabaseDescriptor.java}}, but are stored in int's. This means that their maximum values are 2047, as any higher when converted to bytes overflows the int.

Where possible/reasonable we should convert these to be long's, and stored as long's. If there is no reason for the value to ever be >2047 we should at least document that as the max value, or better yet make it error if set higher than that. Noting that although it's bad practice to increase a lot of them to such high values, there may be cases where it is necessary and in which case we should handle it appropriately rather than overflowing and surprising the user. That is, causing it to break but not in the way the user expected it to :)

Following are functions that currently could be at risk of the above:

{code:java|title=DatabaseDescriptor.java}
getThriftFramedTransportSize()
getMaxValueSize()
getCompactionLargePartitionWarningThreshold()
getCommitLogSegmentSize()
getNativeTransportMaxFrameSize()
# These are in KB so max value of 2096128
getBatchSizeWarnThreshold()
getColumnIndexSize()
getColumnIndexCacheSize()
getMaxMutationSize()
{code}

Note we may not actually need to fix all of these, and there may be more. This was just from a rough scan over the code.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13620,Don't skip corrupt sstables on startup,"If we get an IOException when opening an sstable on startup, we just [skip|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java#L563-L567] it and continue starting

we should use the DiskFailurePolicy and never explicitly catch an IOException here",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13619,java.nio.BufferOverflowException: null while flushing hints,"I'm seeing the following exception running Cassandra 3.0.11 on 21 node cluster in two AWS regions when half of the nodes in one region go down, and the load is high on the rest of the nodes:

{code}
WARN  [SharedPool-Worker-10] 2017-06-14 12:57:15,017 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-10,5,main]: {}
java.lang.RuntimeException: java.nio.BufferOverflowException
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2549) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0-zing_17.03.1.0]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.11.jar:3.0.11]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0-zing_17.03.1.0]
Caused by: java.nio.BufferOverflowException: null
        at org.apache.cassandra.io.util.DataOutputBufferFixed.doFlush(DataOutputBufferFixed.java:52) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.write(BufferedDataOutputStreamPlus.java:195) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.io.util.BufferedDataOutputStreamPlus.writeUnsignedVInt(BufferedDataOutputStreamPlus.java:258) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithVIntLength(ByteBufferUtil.java:296) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Columns$Serializer.serialize(Columns.java:405) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.SerializationHeader$Serializer.serializeForMessaging(SerializationHeader.java:407) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:120) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:87) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.serialize(PartitionUpdate.java:625) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.db.Mutation$MutationSerializer.serialize(Mutation.java:305) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.Hint$Serializer.serialize(Hint.java:141) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:251) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBuffer$Allocation.write(HintsBuffer.java:230) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsBufferPool.write(HintsBufferPool.java:61) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.hints.HintsService.write(HintsService.java:154) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$11.runMayThrow(StorageProxy.java:2627) ~[apache-cassandra-3.0.11.jar:3.0.11]
        at org.apache.cassandra.service.StorageProxy$HintRunnable.run(StorageProxy.java:2545) ~[apache-cassandra-3.0.11.jar:3.0.11]
        ... 5 common frames omitted
{code}

Relevant configurations from cassandra.yaml:

{code}
-cassandra_hinted_handoff_throttle_in_kb: 1024
 cassandra_max_hints_delivery_threads: 4
-cassandra_hints_flush_period_in_ms: 10000
-cassandra_max_hints_file_size_in_mb: 512
{code}

When I reduce -cassandra_hints_flush_period_in_ms: 10000 to 5000, the number of exceptions lowers significantly, but they are still present.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13603,Change repair midpoint logging from  CASSANDRA-13052,"In CASSANDRA-13052 , we changed the way we handle repairs on small ranges to make them more sane in general, but {{MerkleTree.differenceHelper}} now erroneously logs at error.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13600,sstabledump possible problem,"h2. Possible bug in sstabledump

{noformat}
cqlsh> show version
[cqlsh 5.0.1 | Cassandra 3.10 | CQL spec 3.4.4 | Native protocol v4]
{noformat}

h2. Execute script in cqlsh in new keyspace

{noformat}
CREATE TABLE IF NOT EXISTS test_data (   
    // partitioning key
    PK TEXT, 

    // data
    Data TEXT,
    
    PRIMARY KEY (PK)
);

insert into test_data(PK,Data) values('0','aaaa');
insert into test_data(PK,Data) values('1','bbbb');
insert into test_data(PK,Data) values('2','cccc');
delete from test_data where PK='1';
insert into test_data(PK,Data) values('1','dddd');
{noformat}

h2. Execute the following commands

{noformat}
nodetool flush
nodetool compact
sstabledump mc-2-big-Data.db
sstabledump -d mc-2-big-Data.db
{noformat}

h3. default dump - missing data for partiotion key = ""1""

{noformat}
[
  {
    ""partition"" : {
      ""key"" : [ ""0"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 15,
        ""liveness_info"" : { ""tstamp"" : ""2017-06-14T12:23:13.529389Z"" },
        ""cells"" : [
          { ""name"" : ""data"", ""value"" : ""aaaa"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""2"" ],
      ""position"" : 26
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 41,
        ""liveness_info"" : { ""tstamp"" : ""2017-06-14T12:23:13.544132Z"" },
        ""cells"" : [
          { ""name"" : ""data"", ""value"" : ""cccc"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""1"" ],
      ""position"" : 53,
      ""deletion_info"" : { ""marked_deleted"" : ""2017-06-14T12:23:13.545988Z"", ""local_delete_time"" : ""2017-06-14T12:23:13Z"" }
    }
  }
]
{noformat}

h3. dump with -d option - correct data for partiotion key = ""1""

{noformat}
[0]@0 Row[info=[ts=1497442993529389] ]:  | [data=aaaa ts=1497442993529389]
[2]@26 Row[info=[ts=1497442993544132] ]:  | [data=cccc ts=1497442993544132]
[1]@53 deletedAt=1497442993545988, localDeletion=1497442993
[1]@53 Row[info=[ts=1497442993550159] ]:  | [data=dddd ts=1497442993550159]
{noformat}
",N/A,3.11.11
CASSANDRA-13596,Separation of commit logs on VM,"To improve performance and to achieve low latency, I wanted to separate Commit logs to different VM i.e. If my cassandra is running on VM with ip x.x.x.x then i want my commit logs to be generated at VM y.y.y.y.

I tried changing commitlog_directory parameter from cassandra.yml

Help me achieve this 

Thanks",N/A,3.0.9
CASSANDRA-13595,Implement short read protection on partition boundaries,"It seems that short read protection doesn't work when the short read is done at the end of a partition in a range query. The final assertion of this dtest fails:
{code}
def short_read_partitions_delete_test(self):
        cluster = self.cluster
        cluster.set_configuration_options(values={'hinted_handoff_enabled': False})
        cluster.set_batch_commitlog(enabled=True)
        cluster.populate(2).start(wait_other_notice=True)
        node1, node2 = self.cluster.nodelist()

        session = self.patient_cql_connection(node1)
        create_ks(session, 'ks', 2)
        session.execute(""CREATE TABLE t (k int, c int, PRIMARY KEY(k, c)) WITH read_repair_chance = 0.0"")

        # we write 1 and 2 in a partition: all nodes get it.
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (1, 1)"", consistency_level=ConsistencyLevel.ALL))
        session.execute(SimpleStatement(""INSERT INTO t (k, c) VALUES (2, 1)"", consistency_level=ConsistencyLevel.ALL))

        # we delete partition 1: only node 1 gets it.
        node2.flush()
        node2.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 1""))
        node2.start(wait_other_notice=True)

        # we delete partition 2: only node 2 gets it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        session = self.patient_cql_connection(node2, 'ks', consistency_level=ConsistencyLevel.ONE)
        session.execute(SimpleStatement(""DELETE FROM t WHERE k = 2""))
        node1.start(wait_other_notice=True)

        # read from both nodes
        session = self.patient_cql_connection(node1, 'ks', consistency_level=ConsistencyLevel.ALL)
        assert_none(session, ""SELECT * FROM t LIMIT 1"")
{code}
However, the dtest passes if we remove the {{LIMIT 1}}.

Short read protection [uses a {{SinglePartitionReadCommand}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/DataResolver.java#L484], maybe it should use a {{PartitionRangeReadCommand}} instead?",N/A,"3.0.15, 3.11.1"
CASSANDRA-13592,Null Pointer exception at SELECT JSON statement,"A Nulll pointer exception appears when the command

{code}
SELECT JSON * FROM examples.basic;

---MORE---
<Error from server: code=0000 [Server error] message=""java.lang.NullPointerException"">

Examples.basic has the following description (DESC examples.basic;):
CREATE TABLE examples.basic (
    key frozen<tuple<uuid, int>> PRIMARY KEY,
    wert text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

The error appears after the ---MORE--- line.

The field ""wert"" has a JSON formatted string.",N/A,"2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13587,Deadlock during CommitLog replay when Cassandra restarts,"Possible deadlock found when Cassandra is replaying commit log and at the same time Mutation gets triggered by SSTableReader(SystemKeyspace.persistSSTableReadMeter). As a result Cassandra restart hangs forever

Please find details of stack trace here:

*Frame#1* This thread is trying to apply {{persistSSTableReadMeter}} mutation and as a result it has called {{writeOrder.start()}} in {{Keyspace.java:533}}
but there are no Commitlog Segments available because {{createReserveSegments (CommitLogSegmentManager.java)}} is not yet {{true}} 

Hence this thread is blocked on {{createReserveSegments}} to become {{true}}, please note this thread has already started {{writeOrder}}

{quote}
""pool-11-thread-1"" #251 prio=5 os_prio=0 tid=0x00007fe128478400 nid=0x1b274 waiting on condition [0x00007fe1389a0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.advanceAllocatingFrom(CommitLogSegmentManager.java:277)
        at org.apache.cassandra.db.commitlog.CommitLogSegmentManager.allocate(CommitLogSegmentManager.java:196)
        at org.apache.cassandra.db.commitlog.CommitLog.add(CommitLog.java:260)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:540)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:421)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:210)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:215)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:224)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:566)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:556)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:295)
        at org.apache.cassandra.db.SystemKeyspace.persistSSTableReadMeter(SystemKeyspace.java:1181)
        at org.apache.cassandra.io.sstable.format.SSTableReader$GlobalTidy$1.run(SSTableReader.java:2202)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Frame#2* This thread is trying to recover commit logs and as a result it tries to flush Memtable by calling following code:
{{futures.add(Keyspace.open(SystemKeyspace.NAME).getColumnFamilyStore(SystemKeyspace.BATCHES).forceFlush());}}
As a result Frame#3 (below) gets created

{quote}
""main"" #1 prio=5 os_prio=0 tid=0x00007fe1c64ec400 nid=0x1af29 waiting on condition [0x00007fe1c94a1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
parking to wait for  <0x00000006370da0c0> (a com.google.common.util.concurrent.ListenableFutureTask)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429)
        at java.util.concurrent.FutureTask.get(FutureTask.java:191)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:383)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.blockForWrites(CommitLogReplayer.java:207)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:182)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:161)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:295)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{quote}

*Frame#3* This thread is waiting at {{writeBarrier.await();}} in {{ColumnFamilyStore.java:1027}} 
but {{writeBarrier}} is locked by thread in Frame#1, and Frame#1 thread is waiting for more CommitlogSegements to be available. 
Frame#1 thread will not get new segment because variable {{createReserveSegments(CommitLogSegmentManager.java)}} is not yet {{true}}. 
This variable gets set to {{true}} after successful execution of Frame#2.

Here we can see Frame#3 and Frame#1 are in deadlock state and Cassandra restart hangs forever.
 
{quote}
""MemtableFlushWriter:5"" #433 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0400 nid=0x1dea8 waiting on condition [0x00007e753c2ca000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
        at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:279)
        at org.apache.cassandra.utils.concurrent.OpOrder$Barrier.await(OpOrder.java:419)
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1027)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)


""MemtablePostFlush:3"" #432 daemon prio=5 os_prio=0 tid=0x00007e7a4b8b0000 nid=0x1dea7 waiting on condition [0x00007e753c30b000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
 parking to wait for  <0x00000006370d9cd0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:941)
        at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.call(ColumnFamilyStore.java:924)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
        at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$4/1527007086.run(Unknown Source)
        at java.lang.Thread.run(Thread.java:745)
{quote}

*Reproducible steps*: Reproducing this problem is tricky as it involves multiple conditions to happen at the same time and is timing bases, so I have done some small code change to reproduce this:
1. Create a Keyspace and table
2. Inject data until there are few SSTables generated and CommitLog available
3. Kill Cassandra process
4. Use the custom code (in the attached file ""Reproduce_CASSANDRA-13587.txt"") on top of 3.0.14 branch 
5. Build Cassandra jar and use this custom jar
6. Restart Cassandra
    Here you will see Cassandra is hanging forever
7. Now apply this fix on top of ""Reproduce_CASSANDRA-13587.txt"", and repeat step-6
    Here you should see Cassandra is starting normally

*Solution*: I am proposing that we should enable variable {{createReserveSegments(CommitLogSegmentManager.java)}} before recovering any CommitLogs in CommitLog.java file
so this will not block Frame#1 from acquiring new segment as a result Frame#1 will finish and then Frame#2 will also finish.
Please note, this variable {{createReserveSegments}} has been removed from the trunk branch as part of (https://issues.apache.org/jira/browse/CASSANDRA-10202), also in the trunk branch CommitLog segments gets created when needed. So as per my understanding enabling this variable before CommitLog recovery should not create any other side effect, please let me know your comments.
",N/A,3.0.15
CASSANDRA-13584,Inclusion of cassandra-dtest project from code donation,"The issue is for tracking legal and incubator acceptance of cassandra-dtest. 

IP clearance template is located here:
https://svn.apache.org/repos/asf/incubator/public/trunk/content/ip-clearance/cassandra-dtest.xml

The dtest project is located here:
https://github.com/riptano/cassandra-dtest 

For convienience, vote thread is here:
https://lists.apache.org/thread.html/d9e694ba8eaac8e8c70cbfd3f6ee249d43f8c67279882ffc65e56cac@%3Cdev.cassandra.apache.org%3E

Software grant from DataStax is attached. 

",N/A,"2.1.19, 2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13574,mx4j default listening configuration comment is not correct,"{noformat}
By default mx4j listens on 0.0.0.0:8081.
{noformat}
https://github.com/apache/cassandra/blob/cassandra-2.2/conf/cassandra-env.sh#L302

It's actually set to Cassandra broadcast_address and it will never be {{0.0.0.0}}:
https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/utils/Mx4jTool.java#L79",N/A,"3.0.25, 3.11.11, 4.0.1, 4.1-alpha1, 4.1"
CASSANDRA-13573,ColumnMetadata.cellValueType() doesn't return correct type for non-frozen collection,"Schema and data""
{noformat}
CREATE TABLE ks.cf (
    hash blob,
    report_id timeuuid,
    subject_ids frozen<set<int>>,
    PRIMARY KEY (hash, report_id)
) WITH CLUSTERING ORDER BY (report_id DESC);

INSERT INTO ks.cf (hash, report_id, subject_ids) VALUES (0x1213, now(), {1,2,4,5});
{noformat}

sstabledump output is:

{noformat}
sstabledump mc-1-big-Data.db 
[
  {
    ""partition"" : {
      ""key"" : [ ""1213"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 16,
        ""clustering"" : [ ""ec01eed0-49d9-11e7-b39a-97a96f529c02"" ],
        ""liveness_info"" : { ""tstamp"" : ""2017-06-05T10:29:57.434856Z"" },
        ""cells"" : [
          { ""name"" : ""subject_ids"", ""value"" : """" }
        ]
      }
    ]
  }
]
{noformat}

While the values are really there:

{noformat}
cqlsh:ks> select * from cf ;

 hash   | report_id                            | subject_ids
--------+--------------------------------------+-------------
 0x1213 | 02bafff0-49d9-11e7-b39a-97a96f529c02 |   {1, 2, 4}
{noformat}
",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13568,"nodetool listsnapshots output is missing a newline, if there are no snapshots","When there are no snapshots, the nodetool listsnaphots command output is missing a newline, which gives a somewhat bad user experience:

{code}
root@cassandra2:~# nodetool listsnapshots
Snapshot Details: 
There are no snapshotsroot@cassandra2:~# 
{code}

I",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13562,Cassandra removenode makes Gossiper Thread hang forever,"We have seen nodes in Cassandra (3.0.11) ring gets into split-brain somehow. We don't know exact reproducible steps but here is our observation:

Let's assume we have 5 node cluster n1,n2,n3,n4,n5. In this bug when do nodetool status on each node then each one has different view of DN node

e.g.
n1 sees n3 as DN and other nodes are UN
n3 sees n4 as DN and other nodes are UN
n4 sees n5 as DN and other nodes are UN and so on...

One thing we have observed is once n/w link is broken and restored then sometimes nodes go into this split-brain mode but we still don't have exact reproducible steps.

Please let us know if I am missing anything specific here.",N/A,3.0.14
CASSANDRA-13559,Schema version id mismatch while upgrading to 3.0.13,"As the order of SchemaKeyspace is changed ([6991556 | https://github.com/apache/cassandra/commit/6991556e431a51575744248a4c484270c4f918c9], CASSANDRA-12213), the result of function [{{calculateSchemaDigest}}|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/schema/SchemaKeyspace.java#L311] is also changed for the same schema. Which causes schema mismatch while upgrading 3.0.x -> 3.0.13.
It could cause cassandra fail to start because Unknown CF exception. And streaming will fail:
{noformat}
ERROR [main] 2017-05-26 18:58:57,572 CassandraDaemon.java:709 - Exception encountered during startup
java.lang.IllegalArgumentException: Unknown CF 83c8eae0-3a65-11e7-9a27-e17fd11571e3
{noformat}
{noformat}
WARN  [MessagingService-Incoming-/IP] 2017-05-26 19:27:11,523 IncomingTcpConnection.java:101 - UnknownColumnFamilyException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find table for cfId 922b7940-3a65-11e7-adf3-a3ff55d9bcf1. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
{noformat}

Restart the new node will cause:
{noformat}
Exception (java.lang.NoSuchFieldError) encountered during startup: ALL
java.lang.NoSuchFieldError: ALL
        at org.apache.cassandra.service.ClientState.<clinit>(ClientState.java:67)
        at org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance.<init>(QueryProcessor.java:155)
        at org.apache.cassandra.cql3.QueryProcessor$InternalStateInstance.<clinit>(QueryProcessor.java:149)
        at org.apache.cassandra.cql3.QueryProcessor.internalQueryState(QueryProcessor.java:163)
        at org.apache.cassandra.cql3.QueryProcessor.prepareInternal(QueryProcessor.java:286)
        at org.apache.cassandra.cql3.QueryProcessor.executeInternal(QueryProcessor.java:294)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:900)
        at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:354)
        at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:110)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:179)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:569)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:697)
{noformat}

I would suggest to have the older list back for digest calculation and release 3.0.14.",N/A,"3.0.14, 3.11.0"
CASSANDRA-13557,allow different NUMACTL_ARGS to be passed in,"Currently in bin/cassandra the following is hardcoded:
NUMACTL_ARGS=""--interleave=all""
Ideally users of cassandra/bin could pass in a different set of NUMACTL_ARGS if they wanted to say bind the process to a socket for cpu/memory reasons, rather than having to comment out/modify this line in the deployed cassandra/bin. e.g as described in:
https://tobert.github.io/pages/als-cassandra-21-tuning-guide.html

This could be done by just having the default be set to ""--interleave=all"" but pickup any value which has already been set for the variable NUMACTL_ARGS.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13549,Cqlsh throws and error when querying a duration data type,"h3. Overview

Querying duration related data from the cqlsh prompt results in an error.

Consider the following create table and insert statement.
{code:title=Table and insert statement with duration data type|borderStyle=solid}
CREATE TABLE duration_test (
  primary_key text,
  col20 duration,
  PRIMARY KEY (primary_key)
);
INSERT INTO duration_test (primary_key, col20) VALUES ('primary_key_example', 1y5mo89h4m48s);
{code}

On executing a select query on col20 in cqlsh I get an error ""Failed to format value '""\x00\xfe\x02GS\xfc\xa5\xc0\x00' : 'ascii' codec can't decode byte 0xfe in position 2: ordinal not in range(128)""
{code:title=Duration Query|borderStyle=solid}
Select  col20 from duration_test;
{code}

h3. Investigation

On investigating this further I found that the current python Cassandra driver used found in lib/cassandra-driver-internal-only-3.7.0.post0-2481531.zip does not seem to support duration data type. This was added in Jan this year https://github.com/datastax/python-driver/pull/689.

So I downloaded the latest driver release https://github.com/datastax/python-driver/releases/tag/3.9.0. I embedded the latest driver into cassandra-driver-internal-only-3.7.0.post0-2481531.zip. This fixed the driver related issue but there was still a formatting issue. 

I then went on to modify the format_value_duration methos in the pylib/cqlshlib/formatting.py. Diff posted below

{code}
 @formatter_for('Duration')
 def format_value_duration(val, colormap, **_):
-    buf = six.iterbytes(val)
-    months = decode_vint(buf)
-    days = decode_vint(buf)
-    nanoseconds = decode_vint(buf)
-    return format_python_formatted_type(duration_as_str(months, days, nanoseconds), colormap, 'duration')
+    return format_python_formatted_type(duration_as_str(val.months, val.days, val.nanoseconds), colormap, 'duration')
{code}

This resulted in fixing the issue and duration types are now correctly displayed.

Happy to fix the issue if I can get some guidance on:
# If this is a valid issue. Tried searching JIRA but did not find anything reported. 
# If my assumptions are correct i.e. this is actually a bug
# how to package the new driver into the source code. 







",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13547,Filtered materialized views missing data,"When creating a materialized view against a base table the materialized view does not always reflect the correct data.

Using the following test schema:

{code:title=Schema|language=sql}
DROP KEYSPACE IF EXISTS test;
CREATE KEYSPACE test
  WITH REPLICATION = { 
   'class' : 'SimpleStrategy', 
   'replication_factor' : 1 
  };
CREATE TABLE test.table1 (
                id int,
                name text,
                enabled boolean,
                foo text,
                PRIMARY KEY (id, name));
CREATE MATERIALIZED VIEW test.table1_mv1 AS SELECT id, name, foo
                FROM test.table1
                WHERE id IS NOT NULL 
                AND name IS NOT NULL 
                AND enabled = TRUE
                PRIMARY KEY ((name), id);
CREATE MATERIALIZED VIEW test.table1_mv2 AS SELECT id, name, foo, enabled
                FROM test.table1
                WHERE id IS NOT NULL 
                AND name IS NOT NULL 
                AND enabled = TRUE
                PRIMARY KEY ((name), id);
{code}

When I insert a row into the base table the materialized views are updated appropriately. (+)
{code:title=Insert row|language=sql}
cqlsh> INSERT INTO test.table1 (id, name, enabled, foo) VALUES (1, 'One', TRUE, 'Bar');
cqlsh> SELECT * FROM test.table1;

 id | name | enabled | foo
----+------+---------+-----
  1 |  One |    True | Bar

(1 rows)
cqlsh> SELECT * FROM test.table1_mv1;

 name | id | foo
------+----+-----
  One |  1 | Bar

(1 rows)
cqlsh> SELECT * FROM test.table1_mv2;

 name | id | enabled | foo
------+----+---------+-----
  One |  1 |    True | Bar

(1 rows)
{code}


Updating the record in the base table and setting enabled to FALSE will filter the record from both materialized views. (+)
{code:title=Disable the row|language=sql}
cqlsh> UPDATE test.table1 SET enabled = FALSE WHERE id = 1 AND name = 'One';
cqlsh> SELECT * FROM test.table1;

 id | name | enabled | foo
----+------+---------+-----
  1 |  One |   False | Bar

(1 rows)
cqlsh> SELECT * FROM test.table1_mv1;

 name | id | foo
------+----+-----

(0 rows)
cqlsh> SELECT * FROM test.table1_mv2;

 name | id | enabled | foo
------+----+---------+-----

(0 rows)
{code}


However a further update to the base table setting enabled to TRUE should include the record in both materialzed views, however only one view (table1_mv2) gets updated. (-)
It appears that only the view (table1_mv2) that returns the filtered column (enabled) is updated. (-)
Additionally columns that are not part of the partiion or clustering key are not updated. You can see that the foo column has a null value in table1_mv2. (-)
{code:title=Enable the row|language=sql}
cqlsh> UPDATE test.table1 SET enabled = TRUE WHERE id = 1 AND name = 'One';
cqlsh> SELECT * FROM test.table1;

 id | name | enabled | foo
----+------+---------+-----
  1 |  One |    True | Bar

(1 rows)
cqlsh> SELECT * FROM test.table1_mv1;

 name | id | foo
------+----+-----

(0 rows)
cqlsh> SELECT * FROM test.table1_mv2;

 name | id | enabled | foo
------+----+---------+------
  One |  1 |    True | null

(1 rows)
{code}
",N/A,3.11.5
CASSANDRA-13542,nodetool scrub/cleanup/upgradesstables exit code,We exit nodetool with success if we fail marking sstables as compacting,N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13533,ColumnIdentifier object size wrong when tables are not flushed,"It turns out that the object size of {{ColumnIdentifier}} is wrong when *cassandra.test.flush_local_schema_changes: false*. This looks like stuff is being wrongly reused when no flush is happening.

We only noticed this because we were using the prepared stmt cache and noticed that prepared statements would account for *1-6mb* when *cassandra.test.flush_local_schema_changes: false*. With *cassandra.test.flush_local_schema_changes: true* (which is the default) those would be around *5000 bytes*.

Attached is a test that reproduces the problem and also a fix.

Also after talking to [~jkni] / [~blerer] we shouldn't probably take {{ColumnDefinition}} into account when measuring object sizes with {{MemoryMeter}}
",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13532,sstabledump reports incorrect usage for argument order,"sstabledump usage reports 

{{usage: sstabledump <options> <sstable file path>}}

However the actual usage is 

{{sstabledump  <sstable file path> <options>}}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13528,nodetool describeclusters shows different snitch info as to what is configured.,"I couldn't find any similar issue as this one so I'm creating one.
I noticed that doing nodetool describecluster shows a different Snitch Information as to what is being set in the configuration file.

My setup is hosted in AWS and I am using Ec2Snitch.

cassandra@cassandra3$ nodetool describecluster
Cluster Information:
	Name: testv3
	Snitch: org.apache.cassandra.locator.DynamicEndpointSnitch
	Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
	Schema versions:
		fc6e8656-ee7a-341b-9782-b569d1fd1a51: [10.0.3.61,10.0.3.62,10.0.3.63]

I checked via MX4J and it shows the same, I haven't verified tho using a different Snitch and I am using 2.2.6 above and 3.0.X ",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13526,"nodetool cleanup on KS with no replicas should remove old data, not silently complete","From the user list:

https://lists.apache.org/thread.html/5d49cc6bbc6fd2e5f8b12f2308a3e24212a55afbb441af5cb8cd4167@%3Cuser.cassandra.apache.org%3E

If you have a multi-dc cluster, but some keyspaces not replicated to a given DC, you'll be unable to run cleanup on those keyspaces in that DC, because [the cleanup code will see no ranges and exit early|https://github.com/apache/cassandra/blob/4cfaf85/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L427-L441]",N/A,"3.0.16, 3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13525,ReverseIndexedReader may drop rows during 2.1 to 3.0 upgrade,"During an upgrade from 2.1 (or 2.2) to 3.0 (or 3.x) queries which perform reverse iteration may silently drop rows from their results. This can happen before sstableupgrade is run and when the sstables are indexed.
",N/A,"3.0.14, 3.11.0"
CASSANDRA-13518,sstableloader doesn't support non default storage_port and ssl_storage_port.,"Currently these 2 ports are using hardcoded default ports: https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/config/Config.java#L128-L129

The proposed fix is to add command line option for these two ports like what NATIVE_PORT_OPTION currently does",N/A,"3.0.14, 3.11.0"
CASSANDRA-13514,Cassandra 3.10 DSE DevCenter 1.6 Select with SASI index doesn't accept LIKE clause,Created SASI index for prefix search. Select with LIKE clause fails only on DSE DevCenter. Select with LIKE clause works fine locally at the server using CQLSH.,N/A,3.10
CASSANDRA-13499,Avoid duplicate calls to the same custom row index,Avoid duplicate calls to the same custom row index by using a dedicated Set<Index> rather than the collection indexes.values().,N/A,"3.0.14, 3.11.0, 4.1-alpha1, 4.1"
CASSANDRA-13493,RPM Init: Service startup ordering,"Currently, Cassandra is setup to start _before_ network and name services come up, and setup to be town down _after_ them, dangerously close to the final shutdown call.

A service daemon which may use network-based storage, and serves requests over a network needs to start clearly after network and network mounts, and come down clearly after.
",N/A,"2.2.10, 3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13490,"RPM Spec - disable binary check, improve readme instructions",,N/A,"2.2.10, 3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13485,Better handle IO errors on 3.0+ flat files ,"In 3.0, hints and compaction transaction data both move into flat files. Like every other part of cassandra, we can have IO errors either reading or writing those files, and should properly handle IO exceptions on those files (including respecting the disk failure policies).



",N/A,"3.0.19, 3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-13482,NPE on non-existing row read when row cache is enabled,"The problem is reproducible on 3.0 with:

{code}
-# row_cache_class_name: org.apache.cassandra.cache.OHCProvider
+row_cache_class_name: org.apache.cassandra.cache.OHCProvider

-row_cache_size_in_mb: 0
+row_cache_size_in_mb: 100
{code}

Table setup:

{code}
CREATE TABLE cache_tables (pk int, v1 int, v2 int, v3 int, primary key (pk, v1)) WITH CACHING = { 'keys': 'ALL', 'rows_per_partition': '1' } ;
{code}

No data is required, only a head query (or any pk/ck query but with full partitions cached). 

{code}
select * from cross_page_queries where pk = 10000 ;
{code}

{code}
java.lang.AssertionError: null
        at org.apache.cassandra.db.rows.UnfilteredRowIterators.concat(UnfilteredRowIterators.java:193) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.getThroughCache(SinglePartitionReadCommand.java:461) ~[main/:na]
        at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:358) ~[main/:na]
        at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:395) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1794) ~[main/:na]
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2472) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
{code}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13467,[Backport CASSANDRA-10876]: Alter behavior of batch WARN and fail on single partition batches,"Would anyone think this backport may cause problem? We're running Cassandra 3.0 and hit this problem. There are some other people would like this backport (see the last few comments from CASSANDRA-10876).

I'll provide the patch soon.",N/A,3.0.25
CASSANDRA-13464,Failed to create Materialized view with a specific token range,"Failed to create Materialized view with a specific token range.

Example :

{code:java}
$ ccm create ""MaterializedView"" -v 3.0.13
$ ccm populate  -n 3
$ ccm start
$ ccm status
Cluster: 'MaterializedView'
---------------------------
node1: UP
node3: UP
node2: UP
$ccm node1 cqlsh
Connected to MaterializedView at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.13 | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':3};
cqlsh> CREATE TABLE test.test ( id text PRIMARY KEY , value1 text , value2 text, value3 text);

$ccm node1 ring test 
Datacenter: datacenter1
==========
Address    Rack        Status State   Load            Owns                Token
                                                                          3074457345618258602
127.0.0.1  rack1       Up     Normal  64.86 KB        100.00%             -9223372036854775808
127.0.0.2  rack1       Up     Normal  86.49 KB        100.00%             -3074457345618258603
127.0.0.3  rack1       Up     Normal  89.04 KB        100.00%             3074457345618258602

$ ccm node1 cqlsh
cqlsh> INSERT INTO test.test (id, value1 , value2, value3 ) VALUES ('aaa', 'aaa', 'aaa' ,'aaa');
cqlsh> INSERT INTO test.test (id, value1 , value2, value3 ) VALUES ('bbb', 'bbb', 'bbb' ,'bbb');
cqlsh> SELECT token(id),id,value1 FROM test.test;

 system.token(id)     | id  | value1
----------------------+-----+--------
 -4737872923231490581 | aaa |    aaa
 -3071845237020185195 | bbb |    bbb

(2 rows)

cqlsh> CREATE MATERIALIZED VIEW test.test_view AS SELECT value1, id FROM test.test WHERE id IS NOT NULL AND value1 IS NOT NULL AND TOKEN(id) > -9223372036854775808 AND TOKEN(id) < -3074457345618258603 PRIMARY KEY(value1, id) WITH CLUSTERING ORDER BY (id ASC);
ServerError: java.lang.ClassCastException: org.apache.cassandra.cql3.TokenRelation cannot be cast to org.apache.cassandra.cql3.SingleColumnRelation
{code}

Stacktrace :
{code:java}
INFO  [MigrationStage:1] 2017-04-19 18:32:48,131 ColumnFamilyStore.java:389 - Initializing test.test
WARN  [SharedPool-Worker-1] 2017-04-19 18:44:07,263 FBUtilities.java:337 - Trigger directory doesn't exist, please create it and try again.
ERROR [SharedPool-Worker-1] 2017-04-19 18:46:10,072 QueryMessage.java:128 - Unexpected error during query
java.lang.ClassCastException: org.apache.cassandra.cql3.TokenRelation cannot be cast to org.apache.cassandra.cql3.SingleColumnRelation
	at org.apache.cassandra.db.view.View.relationsToWhereClause(View.java:275) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.CreateViewStatement.announceMigration(CreateViewStatement.java:219) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.13.jar:3.0.13]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.13.jar:3.0.13]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
ERROR [SharedPool-Worker-1] 2017-04-19 18:46:10,073 ErrorMessage.java:349 - Unexpected exception during request
java.lang.ClassCastException: org.apache.cassandra.cql3.TokenRelation cannot be cast to org.apache.cassandra.cql3.SingleColumnRelation
	at org.apache.cassandra.db.view.View.relationsToWhereClause(View.java:275) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.CreateViewStatement.announceMigration(CreateViewStatement.java:219) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:237) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.13.jar:3.0.13]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.13.jar:3.0.13]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.13.jar:3.0.13]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
INFO  [IndexSummaryManager:1] 2017-04-19 19:20:43,246 IndexSummaryRedistribution.java:74 - Redistributing index summaries
{code}


I don't know if it is a bug.
I want to create materialized view with a specific token range.
",N/A,"3.0.25, 3.11.11, 4.0.1"
CASSANDRA-13448,Possible divide by 0 in 2i,Possible divide by zero issue in  {{org.apache.cassandra.index.SecondaryIndexManager.calculateIndexingPageSize}} if {{columnsPerRow}} evaluates to 0 (table without non-primary-key columns and a secondary index will throw an exception when that index is rebuilt ).,N/A,"3.0.14, 3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-13443,V5 protocol flags decoding broken,"Since native protocol version 5 we deserialize the flags in {{org.apache.cassandra.cql3.QueryOptions.Codec#decode}} as follows:
{code}
            EnumSet<Flag> flags = Flag.deserialize(version.isGreaterOrEqualTo(ProtocolVersion.V5)
                                                   ? (int)body.readUnsignedInt()
                                                   : (int)body.readByte());
{code}

This works until the highest bit (0x80) is not used. {{readByte}} must be changed to {{readUnsignedByte}}.",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13441,"Schema version changes for each upgraded node in a rolling upgrade, causing migration storms","In versions < 3.0, during a rolling upgrade (say 2.0 -> 2.1), the first node to upgrade to 2.1 would add the new tables, setting the new 2.1 version ID, and subsequently upgraded hosts would settle on that version.

When a 3.0 node upgrades and writes its own new-in-3.0 system tables, it'll write the same tables that exist in the schema with brand new timestamps. As written, this will cause all nodes in the cluster to change schema (to the version with the newest timestamp). On a sufficiently large cluster with a non-trivial schema, this could cause (literally) millions of migration tasks to needlessly bounce across the cluster.

",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13440,Sign RPM artifacts,RPMs should be gpg signed just as the deb packages. Also add documentation how to verify to download page.,N/A,"2.1.18, 2.2.10, 3.0.14, 3.11.0"
CASSANDRA-13435,Incorrect status check use when stopping Cassandra,"Function {{status}} from {{/etc/rc.d/init.d/functions}} will delegate to {{systemctl status}} and we can't keep using the output to determine the status result. This should be changed to match the return value instead, which is supposed to [return 3|http://refspecs.linuxbase.org/LSB_3.0.0/LSB-PDA/LSB-PDA/iniscrptact.html] for non-running processes",N/A,"2.2.10, 3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13434,Add PID file directive in /etc/init.d/cassandra,"As mentioned in CASSANDRA-10920, we should add directive for pid file in header that allows creating a unit file with the correct PIDFile=.. entry. Else systemd won't be able to tell if Cassandra is still running.
{noformat}
# pidfile: /var/run/cassandra/cassandra.pid
{noformat}",N/A,"2.2.10, 3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13422,CompactionStrategyManager should take write not read lock when handling remove notifications,"{{getNextBackgroundTask}} in various compaction strategies (definitely {{LCS}}) rely on checking the result of {{DataTracker.getCompacting()}} to avoid accessing data and metadata related to tables that have already head their resources released.

There is a race where this check is unreliable and will claim a table that has its resources already released is not compacting resulting in use after free.

[{{LeveledCompactionStrategy.findDroppableSSTable}}|https://github.com/apache/cassandra/blob/c794d2bed7ca1d10e13c4da08a3d45f5c755c1d8/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java#L504] for instance has this three part logical && condition where the first check is against the compacting set before calling {{worthDroppingTombstones}} which fails if the table has been released.

The order of events is basically that CompactionStrategyManager acquires the read lock in getNextBackgroundTask(), then proceeds eventually to findDroppableSSTable and acquires a set of SSTables from the manifest. While the manifest is thread safe it's not accessed atomically WRT to other operations. Once it has acquired the set of tables it acquires (not atomically) the set of compacting SSTables and iterates checking the former against the latter.

Meanwhile other compaction threads are marking tables obsolete or compacted and releasing their references. Doing this removes them from {{DataTracker}} and publishes a notification to the strategies, but this notification only requires the read lock. After the compaction thread has published the notifications it eventually marks the table as not compacting in {{DataTracker}} or removes it entirely.

The race is then that the compaction thread generating a new background task acquires the sstables from the manifest on the stack. Any table in that set that was compacting at that time must remain compacting so that it can be skipped. Another compaction thread finishes a compaction and is able to remove the table from the manifest and then remove it from the compacting set. The thread generating the background task then acquires the list of compacting tables which doesn't include the table it is supposed to skip.

The simple fix appears to be to require threads to acquire the write lock in order to publish notifications of tables being removed from compaction strategies. While holding the write lock it won't be possible for someone to see a view of tables in the manifest where tables that are compacting aren't compacting in the view.",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13418,Allow TWCS to ignore overlaps when dropping fully expired sstables,"http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html explains it well. If you really want read-repairs you're going to have sstables blocking the expiration of other fully expired SSTables because they overlap.

You can set unchecked_tombstone_compaction = true or tombstone_threshold to a very low value and that will purge the blockers of old data that should already have expired, thus removing the overlaps and allowing the other SSTables to expire.

The thing is that this is rather CPU intensive and not optimal. If you have time series, you might not care if all your data doesn't exactly expire at the right time, or if data re-appears for some time, as long as it gets deleted as soon as it can. And in this situation I believe it would be really beneficial to allow users to simply ignore overlapping SSTables when looking for fully expired ones.

To the question: why would you need read-repairs ?
- Full repairs basically take longer than the TTL of the data on my dataset, so this isn't really effective.
- Even with a 10% chances of doing a repair, we found out that this would be enough to greatly reduce entropy of the most used data (and if you have timeseries, you're likely to have a dashboard doing the same important queries over and over again).
- LOCAL_QUORUM is too expensive (need >3 replicas), QUORUM is too slow.

I'll try to come up with a patch demonstrating how this would work, try it on our system and report the effects.

cc: [~adejanovski], [~rgerard] as I know you worked on similar issues already.",N/A,"3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13417,Illegal unicode character breaks compilation on Chinese env OS,"Creating JIRA for tracking GH issue https://github.com/apache/cassandra/pull/104

Fix is contained within a comment block, so skipping CI.

",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13414,circle.yml: parallelize tests and install word list,"- Run 2 test runners, since circleci container has 2 vCPUs.
- o.a.c.utils.BitSetTest.compareBitSets fails in 2.1 and is skipped in later branches when test word list is unavailable.

I verified that compareBitSets passes again in a 2.1 run, and I have a trunk test with 2 runners in progress. Will check run time, when that has completed.",N/A,"2.1.x, 2.2.x, 3.0.13, 3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-13413,Run more test targets on CircleCI,"Currently we only run {{ant test}} on circleci, we should use all the (free) containers we have and run more targets in parallel.",N/A,"2.1.18, 2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13410,nodetool upgradesstables/scrub/compact ignores system tables,"CASSANDRA-11627 changed the behavior of nodetool commands that work across all keyspaces. Sometimes it's OK (not compacting system.peers when you call compact probably isn't going to anger anyone), but sometimes it's not (disableautocompaction, flush, upgradesstables, etc).

",N/A,"3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13405,ViewBuilder can miss data due to sstable generation filter,"The view builder for one MV is restarted when other MVs are added on the same keyspace.  There is an issue if compactions are running between these restarts that can cause the view builder to skip data, since the builder tracks the max sstable generation to filter by when it starts back up.

I don't see a need for this generation tracking across restarts, it only needs to be tracked during a builders life (to avoid adding in newly compacted data).  

",N/A,"3.0.13, 3.11.0"
CASSANDRA-13400,java.lang.ArithmeticException: / by zero when index is created on table with clustering columns only,"If we create an index over the clustering key of a table without regular columns:
{code}
CREATE KEYSPACE k WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
CREATE TABLE k.t (pk text, ck text, PRIMARY KEY (pk, ck));
INSERT INTO k.t (pk, ck) VALUES ( 'pk','ck');
CREATE INDEX idx ON k.t(ck);
{code}
Then the following error index creation erros is logged:
{code}
INFO  10:19:34 Submitting index build of idx for data in BigTableReader(path='/Users/adelapena/datastax/cassandra/data/data/k/t-ed3d6f90185611e7949f55d18a2e5858/mc-1-big-Data.db')
ERROR 10:19:34 Exception in thread Thread[SecondaryIndexManagement:2,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:402) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.buildBlocking(CassandraIndex.java:723) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.lambda$getBuildIndexTask$5(CassandraIndex.java:693) ~[main/:na]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_112]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79) [main/:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_112]
Caused by: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_112]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_112]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:398) ~[main/:na]
	... 7 common frames omitted
Caused by: java.lang.ArithmeticException: / by zero
	at org.apache.cassandra.index.SecondaryIndexManager.calculateIndexingPageSize(SecondaryIndexManager.java:629) ~[main/:na]
	at org.apache.cassandra.index.SecondaryIndexBuilder.build(SecondaryIndexBuilder.java:62) ~[main/:na]
	at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1347) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_112]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_112]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_112]
	... 5 common frames omitted
{code}
Any further queries using the index will fail:
{code}
SELECT * FROM k.t where ck = 'ck';
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}",N/A,3.0.13
CASSANDRA-13399,UDA fails without input rows,"When creating the following user defined AGGREGATION and FUNCTION:

{code:title=init.cql|borderStyle=solid}
CREATE FUNCTION state_group_and_total(state map<uuid, int>, type uuid)
    RETURNS NULL ON NULL INPUT
    RETURNS map<uuid, int>
    LANGUAGE java AS '
        Integer count = (Integer) state.get(type);

        count = (count == null ? 1 : count + 1);
        state.put(type, count);

        return state;
    ';

CREATE OR REPLACE AGGREGATE group_and_total(uuid)
    SFUNC state_group_and_total
    STYPE map<uuid, int>
    INITCOND {};
{code}

And creating a statement like:

{code}
SELECT group_and_total(""id"") FROM mytable;
{code}

When mytable is empty, it throws the following null assertion

{code}
ERROR [Native-Transport-Requests-1] 2017-04-03 07:25:09,787 Message.java:623 - Unexpected exception during request; channel = [id: 0xd7d9159b, L:/172.19.0.2:9042 - R:/172.19.0.3:43444]
java.lang.AssertionError: null
        at org.apache.cassandra.cql3.functions.UDAggregate$2.compute(UDAggregate.java:189) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.AggregateFunctionSelector.getOutput(AggregateFunctionSelector.java:53) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:592) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:430) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.build(Selection.java:424) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:763) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:378) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:251) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:79) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:217) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:248) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:233) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.10.jar:3.10]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:366) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:357) [netty-all-4.0.39.Final.jar:4.0.39.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.10.jar:3.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.10.jar:3.10]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]

{code}

Even if my FUNCTION only returns state, it creates that assertion null.

Thank you in advance.",N/A,3.11.0
CASSANDRA-13397,Return value of CountDownLatch.await() not being checked in Repair,"While looking into repair code, I realize that we should check return value of CountDownLatch.await(). Most of the places that we don't check the return value, nothing bad would happen due to other protection. However, ActiveRepairService#prepareForRepair should have the check. Code to reproduce:
{code}
    public static void testLatch() throws InterruptedException {
        CountDownLatch latch = new CountDownLatch(2);
        latch.countDown();

        new Thread(() -> {
            try {
                Thread.sleep(1200);
            } catch (InterruptedException e) {
                System.err.println(""interrupted"");
            }
            latch.countDown();
            System.out.println(""counted down"");
        }).start();


        latch.await(1, TimeUnit.SECONDS);
        if (latch.getCount() > 0) {
            System.err.println(""failed"");
        } else {
            System.out.println(""success"");
        }
    }
{code}",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13396,Cassandra 3.10: ClassCastException in ThreadAwareSecurityManager,https://www.mail-archive.com/user@cassandra.apache.org/msg51603.html,N/A,"3.11.3, 4.0-alpha1, 4.0"
CASSANDRA-13395,Expired rows without regular column data can crash upgradesstables,"In {{2.x}} if an expired row is compacted its row marker will be converted into a {{DeletedCell}}. In {{3.0}}, when the row is read by {{LegacyLayout}} it will be converted in a row without {{PrimaryKeyLivenessInfo}}. If the row does not contains any data for the regular columns, or if the table simply has no regular columns it will then be considered as {{empty}}. Which will crash {{upgradesstables}} with the following error:
{code}
java.lang.AssertionError
        at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70)
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:207)
        at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:116)
        at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:107)
        at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41)
        at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:156)
        at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:122)
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:147)
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125)
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57)
        at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109)
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:416)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:308)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$0(NamedThreadFactory.java:79)
        at java.lang.Thread.run(Thread.java:745)
{code}
This problem is cause",N/A,"3.0.13, 3.11.0"
CASSANDRA-13393,Fix weightedSize() for row-cache reported by JMX and NodeTool,"Row Cache size is reported in entries but should be reported in bytes (as KeyCache do).
It happens because incorrect OHCProvider.OHCacheAdapter.weightedSize method. Currently it returns cache size but should return ohCache.memUsed()",N/A,"2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13392,Repaired status should be cleared on new sstables when issuing nodetool refresh,We can't assume that new sstables added when doing nodetool refresh (ColumnFamilyStore#loadNewSSTables) are actually repaired if they have the repairedAt flag set,N/A,"3.0.13, 3.11.5, 4.1-alpha1, 4.1"
CASSANDRA-13389,Possible NPE on upgrade to 3.0/3.X in case of IO errors,"There is a NPE on upgrade to 3.0/3.X if a data directory contains directories that generate IO errors, for example if the cassandra process does not have permission to read them.

Here is the exception:

{code}
ERROR [main] 2017-03-06 16:41:30,678  CassandraDaemon.java:710 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.io.util.FileUtils.delete(FileUtils.java:372) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.db.SystemKeyspace.migrateDataDirs(SystemKeyspace.java:1359) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:190) ~[cassandra-all-3.0.11.1564.jar:3.0.11.1564]
{code}

This is caused by {{File.listFiles()}}, which returns null in case of an IO error.",N/A,"3.0.13, 3.11.0"
CASSANDRA-13388,Add hosted CI config files (such as Circle and TravisCI) for easier developer testing,"We currently require unit tests and dtests to accept a patch, but it's not easy for most contributors to actually execute these tests in a way that's visible for the reviewer / other JIRA users.

We should push some standard config files into the various branches to facilitate easier testing. 

I propose we start with TravisCI and CircleCI, because:

- Travis has limited free support for developers working on OSS projects, and is already an accepted vendor at the ASF (apparently the ASF pays for 30 concurrent tests already), and
- CircleCI is also free for developers working on OSS projects, and has slightly more flexibility than Travis in terms of the number of free workers and durations.

Both are enabled by pushing a single YAML file into each branch to configure jobs, and require each individual developer who WANTS to run tests to link their github account to the vendor. Developers who don't want to use this functionality can simply ignore it - they're effectively no-ops for anyone who's not already using those services.

Are there any others we should consider including? ",N/A,"2.1.18, 2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13384,Legacy caching options can prevent 3.0 upgrade,"In 2.1, we wrote caching options as a JSONified map, but we tolerated raw strings [""ALL"", ""KEYS_ONLY"", ""ROWS_ONLY"", and ""NONE""|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/cache/CachingOptions.java#L42].

If a 2.1 node with any of these strings is upgraded to 3.0, the legacy schema migration will fail.",N/A,"3.0.13, 3.11.0"
CASSANDRA-13382,cdc column addition strikes again,"This is a followup of CASSANDRA-12697, where the patch mistakenly only handled the {{system_schema.tables}} table, while the {{cdc}} column has been added to {{system_schema.views}} table.

The patch is pretty trivial, though this highlight that we don't seem to have upgrade tests for materialized views.",N/A,3.11.0
CASSANDRA-13378,DS Cassandra3.0  Adding a datacenter to a cluster procedure not working for us,"i have replicated the issue on my personal cluster using VMs.
we have many keyspaces and users developing on the Dev Cluster we are trying to stretch.
With my current VMs (10 total) i can create a 2 DC cluster  dc1 and dc2.
i rebuild all nodes - clear data dirs and restart dc1.
i also clear data dirs on dc2 nodes,  i donot restart yet,
now i have single dc1 5 nodes cluster.
snitch - GossipingPropertyFileSnitch
i create keyspaces on DC1
after i alter keyspaces with replication dc1 3   dc2 0    i can no longer query tables - not enough replicas available for query at consistency ONE.
same error with CQL using consistency local_one
continue with procedure, startup dc2 nodes,  
alter replication on keyspaces to  dc1 3  dc2 2
from dc2 nodes , nodetool rebuild -- dc1  fails

i am attaching detailed steps with errors  and cassandra.yaml
",N/A,3.0.9
CASSANDRA-13375,Cassandra v3.10 - UDA throws AssertionError when no records in select ,"If the aggregate function is used in a select which fetches no records there is a ServerError reported. 

It seems the bug is in org.apache.cassandra.cql3.functions.UDAggregate, there is an  ""assert !needsInit;"" done on compute. 

Attached is a file which creates a keyspace / table / data records and UDF/UDA to replicate the issue. ",N/A,3.11.0
CASSANDRA-13371,Remove legacy auth tables support,"Starting with Cassandra 3.0, we include support for converting pre CASSANDRA-7653 user tables, until they will be dropped by the operator. Converting e.g. permissions happens by simply copying all of them from {{permissions}} -> {{role_permissions}}, until the {{permissions}} table has been dropped.

Upgrading to 4.0 will only be possible from 3.0 upwards, so I think it's safe to assume that the new permissions table has already been populated, whether the old table was dropped or not. Therefor I'd suggest to just get rid of the legacy support.",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13370,unittest CipherFactoryTest failed on MacOS,"Seems like MacOS(El Capitan) doesn't allow writing to {{/dev/urandom}}:
{code}
$ echo 1 > /dev/urandom
echo: write error: operation not permitted
{code}
Which is causing CipherFactoryTest failed:
{code}
$ ant test -Dtest.name=CipherFactoryTest
...
    [junit] Testsuite: org.apache.cassandra.security.CipherFactoryTest
    [junit] Testsuite: org.apache.cassandra.security.CipherFactoryTest Tests run: 7, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 2.184 sec
    [junit]
    [junit] Testcase: buildCipher_SameParams(org.apache.cassandra.security.CipherFactoryTest):  Caused an ERROR
    [junit] setSeed() failed
    [junit] java.security.ProviderException: setSeed() failed
    [junit]     at sun.security.provider.NativePRNG$RandomIO.implSetSeed(NativePRNG.java:472)
    [junit]     at sun.security.provider.NativePRNG$RandomIO.access$300(NativePRNG.java:331)
    [junit]     at sun.security.provider.NativePRNG.engineSetSeed(NativePRNG.java:214)
    [junit]     at java.security.SecureRandom.getDefaultPRNG(SecureRandom.java:209)
    [junit]     at java.security.SecureRandom.<init>(SecureRandom.java:190)
    [junit]     at org.apache.cassandra.security.CipherFactoryTest.setup(CipherFactoryTest.java:50)
    [junit] Caused by: java.io.IOException: Operation not permitted
    [junit]     at java.io.FileOutputStream.writeBytes(Native Method)
    [junit]     at java.io.FileOutputStream.write(FileOutputStream.java:313)
    [junit]     at sun.security.provider.NativePRNG$RandomIO.implSetSeed(NativePRNG.java:470)
...
{code}

I'm able to reproduce the issue on two Mac machines. But not sure if it's affecting all other developers.

{{-Djava.security.egd=file:/dev/urandom}} was introduced in:
CASSANDRA-9581

I would suggest to revert the [change|https://github.com/apache/cassandra/commit/ae179e45327a133248c06019f87615c9cf69f643] as {{pig-test}} is removed ([pig is no longer supported|https://github.com/apache/cassandra/commit/56cfc6ea35d1410f2f5a8ae711ae33342f286d79]).
Or adding a condition for MacOS in build.xml.

[~aweisberg] [~jasobrown] any thoughts?",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13369,"If there are multiple values for a key, CQL grammar choses last value. This should not be silent or should not be allowed.","If through CQL, multiple values are specified for a key, grammar parses the map and last value for the key wins. This behavior is bad.
e.g. 
{code}
CREATE KEYSPACE Excalibur WITH REPLICATION = {'class': 'NetworkTopologyStrategy', 'dc1': 2, 'dc1': 5};
{code}

Parsing this statement, 'dc1' gets RF = 5. This can be catastrophic, may even result in loss of data. This behavior should not be silent or not be allowed at all.  
",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13366,Possible AssertionError in UnfilteredRowIteratorWithLowerBound,"In the code introduced by CASSANDRA-8180, we build a lower bound for a partition (sometimes) based on the min clustering values of the stats file. We can't do that if the sstable has and range tombston marker and the code does check that this is the case, but unfortunately the check is done using the stats {{minLocalDeletionTime}} but that value isn't populated properly in pre-3.0. This means that if you upgrade from 2.1/2.2 to 3.4+, you may end up getting an exception like
{noformat}
WARN  [ReadStage-2] 2017-03-20 13:29:39,165  AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.AssertionError: Lower bound [INCL_START_BOUND(Foo, -9223372036854775808, -9223372036854775808) ]is bigger than first returned value [Marker INCL_START_BOUND(Foo)@1490013810540999] for sstable /var/lib/cassandra/data/system/size_estimates-618f817b005f3678b8a453f3930b8e86/system-size_estimates-ka-1-Data.db
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:122)
{noformat}
and this until the sstable is upgraded.",N/A,3.11.0
CASSANDRA-13364,"Cqlsh COPY fails importing Map<String,List<String>>, ParseError unhashable type list","When importing data with the _COPY_ command into a column family that has a _map<text, frozen<list<text>>>_ field, I get a _unhashable type: 'list'_ error. Here is how to reproduce:

{code}
CREATE TABLE table1 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<list<text>>>
);

insert into table1 (col1, col2map) values (1, {'key': ['value1']});

cqlsh:ks> copy table1 to 'table1.csv';


table1.csv file content:
1,{'key': ['value1']}


cqlsh:ks> copy table1 from 'table1.csv';
...
Failed to import 1 rows: ParseError - Failed to parse {'key': ['value1']} : unhashable type: 'list',  given up without retries
Failed to process 1 rows; failed rows written to kv_table1.err
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.420 seconds (0 skipped).
{code}

But it works fine for Map<String, Set<String>>.

{code}
CREATE TABLE table2 (
    col1 int PRIMARY KEY,
    col2map map<text, frozen<set<text>>>
);

insert into table2 (col1, col2map) values (1, {'key': {'value1'}});

cqlsh:ks> copy table2 to 'table2.csv';


table2.csv file content:
1,{'key': {'value1'}}


cqlsh:ks> copy table2 from 'table2.csv';
Processed: 1 rows; Rate:       2 rows/s; Avg. rate:       2 rows/s
1 rows imported from 1 files in 0.417 seconds (0 skipped).
{code}

The exception seems to arrive in _convert_map_ function in _ImportConversion_ class inside _copyutil.py_.",N/A,"2.1.18, 2.2.10, 3.0.13, 3.11.0"
CASSANDRA-13363,Fix racy read command serialization,"Constantly see this error in the log without any additional information or a stack trace.

{code}
Exception in thread Thread[MessagingService-Incoming-/10.0.1.26,5,main]
{code}

{code}
java.lang.ArrayIndexOutOfBoundsException: null
{code}

Logger: org.apache.cassandra.service.CassandraDaemon
Thrdead: MessagingService-Incoming-/10.0.1.12
Method: uncaughtException
File: CassandraDaemon.java
Line: 229",N/A,"3.0.15, 3.11.1"
CASSANDRA-13352,Cassandra does not respond back in 12000ms,"I have my system with a table design, 

Create Table lamscope_dashboard.events (
ToolId text,
Date timestamp,
End_Time timestamp,
DeviceId text,
Logtype text,
EventId text,
MaterialId text,
CfgId text,
MaterialType text,
Status text,
SlotNo text,
LotId text,
RecipeId text,
StepNum int ,
Fromdevice text,
Fromslot text,
ToDevice text,
ToSlot int ,
FlowRecipeId text,
Flowinfo text,
CarrierId text,
JobId text,
Data text,
PRIMARY KEY( ToolId, Date, MaterialId, DeviceId, EventId))
WITH CLUSTERING ORDER BY (Date ASC,MaterialId ASC, DeviceId ASC)
AND COMPRESSION = { 'sstable_compression' : 'SnappyCompressor'} ;

Query 
select * from events where eventid='xxxxx' allow filtering;
It is a single instance cluster.
System is not responding back in 12000 ms. 
Query goes timeout.

",N/A,3.10
CASSANDRA-13348,Duplicate tokens after bootstrap,"This one is a bit scary, and probably results in data loss. After a bootstrap of a few new nodes into an existing cluster, two new nodes have chosen some overlapping tokens.

In fact, of the 256 tokens chosen, 51 tokens were already in use on the other node.

Node 1 log :
{noformat}
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,461 StorageService.java:1160 - JOINING: waiting for ring information
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,461 StorageService.java:1160 - JOINING: waiting for schema information to complete
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,461 StorageService.java:1160 - JOINING: schema complete, ready to bootstrap
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,462 StorageService.java:1160 - JOINING: waiting for pending range calculation
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,462 StorageService.java:1160 - JOINING: calculation complete, ready to bootstrap
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,462 StorageService.java:1160 - JOINING: getting bootstrap token
WARN  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,564 TokenAllocation.java:61 - Selected tokens [............, 2959334889475814712, 3727103702384420083, 7183119311535804926, 6013900799616279548, -1222135324851761575, 1645259890258332163, -1213352346686661387, 7604192574911909354]
WARN  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,729 TokenAllocation.java:65 - Replicated node load in datacentre before allocation max 1.00 min 1.00 stddev 0.0000
WARN  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,729 TokenAllocation.java:66 - Replicated node load in datacentre after allocation max 1.00 min 1.00 stddev 0.0000
WARN  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:43,729 TokenAllocation.java:70 - Unexpected growth in standard deviation after allocation.
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:42:44,150 StorageService.java:1160 - JOINING: sleeping 30000 ms for pending range setup
INFO  [RMI TCP Connection(107)-127.0.0.1] 2017-03-09 07:43:14,151 StorageService.java:1160 - JOINING: Starting to bootstrap...
{noformat}

Node 2 log:
{noformat}
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:51,937 StorageService.java:971 - Joining ring by operator request
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,513 StorageService.java:1160 - JOINING: waiting for ring information
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,513 StorageService.java:1160 - JOINING: waiting for schema information to complete
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,513 StorageService.java:1160 - JOINING: schema complete, ready to bootstrap
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,513 StorageService.java:1160 - JOINING: waiting for pending range calculation
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,514 StorageService.java:1160 - JOINING: calculation complete, ready to bootstrap
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,514 StorageService.java:1160 - JOINING: getting bootstrap token
WARN  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,630 TokenAllocation.java:61 - Selected tokens [......, 2890709530010722764, -2416006722819773829, -5820248611267569511, -5990139574852472056, 1645259890258332163, 9135021011763659240, -5451286144622276797, 7604192574911909354]
WARN  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,794 TokenAllocation.java:65 - Replicated node load in datacentre before allocation max 1.02 min 0.98 stddev 0.0000
WARN  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:52,795 TokenAllocation.java:66 - Replicated node load in datacentre after allocation max 1.00 min 1.00 stddev 0.0000
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:55:53,149 StorageService.java:1160 - JOINING: sleeping 30000 ms for pending range setup
INFO  [RMI TCP Connection(380)-127.0.0.1] 2017-03-17 15:56:23,149 StorageService.java:1160 - JOINING: Starting to bootstrap...
{noformat}

eg. 7604192574911909354 has been chosen by both.

The joins were eight days apart, so I don't think it's a race :)",N/A,3.0.18
CASSANDRA-13347,dtest failure in upgrade_tests.upgrade_through_versions_test.TestUpgrade_current_2_2_x_To_indev_3_0_x.rolling_upgrade_test,"example failure:

http://cassci.datastax.com/job/cassandra-3.0_large_dtest/58/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_2_x_To_indev_3_0_x/rolling_upgrade_test

{code}
Error Message

Subprocess ['nodetool', '-h', 'localhost', '-p', '7100', ['upgradesstables', '-a']] exited with non-zero status; exit status: 2; 
stderr: error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:197)
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:116)
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:107)
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41)
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:156)
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:122)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:147)
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125)
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:415)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:307)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)
{code}{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 279, in rolling_upgrade_test
    self.upgrade_scenario(rolling=True)
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 345, in upgrade_scenario
    self.upgrade_to_version(version_meta, partial=True, nodes=(node,))
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_through_versions_test.py"", line 446, in upgrade_to_version
    node.nodetool('upgradesstables -a')
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 789, in nodetool
    return handle_external_tool_process(p, ['nodetool', '-h', 'localhost', '-p', str(self.jmx_port), cmd.split()])
  File ""/home/automaton/venv/local/lib/python2.7/site-packages/ccmlib/node.py"", line 2002, in handle_external_tool_process
    raise ToolError(cmd_args, rc, out, err)
{code}

Related failures:

http://cassci.datastax.com/job/cassandra-3.0_large_dtest/58/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_1_x_To_indev_3_0_x/rolling_upgrade_with_internode_ssl_test/

http://cassci.datastax.com/job/cassandra-3.0_large_dtest/58/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_2_x_To_indev_3_0_x/rolling_upgrade_with_internode_ssl_test/

http://cassci.datastax.com/job/cassandra-3.0_large_dtest/58/testReport/upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_2_1_x_To_indev_3_0_x/rolling_upgrade_test/",N/A,"3.0.13, 3.11.0"
CASSANDRA-13346,Failed unregistering mbean during drop keyspace,"All node throw exceptions about materialized views during drop keyspace:
{code}

WARN  [MigrationStage:1] 2017-03-16 16:54:25,016 ColumnFamilyStore.java:535 - Failed unregistering mbean: org.apache.cassandra.db:type=Tables,keyspace=test20160810,table=unit_by_account
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106) ~[na:1.8.0_121]
        at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097) ~[na:1.8.0_121]
        at java.util.concurrent.ConcurrentHashMap$KeySetView.remove(ConcurrentHashMap.java:4569) ~[na:1.8.0_121]
        at org.apache.cassandra.metrics.TableMetrics.release(TableMetrics.java:712) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.unregisterMBean(ColumnFamilyStore.java:570) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:527) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:517) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.Keyspace.unloadCf(Keyspace.java:365) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.Keyspace.dropCf(Keyspace.java:358) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.config.Schema.dropView(Schema.java:744) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.schema.SchemaKeyspace.lambda$mergeSchema$373(SchemaKeyspace.java:1287) [apache-cassandra-3.9.0.jar:3.9.0]
        at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1287) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1256) [apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:51) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.9.0.jar:3.9.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_121]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{code}
",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13343,Wrong logger name in AnticompactionTask,"We have the below code in AnticompactionTask.java. The parameter is wrong.
{code}
private static Logger logger = LoggerFactory.getLogger(RepairSession.class);
{code}",N/A,"2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13341,Legacy deserializer can create empty range tombstones,"Range tombstones in the 2.x file format is a bit far-westy so you can actually get sequences of range tombstones like {{\[1, 4\]@3 \[1, 10\]@5}}. But the current legacy deserializer doesn't handle this correctly. On the first range, it will generate a {{INCL_START(1)@3}} open marker, but upon seeing the next tombstone it will decide to close the previously opened range and re-open with deletion time 5, so will generate {{EXCL_END_INCL_START(1)@3-5}}. That result in the first range being empty, which break future assertions in the code.",N/A,"3.0.13, 3.11.0"
CASSANDRA-13340,Bugs handling range tombstones in the sstable iterators,"There is 2 bugs in the way sstable iterators handle range tombstones:
# empty range tombstones can be returned due to a strict comparison that shouldn't be.
# the sstable reversed iterator can actually return completely bogus results when range tombstones are spanning multiple index blocks.

The 2 bugs are admittedly separate but as they both impact the same area of code and are both range tombstones related, I suggest just fixing both here (unless something really really mind).

Marking the ticket critical mostly for the 2nd bug: it can truly make use return bad results on reverse queries.",N/A,"3.0.13, 3.11.0"
CASSANDRA-13337,"Dropping column results in ""corrupt"" SSTable","It seems like dropping a column can make SSTables containing rows with writes to only the dropped column will become uncompactable.

Also Cassandra <= 3.9 and <= 3.0.11 will even refuse to start with the same stack trace

{code}
cqlsh -e ""create keyspace test with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }""
cqlsh -e ""create table test.test(pk text primary key, x text, y text)""

cqlsh -e ""update test.test set x='1' where pk='1'""
nodetool flush

cqlsh -e ""update test.test set x='1', y='1' where pk='1'""
nodetool flush
cqlsh -e ""alter table test.test drop x""

nodetool compact test test
error: Corrupt empty row found in unfiltered partition
-- StackTrace --
java.io.IOException: Corrupt empty row found in unfiltered partition
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:382)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:87)
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$CurrentFormatIterator.computeNext(SSTableSimpleIterator.java:65)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.doCompute(SSTableIdentityIterator.java:123)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.computeNext(SSTableIdentityIterator.java:100)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.computeNext(SSTableIdentityIterator.java:30)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95)
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:369)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:189)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:158)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:509)
	at org.apache.cassandra.db.rows.UnfilteredRowIterators$UnfilteredRowMergeIterator.computeNext(UnfilteredRowIterators.java:369)
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47)
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129)
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:58)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:67)
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:26)
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96)
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:227)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:190)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$8.runMayThrow(CompactionManager.java:610)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)

{code}",N/A,"3.0.13, 3.11.0"
CASSANDRA-13329,max_hints_delivery_threads does not work,"HintsDispatchExecutor creates JMXEnabledThreadPoolExecutor with corePoolSize  == 1 and maxPoolSize==max_hints_delivery_threads and unbounded LinkedBlockingQueue.

In this configuration additional threads will not be created.

Same problem with PerSSTableIndexWriter.",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13326,Support unaligned memory access for AArch64,ARMv8 (AArch64)  supports unaligned memory access. The patch will enable it and will improve performance on AArch64,N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13323,IncomingTcpConnection closed due to one bad message,"We got this exception:
{code}
WARN  [MessagingService-Incoming-/****] 2017-02-14 17:33:33,177 IncomingTcpConnection.java:101 - UnknownColumnFamilyException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find table for cfId 2a3ab630-df74-11e6-9f81-b56251e1559e. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
    at org.apache.cassandra.config.CFMetaData$Serializer.deserialize(CFMetaData.java:1336) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:660) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:635) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:131) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:113) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.net.MessageIn.read(MessageIn.java:98) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:201) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:178) ~[apache-cassandra-3.0.10.jar:3.0.10]
    at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:92) ~[apache-cassandra-3.0.10.jar:3.0.10]
{code}

Also we saw this log in another host indicating it needs to re-connect:
{code}
INFO  [HANDSHAKE-/****] 2017-02-21 13:37:50,216 OutboundTcpConnection.java:515 - Handshaking version with /****
{code}

The reason is that the node was receiving hinted data for a dropped table. This may happen with other messages as well. On Cassandra side, IncomingTcpConnection shouldn't close on just one bad message, even though it will be restarted soon later by SocketThread in MessagingService.",N/A,3.0.15
CASSANDRA-13320,upgradesstables fails after upgrading from 2.1.x to 3.0.11,"I tried to execute {{nodetool upgradesstables}} after upgrading cluster from 2.1.16 to 3.0.11, but it fails when upgrading a table with 2i.

This problem can be reproduced as follows.
{code}
$ ccm create test -v 2.1.16 -n 1 -s
$ ccm node1 cqlsh  -e ""CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':1}""
$ ccm node1 cqlsh  -e ""CREATE TABLE test.test(k1 text, k2 text, PRIMARY KEY( k1 ));""
$ ccm node1 cqlsh  -e ""CREATE INDEX k2 ON test.test(k2);""
 
$ ccm node1 cqlsh  -e ""INSERT INTO test.test (k1, k2 ) VALUES ( 'a', 'a') ;""
$ ccm node1 cqlsh  -e ""INSERT INTO test.test (k1, k2 ) VALUES ( 'a', 'b') ;""
 
$ ccm node1 nodetool flush
 
$ for i in `seq 1 `; do ccm node${i} stop; ccm node${i} setdir -v3.0.11;ccm node${i} start; done
$ ccm node1 nodetool upgradesstables test test
Traceback (most recent call last):
  File ""/home/y/bin/ccm"", line 86, in <module>
    cmd.run()
  File ""/home/y/lib/python2.7/site-packages/ccmlib/cmds/node_cmds.py"", line 267, in run
    stdout, stderr = self.node.nodetool("" "".join(self.args[1:]))
  File ""/home/y/lib/python2.7/site-packages/ccmlib/node.py"", line 742, in nodetool
    raise NodetoolError("" "".join(args), exit_status, stdout, stderr)
ccmlib.node.NodetoolError: Nodetool command '/home/zzheng/.ccm/repository/3.0.11/bin/nodetool -h localhost -p 7100 upgradesstables test test' failed; exit status: 2; stderr: WARN  06:29:08 Only 10476 MB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:197)
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:116)
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:107)
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41)
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:156)
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:122)
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:147)
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:125)
	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.realAppend(DefaultCompactionWriter.java:57)
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:109)
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:195)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61)
	at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:415)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:307)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:79)
	at java.lang.Thread.run(Thread.java:745)
{code}

The result of dumping the 2i sstable is as follows.
{code}
[
{""key"": ""a"",
 ""cells"": [[""61"",1488961273,1488961269822817,""d""]]},
{""key"": ""b"",
 ""cells"": [[""61"","""",1488961273015759]]}
]
{code}

This problem is occurred by the tombstone row. When this row is processed in {{LegacyLayout.java}}, it will be treated as a row maker.
https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/LegacyLayout.java#L1195
Then the deletion info will be lost.

As a result, the row will be a empty row, which causes the assertion error.

To avoid this, I added the code to add row deletion info when the row is a tombstone and *not* a row marker, and it works as I expect, which means that {{upgradesstables}} succeeds and row deletion info is remained.

However I don't understand whether this change will cause another problem. Anyway, I submit my patch as a reference.",N/A,"3.0.13, 3.11.0"
CASSANDRA-13317,"Default logging we ship will incorrectly print ""?:?"" for ""%F:%L"" pattern due to includeCallerData being false by default no appender","We specify the logging pattern as ""%-5level [%thread] %date{ISO8601} %F:%L - %msg%n"". 

%F:%L is intended to print the Filename:Line Number. For performance reasons logback (like log4j2) disables tracking line numbers as it requires the entire stack to be materialized every time.

This causes logs to look like:
WARN  [main] 2017-03-09 13:27:11,272 ?:? - Protocol Version 5/v5-beta not supported by java driver
INFO  [main] 2017-03-09 13:27:11,813 ?:? - No commitlog files found; skipping replay
INFO  [main] 2017-03-09 13:27:12,477 ?:? - Initialized prepared statement caches with 14 MB
INFO  [main] 2017-03-09 13:27:12,727 ?:? - Initializing system.IndexInfo

When instead you'd expect something like:
INFO  [main] 2017-03-09 13:23:44,204 ColumnFamilyStore.java:419 - Initializing system.available_ranges
INFO  [main] 2017-03-09 13:23:44,210 ColumnFamilyStore.java:419 - Initializing system.transferred_ranges
INFO  [main] 2017-03-09 13:23:44,215 ColumnFamilyStore.java:419 - Initializing system.views_builds_in_progress

The fix is to add ""<includeCallerData>true</includeCallerData>"" to the appender config to enable the line number and stack tracing.",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13316,Build error because of dependent jar (byteman-install-3.0.3.jar) currupted ,"When build  cassandra 3.10 on amd64, CentOS Linux 7, there is a build error caused by corrupted jar file (byteman-install-3.0.3.jar).

Here is the replicated steps:

After install necessary dependent packages and apache-ant, git clone cassandra 3.10:

1)
  git clone https://github.com/apache/cassandra.git
  cd cassandra
  git checkout cassandra-3.10
  ant 

Then gets errors like:
""
build-project:
     [echo] apache-cassandra: /cassandra/build.xml
    [javac] Compiling 45 source files to /cassandra/build/classes/thrift
    [javac] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
    [javac] error: error reading /cassandra/build/lib/jars/byteman-install-3.0.3.jar; error in opening zip file
    [javac] Compiling 1474 source files to /cassandra/build/classes/main
    [javac] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
    [javac] error: error reading /cassandra/build/lib/jars/byteman-install-3.0.3.jar; error in opening zip file
    [javac] Creating empty /cassandra/build/classes/main/org/apache/cassandra/hints/package-info.class 
""
2) 
To check the jar and get:

# jar -i /cassandra/build/lib/jars/byteman-install-3.0.3.jar
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
java.util.zip.ZipException: error in opening zip file
        at java.util.zip.ZipFile.open(Native Method)
        at java.util.zip.ZipFile.<init>(ZipFile.java:219)
        at java.util.zip.ZipFile.<init>(ZipFile.java:149)
        at java.util.jar.JarFile.<init>(JarFile.java:166)
        at java.util.jar.JarFile.<init>(JarFile.java:103)
        at sun.tools.jar.Main.getJarPath(Main.java:1163)
        at sun.tools.jar.Main.genIndex(Main.java:1195)
        at sun.tools.jar.Main.run(Main.java:317)
        at sun.tools.jar.Main.main(Main.java:1288)

3) if download the jar and replace it, the build will be successful.

wget http://downloads.jboss.org/byteman/3.0.3/byteman-download-3.0.3-bin.zip
  unzip byteman-download-3.0.3-bin.zip -d /tmp
  rm  -f  build/lib/jars/byteman-install-3.0.3.jar
  cp  /tmp/byteman-download-3.0.3/lib/byteman-install.jar build/lib/jars/byteman-install-3.0.3.jar
  
  ant

....
BUILD SUCCESSFUL
Total time: 36 seconds
",N/A,"2.2.10, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13313,Compaction leftovers not removed on upgrade 2.1/2.2 -> 3.0,Before 3.0 we used sstable ancestors to figure out if an sstable was left over after a compaction. In 3.0 the ancestors are ignored and instead we use LogTransaction files to figure it out. 3.0 should still clean up 2.1/2.2 compaction leftovers using the on-disk sstable ancestors when available.,N/A,"3.0.21, 3.11.7"
CASSANDRA-13311,cqlsh doesn't let you connect to an older major version,"When trying to connect to an older version of cassandra from trunk I get the following error even though I am setting the correct version with --cqlversion

bq. ProtocolError returned from server while using explicitly set client protocol_version 4

If I go in to cqlsh.py and change 
DEFAULT_PROTOCOL_VERSION = 3

it works.

",N/A,3.0.13
CASSANDRA-13309,i couldnot able to run the cqlsh service.i am getting an syntax error in cqlsh.py file when i was trying to run cqlsh from bin folder,"C:\Program Files\apache-cassandra-3.0.11\bin>cqlsh
 File ""C:\Program Files\apache-cassandra-3.0.11\bin\\cqlsh.py"", line 141
    except ImportError, e:
                      ^
SyntaxError: invalid syntax
C:\Program Files\apache-cassandra-3.0.11\bin>",N/A,3.0.22
CASSANDRA-13308,"Gossip breaks, Hint files not being deleted on nodetool decommission","How to reproduce the issue I'm seeing:
Shut down Cassandra on one node of the cluster and wait until we accumulate a ton of hints. Start Cassandra on the node and immediately run ""nodetool decommission"" on it.

The node streams its replicas and marks itself as DECOMMISSIONED, but other nodes do not seem to see this message. ""nodetool status"" shows the decommissioned node in state ""UL"" on all other nodes (it is also present in system.peers), and Cassandra logs show that gossip tasks on nodes are not proceeding (number of pending tasks keeps increasing). Jstack suggests that a gossip task is blocked on hints dispatch (I can provide traces if this is not obvious). Because the cluster is large and there are a lot of hints, this is taking a while. 

On inspecting ""/var/lib/cassandra/hints"" on the nodes, I see a bunch of hint files for the decommissioned node. Documentation seems to suggest that these hints should be deleted during ""nodetool decommission"", but it does not seem to be the case here. This is the bug being reported.

To recover from this scenario, if I manually delete hint files on the nodes, the hints dispatcher threads throw a bunch of exceptions and the decommissioned node is now in state ""DL"" (perhaps it missed some gossip messages?). The node is still in my ""system.peers"" table

Restarting Cassandra on all nodes after this step does not fix the issue (the node remains in the peers table). In fact, after this point the decommissioned node is in state ""DN""",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13307,The specification of protocol version in cqlsh means the python driver doesn't automatically downgrade protocol version.,"Hi,
Looks like we've regressed on the issue described in:
https://issues.apache.org/jira/browse/CASSANDRA-9467
In that we're no longer able to connect from newer cqlsh versions
(e.g trunk) to older versions of Cassandra with a lower version of the protocol (e.g 2.1 with protocol version 3)

The problem seems to be that we're relying on the ability for the client to automatically downgrade protocol version implemented in Cassandra here:
https://issues.apache.org/jira/browse/CASSANDRA-12838
and utilised in the python client here:
https://datastax-oss.atlassian.net/browse/PYTHON-240

The problem however comes when we implemented:
https://datastax-oss.atlassian.net/browse/PYTHON-537
""Don't downgrade protocol version if explicitly set"" 
(included when we bumped from 3.5.0 to 3.7.0 of the python driver as part of fixing: https://issues.apache.org/jira/browse/CASSANDRA-11534)

Since we do explicitly specify the protocol version in the bin/cqlsh.py.

I've got a patch which just adds an option to explicitly specify the protocol version (for those who want to do that) and then otherwise defaults to not setting the protocol version, i.e using the protocol version from the client which we ship, which should by default be the same protocol as the server.
Then it should downgrade gracefully as was intended. 
Let me know if that seems reasonable.
Thanks,
Matt
",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13305,Slice.isEmpty() returns false for some empty slices,"{{Slice.isEmpty}} is currently defined as {{comparator.compare(end, start) < 0}} but this shouldn't be a strict inequality. Indeed, the way {{Slice.Bound}} is defined, having a start equal to an end implies a range like {{[1, 1)}}, but that range is definitively empty and something we shouldn't let in as that would break merging and other range tombstone related code.

In practice, if you can currently insert such empty range (with something like {{DELETE FROM t WHERE k = 'foo' AND i >= 1 AND i < 1}}), and that can trigger assertions in {{RangeTomstoneList}} (and possibly other problem).",N/A,"3.0.13, 3.11.0"
CASSANDRA-13302,last row of previous page == first row of next page while querying data using SASI index,"Apologies if this is a duplicate (couldn't track down an existing bug).

Similarly to [CASSANDRA-11208], it appears it is possible to retrieve duplicate rows when paging using a SASI index as documented in [JAVA-1413|https://datastax-oss.atlassian.net/browse/JAVA-1413], the following test demonstrates that data is repeated while querying using a SASI index:

{code:java}
public class TestPagingBug
{
	public static void main(String[] args)
	{
		Cluster.Builder builder = Cluster.builder();
		Cluster c = builder.addContactPoints(""192.168.98.190"").build();		
		Session s = c.connect();
		
		s.execute(""CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 }"");
		s.execute(""CREATE TABLE IF NOT EXISTS test.test_table_sec(sec BIGINT PRIMARY KEY, id INT)"");
                //create secondary index on ID column, used for select statement
                String index = ""CREATE CUSTOM INDEX test_table_sec_idx ON test.test_table_sec (id) USING 'org.apache.cassandra.index.sasi.SASIIndex' ""
                + ""WITH OPTIONS = { 'mode': 'PREFIX' }"";
                s.execute(index);
		
		PreparedStatement insert = s.prepare(""INSERT INTO test.test_table_sec (id, sec) VALUES (1, ?)"");		
		for (int i = 0; i < 1000; i++)
			s.execute(insert.bind((long) i));
		
		PreparedStatement select = s.prepare(""SELECT sec FROM test.test_table_sec WHERE id = 1"");
		
		long lastSec = -1;		
		for (Row row : s.execute(select.bind().setFetchSize(300)))
		{
			long sec = row.getLong(""sec"");
			if (sec == lastSec)
				System.out.println(String.format(""Duplicated id %d"", sec));
			
			lastSec = sec;
		}
		System.exit(0);
	}
}
{code}

The program outputs the following:

{noformat}
Duplicated id 23
Duplicated id 192
Duplicated id 684
{noformat}

Note that the simple primary key is required to reproduce this.",N/A,"3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-13298,DataOutputBuffer.asNewBuffer broken,"The implementation of {{DataOutputBuffer.asNewBuffer()}} reuses the underlying {{ByteBuffer}} array. This is probably not an issue, but it is definitely incorrect and may lead to incorrect/overwritten information returned by {{SystemKeyspace.truncationAsMapEntry}}.",N/A,3.11.0
CASSANDRA-13294,Possible data loss on upgrade 2.1 - 3.0,"After finishing a compaction we delete the compacted away files. This is done [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/lifecycle/LogFile.java#L328-L337] which uses [this|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/db/lifecycle/LogRecord.java#L265-L271] to get the files - we get all files starting with {{absoluteFilePath}}. Absolute file path is generated [here|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/io/sstable/Descriptor.java#L142-L153]. For 3.0 version files the filename looks like this: {{/blabla/keyspace1/standard1-bdb031c0ff7b11e6940fdd0479dd8912/mc-1332-big}} but for 2.1 version files, they look like this: {{/blabla/keyspace1/standard1-bdb031c0ff7b11e6940fdd0479dd8912/keyspace1-standard1-ka-2}}.

The problem is then that if we were to finish a compaction including the legacy file, we would actually delete all legacy files having a generation starting with '2'",N/A,"3.0.12, 3.11.0"
CASSANDRA-13282,Commitlog replay may fail if last mutation is within 4 bytes of end of segment,"Following CASSANDRA-9749 , stricter correctness checks on commitlog replay can incorrectly detect ""corrupt segments"" and stop commitlog replay (and potentially stop cassandra, depending on the configured policy). In {{CommitlogReplayer#replaySyncSection}} we try to read a 4 byte int {{serializedSize}}, and if it's 0 (which will happen due to zeroing when the segment was created), we continue on to the next segment. However, it appears that if a mutation is sized such that it ends with 1, 2, or 3 bytes remaining in the segment, we'll pass the {{isEOF}} on the while loop but fail to read the {{serializedSize}} int, and fail. ",N/A,"2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13278,Update build.xml and build.properties.default maven repos,"Only 2 of the 5 urls in build.properties.default are currently valid.

java.net2, jclouds, and oauth urls all 404.

{noformat}
$ git grep remoteRepository
build.properties.default:artifact.remoteRepository.central:     http://repo1.maven.org/maven2
build.properties.default:artifact.remoteRepository.java.net2:   http://download.java.net/maven/2
build.properties.default:artifact.remoteRepository.apache:      https://repository.apache.org/content/repositories/releases
build.properties.default:artifact.remoteRepository.jclouds:     http://jclouds.googlecode.com/svn/repo
build.properties.default:artifact.remoteRepository.oauth:       http://oauth.googlecode.com/svn/code/maven
build.xml:      <artifact:remoteRepository id=""central""   url=""${artifact.remoteRepository.central}""/>
build.xml:      <artifact:remoteRepository id=""java.net2"" url=""${artifact.remoteRepository.java.net2}""/>
build.xml:      <artifact:remoteRepository id=""apache""    url=""${artifact.remoteRepository.apache}""/>
build.xml:          <remoteRepository refid=""central""/>
build.xml:          <remoteRepository refid=""apache""/>
build.xml:          <remoteRepository refid=""java.net2""/>
build.xml:          <remoteRepository refid=""central""/>
build.xml:          <remoteRepository refid=""apache""/>
build.xml:          <remoteRepository refid=""java.net2""/>
build.xml:          <remoteRepository refid=""central""/>
build.xml:        <remoteRepository refid=""apache""/>
build.xml:        <remoteRepository refid=""central""/>
build.xml:        <remoteRepository refid=""oauth""/>
{noformat}",N/A,"2.1.18, 2.2.10, 3.0.12, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13277,Duplicate results with secondary index on static column,"As a follow up of http://www.mail-archive.com/user@cassandra.apache.org/msg50816.html 

Duplicate results appear with secondary index on static column with RF > 1.
Number of results vary depending on consistency level.

Here is a CCM session to reproduce the issue:
{code}
romain@debian:~$ ccm create 39 -n 3 -v 3.9 -s
Current cluster is now: 39
romain@debian:~$ ccm node1 cqlsh
Connected to 39 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};
cqlsh> CREATE TABLE test.idx_static (id text, id2 bigint static, added timestamp, source text static, dest text, primary key (id, added));
cqlsh> CREATE index ON test.idx_static (id2);
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(2 rows)
cqlsh> CONSISTENCY ALL 
Consistency level set to ALL.
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(3 rows)
{code}

When RF matches the number of nodes, it works as expected.

Example with RF=3 and 3 nodes:
{code}
romain@debian:~$ ccm create 39 -n 3 -v 3.9 -s
Current cluster is now: 39

romain@debian:~$ ccm node1 cqlsh
Connected to 39 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.

cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
cqlsh> CREATE TABLE test.idx_static (id text, id2 bigint static, added timestamp, source text static, dest text, primary key (id, added));
cqlsh> CREATE index ON test.idx_static (id2);
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
cqlsh> CONSISTENCY all
Consistency level set to ALL.
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
{code}

Example with RF = 2 and 2 nodes:

{code}
romain@debian:~$ ccm create 39 -n 2 -v 3.9 -s
Current cluster is now: 39
romain@debian:~$ ccm node1 cqlsh
Connected to 39 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.9 | CQL spec 3.4.2 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};
cqlsh> CREATE TABLE test.idx_static (id text, id2 bigint static, added timestamp, source text static, dest text, primary key (id, added));
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> CREATE index ON test.idx_static (id2);
cqlsh> INSERT INTO test.idx_static (id, id2, added, source, dest) values ('id1', 22,'2017-01-28', 'src1', 'dst1');
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
cqlsh> CONSISTENCY ALL 
Consistency level set to ALL.
cqlsh> SELECT * FROM test.idx_static where id2=22;

 id  | added                           | id2 | source | dest
-----+---------------------------------+-----+--------+------
 id1 | 2017-01-27 23:00:00.000000+0000 |  22 |   src1 | dst1

(1 rows)
{code}",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13276,Regression on CASSANDRA-11416: can't load snapshots of tables with dropped columns,"I'm running Cassandra 3.10 and running into the exact same issue described in CASSANDRA-11416: 

1. A table is created with columns 'a' and 'b'
2. Data is written to the table
3. Drop column 'b'
4. Take a snapshot
5. Drop the table
6. Run the snapshot schema.cql to recreate the table and the run the alter
7. Try to restore the snapshot data using sstableloader

sstableloader yields the error:
java.lang.RuntimeException: Unknown column b during deserialization",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13274,Fix code to not exchange schema across major versions,"A rolling upgrade from 3.* to 4.0 (messaging version {{11}}) unveils a regression caused by CASSANDRA-11128.

Generally, we store all possible options/attributes including the default values in the schema. This causes (expected) schema-version-mismatches during rolling upgrades and therefore we prevent schema pulls/pushes in this situation, which has been broken by CASSANDRA-11128.",N/A,"3.0.13, 3.11.0"
CASSANDRA-13272,"""nodetool bootstrap resume"" does not exit","I have a script that calls ""nodetool bootstrap resume"" after a failed join (in my environment some streams sometimes fail due to mis-tuning of stream bandwidth settings). However, if the streams fail again, nodetool won't exit.

Last lines before it just hangs forever :

{noformat}
[2017-02-26 07:02:42,287] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12670-big-Data.db (progress: 1112%)
[2017-02-26 07:02:42,287] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12670-big-Data.db (progress: 1112%)
[2017-02-26 07:02:59,843] received file /var/lib/cassandra/data/keyspace/table-63d5d42009fa11e5879ebd9463bffdac/mc-12671-big-Data.db (progress: 1112%)
[2017-02-26 09:25:51,000] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:33:45,017] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:39:27,216] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:53:33,084] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 09:55:07,115] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 10:06:49,557] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 10:40:55,880] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 11:09:21,025] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 12:44:35,755] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 12:49:18,867] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 13:23:50,611] session with /10.x.y.z complete (progress: 1112%)
[2017-02-26 13:23:50,612] Stream failed
{noformat}

At that point (""Stream failed"") I would expect nodetool to exit with a non-zero exit code. Instead, it just wants me to ^C it.",N/A,"2.2.11, 3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13265,Expiration in OutboundTcpConnection can block the reader Thread,"I observed that sometimes a single node in a Cassandra cluster fails to communicate to the other nodes. This can happen at any time, during peak load or low load. Restarting that single node from the cluster fixes the issue.

Before going in to details, I want to state that I have analyzed the situation and am already developing a possible fix. Here is the analysis so far:

- A Threaddump in this situation showed  324 Threads in the OutboundTcpConnection class that want to lock the backlog queue for doing expiration.
- A class histogram shows 262508 instances of OutboundTcpConnection$QueuedMessage.

What is the effect of it? As soon as the Cassandra node has reached a certain amount of queued messages, it starts thrashing itself to death. Each of the Thread fully locks the Queue for reading and writing by calling iterator.next(), making the situation worse and worse.
- Writing: Only after 262508 locking operation it can progress with actually writing to the Queue.
- Reading: Is also blocked, as 324 Threads try to do iterator.next(), and fully lock the Queue

This means: Writing blocks the Queue for reading, and readers might even be starved which makes the situation even worse.

-----
The setup is:
 - 3-node cluster
 - replication factor 2
 - Consistency LOCAL_ONE
 - No remote DC's
 - high write throughput (100000 INSERT statements per second and more during peak times).
 ",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13263,Incorrect ComplexColumnData hashCode implementation,"I went through some of the logs from CASSANDRA-13175 and one of the more serious issues that we should address seem to be the {{ComplexColumnData.hashCode()}} implementation. As Objects.hashCode is not using deep hashing for array arguments, hashed will be based on the identity instead of the array's content. See patch for simple fix.
",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13262,Incorrect cqlsh results when selecting same columns multiple times,"Just stumbled over this on trunk:

{quote}
cqlsh:test1> select a, b, c from table1;

 a | b    | c
---+------+-----
 1 |    b |   2
 2 | null | 2.2

(2 rows)
cqlsh:test1> select a, a, b, c from table1;

 a | a    | b   | c
---+------+-----+------
 1 |    b |   2 | null
 2 | null | 2.2 | null

(2 rows)
cqlsh:test1> select a, a, a, b, c from table1;

 a | a    | a             | b    | c
---+------+---------------+------+------
 1 |    b |           2.0 | null | null
 2 | null | 2.20000004768 | null | null
{quote}

My guess is that his is on the Python side, but haven't really looked into it.
",N/A,"2.2.14, 3.0.18, 3.11.4, 4.0-alpha1, 4.0"
CASSANDRA-13247,index on udt built failed and no data could be inserted,"index on udt built failed and no data could be inserted

steps to reproduce:

CREATE KEYSPACE ks1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '2'}  AND durable_writes = true;

CREATE TYPE ks1.address (
    street text,
    city text,
    zip_code int,
    phones set<text>
);

CREATE TYPE ks1.fullname (
    firstname text,
    lastname text
);

CREATE TABLE ks1.users (
    id uuid PRIMARY KEY,
    addresses map<text, frozen<address>>,
    age int,
    direct_reports set<frozen<fullname>>,
    name fullname
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';


SELECT * FROM users where name = { firstname : 'first' , lastname : 'last'} allow filtering;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}

WARN  [ReadStage-2] 2017-02-22 16:59:33,392 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.AssertionError: Only CONTAINS and CONTAINS_KEY are supported for 'complex' types
	at org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:683) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToRow(RowFilter.java:303) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:110) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:41) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:162) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:128) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:292) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:281) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:289) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:145) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:138) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:134) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:323) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1803) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2486) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]


CREATE INDEX users_name_idx ON ks1.users (name);

ERROR [CompactionExecutor:776] 2017-02-22 16:49:41,934 CassandraDaemon.java:226 - Exception in thread Thread[CompactionExecutor:776,1,main]
java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCells(CassandraIndex.java:436) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:386) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.SecondaryIndexManager.lambda$indexPartition$17(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.index.SecondaryIndexManager.indexPartition(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.indexPartition(Keyspace.java:566) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CollatedViewIndexBuilder.build(CollatedViewIndexBuilder.java:70) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.compaction.CompactionManager$12.run(CompactionManager.java:1468) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[na:1.8.0_121]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:361) ~[na:1.8.0_121]
	at org.apache.cassandra.db.marshal.TupleType.compareCustom(TupleType.java:109) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:159) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:139) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_121]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:234) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 16 common frames omitted
ERROR [SecondaryIndexManagement:3] 2017-02-22 16:49:41,934 CassandraDaemon.java:226 - Exception in thread Thread[SecondaryIndexManagement:3,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:403) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.buildBlocking(CassandraIndex.java:715) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.lambda$getBuildIndexTask$5(CassandraIndex.java:685) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_121]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:399) ~[apache-cassandra-3.9.jar:3.9]
	... 6 common frames omitted
Caused by: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCells(CassandraIndex.java:436) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:386) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.SecondaryIndexManager.lambda$indexPartition$17(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_121]
	at org.apache.cassandra.index.SecondaryIndexManager.indexPartition(SecondaryIndexManager.java:552) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.indexPartition(Keyspace.java:566) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CollatedViewIndexBuilder.build(CollatedViewIndexBuilder.java:70) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.compaction.CompactionManager$12.run(CompactionManager.java:1468) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	... 4 common frames omitted
Caused by: java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[na:1.8.0_121]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:361) ~[na:1.8.0_121]
	at org.apache.cassandra.db.marshal.TupleType.compareCustom(TupleType.java:109) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:159) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:139) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_121]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:234) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 16 common frames omitted


SELECT * FROM users where name = { firstname : 'first' , lastname : 'last'};
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}

WARN  [ReadStage-2] 2017-02-22 16:55:43,139 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.RuntimeException: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'users_name_idx' is not yet available
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2490) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'users_name_idx' is not yet available
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:390) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1801) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2486) ~[apache-cassandra-3.9.jar:3.9]
	... 5 common frames omitted


insert into users (id, name) values (uuid(), {firstname: 'a', lastname: 'b'});
WriteFailure: Error from server: code=1500 [Replica(s) failed to execute write] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}

ERROR [MutationStage-2] 2017-02-22 17:04:34,355 StorageProxy.java:1353 - Failed to apply mutation locally : {}
java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx for ks: ks1, table: users
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:526) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:396) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.applyFuture(Mutation.java:215) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1347) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2539) [apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: java.lang.RuntimeException: null for ks: ks1, table: users.users_name_idx
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCells(CassandraIndex.java:436) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:386) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:808) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:335) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:137) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:119) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:175) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:258) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 12 common frames omitted
Caused by: java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:506) ~[na:1.8.0_121]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:361) ~[na:1.8.0_121]
	at org.apache.cassandra.db.marshal.TupleType.compareCustom(TupleType.java:109) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:159) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:139) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:120) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[apache-cassandra-3.9.jar:3.9]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:794) ~[na:1.8.0_121]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_121]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:234) ~[apache-cassandra-3.9.jar:3.9]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1303) ~[apache-cassandra-3.9.jar:3.9]
	... 26 common frames omitted





",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13246,Querying by secondary index on collection column returns NullPointerException sometimes,"Not sure if this is the absolute minimal case that produces the bug, but here are the steps for reproducing.

1. Create table
{code}
CREATE TABLE test (
id text,
ck1 text,
ck2 text,
static_value text static,
set_value set<text>,
primary key (id, ck1, ck2)
);
{code}
2. Create secondary indices on the clustering columns, static column, and collection column
{code}
create index on test (set_value);
create index on test (static_value);
create index on test (ck1);
create index on test (ck2);
{code}
3. Insert a null value into the `set_value` column
{code}
insert into test (id, ck1, ck2, static_value, set_value) values ('id', 'key1', 'key2', 'static', {'one', 'two'} );
{code}
Sanity check: 
{code}
select * from test;

 id | ck1  | ck2  | static_value | set_value
----+------+------+--------------+----------------
 id | key1 | key2 |       static | {'one', 'two'}
{code}
4. Set the set_value to be empty
{code}
update test set set_value = {} where id = 'id' and ck1 = 'key1' and ck2 = 'key2';
{code}
5. Make a select query that uses `CONTAINS` in the `set_value` column
{code}
select * from test where ck2 = 'key2' and static_value = 'static' and set_value contains 'one' allow filtering;
{code}
Here we get a ReadFailure:
{code}
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{code}
Logs show a NullPointerException
{code}
java.lang.RuntimeException: java.lang.NullPointerException
       	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2470) ~[apache-cassandra-3.7.jar:3.7]
       	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_101]
       	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.7.jar:3.7]
       	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
Caused by: java.lang.NullPointerException: null
       	at org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:720) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToRow(RowFilter.java:303) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:120) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:293) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:281) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:76) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:289) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:134) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:127) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:123) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:65) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:292) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1799) ~[apache-cassandra-3.7.jar:3.7]
       	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2466) ~[apache-cassandra-3.7.jar:3.7]
       	... 5 common frames omitted
{code}",N/A,"3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13238,Add actual row output to assertEmpty error message,"We have several issues popping up every now and then that are hard to debug and the test failure messages aren't entirely helpful, for example: 

{code}
java.lang.AssertionError: Expected empty result but got 1 rows:
{code}

It could be much better if we could have an actual output (what exactly the row that got returned appended to it:

{code}
java.lang.AssertionError: Expected empty result but got 1 rows: [row(value=null)]
{code}

The nice side-effect of this change is that now we will have a helper method that can nicely turn an {{UntypedResultSet}} into {{String}} (I apologise if I did overlooked one).",N/A,"3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13237,Legacy deserializer can create unexpected boundary range tombstones,"Most of the code don't generate a range tombstone boundary with the same deletion time on both side as this is basically useless, and there is some assertion in {{DataResolver}} that actually expect this. However, the deserializer for legacy sstable doesn't always properly avoid their creation and we can thus generate them (and break the {{DataResolver}} assertion.",N/A,"3.0.12, 3.11.0"
CASSANDRA-13236,corrupt flag error after upgrade from 2.2 to 3.0.10,"After upgrade from 2.2.5 to 3.0.9/10 we're getting a bunch of errors like this:

{code}
ERROR [SharedPool-Worker-1] 2017-02-17 12:58:43,859 Message.java:617 - Unexpected exception during request; channel = [id: 0xa8b98684, /10.0.70.104:56814 => /10.0.80.24:9042]
java.io.IOError: java.io.IOException: Corrupt flags value for unfiltered partition (isStatic flag set): 160
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:222) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:210) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:749) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:711) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:400) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:265) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:224) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:76) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:487) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:464) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.10.jar:3.0.10]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.10.jar:3.0.10]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72]
Caused by: java.io.IOException: Corrupt flags value for unfiltered partition (isStatic flag set): 160
        at org.apache.cassandra.db.rows.UnfilteredSerializer.deserialize(UnfilteredSerializer.java:374) ~[apache-cassandra-3.0.10.jar:3.0.10]
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer$1.computeNext(UnfilteredRowIteratorSerializer.java:217) ~[apache-cassandra-3.0.10.jar:3.0.10]
        ... 23 common frames omitted
{code}",N/A,"3.0.14, 3.11.0"
CASSANDRA-13233,Improve testing on macOS by eliminating sigar logging,"The changes introduced in CASSANDRA-7838 (Resolved; Fixed; 2.2.0 beta 1): ""Warn user when OS settings are poor / integrate sigar"" are not Mac friendly.

{code}

INFO  [main] 2016-10-18T11:20:10,330 SigarLibrary.java:44 - Initializing SIGAR library
DEBUG [main] 2016-10-18T11:20:10,342 SigarLog.java:60 - no libsigar-universal64-macosx.dylib in java.library.path
org.hyperic.sigar.SigarException: no libsigar-universal64-macosx.dylib in java.library.path
        at org.hyperic.sigar.Sigar.loadLibrary(Sigar.java:172) ~[sigar-1.6.4.jar:?]
        at org.hyperic.sigar.Sigar.<clinit>(Sigar.java:100) [sigar-1.6.4.jar:?]
        at org.apache.cassandra.utils.SigarLibrary.<init>(SigarLibrary.java:47) [main/:?]
        at org.apache.cassandra.utils.SigarLibrary.<clinit>(SigarLibrary.java:28) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.hash(UUIDGen.java:363) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.makeNode(UUIDGen.java:342) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.makeClockSeqAndNode(UUIDGen.java:291) [main/:?]
        at org.apache.cassandra.utils.UUIDGen.<clinit>(UUIDGen.java:42) [main/:?]
        at org.apache.cassandra.config.CFMetaData$Builder.build(CFMetaData.java:1278) [main/:?]
        at org.apache.cassandra.SchemaLoader.standardCFMD(SchemaLoader.java:369) [classes/:?]
        at org.apache.cassandra.SchemaLoader.standardCFMD(SchemaLoader.java:356) [classes/:?]
        at org.apache.cassandra.SchemaLoader.standardCFMD(SchemaLoader.java:351) [classes/:?]
        at org.apache.cassandra.batchlog.BatchTest.defineSchema(BatchTest.java:59) [classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]
        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) [junit-4.6.jar:?]
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) [junit-4.6.jar:?]
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) [junit-4.6.jar:?]
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:27) [junit-4.6.jar:?]
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) [junit-4.6.jar:?]
        at org.junit.runners.ParentRunner.run(ParentRunner.java:220) [junit-4.6.jar:?]
        at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39) [junit-4.6.jar:?]
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:535) [ant-junit.jar:?]
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1182) [ant-junit.jar:?]
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1033) [ant-junit.jar:?]
INFO  [main] 2016-10-18T11:20:10,350 SigarLibrary.java:57 - Could not initialize SIGAR library org.hyperic.sigar.Sigar.getFileSystemListNative()[Lorg/hyperic/sigar/FileSystem;
{code}

There are 2 issues addressed by the attached patch:
# Create platform aware (windows, Darwin, linux) implementations of CLibrary (for instance CLibrary today assumes all platforms have support for posix_fadvise but this doesn't exist in the Darwin kernel). If methods are defined with the ""native"" JNI keyword in java when the class is loaded it will cause our jna check to fail incorrectly making all of CLibrary ""disabled"" even though because jnaAvailable = false even though on a platform like Darwin all of the native methods except posix_fadvise are supported.
# Replace sigar usage to get current pid with calls to CLibrary/native equivalent -- and fall back to Sigar for platforms like Windows who don't have that support with JDK8 (and without a CLibrary equivalent)",N/A,"3.0.12, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13232,"""multiple versions of ant detected in path for junit"" printed for every junit test case spawned by ""ant test""","There is a super annoying junit warning logged before every junit test case when you run ""ant test"". This is due to the fact that the ant junit task that we have configured in our build.xml sources the system class path and most importantly what's in ant.library.dir.

    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/local/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/mkjellman/Documents/mkjellman-cie-cassandra-trunk/build/lib/jars/ant-1.9.6.jar!/org/apache/tools/ant/Project.class

The fix here is to explicitly exclude the ant jar downloaded from the maven tasks that ends up in ${build.lib} and ${build.dir.lib} so only the ant libraries from the system class path are used.

I played around with excluding the ant classes/jars from the system class path in favor of using the ones we copy into ${build.lib} and ${build.dir.lib} with no success. After reading the documentation it seems you always want to use the libs that shipped with whatever is in $ANT_HOME so i believe excluding the jars from the build lib directories is the correct change anyways.",N/A,"2.2.10, 3.0.12, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13231,org.apache.cassandra.db.DirectoriesTest(testStandardDirs) unit test failing,"The testStandardDirs(org.apache.cassandra.db.DirectoriesTest) unit test always fails. This appears to be due to a commit by Yuki for CASSANDRA-10587 which switched the SSTable descriptor to use the canonical path.

From one of Yuki's comments in CASSANDRA-10587:
""I ended up fixing Descriptor object to always have canonical path as its directory.
This way we don't need to think about given directory is relative or absolute.
In fact, right now Desctiptor (and corresponding SSTable) is not considered equal between Descriptor's directory being relative and absolute. (Added simple unit test to DescriptorTest).""

The issue here is that canonical path will expand out differently than even absolute path. In this case /var/folders -> /private/var/folders. The unit test is looking for /var/folders/... but the Descriptor expands out to /private/var/folders and the unit test fails.

Descriptor#L88 seems to be the real root cause.

   [junit] Testcase: testStandardDirs(org.apache.cassandra.db.DirectoriesTest):	FAILED
    [junit] expected:</var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42> but was:</private/var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42>
    [junit] junit.framework.AssertionFailedError: expected:</var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42> but was:</private/var/folders/fk/5j141flj5kbg_01sfvbwgckm0000gn/T/cassandra3608150262426920207unittest/ks/cf1-f5cce670f32311e689fe991a3af9a2eb/snapshots/42>
    [junit] 	at org.apache.cassandra.db.DirectoriesTest.testStandardDirs(DirectoriesTest.java:159)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.DirectoriesTest FAILED

I'm guessing given we went to canonicalPath() on purpose the ""fix"" here is to call .getCanonicalFile() on both expected Files generated (snapshotDir and backupsDir) for the junit assert.",N/A,"2.2.10, 3.0.12, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13230,Build RPM packages for release,"Currently, releases are built locally on Debian/Ubuntu based machines, without native support for building RPM packages. This can be done with a docker image.

The current cassandra-release scripts are here (please, do not randomly run and push tags..):
https://git-wip-us.apache.org/repos/asf?p=cassandra-builds.git;a=tree;f=cassandra-release
A couple incomplete docker run scripts are here:
https://git-wip-us.apache.org/repos/asf?p=cassandra-builds.git;a=tree;f=docker-wip

{code}
git clone https://git-wip-us.apache.org/repos/asf/cassandra-builds.git
{code}

Patches for build infra improvements are welcome!
/cc [~spodxx@gmail.com] if you want to assign to yourself, I'd be happy to review :)",N/A,"2.1.18, 2.2.10, 3.0.12, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13229,dtest failure in topology_test.TestTopology.size_estimates_multidc_test,"example failure:

http://cassci.datastax.com/job/trunk_novnode_dtest/508/testReport/topology_test/TestTopology/size_estimates_multidc_test

{code}
Standard Output

Unexpected error in node1 log, error: 
ERROR [MemtablePostFlush:1] 2017-02-15 16:07:33,837 CassandraDaemon.java:211 - Exception in thread Thread[MemtablePostFlush:1,5,main]
java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[na:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[na:1.8.0_45]
	at org.apache.cassandra.dht.Splitter.splitOwnedRangesNoPartialRanges(Splitter.java:92) ~[main/:na]
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:59) ~[main/:na]
	at org.apache.cassandra.service.StorageService.getDiskBoundaries(StorageService.java:5180) ~[main/:na]
	at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:312) ~[main/:na]
	at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:304) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1150) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$290(NamedThreadFactory.java:81) [main/:na]
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/1321203216.run(Unknown Source) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Unexpected error in node1 log, error: 
ERROR [MigrationStage:1] 2017-02-15 16:07:33,853 CassandraDaemon.java:211 - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:401) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$496(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$222/1949434065.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_45]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.applyChanges(SchemaKeyspace.java:1265) ~[main/:na]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:577) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager$1.runMayThrow(MigrationManager.java:402) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$290(NamedThreadFactory.java:81) [main/:na]
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/1321203216.run(Unknown Source) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:397) ~[main/:na]
	... 16 common frames omitted
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[na:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[na:1.8.0_45]
	at org.apache.cassandra.dht.Splitter.splitOwnedRangesNoPartialRanges(Splitter.java:92) ~[main/:na]
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:59) ~[main/:na]
	at org.apache.cassandra.service.StorageService.getDiskBoundaries(StorageService.java:5180) ~[main/:na]
	at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:312) ~[main/:na]
	at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:304) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1150) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115) ~[main/:na]
	... 5 common frames omitted
Unexpected error in node1 log, error: 
ERROR [main] 2017-02-15 16:07:33,857 CassandraDaemon.java:663 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:401) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager.announce(MigrationManager.java:384) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager.announceNewKeyspace(MigrationManager.java:176) ~[main/:na]
	at org.apache.cassandra.service.StorageService.maybeAddKeyspace(StorageService.java:1066) ~[main/:na]
	at org.apache.cassandra.service.StorageService.maybeAddOrUpdateKeyspace(StorageService.java:1091) ~[main/:na]
	at org.apache.cassandra.service.StorageService.doAuthSetup(StorageService.java:1048) ~[main/:na]
	at org.apache.cassandra.service.StorageService.finishJoiningRing(StorageService.java:1043) ~[main/:na]
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:966) ~[main/:na]
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:649) ~[main/:na]
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:581) ~[main/:na]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:364) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:557) [main/:na]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:646) [main/:na]
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:397) ~[main/:na]
	... 12 common frames omitted
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:401) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$496(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$222/1949434065.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_45]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:284) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.applyChanges(SchemaKeyspace.java:1265) ~[main/:na]
	at org.apache.cassandra.schema.Schema.merge(Schema.java:577) ~[main/:na]
	at org.apache.cassandra.schema.Schema.mergeAndAnnounceVersion(Schema.java:564) ~[main/:na]
	at org.apache.cassandra.schema.MigrationManager$1.runMayThrow(MigrationManager.java:402) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$290(NamedThreadFactory.java:81) ~[main/:na]
	at org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$5/1321203216.run(Unknown Source) ~[na:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
Caused by: java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:397) ~[main/:na]
	... 16 common frames omitted
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.rangeCheck(ArrayList.java:653) ~[na:1.8.0_45]
	at java.util.ArrayList.get(ArrayList.java:429) ~[na:1.8.0_45]
	at org.apache.cassandra.dht.Splitter.splitOwnedRangesNoPartialRanges(Splitter.java:92) ~[main/:na]
	at org.apache.cassandra.dht.Splitter.splitOwnedRanges(Splitter.java:59) ~[main/:na]
	at org.apache.cassandra.service.StorageService.getDiskBoundaries(StorageService.java:5180) ~[main/:na]
	at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:312) ~[main/:na]
	at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:304) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1150) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1115) ~[main/:na]
	... 5 common frames omitted
Unexpected error in node1 log, error: 
ERROR [StorageServiceShutdownHook] 2017-02-15 16:07:35,972 AbstractCommitLogSegmentManager.java:311 - Failed to force-recycle all segments; at least one segment is still in use with dirty CFs.
{code}",N/A,"3.11.7, 4.0-alpha1, 4.0"
CASSANDRA-13228,SASI index on partition key part doesn't match,"I created a SASI index on first part of multi-part partition key. Running query using that index doesn't seem to work.

I have here a log of queries that should indicate the issue:

{code}cqlsh:test> CREATE TABLE test1(name text, event_date date, data_type text, bytes int, PRIMARY KEY ((name, event_date), data_type));
cqlsh:test> CREATE CUSTOM INDEX test_index ON test1(name) USING 'org.apache.cassandra.index.sasi.SASIIndex';
cqlsh:test> INSERT INTO test1(name, event_date, data_type, bytes) values('1234', '2010-01-01', 'sensor', 128);
cqlsh:test> INSERT INTO test1(name, event_date, data_type, bytes) values('abcd', '2010-01-02', 'sensor', 500);
cqlsh:test> select * from test1 where NAME = '1234';

 name | event_date | data_type | bytes
------+------------+-----------+-------

(0 rows)
cqlsh:test> CONSISTENCY ALL;
Consistency level set to ALL.
cqlsh:test> select * from test1 where NAME = '1234';

 name | event_date | data_type | bytes
------+------------+-----------+-------

(0 rows){code}

Note! Creating a SASI index on single part partition key, SASI index creation fails. Apparently this should not work at all, so is it about missing validation on index creation?",N/A,3.11.0
CASSANDRA-13219,Cassandra.yaml now unicode instead of ascii after 13090,"After CASSANDRA-13090, which was commit 5725e2c422d21d8efe5ae3bc4389842939553650, cassandra.yaml now has unicode characters, specifically [0xe2|http://utf8-chartable.de/unicode-utf8-table.pl?start=8320&number=128&names=2&utf8=0x]. Previously, it was only ascii.

This is an admittedly minor change, but it is breaking. It affects (at least) a subset of python yaml parsing tools (which is a large number of tools that use C*).",N/A,"2.2.9, 3.0.11, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13218,Duration validation error is unclear in case of overflow.,"If a user try to insert a {{duration}} with a number of months or days that cannot fit in an {{int}} (for example: {{9223372036854775807mo1d}}), the error message is confusing.",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13216,testall failure in org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages,"example failure:

http://cassci.datastax.com/job/cassandra-3.11_testall/81/testReport/org.apache.cassandra.net/MessagingServiceTest/testDroppedMessages

{code}
Error Message

expected:<... dropped latency: 27[30 ms and Mean cross-node dropped latency: 2731] ms> but was:<... dropped latency: 27[28 ms and Mean cross-node dropped latency: 2730] ms>
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: expected:<... dropped latency: 27[30 ms and Mean cross-node dropped latency: 2731] ms> but was:<... dropped latency: 27[28 ms and Mean cross-node dropped latency: 2730] ms>
	at org.apache.cassandra.net.MessagingServiceTest.testDroppedMessages(MessagingServiceTest.java:83)
{code}",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13215,Cassandra nodes startup time 20x more after upgarding to 3.x,"CompactionStrategyManage.getCompactionStrategyIndex is called on each sstable at startup. And this function calls StorageService.getDiskBoundaries. And getDiskBoundaries calls AbstractReplicationStrategy.getAddressRanges.
It appears that last function can be really slow. In our environment we have 1545 tokens and with NetworkTopologyStrategy it can make 1545*1545 computations in worst case (maybe I'm wrong, but it really takes lot's of cpu).

Also this function can affect runtime later, cause it is called not only during startup.

I've tried to implement simple cache for getDiskBoundaries results and now startup time is about one minute instead of 25m, but I'm not sure if it's a good solution.",N/A,"3.11.2, 4.0-alpha1, 4.0"
CASSANDRA-13211,Use portable stderr for java error in startup,"The cassandra startup shell script contains this line:

    echo Unable to find java executable. Check JAVA_HOME and PATH environment variables. > /dev/stderr

The problem here is the construct ""> /dev/stderr"". If the user invoking Cassandra has changed user (for example, by SSHing in as a personal user, and then sudo-ing to an application user responsible for executing the Cassandra daemon), then the attempt to open /dev/stderr will fail, because it will point to a PTY node under /dev/pts/ owned by the original user.

Ultimately this leads to the real problem being masked by the confusing error message ""bash: /dev/stderr: Permission denied"".

The correct technique is to replace ""> /dev/stderr"" with "">&2"" which will write to the already open stderr file descriptor, instead of resolving the chain of symlinks starting at /dev/stderr, and attempting to reopen the target by name.",N/A,"2.1.17, 2.2.9, 3.0.11, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13209,test failure in cqlsh_tests.cqlsh_copy_tests.CqlshCopyTest.test_bulk_round_trip_blogposts_with_max_connections,"example failure:

http://cassci.datastax.com/job/cassandra-2.1_dtest/528/testReport/cqlsh_tests.cqlsh_copy_tests/CqlshCopyTest/test_bulk_round_trip_blogposts_with_max_connections

{noformat}
Error Message

errors={'127.0.0.4': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.4
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-792s6j
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
dtest: DEBUG: removing ccm cluster test at: /tmp/dtest-792s6j
dtest: DEBUG: clearing ssl stores from [/tmp/dtest-792s6j] directory
dtest: DEBUG: cluster ccm directory: /tmp/dtest-uNMsuW
dtest: DEBUG: Done setting configuration options:
{   'initial_token': None,
    'num_tokens': '32',
    'phi_convict_threshold': 5,
    'range_request_timeout_in_ms': 10000,
    'read_request_timeout_in_ms': 10000,
    'request_timeout_in_ms': 10000,
    'truncate_request_timeout_in_ms': 10000,
    'write_request_timeout_in_ms': 10000}
cassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.5 datacenter1> discovered
cassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.4 datacenter1> discovered
dtest: DEBUG: Running stress with user profile /home/automaton/cassandra-dtest/cqlsh_tests/blogposts.yaml
--------------------- >> end captured logging << ---------------------
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 1090, in wrapped
    f(obj)
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2571, in test_bulk_round_trip_blogposts_with_max_connections
    copy_from_options={'NUMPROCESSES': 2})
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2500, in _test_bulk_round_trip
    num_records = create_records()
  File ""/home/automaton/cassandra-dtest/cqlsh_tests/cqlsh_copy_tests.py"", line 2473, in create_records
    ret = rows_to_list(self.session.execute(count_statement))[0][0]
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 1998, in execute
    return self.execute_async(query, parameters, trace, custom_payload, timeout, execution_profile, paging_state).result()
  File ""/home/automaton/src/cassandra-driver/cassandra/cluster.py"", line 3784, in result
    raise self._final_exception
""errors={'127.0.0.4': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.4\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-792s6j\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: removing ccm cluster test at: /tmp/dtest-792s6j\ndtest: DEBUG: clearing ssl stores from [/tmp/dtest-792s6j] directory\ndtest: DEBUG: cluster ccm directory: /tmp/dtest-uNMsuW\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ncassandra.policies: INFO: Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '127.0.0.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.3 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.2 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.5 datacenter1> discovered\ncassandra.cluster: INFO: New Cassandra host <Host: 127.0.0.4 datacenter1> discovered\ndtest: DEBUG: Running stress with user profile /home/automaton/cassandra-dtest/cqlsh_tests/blogposts.yaml\n--------------------- >> end captured logging << ---------------------""
{noformat}",N/A,"2.2.10, 3.0.14, 3.11.0"
CASSANDRA-13205,Hint related logging should include the IP address of the destination in addition to host ID,"After the hint rewrite in 3.0, many of the hint related logs now use hostId UUIDs rather than endpoint addresses. This complicates debugging unnecessarily. We should include both.",N/A,"3.0.11, 3.11.0"
CASSANDRA-13204,Thread Leak in OutboundTcpConnection,"We found threads leaking from OutboundTcpConnection to machines which are not part of the cluster and still in Gossip for some reason. There are two issues here, this JIRA will cover the second one which is most important. 



1) First issue is that Gossip has information about machines not in the ring which has been replaced out. It causes Cassandra to connect to those machines but due to internode auth, it wont be able to connect to them at the socket level.  

2) Second issue is a race between creating a connection and closing a connections which is triggered by the gossip bug explained above. Let me try to explain it using the code

In OutboundTcpConnection, we are calling closeSocket(true) which will set isStopped=true and also put a close sentinel into the queue to exit the thread. On the ack connection, Gossip tries to send a message which calls connect() which will block for 10 seconds which is RPC timeout. The reason we will block is because Cassandra might not be running there or internode auth will not let it connect. During this 10 seconds, if Gossip calls closeSocket, it will put close sentinel into the queue. When we return from the connect method after 10 seconds, we will clear the backlog queue causing this thread to leak. 

Proofs from the heap dump of the affected machine which is leaking threads 
1. Only ack connection is leaking and not the command connection which is not used by Gossip. 
2. We see thread blocked on the backlog queue, isStopped=true and backlog queue is empty. This is happening on the threads which have already leaked. 
3. A running thread was blocked on the connect waiting for timeout(10 seconds) and we see backlog queue to contain the close sentinel. Once the connect will return false, we will clear the backlog and this thread will have leaked.  


Interesting bits from j stack 
1282 number of threads for ""MessagingService-Outgoing-/<IP-Address>""

Thread which is about to leak:
""MessagingService-Outgoing-/<IP Address>"" 
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:454)
	at sun.nio.ch.Net.connect(Net.java:446)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:137)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:119)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:381)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:217)

Thread already leaked:
""MessagingService-Outgoing-/<IP Address>""
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy.coalesceInternal(CoalescingStrategies.java:482)
	at org.apache.cassandra.utils.CoalescingStrategies$CoalescingStrategy.coalesce(CoalescingStrategies.java:213)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:190)
",N/A,"2.1.17, 2.2.9, 3.0.11, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13188,compaction-stress AssertionError from getMemtableFor(),"Exception:
{noformat}
./compaction-stress compact -d /tmp/compaction -p https://gist.githubusercontent.com/tjake/8995058fed11d9921e31/raw/a9334d1090017bf546d003e271747351a40692ea/blogpost.yaml -t 4
WARN  18:45:04,854 JNA link failure, one or more native method will be unavailable.
java.lang.AssertionError: []
        at org.apache.cassandra.db.lifecycle.Tracker.getMemtableFor(Tracker.java:312)
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1315)
        at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:618)
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:462)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:227)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:232)
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:241)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternalWithoutCondition(ModificationStatement.java:570)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:564)
        at org.apache.cassandra.cql3.QueryProcessor.executeOnceInternal(QueryProcessor.java:356)
        at org.apache.cassandra.schema.SchemaKeyspace.saveSystemKeyspacesSchema(SchemaKeyspace.java:265)
        at org.apache.cassandra.db.SystemKeyspace.finishStartup(SystemKeyspace.java:495)
        at org.apache.cassandra.stress.CompactionStress$Compaction.run(CompactionStress.java:209)
        at org.apache.cassandra.stress.CompactionStress.main(CompactionStress.java:349)
{noformat}",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13185,cqlsh COPY doesn't support dates before 1900 or after 9999,"Although we fixed this problem for standard queries in CASSANDRA-10625, it still exists for COPY.  In CASSANDRA-10625, we replaced datetimes outside of the supported time range with a simple milliseconds-since-epoch long.  We may not want to use the same solution for COPY, because we wouldn't be able to load the same data back in through COPY.  Having consistency in the format of values and support for loading exported data seems more important for COPY.",N/A,"3.0.12, 3.11.0"
CASSANDRA-13180,Better handling of missing entries in system_schema.columns during startup,"Like the error in CASSANDRA-12213 and CASSANDRA-12165, it's possible for {{system_schema.keyspaces}} and {{tables}} to contain entries for a table while {{system_schema.columns}} has none.  This produces an error during startup, and there's no way for a user to recover from this without restoring from backups.

Although this has been seen in the wild on one occasion, the cause is still not entirely known.  (It may be due to a concurrent DROP TABLE and ALTER TABLE where a table property is altered.)  Until we know the root cause, it makes sense to give users a way to recover from that situation.",N/A,"3.0.11, 3.11.0"
CASSANDRA-13179,Remove offheap_buffer as option for memtable_allocation_type in cassandra.yaml,"With [CASSANDRA-11039] disallowing offheap_buffers as option for memtable_allocation_type the cassandra.yaml included documentation should be updated to match.

Patch included.",N/A,3.0.12
CASSANDRA-13177,sstabledump doesn't handle non-empty partitions with a partition-level deletion correctly,"If a partition has a partition-level deletion, but still contains rows (with timestamps higher than the deletion), sstabledump will only show the deletion and not the rows.",N/A,"3.0.11, 3.11.0"
CASSANDRA-13174,Indexing is allowed on Duration type when it should not be,"Looks like secondary indexing is allowed on duration type columns. Since comparisons are not possible for the duration type, indexing on it also should be invalid.

1) 
{noformat}
CREATE TABLE duration_table (k int PRIMARY KEY, d duration);
INSERT INTO duration_table (k, d) VALUES (0, 1s);
SELECT * from duration_table WHERE d=1s ALLOW FILTERING;
{noformat}

The above throws an error: 
{noformat}
WARN  [ReadStage-2] 2017-01-31 17:09:57,821 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,10,main]: {}
java.lang.RuntimeException: java.lang.UnsupportedOperationException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2591) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: java.lang.UnsupportedOperationException: null
	at org.apache.cassandra.db.marshal.AbstractType.compareCustom(AbstractType.java:174) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:160) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.compareForCQL(AbstractType.java:204) ~[main/:na]
	at org.apache.cassandra.cql3.Operator.isSatisfiedBy(Operator.java:201) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$SimpleExpression.isSatisfiedBy(RowFilter.java:719) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToRow(RowFilter.java:324) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.applyOne(BaseRows.java:120) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.add(BaseRows.java:110) ~[main/:na]
	at org.apache.cassandra.db.transform.UnfilteredRows.add(UnfilteredRows.java:44) ~[main/:na]
	at org.apache.cassandra.db.transform.Transformation.add(Transformation.java:174) ~[main/:na]
	at org.apache.cassandra.db.transform.Transformation.apply(Transformation.java:140) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:307) ~[main/:na]
	at org.apache.cassandra.db.filter.RowFilter$CQLFilter$1IsSatisfiedFilter.applyToPartition(RowFilter.java:292) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:310) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:145) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:138) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:134) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[main/:na]
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:333) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1884) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2587) ~[main/:na]
	... 5 common frames omitted
{noformat}

2)
Similarly, if an index is created on the duration column:
{noformat}
CREATE INDEX d_index ON simplex.duration_table (d);
SELECT * from duration_table WHERE d=1s;
{noformat}

results in:
{noformat}
WARN  [ReadStage-2] 2017-01-31 17:12:00,623 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,10,main]: {}
java.lang.RuntimeException: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'd_index' is not yet available
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2591) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_91]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: org.apache.cassandra.index.IndexNotAvailableException: The secondary index 'd_index' is not yet available
	at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:400) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1882) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2587) ~[main/:na]
	... 5 common frames omitted
{noformat}

3) 
Finally, no further updates can be made to the table once the index has been created. Attempting to modify or insert a new row with a non-null value for ""d"" results in an error:
{noformat}
ERROR [MutationStage-2] 2017-01-31 17:13:33,106 StorageProxy.java:1422 - Failed to apply mutation locally : {}
java.lang.RuntimeException: null for ks: simplex, table: duration_table.d_index for ks: simplex, table: duration_table
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1334) ~[main/:na]
	at org.apache.cassandra.db.Keyspace.applyInternal(Keyspace.java:618) ~[main/:na]
	at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:462) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:227) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:232) ~[main/:na]
	at org.apache.cassandra.db.Mutation.apply(Mutation.java:241) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$8.runMayThrow(StorageProxy.java:1416) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2640) [main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_91]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [main/:na]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
Caused by: java.lang.RuntimeException: null for ks: simplex, table: duration_table.d_index
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1334) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.insert(CassandraIndex.java:531) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex.access$100(CassandraIndex.java:72) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex$1.indexCell(CassandraIndex.java:444) ~[main/:na]
	at org.apache.cassandra.index.internal.CassandraIndex$1.insertRow(CassandraIndex.java:388) ~[main/:na]
	at org.apache.cassandra.index.SecondaryIndexManager$WriteTimeTransaction.onInserted(SecondaryIndexManager.java:914) ~[main/:na]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:333) ~[main/:na]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition$RowUpdater.apply(AtomicBTreePartition.java:295) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.buildInternal(BTree.java:139) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.build(BTree.java:121) ~[main/:na]
	at org.apache.cassandra.utils.btree.BTree.update(BTree.java:178) ~[main/:na]
	at org.apache.cassandra.db.partitions.AtomicBTreePartition.addAllWithSizeDelta(AtomicBTreePartition.java:156) ~[main/:na]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:284) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[main/:na]
	... 12 common frames omitted
Caused by: java.lang.UnsupportedOperationException: null
	at org.apache.cassandra.db.marshal.AbstractType.compareCustom(AbstractType.java:174) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.compare(AbstractType.java:160) ~[main/:na]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:156) ~[main/:na]
	at org.apache.cassandra.dht.LocalPartitioner$LocalToken.compareTo(LocalPartitioner.java:132) ~[main/:na]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:85) ~[main/:na]
	at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:39) ~[main/:na]
	at java.util.concurrent.ConcurrentSkipListMap.cpr(ConcurrentSkipListMap.java:655) ~[na:1.8.0_91]
	at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:682) ~[na:1.8.0_91]
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:781) ~[na:1.8.0_91]
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1546) ~[na:1.8.0_91]
	at org.apache.cassandra.db.Memtable.put(Memtable.java:264) ~[main/:na]
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:1316) ~[main/:na]
	... 25 common frames omitted
{noformat}

Similar errors/inconsistencies exist for materialized views.",N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13173,Reloading logback.xml does not work,"Regression of CASSANDRA-12535

Reloading of logback.xml is broken by CASSANDRA-12535 because the delegate {{ReconfigureOnChangeFilter}} is not properly initialized.
(Broken in 3.0.11 + 3.10)",N/A,"3.0.11, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13172,compaction_large_partition_warning_threshold_mb not working properly when set to high value,"compaction_large_partition_warning_threshold_mb has been set either by mistake or as an attempt to disable warnings completely to high value 512000
However system started to produce warning no matter what the partition size is:

 Compacting large partition system/compactions_in_progress:e631fe20-e488-11e6-bcd7-bf6151c7fa28 (32 bytes)

When looking into the code:
{code}
 public static int getCompactionLargePartitionWarningThreshold() { return conf.compaction_large_partition_warning_threshold_mb * 1024 * 1024; }
{code}
which is called in 
{code}
private void maybeLogLargePartitionWarning(DecoratedKey key, long rowSize)
    {
        if (rowSize > DatabaseDescriptor.getCompactionLargePartitionWarningThreshold())
        {
            String keyString = metadata().partitionKeyType.getString(key.getKey());
            logger.warn(""Writing large partition {}/{}:{} ({}) to sstable {}"", metadata.keyspace, metadata.name, keyString, FBUtilities.prettyPrintMemory(rowSize), getFilename());
        }
}
{code}
it looks like 512000 is multiplied by 1M and returned as int so being out of range... Maybe it would be better to use long  as it is used for rowSize


",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13163,NPE in StorageService.excise,"{code}
    private void excise(Collection<Token> tokens, InetAddress endpoint)
    {
        logger.info(""Removing tokens {} for {}"", tokens, endpoint);

        if (tokenMetadata.isMember(endpoint))
            HintsService.instance.excise(tokenMetadata.getHostId(endpoint));

{code}

The check for TMD.isMember() is not enough to guarantee that TMD.getHostId() will not return null. If HintsService.excise() is called with null you get an NPE in a map lookup.",N/A,"3.0.14, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13160,batch documentation should note the single partition optimization,,N/A,"3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13159,Coalescing strategy can enter infinite loop,"{code}boolean maybeSleep(int messages, long averageGap, long maxCoalesceWindow, Parker parker){code} 

maybeSleep() can enter an infinite loop if messages or averageGap ends up being 0 because sleep will be 0 and the while loop will never exit. I've noticed that on one of my clusters twice this week.

This can happen if in averageGap() sum is bigger than MEASURED_INTERVAL, which should be pretty rare but apparently happen to me.

Even if the diagnostic is wrong (and I'm pretty sure that this thread was using 100% CPU doing nothing), the fix seems pretty safe to apply.

{code}
diff --git a/src/java/org/apache/cassandra/utils/CoalescingStrategies.java b/src/java/org/apache/cassandra/utils/CoalescingStrategies.java
index 0aa980f..982d4a6 100644
--- a/src/java/org/apache/cassandra/utils/CoalescingStrategies.java
+++ b/src/java/org/apache/cassandra/utils/CoalescingStrategies.java
@@ -100,7 +100,7 @@ public class CoalescingStrategies
     {
         // only sleep if we can expect to double the number of messages we're sending in the time interval
         long sleep = messages * averageGap;
-        if (sleep > maxCoalesceWindow)
+        if (!sleep || sleep > maxCoalesceWindow)
             return false;
 
         // assume we receive as many messages as we expect; apply the same logic to the future batch:
{code}",N/A,"2.1.17, 2.2.9, 3.0.11, 3.11.0"
CASSANDRA-13153,Reappeared Data when Mixing Incremental and Full Repairs,"This happens for both LeveledCompactionStrategy and SizeTieredCompactionStrategy.  I've only tested it on Cassandra version 2.2 but it most likely also affects all Cassandra versions after 2.2, if they have anticompaction with full repair.

When mixing incremental and full repairs, there are a few scenarios where the Data SSTable is marked as unrepaired and the Tombstone SSTable is marked as repaired.  Then if it is past gc_grace, and the tombstone and data has been compacted out on other replicas, the next incremental repair will push the Data to other replicas without the tombstone.

Simplified scenario:
3 node cluster with RF=3
Intial config:
	Node 1 has data and tombstone in separate SSTables.
	Node 2 has data and no tombstone.
	Node 3 has data and tombstone in separate SSTables.

Incremental repair (nodetool repair -pr) is run every day so now we have tombstone on each node.
Some minor compactions have happened since so data and tombstone get merged to 1 SSTable on Nodes 1 and 3.
	Node 1 had a minor compaction that merged data with tombstone. 1 SSTable with tombstone.
	Node 2 has data and tombstone in separate SSTables.
	Node 3 had a minor compaction that merged data with tombstone. 1 SSTable with tombstone.

Incremental repairs keep running every day.
Full repairs run weekly (nodetool repair -full -pr). 
Now there are 2 scenarios where the Data SSTable will get marked as ""Unrepaired"" while Tombstone SSTable will get marked as ""Repaired"".

Scenario 1:
        Since the Data and Tombstone SSTable have been marked as ""Repaired"" and anticompacted, they have had minor compactions with other SSTables containing keys from other ranges.  During full repair, if the last node to run it doesn't own this particular key in it's partitioner range, the Data and Tombstone SSTable will get anticompacted and marked as ""Unrepaired"".  Now in the next incremental repair, if the Data SSTable is involved in a minor compaction during the repair but the Tombstone SSTable is not, the resulting compacted SSTable will be marked ""Unrepaired"" and Tombstone SSTable is marked ""Repaired"".

Scenario 2:
        Only the Data SSTable had minor compaction with other SSTables containing keys from other ranges after being marked as ""Repaired"".  The Tombstone SSTable was never involved in a minor compaction so therefore all keys in that SSTable belong to 1 particular partitioner range. During full repair, if the last node to run it doesn't own this particular key in it's partitioner range, the Data SSTable will get anticompacted and marked as ""Unrepaired"".   The Tombstone SSTable stays marked as Repaired.

Then it’s past gc_grace.  Since Node’s #1 and #3 only have 1 SSTable for that key, the tombstone will get compacted out.
	Node 1 has nothing.
	Node 2 has data (in unrepaired SSTable) and tombstone (in repaired SSTable) in separate SSTables.
	Node 3 has nothing.

Now when the next incremental repair runs, it will only use the Data SSTable to build the merkle tree since the tombstone SSTable is flagged as repaired and data SSTable is marked as unrepaired.  And the data will get repaired against the other two nodes.
	Node 1 has data.
	Node 2 has data and tombstone in separate SSTables.
	Node 3 has data.
If a read request hits Node 1 and 3, it will return data.  If it hits 1 and 2, or 2 and 3, however, it would return no data.

Tested this with single range tokens for simplicity.
",N/A,"2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13152,UPDATE on counter columns with empty list as argument in IN disables cluster,"On a 3 node cluster
with this table (replication factor of 2):

{code}
CREATE TABLE tracking.item_items_rec_history (
	reference_id bigint,
	country text,
	portal text,
	app_name text,
	recommended_id bigint,
	counter counter,
	PRIMARY KEY (reference_id, country, portal, app_name, recommended_id)
);
{code}
If I execute 

{code}
UPDATE user_items_rec_history 
SET counter = counter + 1 
WHERE reference_id = 1 AND country = '' AND portal = '' AND app_name = '' AND recommended_id IN ();
{code}

Take note that the IN is empty

The cluster starts to malfunction and responds a lot of timeouts to any query.

After resetting some of the nodes, the cluster starts to function normally again.
",N/A,"3.0.11, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13149,AssertionError prepending to a list,"Prepending to a list produces the following AssertionError randomly. Changing the update to append (and sort in the client) works around the issue.

{code}
java.lang.AssertionError: null
	at org.apache.cassandra.cql3.Lists$PrecisionTime.getNext(Lists.java:275) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.Lists$Prepender.execute(Lists.java:430) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:94) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:682) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:613) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:420) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:408) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:206) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:487) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:464) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) ~[apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [apache-cassandra-3.0.8.jar:3.0.8]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.8.jar:3.0.8]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.8.jar:3.0.8]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{code}",N/A,"3.0.15, 3.11.1, 4.0-alpha1, 4.0"
CASSANDRA-13147,Secondary index query on partition key columns might not return all the rows.,"A secondary index query on a partition key column will, apparently, not return the empty partitions with static data.

The following unit test can be used to reproduce the problem.

{code}
    public void testIndexOnPartitionKeyWithStaticColumnAndNoRows() throws Throwable
    {
        createTable(""CREATE TABLE %s (pk1 int, pk2 int, c int, s int static, v int, PRIMARY KEY((pk1, pk2), c))"");
        createIndex(""CREATE INDEX ON %s (pk2)"");
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 1, 9, 1);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 1, 1, 2, 9, 2);
        execute(""INSERT INTO %s (pk1, pk2, s) VALUES (?, ?, ?)"", 2, 1, 9);
        execute(""INSERT INTO %s (pk1, pk2, c, s, v) VALUES (?, ?, ?, ?, ?)"", 3, 1, 1, 9, 1);

        assertRows(execute(""SELECT * FROM %s WHERE pk2 = ?"", 1),
                   row(1, 1, 1, 9, 1),
                   row(1, 1, 2, 9, 2),
                   row(2, 1, null, 9, null), <-- is not returned
                   row(3, 1, 1, 9, 1));
    }
{code}",N/A,"2.1.18, 2.2.10, 3.0.13, 3.11.0, 4.0-alpha1, 4.0"
CASSANDRA-13143,Cassandra can accept invalid durations,"A duration can be positive or negative. If the duration is positive the months, days and nanoseconds must be greater or equals to zero. If the duration is negative the months, days and nanoseconds must be smaller or equals to zero.
Currently, it is possible to send to C* a duration which does not respect that rule and the data will not be reject.",N/A,3.10
CASSANDRA-13133,Unclosed file descriptors when querying SnapshotsSize metric,"Started to notice many open file descriptors (100k+) per node, growing at a rate of about 30 per minute in our cluster. After turning off our JMX exporting server(https://github.com/prometheus/jmx_exporter), which gets queried every 30 seconds, the number of file descriptors remained static. 

Digging a bit further I ran a jmx dump tool over all the cassandra metrics and tracked the number of file descriptors after each query, boiling it down to a single metric causing the number of file descriptors to increase:

org.apache.cassandra.metrics:keyspace=tpsv1,name=SnapshotsSize,scope=events_by_engagement_id,type=Table

running a query a few times against this metric shows the file descriptors increasing after each query:

{code}
for _ in {0..3} 
do 
   java -jar jmx-dump-0.4.2-standalone.jar --port 7199 --dump org.apache.cassandra.metrics:keyspace=tpsv1,name=SnapshotsSize,scope=events_by_engagement_id,type=Table > /dev/null; 
   sudo lsof -p `pgrep -f CassandraDaemon` | fgrep ""DIR"" | awk '{a[$(NF)]+=1}END{for(k in a){print k, a[k]}}' | grep ""events_by"" 
done


> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33176
> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33177
> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33178
> /data/cassandra/data/tpsv1/events_by_engagement_id-01d8f450a54911e6917ec93f8a91ec71 33179
{code}

it should be noted that the file descriptor is open on a directory, not an actual file",N/A,3.10
