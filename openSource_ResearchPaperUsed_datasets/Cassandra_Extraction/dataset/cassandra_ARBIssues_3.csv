Bug_ID,Bug_Summary,Bug_Description
CASSANDRA-19927,[Analytics] Deprecate old compression cache and move to using cache of CompressionMetadata,"The compression cache currently caches a single byte array for the CompressionInfo.db file, this is a problem for large files as it involves allocating and garbage collecting large memory segments, but also means that every consumer of the bytes will instantiate a CompressionMetadata object and allocate an individual BigLongArray to store the chunk offsets. This is unnecessary as the CompressionMetadata is immutable and can be re-used."
CASSANDRA-19836,[Analytics] Fix NPE when writing UDT values,"When UDT field values are set to null, the bulk writer throws NPE, e.g. the stacktrace below. Although it is on the boolean type, the NPE can be thrown on all other types whenever the value is null.

{code:java}
Caused by: java.lang.NullPointerException
  at org.apache.cassandra.spark.data.types.Boolean.setInnerValue(Boolean.java:91)
  at org.apache.cassandra.spark.data.complex.CqlUdt.setInnerValue(CqlUdt.java:534)
  at org.apache.cassandra.spark.data.complex.CqlUdt.toUserTypeValue(CqlUdt.java:522)
  at org.apache.cassandra.spark.data.complex.CqlUdt.convertForCqlWriter(CqlUdt.java:169)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.maybeConvertUdt(RecordWriter.java:450)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.getBindValuesForColumns(RecordWriter.java:432)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.writeRow(RecordWriter.java:415)
  at org.apache.cassandra.spark.bulkwriter.RecordWriter.write(RecordWriter.java:202)
{code}

"
CASSANDRA-18461,CEP-21 Avoid NPE when getting dc/rack for not yet registered endpoints,"If a snitch is asked for location info for a node not yet added to the cluster, it should not NPE. In future, it may be desirable to fine tune the actual behaviour, but for now returning a default would be an improvement.
"
CASSANDRA-18062,On-disk string index with index building and on-disk query path,"An on-disk index for string (literal) datatypes. This index is used for the following datatypes:
 * UTF8Type
 * AsciiType
 * CompositeType
 * Frozen types

This includes the ability to write the index to disk at index creation, by specific index rebuild and during SSTable compaction. 

Also the ability to query the on-disk index and combine the results with those from the in-memory indexes created by CASSANDRA-18058."
CASSANDRA-18058,In-memory index and query path,An in-memory index using the in-memory trie structure introduced with CASSANDRA-17240 along with a query path implementation to perform index queries from the in-memory index.
CASSANDRA-17855,Add rat and licenses to Accord,"Right now accord doesn’t include rat to validate files come with a license so non-licensed files may leak in; we should block the build on this.
"
CASSANDRA-17852,"WEBSITE - Community page - encourage users to search answered questions on Stack Overflow, Stack Exchange",We need to encourage users to first search the {{[cassandra]}} tag on [Stack Overflow|http://stackoverflow.com/questions/tagged/cassandra] and [Stack Exchange|https://dba.stackexchange.com/questions/tagged/cassandra] before asking a question so contributors are not repeatedly asking the same questions over and over.
CASSANDRA-17288,"In Jenkins, replace PostBuildTask plugin with PostBuildScript plugin","PostbuildTask loads the build log into memory, causing OutOfMemoryException.
https://issues.jenkins.io/browse/JENKINS-12830

We started hitting this problem this week, see discussion here:
https://the-asf.slack.com/archives/CK23JSY2K/p1643063068079200?thread_ts=1643052183.068300&cid=CK23JSY2K

Fix is to use the PostBuildTask plugin instead, which does not perform any reading off or matching on the build log.

PR: https://github.com/apache/cassandra-builds/pull/61
"
CASSANDRA-17143,Fix test NetstatsBootstrapWithEntireSSTablesCompressionStreamingTest#testWithStreamingEntireSSTablesWithoutCompressionWithoutThrottling,"https://app.circleci.com/pipelines/github/dcapwell/cassandra/1099/workflows/2f08ff99-9ea5-4023-b027-84150efbd2e9/jobs/7850

Found in CASSANDRA-17069.

What I see is that there is a race condition, which is easier to hit when we disable throttling.  If zero-copy-streaming starts and completes within the gap between nodetool calls, then the test fails with a timeout as the expected condition is not seen (waits to see streaming).  We can avoid this by detecting that streaming completed by grepping the logs."
CASSANDRA-16394,Fix schema aggreement race conditions in in-JVM dtests ,"There there are two race conditions in in-JVM dtest schema agreement, which are causing test failures:

1. First is caused by the fact we’re starting waiting for schema propagation already after the schema agreement was reached (which was resulting into us endlessly waiting for an agreement that has already been established);
 2. The other one was because the callback to notify about successful agreement can be triggered already after the other node has notified about it, and control flow might have moved cluster to a different configuration.

Example of exception:
{code:java}
Caused by: java.lang.IllegalStateException: Schema agreement not reached
	at org.apache.cassandra.distributed.impl.AbstractCluster$ChangeMonitor.waitForCompletion(AbstractCluster.java:?)
	at org.apache.cassandra.distributed.impl.AbstractCluster.lambda$schemaChange$5(AbstractCluster.java:?)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:?)
	at java.util.concurrent.FutureTask.run(FutureTask.java:?)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:?)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:?)
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:?)
	at java.lang.Thread.run(Thread.java:?)
{code}"
CASSANDRA-16120,Add ability for jvm-dtest to grep instance logs,"One of the main gaps between python dtest and jvm dtest is python dtest supports the ability to grep the logs of an instance; we need this capability as some tests require validating logs were triggered.

Pydocs for common log methods 

{code}
|  grep_log(self, expr, filename='system.log', from_mark=None)
|      Returns a list of lines matching the regular expression in parameter
|      in the Cassandra log of this node
|
|  grep_log_for_errors(self, filename='system.log')
|      Returns a list of errors with stack traces
|      in the Cassandra log of this node
|
|  grep_log_for_errors_from(self, filename='system.log', seek_start=0)
{code}

{code}
|  watch_log_for(self, exprs, from_mark=None, timeout=600, process=None, verbose=False, filename='system.log')
|      Watch the log until one or more (regular) expression are found.
|      This methods when all the expressions have been found or the method
|      timeouts (a TimeoutError is then raised). On successful completion,
|      a list of pair (line matched, match object) is returned.
{code}

Below is a POC showing a way to do such logic

{code}
package org.apache.cassandra.distributed.test;
import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.UncheckedIOException;
import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.Spliterator;
import java.util.Spliterators;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Stream;
import java.util.stream.StreamSupport;
import com.google.common.io.Closeables;
import org.junit.Test;
import org.apache.cassandra.distributed.Cluster;
import org.apache.cassandra.utils.AbstractIterator;
public class AllTheLogs extends TestBaseImpl
{
   @Test
   public void test() throws IOException
   {
       try (final Cluster cluster = init(Cluster.build(1).start()))
       {
           String tag = System.getProperty(""cassandra.testtag"", ""cassandra.testtag_IS_UNDEFINED"");
           String suite = System.getProperty(""suitename"", ""suitename_IS_UNDEFINED"");
           String log = String.format(""build/test/logs/%s/TEST-%s.log"", tag, suite);
           grep(log, ""Enqueuing flush of tables"").forEach(l -> System.out.println(""I found the thing: "" + l));
       }
   }
   private static Stream<String> grep(String file, String regex) throws IOException
   {
       return grep(file, Pattern.compile(regex));
   }
   private static Stream<String> grep(String file, Pattern regex) throws IOException
   {
       BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(file), StandardCharsets.UTF_8));
       Iterator<String> it = new AbstractIterator<String>()
       {
           protected String computeNext()
           {
               try
               {
                   String s;
                   while ((s = reader.readLine()) != null)
                   {
                       Matcher m = regex.matcher(s);
                       if (m.find())
                           return s;
                   }
                   reader.close();
                   return endOfData();
               }
               catch (IOException e)
               {
                   Closeables.closeQuietly(reader);
                   throw new UncheckedIOException(e);
               }
           }
       };
       return StreamSupport.stream(Spliterators.spliteratorUnknownSize(it, Spliterator.ORDERED), false);
   }
}
{code}

And

{code}
@Test
   public void test() throws IOException
   {
       try (final Cluster cluster = init(Cluster.build(1).start()))
       {
           String tag = System.getProperty(""cassandra.testtag"", ""cassandra.testtag_IS_UNDEFINED"");
           String suite = System.getProperty(""suitename"", ""suitename_IS_UNDEFINED"");
           //TODO missing way to get node id
//            cluster.get(1);
           String log = String.format(""build/test/logs/%s/TEST-%s-node%d.log"", tag, suite, 1);
           grep(log, ""Enqueuing flush of tables"").forEach(l -> System.out.println(""I found the thing: "" + l));
       }
   }
{code}"
CASSANDRA-16008,2.2.17 fails to start up with ExceptionInInitializerError,"After the upgrade to 2.2.17, Cassandra fails to start with the following error:
{noformat}
INFO  20:28:57 JVM Arguments: [-Dcom.sun.management.jmxremote.port=7199, -Dcom.sun.management.jmxremote.ssl=false, -Dcom.sun.management.jmxremote.authenticate=false, -ea, -javaagent:/opt/cassandra/lib/jamm-0.3.0.jar, -XX:+CMSClassUnloadingEnabled, -XX:+UseThreadPriorities, -XX:ThreadPriorityPolicy=42, -Xms128m, -Xmx128m, -Xmn32m, -XX:+HeapDumpOnOutOfMemoryError, -Xss256k, -XX:StringTableSize=1000003, -XX:+UseParNewGC, -XX:+UseConcMarkSweepGC, -XX:+CMSParallelRemarkEnabled, -XX:SurvivorRatio=8, -XX:MaxTenuringThreshold=1, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+UseTLAB, -XX:+PerfDisableSharedMem, -XX:CompileCommandFile=/etc/cassandra/hotspot_compiler, -XX:CMSWaitDuration=10000, -XX:+CMSParallelInitialMarkEnabled, -XX:+CMSEdenChunksRecordAlways, -XX:CMSWaitDuration=10000, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintHeapAtGC, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -XX:+PrintPromotionFailure, -Xloggc:/opt/cassandra/logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=10, -XX:GCLogFileSize=10M, -Djava.net.preferIPv4Stack=true, -Dcassandra.jmx.local.port=7199, -XX:+DisableExplicitGC, -Djava.library.path=/opt/cassandra/lib/sigar-bin, -Dcassandra.libjemalloc=/usr/lib/x86_64-linux-gnu/libjemalloc.so.1, -XX:OnOutOfMemoryError=kill -9 %p, -Dlogback.configurationFile=logback.xml, -Dcassandra.logdir=/opt/cassandra/logs, -Dcassandra.storagedir=/opt/cassandra/data, -Dcassandra-foreground=yes]
WARN  20:28:57 Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
INFO  20:28:57 jemalloc seems to be preloaded from /usr/lib/x86_64-linux-gnu/libjemalloc.so.1
INFO  20:28:57 JMX is enabled to receive remote connections on port: 7199
WARN  20:28:57 OpenJDK is not recommended. Please upgrade to the newest Oracle Java release
INFO  20:28:57 Initializing SIGAR library
INFO  20:28:57 Checked OS settings and found them configured for optimal performance.
WARN  20:28:57 Directory /opt/cassandra/data/commitlog doesn't exist
WARN  20:28:57 Directory /opt/cassandra/data/saved_caches doesn't exist
Exception (java.lang.ExceptionInInitializerError) encountered during startup: null
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:709)
	at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:351)
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:109)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:607)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:717)
Caused by: java.lang.IllegalArgumentException: Bad configuration; unable to start server: At least one DataFileDirectory must be specified
	at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:846)
	at org.apache.cassandra.db.Keyspace.<clinit>(Keyspace.java:66)
	... 6 more
ERROR 20:28:58 Exception encountered during startup
java.lang.ExceptionInInitializerError: null
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:709) ~[apache-cassandra-2.2.17.jar:2.2.17]
	at org.apache.cassandra.service.StartupChecks$9.execute(StartupChecks.java:351) ~[apache-cassandra-2.2.17.jar:2.2.17]
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:109) ~[apache-cassandra-2.2.17.jar:2.2.17]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188) [apache-cassandra-2.2.17.jar:2.2.17]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:607) [apache-cassandra-2.2.17.jar:2.2.17]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:717) [apache-cassandra-2.2.17.jar:2.2.17]
Caused by: java.lang.IllegalArgumentException: Bad configuration; unable to start server: At least one DataFileDirectory must be specified
	at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:846) ~[apache-cassandra-2.2.17.jar:2.2.17]
	at org.apache.cassandra.db.Keyspace.<clinit>(Keyspace.java:66) ~[apache-cassandra-2.2.17.jar:2.2.17]
	... 6 common frames omitted
	at org.apache.cassandra.service.StartupChecks.verify(StartupChecks.java:109)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:188)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:607)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:717)
Caused by: java.lang.IllegalArgumentException: Bad configuration; unable to start server: At least one DataFileDirectory must be specified
	at org.apache.cassandra.config.DatabaseDescriptor.createAllDirectories(DatabaseDescriptor.java:846)
	at org.apache.cassandra.db.Keyspace.<clinit>(Keyspace.java:66)
	... 6 more
{noformat}
I've traced this down to what I believe is the issue in [https://github.com/apache/cassandra/commit/257fb0377343cbfdb58327da17f31d4eaed940f5], specifically [https://github.com/apache/cassandra/commit/257fb0377343cbfdb58327da17f31d4eaed940f5#r40944000] – the addition of an empty value for {{data_file_directories}} needs to be accompanied with a change to {{DatabaseDescriptor.java}} to handle an empty array the same way as the previous nil value was (and seed the value of {{cassandra.storagedir}} into that empty array), as was done in [https://github.com/apache/cassandra/commit/b09e60f72bb2f37235d9e9190c25db36371b3c18#diff-b66584c9ce7b64019b5db5a531deeda1] (which I believe is the origin of this change)."
CASSANDRA-15938,Fix support for adding UDT fields to clustering keys,"Adding UDT fields to clustering keys is broken in all versions, however slightly differently.

In 4.0, there will be a brief moment while schema changes are propagated during which we won’t be able to decode and compare byte sequences. Unfortunately, it is unclear what we should do in such cases, since we can’t just come up with a comparator, and we can’t ignore non-null trailing values, since this will lead to cases where compare for tuples `a;1` and `a;2` would return 0, effectively making them equal, and we don’t know how to compare unknown trailing values. Probably we should reject such query since we can’t sort correctly, but we should make the error message more descriptive than just ""Index 1 out of bounds for length 1”. The only problem is that we get this exception only on flush right now, so data already propagates to the node by that time.

In 3.0, the problem is a bit worse than that, since in 3.0 we do not ignore trailing nulls, so some of the values, written before `ALTER TYPE .. ADD` become inaccessible. Both old values, and the new ones should always be accessible.
"
CASSANDRA-15427,2.2 eclipse-warnings,"Cassandra-2.2 artifact builds are failing from eclipse-warnings.

{noformat}
# 11/11/19 2:58:41 PM UTC
# Eclipse Compiler for Java(TM) v20150120-1634, 3.10.2, Copyright IBM Corp 2000, 2013. All rights reserved.
incorrect classpath: /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/build/cobertura/classes
----------
1. ERROR in /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/CompactionManager.java (at line 880)
	ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());
	                ^^^^^^^
Resource 'scanner' should be managed by try-with-resource
----------
----------
2. ERROR in /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java (at line 257)
	scanners.add(new LeveledScanner(intersecting, range));
	             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Potential resource leak: '<unassigned Closeable value>' may not be closed
----------
----------
3. ERROR in /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/src/java/org/apache/cassandra/tools/SSTableExport.java (at line 315)
	ISSTableScanner scanner = reader.getScanner();
	                ^^^^^^^
Resource 'scanner' should be managed by try-with-resource
----------
3 problems (3 errors)
{noformat}
ref: https://builds.apache.org/job/Cassandra-2.2-artifacts/180/artifact/build/ecj/eclipse_compiler_checks.txt"
CASSANDRA-15368,Failing to flush Memtable without terminating process results in permanent data loss,"{{Memtable}} do not contain records that cover a precise contiguous range of {{ReplayPosition}}, since there are only weak ordering constraints when rolling over to a new {{Memtable}} - the last operations for the old {{Memtable}} may obtain their {{ReplayPosition}} after the first operations for the new {{Memtable}}.

Unfortunately, we treat the {{Memtable}} range as contiguous, and invalidate the entire range on flush.  Ordinarily we only invalidate records when all prior {{Memtable}} have also successfully flushed.  However, in the event of a flush that does not terminate the process (either because of disk failure policy, or because it is a software error), the later flush is able to invalidate the region of the commit log that includes records that should have been flushed in the prior {{Memtable}}

More problematically, this can also occur on restart without any associated flush failure, as we use commit log boundaries written to our flushed sstables to filter {{ReplayPosition}} on recovery, which is meant to replicate our {{Memtable}} flush behaviour above.  However, we do not know that earlier flushes have completed, and they may complete successfully out-of-order.  So any flush that completes before the process terminates, but began after another flush that _doesn’t_ complete before the process terminates, has the potential to cause permanent data loss.
"
CASSANDRA-14814,"Can anyone give me some pointers where to look, or start to look for where this issue is coming from?   On startup I get this in the logs and my service goes into a failed state. Active(exited)","Hi,
Can anyone give me some pointers where to look, or start to look for where this issue is coming from?
On startup I get this in the logs and my service goes into a failed state. Active(exited)

ERROR [main] 2018-08-28 16:25:01,443 CassandraDaemon.java:655 - Exception encountered during startup
java.lang.IllegalStateException: Failed to bind port 9042 on 192.168.118.237.
	at org.apache.cassandra.transport.Server.run(Server.java:187) ~[apache-cassandra-2.2.11.jar:2.2.11]
	at org.apache.cassandra.transport.Server.start(Server.java:120) ~[apache-cassandra-2.2.11.jar:2.2.11]
	at org.apache.cassandra.service.CassandraDaemon.start(CassandraDaemon.java:446) [apache-cassandra-2.2.11.jar:2.2.11]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:548) [apache-cassandra-2.2.11.jar:2.2.11]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:642) [apache-cassandra-2.2.11.jar:2.2.11]

WARN  [main] 2018-08-28 16:24:57,894 CLibrary.java:159 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
WARN  [main] 2018-08-28 16:24:57,894 StartupChecks.java:118 - jemalloc shared library could not be preloaded to speed up memory allocations
WARN  [main] 2018-08-28 16:24:57,895 StartupChecks.java:150 - JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.


WARN  [SharedPool-Worker-6] 2018-08-28 16:28:21,624 SliceQueryFilter.java:307 - Read 643 live and 1286 tombstone cells in system.schema_columns for key: hdb (see tombstone_warn_threshold). 2147483647 columns were requested, slices=[-]
WARN  [SharedPool-Worker-4] 2018-08-28 16:29:22,318 SliceQueryFilter.java:307 - Read 643 live and 1286 tombstone cells in system.schema_columns for key: hdb (see tombstone_warn_threshold). 2147483647 columns were requested, slices=[-]


WARN  [main] 2018-08-28 16:55:09,948 StartupChecks.java:118 - jemalloc shared library could not be preloaded to speed up memory allocations
WARN  [main] 2018-08-28 16:55:09,948 StartupChecks.java:150 - JMX is not enabled to receive remote connections. Please see cassandra-env.sh for more info.
 
Regards,
Ravi"
CASSANDRA-14649,Index summaries fail when their size gets > 2G and use more space than necessary,"After building a summary, {{IndexSummaryBuilder}} tries to trim the memory writers by calling {{SafeMemoryWriter.setCapacity(capacity())}}. Instead of trimming, this ends up allocating at least as much extra space and failing the {{Buffer.position()}} call when the size is greater than {{Integer.MAX_VALUE}}."
CASSANDRA-14423,SSTables stop being compacted,"So seeing a problem in 3.11.0 where SSTables are being lost from the view and not being included in compactions/as candidates for compaction. It seems to get progressively worse until there's only 1-2 SSTables in the view which happen to be the most recent SSTables and thus compactions completely stop for that table.

The SSTables seem to still be included in reads, just not compactions.

The issue can be fixed by restarting C*, as it will reload all SSTables into the view, but this is only a temporary fix. User defined/major compactions still work - not clear if they include the result back in the view but is not a good work around.

This also results in a discrepancy between SSTable count and SSTables in levels for any table using LCS.
{code:java}
Keyspace : xxx
Read Count: 57761088
Read Latency: 0.10527088681224288 ms.
Write Count: 2513164
Write Latency: 0.018211106398149903 ms.
Pending Flushes: 0
Table: xxx
SSTable count: 10
SSTables in each level: [2, 0, 0, 0, 0, 0, 0, 0, 0]
Space used (live): 894498746
Space used (total): 894498746
Space used by snapshots (total): 0
Off heap memory used (total): 11576197
SSTable Compression Ratio: 0.6956629530569777
Number of keys (estimate): 3562207
Memtable cell count: 0
Memtable data size: 0
Memtable off heap memory used: 0
Memtable switch count: 87
Local read count: 57761088
Local read latency: 0.108 ms
Local write count: 2513164
Local write latency: NaN ms
Pending flushes: 0
Percent repaired: 86.33
Bloom filter false positives: 43
Bloom filter false ratio: 0.00000
Bloom filter space used: 8046104
Bloom filter off heap memory used: 8046024
Index summary off heap memory used: 3449005
Compression metadata off heap memory used: 81168
Compacted partition minimum bytes: 104
Compacted partition maximum bytes: 5722
Compacted partition mean bytes: 175
Average live cells per slice (last five minutes): 1.0
Maximum live cells per slice (last five minutes): 1
Average tombstones per slice (last five minutes): 1.0
Maximum tombstones per slice (last five minutes): 1
Dropped Mutations: 0
{code}
Also for STCS we've confirmed that SSTable count will be different to the number of SSTables reported in the Compaction Bucket's. In the below example there's only 3 SSTables in a single bucket - no more are listed for this table. Compaction thresholds haven't been modified for this table and it's a very basic KV schema.
{code:java}
Keyspace : yyy
    Read Count: 30485
    Read Latency: 0.06708991307200263 ms.
    Write Count: 57044
    Write Latency: 0.02204061776873992 ms.
    Pending Flushes: 0
        Table: yyy
        SSTable count: 19
        Space used (live): 18195482
        Space used (total): 18195482
        Space used by snapshots (total): 0
        Off heap memory used (total): 747376
        SSTable Compression Ratio: 0.7607394576769735
        Number of keys (estimate): 116074
        Memtable cell count: 0
        Memtable data size: 0
        Memtable off heap memory used: 0
        Memtable switch count: 39
        Local read count: 30485
        Local read latency: NaN ms
        Local write count: 57044
        Local write latency: NaN ms
        Pending flushes: 0
        Percent repaired: 79.76
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 690912
        Bloom filter off heap memory used: 690760
        Index summary off heap memory used: 54736
        Compression metadata off heap memory used: 1880
        Compacted partition minimum bytes: 73
        Compacted partition maximum bytes: 124
        Compacted partition mean bytes: 96
        Average live cells per slice (last five minutes): NaN
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): NaN
        Maximum tombstones per slice (last five minutes): 0
        Dropped Mutations: 0 
{code}
{code:java}
Apr 27 03:10:39 cassandra[9263]: TRACE o.a.c.d.c.SizeTieredCompactionStrategy Compaction buckets are [[BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67168-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67167-big-Data.db'), BigTableReader(path='/var/lib/cassandra/data/yyy/yyy-5f7a2d60e4a811e6868a8fd39a64fd59/mc-67166-big-Data.db')]]
{code}
Also for every LCS table we're seeing the following warning being spammed (seems to be in line with anticompaction spam):
{code:java}
Apr 26 21:30:09 cassandra[9263]: WARN  o.a.c.d.c.LeveledCompactionStrategy Live sstable /var/lib/cassandra/data/xxx/xxx-8c3ef9e0e3fc11e6868a8fd39a64fd59/mc-79024-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.{code}
This is a vnodes cluster with 256 tokens per node, and the only thing that seems like it could be causing issues is anticompactions.

CASSANDRA-14079 might be related but doesn't quite describe the same issue, and in this case we're using only a single disk for data. Have yet to reproduce but figured worth reporting here first."
CASSANDRA-14384,If fsync fails it's always an issue and continuing execution is suspect,"We can't catch fsync errors and continue so we shouldn't have code that does that in C*. There was a Postgres bug where fsync returned an error and the FS lost data, but subsequent fsyncs succeeded.

The [LastErrorException code in NativeLibrary.trySync|https://github.com/apache/cassandra/commit/be313935e54be450d9aaabda7965a2f266e922c9#diff-4258621cdf765f0fea6770db5d40038fR307] looks a little janky. What's up with that? When would trySync be something we would merely try? If try is good enough why do it at all considering try is the default behavior of a series of unsynced filesystem operations.

Also when we fsync in FD it's not just fsyncing that file the FS is potentially fsyncing other data and the error code we get could be related to that other data so we can't safely ignore it. The filesystem could be internally inconsistent as well. This happens because the FS journaling may force the FS to flush other data as well to preserve the ordering requirements of journaled metadata.

If we ignore fsync errors it needs to be for whitelisted reasons such as a bad FD.

I know we have FSErrorHandler and it makes sense for reads, but I'm not sold on it being the right answer for writes. We don't retry flushing a memtable or writing to the commit log to my knowledge. We could go read only and I need to check if that is w"
CASSANDRA-14284,Chunk checksum test needs to occur before uncompress to avoid JVM crash,"While checksums are (generally) performed on compressed data, the checksum test when reading is currently (in all variants of C* 2.x, 3.x I've looked at) done [on the compressed data] only after the uncompress operation has completed. 

The issue here is that LZ4_decompress_fast (as documented in e.g. [https://github.com/lz4/lz4/blob/dev/lib/lz4.h#L214)] can result in memory overruns when provided with malformed source data. This in turn can (and does, e.g. in CASSANDRA-13757) lead to JVM crashes during the uncompress of corrupted chunks. The checksum operation would obviously detect the issue, but we'd never get to it if the JVM crashes first.

Moving the checksum test of the compressed data to before the uncompress operation (in cases where the checksum is done on compressed data) will resolve this issue.

-----------------------------

The check-only-after-doing-the-decompress logic appears to be in all current releases.

Here are some samples at different evolution points :

3.11.2:

[https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L146]

https://github.com/apache/cassandra/blob/cassandra-3.11.2/src/java/org/apache/cassandra/io/util/CompressedChunkReader.java#L207

 

3.5:

 [https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L135]

[https://github.com/apache/cassandra/blob/cassandra-3.5/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L196]

2.1.17:

 [https://github.com/apache/cassandra/blob/cassandra-2.1.17/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java#L122]"
CASSANDRA-14092,Max ttl of 20 years will overflow localDeletionTime,"CASSANDRA-4771 added a max value of 20 years for ttl to protect against [year 2038 overflow bug|https://en.wikipedia.org/wiki/Year_2038_problem] for {{localDeletionTime}}.

It turns out that next year the {{localDeletionTime}} will start overflowing with the maximum ttl of 20 years ({{System.currentTimeMillis() + ttl(20 years) > Integer.MAX_VALUE}}), so we should remove this limitation."
CASSANDRA-13840,GraphiteReporter stops reporting from Memory concurrent access,"Gauges in the GraphiteReporter can, when getting compression offheap size, trip over the assertion in {{Memory.size()}}. As the GraphiteReporter only handles Exceptions this crashes the reporter and it sends no more metrics.

[~benedict] described it [here|https://issues.apache.org/jira/browse/CASSANDRA-9625?focusedCommentId=14711914&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14711914] and [~jbellis] confirmed that it's now ok to remove the assertion.

{noformat}
java.lang.AssertionError: null
        at org.apache.cassandra.io.util.Memory.size(Memory.java:359) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.io.compress.CompressionMetadata.offHeapSize(CompressionMetadata.java:167) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.io.sstable.SSTableReader.getCompressionMetadataOffHeapSize(SSTableReader.java:1315) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.metrics.ColumnFamilyMetrics$30.value(ColumnFamilyMetrics.java:573) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at org.apache.cassandra.metrics.ColumnFamilyMetrics$30.value(ColumnFamilyMetrics.java:568) ~[apache-cassandra-2.1.12.jar:2.1.12]
        at com.yammer.metrics.reporting.GraphiteReporter.processGauge(GraphiteReporter.java:309) ~[metrics-graphite-2.2.0.jar:na]
        at com.yammer.metrics.reporting.GraphiteReporter.processGauge(GraphiteReporter.java:26) ~[metrics-graphite-2.2.0.jar:na]
        at com.yammer.metrics.core.Gauge.processWith(Gauge.java:28) ~[metrics-core-2.2.0.jar:na]
        at com.yammer.metrics.reporting.GraphiteReporter.printRegularMetrics(GraphiteReporter.java:251) ~[metrics-graphite-2.2.0.jar:na]
        at com.yammer.metrics.reporting.GraphiteReporter.run(GraphiteReporter.java:216) ~[metrics-graphite-2.2.0.jar:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_101]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_101]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_101]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_101]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{noformat}"
CASSANDRA-13801,CompactionManager sometimes wrongly determines that a background compaction is running for a particular table,"Sometimes after writing different rows to a table, then doing a blocking flush, if you alter the compaction strategy, then run background compaction and wait for it to finish, {{CompactionManager}} may decide that there's an ongoing compaction for that same table.
This may happen even though logs don't indicate that to be the case (compaction may still be running for system_schema tables)."
CASSANDRA-13546,Getting error while adding a node in existing cluster,"Getting error after adding a node in existing cluster, error are coming after 1 hour after started service on seed node and due to this load got increased vastly on seed node not on new node.

Below errors from system.log on seed node:

{code}
ERROR [NonPeriodicTasks:1] 2017-05-18 10:03:01,819 CassandraDaemon.java:153 - Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.io.util.Memory.free(Memory.java:300) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.utils.obs.OffHeapBitSet.close(OffHeapBitSet.java:143) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.utils.BloomFilter.close(BloomFilter.java:116) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.SSTableReader$6.run(SSTableReader.java:645) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_71]
        at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_71]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(Unknown Source) ~[na:1.7.0_71]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source) ~[na:1.7.0_71]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_71]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_71]
        at java.lang.Thread.run(Unknown Source) [na:1.7.0_71]
INFO  [CompactionExecutor:162] 2017-05-18 10:03:01,820 ColumnFamilyStore.java:840 - Enqueuing flush of compactions_in_progress: 164 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:152] 2017-05-18 10:03:01,821 Memtable.java:325 - Writing Memtable-compactions_in_progress@731506998(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:152] 2017-05-18 10:03:01,825 Memtable.java:364 - Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-52434-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1495106933696, position=9156004)
ERROR [CompactionExecutor:162] 2017-05-18 10:03:01,829 CassandraDaemon.java:153 -
 Exception in thread Thread[CompactionExecutor:162,1,main]
java.lang.AssertionError: null
        at org.apache.cassandra.io.util.Memory.free(Memory.java:300) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.utils.obs.OffHeapBitSet.close(OffHeapBitSet.java:143) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.utils.BloomFilter.close(BloomFilter.java:116) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.SSTableWriter.abort(SSTableWriter.java:345) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.SSTableRewriter.abort(SSTableRewriter.java:198) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:204) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:75) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:232) ~[apache-cassandra-2.1.2.jar:2.1.2-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_71]
        at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_71]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_71]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_71]
        at java.lang.Thread.run(Unknown Source) [na:1.7.0_71]
INFO  [main] 2017-05-18 11:06:56,032 CassandraDaemon.java:89 - Hostname: csdb2-dl.intermesh.net
INFO  [main] 2017-05-18 11:06:56,061 YamlConfigurationLoader.java:92 - Loading settings from file:/etc/cassandra/default.conf/cassandra.yaml
{code}
"
CASSANDRA-13432,MemtableReclaimMemory can get stuck because of lack of timeout in getTopLevelColumns(),"This might affect 3.x too, I'm not sure.

{code}
$ nodetool tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
MutationStage                     0         0       32135875         0                 0
ReadStage                       114         0       29492940         0                 0
RequestResponseStage              0         0       86090931         0                 0
ReadRepairStage                   0         0         166645         0                 0
CounterMutationStage              0         0              0         0                 0
MiscStage                         0         0              0         0                 0
HintedHandoff                     0         0             47         0                 0
GossipStage                       0         0         188769         0                 0
CacheCleanupExecutor              0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
CompactionExecutor                0         0          86835         0                 0
ValidationExecutor                0         0              0         0                 0
MigrationStage                    0         0              0         0                 0                                    
AntiEntropyStage                  0         0              0         0                 0                                    
PendingRangeCalculator            0         0             92         0                 0                                    
Sampler                           0         0              0         0                 0                                    
MemtableFlushWriter               0         0            563         0                 0                                    
MemtablePostFlush                 0         0           1500         0                 0                                    
MemtableReclaimMemory             1        29            534         0                 0                                    
Native-Transport-Requests        41         0       54819182         0              1896                            
{code}

{code}
""MemtableReclaimMemory:195"" - Thread t@6268
   java.lang.Thread.State: WAITING
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)
	at org.apache.cassandra.utils.concurrent.WaitQueue$AbstractSignal.awaitUninterruptibly(WaitQueue.java:283)
	at org.apache.cassandra.utils.concurrent.OpOrder$Barrier.await(OpOrder.java:417)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush$1.runMayThrow(ColumnFamilyStore.java:1151)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- locked <6e7b1160> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""SharedPool-Worker-195"" - Thread t@989
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.db.RangeTombstoneList.addInternal(RangeTombstoneList.java:690)
	at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:650)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:171)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:143)
	at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:240)
	at org.apache.cassandra.db.ArrayBackedSortedColumns.delete(ArrayBackedSortedColumns.java:483)
	at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:153)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:184)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:156)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:146)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:125)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:263)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:108)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:82)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:69)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:316)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:2015)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1858)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:353)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:85)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- None

""SharedPool-Worker-206"" - Thread t@1014
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.db.RangeTombstoneList.addInternal(RangeTombstoneList.java:690)
	at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:650)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:171)
	at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:143)
	at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:240)
	at org.apache.cassandra.db.ArrayBackedSortedColumns.delete(ArrayBackedSortedColumns.java:483)
	at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:153)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:184)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:156)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:146)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:89)
	at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:48)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:105)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:82)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:69)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:316)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:2015)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1858)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:353)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:85)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- None
{code}

As you can see MemtableReclaimMemory is waiting on the read barrier to be released, but there are two queries currently being executed which are locking this.

Since most of the time is spent pretty low in the stack, these read operations will never timeout (they are reading rows with tons of tombstones).

We also can easily detect or purge the offending line because there is no easy way to find out which partition is currently being read.

The TombstoneFailureThreshold should also protect us, but it is probably being checked too high in the call stack.

Looks like RangeTombstoneList or DeletionInfo should also check for DatabaseDescriptor.getTombstoneFailureThreshold()"
CASSANDRA-13204,Thread Leak in OutboundTcpConnection,"We found threads leaking from OutboundTcpConnection to machines which are not part of the cluster and still in Gossip for some reason. There are two issues here, this JIRA will cover the second one which is most important. 



1) First issue is that Gossip has information about machines not in the ring which has been replaced out. It causes Cassandra to connect to those machines but due to internode auth, it wont be able to connect to them at the socket level.  

2) Second issue is a race between creating a connection and closing a connections which is triggered by the gossip bug explained above. Let me try to explain it using the code

In OutboundTcpConnection, we are calling closeSocket(true) which will set isStopped=true and also put a close sentinel into the queue to exit the thread. On the ack connection, Gossip tries to send a message which calls connect() which will block for 10 seconds which is RPC timeout. The reason we will block is because Cassandra might not be running there or internode auth will not let it connect. During this 10 seconds, if Gossip calls closeSocket, it will put close sentinel into the queue. When we return from the connect method after 10 seconds, we will clear the backlog queue causing this thread to leak. 

Proofs from the heap dump of the affected machine which is leaking threads 
1. Only ack connection is leaking and not the command connection which is not used by Gossip. 
2. We see thread blocked on the backlog queue, isStopped=true and backlog queue is empty. This is happening on the threads which have already leaked. 
3. A running thread was blocked on the connect waiting for timeout(10 seconds) and we see backlog queue to contain the close sentinel. Once the connect will return false, we will clear the backlog and this thread will have leaked.  


Interesting bits from j stack 
1282 number of threads for ""MessagingService-Outgoing-/<IP-Address>""

Thread which is about to leak:
""MessagingService-Outgoing-/<IP Address>"" 
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:454)
	at sun.nio.ch.Net.connect(Net.java:446)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:648)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	- locked <> (a java.lang.Object)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:137)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.newSocket(OutboundTcpConnectionPool.java:119)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:381)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:217)

Thread already leaked:
""MessagingService-Outgoing-/<IP Address>""
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at org.apache.cassandra.utils.CoalescingStrategies$DisabledCoalescingStrategy.coalesceInternal(CoalescingStrategies.java:482)
	at org.apache.cassandra.utils.CoalescingStrategies$CoalescingStrategy.coalesce(CoalescingStrategies.java:213)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:190)
"
CASSANDRA-13114,Upgrade netty to 4.0.44 to fix memory leak with client encryption,"https://issues.apache.org/jira/browse/CASSANDRA-12032 updated netty for Cassandra 3.8, but this wasn't backported. Netty 4.0.23, which ships with Cassandra 3.0.x, has some serious bugs around memory handling for SSL connections.

It would be nice if both were updated to 4.0.42, a version released this year.

4.0.23 makes it impossible for me to run SSL, because nodes run out of memory every ~30 minutes. This was fixed in 4.0.27."
CASSANDRA-13103,incorrect jvm metric names,"Some jvm metrics have a double dot in name like:
jvm.memory..total.max , jvm.memory..total.init (etc).
it seems that an extra dot is added at the end of the name in CassandraDaemon.java, around line 367 (in 3.0.10):
...
                // enable metrics provided by metrics-jvm.jar
                CassandraMetricsRegistry.Metrics.register(""jvm.buffers."", new BufferPoolMetricSet(ManagementFactory.getPlatformMBeanServer()));
                CassandraMetricsRegistry.Metrics.register(""jvm.gc."", new GarbageCollectorMetricSet());
                CassandraMetricsRegistry.Metrics.register(""jvm.memory."", new MemoryUsageGaugeSet());

and also added in append method of MetricRegistry.
Call stack is:
MetricRegistry>>registerAll(String prefix, MetricSet metrics)
MetricRegistry>>static String name(String name, String... names)
MetricRegistry>>static void append(StringBuilder builder, String part)

and in append the dot is also added:
...
            if(builder.length() > 0) {
                builder.append('.');
            }
            builder.append(part);
...

The codahale MetricRegistry class seems to have no recent modification of name or append methods, so it look like a small bug.
May be the fix could be to simply not to add  the final dot in the metric name, ie  ""jvm.buffers""  instead of ""jvm.buffers.""
"
CASSANDRA-13067,Integer overflows with file system size reported by Amazon Elastic File System (EFS),"When not explicitly configured Cassandra uses [{{nio.FileStore.getTotalSpace}}|https://docs.oracle.com/javase/7/docs/api/java/nio/file/FileStore.html] to determine the total amount of available space in order to [calculate the preferred commit log size|https://github.com/apache/cassandra/blob/cassandra-3.9/src/java/org/apache/cassandra/config/DatabaseDescriptor.java#L553]. [Amazon EFS|https://aws.amazon.com/efs/] instances report a filesystem size of 8 EiB when empty. [{{getTotalSpace}} causes an integer overflow (JDK-8162520)|https://bugs.openjdk.java.net/browse/JDK-8162520] and returns a negative number, resulting in a negative preferred size and causing the checked integer to throw.

Overriding {{commitlog_total_space_in_mb}} is not sufficient as [{{DataDirectory.getAvailableSpace}}|https://github.com/apache/cassandra/blob/cassandra-3.9/src/java/org/apache/cassandra/db/Directories.java#L550] makes use of {{nio.FileStore.getUsableSpace}}.

[AMQ-6441] is a comparable issue in ActiveMQ."
CASSANDRA-13011,heap exhaustion when cleaning table with wide partitions and a secondary index attached to it,"We have a table with rather wide partitions and a secondary index attached to it. When tried to clean unused data on a node after expansion of our cluster via issuing {{nodetool cleanup}} command we observed a heap exhaustion issue. The culprit appears to be in method {{org.apache.cassandra.db.compaction.CompactionManager.CleanupStrategy.Full.cleanup}} as it tries to remove related secondary index entries. The method first populates a list will all cells belonging to the given partition...
{code:java}
                while (row.hasNext())
                {
                    OnDiskAtom column = row.next();

                    if (column instanceof Cell && cfs.indexManager.indexes((Cell) column))
                    {
                        if (indexedColumnsInRow == null)
                            indexedColumnsInRow = new ArrayList<>();

                        indexedColumnsInRow.add((Cell) column);
                    }
                }
{code}

... and then submits it to the index manager for removal.
{code:java}
                    // acquire memtable lock here because secondary index deletion may cause a race. See CASSANDRA-3712
                    try (OpOrder.Group opGroup = cfs.keyspace.writeOrder.start())
                    {
                        cfs.indexManager.deleteFromIndexes(row.getKey(), indexedColumnsInRow, opGroup);
                    }
{code}

After imposing a limit on array size and implementing some sort of pagination the cleanup worked fine."
CASSANDRA-13006,Disable automatic heap dumps on OOM error,"With CASSANDRA-9861, a change was added to enable collecting heap dumps by default if the process encountered an OOM error. These heap dumps are stored in the Apache Cassandra home directory unless configured otherwise (see [Cassandra Support Document|https://support.datastax.com/hc/en-us/articles/204225959-Generating-and-Analyzing-Heap-Dumps] for this feature).
 
The creation and storage of heap dumps aides debugging and investigative workflows, but is not be desirable for a production environment where these heap dumps may occupy a large amount of disk space and require manual intervention for cleanups. 
 
Managing heap dumps on out of memory errors and configuring the paths for these heap dumps are available as JVM options in JVM. The current behavior conflicts with the Boolean JVM flag HeapDumpOnOutOfMemoryError. 
 
A patch can be proposed here that would make the heap dump on OOM error honor the HeapDumpOnOutOfMemoryError flag. Users who would want to still generate heap dumps on OOM errors can set the -XX:+HeapDumpOnOutOfMemoryError JVM option.
"
CASSANDRA-12920,Possible messaging latency caused by not flushing in time,"Not sure if this can be improved. We're using Cassandra 2.2.5. Cassandra considers messages whose payload size <= 64KB as small messages. For response for such as digest request, it's very likely a small message. Even quite a few such responses cannot fill in the output buffer and thus trigger a flush.

One possible issue is that, we use conditions count == 1 && backlog.isEmpty() to decide whether to flush the output stream. That means if the backlog is empty, we won't flush the output stream even we drained, say, 10 messages whose sizes sum < 64K. This may cause delay for small messages. Shouldn't we flush after writing the last drained message if backlog is empty? Of course if backlog is not empty, we can continue draining more messages and very likely trigger a flush very soon.

Here are some tracing events that show the latency. Be noted that the involved nodes are in the same data center whose round trip network latency is well below 1ms. Alao we have disabled message coalescing due to CASSANDRA-12676.

{noformat}
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127c7c28-aaa6-11e6-941f-f3a2e2759bd2 |                                       Enqueuing response to /*******32.28 |  *******29.3 |           1199 |                     SharedPool-Worker-4
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127cca45-aaa6-11e6-941f-f3a2e2759bd2 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******29.3 |           2667 | MessagingService-Outgoing-/*******32.28
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127cca40-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******29.3 | *******32.28 |         152635 |  MessagingService-Incoming-/*******29.3
 126571b0-aaa6-11e6-ab2b-77eb6f529c59 | 127cca41-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******29.3 | *******32.28 |         152665 |                    SharedPool-Worker-45


 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b08e68a-aaa6-11e6-8dce-951d39203586 |                                       Enqueuing response to /*******32.28 |  *******29.5 |            532 |                   SharedPool-Worker-178
 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b090d90-aaa6-11e6-8dce-951d39203586 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******29.5 |            579 | MessagingService-Outgoing-/*******32.28
 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b090d90-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******29.5 | *******32.28 |         143865 |  MessagingService-Incoming-/*******29.5
 1af31490-aaa6-11e6-ab2b-77eb6f529c59 | 1b090d91-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******29.5 | *******32.28 |         143908 |                 
   SharedPool-Worker-41
 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b0934a8-aaa6-11e6-8dce-951d39203586 |                                       Enqueuing response to /*******32.28 |  *******29.5 |           1632 |                 
  SharedPool-Worker-159 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b0934a9-aaa6-11e6-8dce-951d39203586 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******29.5 |           1660 | MessagingService
-Outgoing-/*******32.28 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b095bb0-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******29.5 | *******32.28 |         129813 |  MessagingServic
e-Incoming-/*******29.5 1af58590-aaa6-11e6-ab2b-77eb6f529c59 | 1b095bb1-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******29.5 | *******32.28 |         129835 |                 
    SharedPool-Worker-3

 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 3029fd17-aaa6-11e6-a5cc-8b24438aa927 |                                       Enqueuing response to /*******32.28 |  *******30.8 |           1686 |                   SharedPool-Worker-271
 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 302ae770-aaa6-11e6-a5cc-8b24438aa927 |                         Sending REQUEST_RESPONSE message to /*******32.28 |  *******30.8 |           6938 | MessagingService-Outgoing-/*******32.28
 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 302ae770-aaa6-11e6-ab2b-77eb6f529c59 |                       REQUEST_RESPONSE message received from /*******30.8 | *******32.28 |         171380 |  MessagingService-Incoming-/*******30.8
 3010cfc0-aaa6-11e6-ab2b-77eb6f529c59 | 302ae771-aaa6-11e6-ab2b-77eb6f529c59 |                                     Processing response from /*******30.8 | *******32.28 |         171472 |                    SharedPool-Worker-55
{noformat}
"
CASSANDRA-12899,stand alone sstableupgrade prints LEAK DETECTED warnings some times,"the stand alone sstableupgrade prints LEAK DETECTED warnings some times.

{code}
sstableupgrade keyspace1 standard1
WARN  04:22:41,942 Small commitlog volume detected at /var/lib/cassandra/commitlog; setting commitlog_total_space_in_mb to 8047.  You can override this in cassandra.yaml
WARN  04:22:41,948 Small cdc volume detected at /var/lib/cassandra/cdc_raw; setting cdc_total_space_in_mb to 4023.  You can override this in cassandra.yaml
WARN  04:22:41,950 Only 29.016GiB free across all data volumes. Consider adding more capacity to your cluster or removing obsolete snapshots
Found 0 sstables that need upgrading.
ERROR 04:22:45,472 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7b135bd7) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@2080155253:/var/lib/cassandra/data/keyspace1/standard1-24e490e1a6fb11e68ebe232ff1b97ca7/mc-17-big was not released before the reference was garbage collected
ERROR 04:22:45,472 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@7b135bd7:
Thread[main,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:245)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:175)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:97)
	at org.apache.cassandra.io.sstable.format.SSTableReader.<init>(SSTableReader.java:237)
	at org.apache.cassandra.io.sstable.format.big.BigTableReader.<init>(BigTableReader.java:57)
	at org.apache.cassandra.io.sstable.format.big.BigFormat$ReaderFactory.open(BigFormat.java:101)
	at org.apache.cassandra.io.sstable.format.SSTableReader.internalOpen(SSTableReader.java:638)
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:506)
	at org.apache.cassandra.io.sstable.format.SSTableReader.openNoValidation(SSTableReader.java:401)
	at org.apache.cassandra.tools.StandaloneUpgrader.main(StandaloneUpgrader.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at com.datastax.bdp.tools.ShellToolWrapper.main(ShellToolWrapper.java:34)
{code}"
CASSANDRA-12886,Streaming failed due to SSL Socket connection reset,"While running ""nodetool repair"", I see many instances of ""javax.net.ssl.SSLException: java.net.SocketException: Connection reset"" in system.logs on some nodes in the cluster. Timestamps correspond to streaming source/initiator's error messages of ""sync failed between ...""

Setup: 
- Cassandra 3.7.01 
- CentOS 6.7 in AWS (multi-region)
- JDK version: {noformat}
java version ""1.8.0_102""
Java(TM) SE Runtime Environment (build 1.8.0_102-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)
{noformat}
- cassandra.yaml:
{noformat}
server_encryption_options:
    internode_encryption: all
    keystore: [path]
    keystore_password: [password]
    truststore: [path]
    truststore_password: [password]
    # More advanced defaults below:
    # protocol: TLS
    # algorithm: SunX509
    # store_type: JKS
    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]
    require_client_auth: false
{noformat}

Error messages in system.log on the target host:
{noformat}
ERROR [STREAM-OUT-/54.247.111.232:7001] 2016-11-07 07:30:56,475 StreamSession.java:529 - [Stream #e14abcb0-a4bb-11e6-9758-55b9ac38b78e] Streaming error occurred on session with peer 54.247.111.232
javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Connection reset
        at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541) ~[na:1.8.0_102]
        at sun.security.ssl.SSLSocketImpl.checkWrite(SSLSocketImpl.java:1553) ~[na:1.8.0_102]
        at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:71) ~[na:1.8.0_102]
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) ~[na:1.8.0_102]
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) ~[na:1.8.0_102]
        at org.apache.cassandra.io.util.WrappedDataOutputStreamPlus.flush(WrappedDataOutputStreamPlus.java:66) ~[apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:371) [apache-cassandra-3.7.0.jar:3.7.0]
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:342) [apache-cassandra-3.7.0.jar:3.7.0]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_102]
Caused by: javax.net.ssl.SSLException: java.net.SocketException: Connection reset
{noformat}"
CASSANDRA-12841,testall failure in org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest-compression,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/597/testReport/org.apache.cassandra.db.compaction/NeverPurgeTest/minorNeverPurgeTombstonesTest_compression/

{code}
Error Message

Memory was freed by Thread[NonPeriodicTasks:1,5,main]
{code}{code}
Stacktrace

junit.framework.AssertionFailedError: Memory was freed by Thread[NonPeriodicTasks:1,5,main]
	at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:103)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:260)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:223)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBufferMmap(CompressedRandomAccessReader.java:168)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:226)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:303)
	at org.apache.cassandra.io.util.AbstractDataInput.readInt(AbstractDataInput.java:202)
	at org.apache.cassandra.io.util.AbstractDataInput.readLong(AbstractDataInput.java:264)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:131)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52)
	at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:169)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.verifyContainsTombstones(NeverPurgeTest.java:114)
	at org.apache.cassandra.db.compaction.NeverPurgeTest.minorNeverPurgeTombstonesTest(NeverPurgeTest.java:85)
{code}

Related failure:

http://cassci.datastax.com/job/cassandra-2.2_testall/598/testReport/org.apache.cassandra.db.compaction/NeverPurgeTest/minorNeverPurgeTombstonesTest/"
CASSANDRA-12813,NPE in auth for bootstrapping node,"{code}
ERROR [SharedPool-Worker-1] 2016-10-19 21:40:25,991 Message.java:617 - Unexpected exception during request; channel = [id: 0x15eb017f, /<public IP omitted>:40869 => /10.0.0.254:9042]
java.lang.NullPointerException: null
	at org.apache.cassandra.auth.PasswordAuthenticator.doAuthenticate(PasswordAuthenticator.java:144) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.auth.PasswordAuthenticator.authenticate(PasswordAuthenticator.java:86) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.auth.PasswordAuthenticator.access$100(PasswordAuthenticator.java:54) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.auth.PasswordAuthenticator$PlainTextSaslAuthenticator.getAuthenticatedUser(PasswordAuthenticator.java:182) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:78) ~[apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:513) [apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:407) [apache-cassandra-3.0.9.jar:3.0.9]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.0.9.jar:3.0.9]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.0.9.jar:3.0.9]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
{code}

I have a node that has been joining for around 24 hours.  My application is configured with the IP address of the joining node in the list of nodes to connect to (ruby driver), and I have been getting around 200 events of this NPE per hour.  I removed the IP of the joining node from the list of nodes for my app to connect to and the errors stopped."
CASSANDRA-12796,Heap exhaustion when rebuilding secondary index over a table with wide partitions,"We have a table with rather wide partition and a secondary index defined over it. As soon as we try to rebuild the index we observed exhaustion of Java heap and eventual OOM error. After a lengthy investigation we have managed to find a culprit which appears to be a wrong granule of barrier issuances in method {{org.apache.cassandra.db.Keyspace.indexRow}}:

{code}
        try (OpOrder.Group opGroup = cfs.keyspace.writeOrder.start()){html}
        {
            Set<SecondaryIndex> indexes = cfs.indexManager.getIndexesByNames(idxNames);

            Iterator<ColumnFamily> pager = QueryPagers.pageRowLocally(cfs, key.getKey(), DEFAULT_PAGE_SIZE);
            while (pager.hasNext())
            {
                ColumnFamily cf = pager.next();
                ColumnFamily cf2 = cf.cloneMeShallow();
                for (Cell cell : cf)
                {
                    if (cfs.indexManager.indexes(cell.name(), indexes))
                        cf2.addColumn(cell);
                }
                cfs.indexManager.indexRow(key.getKey(), cf2, opGroup);
            }
        }
{code}

Please note the operation group granule is a partition of the source table which poses a problem for wide partition tables as flush runnable ({{org.apache.cassandra.db.ColumnFamilyStore.Flush.run()}}) won't proceed with flushing secondary index memtable before completing operations prior recent issue of the barrier. In our situation the flush runnable waits until whole wide partition gets indexed into the secondary index memtable before flushing it. This causes an exhaustion of the heap and eventual OOM error.

After we changed granule of barrier issue in method {{org.apache.cassandra.db.Keyspace.indexRow}} to query page as opposed to table partition secondary index (see [https://github.com/mmajercik/cassandra/commit/7e10e5aa97f1de483c2a5faf867315ecbf65f3d6?diff=unified]), rebuild started to work without heap exhaustion. "
CASSANDRA-12765,SSTable ignored incorrectly with partition level tombstone,"{noformat}
CREATE TABLE test.payload(
  bucket_id TEXT,
  name TEXT,
  data TEXT,
  PRIMARY KEY (bucket_id, name)
);
insert into test.payload (bucket_id, name, data) values ('8772618c9009cf8f5a5e0c18', 'test', 'hello');
{noformat}

Flush nodes (nodetool flush)

{noformat}
insert into test.payload (bucket_id, name, data) values ('8772618c9009cf8f5a5e0c19', 'test2', 'hello');
delete from test.payload where bucket_id = '8772618c9009cf8f5a5e0c18';
{noformat}

Flush nodes (nodetool flush)

{noformat}
select * from test.payload where bucket_id = '8772618c9009cf8f5a5e0c18' and name = 'test';
{noformat}

Expected 0 rows but get 1 row back."
CASSANDRA-12700,"During writing data into Cassandra 3.7.0 using Python driver 3.7 sometimes Connection get lost, because of Server NullPointerException","In our C* cluster we are using the latest Cassandra 3.7.0 (datastax-ddc.3.70) with Python driver 3.7. Trying to insert 2 million row or more data into the database, but sometimes we are getting ""Null pointer Exception"". 

We are using Python 2.7.11 and Java 1.8.0_73 in the Cassandra nodes and in the client its Python 2.7.12.

{code:title=cassandra server log}
ERROR [SharedPool-Worker-6] 2016-09-23 09:42:55,002 Message.java:611 - Unexpected exception during request; channel = [id: 0xc208da86, L:/IP1.IP2.IP3.IP4:9042 - R:/IP5.IP6.IP7.IP8:58418]
java.lang.NullPointerException: null
    at org.apache.cassandra.serializers.BooleanSerializer.deserialize(BooleanSerializer.java:33) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.serializers.BooleanSerializer.deserialize(BooleanSerializer.java:24) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:113) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager$1.apply(CassandraRoleManager.java:85) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager$1.apply(CassandraRoleManager.java:81) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:503) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:485) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:298) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.service.ClientState.login(ClientState.java:227) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:79) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [apache-cassandra-3.7.0.jar:3.7.0]
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:292) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:32) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:283) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_73]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.7.0.jar:3.7.0]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_73]
ERROR [SharedPool-Worker-1] 2016-09-23 09:42:56,238 Message.java:611 - Unexpected exception during request; channel = [id: 0x8e2eae00, L:/IP1.IP2.IP3.IP4:9042 - R:/IP5.IP6.IP7.IP8:58421]
java.lang.NullPointerException: null
    at org.apache.cassandra.serializers.BooleanSerializer.deserialize(BooleanSerializer.java:33) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.serializers.BooleanSerializer.deserialize(BooleanSerializer.java:24) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:113) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.cql3.UntypedResultSet$Row.getBoolean(UntypedResultSet.java:273) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager$1.apply(CassandraRoleManager.java:85) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager$1.apply(CassandraRoleManager.java:81) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:503) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:485) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.auth.CassandraRoleManager.canLogin(CassandraRoleManager.java:298) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.service.ClientState.login(ClientState.java:227) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.transport.messages.AuthResponse.execute(AuthResponse.java:79) ~[apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [apache-cassandra-3.7.0.jar:3.7.0]
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:292) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:32) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:283) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_73]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.7.0.jar:3.7.0]
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.7.0.jar:3.7.0]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_73]
{code}

{code:title=python client error}
Traceback (most recent call last):
 File ""vssandra_v5.py"", line 132, in <module>
   db.insert_batch()
 File ""vssandra_v5.py"", line 60, in insert_batch
   self.session.execute(self.batch, timeout=None)
 File ""cassandra/cluster.py"", line 1998, in cassandra.cluster.Session.execute (cassandra/cluster.c:34869)
 File ""cassandra/cluster.py"", line 3781, in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:73073)
cassandra.protocol.ServerError: <Error from server: code=0000 [Server error] message=""com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException"">
{code}

{code:title=cassandra config}
 Node configuration:[allocate_tokens_for_keyspace=null; authenticator=PasswordAuthenticator; authorizer=CassandraAuthorizer; auto_bootstrap=true; auto_snapshot=true; batch_size_fail_threshold_in_kb=100; batch_size_warn_threshold_in_kb=10; batchlog_replay_throttle_in_kb=1024; broadcast_address=null; broadcast_rpc_address=null; buffer_pool_use_heap_if_exhausted=true; cas_contention_timeout_in_ms=5000; client_encryption_options=<REDACTED>; cluster_name=VCF Stress Cluster; column_index_cache_size_in_kb=2; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=null; commitlog_directory=null; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=null; commitlog_sync_period_in_ms=10000; commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=150; compaction_throughput_mb_per_sec=20; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_materialized_view_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_writes=32; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; credentials_cache_max_entries=1000; credentials_update_interval_in_ms=-1; credentials_validity_in_ms=2000; cross_node_timeout=false; data_file_directories=[Ljava.lang.String;@36fc695d; disk_access_mode=auto; disk_failure_policy=stop; disk_optimization_estimate_percentile=0.95; disk_optimization_page_cross_chance=0.1; disk_optimization_strategy=spinning; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; enable_scripted_user_defined_functions=false; enable_user_defined_functions=false; enable_user_defined_functions_threads=true; encryption_options=null; endpoint_snitch=SimpleSnitch; file_cache_size_in_mb=null; gc_warn_threshold_in_ms=1000; hinted_handoff_disabled_datacenters=[]; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; hints_compression=null; hints_directory=null; hints_flush_period_in_ms=10000; incremental_backups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticator=null; internode_compression=dc; internode_recv_buff_size_in_bytes=null; internode_send_buff_size_in_bytes=null; key_cache_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=IP.251; listen_interface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; max_hints_file_size_in_mb=128; max_mutation_size_in_kb=null; max_streaming_retries=3; max_value_size_in_mb=256; memtable_allocation_type=offheap_objects; memtable_cleanup_threshold=null; memtable_flush_writers=1; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; native_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_max_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; native_transport_port_ssl=null; num_tokens=256; otc_coalescing_strategy=TIMEHORIZON; otc_coalescing_window_us=200; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validity_in_ms=2000; phi_convict_threshold=8.0; prepared_statements_cache_size_mb=null; range_request_timeout_in_ms=11000; read_request_timeout_in_ms=6000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_timeout_in_ms=20000; role_manager=CassandraRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_validity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=IP.251; rpc_interface=null; rpc_interface_prefer_ipv6=false; rpc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_size_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=null; seed_provider=org.apache.cassandra.locator.SimpleSeedProvider{seeds=IP.251}; server_encryption_options=<REDACTED>; snapshot_before_compaction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=false; storage_port=7000; stream_throughput_outbound_megabits_per_sec=250; streaming_socket_timeout_in_ms=86400000; thrift_framed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; thrift_prepared_statements_cache_size_mb=null; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; transparent_data_encryption_options=org.apache.cassandra.config.TransparentDataEncryptionOptions@28701274; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; user_defined_function_fail_timeout=1500; user_defined_function_warn_timeout=500; user_function_timeout_policy=die; windows_timer_interval=1; write_request_timeout_in_ms=10000]
{code}"
CASSANDRA-12457,dtest failure in upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x.bug_5732_test,"example failure:

http://cassci.datastax.com/job/cassandra-2.2_dtest_upgrade/16/testReport/upgrade_tests.cql_tests/TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_2_x/bug_5732_test

{code}
Stacktrace

  File ""/usr/lib/python2.7/unittest/case.py"", line 358, in run
    self.tearDown()
  File ""/home/automaton/cassandra-dtest/upgrade_tests/upgrade_base.py"", line 216, in tearDown
    super(UpgradeTester, self).tearDown()
  File ""/home/automaton/cassandra-dtest/dtest.py"", line 666, in tearDown
    raise AssertionError('Unexpected error in log, see stdout')
""Unexpected error in log, see stdout\n-------------------- >> begin captured logging << --------------------\ndtest: DEBUG: Upgrade test beginning, setting CASSANDRA_VERSION to 2.1.15, and jdk to 8. (Prior values will be restored after test).\ndtest: DEBUG: cluster ccm directory: /mnt/tmp/dtest-D8UF3i\ndtest: DEBUG: Done setting configuration options:\n{   'initial_token': None,\n    'num_tokens': '32',\n    'phi_convict_threshold': 5,\n    'range_request_timeout_in_ms': 10000,\n    'read_request_timeout_in_ms': 10000,\n    'request_timeout_in_ms': 10000,\n    'truncate_request_timeout_in_ms': 10000,\n    'write_request_timeout_in_ms': 10000}\ndtest: DEBUG: [[Row(table_name=u'ks', index_name=u'test.testindex')], [Row(table_name=u'ks', index_name=u'test.testindex')]]\ndtest: DEBUG: upgrading node1 to git:91f7387e1f785b18321777311a5c3416af0663c2\nccm: INFO: Fetching Cassandra updates...\ndtest: DEBUG: Querying upgraded node\ndtest: DEBUG: Querying old node\ndtest: DEBUG: removing ccm cluster test at: /mnt/tmp/dtest-D8UF3i\ndtest: DEBUG: clearing ssl stores from [/mnt/tmp/dtest-D8UF3i] directory\n--------------------- >> end captured logging << ---------------------""
{code}

{code}
Standard Output

http://git-wip-us.apache.org/repos/asf/cassandra.git git:91f7387e1f785b18321777311a5c3416af0663c2
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@73deb57f) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@2098812276:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7926de0f) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1009016655:[[OffHeapBitSet]] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,581 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3a5760f9) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@223486002:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@42cb4131) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1544265728:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5dda43d0) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1100327913:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,582 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@59cfa823) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1480923322:[Memory@[0..4), Memory@[0..a)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,601 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@570e14a1) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@992487242:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-4-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,602 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1f021ebc) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1878148398:[Memory@[0..4), Memory@[0..e)] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,604 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@48feef6d) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@848724815:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,605 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3dee8c5f) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@1078490617:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2 was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,614 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7f726f1a) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@2037913408:/mnt/tmp/dtest-D8UF3i/test/node1/data0/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-3-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,615 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@303df044) to class org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Cleanup@861514759:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Data.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5fbf0fc9) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1715786089:[[OffHeapBitSet]] was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3924b235) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1197672578:/mnt/tmp/dtest-D8UF3i/test/node1/data1/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-2-Index.db was not released before the reference was garbage collected
Unexpected error in node1 log, error: 
ERROR [Reference-Reaper:1] 2016-08-13 01:34:34,616 Ref.java:199 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@600596e0) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@545967120:[[OffHeapBitSet]] was not released before the reference was garbage collected
{code}"
CASSANDRA-12312,Restore JVM metric export for metric reporters,"JVM instrumentation as part of dropwizard metrics has been moved to a separate {{metrics-jvm}} artifact in metrics-v3.0. After CASSANDRA-5657, no jvm related metrics will be exported to any reporter configured via {{metrics-reporter-config}}, as this isn't part of {{metrics-core}} anymore. As memory and GC stats are essential for monitoring Cassandra, this turns out to be a blocker for us for upgrading to 2.2.

I've included a patch that would add the now separate {{metrics-jvm}} package and enables some of the provided metrics on startup in case a metrics reporter is used ({{-Dcassandra.metricsReporterConfigFile}})."
CASSANDRA-12251,"Move migration tasks to non-periodic queue, assure flush executor shutdown after non-periodic executor","example failure:

http://cassci.datastax.com/job/cassandra-3.8_dtest_upgrade/1/testReport/upgrade_tests.cql_tests/TestCQLNodes3RF3_Upgrade_current_3_x_To_indev_3_x/whole_list_conditional_test

Failed on CassCI build cassandra-3.8_dtest_upgrade #1

Relevant error in logs is
{code}
Unexpected error in node1 log, error: 
ERROR [InternalResponseStage:2] 2016-07-20 04:58:45,876 CassandraDaemon.java:217 - Exception in thread Thread[InternalResponseStage:2,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369) ~[na:1.8.0_51]
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:165) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) ~[na:1.8.0_51]
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:842) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtableIfCurrent(ColumnFamilyStore.java:822) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:891) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.lambda$flush$1(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace$$Lambda$200/1129213153.accept(Unknown Source) ~[na:na]
	at java.lang.Iterable.forEach(Iterable.java:75) ~[na:1.8.0_51]
	at org.apache.cassandra.schema.SchemaKeyspace.flush(SchemaKeyspace.java:279) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:1271) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchemaAndAnnounceVersion(SchemaKeyspace.java:1253) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:92) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:53) ~[apache-cassandra-3.7.jar:3.7]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:64) ~[apache-cassandra-3.7.jar:3.7]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_51]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
{code}
This is on a mixed 3.0.8, 3.8-tentative cluster"
CASSANDRA-12105,ThriftServer.stop is not thread safe,"There is a small thread safety issue in ThriftServer.stop(). If we have multiple calls to stop, one thread may NPE or otherwise do bad stuff."
CASSANDRA-12043,Syncing most recent commit in CAS across replicas can cause all CAS queries in the CQL partition to fail,"We update the most recent commit on requiredParticipant replicas if out of sync during the prepare round in beginAndRepairPaxos method. We keep doing this in a loop till the requiredParticipant replicas have the same most recent commit or we hit timeout. 

Say we have 3 machines A,B and C and gc grace on the table is 10 days. We do a CAS write at time 0 and it went to A and B but not to C.  C will get the hint later but will not update the most recent commit in paxos table. This is how CAS hints work. 
In the paxos table whose gc_grace=0, most_recent_commit in A and B will be inserted with timestamp 0 and with a TTL of 10 days. After 10 days, this insert will become a tombstone at time 0 till it is compacted away since gc_grace=0.

Do a CAS read after say 1 day on the same CQL partition and this time prepare phase involved A and C. most_recent_commit on C for this CQL partition is empty. A sends the most_recent_commit to C with a timestamp of 0 and with a TTL of 10 days. This most_recent_commit on C will expire on 11th day since it is inserted after 1 day. 

most_recent_commit are now in sync on A,B and C, however A and B most_recent_commit will expire on 10th day whereas for C it will expire on 11th day since it was inserted one day later. 

Do another CAS read after 10days when most_recent_commit on A and B have expired and is treated as tombstones till compacted. In this CAS read, say A and C are involved in prepare phase. most_recent_commit will not match between them since it is expired in A and is still there on C. This will cause most_recent_commit to be applied to A with a timestamp of 0 and TTL of 10 days. If A has not compacted away the original most_recent_commit which has expired, this new write to most_recent_commit wont be visible on reads since there is a tombstone with same timestamp(Delete wins over data with same timestamp). 

Another round of prepare will follow and again A would say it does not know about most_recent_write(covered by original write which is not a tombstone) and C will again try to send the write to A. This can keep going on till the request timeouts or only A and B are involved in the prepare phase. 

When A’s original most_recent_commit which is now a tombstone is compacted, all the inserts which it was covering will come live. This will in turn again get played to another replica. This ping pong can keep going on for a long time. 

The issue is that most_recent_commit is expiring at different times across replicas. When they get replayed to a replica to make it in sync, we again set the TTL from that point.  
During the CAS read which timed out, most_recent_commit was being sent to another replica in a loop. Even in successful requests, it will try to loop for a couple of times if involving A and C and then when the replicas which respond are A and B, it will succeed. So this will have impact on latencies as well. 

These timeouts gets worse when a machine is down as no progress can be made as the machine with unexpired commit is always involved in the CAS prepare round. Also with range movements, the new machine gaining range has empty most recent commit and gets the commit at a later time causing same issue. 

Repro steps:
1. Paxos TTL is max(3 hours, gc_grace) as defined in SystemKeyspace.paxosTtl(). Change this method to not put a minimum TTL of 3 hours. 
Method  SystemKeyspace.paxosTtl() will look like return metadata.getGcGraceSeconds();   instead of return Math.max(3 * 3600, metadata.getGcGraceSeconds());
We are doing this so that we dont need to wait for 3 hours. 

Create a 3 node cluster with the code change suggested above with machines A,B and C
CREATE KEYSPACE  test WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };
use test;
CREATE TABLE users (a int PRIMARY KEY,b int);
alter table users WITH gc_grace_seconds=120;
consistency QUORUM;
bring down machine C
INSERT INTO users (user_name, password ) VALUES ( 1,1) IF NOT EXISTS;
Nodetool flush on machine A and B
Bring up the down machine B 
consistency SERIAL;
tracing on;
wait 80 seconds
Bring up machine C
select * from users where user_name = 1;
Wait 40 seconds 
select * from users where user_name = 1;  //All queries from this point forward will timeout. 

One of the potential fixes could be to set the TTL based on the remaining time left on another replicas. This will be TTL-timestamp of write. This timestamp is calculated from ballot which uses server time."
CASSANDRA-11973,Is MemoryUtil.getShort() supposed to return a sign-extended or non-sign-extended value?,"In org.apache.cassandra.utils.memory.MemoryUtil.getShort(), the returned value of unsafe.getShort(address) is bit-wise-AND'ed with 0xffff, while that of getShortByByte(address) is not. This inconsistency results in different returned values when the short integer is negative. Which is preferred behavior? Looking at NativeClustering and NativeCellTest, it seems like non-sign-extension is assumed.

By the way, is there any reason MemoryUtil.getShort() and MemoryUtil.getShortByByte() return ""int"", not ""short""?"
CASSANDRA-11920,bloom_filter_fp_chance needs to be validated up front,"Hi,

I was doing some bench-marking on bloom_filter_fp_chance values. Everything worked fine for values .01(default for STCS), .001, .0001. But when I set bloom_filter_fp_chance = .00001 i observed following behaviour:

1). Reads and writes looked normal from cqlsh.
2). SSttables are never created.
3). It just creates two files (*-Data.db and *-index.db) of size 0kb.
4). nodetool flush does not work and produce following exception:

java.lang.UnsupportedOperationException: Unable to satisfy 1.0E-5 with 20 buckets per element
        at org.apache.cassandra.utils.BloomCalculations.computeBloomSpec(BloomCalculations.java:150) .....


I checked BloomCalculations class and following lines are responsible for this exception:

if (maxFalsePosProb < probs[maxBucketsPerElement][maxK]) {
      throw new UnsupportedOperationException(String.format(""Unable to satisfy %s with %s buckets per element"",
                                                 maxFalsePosProb, maxBucketsPerElement));
  }


From  the code it looks like a hard coaded validation (unless we can change the nuber of buckets).
So, if this validation is hard coaded then why it is even allowed to set such value of bloom_fileter_fp_chance, that can prevent ssTable generation?

Please correct this issue.
"
CASSANDRA-11867,Possible memory leak in NIODataInputStream,{{NIODataInputStream}} allocates direct memory but does not clean it on close. Proposed patch adds a call to {{FileUtils.clean()}} for this direct memory buffer.
CASSANDRA-11828,Commit log needs to track unflushed intervals rather than positions,"In CASSANDRA-11448 in an effort to give a more thorough handling of flush errors I have introduced a possible correctness bug with disk failure policy ignore if a flush fails with an error:
- we report the error but continue
- we correctly do not update the commit log with the flush position
- but we allow the post-flush executor to resume
- a successful later flush can thus move the log's clear position beyond the data from the failed flush
- the log will then delete segment(s) that contain unflushed data.

After CASSANDRA-9669 it is relatively easy to fix this problem by making the commit log track sets of intervals of unflushed data (as described in CASSANDRA-8496)."
CASSANDRA-11621,Stack Overflow inserting value with many columns,"I am using CQL to insert into a table that has ~4000 columns

{code}
  TABLE_DEFINITION = ""
      id uuid,
      ""dimension_n"" for n in _.range(N_DIMENSIONS)
      ETAG timeuuid,
      PRIMARY KEY (id)
    ""
{code}

I am using the node.js library from Datastax to execute CQL. This creates a prepared statement and then uses it to perform an insert. It works fine on C* 2.1 but after upgrading to C* 2.2.5 I get the stack overflow below.

I know enough Java to think that recursing an iterator is bad form and should be easy to fix.

{code}
ERROR 14:59:01 Unexpected exception during request; channel = [id: 0xaac42a5d, /10.0.7.182:58736 => /10.0.0.87:9042]
java.lang.StackOverflowError: null
	at com.google.common.base.Preconditions.checkPositionIndex(Preconditions.java:339) ~[guava-16.0.jar:na]
	at com.google.common.collect.AbstractIndexedListIterator.<init>(AbstractIndexedListIterator.java:69) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$11.<init>(Iterators.java:1048) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators.forArray(Iterators.java:1048) ~[guava-16.0.jar:na]
	at com.google.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.listIterator(ImmutableList.java:344) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.iterator(ImmutableList.java:340) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableList.iterator(ImmutableList.java:61) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables.iterators(Iterables.java:504) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables.access$100(Iterables.java:60) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$2.iterator(Iterables.java:494) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$3.transform(Iterables.java:508) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterables$3.transform(Iterables.java:505) ~[guava-16.0.jar:na]
	at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:543) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
	at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
...
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
        at com.google.common.collect.Iterators$5.hasNext(Iterators.java:542) ~[guava-16.0.jar:na]
        at org.apache.cassandra.cql3.statements.ModificationStatement.checkAccess(ModificationStatement.java:168) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:223) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:257) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:242) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:123) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_77]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_77]
{code}"
CASSANDRA-11541,correct the java documentation for SlabAllocator and NativeAllocator,"I heard a lot that Cassandra uses 2MB slab allocation strategy for memtable to improve its memory efficiency. But in fact, it has been changed from 2MB to 1 MB. 

And in NativeAllocator, it's logarithmically from 8kb to 1mb.

large number of tables may not cause too much trouble in term of memtable."
CASSANDRA-11448,Running OOS should trigger the disk failure policy,"Currently when you run OOS, this happens:

{noformat}
ERROR [MemtableFlushWriter:8561] 2016-03-28 01:17:37,047  CassandraDaemon.java:229 - Exception in thread Thread[MemtableFlushWriter:8561,5,main]   java.lang.RuntimeException: Insufficient disk space to write 48 bytes 
    at org.apache.cassandra.io.util.DiskAwareRunnable.getWriteDirectory(DiskAwareRunnable.java:29) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]
    at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:332) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]
    at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.1.jar:na]
    at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1120) ~[cassandra-all-2.1.12.1046.jar:2.1.12.1046]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_66]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_66]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_66]
{noformat}

Now your flush writer is dead and postflush tasks build up forever.  Instead we should throw FSWE and trigger the failure policy."
CASSANDRA-11427,Range slice queries CL > ONE trigger read-repair of purgeable tombstones,"Range queries will trigger read repairs for purgeable tombstones on hosts that already compacted given tombstones. Clusters with periodical jobs for scanning data ranges will likely see tombstones ressurected through RRs just to have them compacted again later at the destination host.

Executing range queries (e.g. for reading token ranges) will compare the actual data instead of using digests when executed with CL > ONE. Responses will be consolidated by {{RangeSliceResponseResolver.Reducer}}, where the result of {{RowDataResolver.resolveSuperset}} is used as the reference version for the results. {{RowDataResolver.scheduleRepairs}} will then send the superset to all nodes that returned a different result before. 

Unfortunately this does also involve cases where the superset is just made up of purgeable tombstone(s) that already have been compacted on the other nodes. In this case a read-repair will be triggered for transfering the purgeable tombstones to all other nodes nodes that returned an empty result.

The issue can be reproduced with the provided dtest or manually using the following steps:

{noformat}
create keyspace test1 with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 2 };
use test1;
create table test1 ( a text, b text, primary key(a, b) ) WITH compaction = {'class': 'SizeTieredCompactionStrategy', 'enabled': 'false'} AND dclocal_read_repair_chance = 0 AND gc_grace_seconds = 0;

delete from test1 where a = 'a';
{noformat}

{noformat}
ccm flush;
ccm node2 compact;
{noformat}

{noformat}
use test1;
consistency all;
tracing on;
select * from test1;
{noformat}

"
CASSANDRA-11373,Cancelled compaction leading to infinite loop in compaction strategy getNextBackgroundTask,"Our test is basically running *nodetool repair* on specific keyspaces (such as keyspace1) and the test is also triggering *nodetool compact keyspace1 standard1* in the background. 
And so it looks like running major compactions & repairs lead to that issue when using *LCS*.


Below is an excerpt from the *thread dump* (the rest is attached)
{code}
""CompactionExecutor:2"" #33 daemon prio=1 os_prio=4 tid=0x00007f5363e64f10 nid=0x3c4e waiting for monitor entry [0x00007f53340d8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(CompactionStrategyManager.java:252)
	- waiting to lock <0x00000006c9362c80> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.lifecycle.Tracker.notifySSTableRepairedStatusChanged(Tracker.java:434)
	at org.apache.cassandra.db.compaction.CompactionManager.performAnticompaction(CompactionManager.java:550)
	at org.apache.cassandra.db.compaction.CompactionManager$7.runMayThrow(CompactionManager.java:465)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- <0x00000006c9362ca8> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""CompactionExecutor:1"" #32 daemon prio=1 os_prio=4 tid=0x00007f5363e618b0 nid=0x3c4d runnable [0x00007f5334119000]
   java.lang.Thread.State: RUNNABLE
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:650)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at com.google.common.collect.Iterators.addAll(Iterators.java:361)
	at com.google.common.collect.Iterables.addAll(Iterables.java:354)
	at org.apache.cassandra.db.compaction.LeveledManifest.getCandidatesFor(LeveledManifest.java:589)
	at org.apache.cassandra.db.compaction.LeveledManifest.getCompactionCandidates(LeveledManifest.java:349)
	- locked <0x00000006d0f7a6a8> (a org.apache.cassandra.db.compaction.LeveledManifest)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:98)
	- locked <0x00000006d0f7a568> (a org.apache.cassandra.db.compaction.LeveledCompactionStrategy)
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:95)
	- locked <0x00000006c9362c80> (a org.apache.cassandra.db.compaction.CompactionStrategyManager)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:257)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

*CPU usage is at 100%*
{code}
top -p 15386
top - 12:12:40 up  1:28,  1 user,  load average: 1.08, 1.11, 1.16
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.3 us,  0.0 sy, 12.4 ni, 87.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem:  16433792 total,  8947336 used,  7486456 free,    89552 buffers
KiB Swap:        0 total,        0 used,        0 free.  3326796 cached Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
15386 automat+  20   0 7891448 5.004g 290184 S 102.9 31.9  80:07.06 java

{code}

*ttop shows that the compaction thread consumes all the CPU*
{code}
$ java -jar sjk.jar ttop -p 15386
Monitoring threads ...

2016-03-17T12:17:13.514+0000 Process summary
  process cpu=126.34%
  application cpu=102.81% (user=102.46% sys=0.35%)
  other: cpu=23.53%
  heap allocation rate 375mb/s
[000002] user= 0.00% sys= 0.00% alloc=     0b/s - Reference Handler
[000003] user= 0.00% sys= 0.00% alloc=     0b/s - Finalizer
[000005] user= 0.00% sys= 0.00% alloc=     0b/s - Signal Dispatcher
[000012] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Accept-7199
[000013] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Accept-0
[000015] user= 0.00% sys= 0.00% alloc=   476b/s - AsyncAppender-Worker-ASYNCDEBUGLOG
[000016] user= 0.00% sys= 0.05% alloc=  1070b/s - ScheduledTasks:1
[000017] user= 0.00% sys= 0.00% alloc=    33b/s - EXPIRING-MAP-REAPER:1
[000018] user= 0.00% sys= 0.02% alloc=  1932b/s - Background_Reporter:1
[000022] user= 0.00% sys= 0.00% alloc=     0b/s - MemtablePostFlush:1
[000023] user= 0.00% sys= 0.00% alloc=     0b/s - MemtableReclaimMemory:1
[000026] user= 0.00% sys= 0.00% alloc=     0b/s - SlabPoolCleaner
[000027] user= 0.00% sys= 0.00% alloc=     0b/s - PERIODIC-COMMIT-LOG-SYNCER
[000028] user= 0.00% sys= 0.00% alloc=     0b/s - COMMIT-LOG-ALLOCATOR
[000029] user= 0.00% sys= 0.01% alloc=  7086b/s - OptionalTasks:1
[000030] user= 0.00% sys= 0.00% alloc=     0b/s - Reference-Reaper:1
[000031] user= 0.00% sys= 0.00% alloc=     0b/s - Strong-Reference-Leak-Detector:1
[000032] user=99.45% sys= 0.07% alloc=  374mb/s - CompactionExecutor:1
[000033] user= 0.00% sys= 0.00% alloc=     0b/s - CompactionExecutor:2
[000036] user= 0.00% sys= 0.00% alloc=     0b/s - NonPeriodicTasks:1
[000037] user= 0.00% sys= 0.00% alloc=     0b/s - LocalPool-Cleaner:1
[000041] user= 0.00% sys= 0.00% alloc=     0b/s - IndexSummaryManager:1
[000043] user= 0.00% sys= 0.01% alloc=  2705b/s - GossipTasks:1
[000044] user= 0.00% sys= 0.00% alloc=     0b/s - ACCEPT-/10.200.182.146
[000045] user= 0.00% sys= 0.01% alloc=  2283b/s - BatchlogTasks:1
[000055] user= 0.00% sys= 0.02% alloc=  9494b/s - GossipStage:1
[000056] user= 0.00% sys= 0.00% alloc=     0b/s - AntiEntropyStage:1
[000057] user= 0.00% sys= 0.00% alloc=     0b/s - MigrationStage:1
[000058] user= 0.00% sys= 0.00% alloc=     0b/s - MiscStage:1
[000067] user= 0.00% sys= 0.02% alloc=  2445b/s - MessagingService-Incoming-/10.200.182.144
[000068] user= 0.00% sys= 0.01% alloc=   968b/s - MessagingService-Outgoing-/10.200.182.144
[000069] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.144
[000070] user= 0.00% sys= 0.02% alloc=   512b/s - MessagingService-Outgoing-/10.200.182.144
[000072] user= 0.00% sys= 0.00% alloc=     0b/s - NanoTimeToCurrentTimeMillis updater
[000073] user= 0.00% sys= 0.02% alloc=  3113b/s - MessagingService-Incoming-/10.200.182.144
[000074] user= 0.00% sys= 0.00% alloc=     0b/s - PendingRangeCalculator:1
[000075] user= 0.00% sys= 0.41% alloc=   66kb/s - SharedPool-Worker-1
[000076] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-2
[000077] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-3
[000078] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-4
[000079] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-5
[000080] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-6
[000081] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-7
[000082] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-8
[000084] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-2
[000085] user= 0.00% sys= 0.00% alloc=   181b/s - HintsWriteExecutor:1
[000091] user= 0.00% sys= 0.00% alloc=     0b/s - PO-thread-0
[000092] user= 0.00% sys= 0.00% alloc=     0b/s - NodeHealthPlugin-Scheduler-thread-0
[000093] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-1
[000094] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-2
[000097] user= 0.00% sys= 0.00% alloc=     0b/s - Lease RemoteMessageServer acceptor-2-1
[000104] user= 0.00% sys= 0.00% alloc=     0b/s - RemoteMessageClient worker-4-1
[000120] user= 0.00% sys= 0.00% alloc=     0b/s - RemoteMessageClient connection limiter - 0
[000121] user= 0.00% sys= 0.00% alloc=     0b/s - threadDeathWatcher-5-1
[000122] user= 0.00% sys= 0.00% alloc=     0b/s - PO-thread scheduler
[000123] user= 0.00% sys= 0.00% alloc=     0b/s - JOB-TRACKER
[000124] user= 0.00% sys= 0.01% alloc=  1276b/s - TASK-TRACKER
[000127] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-1
[000128] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-2
[000129] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-3
[000130] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-4
[000131] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-5
[000132] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-6
[000133] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-7
[000134] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-8
[000135] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-9
[000136] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-10
[000137] user= 0.19% sys=-0.18% alloc=     0b/s - epollEventLoopGroup-6-11
[000138] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-12
[000139] user= 0.19% sys=-0.19% alloc=     0b/s - epollEventLoopGroup-6-13
[000140] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-14
[000141] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-15
[000142] user= 0.00% sys= 0.00% alloc=     0b/s - epollEventLoopGroup-6-16
[000143] user= 0.19% sys=-0.04% alloc=   13kb/s - Thread-7
[000144] user= 0.00% sys= 0.00% alloc=     0b/s - taskCleanup
[000145] user= 0.00% sys= 0.00% alloc=     0b/s - DseGossipStateUpdater
[000146] user= 0.00% sys= 0.00% alloc=     0b/s - DestroyJavaVM
[000149] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-10
[000150] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-11
[000151] user= 0.00% sys= 0.00% alloc=     0b/s - Directory/File cleanup thread
[000153] user= 0.00% sys= 0.00% alloc=     0b/s - pool-15-thread-1
[000190] user= 0.00% sys= 0.00% alloc=     0b/s - pool-18-thread-1
[000215] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-3
[000217] user= 0.00% sys= 0.00% alloc=     0b/s - RMI Scheduler(0)
[000220] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Connection(335)-10.200.182.146
[000222] user= 0.00% sys= 0.00% alloc=     0b/s - pool-10-thread-4
[000223] user= 0.00% sys= 0.00% alloc=     0b/s - taskCleanup
[000224] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-69
[000225] user= 0.00% sys= 0.00% alloc=     0b/s - Thread-70
[000227] user= 0.00% sys= 0.00% alloc=     0b/s - pool-19-thread-1
[000254] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-9
[000255] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-11
[000256] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-10
[000269] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-13
[000270] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-12
[000272] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-14
[000273] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-15
[000274] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-18
[000275] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-19
[000276] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-20
[000277] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-17
[000278] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-16
[000279] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-22
[000280] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-21
[000281] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-23
[000282] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-24
[000283] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-25
[000284] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-26
[000285] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-27
[000286] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-28
[000287] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-29
[000288] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-30
[000289] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-31
[000290] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-32
[000296] user= 0.00% sys= 0.00% alloc=  1970b/s - pool-2-thread-1
[000297] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-33
[000298] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-34
[000302] user= 0.00% sys= 0.01% alloc=  1576b/s - MessagingService-Incoming-/10.200.182.145
[000303] user= 0.00% sys= 0.00% alloc=   451b/s - MessagingService-Outgoing-/10.200.182.145
[000304] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.145
[000305] user= 0.00% sys= 0.01% alloc=   206b/s - MessagingService-Outgoing-/10.200.182.145
[000308] user= 0.00% sys= 0.00% alloc=   424b/s - MessagingService-Incoming-/10.200.182.145
[000314] user= 0.00% sys= 0.00% alloc=     0b/s - StreamingTransferTaskTimeouts:1
[000324] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000325] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000326] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Outgoing-/10.200.182.146
[000328] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.146
[000329] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-35
[000330] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-37
[000331] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-36
[000332] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-39
[000333] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-38
[000334] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-42
[000335] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-41
[000336] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-40
[000337] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-46
[000338] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-44
[000339] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-45
[000340] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-43
[000341] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-47
[000342] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-48
[000343] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-50
[000344] user= 0.00% sys= 0.00% alloc=     0b/s - SharedPool-Worker-49
[000375] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:1
[000376] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:2
[000406] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.145
[000408] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.146
[000409] user= 0.00% sys= 0.00% alloc=     0b/s - MessagingService-Incoming-/10.200.182.144
[000415] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:3
[000418] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:4
[000435] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:5
[000438] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:6
[000439] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:7
[000444] user= 0.00% sys= 0.00% alloc=     0b/s - StreamConnectionEstablisher:8
[000687] user= 0.00% sys= 0.00% alloc=     0b/s - JMX server connection timeout 687
[000688] user= 2.44% sys= 0.16% alloc= 1380kb/s - RMI TCP Connection(401)-10.200.182.146
[000694] user= 0.00% sys= 0.00% alloc=     0b/s - Attach Listener
[000726] user= 0.00% sys= 0.00% alloc=     0b/s - RMI TCP Connection(400)-10.200.182.146
[000743] user=-0.00% sys=-0.16% alloc=-109800b/s - MemtableFlushWriter:112
[000745] user= 0.00% sys= 0.00% alloc=     0b/s - MemtableFlushWriter:113
[000746] user= 0.00% sys= 0.03% alloc=  4295b/s - JMX server connection timeout 746
{code}

"
CASSANDRA-11349,MerkleTree mismatch when multiple range tombstones exists for the same partition and interval,"We observed that repair, for some of our clusters, streamed a lot of data and many partitions were ""out of sync"".
Moreover, the read repair mismatch ratio is around 3% on those clusters, which is really high.

After investigation, it appears that, if two range tombstones exists for a partition for the same range/interval, they're both included in the merkle tree computation.
But, if for some reason, on another node, the two range tombstones were already compacted into a single range tombstone, this will result in a merkle tree difference.
Currently, this is clearly bad because MerkleTree differences are dependent on compactions (and if a partition is deleted and created multiple times, the only way to ensure that repair ""works correctly""/""don't overstream data"" is to major compact before each repair... which is not really feasible).

Below is a list of steps allowing to easily reproduce this case:
{noformat}
ccm create test -v 2.1.13 -n 2 -s
ccm node1 cqlsh
CREATE KEYSPACE test_rt WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};
USE test_rt;
CREATE TABLE IF NOT EXISTS table1 (
    c1 text,
    c2 text,
    c3 float,
    c4 float,
    PRIMARY KEY ((c1), c2)
);
INSERT INTO table1 (c1, c2, c3, c4) VALUES ( 'a', 'b', 1, 2);
DELETE FROM table1 WHERE c1 = 'a' AND c2 = 'b';
ctrl ^d
# now flush only one of the two nodes
ccm node1 flush 
ccm node1 cqlsh
USE test_rt;
INSERT INTO table1 (c1, c2, c3, c4) VALUES ( 'a', 'b', 1, 3);
DELETE FROM table1 WHERE c1 = 'a' AND c2 = 'b';
ctrl ^d
ccm node1 repair
# now grep the log and observe that there was some inconstencies detected between nodes (while it shouldn't have detected any)
ccm node1 showlog | grep ""out of sync""
{noformat}
Consequences of this are a costly repair, accumulating many small SSTables (up to thousands for a rather short period of time when using VNodes, the time for compaction to absorb those small files), but also an increased size on disk.
"
CASSANDRA-11345,"Assertion Errors ""Memory was freed"" during streaming","We encountered the following AssertionError (twice on the same node) during a repair :

On node /172.16.63.41

{noformat}
INFO  [STREAM-IN-/10.174.216.160] 2016-03-09 02:38:13,900 StreamResultFuture.java:180 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Session with /10.174.216.160 is complete                                                                                                        
WARN  [STREAM-IN-/10.174.216.160] 2016-03-09 02:38:13,900 StreamResultFuture.java:207 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Stream failed                           
ERROR [STREAM-OUT-/10.174.216.160] 2016-03-09 02:38:13,906 StreamSession.java:505 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Streaming error occurred                    
java.lang.AssertionError: Memory was freed                                                                                                                                     
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:97) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                   
        at org.apache.cassandra.io.util.Memory.getLong(Memory.java:249) ~[apache-cassandra-2.1.13.jar:2.1.13]                                                              
        at org.apache.cassandra.io.compress.CompressionMetadata.getTotalSizeForSections(CompressionMetadata.java:247) ~[apache-cassandra-2.1.13.jar:2.1.13]                
        at org.apache.cassandra.streaming.messages.FileMessageHeader.size(FileMessageHeader.java:112) ~[apache-cassandra-2.1.13.jar:2.1.13]                                
        at org.apache.cassandra.streaming.StreamSession.fileSent(StreamSession.java:546) ~[apache-cassandra-2.1.13.jar:2.1.13]                                             
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:50) ~[apache-cassandra-2.1.13.jar:2.1.13]                      
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:41) ~[apache-cassandra-2.1.13.jar:2.1.13]                      
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45) ~[apache-cassandra-2.1.13.jar:2.1.13]                                    
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:351) ~[apache-cassandra-2.1.13.jar:2.1.13]           
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:331) ~[apache-cassandra-2.1.13.jar:2.1.13]                   
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]                                                                                                                 
{noformat}     

On node /10.174.216.160
 
{noformat}       
ERROR [STREAM-OUT-/172.16.63.41] 2016-03-09 02:38:14,140 StreamSession.java:505 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Streaming error occurred                          
java.io.IOException: Connection reset by peer                                                                                                                              
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.7.0_65]                                                                                              
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.7.0_65]                                                                                      
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.7.0_65]                                                                                          
        at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.7.0_65]                                                                                                          
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487) ~[na:1.7.0_65]                                                                                   
        at org.apache.cassandra.io.util.DataOutputStreamAndChannel.write(DataOutputStreamAndChannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                     
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]        
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:323) [apache-cassandra-2.1.13.jar:2.1.13]                
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]                                                                                                             
INFO  [STREAM-IN-/172.16.63.41] 2016-03-09 02:38:14,142 StreamResultFuture.java:180 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Session with /172.16.63.41 is complete
WARN  [STREAM-IN-/172.16.63.41] 2016-03-09 02:38:14,142 StreamResultFuture.java:207 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Stream failed                         
ERROR [STREAM-OUT-/172.16.63.41] 2016-03-09 02:38:14,143 StreamSession.java:505 - [Stream #f6980580-e55f-11e5-8f08-ef9e099ce99e] Streaming error occurred                  
java.io.IOException: Broken pipe                                                                                                                                           
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method) ~[na:1.7.0_65]                                                                                              
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) ~[na:1.7.0_65]                                                                                      
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) ~[na:1.7.0_65]                                                                                          
        at sun.nio.ch.IOUtil.write(IOUtil.java:65) ~[na:1.7.0_65]                                                                                                          
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487) ~[na:1.7.0_65]                                                                                   
        at org.apache.cassandra.io.util.DataOutputStreamAndChannel.write(DataOutputStreamAndChannel.java:48) ~[apache-cassandra-2.1.13.jar:2.1.13]                     
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44) ~[apache-cassandra-2.1.13.jar:2.1.13]                                
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:351) [apache-cassandra-2.1.13.jar:2.1.13]        
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:331) [apache-cassandra-2.1.13.jar:2.1.13]                
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]     
{noformat}"
CASSANDRA-11215,Reference leak with parallel repairs on the same table,"When starting multiple repairs on the same table Cassandra starts to log about reference leak as:
{noformat}
ERROR [Reference-Reaper:1] 2016-02-23 15:02:05,516 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5213f926) to class org.apache.cassandra.io.sstable.format.SSTableReader
$InstanceTidier@605893242:.../testrepair/standard1-dcf311a0da3411e5a5c0c1a39c091431/la-30-big was not released before the reference was garbage collected
{noformat}

Reproducible with:
{noformat}
ccm create repairtest -v 2.2.5 -n 3
ccm start
ccm stress write n=1000000 -schema replication(strategy=SimpleStrategy,factor=3) keyspace=testrepair
# And then perform two repairs concurrently with:
ccm node1 nodetool repair testrepair
{noformat}

I know that starting multiple repairs in parallel on the same table isn't very wise, but this shouldn't result in reference leaks."
CASSANDRA-11202,Cassandra - Commit log rename error,"I am a newbie please bear with me if not very clear with the query.

I have been working with cassandra for sometime. But suddenly it has stopped working with the issue being reported as ""Error processing commit log during intialization"" and that it is ""rename of .log file failed"".

I have searched for similar reports but could not solve the issue.

I tried the following: 1. clearing the DataStax Community\data\commitlog folder all but one log file is always locked with cassandra. 2. Changed the Windows Defender setting to allow the cassandra folder and not scan the .log files

I am using Datastax Communitity Cassandra version 2.1.10 with Java 1.8 64-bit working on a single node.

Thanking in anticipation Sujata

The console log:

:1.8.0_45]
    ... 11 common frames omitted
INFO  11:32:11 Initializing key cache with capacity of 100 MBs.
INFO  11:32:11 Initializing row cache with capacity of 0 MBs
INFO  11:32:11 Initializing counter cache with capacity of 50 MBs
INFO  11:32:11 Scheduling counter cache save to every 7200 seconds (going to         save all keys).
INFO  11:32:12 Initializing system.sstable_activity
INFO  11:32:16 Opening C:\Program Files (x86)\DataStax     Community\data\data\system\sstable_activity- 5a1ff267ace03f128563cfae6103c65e\system-sstable_activity-ka-1299 (1471 bytes)
INFO  11:32:16 Opening C:\Program Files (x86)\DataStax Community\data\data\system\sstable_activity-5a1ff267ace03f128563cfae6103c65e\system-sstable_activity-ka-1301 (1698 bytes)
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\sstable_activity-5a1ff267ace03f128563cfae6103c65e\system-sstable_activity-ka-1300 (1560 bytes)
INFO  11:32:17 Initializing system.hints
INFO  11:32:17 Initializing system.compaction_history
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca\system-compaction_history-ka-938 (343 bytes)
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca\system-compaction_history-ka-937 (14710 bytes)
INFO  11:32:17 Initializing system.peers
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\peers-37f71aca7dc2383ba70672528af04d4f\system-peers-ka-1 (30 bytes)
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\peers-37f71aca7dc2383ba70672528af04d4f\system-peers-ka-2 (30 bytes)
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\peers-37f71aca7dc2383ba70672528af04d4f\system-peers-ka-3 (30 bytes)
INFO  11:32:17 Initializing system.schema_columnfamilies
INFO  11:32:17 Opening C:\Program Files (x86)\DataStax Community\data\data\system\schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697\system-schema_columnfamilies-ka-532 (6166 bytes)
.....
INFO  11:32:23 Initializing OpsCenter.pdps
INFO  11:32:23 Opening C:\Program Files (x86)\DataStax Community\data\data\OpsCenter\pdps-2d611410725d11e58514c92bd1990991\OpsCenter-pdps-ka-234 (13992 bytes)
INFO  11:32:23 Opening C:\Program Files (x86)\DataStax Community\data\data\OpsCenter\pdps-2d611410725d11e58514c92bd1990991\OpsCenter-pdps-ka-233 (13727 bytes)
ERROR 11:32:23 Unable to delete C:\Program Files (x86)\DataStax Community\data\data\system\local-7ad54392bcdd35a684174e047860b377\system-local-ka-476-Data.db (it will be removed on server restart; we'll also retry after GC)
.....
INFO  11:32:24 Initializing people.employees
INFO  11:32:24 Opening C:\Program Files (x86)\DataStax  Community\data\data\people\employees-ae275410730211e5b8b0c92bd1990991\people- employees-ka-2 (364 bytes)
INFO  11:32:24 Opening C:\Program Files (x86)\DataStax  Community\data\data\people\employees-ae275410730211e5b8b0c92bd1990991\people- employees-ka-3 (188 bytes)
INFO  11:32:24 Opening C:\Program Files (x86)\DataStax  Community\data\data\people\employees-ae275410730211e5b8b0c92bd1990991\people- employees-ka-1 (238 bytes)
INFO  11:32:24 reading saved cache C:\Program Files (x86)\DataStax  Community\data\saved_caches\KeyCache-ba.db
INFO  11:32:24 Completed loading (15 ms; 124 keys) KeyCache cache
INFO  11:32:24 Replaying C:\Program Files (x86)\DataStax  Community\data\commitlog\CommitLog-4-1456038026082.log
INFO  11:32:24 Replaying C:\Program Files (x86)\DataStax  Community\data\commitlog\CommitLog-4-1456038026082.log
INFO  11:32:24 Replaying C:\Program Files (x86)\DataStax  Community\data\commitlog\CommitLog-4-1456038026082.log (CL version 4, messaging  version 8)
INFO  11:32:28 Finished reading C:\Program Files (x86)\DataStax  Community\data\commitlog\CommitLog-4-1456038026082.log
INFO  11:32:28 Enqueuing flush of compaction_history: 1188 (0%) on-heap, 0  (0%)off-heap
INFO  11:32:28 Enqueuing flush of compactions_in_progress: 1325 (0%) on- heap, 0(0%) off-heap
INFO  11:32:28 Writing Memtable-compaction_history@1259797467(0.231KiB  serialized bytes, 9 ops, 0%/0% of on/off-heap limit)
INFO  11:32:28 Enqueuing flush of sstable_activity: 692 (0%) on-heap, 0 (0%)  off-heap
INFO  11:32:28 Writing Memtable-compactions_in_progress@165723692(0.154KiB  serialized bytes, 10 ops, 0%/0% of on/off-heap limit)
INFO  11:32:28 Completed flushing C:\Program Files (x86)\DataStax  Community\data\data\system\compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-tmp-ka-1-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1456054336525, position=1020)
INFO  11:32:28 Completed flushing C:\Program Files (x86)\DataStax Community\data\data\system\compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca\system-compaction_history-tmp-ka-939-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1456054336525, position=1020)
INFO  11:32:28 Writing Memtable-sstable_activity@810221833(0.031KiB serialized bytes, 4 ops, 0%/0% of on/off-heap limit)
INFO  11:32:28 Completed flushing C:\Program Files (x86)\DataStax Community\data\data\system\sstable_activity-5a1ff267ace03f128563cfae6103c65e\system-sstable_activity-tmp-ka-1302-Data.db (0.000KiB) for commitlog position ReplayPosition(segmentId=1456054336525, position=1020)
INFO  11:32:28 Log replay complete, 2 replayed mutations
ERROR 11:32:28 Exiting due to error while processing commit log during initialization.org.apache.cassandra.io.FSWriteError: java.io.IOException: Rename from C:\Program Files (x86)\DataStax Community\data\commitlog\CommitLog-4-1456038026082.log to 1456054336526 failed 
at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:177) ~[apache-cassandra-2.1.10.jar:2.1.10]
at org.apache.cassandra.db.commitlog.CommitLogSegmentManager$4.call(CommitLogSegmentManager.java:397) ~[apache-cassandra-2.1.10.jar:2.1.10]
at org.apache.cassandra.db.commitlog.CommitLogSegmentManager$4.call(CommitLogSegmentManager.java:394) ~[apache-cassandra-2.1.10.jar:2.1.10]
at org.apache.cassandra.db.commitlog.CommitLogSegmentManager$1.runMayThrow(CommitLo gSegmentManager.java:152) ~[apache-cassandra-2.1.10.jar:2.1.10]
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) [apache-cassandra-2.1.10.jar:2.1.10]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.io.IOException: Rename from C:\Program Files (x86)\DataStax  Community\data\commitlog\CommitLog-4-1456038026082.log to 1456054336526 failed
at org.apache.cassandra.db.commitlog.CommitLogSegment.<init> (CommitLogSegment.java:149) ~[apache-cassandra-2.1.10.jar:2.1.10]
        ... 5 common frames omitted
INFO  11:32:28 Compacting [SSTableReader(path='C:\Program Files    (x86)\DataStax Community\data\data\system\size_estimates    618f817b005f3678b8a453f3930b8e86\system-size_estimates-ka-1280-Data.db'),    SSTableReader(path='C:\Program Files (x86)\DataStax    Community\data\data\system\size_estimates- 618f817b005f3678b8a453f3930b8e86\system-size_estimates-ka-1281-Data.db'),  SSTableReader(path='C:\Program Files (x86)\DataStax  Community\data\data\system\size_estimates-618f817b005f3678b8a453f3930b8e86\system-size_estimates-ka-1278-Data.db'), SSTableReader(path='C:\Program Files (x86)\DataStax Community\data\data\system\size_estimates-618f817b005f3678b8a453f3930b8e86\system-size_estimates-ka-1279-Data.db')]"
CASSANDRA-11176,SSTableRewriter.InvalidateKeys should have a weak reference to cache,"From [~aweisberg]

bq. The SSTableReader.DropPageCache runnable references SSTableRewriter.InvalidateKeys which references the cache. The cache reference should be a WeakReference.

{noformat}
ERROR [Strong-Reference-Leak-Detector:1] 2016-02-17 14:51:52,111  NoSpamLogger.java:97 - Strong self-ref loop detected [/var/lib/cassandra/data/keyspace1/standard1-990bc741d56411e591d5590d7a7ad312/ma-20-big,
private java.lang.Runnable org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier.runOnClose-org.apache.cassandra.io.sstable.format.SSTableReader$DropPageCache,
final java.lang.Runnable org.apache.cassandra.io.sstable.format.SSTableReader$DropPageCache.andThen-org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys,
final org.apache.cassandra.cache.InstrumentingCache org.apache.cassandra.io.sstable.SSTableRewriter$InvalidateKeys.cache-org.apache.cassandra.cache.AutoSavingCache,
protected volatile java.util.concurrent.ScheduledFuture org.apache.cassandra.cache.AutoSavingCache.saveTask-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask,
final java.util.concurrent.ScheduledThreadPoolExecutor java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.this$0-org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor,
private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue,
private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask,
private java.util.concurrent.Callable java.util.concurrent.FutureTask.callable-java.util.concurrent.Executors$RunnableAdapter,
final java.lang.Runnable java.util.concurrent.Executors$RunnableAdapter.task-org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable,
private final java.lang.Runnable org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.runnable-org.apache.cassandra.db.ColumnFamilyStore$3,
final org.apache.cassandra.db.ColumnFamilyStore org.apache.cassandra.db.ColumnFamilyStore$3.this$0-org.apache.cassandra.db.ColumnFamilyStore,
public final org.apache.cassandra.db.Keyspace org.apache.cassandra.db.ColumnFamilyStore.keyspace-org.apache.cassandra.db.Keyspace,
private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-java.util.concurrent.ConcurrentHashMap,
private final java.util.concurrent.ConcurrentMap org.apache.cassandra.db.Keyspace.columnFamilyStores-org.apache.cassandra.db.ColumnFamilyStore,
private final org.apache.cassandra.db.lifecycle.Tracker org.apache.cassandra.db.ColumnFamilyStore.data-org.apache.cassandra.db.lifecycle.Tracker,
final java.util.concurrent.atomic.AtomicReference org.apache.cassandra.db.lifecycle.Tracker.view-java.util.concurrent.atomic.AtomicReference,
private volatile java.lang.Object java.util.concurrent.atomic.AtomicReference.value-org.apache.cassandra.db.lifecycle.View,
public final java.util.List org.apache.cassandra.db.lifecycle.View.liveMemtables-com.google.common.collect.SingletonImmutableList,
final transient java.lang.Object com.google.common.collect.SingletonImmutableList.element-org.apache.cassandra.db.Memtable,
private final org.apache.cassandra.utils.memory.MemtableAllocator org.apache.cassandra.db.Memtable.allocator-org.apache.cassandra.utils.memory.SlabAllocator,
private final org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator org.apache.cassandra.utils.memory.MemtableAllocator.onHeap-org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator,
private final org.apache.cassandra.utils.memory.MemtablePool$SubPool org.apache.cassandra.utils.memory.MemtableAllocator$SubAllocator.parent-org.apache.cassandra.utils.memory.MemtablePool$SubPool,
final org.apache.cassandra.utils.memory.MemtablePool org.apache.cassandra.utils.memory.MemtablePool$SubPool.this$0-org.apache.cassandra.utils.memory.SlabPool,
final org.apache.cassandra.utils.memory.MemtableCleanerThread org.apache.cassandra.utils.memory.MemtablePool.cleaner-org.apache.cassandra.utils.memory.MemtableCleanerThread,
private java.lang.ThreadGroup java.lang.Thread.group-java.lang.ThreadGroup,
private final java.lang.ThreadGroup java.lang.ThreadGroup.parent-java.lang.ThreadGroup,
java.lang.Thread[] java.lang.ThreadGroup.threads-
[Ljava.lang.Thread;,
java.lang.Thread[] java.lang.ThreadGroup.threads-java.lang.Thread,
private java.lang.Runnable java.lang.Thread.target-java.util.concurrent.ThreadPoolExecutor$Worker,
final java.util.concurrent.ThreadPoolExecutor java.util.concurrent.ThreadPoolExecutor$Worker.this$0-java.util.concurrent.ScheduledThreadPoolExecutor,
private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue,
private final java.util.concurrent.BlockingQueue java.util.concurrent.ThreadPoolExecutor.workQueue-java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask,
private java.util.concurrent.Callable java.util.concurrent.FutureTask.callable-java.util.concurrent.Executors$RunnableAdapter,
final java.lang.Runnable java.util.concurrent.Executors$RunnableAdapter.task-sun.rmi.transport.DGCImpl$1,
final sun.rmi.transport.DGCImpl sun.rmi.transport.DGCImpl$1.this$0-sun.rmi.transport.DGCImpl,
private java.util.Map sun.rmi.transport.DGCImpl.leaseTable-java.util.HashMap,
transient java.util.HashMap$Node[] java.util.HashMap.table-
[Ljava.util.HashMap$Node;,
transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node,
java.lang.Object java.util.HashMap$Node.value-sun.rmi.transport.DGCImpl$LeaseInfo,
java.util.Set sun.rmi.transport.DGCImpl$LeaseInfo.notifySet-java.util.HashSet,
private transient java.util.HashMap java.util.HashSet.map-java.util.HashMap,
transient java.util.HashMap$Node[] java.util.HashMap.table-
[Ljava.util.HashMap$Node;,
transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node,
final java.lang.Object java.util.HashMap$Node.key-sun.rmi.transport.Target,
private final sun.rmi.transport.WeakRef sun.rmi.transport.Target.weakImpl-sun.rmi.transport.WeakRef,
private java.lang.Object sun.rmi.transport.WeakRef.strongRef-javax.management.remote.rmi.RMIJRMPServerImpl,
private javax.management.MBeanServer javax.management.remote.rmi.RMIServerImpl.mbeanServer-com.sun.jmx.mbeanserver.JmxMBeanServer,
private volatile javax.management.MBeanServer com.sun.jmx.mbeanserver.JmxMBeanServer.mbsInterceptor-com.sun.jmx.interceptor.DefaultMBeanServerInterceptor,
private final transient com.sun.jmx.mbeanserver.Repository com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.repository-com.sun.jmx.mbeanserver.Repository,
private final java.util.Map com.sun.jmx.mbeanserver.Repository.domainTb-java.util.HashMap,
transient java.util.HashMap$Node[] java.util.HashMap.table-
[Ljava.util.HashMap$Node;,
transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node,
java.lang.Object java.util.HashMap$Node.value-java.util.HashMap,
transient java.util.HashMap$Node[] java.util.HashMap.table-
[Ljava.util.HashMap$Node;,
transient java.util.HashMap$Node[] java.util.HashMap.table-java.util.HashMap$Node,
java.lang.Object java.util.HashMap$Node.value-com.sun.jmx.mbeanserver.NamedObject,
private final javax.management.DynamicMBean com.sun.jmx.mbeanserver.NamedObject.object-com.sun.jmx.mbeanserver.StandardMBeanSupport,
private final java.lang.Object com.sun.jmx.mbeanserver.MBeanSupport.resource-org.apache.cassandra.db.ColumnFamilyStore,
private final org.apache.cassandra.db.lifecycle.Tracker org.apache.cassandra.db.ColumnFamilyStore.data-org.apache.cassandra.db.lifecycle.Tracker,
public final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-java.util.concurrent.CopyOnWriteArrayList,
public final java.util.Collection org.apache.cassandra.db.lifecycle.Tracker.subscribers-org.apache.cassandra.db.compaction.CompactionStrategyManager,
private volatile org.apache.cassandra.db.compaction.AbstractCompactionStrategy org.apache.cassandra.db.compaction.CompactionStrategyManager.repaired-org.apache.cassandra.db.compaction.LeveledCompactionStrategy,
final org.apache.cassandra.db.compaction.LeveledManifest org.apache.cassandra.db.compaction.LeveledCompactionStrategy.manifest-org.apache.cassandra.db.compaction.LeveledManifest,
protected final java.util.List[] org.apache.cassandra.db.compaction.LeveledManifest.generations-
[Ljava.util.List;,
protected final java.util.List[] org.apache.cassandra.db.compaction.LeveledManifest.generations-java.util.ArrayList,
transient java.lang.Object[] java.util.ArrayList.elementData-[Ljava.lang.Object;,
transient java.lang.Object[] java.util.ArrayList.elementData-org.apache.cassandra.io.sstable.format.big.BigTableReader,
private final org.apache.cassandra.utils.concurrent.Ref org.apache.cassandra.io.sstable.format.SSTableReader.selfRef-org.apache.cassandra.utils.concurrent.Ref]
{noformat}"
CASSANDRA-11109,Cassandra process killed by OS due to out of memory issue,"After we upgraded Cassandra from 2.1.12 to 2.2.4 on one of our nodes (A three nodes Cassandra cluster), we've been experiencing an og-going issue with cassandra process running with continuously increasing memory util killed by OOM. 

{quote}
Feb  1 23:53:10 kernel: [24135455.025185] [19862]   494 19862 133728623  7379077  139068        0             0 java
Feb  1 23:53:10 kernel: [24135455.029678] Out of memory: Kill process 19862 (java) score 973 or sacrifice child
Feb  1 23:53:10 kernel: [24135455.035434] Killed process 19862 (java) total-vm:534918588kB, anon-rss:29413728kB, file-rss:102940kB
{quote}"
CASSANDRA-11083,cassandra-2.2 eclipse-warnings,"REF = origin/cassandra-2.2 
COMMIT = fa2fa602d989ed911b60247e3dd8f2d580188782

{noformat}
# 1/27/16 6:19:23 PM UTC
# Eclipse Compiler for Java(TM) v20150120-1634, 3.10.2, Copyright IBM Corp 2000, 2013. All rights reserved.
incorrect classpath: /var/lib/jenkins/workspace/cassandra-2.2_eclipse-warnings/build/cobertura/classes
----------
1. ERROR in /var/lib/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java (at line 141)
	return channel.socket();
	^^^^^^^^^^^^^^^^^^^^^^^^
Potential resource leak: 'channel' may not be closed at this location
----------
1 problem (1 error)
{noformat}

Check latest job on http://cassci.datastax.com/job/cassandra-2.2_eclipse-warnings/ for the most recent artifact"
CASSANDRA-11065,null pointer exception in CassandraDaemon.java:195,"Running Cassandra 3.0.1 installed from apt-get on debian.

I had a keyspace called 'tests'. I dropped it. Then I checked some nodes and one of them still had that keyspace 'tests'. On a node that still has the dropped keyspace I ran:
nodetools repair tests;

In the system logs of another node that did not have keyspace 'tests' I am seeing a null pointer exception:

{code:java}
ERROR [AntiEntropyStage:2] 2016-01-25 15:02:46,323 RepairMessageVerbHandler.java:161 - Got error, removing parent repair session
ERROR [AntiEntropyStage:2] 2016-01-25 15:02:46,324 CassandraDaemon.java:195 - Exception in thread Thread[AntiEntropyStage:2,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:164) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[apache-cassandra-3.0.1.jar:3.0.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_66-internal]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_66-internal]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_66-internal]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:69) ~[apache-cassandra-3.0.1.jar:3.0.1]
	... 4 common frames omitted
{code}

The error appears every time I run:
nodetools repair tests;

I can see it in the logs of all nodes, including the node on which I run the repair."
CASSANDRA-11007,Exception when running nodetool info during bootstrap,"{code}
automaton@ip-172-31-26-150:~$ nodetool info
ID                     : 7d9aa832-de94-43ab-9548-b2e710418301
Gossip active          : true
Thrift active          : false
Native Transport active: false
Load                   : 114.52 KB
Generation No          : 1452721596
Uptime (seconds)       : 1396
Heap Memory (MB)       : 71.80 / 1842.00
Off Heap Memory (MB)   : 0.00
Data Center            : datacenter1
Rack                   : rack1
Exceptions             : 0
Key Cache              : entries 11, size 888 bytes, capacity 92 MB, 180 hits, 212 requests, 0.849 recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 46 MB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
error: null
-- StackTrace --
java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getTokens(TokenMetadata.java:488)
	at org.apache.cassandra.service.StorageService.getTokens(StorageService.java:2561)
	at org.apache.cassandra.service.StorageService.getTokens(StorageService.java:2550)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1443)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1399)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:637)
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:323)
	at sun.rmi.transport.Transport$1.run(Transport.java:200)
	at sun.rmi.transport.Transport$1.run(Transport.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$94(TCPTransport.java:683)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$$Lambda$179/2070569218.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

{code}"
CASSANDRA-10980,nodetool scrub NPEs when keyspace isn't specified,I've attached logs of what I saw. Running nodetool scrub without anything else specified resulted in the NPE. Running with the keyspace specified saw successful termination.
CASSANDRA-10955,Multi-partitions queries with ORDER BY can result in a NPE,"In the case of a table with static columns, if only the static columns have been set for some partitions, a multi-partitions query with an {{ORDER BY}} can cause a {{NPE}}.

The following unit test can be used to reproduce the problem:
{code}
    @Test
    public void testOrderByForInClauseWithNullValue() throws Throwable
    {
        createTable(""CREATE TABLE %s (a int, b int, c int, s int static, d int, PRIMARY KEY (a, b, c))"");

        execute(""INSERT INTO %s (a, b, c, d) VALUES (1, 1, 1, 1)"");
        execute(""INSERT INTO %s (a, b, c, d) VALUES (1, 1, 2, 1)"");
        execute(""INSERT INTO %s (a, b, c, d) VALUES (2, 2, 1, 1)"");
        execute(""INSERT INTO %s (a, b, c, d) VALUES (2, 2, 2, 1)"");

        execute(""UPDATE %s SET s = 1 WHERE a = 1"");
        execute(""UPDATE %s SET s = 2 WHERE a = 2"");
        execute(""UPDATE %s SET s = 3 WHERE a = 3"");

        assertRows(execute(""SELECT a, b, c, d, s FROM %s WHERE a IN (1, 2, 3) ORDER BY b DESC""),
                   row(2, 2, 2, 1, 2),
                   row(2, 2, 1, 1, 2),
                   row(1, 1, 2, 1, 1),
                   row(1, 1, 1, 1, 1),
                   row(3, null, null, null, 3));
    }
{code} "
CASSANDRA-10909,NPE in ActiveRepairService,"NPE after one started multiple incremental repairs

{code}
INFO  [Thread-62] 2015-12-21 11:40:53,742  RepairRunnable.java:125 - Starting repair command #1, repairing keyspace keyspace1 with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 2)
INFO  [Thread-62] 2015-12-21 11:40:53,813  RepairSession.java:237 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] new session: will sync /10.200.177.32, /10.200.177.33 on range [(10,-9223372036854775808]] for keyspace1.[counter1, standard1]
INFO  [Repair#1:1] 2015-12-21 11:40:53,853  RepairJob.java:100 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] requesting merkle trees for counter1 (to [/10.200.177.33, /10.200.177.32])
INFO  [Repair#1:1] 2015-12-21 11:40:53,853  RepairJob.java:174 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Requesting merkle trees for counter1 (to [/10.200.177.33, /10.200.177.32])
INFO  [Thread-62] 2015-12-21 11:40:53,854  RepairSession.java:237 - [repair #b1449fe0-a7d7-11e5-b568-f565b837eb0d] new session: will sync /10.200.177.32, /10.200.177.31 on range [(0,10]] for keyspace1.[counter1, standard1]
INFO  [AntiEntropyStage:1] 2015-12-21 11:40:53,896  RepairSession.java:181 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Received merkle tree for counter1 from /10.200.177.32
INFO  [AntiEntropyStage:1] 2015-12-21 11:40:53,906  RepairSession.java:181 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Received merkle tree for counter1 from /10.200.177.33
INFO  [Repair#1:1] 2015-12-21 11:40:53,906  RepairJob.java:100 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] requesting merkle trees for standard1 (to [/10.200.177.33, /10.200.177.32])
INFO  [Repair#1:1] 2015-12-21 11:40:53,906  RepairJob.java:174 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Requesting merkle trees for standard1 (to [/10.200.177.33, /10.200.177.32])
INFO  [RepairJobTask:2] 2015-12-21 11:40:53,910  SyncTask.java:66 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] Endpoints /10.200.177.33 and /10.200.177.32 are consistent for counter1
INFO  [RepairJobTask:1] 2015-12-21 11:40:53,910  RepairJob.java:145 - [repair #b13e3740-a7d7-11e5-b568-f565b837eb0d] counter1 is fully synced
INFO  [AntiEntropyStage:1] 2015-12-21 11:40:54,823  Validator.java:272 - [repair #b17a2ed0-a7d7-11e5-ada8-8304f5629908] Sending completed merkle tree to /10.200.177.33 for keyspace1.counter1
ERROR [ValidationExecutor:3] 2015-12-21 11:40:55,104  CompactionManager.java:1065 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:3] 2015-12-21 11:40:55,105  Validator.java:259 - Failed creating a merkle tree for [repair #b17a2ed0-a7d7-11e5-ada8-8304f5629908 on keyspace1/standard1, [(10,-9223372036854775808]]], /10.200.177.33 (see log for details)
ERROR [ValidationExecutor:3] 2015-12-21 11:40:55,110  CassandraDaemon.java:195 - Exception in thread Thread[ValidationExecutor:3,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1066) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:80) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.db.compaction.CompactionManager$10.call(CompactionManager.java:679) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_40]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_40]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]
ERROR [AntiEntropyStage:1] 2015-12-21 11:40:55,174  RepairMessageVerbHandler.java:161 - Got error, removing parent repair session
INFO  [CompactionExecutor:3] 2015-12-21 11:40:55,175  CompactionManager.java:489 - Starting anticompaction for keyspace1.counter1 on 0/[] sstables
INFO  [CompactionExecutor:3] 2015-12-21 11:40:55,176  CompactionManager.java:547 - Completed anticompaction successfully
ERROR [AntiEntropyStage:1] 2015-12-21 11:40:55,179  CassandraDaemon.java:195 - Exception in thread Thread[AntiEntropyStage:1,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:164) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:67) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_40]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_40]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_40]
Caused by: java.lang.NullPointerException: null
	at org.apache.cassandra.service.ActiveRepairService$ParentRepairSession.getAndReferenceSSTables(ActiveRepairService.java:452) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.service.ActiveRepairService.doAntiCompaction(ActiveRepairService.java:379) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:136) ~[cassandra-all-3.0.1.777.jar:3.0.1.777]
	... 4 common frames omitted
{code}"
CASSANDRA-10787,OutOfMemoryError after few hours from node restart,"Cassandra Cluster was operating flawessly for around 3 months. Lately I've got a critical problem with it - after few hours of running clients are disconnected permanently (that may be Datastax C# Driver problem, though), however few more hours later (with smaller load), on all 2 nodes there is thrown an exception (details in files):

bq. java.lang.OutOfMemoryError: Java heap space


Cases description:

    Case 2 (heavy load):

        - 2015-11-26 16:09:40,834 Restarted all nodes in cassandra cluster
		- 2015-11-26 17:03:46,774 First client disconnected permanently
		- 2015-11-26 22:17:02,327 Node shutdown

	Case 3 (unknown load, different node):
		- 2015-11-26 02:19:49,585 Node shutdown (visible only in systemlog, I don't know why not in debug log)

	Case 4 (low load):
		- 2015-11-27 13:00:24,994 Node restart
		- 2015-11-27 22:26:56,131 Node shutdown

Is that a software issue or I am using too weak Amazon instances? If so, how can the required amount of memory be calculated?"
CASSANDRA-10736,TestTopology.simple_decommission_test failing due to assertion triggered by SizeEstimatesRecorder,"Example [here|http://cassci.datastax.com/job/cassandra-2.2_dtest/369/testReport/junit/topology_test/TestTopology/simple_decommission_test/].

{{SizeEstimatesRecorder}} can race with decommission when it tries to get the primary ranges for a node.

This is because {{getPredecessor}} in {{TokenMetadata}} hits an assertion if the token is no longer in {{TokenMetadata}}.

This no longer occurs in 3.0 because this assertion has been removed and replace with different data.

In both cases, the relationship between the set of tokens in {{getPrimaryRangesFor}} (passed in as an argument) and the set of tokens used in calls by {{getPredecessor}} (the system ones) should be investigated."
CASSANDRA-10697,Leak detected while running offline scrub,"I got couple of those:
{code}
ERROR 05:09:15 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3b60e162) to class org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier@1433208674:/var/lib/cassandra/data/sync/entity2-e24b5040199b11e5a30f75bb514ae072/sync-entity2-ka-405434 was not released before the reference was garbage collected
{code}

and then:
{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space

        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.decompressChunk(CompressedRandomAccessReader.java:99)

        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:81)

        at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:353)

        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:444)

        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:424)

        at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:378)

        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:348)

        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:327)

        at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:397)

        at org.apache.cassandra.db.composites.AbstractCType$Serializer.deserialize(AbstractCType.java:381)

        at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:75)

        at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:52)

        at org.apache.cassandra.db.AbstractCell$1.computeNext(AbstractCell.java:46)

        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)

        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)

        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.hasNext(SSTableIdentityIterator.java:120)

        at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:202)

        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)

        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)

        at com.google.common.collect.Iterators$7.computeNext(Iterators.java:645)

        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)

        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)

        at org.apache.cassandra.db.ColumnIndex$Builder.buildForCompaction(ColumnIndex.java:165)

        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:121)

        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:192)

        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:127)

        at org.apache.cassandra.io.sstable.SSTableRewriter.tryAppend(SSTableRewriter.java:158)

        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:220)

        at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:116)
{code}"
CASSANDRA-10614,AssertionError while flushing memtables,"While running mvbench against a single local node, I noticed this stacktrace showing up multiple times in the logs:

{noformat}
ERROR 16:40:01 Exception in thread Thread[MemtableFlushWriter:1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.db.rows.Rows.collectStats(Rows.java:70) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$StatsCollector.applyToRow(BigTableWriter.java:197) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:116) ~[main/:na]
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:38) ~[main/:na]
	at org.apache.cassandra.db.ColumnIndex.writeAndBuildIndex(ColumnIndex.java:49) ~[main/:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.append(BigTableWriter.java:149) ~[main/:na]
	at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.append(SimpleSSTableMultiWriter.java:45) ~[main/:na]
	at org.apache.cassandra.io.sstable.SSTableTxnWriter.append(SSTableTxnWriter.java:52) ~[main/:na]
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:389) ~[main/:na]
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:352) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299) ~[guava-18.0.jar:na]
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1037) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]
{noformat}

To reproduce, run mvbench with the regular schema and the following arguments:

{noformat}
mvn exec:java -Dexec.args=""--num-users 100000 --num-songs 1000000 --num-artists 10000 -n 500000 --endpoint 127.0.0.1""
{noformat}"
CASSANDRA-10602,2 upgrade test failures: static_columns_paging_test and multi_list_set_test,"The two following test throws a NPE:
* http://cassci.datastax.com/job/cassandra-3.0_dtest/293/testReport/junit/upgrade_tests.paging_test/TestPagingDataNodes2RF1/static_columns_paging_test/
* http://cassci.datastax.com/job/cassandra-3.0_dtest/293/testReport/junit/upgrade_tests.cql_tests/TestCQLNodes3RF3/multi_list_set_test/
"
CASSANDRA-10593,Unintended interactions between commitlog archiving and commitlog recycling,"Currently the comments in commitlog_archiving.properties suggest using either cp or ln for the archive_command.  

Using ln is problematic because commitlog recycling marks segments as recycled once the corresponding memtables are flushed and Cassandra will no longer replay them. This means it's only possible to do PITR on any records that were written since the last flush.

Using cp works, and this is currently how OpsCenter does for PITR, however [~brandon.williams] has pointed out this could have some performance impact because of the additional I/O overhead of copying the commitlog segments.

Starting in 2.1, we can disable commit log recycling in cassandra.yaml so I thought this would allow me to do PITR without the extra overhead of using cp.  However, when I disable commitlog recycling and try to do a PITR, Cassandra blows up when trying to replay the restored commit logs:

{code}
ERROR 16:56:42  Exception encountered during startup
java.lang.IllegalStateException: Cannot safely construct descriptor for segment, as name and header descriptors do not match ((4,1445878452545) vs (4,1445876822565)): /opt/dse/backup/CommitLog-4-1445876822565.log
	at org.apache.cassandra.db.commitlog.CommitLogArchiver.maybeRestoreArchive(CommitLogArchiver.java:207) ~[cassandra-all-2.1.9.791.jar:2.1.9.791]
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:116) ~[cassandra-all-2.1.9.791.jar:2.1.9.791]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:352) ~[cassandra-all-2.1.9.791.jar:2.1.9.791]
	at com.datastax.bdp.server.DseDaemon.setup(DseDaemon.java:335) ~[dse-core-4.8.0.jar:4.8.0]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:537) ~[cassandra-all-2.1.9.791.jar:2.1.9.791]
	at com.datastax.bdp.DseModule.main(DseModule.java:75) [dse-core-4.8.0.jar:4.8.0]
java.lang.IllegalStateException: Cannot safely construct descriptor for segment, as name and header descriptors do not match ((4,1445878452545) vs (4,1445876822565)): /opt/dse/backup/CommitLog-4-1445876822565.log
	at org.apache.cassandra.db.commitlog.CommitLogArchiver.maybeRestoreArchive(CommitLogArchiver.java:207)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:116)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:352)
	at com.datastax.bdp.server.DseDaemon.setup(DseDaemon.java:335)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:537)
	at com.datastax.bdp.DseModule.main(DseModule.java:75)
Exception encountered during startup: Cannot safely construct descriptor for segment, as name and header descriptors do not match ((4,1445878452545) vs (4,1445876822565)): /opt/dse/backup/CommitLog-4-1445876822565.log
INFO  16:56:42  DSE shutting down...
INFO  16:56:42  All plugins are stopped.
ERROR 16:56:42  Exception in thread Thread[Thread-2,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1403) ~[cassandra-all-2.1.9.791.jar:2.1.9.791]
	at com.datastax.bdp.gms.DseState.setActiveStatus(DseState.java:196) ~[dse-core-4.8.0.jar:4.8.0]
	at com.datastax.bdp.server.DseDaemon.preStop(DseDaemon.java:426) ~[dse-core-4.8.0.jar:4.8.0]
	at com.datastax.bdp.server.DseDaemon.safeStop(DseDaemon.java:436) ~[dse-core-4.8.0.jar:4.8.0]
	at com.datastax.bdp.server.DseDaemon$1.run(DseDaemon.java:676) ~[dse-core-4.8.0.jar:4.8.0]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_31]
{code}

For the sake of completeness, I also tested using cp for the archive_command and commitlog recycling disabled, and PITR works as expected, but this of course defeats the point.

It would be good to have some guidance on what is supported here. If ln isn't expected to work at all, it shouldn't be documented as an acceptable option for the archive_command in commitlog_archiving.properties.  If it should work with commitlog recycling disabled, the bug causing the IllegalStateException needs to be fixed. 

It would also be good to do some testing and quantify the performance impact of enabling commitlog archiving using cp as the archve_command.

I realize there are several different issues described here, so maybe they should be separate JIRAs, but first I wanted to just clarify whether we want to support ln at all, and we can go from there."
CASSANDRA-10550,NPE on null 'highestSelectivityIndex()',{{org.apache.cassandra.db.index.SecondaryIndexSearcher.highestSelectivityIndex()}} might return 'null' which makes {{org.apache.cassandra.service.StorageProxy.estimateResultRowsPerRange()}} NPE on some custom index implementations.
CASSANDRA-10547,Updating a CQL List many times creates many tombstones ,"We encountered a TombstoneOverwhelmingException in cassandra system.log which caused some of our CQL queries to fail.

We are able to reproduce this issue by updating a CQL List column many times. The number of tombstones created seems to be related to (number of list items * number of list updates). We update the entire list on each update using the java driver. (see attached code for details)

Running nodetool compact does not help, but nodetool flush does. It appears that the tombstones are being accumulated in memory. 

For example if we update a list of 100 items 1000 times, this creates more  than 100,000 tombstones and exceeds the default tombstone_failure_threshold.

"
CASSANDRA-10543,Self-reference leak in SegmentedFile,"CASSANDRA-9839, which moved {{crc_check_chance}} out of compression params and made it a top level table property introduced a reference leak in {{SegmentedFile}}. See [this comment|https://issues.apache.org/jira/browse/CASSANDRA-9839?focusedCommentId=14960528&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14960528 ] from [~benedict]"
CASSANDRA-10529,"Channel.size() is costly, mutually exclusive, and on the critical path","[~stefania_alborghetti] mentioned this already on another ticket, but I have lost track of exactly where. While benchmarking it became apparent this was a noticeable bottleneck for small in-memory workloads with few files, especially with RF=1. We should probably fix this soon, since it is trivial to do so, and the call is only to impose an assertion that our requested length is less than the file size. It isn't possible to safely memoize a value anywhere we can guarantee to be able to safely refer to it without some refactoring, so I suggest simply removing the assertion for now.
"
CASSANDRA-10503,NPE in MVs on update,"I've stumbled upon an NPE in MVs on update. This script will reproduce 100% on trunk from {{Date:   Sat Oct 10 09:23:15 2015 +0100}}

{code}
ERROR [SharedPool-Worker-3] 2015-10-10 21:35:01,867 Keyspace.java:487 - Unknown exception caught while attempting to update MaterializedView! test.test_with_cluster
java.lang.NullPointerException: null
        at org.apache.cassandra.db.view.TemporalRow.clusteringValue(TemporalRow.java:381) ~[main/:na]
        at org.apache.cassandra.db.view.View.createUpdatesForInserts(View.java:355) ~[main/:na]
        at org.apache.cassandra.db.view.View.createMutations(View.java:664) ~[main/:na]
        at org.apache.cassandra.db.view.ViewManager.pushViewReplicaUpdates(ViewManager.java:130) ~[main/:na]
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:482) [main/:na]
        at org.apache.cassandra.db.Keyspace.apply(Keyspace.java:387) [main/:na]
        at org.apache.cassandra.db.Mutation.apply(Mutation.java:205) [main/:na]
        at org.apache.cassandra.service.StorageProxy$$Lambda$149/1333013217.run(Unknown Source) [main/:na]
        at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:1247) [main/:na]
        at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:2399) [main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{code}

And the script to trigger:

{code}
ccm remove test; 
ccm create test --install-dir=/Users/jeff/Desktop/Dev/cassandra/ -s -n 1 ; 
echo ""create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; use test; create table test ( id text primary key, last text, first text, high int, low int); insert into test(id,last,first,high,low) values ('a', 'a', 'a', 1, 1); insert into test(id,last,first,high,low) values ('a', 'b', 'b', 2, 2); insert into test(id,last,first,high,low) values ('a', 'c', 'c', 3, 3); insert into test(id,last,first,high,low) values ('a', 'e', 'e', 5, 5); insert into test(id,last,first,high,low) values ('a', 'd', 'd', 4, 4); select * from test where id='a';"" | ccm node1 cqlsh

echo ""Creating MV test_by_high on test""
echo ""use test; create materialized view test_by_high as select id, high from test where high is not null primary key(high, id);"" | ccm node1 cqlsh

echo ""Insert high score 6, this will succeed""
echo ""use test; insert into test(id,last,first,high,low) values ('a', 'f', 'f', 6, 6); "" | ccm node1 cqlsh 

sleep 1

echo ""Select from MV where score = 6, this will succeed""
echo ""use test; select * from test_by_high where high=6; "" | ccm node1 cqlsh

echo ""Create a larger table with clustering key""
echo ""use test; create table test_with_cluster(part text, clus text, last text, first text, high int, low int, primary key (part, clus));"" | ccm node1 cqlsh

echo ""use test; create materialized view high_view as select part, clus, high from test_with_cluster where part is not null and clus is not null and high is not null primary key(high, part, clus);"" | ccm node1 cqlsh

echo ""Populate test_with_cluster, this will break""
echo ""use test; insert into test_with_cluster(part, clus,last,first,high,low) values ('a', 'a', 'a', 'a', 1, 1); "" | ccm node1 cqlsh
{code}

Logs from my previous tests (which I've deleted, unfortunately) suggest that the NPE is due to using the wrong {{ColumnIdentifier}} - it's using {{id}} (from test.test?) which causes the NPE in {{clusteringValue()}}, since it's in the wrong base table. "
CASSANDRA-10447,Stop TeeingAppender on shutdown hook,"Stefania discovered that tests that don't produce a lot of log output end up producing 0 debug output to files because the data is not flushed as part of the shutdown hook. I traced through and it looks like the shutdown hook doesn't actually invoke code that does anything useful. It shuts down an executor service in the logging context but doesn't call stop on any appenders.

A hackish thing we can do is use a status listener to collect all the appenders and then stop them when the shutdown hook runs. Even adding a small delay to the shutdown hook (no code changes on our part) would in let the async appender flush in 90% of cases.

We still need to fix it for test which uses a different config file and for which a small delay is not desirable."
CASSANDRA-10429,Flush schema tables after local schema change,"In CASSANDRA-7327, we disabled the normal flush of system schema tables after ""local"" schema changes to improve the runtime of unit tests.  However, there are some cases where this flush is necessary for schema durability.  For example, if a custom secondary index needs to make schema changes as part of it's creation, this is desirable."
CASSANDRA-10427,compactionstats 'completed' field not updating,"nodetool compactionstats is no longer showing any change to completed field or progress.

Can be reproduced with:

 * Start cassandra
 * Run stress write
 * run nodetool flush followed by nodetool compactionstats

{code}
➜  cassandra git:(cassandra-3.0) ✗ ./bin/nodetool flush          
➜  cassandra git:(cassandra-3.0) ✗ ./bin/nodetool compactionstats
pending tasks: 1
                                     id   compaction type    keyspace       table   completed       total    unit   progress
   75609a20-67a8-11e5-8003-c909ff7fd15e        Compaction   keyspace1   standard1           0   350132121   bytes      0.00%
Active compaction remaining time :   0h00m20s
➜  cassandra git:(cassandra-3.0) ✗ ./bin/nodetool compactionstats
pending tasks: 1
                                     id   compaction type    keyspace       table   completed       total    unit   progress
   75609a20-67a8-11e5-8003-c909ff7fd15e        Compaction   keyspace1   standard1           0   350132121   bytes      0.00%
Active compaction remaining time :   0h00m20s
➜  cassandra git:(cassandra-3.0) ✗ ./bin/nodetool compactionstats
pending tasks: 1
                                     id   compaction type    keyspace       table   completed       total    unit   progress
   75609a20-67a8-11e5-8003-c909ff7fd15e        Compaction   keyspace1   standard1           0   350132121   bytes      0.00%
Active compaction remaining time :   0h00m20s
➜  cassandra git:(cassandra-3.0) ✗ ./bin/nodetool compactionstats
pending tasks: 1
                                     id   compaction type    keyspace       table   completed       total    unit   progress
   75609a20-67a8-11e5-8003-c909ff7fd15e        Compaction   keyspace1   standard1           0   350132121   bytes      0.00%
Active compaction remaining time :   0h00m20s
➜  cassandra git:(cassandra-3.0) ✗ ./bin/nodetool compactionstats
pending tasks: 0
➜  cassandra git:(cassandra-3.0) ✗ 

{code}"
CASSANDRA-10424,Altering base table column with materialized view causes unexpected server error.,"When attempting to alter column type of base table which has a corresponding  materialized view we get an exception from the server.

Steps to reproduce.

1. Create a base table
{code}
CREATE TABLE test.scores(
                        user TEXT,
                        game TEXT,
                        year INT,
                        month INT,
                        day INT,
                        score TEXT,
                        PRIMARY KEY (user, game, year, month, day)
                        )
{code}

2. Create a corresponding materialized view

{code}
CREATE MATERIALIZED VIEW test.monthlyhigh AS
                SELECT game, year, month, score, user, day FROM test.scores
                WHERE game IS NOT NULL AND year IS NOT NULL AND month IS NOT NULL AND score IS NOT NULL AND user IS NOT NULL AND day IS NOT NULL
                PRIMARY KEY ((game, year, month), score, user, day)
                WITH CLUSTERING ORDER BY (score DESC, user ASC, day ASC)
{code}

3. Attempt to Alter the base table 
{code}
ALTER TABLE test.scores ALTER score TYPE blob
{code}

In the python driver we see the following exception returned from the server

{code}
Ignoring schedule_unique for already-scheduled task: (<bound method ControlConnection.refresh_schema of <cassandra.cluster.ControlConnection object at 0x100f72c50>>, (), (('keyspace', 'test'), ('target_type', 'KEYSPACE'), ('change_type', 'UPDATED')))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""./cassandra/cluster.py"", line 1623, in execute
    result = future.result()
  File ""./cassandra/cluster.py"", line 3205, in result
    raise self._final_exception
cassandra.protocol.ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator)."">
{code}

On the server I see the following stack trace

{code}
INFO  [MigrationStage:1] 2015-09-30 11:45:47,457 ColumnFamilyStore.java:825 - Enqueuing flush of keyspaces: 512 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:11] 2015-09-30 11:45:47,457 Memtable.java:362 - Writing Memtable-keyspaces@1714565887(0.146KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:11] 2015-09-30 11:45:47,463 Memtable.java:395 - Completed flushing /Users/gregbestland/.ccm/tests/node1/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/ma-54-big-Data.db (0.109KiB) for commitlog position ReplayPosition(segmentId=1443623481894, position=9812)
INFO  [MigrationStage:1] 2015-09-30 11:45:47,472 ColumnFamilyStore.java:825 - Enqueuing flush of columns: 877 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:12] 2015-09-30 11:45:47,472 Memtable.java:362 - Writing Memtable-columns@771367282(0.182KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:12] 2015-09-30 11:45:47,478 Memtable.java:395 - Completed flushing /Users/gregbestland/.ccm/tests/node1/data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/ma-51-big-Data.db (0.107KiB) for commitlog position ReplayPosition(segmentId=1443623481894, position=9812)
INFO  [MigrationStage:1] 2015-09-30 11:45:47,490 ColumnFamilyStore.java:825 - Enqueuing flush of views: 2641 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:11] 2015-09-30 11:45:47,490 Memtable.java:362 - Writing Memtable-views@1740452585(0.834KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:11] 2015-09-30 11:45:47,496 Memtable.java:395 - Completed flushing /Users/gregbestland/.ccm/tests/node1/data/system_schema/views-9786ac1cdd583201a7cdad556410c985/ma-22-big-Data.db (0.542KiB) for commitlog position ReplayPosition(segmentId=1443623481894, position=9812)
ERROR [MigrationStage:1] 2015-09-30 11:45:47,507 CassandraDaemon.java:195 - Exception in thread Thread[MigrationStage:1,5,main]
org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at org.apache.cassandra.config.CFMetaData.validateCompatility(CFMetaData.java:789) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:744) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:729) ~[main/:na]
	at org.apache.cassandra.config.Schema.updateView(Schema.java:677) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$2.onUpdated(SchemaKeyspace.java:587) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.diffSchema(SchemaKeyspace.java:701) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeViews(SchemaKeyspace.java:573) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:515) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:481) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:502) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
ERROR [SharedPool-Worker-1] 2015-09-30 11:45:47,508 QueryMessage.java:128 - Unexpected error during query
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:483) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceViewUpdate(MigrationManager.java:389) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:359) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:205) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:236) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:221) ~[main/:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:364) ~[main/:na]
	... 18 common frames omitted
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at org.apache.cassandra.config.CFMetaData.validateCompatility(CFMetaData.java:789) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:744) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:729) ~[main/:na]
	at org.apache.cassandra.config.Schema.updateView(Schema.java:677) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$2.onUpdated(SchemaKeyspace.java:587) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.diffSchema(SchemaKeyspace.java:701) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeViews(SchemaKeyspace.java:573) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:515) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:481) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:502) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	... 1 common frames omitted
ERROR [SharedPool-Worker-1] 2015-09-30 11:45:47,508 ErrorMessage.java:336 - Unexpected exception during request
java.lang.RuntimeException: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:483) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceViewUpdate(MigrationManager.java:389) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:359) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:93) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:205) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:236) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:221) ~[main/:na]
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:115) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.util.concurrent.ExecutionException: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_45]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_45]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:364) ~[main/:na]
	... 18 common frames omitted
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Column family comparators do not match or are not compatible (found ClusteringComparator; expected ClusteringComparator).
	at org.apache.cassandra.config.CFMetaData.validateCompatility(CFMetaData.java:789) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:744) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.reload(CFMetaData.java:729) ~[main/:na]
	at org.apache.cassandra.config.Schema.updateView(Schema.java:677) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace$2.onUpdated(SchemaKeyspace.java:587) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.diffSchema(SchemaKeyspace.java:701) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeViews(SchemaKeyspace.java:573) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:515) ~[main/:na]
	at org.apache.cassandra.schema.SchemaKeyspace.mergeSchema(SchemaKeyspace.java:481) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager$1.runMayThrow(MigrationManager.java:502) ~[main/:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45]
	... 1 common frames omitted
{code}

It would appear as though the base table column was migrated, as it now seems to be of type blob, but I'm not sure internally what state its in.
"
CASSANDRA-10401,Improve json2sstable error reporting on nonexistent column,"We have the following table...

{noformat}
CREATE TABLE keyspace_name.table_name (
    col1 text,
    col2 text,
    col3 text,
    col4 text,
    PRIMARY KEY ((col1, col2), col3)
) WITH CLUSTERING ORDER BY (col3 ASC)
{noformat}

And the following  json in a file created from sstable2json tool

{noformat}
[
{""key"": ""This is col1:This is col2,
 ""cells"": [[""This is col3:"","""",1443217787319002],
           [""This is col3:""col4"",""This is col4"",1443217787319002]]}
]
{noformat}

Let's say we deleted that record form the DB and wanted to bring it back
If we try to create an sstable from this data in a json file named test_file.json, we get a NPE 

{noformat}
-bash-4.1$ json2sstable -K elp -c table_name-3264cbe063c211e5bc34e746786b7b29 test_file.json  /var/lib/cassandra/data/keyspace_name/table_name-3264cbe063c211e5bc34e746786b7b29/keyspace_name-table_name-ka-1-Data.db
Importing 1 keys...
java.lang.NullPointerException
	at org.apache.cassandra.tools.SSTableImport.getKeyValidator(SSTableImport.java:442)
	at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:316)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:287)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:514)
ERROR: null
-bash-4.1$
{noformat}"
CASSANDRA-10393,LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref),"When trying to repair full on a table with the following schema my nodes stall 
and end up with spamming this 

I've recently changed the table from SizeTieredCompactionStrategy to LeveledCompactionStrategy.

Coming from 2.1.9 -> 2.2.0 -> 2.2.1 i ran upgradesstable without issue as well

When trying to full repair post compaction change, I got ""out of order"" errors. A few google searches later, I was told to ""scrub"" the keyspace - did that during the night (no problems logged, and no data lost)

Now a repair just stalls and output memory leaks all over the place 

{code}
CREATE KEYSPACE sessions WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;

CREATE TABLE sessions.sessions (
    id text PRIMARY KEY,
    client_ip text,
    controller text,
    controller_action text,
    created timestamp,
    data text,
    expires timestamp,
    http_host text,
    modified timestamp,
    request_agent text,
    request_agent_bot boolean,
    request_path text,
    site_id int,
    user_id int
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""NONE"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
{code}

{code}
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@4428a373) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1811114765:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104037-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@368dd97) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1811114765:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104037-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@66fb78be) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1811114765:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104037-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@9fdd2e6) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1460906269:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104788-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-24 10:25:28,475 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@84fcb91) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1460906269:/data/1/cassandra/sessions/sessions-77dd22f0ab9711e49cbc410c6b6f53a6/la-104788-big was not released before the reference was garbage collected
{code}"
CASSANDRA-10382,nodetool info doesn't show the correct DC and RACK,"When running *nodetool info* cassandra returns UNKNOWN_DC and UNKNOWN_RACK:

{code}
# nodetool info
ID                     : b94f9ca0-f886-4111-a471-02f295573f37
Gossip active          : true
Thrift active          : true
Native Transport active: true
Load                   : 44.97 MB
Generation No          : 1442913138
Uptime (seconds)       : 5386
Heap Memory (MB)       : 429.07 / 3972.00
Off Heap Memory (MB)   : 0.08
Data Center            : UNKNOWN_DC
Rack                   : UNKNOWN_RACK
Exceptions             : 1
Key Cache              : entries 642, size 58.16 KB, capacity 100 MB, 5580 hits, 8320 requests, 0.671 recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 50 MB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Token                  : (invoke with -T/--tokens to see all 256 tokens)
{code}

Correct DCs and RACKs are returned by *nodetool status* and *nodetool gossipinfo* commands:

{code}
# nodetool gossipinfo|grep -E 'RACK|DC'
  DC:POZ
  RACK:RACK30
  DC:POZ
  RACK:RACK30
  DC:SJC
  RACK:RACK68
  DC:POZ
  RACK:RACK30
  DC:SJC
  RACK:RACK62
  DC:SJC
  RACK:RACK62
{code}
{code}
# nodetool status|grep Datacenter
Datacenter: SJC
Datacenter: POZ
{code}"
CASSANDRA-10381,NullPointerException in cqlsh paging through CF with static columns,"When running select count( * ) from cqlsh with limit, the following NPE occurs:

select count( * ) from tbl1 limit 50000 ; 
{code}
ERROR [SharedPool-Worker-4] 2015-09-16 14:49:43,480 QueryMessage.java:132 - Unexpected error during query
java.lang.NullPointerException: null
at org.apache.cassandra.service.pager.RangeSliceQueryPager.containsPreviousLast(RangeSliceQueryPager.java:99) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:119) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.service.pager.RangeSliceQueryPager.fetchPage(RangeSliceQueryPager.java:37) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.statements.SelectStatement.pageCountQuery(SelectStatement.java:286) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:230) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:67) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler$StatementExecution.execute(DseQueryHandler.java:291) ~[dse-4.7.2.jar:4.7.2]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithTiming(DseQueryHandler.java:223) ~[dse-4.7.2.jar:4.7.2]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler$Operation.executeWithAuditLogging(DseQueryHandler.java:259) ~[dse-4.7.2.jar:4.7.2]
at com.datastax.bdp.cassandra.cql3.DseQueryHandler.process(DseQueryHandler.java:94) ~[dse-4.7.2.jar:4.7.2]
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:119) ~[cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_75]
at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [cassandra-all-2.1.8.621.jar:2.1.8.621]
at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
{code}

Table definition looks something like:
{code}
CREATE TABLE tbl1 (
    field1 bigint,
    field2 int,
    field3 timestamp,
    field4 map<int, float>,
    field5 text static,
    field6 text static,
    field7 text static
    PRIMARY KEY (field1, field2, field3)
) WITH CLUSTERING ORDER BY (field2 ASC, field3 ASC)
    AND bloom_filter_fp_chance = 0.1
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
   ...
{code}
Following appears in debug log leading up to the error:
{code}
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,484  AbstractQueryPager.java:95 - Fetched 101 live rows
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,484  AbstractQueryPager.java:133 - Remaining rows to page: 1
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,485  SelectStatement.java:285 - New maxLimit for paged count query is 1
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,486  StorageProxy.java:1646 - Estimated result rows per range: 2586.375; requested rows: 2, ranges.size(): 762; concurrent range requests: 1
DEBUG [SharedPool-Worker-1] 2015-09-17 15:32:06,487  AbstractQueryPager.java:95 - Fetched 2 live rows
ERROR [SharedPool-Worker-1] 2015-09-17 15:32:06,487  QueryMessage.java:132 - Unexpected error during query
java.lang.NullPointerException: null
{code}

I'm working on recreating to have a workable dataset.  When running cqlsh from remote node version 2.0.14, query returns successfully"
CASSANDRA-10375,nodetool repair fails,"When I'm running a *nodetool repair* it gets stalled after few seconds:

{code}[2015-09-19 11:12:13,807] Repair session 479ca1c0-5ebf-11e5-9619-3f4813058061 for range (40511972970986385,59154612555757611] failed with error [repair #479ca1c0-5ebf-11e5-9619-3f4813058061 on static_assets/assets, (40511972970986385,59154612555757611]] Validation failed in /10.8.34.113 (progress: 0%)
[2015-09-19 11:12:13,812] Repair session 479cc8d1-5ebf-11e5-9619-3f4813058061 for range (6553929828848556033,6576029219234973671] failed with error [repair #479cc8d1-5ebf-11e5-9619-3f4813058061 on static_assets/assets, (6553929828848556033,6576029219234973671]] Validation failed in /10.8.34.113 (progress: 0%)
{code}

At the same time I have this exception on another node (I'm not running multiple repairs):
{code}
ERROR [ValidationExecutor:66] 2015-09-19 11:12:13,825 CompactionManager.java:1070 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:66] 2015-09-19 11:12:13,826 Validator.java:246 - Failed creating a merkle tree for [repair #479c2c90-5ebf-11e5-9619-3f4813058061 on static_assets/assets, (-2926621365236563900,-2916361392298929067]], /10.8.34.113 (see log for details)
ERROR [ValidationExecutor:66] 2015-09-19 11:12:13,826 CassandraDaemon.java:183 - Exception in thread Thread[ValidationExecutor:66,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1071) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:94) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager$10.call(CompactionManager.java:669) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
ERROR [ValidationExecutor:66] 2015-09-19 11:12:13,900 CompactionManager.java:1070 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:66] 2015-09-19 11:12:13,900 Validator.java:246 - Failed creating a merkle tree for [repair #479c53a1-5ebf-11e5-9619-3f4813058061 on static_assets/assets, (8236929501578674892,8238760988019827700]], /10.8.34.113 (see log for details)
ERROR [ValidationExecutor:68] 2015-09-19 11:12:13,900 CompactionManager.java:1070 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:67] 2015-09-19 11:12:13,900 CompactionManager.java:1070 - Cannot start multiple repair sessions over the same sstables
ERROR [ValidationExecutor:66] 2015-09-19 11:12:13,901 CassandraDaemon.java:183 - Exception in thread Thread[ValidationExecutor:66,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1071) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:94) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager$10.call(CompactionManager.java:669) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
ERROR [ValidationExecutor:68] 2015-09-19 11:12:13,901 Validator.java:246 - Failed creating a merkle tree for [repair #479cc8d1-5ebf-11e5-9619-3f4813058061 on static_assets/assets, (6553929828848556033,6576029219234973671]], /10.8.34.113 (see log for details)
ERROR [ValidationExecutor:67] 2015-09-19 11:12:13,901 Validator.java:246 - Failed creating a merkle tree for [repair #479ca1c0-5ebf-11e5-9619-3f4813058061 on static_assets/assets, (40511972970986385,59154612555757611]], /10.8.34.113 (see log for details)
ERROR [ValidationExecutor:68] 2015-09-19 11:12:13,901 CassandraDaemon.java:183 - Exception in thread Thread[ValidationExecutor:68,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1071) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:94) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager$10.call(CompactionManager.java:669) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
ERROR [ValidationExecutor:67] 2015-09-19 11:12:13,901 CassandraDaemon.java:183 - Exception in thread Thread[ValidationExecutor:67,1,main]
java.lang.RuntimeException: Cannot start multiple repair sessions over the same sstables
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1071) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:94) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at org.apache.cassandra.db.compaction.CompactionManager$10.call(CompactionManager.java:669) ~[apache-cassandra-2.2.1.jar:2.2.1]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{code}

And on the remaining nodes I have something similar to this:
{code}
ERROR [Reference-Reaper:1] 2015-09-19 11:12:14,147 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@320d7d92) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@780334113:/var/lib/cassandra/data/static_assets/assets-ceb62f1056df11e5865db530abf562fa/la-2447-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-19 11:12:14,147 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@972fce9) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@389940435:/var/lib/cassandra/data/static_assets/assets-ceb62f1056df11e5865db530abf562fa/la-6-big was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-09-19 11:12:14,147 Ref.java:187 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5a2a0617) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@156533865:/var/lib/cassandra/data/static_assets/assets-ceb62f1056df11e5865db530abf562fa/la-4-big was not released before the reference was garbage collected
{code}"
CASSANDRA-10339,Prevent ALTER TYPE from creating circular references,"It's possible to define circular/recursive types using {{ALTER TYPE}}. They won't work in practice when you try to insert data, but we should detect this earlier and prevent the type modification.

Recursive type example (from [JAVA-908|https://datastax-oss.atlassian.net/browse/JAVA-908]):
{code}
CREATE TYPE node (name text,);
ALTER TYPE node ADD children frozen<list<node>>;
{code}

Circular example (from [Stack overflow|http://stackoverflow.com/questions/29037733/cassandra-2-1-recursion-by-nesting-udts]):
{code}
create type ping(pingid int);
create type pong(pongid int, ping frozen<ping>);
alter type ping ADD pong frozen<pong>;
{code}

Note that, in the circular example, references are properly checked when dropping the types, so neither type can be dropped."
CASSANDRA-10298,Replaced dead node stayed in gossip forever,"The dead node stayed in the nodetool status,

DN  10.210.165.55                    379.76 GB  256     ?       null

And in the log, it throws NPE when trying to remove it.

{code}
2015-09-10_06:41:22.92453 ERROR 06:41:22 Exception in thread Thread[GossipStage:1,5,main]
2015-09-10_06:41:22.92454 java.lang.NullPointerException: null
2015-09-10_06:41:22.92455       at org.apache.cassandra.utils.UUIDGen.decompose(UUIDGen.java:100) 
2015-09-10_06:41:22.92455       at org.apache.cassandra.db.HintedHandOffManager.deleteHintsForEndpoint(HintedHandOffManager.java:201) 
2015-09-10_06:41:22.92455       at org.apache.cassandra.service.StorageService.excise(StorageService.java:1886) 
2015-09-10_06:41:22.92455       at org.apache.cassandra.service.StorageService.excise(StorageService.java:1902) 
2015-09-10_06:41:22.92456       at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1805)
2015-09-10_06:41:22.92457       at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1473) 
2015-09-10_06:41:22.92457       at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:2099) 
2015-09-10_06:41:22.92457       at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:1009) 
2015-09-10_06:41:22.92458       at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:1085) 
2015-09-10_06:41:22.92458       at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49) 
2015-09-10_06:41:22.92458       at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:62) 
2015-09-10_06:41:22.92459       at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
2015-09-10_06:41:22.92460       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_45]
{code}
"
CASSANDRA-10274,Assertion Errors when interrupting Cleanup,"Exceptions encountered after Interrupting cleanup on 2.1.7 - logging due to nature. Seen on 2 different nodes. May be related to CASSANDRA-10260 . 

{code}
INFO  [CompactionExecutor:5836] 2015-09-06 11:33:39,630 CompactionManager.java:1286 - Compaction interrupted: Cleanup@74bffc10-0fbe-11e5-a5ce-37a8b36fe285(keyspace, table, 44698243387/139158
624314)bytes
INFO  [CompactionExecutor:5838] 2015-09-06 11:33:39,638 CompactionManager.java:1286 - Compaction interrupted: Cleanup@74bffc10-0fbe-11e5-a5ce-37a8b36fe285(keyspace, table, 37886026781/133123
638379)bytes
INFO  [CompactionExecutor:5836] 2015-09-06 11:33:39,639 CompactionManager.java:749 - Cleaning up SSTableReader(path='/mnt/cassandra/data/keyspace/table-74bffc100fbe11e5a5ce37a8b36fe285/keyspace-table-26598-Data.db')
ERROR [CompactionExecutor:5838] 2015-09-06 11:33:39,639 CassandraDaemon.java:223 - Exception in thread Thread[CompactionExecutor:5838,1,main]
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.SafeMemory.checkBounds(SafeMemory.java:97) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:281) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummary.getPositionInSummary(IndexSummary.java:139) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummary.getKey(IndexSummary.java:144) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:113) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableReader.getIndexScanPosition(SSTableReader.java:1183) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableReader.firstKeyBeyond(SSTableReader.java:1667) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.db.compaction.CompactionManager.needsCleanup(CompactionManager.java:693) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupOne(CompactionManager.java:734) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:94) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.db.compaction.CompactionManager$5.execute(CompactionManager.java:389) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:285) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]

{code}

And: 

{code}
ERROR [IndexSummaryManager:1] 2015-09-06 09:32:23,346 CassandraDaemon.java:223 - Exception in thread Thread[IndexSummaryManager:1,1,main]
java.lang.AssertionError: null
        at org.apache.cassandra.io.sstable.SSTableReader.setReplacedBy(SSTableReader.java:955) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableReader.cloneAndReplace(SSTableReader.java:1002) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.SSTableReader.cloneWithNewSummarySamplingLevel(SSTableReader.java:1105) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummaryManager.adjustSamplingLevels(IndexSummaryManager.java:421) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(IndexSummaryManager.java:299) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(IndexSummaryManager.java:238) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.io.sstable.IndexSummaryManager$1.runMayThrow(IndexSummaryManager.java:139) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_51]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_51]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_51]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
ERROR [Reference-Reaper:1] 2015-09-06 09:32:24,891 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@56b339db) to class org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier@1484053885:/mnt/cassandra/data/keyspace/table-74bffc100fbe11e5a5ce37a8b36fe285/keyspace-table-ka-6021 was not released before the reference was garbage collected
{code}"
CASSANDRA-10237,CFS.loadNewSSTables() broken for pre-3.0 sstables,"While working on CASSANDRA-10236 I discovered that {{CFS.loadNewSSTables()}} doesn't work for pre-3.0 sstables - just for version {{ma}} sstables.

TBC: Starting C* with 2.0, 2.1 or 2.2 sstables works - but loading new sstables during runtime doesn't.

Issues with {{CFS.loadNewSSTables()}} discovered so far:
# {{MetadataSerializer.deserialize(Descriptor,FileDataInput,EnumSet)}} returns {{null}} for {{MetadataType.HEADER}} which results in a NPE later in {{MetadataSerializer.serialize}} executing {{Collections.sort}}.
# After working around the previous issue, it turns out that it couldn't load the digest file, since {{Component.DIGEST}} is a singleton which refers to CRC32, but pre-3.0 sstables use Adler32.
# After working around that one, it fails in {{StreamingHistogram$StreamingHistogramSerializer.deserialize}} as {{maxBinSize==Integer.MAX_VALUE}}.

As loading legacy sstables works fine during startup, I assume my workarounds are not correct.

For reference, [this commit|https://github.com/snazy/cassandra/commit/2f0668a1e1d8a101e8301b9c4211b164c113afaa] contains a ton of legacy sstables (simple, counter, clustered and clustered+counter) for 2.0, 2.1 and 2.2. I've extended LegacySSTablesTest to read these tables using {{CFS.loadNewSSTables()}}.

{noformat:title=LegacySSTablesTest.txt}
diff --git a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
index d2922cc..1be6450 100644
--- a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
+++ b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
@@ -18,6 +18,9 @@
 package org.apache.cassandra.io.sstable;
 
 import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
 import java.util.HashSet;
@@ -27,10 +30,15 @@ import java.util.Set;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import org.apache.cassandra.SchemaLoader;
 import org.apache.cassandra.Util;
 import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.cql3.QueryProcessor;
 import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ConsistencyLevel;
 import org.apache.cassandra.db.DeletionTime;
 import org.apache.cassandra.db.Keyspace;
 import org.apache.cassandra.db.rows.SliceableUnfilteredRowIterator;
@@ -43,6 +51,7 @@ import org.apache.cassandra.exceptions.ConfigurationException;
 import org.apache.cassandra.io.sstable.format.SSTableFormat;
 import org.apache.cassandra.io.sstable.format.SSTableReader;
 import org.apache.cassandra.io.sstable.format.Version;
+import org.apache.cassandra.io.sstable.format.big.BigFormat;
 import org.apache.cassandra.schema.KeyspaceParams;
 import org.apache.cassandra.service.StorageService;
 import org.apache.cassandra.streaming.StreamPlan;
@@ -57,6 +66,8 @@ import static org.apache.cassandra.utils.ByteBufferUtil.bytes;
  */
 public class LegacySSTableTest
 {
+    private static final Logger logger = LoggerFactory.getLogger(LegacySSTableTest.class);
+
     public static final String LEGACY_SSTABLE_PROP = ""legacy-sstable-root"";
     public static final String KSNAME = ""Keyspace1"";
     public static final String CFNAME = ""Standard1"";
@@ -64,6 +75,8 @@ public class LegacySSTableTest
     public static Set<String> TEST_DATA;
     public static File LEGACY_SSTABLE_ROOT;
 
+    public static final String[] legacyVersions = {""jb"", ""ka"", ""la""};
+
     @BeforeClass
     public static void defineSchema() throws ConfigurationException
     {
@@ -208,4 +221,65 @@ public class LegacySSTableTest
             throw e;
         }
     }
+
+    @Test
+    public void testLegacyCqlTables() throws Exception
+    {
+        QueryProcessor.executeInternal(""CREATE KEYSPACE legacy_tables WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}"");
+
+        loadLegacyTables();
+    }
+
+    private void loadLegacyTables() throws IOException
+    {
+        for (String legacyVersion : legacyVersions)
+        {
+            logger.info(""Preparing legacy version {}"", legacyVersion);
+
+            QueryProcessor.executeInternal(String.format(""CREATE TABLE legacy_tables.legacy_%s_simple (pk text PRIMARY KEY, val text)"", legacyVersion));
+            QueryProcessor.executeInternal(String.format(""CREATE TABLE legacy_tables.legacy_%s_simple_counter (pk text PRIMARY KEY, val counter)"", legacyVersion));
+            QueryProcessor.executeInternal(String.format(""CREATE TABLE legacy_tables.legacy_%s_clust (pk text, ck text, val text, PRIMARY KEY (pk, ck))"", legacyVersion));
+            QueryProcessor.executeInternal(String.format(""CREATE TABLE legacy_tables.legacy_%s_clust_counter (pk text, ck text, val counter, PRIMARY KEY (pk, ck))"", legacyVersion));
+
+            loadLegacyTable(""legacy_%s_simple"", legacyVersion);
+            loadLegacyTable(""legacy_%s_simple_counter"", legacyVersion);
+            loadLegacyTable(""legacy_%s_clust"", legacyVersion);
+            loadLegacyTable(""legacy_%s_clust_counter"", legacyVersion);
+
+        }
+    }
+
+    private void loadLegacyTable(String tablePattern, String legacyVersion) throws IOException
+    {
+        String table = String.format(tablePattern, legacyVersion);
+
+        logger.info(""Loading legacy table {}"", table);
+
+        ColumnFamilyStore cfs = Keyspace.open(""legacy_tables"").getColumnFamilyStore(table);
+
+        for (File cfDir : cfs.getDirectories().getCFDirectories())
+        {
+            copySstables(legacyVersion, table, cfDir);
+        }
+
+        cfs.loadNewSSTables();
+    }
+
+    private static void copySstables(String legacyVersion, String table, File cfDir) throws IOException
+    {
+        byte[] buf = new byte[65536];
+
+        for (File file : new File(LEGACY_SSTABLE_ROOT, String.format(""%s/legacy_tables/%s"", legacyVersion, table)).listFiles())
+        {
+            if (file.isFile())
+            {
+                File target = new File(cfDir, file.getName());
+                int rd;
+                FileInputStream is = new FileInputStream(file);
+                FileOutputStream os = new FileOutputStream(target);
+                while ((rd = is.read(buf)) >= 0)
+                    os.write(buf, 0, rd);
+            }
+        }
+    }
 }
{noformat}

{noformat:title=broken-workaround}
diff --git a/src/java/org/apache/cassandra/db/compaction/Verifier.java b/src/java/org/apache/cassandra/db/compaction/Verifier.java
index 554c782..e953b1d 100644
--- a/src/java/org/apache/cassandra/db/compaction/Verifier.java
+++ b/src/java/org/apache/cassandra/db/compaction/Verifier.java
@@ -96,7 +96,7 @@ public class Verifier implements Closeable
         {
             validator = null;
 
-            if (new File(sstable.descriptor.filenameFor(Component.DIGEST)).exists())
+            if (new File(sstable.descriptor.filenameFor(sstable.descriptor.digestComponent())).exists())
             {
                 validator = DataIntegrityMetadata.fileDigestValidator(sstable.descriptor);
                 validator.validate();
diff --git a/src/java/org/apache/cassandra/io/sstable/Component.java b/src/java/org/apache/cassandra/io/sstable/Component.java
index 54dd35b..d0405e4 100644
--- a/src/java/org/apache/cassandra/io/sstable/Component.java
+++ b/src/java/org/apache/cassandra/io/sstable/Component.java
@@ -34,6 +34,7 @@ public class Component
     public static final char separator = '-';
 
     final static EnumSet<Type> TYPES = EnumSet.allOf(Type.class);
+
     public enum Type
     {
         // the base data for an sstable: the remaining components can be regenerated
@@ -79,13 +80,17 @@ public class Component
         }
     }
 
+    private static final String DIGEST_CRC32_NAME = ""Digest.crc32"";
+    private static final String DIGEST_ADLER32_NAME = ""Digest.adler32"";
+
     // singleton components for types that don't need ids
     public final static Component DATA = new Component(Type.DATA);
     public final static Component PRIMARY_INDEX = new Component(Type.PRIMARY_INDEX);
     public final static Component FILTER = new Component(Type.FILTER);
     public final static Component COMPRESSION_INFO = new Component(Type.COMPRESSION_INFO);
     public final static Component STATS = new Component(Type.STATS);
-    public final static Component DIGEST = new Component(Type.DIGEST);
+    public final static Component DIGEST_CRC32 = new Component(Type.DIGEST, DIGEST_CRC32_NAME);
+    public final static Component DIGEST_ADLER32 = new Component(Type.DIGEST, DIGEST_ADLER32_NAME);
     public final static Component CRC = new Component(Type.CRC);
     public final static Component SUMMARY = new Component(Type.SUMMARY);
     public final static Component TOC = new Component(Type.TOC);
@@ -138,11 +143,23 @@ public class Component
             case FILTER:            component = Component.FILTER;                       break;
             case COMPRESSION_INFO:  component = Component.COMPRESSION_INFO;             break;
             case STATS:             component = Component.STATS;                        break;
-            case DIGEST:            component = Component.DIGEST;                       break;
             case CRC:               component = Component.CRC;                          break;
             case SUMMARY:           component = Component.SUMMARY;                      break;
             case TOC:               component = Component.TOC;                          break;
             case CUSTOM:            component = new Component(Type.CUSTOM, path.right); break;
+            case DIGEST:
+                switch (path.right)
+                {
+                    case DIGEST_CRC32_NAME:
+                        component = Component.DIGEST_CRC32;
+                        break;
+                    case DIGEST_ADLER32_NAME:
+                        component = Component.DIGEST_ADLER32;
+                        break;
+                    default:
+                        throw new IllegalStateException();
+                }
+                break;
             default:
                  throw new IllegalStateException();
         }
diff --git a/src/java/org/apache/cassandra/io/sstable/Descriptor.java b/src/java/org/apache/cassandra/io/sstable/Descriptor.java
index 38829df..0db6f00 100644
--- a/src/java/org/apache/cassandra/io/sstable/Descriptor.java
+++ b/src/java/org/apache/cassandra/io/sstable/Descriptor.java
@@ -32,6 +32,7 @@ import org.apache.cassandra.io.sstable.metadata.IMetadataSerializer;
 import org.apache.cassandra.io.sstable.metadata.LegacyMetadataSerializer;
 import org.apache.cassandra.io.sstable.metadata.MetadataSerializer;
 import org.apache.cassandra.utils.Pair;
+import org.apache.hadoop.mapred.JobTracker;
 
 import static org.apache.cassandra.io.sstable.Component.separator;
 
@@ -344,4 +345,16 @@ public class Descriptor
     {
         return hashCode;
     }
+
+    public Component digestComponent()
+    {
+        switch (version.compressedChecksumType())
+        {
+            case Adler32:
+                return Component.DIGEST_ADLER32;
+            case CRC32:
+                return Component.DIGEST_CRC32;
+        }
+        throw new IllegalStateException();
+    }
 }
diff --git a/src/java/org/apache/cassandra/io/sstable/format/SSTableWriter.java b/src/java/org/apache/cassandra/io/sstable/format/SSTableWriter.java
index bd21536..74e4b56 100644
--- a/src/java/org/apache/cassandra/io/sstable/format/SSTableWriter.java
+++ b/src/java/org/apache/cassandra/io/sstable/format/SSTableWriter.java
@@ -131,7 +131,7 @@ public abstract class SSTableWriter extends SSTable implements Transactional
                 Component.STATS,
                 Component.SUMMARY,
                 Component.TOC,
-                Component.DIGEST));
+                Component.DIGEST_CRC32));
 
         if (metadata.params.bloomFilterFpChance < 1.0)
             components.add(Component.FILTER);
diff --git a/src/java/org/apache/cassandra/io/sstable/metadata/MetadataSerializer.java b/src/java/org/apache/cassandra/io/sstable/metadata/MetadataSerializer.java
index 9a5eae8..a40c37a 100644
--- a/src/java/org/apache/cassandra/io/sstable/metadata/MetadataSerializer.java
+++ b/src/java/org/apache/cassandra/io/sstable/metadata/MetadataSerializer.java
@@ -122,7 +122,12 @@ public class MetadataSerializer implements IMetadataSerializer
                 in.seek(offset);
                 component = type.serializer.deserialize(descriptor.version, in);
             }
-            components.put(type, component);
+            if (component == null)
+            {
+                assert type != MetadataType.HEADER || !descriptor.version.storeRows();
+            }
+            else
+                components.put(type, component);
         }
         return components;
     }
diff --git a/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java b/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java
index 70cd860..b88f4f2 100644
--- a/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java
+++ b/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java
@@ -109,7 +109,7 @@ public class DataIntegrityMetadata
         {
             this.descriptor = descriptor;
             checksum = descriptor.version.uncompressedChecksumType().newInstance();
-            digestReader = RandomAccessReader.open(new File(descriptor.filenameFor(Component.DIGEST)));
+            digestReader = RandomAccessReader.open(new File(descriptor.filenameFor(descriptor.digestComponent())));
             dataReader = RandomAccessReader.open(new File(descriptor.filenameFor(Component.DATA)));
             try
             {
@@ -211,7 +211,7 @@ public class DataIntegrityMetadata
 
         public void writeFullChecksum(Descriptor descriptor)
         {
-            File outFile = new File(descriptor.filenameFor(Component.DIGEST));
+            File outFile = new File(descriptor.filenameFor(descriptor.digestComponent()));
             try (BufferedWriter out =Files.newBufferedWriter(outFile.toPath(), Charsets.UTF_8))
             {
                 out.write(String.valueOf(fullChecksum.getValue()));
diff --git a/src/java/org/apache/cassandra/io/util/FileUtils.java b/src/java/org/apache/cassandra/io/util/FileUtils.java
index 920eee0..1420cae 100644
--- a/src/java/org/apache/cassandra/io/util/FileUtils.java
+++ b/src/java/org/apache/cassandra/io/util/FileUtils.java
@@ -173,7 +173,7 @@ public class FileUtils
 
     public static void renameWithConfirm(File from, File to)
     {
-        assert from.exists();
+        assert from.exists() : String.format(""File to rename does not exist: %s"", from.getPath());
         if (logger.isDebugEnabled())
             logger.debug((String.format(""Renaming %s to %s"", from.getPath(), to.getPath())));
         // this is not FSWE because usually when we see it it's because we didn't close the file before renaming it,
diff --git a/test/unit/org/apache/cassandra/db/VerifyTest.java b/test/unit/org/apache/cassandra/db/VerifyTest.java
index 13ce0c1..0233169 100644
--- a/test/unit/org/apache/cassandra/db/VerifyTest.java
+++ b/test/unit/org/apache/cassandra/db/VerifyTest.java
@@ -275,11 +275,11 @@ public class VerifyTest
         SSTableReader sstable = cfs.getLiveSSTables().iterator().next();
 
 
-        RandomAccessFile file = new RandomAccessFile(sstable.descriptor.filenameFor(Component.DIGEST), ""rw"");
+        RandomAccessFile file = new RandomAccessFile(sstable.descriptor.filenameFor(sstable.descriptor.digestComponent()), ""rw"");
         Long correctChecksum = Long.parseLong(file.readLine());
         file.close();
 
-        writeChecksum(++correctChecksum, sstable.descriptor.filenameFor(Component.DIGEST));
+        writeChecksum(++correctChecksum, sstable.descriptor.filenameFor(sstable.descriptor.digestComponent()));
 
         try (Verifier verifier = new Verifier(cfs, sstable, false))
         {
@@ -315,7 +315,7 @@ public class VerifyTest
         file.close();
 
         // Update the Digest to have the right Checksum
-        writeChecksum(simpleFullChecksum(sstable.getFilename()), sstable.descriptor.filenameFor(Component.DIGEST));
+        writeChecksum(simpleFullChecksum(sstable.getFilename()), sstable.descriptor.filenameFor(sstable.descriptor.digestComponent()));
 
         try (Verifier verifier = new Verifier(cfs, sstable, false))
         {
{noformat}
"
CASSANDRA-10222,Periodically attempt to delete failed snapshot deletions on Windows,"The changes in CASSANDRA-9658 leave us in a position where a node on Windows will have to be restarted to clear out snapshots that cannot be deleted at request time due to sstables still being mapped, thus preventing deletions of hard links. A simple periodic task to categorize failed snapshot deletions and retry them would help prevent node disk utilization from growing unbounded by snapshots as compaction will eventually make these snapshot files deletable.

Given that hard links to files in NTFS don't take up any extra space on disk so long as the original file still exists, the only limitation for users from this approach will be the inability to 'move' a snapshot file to another drive share. They will be copyable, however, so it's a minor platform difference.

This goes directly against the goals of CASSANDRA-8271 and will likely be built on top of that code. Until such time as we get buffered performance in-line with memory-mapped, this is an interim necessity for production roll-outs."
CASSANDRA-10181,Deadlock flushing tables with CUSTOM indexes,"In 3.0, if a table with a CUSTOM secondary index is force flushed, Cassandra will deadlock while attempting to perform a blocking flush on the tables backing the secondary indexes.

The basic problem is that the base table's post-flush task ends up waiting on the post-flush task for the secondary index to complete.  However, since the post-flush executor is single-threaded, this results in a deadlock.

Here's the partial stacktrace for the base table part of this (line numbers may not be 100% accurate):
{noformat}
org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:927)
	at org.apache.cassandra.index.internal.CustomIndex.lambda$getBlockingFlushTask$0(VertexCentricIndex.java:114)
	at org.apache.cassandra.index.internal.CustomIndex$$Lambda$95/057902870.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134)
	at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:58)
	at com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:37)
	at org.apache.cassandra.index.SecondaryIndexManager.lambda$executeAllBlocking$39(SecondaryIndexManager.java:896)
	at org.apache.cassandra.index.SecondaryIndexManager$$Lambda$94/25774682.accept(Unknown Source)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580)
	at org.apache.cassandra.index.SecondaryIndexManager.executeAllBlocking(SecondaryIndexManager.java:893)
	at org.apache.cassandra.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:346)
	at org.apache.cassandra.index.SecondaryIndexManager.flushAllCustomIndexesBlocking(SecondaryIndexManager.java:358)
	at org.apache.cassandra.db.ColumnFamilyStore$PostFlush.run(ColumnFamilyStore.java:960)
{noformat}

First, note that the base of this stacktrace is in CFS$PostFlush.run(), which means it's running on the post-flush executor.  When {{CFS.forceBlockingFlush()}} is called on the secondary index table, we end up blocking on another task that's submitted to the post-flush executor.  Since that executor is single-threaded and is already running the base table task, this results in deadlock.

The attached patch includes a unit test and custom secondary index class (basically just KeysIndex) to reproduce the issue."
CASSANDRA-10132,sstablerepairedset throws exception while loading metadata,"{{sstablerepairedset}} displays exception trying to load schema through DatabaseDescriptor.

{code}
$ ./tools/bin/sstablerepairedset --really-set --is-repaired ~/.ccm/3.0/node1/data/keyspace1/standard1-2c0b226046aa11e596f58106a0d438e8/ma-1-big-Data.db
14:42:36.714 [main] DEBUG o.a.c.i.s.m.MetadataSerializer - Mutating /home/yuki/.ccm/3.0/node1/data/keyspace1/standard1-2c0b226046aa11e596f58106a0d438e8/ma-1-big-Statistics.db to repairedAt time 1440013248000
14:42:36.721 [main] DEBUG o.a.c.i.s.m.MetadataSerializer - Load metadata for /home/yuki/.ccm/3.0/node1/data/keyspace1/standard1-2c0b226046aa11e596f58106a0d438e8/ma-1-big
Exception in thread ""main"" java.lang.ExceptionInInitializerError
        at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:123)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:110)
        at org.apache.cassandra.utils.memory.BufferPool.<clinit>(BufferPool.java:51)
        at org.apache.cassandra.io.util.RandomAccessReader.allocateBuffer(RandomAccessReader.java:76)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:58)
        at org.apache.cassandra.io.util.RandomAccessReader$RandomAccessReaderWithChannel.<init>(RandomAccessReader.java:89)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:108)
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:91)
        at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.mutateRepairedAt(MetadataSerializer.java:143)
        at org.apache.cassandra.tools.SSTableRepairedAtSetter.main(SSTableRepairedAtSetter.java:86)
Caused by: org.apache.cassandra.exceptions.ConfigurationException: Expecting URI in variable: [cassandra.config]. Found[cassandra.yaml]. Please prefix the file with [file:///] for local files and [file://<server>/] for remote files. If you are executing this from an external tool, it needs to set Config.setClientMode(true) to avoid loading configuration.
        at org.apache.cassandra.config.YamlConfigurationLoader.getStorageConfigURL(YamlConfigurationLoader.java:78)
        at org.apache.cassandra.config.YamlConfigurationLoader.<clinit>(YamlConfigurationLoader.java:92)
        ... 10 more
{code}

MetadataSerializer uses RandomAccessReader which allocates buffer through BufferPool. BufferPool gets its settings from DatabaseDescriptor and it won't work in offline tool."
CASSANDRA-10109,Windows dtest 3.0: ttl_test.py failures,"ttl_test.py:TestTTL.update_column_ttl_with_default_ttl_test2
ttl_test.py:TestTTL.update_multiple_columns_ttl_test
ttl_test.py:TestTTL.update_single_column_ttl_test

Errors locally are different than CI from yesterday. Yesterday on CI we have timeouts and general node hangs. Today on all 3 tests when run locally I see:
{noformat}
Traceback (most recent call last):
  File ""c:\src\cassandra-dtest\dtest.py"", line 532, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
AssertionError: Unexpected error in node1 node log: ['ERROR [main] 2015-08-17 16:53:43,120 NoSpamLogger.java:97 - This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr']
{noformat}

This traces back to the commit for CASSANDRA-7066 today by [~Stefania] and [~benedict].  Stefania - care to take this ticket and also look further into whether or not we're going to have issues with 7066 on Windows? That error message certainly *sounds* like it's not a good thing."
CASSANDRA-10104,Windows dtest 3.0: jmx_test.py:TestJMX.netstats_test fails,"{noformat}
Unexpected error in node1 node log: ['ERROR [HintedHandoff:2] 2015-08-16 23:14:04,419 CassandraDaemon.java:191 - Exception in thread Thread[HintedHandoff:2,1,main] org.apache.cassandra.exceptions.WriteFailureException: Operation failed - received 0 responses and 1 failures \tat org.apache.cassandra.service.AbstractWriteResponseHandler.get(AbstractWriteResponseHandler.java:106) ~[main/:na] \tat org.apache.cassandra.db.HintedHandOffManager.checkDelivered(HintedHandOffManager.java:358) ~[main/:na] \tat org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:414) ~[main/:na] \tat org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:346) ~[main/:na] \tat org.apache.cassandra.db.HintedHandOffManager.access$400(HintedHandOffManager.java:91) ~[main/:na] \tat org.apache.cassandra.db.HintedHandOffManager$5.run(HintedHandOffManager.java:537) ~[main/:na] \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_45] \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_45] \tat java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_45]']
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: d:\temp\dtest-j1ttp3
dtest: DEBUG: Nodetool command 'D:\jenkins\workspace\cassandra-3.0_dtest_win32\cassandra\bin\nodetool.bat -h localhost -p 7100 netstats' failed; exit status: 1; stdout: Starting NodeTool
; stderr: nodetool: Failed to connect to 'localhost:7100' - ConnectException: 'Connection refused: connect'.

dtest: DEBUG: removing ccm cluster test at: d:\temp\dtest-j1ttp3
dtest: DEBUG: clearing ssl stores from [d:\temp\dtest-j1ttp3] directory
--------------------- >> end captured logging << ---------------------
{noformat}

Failure history: [consistent|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/17/testReport/junit/jmx_test/TestJMX/netstats_test/history/]. Looks to have regressed on build [#5|http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest_win32/5/] which seems unlikely given the commit.

Env: Both, though on a local run the test fails due to:

{noformat}
Traceback (most recent call last):
  File ""c:\src\cassandra-dtest\dtest.py"", line 532, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
AssertionError: Unexpected error in node1 node log: ['ERROR [main] 2015-08-17 15:42:07,717 NoSpamLogger.java:97 - This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr', 'ERROR [main] 2015-08-17 15:50:43,978 NoSpamLogger.java:97 - This platform does not support atomic directory streams (SecureDirectoryStream); race conditions when loading sstable files could occurr']
{noformat}"
CASSANDRA-10079,"LEAK DETECTED, after nodetool drain","6 node cluster running 2.1.8

Sequence of events:

{quote}
2015-08-14 13:37:07,049 - Drain the node
2015-08-14 13:37:11,943 - Drained
2015-08-14 13:37:37,055 Ref.java:179 - LEAK DETECTED:
{quote}

{code}
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,055 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5534701) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@fab2c71) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@555d8efb) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7b29bfea) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1252635616:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-6883-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2d37dc5a) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@713444527:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@13153552) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@713444527:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@25f51e35) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@713444527:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3633d3dd) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,057 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2ec81280) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,058 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@144d1dae) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,058 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1944bda4) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@194296283:[[OffHeapBitSet]] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-08-14 13:37:37,058 Ref.java:179 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@31c1386a) to class org.apache.cassandra.io.util.MmappedSegmentedFile$Cleanup@1601396928:/var/lib/cassandra/data/metric/metric-811fa5402a3b11e5a2c0870545c0f352/metric-metric-ka-8229-Index.db was not released before the reference was garbage collected
{code}

See full log here:
https://dl.dropboxusercontent.com/u/4179566/cassandra-system.log"
CASSANDRA-10058,Close Java driver Client object in Hadoop and Pig classes,"I found that some Hadoop and Pig code in Cassandra doesn't close the Client object, that's the cause for the following errors in java driver 2.2.0-rc1.

{code}
ERROR 11:37:45 LEAK: You are creating too many HashedWheelTimer instances.  HashedWheelTimer is a shared resource that must be reused across the JVM,so that only a few instances are created.
{code}

We should close the Client objects."
CASSANDRA-10049,Commitlog initialization failure,"I've encountered this error locally during some dtests.

It looks like a race condition in the commit log code.  

http://cassci.datastax.com/view/cassandra-3.0/job/cassandra-3.0_dtest/lastCompletedBuild/testReport/consistency_test/TestAccuracy/test_network_topology_strategy_users_2/
"
CASSANDRA-10046,RangeTombstones handled incorrectly in Thrift,"Currently a number of thrift tests fail due to a NPE

{code}
test_range_deletion 
test_batch_mutate_remove_slice_standard
test_batch_mutate_remove_slice_of_entire_supercolumns
{code}

These errors all stem from the fact we aren't properly throwing a validation exception when a deletion with a slice that is not start == finish.  

We should add better validation to throw a clear exception sooner.

"
CASSANDRA-9973,java.lang.IllegalStateException: Unable to compute when histogram overflowed,"I recently, and probably mistakenly, upgraded one of my production C* clusters to 2.2.0.  I am seeing these errors in the logs, followed by an intense period of garbage collection until the node, then the ring, becomes crippled:

{code}
ERROR [OptionalTasks:1] 2015-08-04 03:24:56,057 CassandraDaemon.java:182 - Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.IllegalStateException: Unable to compute when histogram overflowed
        at org.apache.cassandra.utils.EstimatedHistogram.percentile(EstimatedHistogram.java:179) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at org.apache.cassandra.metrics.EstimatedHistogramReservoir$HistogramSnapshot.getValue(EstimatedHistogramReservoir.java:84) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at org.apache.cassandra.db.ColumnFamilyStore$3.run(ColumnFamilyStore.java:405) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:118) ~[apache-cassandra-2.2.0.jar:2.2.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_45]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_45]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
{code}

I am not sure if the GC instability is this or something else, but I though this histogram overflow issue was fixed in 2.1.3?  Anyway, reporting now as a possible regression.  Please let me know what I can provide in terms of information to help with this.  Thanks!"
CASSANDRA-9963,Compaction not starting for new tables,"Something committed since 2.1.8 broke cassandra-2.1 HEAD

{noformat}
create keyspace test with replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
create table test.stcs ( a int PRIMARY KEY , b int);
{noformat}

repeat  more than 4 times:
{noformat}
insert into test.stcs (a, b) VALUES ( 1, 1);
nodetool flush test stcs

ls <data dir>/test/stcs-*
{noformat}

See a bunch of sstables where STCS should have kicked in and compacted them down some."
CASSANDRA-9908,Potential race caused by async cleanup of transaction log files,"There seems to be a potential race in the cleanup of transaction log files, introduced in CASSANDRA-7066
It's pretty hard to trigger on trunk, but it's possible to hit it via {{o.a.c.db.SecondaryIndexTest#testCreateIndex}} 

That test creates an index, then removes it to check that the removal is correctly recorded, then adds the index again to assert that it gets rebuilt from the existing data. 
The removal causes the SSTables of the index CFS to be dropped, which is a transactional operation and so writes a transaction log. When the drop is completed and the last reference to an SSTable is released, the cleanup of the transaction log is scheduled on the periodic tasks executor. The issue is that re-creating the index re-creates the index CFS. When this happens, it's possible for the cleanup of the txn log to have not yet happened. If so, the initialization of the CFS attempts to read the log to identify any orphaned temporary files. The cleanup can happen between the finding the log file and reading it's contents, which results in a {{NoSuchFileException}}

{noformat}
[junit] java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit] java.lang.RuntimeException: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:620)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionFile.getTrackedFiles(TransactionLogs.java:190)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs$TransactionData.getTemporaryFiles(TransactionLogs.java:338)
[junit]     at org.apache.cassandra.db.lifecycle.TransactionLogs.getTemporaryFiles(TransactionLogs.java:739)
[junit]     at org.apache.cassandra.db.lifecycle.LifecycleTransaction.getTemporaryFiles(LifecycleTransaction.java:541)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.getFilter(Directories.java:652)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.filter(Directories.java:641)
[junit]     at org.apache.cassandra.db.Directories$SSTableLister.list(Directories.java:606)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:351)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:313)
[junit]     at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:511)
[junit]     at org.apache.cassandra.index.internal.CassandraIndexer.addIndexedColumn(CassandraIndexer.java:115)
[junit]     at org.apache.cassandra.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:265)
[junit]     at org.apache.cassandra.db.SecondaryIndexTest.testIndexCreate(SecondaryIndexTest.java:467)
[junit] Caused by: java.nio.file.NoSuchFileException: build/test/cassandra/data:1/SecondaryIndexTest1/CompositeIndexToBeAdded-d0885f60323211e5a5e8ad83a3dc3e9c/.birthdate_index/transactions/unknowncompactiontype_d4b69fc0-3232-11e5-a5e8-ad83a3dc3e9c_old.log
[junit]     at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
[junit]     at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
[junit]     at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:361)
[junit]     at java.nio.file.Files.newByteChannel(Files.java:407)
[junit]     at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
[junit]     at java.nio.file.Files.newInputStream(Files.java:152)
[junit]     at java.nio.file.Files.newBufferedReader(Files.java:2784)
[junit]     at java.nio.file.Files.readAllLines(Files.java:3202)
[junit]     at org.apache.cassandra.io.util.FileUtils.readLines(FileUtils.java:616)
[junit] 
[junit] 
[junit] Test org.apache.cassandra.db.SecondaryIndexTest FAILED
{noformat}
"
CASSANDRA-9882,DTCS (maybe other strategies) can block flushing when there are lots of sstables,"MemtableFlushWriter tasks can get blocked by Compaction getNextBackgroundTask.  This is in a wonky cluster with 200k sstables in the CF, but seems bad for flushing to be blocked by getNextBackgroundTask when we are trying to make these new ""smart"" strategies that may take some time to calculate what to do.

{noformat}
""MemtableFlushWriter:21"" daemon prio=10 tid=0x00007ff7ad965000 nid=0x6693 waiting for monitor entry [0x00007ff78a667000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:237)
	- waiting to lock <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1475)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1127)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

   Locked ownable synchronizers:
	- <0x0000000743b3ac38> (a java.util.concurrent.ThreadPoolExecutor$Worker)

""MemtableFlushWriter:19"" daemon prio=10 tid=0x00007ff7ac57a000 nid=0x649b waiting for monitor entry [0x00007ff78b8ee000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.handleNotification(WrappingCompactionStrategy.java:237)
	- waiting to lock <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.DataTracker.notifyAdded(DataTracker.java:518)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:178)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:234)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1475)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runMayThrow(Memtable.java:336)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1127)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

""CompactionExecutor:14"" daemon prio=10 tid=0x00007ff7ad359800 nid=0x4d59 runnable [0x00007fecce3ea000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.cassandra.io.sstable.SSTableReader.equals(SSTableReader.java:628)
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:206)
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:220)
	at com.google.common.collect.ImmutableSet.access$000(ImmutableSet.java:74)
	at com.google.common.collect.ImmutableSet$Builder.build(ImmutableSet.java:531)
	at com.google.common.collect.Sets$1.immutableCopy(Sets.java:606)
	at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:1352)
	at org.apache.cassandra.db.compaction.DateTieredCompactionStrategy.getNextBackgroundSSTables(DateTieredCompactionStrategy.java:88)
	at org.apache.cassandra.db.compaction.DateTieredCompactionStrategy.getNextBackgroundTask(DateTieredCompactionStrategy.java:65)
	- locked <0x00000006fcdbbf00> (a org.apache.cassandra.db.compaction.DateTieredCompactionStrategy)
	at org.apache.cassandra.db.compaction.WrappingCompactionStrategy.getNextBackgroundTask(WrappingCompactionStrategy.java:72)
	- locked <0x00000006fcdbbf60> (a org.apache.cassandra.db.compaction.WrappingCompactionStrategy)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:238)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

"
CASSANDRA-9840,global_row_key_cache_test.py fails; loses mutations on cluster restart,"This test is currently failing on trunk. I've attached the test output and logs. It seems that the failure of the test doesn't necessarily have anything to do with global row/key caches - as on the initial loop of the test [neither are used|https://github.com/riptano/cassandra-dtest/blob/master/global_row_key_cache_test.py#L15] and we still hit failure. The test itself fails when a second validation of values after a cluster restart fails to capture deletes issued prior to the restart and first successful validation. However, if I add flushes prior to restarting the cluster the test completes successfully, implying an issue with loss of in-memory mutations due to the cluster restart. Initially I had though this might be due to CASSANDRA-9669, but as Benedict pointed out, the fact that this test has been succeeding consistently on both 2.1 and 2.2 branch indicates there may be another issue at hand."
CASSANDRA-9800,2.2 eclipse-warnings,"commit 052222615a754e5bbb5299d51470a2ccdb70a5b0
Date:   Mon Jul 13 17:53:42 2015 -0400

If you wish to look at the latest output, check the {{eclipse_compiler_checks.txt}} artifact saved on the latest build:
http://cassci.datastax.com/job/cassandra-2.2_eclipse-warnings/lastBuild/

Output of current 2.2 HEAD eclipse-warnings:
{noformat}
# 7/14/15 12:21:30 AM UTC
# Eclipse Compiler for Java(TM) v20150120-1634, 3.10.2, Copyright IBM Corp 2000, 2013. All rights reserved.
incorrect classpath: /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/build/cobertura/classes
----------
1. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/tools/SSTableExport.java (at line 315)
	ISSTableScanner scanner = reader.getScanner();
	                ^^^^^^^
Resource 'scanner' should be managed by try-with-resource
----------
----------
2. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java (at line 247)
	scanners.add(new LeveledScanner(intersecting, range));
	             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Potential resource leak: '<unassigned Closeable value>' may not be closed
----------
----------
3. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/db/compaction/CompactionManager.java (at line 819)
	ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());
	                ^^^^^^^
Resource 'scanner' should be managed by try-with-resource
----------
----------
4. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/db/WindowsFailedSnapshotTracker.java (at line 55)
	BufferedReader reader = new BufferedReader(new FileReader(TODELETEFILE));
	               ^^^^^^
Resource 'reader' should be managed by try-with-resource
----------
5. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/db/WindowsFailedSnapshotTracker.java (at line 55)
	BufferedReader reader = new BufferedReader(new FileReader(TODELETEFILE));
	               ^^^^^^
Resource 'reader' should be managed by try-with-resource
----------
----------
6. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/io/util/SegmentedFile.java (at line 183)
	ChannelProxy channelCopy = getChannel(path);
	             ^^^^^^^^^^^
Resource 'channelCopy' should be managed by try-with-resource
----------
7. ERROR in /mnt/data/jenkins/workspace/cassandra-2.2_eclipse-warnings/src/java/org/apache/cassandra/io/util/SegmentedFile.java (at line 186)
	return complete(channelCopy, overrideLength);
	^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Potential resource leak: 'channelCopy' may not be closed at this location
----------
7 problems (7 errors)
{noformat}"
CASSANDRA-9798,Cassandra seems to have deadlocks during flush operations,"Hi,
We noticed some problem with dropped mutationstages. Usually on one random node there is a situation that:
MutationStage ""active"" is full, ""pending"" is increasing  ""completed"" is stalled.
MemtableFlushWriter ""active"" 6, pending: 25 completed: stalled 
MemtablePostFlush ""active"" is 1, pending 29 completed: stalled

after a some time (30s-10min) pending mutations are dropped and everything is working.
When it happened:
1. Cpu idle is ~95%
2. no gc long pauses or more activity.
3. memory usage 3.5GB form 8GB
4. only writes is processed by cassandra
5. when LOAD > 400GB/node problems appeared 
6. cassandra 2.1.6

There is gap in logs:
{code}
INFO  08:47:01 Timed out replaying hints to /192.168.100.83; aborting (0 delivered)
INFO  08:47:01 Enqueuing flush of hints: 7870567 (0%) on-heap, 0 (0%) off-heap
INFO  08:47:30 Enqueuing flush of table1: 95301807 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:31 Enqueuing flush of table1: 60462632 (3%) on-heap, 0 (0%) off-heap
INFO  08:47:31 Enqueuing flush of table2: 76973746 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:31 Enqueuing flush of table1: 84290135 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:32 Enqueuing flush of table3: 56926652 (3%) on-heap, 0 (0%) off-heap
INFO  08:47:32 Enqueuing flush of table1: 85124218 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:33 Enqueuing flush of table2: 95663415 (4%) on-heap, 0 (0%) off-heap
INFO  08:47:58 CompactionManager                 2        39
INFO  08:47:58 Writing Memtable-table2@1767938721(13843064 serialized bytes, 162359 ops, 4%/0% of on/off-heap l
imit)
INFO  08:47:58 Writing Memtable-hints@1433125911(478703 serialized bytes, 424 ops, 0%/0% of on/off-heap limit)
INFO  08:47:58 Writing Memtable-table2@1318583275(11783615 serialized bytes, 137378 ops, 4%/0% of on/off-heap l
imit)
INFO  08:47:58 Enqueuing flush of compactions_in_progress: 969 (0%) on-heap, 0 (0%) off-heap
INFO  08:47:58 Writing Memtable-table1@541175113(17221327 serialized bytes, 180792 ops, 4%/0% of on/off-heap
 limit)
INFO  08:47:58 Writing Memtable-table1@1361154669(27138519 serialized bytes, 273472 ops, 6%/0% of on/off-hea
p limit)

INFO  08:48:03 2176 MUTATION messages dropped in last 5000ms
{code}

use case:
100% write - 100Mb/s, couples of CF ~10column each. max cell size 100B
CMS and G1GC tested - no difference
"
CASSANDRA-9774,fix sstableverify dtest,"One of our dtests for {{sstableverify}} ({{offline_tools_test.py:TestOfflineTools.sstableverify_test}}) is failing hard on trunk ([cassci history|http://cassci.datastax.com/view/trunk/job/trunk_dtest/lastCompletedBuild/testReport/offline_tools_test/TestOfflineTools/sstableverify_test/history/])

The way the test works is by deleting an SSTable, then running {{sstableverify}} on its table. In earlier versions, it successfully detects this problem and outputs that it ""was not released before the reference was garbage collected"". The test no longer finds this string in the output; looking through the output of the test, it doesn't look like it reports any problems at all.

EDIT: After digging into the C* source a bit, I may have misattributed the problem to {{sstableverify}}; this could be a more general memory management problem, as the error text expected in the dtest is emitted by part of the {{Ref}} implementation:

https://github.com/apache/cassandra/blob/075ff5000ced24b42f3b540815cae471bee4049d/src/java/org/apache/cassandra/utils/concurrent/Ref.java#L187"
CASSANDRA-9770,Expose API to load prepare statement based on preparedId,"Currently whenever client calls session.prepare(Statement), the Java driver prepares the statement with cassandra. Cassandra prepares the statement, caches in memory and returns prepareId. The driver caches it in the session and uses it subsequently. 

In case, we have another session (as in web application or REST services, there are multiple JVM and so different cluster and session instances), currently we need reprepare the same query for which we need the original query string and other details (like consistency level and so on).

Since cassandra is already caching in memory (also a ticket is there to persist as well in 3.x), we can have an API to load the preparestatement from any session (if not there in the cache already) as below

Session.loadPreparedStatement(MD5Digest id) 

where the driver can call Cassandra and load the preparedstatement (to get metadata, resultSetMetadata, routingKeyIndexes etc...).

WIth this, the client need to cache only the prepareId and can avoid multiple round trip to prepare the query (same query) again with different hosts.
"
CASSANDRA-9758,nodetool compactionhistory NPE,"nodetool compactionhistory may trigger NPE : 

{code}
admin@localhost:~$ nodetool compactionhistory
Compaction History: 
error: null
-- StackTrace --
java.lang.NullPointerException
	at com.google.common.base.Joiner$MapJoiner.join(Joiner.java:330)
	at org.apache.cassandra.utils.FBUtilities.toString(FBUtilities.java:515)
	at org.apache.cassandra.db.compaction.CompactionHistoryTabularData.from(CompactionHistoryTabularData.java:78)
	at org.apache.cassandra.db.SystemKeyspace.getCompactionHistory(SystemKeyspace.java:422)
	at org.apache.cassandra.db.compaction.CompactionManager.getCompactionHistory(CompactionManager.java:1490)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$2.run(Transport.java:202)
	at sun.rmi.transport.Transport$2.run(Transport.java:199)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:198)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:567)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.access$400(TCPTransport.java:619)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$1.run(TCPTransport.java:684)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$1.run(TCPTransport.java:681)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:681)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

{code}
admin@localhost:~$ cqlsh -e ""select * from system.compaction_history"" | grep -F null
 ede434c0-2306-11e5-8a1a-85b300e09146 |  120 | 0 |     peers | 2015-07-05 13:13:57+0200 |system | null
 ae33fb90-23a0-11e5-9245-85b300e09146 |  120 | 0 |     peers | 2015-07-06 07:34:32+0200 |system | null
 085cb1f0-2542-11e5-9291-dfb803ff9672 |  120 | 0 |     peers | 2015-07-08 09:22:04+0200 |system | null
 0dbd4240-2349-11e5-a72b-85b300e09146 |  120 | 0 |     peers | 2015-07-05 21:07:17+0200 |system | null
 51e56b70-2261-11e5-8df2-85b300e09146 |  120 | 0 |     peers | 2015-07-04 17:28:28+0200 |system | null
 8152f810-2229-11e5-9218-dfb803ff9672 |   30 | 0 |     peers | 2015-07-04 10:48:56+0200 |system | null
 659c0840-222f-11e5-b9c9-dfb803ff9672 |  120 | 0 |     peers | 2015-07-04 11:31:06+0200 |system | null
 f6538e30-249b-11e5-9bdf-85b300e09146 |  120 | 0 |     peers | 2015-07-07 13:33:17+0200 |system | null
 20817760-2499-11e5-a613-85b300e09146 |  120 | 0 |     peers | 2015-07-07 13:12:59+0200 |system | null
 f70364c0-2470-11e5-9408-dfb803ff9672 |  120 | 0 |     peers | 2015-07-07 08:25:30+0200 |system | null
 fe44d050-2567-11e5-9441-85b300e09146 |  120 | 0 |     peers | 2015-07-08 13:53:48+0200 |system | null
 9fb3fde1-2413-11e5-8b06-dfb803ff9672 |  120 | 0 |     peers | 2015-07-06 21:17:20+0200 |system | null
 084dbdd0-2542-11e5-9291-dfb803ff9672 |   32 | 0 | IndexInfo | 2015-07-08 09:22:04+0200 |system | null
{code}"
CASSANDRA-9686,FSReadError and LEAK DETECTED after upgrading,"After upgrading one of 15 nodes from 2.1.7 to 2.2.0-rc1 I get FSReadError and LEAK DETECTED on start. Deleting the listed files, the failure goes away.
{code:title=system.log}
ERROR [SSTableBatchOpen:1] 2015-06-29 14:38:34,554 DebuggableThreadPoolExecutor.java:242 - Error in ThreadPoolExecutor
org.apache.cassandra.io.FSReadError: java.io.IOException: Compressed file with 0 chunks encountered: java.io.DataInputStream@1c42271
	at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:117) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.compress.CompressionMetadata.create(CompressionMetadata.java:86) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.util.CompressedSegmentedFile$Builder.metadata(CompressedSegmentedFile.java:142) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:101) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.util.SegmentedFile$Builder.complete(SegmentedFile.java:178) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:681) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.load(SSTableReader.java:644) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:443) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:350) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at org.apache.cassandra.io.sstable.format.SSTableReader$4.run(SSTableReader.java:480) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_55]
	at java.lang.Thread.run(Unknown Source) [na:1.7.0_55]
Caused by: java.io.IOException: Compressed file with 0 chunks encountered: java.io.DataInputStream@1c42271
	at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:174) ~[apache-cassandra-2.2.0-rc1.jar:2.2.0-rc1]
	... 15 common frames omitted
ERROR [Reference-Reaper:1] 2015-06-29 14:38:34,734 Ref.java:189 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3e547f) to class org.apache.cassandra.io.sstable.format.SSTableReader$InstanceTidier@1926439:D:\Programme\Cassandra\data\data\system\compactions_in_progress\system-compactions_in_progress-ka-6866 was not released before the reference was garbage collected
{code}"
CASSANDRA-9681,Memtable heap size grows and many long GC pauses are triggered,"C* 2.1.7 cluster is behaving really bad after 1-2 days. {{gauges.cassandra.jmx.org.apache.cassandra.metrics.ColumnFamily.AllMemtablesHeapSize.Value}} jumps to 7 GB (https://www.dropbox.com/s/vraggy292erkzd2/Screenshot%202015-06-29%2019.12.53.png?dl=0) on 3/6 nodes in each data center and then there are many long GC pauses. Cluster is using default heap size values ({{-Xms8192M -Xmx8192M -Xmn2048M}})

Before C* 2.1.5 memtables heap size was basically constant ~500MB (https://www.dropbox.com/s/fjdywik5lojstvn/Screenshot%202015-06-29%2019.30.00.png?dl=0)

After restarting all nodes is behaves stable for 1-2days. Today I've done that and long GC pauses are gone (~18:00 https://www.dropbox.com/s/7vo3ynz505rsfq3/Screenshot%202015-06-29%2019.28.37.png?dl=0). The only pattern we've found so far is that long GC  pauses are happening basically at the same time on all nodes in the same data center - even on the ones where memtables heap size is not growing.

Cliffs on the graphs are nodes restarts.

Used memory on boxes where {{AllMemtabelesHeapSize}} grows, stays at the same level - https://www.dropbox.com/s/tes9abykixs86rf/Screenshot%202015-06-29%2019.37.52.png?dl=0.

Replication factor is set to 3."
CASSANDRA-9676,CQLSSTableWriter gives java.lang.AssertionError: Empty partition in C* 2.0.15,"I've the same issue as described in https://issues.apache.org/jira/browse/CASSANDRA-9071
As I can understand it happens during the buffer flush, which size regulated by the withBufferSizeInMB() method call in
{code} 
    CQLSSTableWriter
      .builder()
      .inDirectory(createOutputDir())
      .forTable(metadata.schema)
      .using(insertStatement)
      .withBufferSizeInMB(128)
    .build()
{code}
For example, when I use 128 Mb buffer, it fails after 210 000 csv lines processed. On 3Mb buffer it fails after 10 000 lines."
CASSANDRA-9658,Re-enable memory-mapped index file reads on Windows,"It appears that the impact of buffered vs. memory-mapped index file reads has changed dramatically since last I tested. [Here's some results on various platforms we pulled together yesterday w/2.2-HEAD|https://docs.google.com/spreadsheets/d/1JaO2x7NsK4SSg_ZBqlfH0AwspGgIgFZ9wZ12fC4VZb0/edit#gid=0].

TL;DR: On linux we see a 40% hit in performance from 108k ops/sec on reads to 64.8k ops/sec. While surprising in itself, the really unexpected result (to me) is on Windows - with standard access we're getting 16.8k ops/second on our bare-metal perf boxes vs. 184.7k ops/sec with memory-mapped index files, an over 10-fold increase in throughput. While testing w/standard access, CPU's on the stress machine and C* node are both sitting < 4%, network doesn't appear bottlenecked, resource monitor doesn't show anything interesting, and performance counters in the kernel show very little. Changes in thread count simply serve to increase median latency w/out impacting any other visible metric that we're measuring, so I'm at a loss as to why the disparity is so huge on the platform.

The combination of my changes to get the 2.1 branch to behave on Windows along with [~benedict] and [~Stefania]'s changes in lifecycle and cleanup patterns on 2.2 should hopefully have us in a state where transitioning back to using memory-mapped I/O on Windows will only cause trouble on snapshot deletion. Fairly simple runs of stress w/compaction aren't popping up any obvious errors on file access or renaming - I'm going to do some much heavier testing (ccm multi-node clusters, long stress w/repair and compaction, etc) and see if there's any outstanding issues that need to be stamped out to call mmap'ed index files on Windows safe. The one thing we'll never be able to support is deletion of snapshots while a node is running and sstables are mapped, but for a > 10x throughput increase I think users would be willing to make that sacrifice.

The combination of the powercfg profile change, the kernel timer resolution, and memory-mapped index files are giving some pretty interesting performance numbers on EC2."
CASSANDRA-9656,Strong circular-reference leaks,"As discussed in CASSANDRA-9423, we are leaking references to the ref-counted object into the Ref.Tidy, so that they remain strongly reachable, significantly limiting the value of the leak detection.

"
CASSANDRA-9624,unable to bootstrap; streaming fails with NullPointerException,"When attempting to bootstrap a new node into a 2.1.3 cluster, the stream source fails with a {{NullPointerException}}:

{noformat}
ERROR [STREAM-IN-/10.xx.x.xxx] 2015-06-13 00:02:01,264 StreamSession.java:477 - [Stream #60e8c120-
115f-11e5-9fee-xxxxxxxxxxxx] Streaming error occurred
java.lang.NullPointerException: null
        at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:1277) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.streaming.StreamSession.getSSTableSectionsForRanges(StreamSession.java:313) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.streaming.StreamSession.addTransferRanges(StreamSession.java:266) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:493) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:425) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:251) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_79]
INFO  [STREAM-IN-/10.xx.x.xxx] 2015-06-13 00:02:01,265 StreamResultFuture.java:180 - [Stream #60e8c120-115f-11e5-9fee-xxxxxxxxxxxx] Session with /10.xx.x.xx1 is complete
{noformat}

_Update (2015-06-26):_

I can also reproduce this on 2.1.7, though without the NPE on the stream-from side.

Stream source / existing node:
{noformat}
INFO  [STREAM-IN-/10.64.32.178] 2015-06-26 06:48:53,060 StreamResultFuture.java:180 - [Stream #8bdeb1b0-1ad2-11e5-abd8-3fcfb96209d9] Session with /10.64.32.178 is complete
INFO  [STREAM-IN-/10.64.32.178] 2015-06-26 06:48:53,064 StreamResultFuture.java:212 - [Stream #8bdeb1b0-1ad2-11e5-abd8-3fcfb96209d9] All sessions completed
{noformat}

Stream sink / bootstrapping node:
{noformat}
INFO  [StreamReceiveTask:57] 2015-06-26 06:48:53,061 StreamResultFuture.java:180 - [Stream #8bdeb1b0-1ad2-11e5-abd8-3fcfb96209d9] Session with /10.64.32.160 is complete
WARN  [StreamReceiveTask:57] 2015-06-26 06:48:53,062 StreamResultFuture.java:207 - [Stream #8bdeb1b0-1ad2-11e5-abd8-3fcfb96209d9] Stream failed
INFO  [CompactionExecutor:2885] 2015-06-26 06:48:53,062 ColumnFamilyStore.java:906 - Enqueuing flush of compactions_in_progress: 428 (0%) on-heap, 379 (0%) off-heap
INFO  [MemtableFlushWriter:959] 2015-06-26 06:48:53,063 Memtable.java:346 - Writing Memtable-compactions_in_progress@1203013482(294 serialized bytes, 12 ops, 0%/0% of on/off-heap limit)
ERROR [main] 2015-06-26 06:48:53,063 CassandraDaemon.java:541 - Exception encountered during startup
java.lang.RuntimeException: Error during boostrap: Stream failed
        at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:86) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:1137) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:927) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:723) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:605) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:378) [apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:524) [apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:613) [apache-cassandra-2.1.7.jar:2.1.7]
Caused by: org.apache.cassandra.streaming.StreamException: Stream failed
        at org.apache.cassandra.streaming.management.StreamEventJMXNotifier.onFailure(StreamEventJMXNotifier.java:85) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.executeListener(ExecutionList.java:156) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.ExecutionList.execute(ExecutionList.java:145) ~[guava-16.0.jar:na]
        at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:202) ~[guava-16.0.jar:na]
        at org.apache.cassandra.streaming.StreamResultFuture.maybeComplete(StreamResultFuture.java:208) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.streaming.StreamResultFuture.handleSessionComplete(StreamResultFuture.java:184) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.streaming.StreamSession.closeSession(StreamSession.java:409) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSession.java:682) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.streaming.StreamSession.taskCompleted(StreamSession.java:645) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at org.apache.cassandra.streaming.StreamReceiveTask$OnCompletionRunnable.run(StreamReceiveTask.java:139) ~[apache-cassandra-2.1.7.jar:2.1.7]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_79]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_79]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_79]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_79]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_79]
{noformat}"
CASSANDRA-9621,Repair of the SystemDistributed keyspace creates a non-trivial amount of memory pressure,"When a repair without any particular option is triggered, the {{SystemDistributed}} keyspace is repaired for all range, and in particular the {{repair_history}} table. For every range, that table is written and flushed (as part of normal repair), meaning that every range triggers the creation of a new 1MB slab region (this also triggers quite a few compactions that also write and flush {{compaction_progress}} at every start and end).

I don't know how much of a big deal this will be in practice, but I wonder if it's really useful to repair the {{repair_*}} tables by default so maybe we could exclude the SystemDistributed keyspace from default repairs and only repair it if asked explicitly?"
CASSANDRA-9592,`Periodically attempt to submit background compaction tasks,"There's more race conditions affecting compaction task submission than CASSANDRA-7745, so to prevent some of these problems stalling compactions, I propose simply submitting background compactions once every minute, if possible. This will typically be a no-op, but there's no harm in that, since it's very cheap to do."
CASSANDRA-9586,ant eclipse-warnings fails in trunk,"{noformat}
eclipse-warnings:
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/ecj
     [echo] Running Eclipse Code Analysis.  Output logged to /home/mshuler/git/cassandra/build/ecj/eclipse_compiler_checks.txt
     [java] incorrect classpath: /home/mshuler/git/cassandra/build/cobertura/classes
     [java] ----------
     [java] 1. ERROR in /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/RandomAccessReader.java (at line 81)
     [java]     super(new ChannelProxy(file), DEFAULT_BUFFER_SIZE, -1L, BufferType.OFF_HEAP);
     [java]           ^^^^^^^^^^^^^^^^^^^^^^
     [java] Potential resource leak: '<unassigned Closeable value>' may not be closed
     [java] ----------
     [java] 1 problem (1 error)

BUILD FAILED
{noformat}

(checked 2.2 and did not find this issue)
git blame on line 81 shows commit 17dd4cc for CASSANDRA-8897"
CASSANDRA-9580,Cardinality check broken during incremental compaction re-opening,"While testing LCS I found cfstats sometimes crashes during compaction 

It looks to be related to the incremental re-opening not having metadata.

{code}
----------------
Keyspace: stresscql
	Read Count: 0
	Read Latency: NaN ms.
	Write Count: 6590571
	Write Latency: 0.026910956273743198 ms.
	Pending Flushes: 0
		Table: ycsb
		SSTable count: 69
		SSTables in each level: [67/4, 1, 0, 0, 0, 0, 0, 0, 0]
		Space used (live): 3454857914
		Space used (total): 3454857914
		Space used by snapshots (total): 0
		Off heap memory used (total): 287361
		SSTable Compression Ratio: 0.0
error: /home/jake/workspace/cassandra/./bin/../data/data/stresscql/ycsb-ff399910104911e5a797a18c989fb6f2/stresscql-ycsb-tmplink-ka-125-Data.db
-- StackTrace --
java.lang.AssertionError: /home/jake/workspace/cassandra/./bin/../data/data/stresscql/ycsb-ff399910104911e5a797a18c989fb6f2/stresscql-ycsb-tmplink-ka-125-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.getApproximateKeyCount(SSTableReader.java:270)
	at org.apache.cassandra.metrics.ColumnFamilyMetrics$9.value(ColumnFamilyMetrics.java:296)
	at org.apache.cassandra.metrics.ColumnFamilyMetrics$9.value(ColumnFamilyMetrics.java:290)
	at com.yammer.metrics.reporting.JmxReporter$Gauge.getValue(JmxReporter.java:63)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:71)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:275)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1443)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:76)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1307)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1399)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:637)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:323)
	at sun.rmi.transport.Transport$1.run(Transport.java:178)
	at sun.rmi.transport.Transport$1.run(Transport.java:175)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:174)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:557)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:812)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:671)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

"
CASSANDRA-9573,OOM when loading sstables (system.hints),"[~andrew.tolbert] discovered an issue while running endurance tests on 2.2. A Node was not able to start and was killed by the OOM Killer.

Briefly, Cassandra use an excessive amount of memory when loading compressed sstables (off-heap?). We have initially seen the issue with system.hints before knowing it was related to compression. system.hints use lz4 compression by default. If we have a sstable of, say 8-10G, Cassandra will be killed by the OOM killer after 1-2 minutes. I can reproduce that bug everytime locally. 

* the issue also happens if we have 10G of data splitted in 13MB sstables.
* I can reproduce the issue if I put a lot of data in the system.hints table.
* I cannot reproduce the issue with a standard table using the same compression (LZ4). Something seems to be different when it's hints?

You wont see anything in the node system.log but you'll see this in /var/log/syslog.log:

{code}
Out of memory: Kill process 30777 (java) score 600 or sacrifice child
{code}

The issue has been introduced in this commit but is not related to the performance issue in CASSANDRA-9240: https://github.com/apache/cassandra/commit/aedce5fc6ba46ca734e91190cfaaeb23ba47a846

Here is the core dump and some yourkit snapshots in attachments. I am not sure you will be able to get useful information from them.
core dump: http://dl.alanb.ca/core.tar.gz

Not sure if this is related, but all dumps and snapshot points to EstimatedHistogramReservoir ... and we can see many javax.management.InstanceAlreadyExistsException: org.apache.cassandra.metrics:... exceptions in system.log before it hangs then crash.

To reproduce the issue: 
1. created a cluster of 3 nodes
2. start the whole cluster
3. shutdown node2 and node3
4. writes 10-15G of data on node1 with replication factor 3. You should see a lot of hints.
5. stop node1
6. start node2 and node3
7. start node1, you should OOM.

//cc [~tjake] [~benedict] [~andrew.tolbert]"
CASSANDRA-9549,Memory leak in Ref.GlobalState due to pathological ConcurrentLinkedQueue.remove behaviour,"We have been experiencing a severe memory leak with Cassandra 2.1.5 that, over the period of a couple of days, eventually consumes all of the available JVM heap space, putting the JVM into GC hell where it keeps trying CMS collection but can't free up any heap space. This pattern happens for every node in our cluster and is requiring rolling cassandra restarts just to keep the cluster running. We have upgraded the cluster per Datastax docs from the 2.0 branch a couple of months ago and have been using the data from this cluster for more than a year without problem.

As the heap fills up with non-GC-able objects, the CPU/OS load average grows along with it. Heap dumps reveal an increasing number of java.util.concurrent.ConcurrentLinkedQueue$Node objects. We took heap dumps over a 2 day period, and watched the number of Node objects go from 4M, to 19M, to 36M, and eventually about 65M objects before the node stops responding. The screen capture of our heap dump is from the 19M measurement.

Load on the cluster is minimal. We can see this effect even with only a handful of writes per second. (See attachments for Opscenter snapshots during very light loads and heavier loads). Even with only 5 reads a sec we see this behavior.

Log files show repeated errors in Ref.java:181 and Ref.java:279 and ""LEAK detected"" messages:
{code}
ERROR [CompactionExecutor:557] 2015-06-01 18:27:36,978 Ref.java:279 - Error when closing class org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier@1302301946:/data1/data/ourtablegoeshere-ka-1150
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@32680b31 rejected from org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor@573464d6[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1644]
{code}
{code}
ERROR [Reference-Reaper:1] 2015-06-01 18:27:37,083 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@74b5df92) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@2054303604:/data2/data/ourtablegoeshere-ka-1151 was not released before the reference was garbage collected
{code}
This might be related to [CASSANDRA-8723]?
"
CASSANDRA-9486,LazilyCompactedRow accumulates all expired RangeTombstones,"LazilyCompactedRow initializes a ColumnIndex.Builder to use its RangeTombstone.Tracker, but it only calls update() with a RT argument, never an atom. The Tracker only ever _adds_ if it receives a RT, never removes. So all the RT ever seen for the partition (that have expired) remain in memory until the compaction completes. To make matters worse, this then forces a linear scan of all of these RT for each live cell we add, so this extra load hangs around for a long time, and compactions stall.

This issue is biting one of our users badly (at least, it seems likely to be this issue), and there may be others. This user is not even making use of RT extensively themselves, only collections (presumably with a complete overwrite of the contents of the collection, resulting in a RT being generated).

Probably the best solution is to make the RT addition itself remove any already present that are no longer helpful."
CASSANDRA-9376,Investigate using asynchronous logback config for tests to avoid timeouts and reduce test time,"unit tests are run with debug logging which can generate a lot of output. Every log statement results in a write to the file descriptor so it is not zippy.

Make the logging async and don't flush for every log statement. Have logback register a shutdown hook to make sure everything flushes."
CASSANDRA-9346,Expand upgrade testing for commitlog changes,"It seems that the current upgrade dtests always flush/drain a node before upgrading it, meaning we have no coverage of reading the commitlog files from a previous version.

We should add (unless they exist somewhere I am not aware of ) a suite of tests that specifically target upgrading with a significant amount of data left in the commitlog files, that needs to be read by the upgraded node."
CASSANDRA-9321,Aggregate UDFs allow SFUNC return type to differ from STYPE if FFUNC specified,"When a final function is specified in an aggregate C* allows the return type of the state function to not match the state type.  

Allowing the mismatch if a final function is specified seems to be intentional as if you don't provide a final function and you provide a state function with a return type that doesn't match the state type, then C* gives you an error that states that they must match ""unless a final function is specified"".  

It seems incorrect regardless of whether or not a final function is present to allow the state functions return type to vary from state type.  And indeed if you do so it produces an error when you try to use the aggregate.

Here is a simple example that shows the problem:
{code}
CREATE OR REPLACE FUNCTION state_func(state int, p2 int)
RETURNS double LANGUAGE java AS 'return Double.valueOf(1.0);';

CREATE OR REPLACE FUNCTION final_func(state int)
RETURNS int
LANGUAGE java
AS 'return Integer.valueOf(1);';

CREATE OR REPLACE AGGREGATE my_aggregate( int )
SFUNC state_func
STYPE int
FINALFUNC final_func
INITCOND 1;

SELECT my_aggregate(column) FROM table;
{code}

The select produces the error:
{noformat}
Aggregate 'ks.my_aggregate : (int) -> int' exists but hasn't been loaded successfully for the following reason: Referenced state function 'ks.state_func [double, int]' for aggregate 'ks.my_aggregate' does not exist.
{noformat}

I was reproducing this with 3.0 trunk, though now I just grabbed the latest and there is an NPE instead of the error above:
{noformat}
java.lang.NullPointerException: at index 1
	at com.google.common.collect.ObjectArrays.checkElementNotNull(ObjectArrays.java:240) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableSet.construct(ImmutableSet.java:195) ~[guava-16.0.jar:na]
	at com.google.common.collect.ImmutableSet.of(ImmutableSet.java:116) ~[guava-16.0.jar:na]
	at org.apache.cassandra.cql3.functions.UDAggregate.getFunctions(UDAggregate.java:110) ~[main/:na]
	at org.apache.cassandra.cql3.selection.AbstractFunctionSelector$1.getFunctions(AbstractFunctionSelector.java:78) ~[main/:na]
	at org.apache.cassandra.cql3.selection.SelectorFactories.getFunctions(SelectorFactories.java:105) ~[main/:na]
{noformat}"
CASSANDRA-9317,Populate TokenMetadata earlier during startup,"For CASSANDRA-6696 we need to know the local tokens earlier when starting up, this is due to the fact that we need to know how to split the local ranges when we flush for example (we can flush during commitlog replay etc).

Breaking out the TMD population into its own method makes it possible to use for the offline tools post 6696 as well - we will need to know tokens when doing scrub etc"
CASSANDRA-9295,Streaming not holding on to refs long enough.,"While doing some testing around adding/removing nodes under load with cassandra-2.1 head as of a few days ago (after was 2.1.5 tagged) I am seeing stream out errors with file not found exceptions.  The file in question just finished being compacted into a new file a few lines earlier in the log.  Seems that streaming isn't holding onto Ref's correctly for the stuff in the stream plans.

I also see a ""corrupt sstable"" exception for the file the ""missing"" file was compacted to. Trimmed logs with just the compaction/streaming related stuff:
You can see the stream plan is initiated in between the compaction starting, and the compaction finishing.

{noformat}
INFO  [MemtableFlushWriter:3] 2015-05-04 16:08:21,239  Memtable.java:380 - Completed flushing /mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-4-Data.db (60666088 bytes) for commitlog position ReplayPosition(segmentId=1430755416941, position=32294797)
INFO  [CompactionExecutor:4] 2015-05-04 16:08:40,856  CompactionTask.java:140 - Compacting [SSTableReader(path='/mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-4-Data.db'), SSTableReader(path='/mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-3-Data.db')]
INFO  [STREAM-INIT-/10.240.213.56:53190] 2015-05-04 16:09:31,047  StreamResultFuture.java:109 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9 ID#0] Creating new streaming plan for Rebuild
INFO  [STREAM-INIT-/10.240.213.56:53190] 2015-05-04 16:09:31,238  StreamResultFuture.java:116 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9, ID#0] Received streaming plan for Rebuild
INFO  [STREAM-INIT-/10.240.213.56:53192] 2015-05-04 16:09:31,249  StreamResultFuture.java:116 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9, ID#0] Received streaming plan for Rebuild
INFO  [STREAM-IN-/10.240.213.56] 2015-05-04 16:09:31,353  ColumnFamilyStore.java:882 - Enqueuing flush of standard1: 91768068 (19%) on-heap, 0 (0%) off-heap
INFO  [STREAM-IN-/10.240.213.56] 2015-05-04 16:09:37,425  ColumnFamilyStore.java:882 - Enqueuing flush of solr: 10012689 (2%) on-heap, 0 (0%) off-heap
INFO  [STREAM-IN-/10.240.213.56] 2015-05-04 16:09:38,073  StreamResultFuture.java:166 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9 ID#0] Prepare completed. Receiving 0 files(0 bytes), sending 6 files(284288285 bytes)
INFO  [CompactionExecutor:4] 2015-05-04 16:10:11,047  CompactionTask.java:270 - Compacted 2 sstables to [/mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-5,/mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-8,].  182,162,816 bytes to 182,162,816 (~100% of original) in 90,188ms = 1.926243MB/s.  339,856 total partitions merged to 339,856.  Partition merge counts were {1:339856, }
ERROR [STREAM-OUT-/10.240.213.56] 2015-05-04 16:10:25,169  StreamSession.java:477 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9] Streaming error occurred
java.io.IOException: Corrupted SSTable : /mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-5-Data.db
	at org.apache.cassandra.io.util.DataIntegrityMetadata$ChecksumValidator.validate(DataIntegrityMetadata.java:79) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:149) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:102) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:58) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:346) [cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:318) [cassandra-all-2.1.5.426.jar:2.1.5.426]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]
INFO  [STREAM-OUT-/10.240.213.56] 2015-05-04 16:10:25,232  StreamResultFuture.java:180 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9] Session with /10.240.213.56 is complete
WARN  [STREAM-OUT-/10.240.213.56] 2015-05-04 16:10:25,269  StreamResultFuture.java:207 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9] Stream failed
ERROR [STREAM-OUT-/10.240.213.56] 2015-05-04 16:10:25,307  StreamSession.java:477 - [Stream #f261c040-f277-11e4-a070-d126f0416bc9] Streaming error occurred
java.lang.RuntimeException: java.io.FileNotFoundException: /mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-4-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:124) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:107) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.io.util.SegmentedFile.createReader(SegmentedFile.java:99) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1955) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:74) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:58) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:346) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:326) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_40]
Caused by: java.io.FileNotFoundException: /mnt/cass_data_disks/data1/keyspace1/standard1-49f17b30f27711e4a438775021e2cd7f/keyspace1-standard1-ka-4-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open0(Native Method) ~[na:1.8.0_40]
	at java.io.RandomAccessFile.open(RandomAccessFile.java:316) ~[na:1.8.0_40]
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:243) ~[na:1.8.0_40]
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:64) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:120) ~[cassandra-all-2.1.5.426.jar:2.1.5.426]
	... 10 common frames omitted
{noformat}"
CASSANDRA-9285,LEAK DETECTED in sstwriter,"reproduce bug : 

{code}
    public static void main(String[] args) throws Exception {
        System.setProperty(""cassandra.debugrefcount"",""true"");
        
        String ks = ""ks1"";
        String table = ""t1"";
        
        String schema = ""CREATE TABLE "" + ks + ""."" + table + ""(a1 INT, PRIMARY KEY (a1));"";
        String insert = ""INSERT INTO ""+ ks + ""."" + table + ""(a1) VALUES(?);"";
        
        File dir = new File(""/var/tmp/"" + ks + ""/"" + table);
        dir.mkdirs();
        
        CQLSSTableWriter writer = CQLSSTableWriter.builder().forTable(schema).using(insert).inDirectory(dir).build();
        
        writer.addRow(1);
        writer.close();
        writer = null;
        
        Thread.sleep(1000);System.gc();
        Thread.sleep(1000);System.gc();
    }
{code}

{quote}
[2015-05-01 16:09:59,139] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@79fa9da9) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@2053866990:Memory@[7f87f8043b20..7f87f8043b48) was not released before the reference was garbage collected
[2015-05-01 16:09:59,143] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@79fa9da9:
Thread[Thread-2,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32)
	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:33)
	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:111)
	at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:576)
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:140)
	at org.apache.cassandra.io.sstable.AbstractSSTableSimpleWriter.getWriter(AbstractSSTableSimpleWriter.java:58)
	at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:227)

[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@664382e3) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@899100784:Memory@[7f87f8043990..7f87f8043994) was not released before the reference was garbage collected
[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@664382e3:
Thread[Thread-2,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32)
	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:33)
	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:110)
	at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:576)
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:140)
	at org.apache.cassandra.io.sstable.AbstractSSTableSimpleWriter.getWriter(AbstractSSTableSimpleWriter.java:58)
	at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:227)

[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3cca0ac2) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@499043670:Memory@[7f87f8039940..7f87f8039c60) was not released before the reference was garbage collected
[2015-05-01 16:09:59,144] [Reference-Reaper:1] ERROR org.apache.cassandra.utils.concurrent.Ref - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@3cca0ac2:
Thread[Thread-2,5,main]
	at java.lang.Thread.getStackTrace(Thread.java:1552)
	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:32)
	at org.apache.cassandra.io.compress.CompressionMetadata$Writer.<init>(CompressionMetadata.java:274)
	at org.apache.cassandra.io.compress.CompressionMetadata$Writer.open(CompressionMetadata.java:285)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:74)
	at org.apache.cassandra.io.util.SequentialWriter.open(SequentialWriter.java:124)
	at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:129)
	at org.apache.cassandra.io.sstable.AbstractSSTableSimpleWriter.getWriter(AbstractSSTableSimpleWriter.java:58)
	at org.apache.cassandra.io.sstable.SSTableSimpleUnsortedWriter$DiskWriter.run(SSTableSimpleUnsortedWriter.java:227)
{quote}
"
CASSANDRA-9277,SSTableRewriterTest.testFileRemoval fails with test-compression,openEarly returns null with compression because not enough data has been written to trigger a flush. Solution is to write more data in the test case so it flushes. 
CASSANDRA-9242,Add PerfDisableSharedMem to default JVM params,"We should add PerfDisableSharedMem to default JVM params. The JVM will save stats to a memory mapped file when reaching a safepoint. This is performed synchronously and the JVM remains paused while this action takes place. Occasionally the OS will stall the calling thread while this happens resulting in significant impact to worst case JVM pauses. By disabling the save in the JVM these mysterious multi-second pauses disappear.

The behavior is outlined in [this article|http://www.evanjones.ca/jvm-mmap-pause.html]. Another manifestation is significant time spent in sys during GC pauses. In [the linked test|http://cstar.datastax.com/graph?stats=762d9c2a-eace-11e4-8236-42010af0688f&metric=gc_max_ms&operation=1_write&smoothing=1&show_aggregates=true&xmin=0&xmax=110.77&ymin=0&ymax=10421.4] you'll notice multiple seconds spent in sys during the longest pauses.
"
CASSANDRA-9238,Race condition after shutdown gossip message,"CASSANDRA-8336 introduced a race condition causing gossip messages to be sent to shutdown nodes even if they have been already marked dead.

That's because CASSANDRA-8336 changed (among other things) the way the SHUTDOWN gossip message is sent by moving it before the gossip task (the one sending SYN messages), and by putting a few secs wait between the two; this opens a race window by the receiving side between the time the SHUTDOWN message is received, causing the outbound sockets to be closed, and the moment the other side listening socket is actually closed, meaning that any SYN gossip message exchanged in such window will reopen the sockets and never close them again, as the node is already marked dead. "
CASSANDRA-9216,NullPointerException (NPE) during Compaction Cache Serialization,"In case this hasn't been reported (I looked but did not see it), a null pointer exception is occurring during compaction. The stack track is as follows:
{code}
ERROR [CompactionExecutor:50] 2015-04-20 13:42:43,827 CassandraDaemon.java:223 - Exception in thread Thread[CompactionExecutor:50,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:475) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:463) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:274) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1152) ~[apache-cassandra-2.1.4.jar:2.1.4]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_75]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_75]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_75]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]{code}"
CASSANDRA-9195,PITR commitlog replay only actually replays mutation every other time,"Version: Cassandra 2.1.4.374 | DSE 4.7.0

The main issue here is that the restore-cycle only replays the mutations
every other try.  On the first try, it will restore the snapshot as expected
and the cassandra system load will show that it's reading the mutations, but
they do not actually get replayed, and at the end you're left with only the
snapshot data (2k records).

If you re-run the restore-cycle again, the commitlogs are replayed as expected,
and the data expected is present in the table (4k records, with a spot check of 
record 4500, as it's in the commitlog but not the snapshot).

Then if you run the cycle again, it will fail.  Then again, and it will work. The work/
not work pattern continues.  Even re-running the commitlog replay a 2nd time, without
reloading the snapshot doesn't work

The load process is:

* Modify commitlog segment to 1mb
* Archive to directory
* create keyspace/table
* insert base data
* initial snapshot
* write more data
* capture timestamp
* write more data
* final snapshot
* copy commitlogs to 2nd location
* modify cassandra-env to replay only specified keyspace
* modify commitlog properties to restore from 2nd location, with noted timestamp

The restore cycle is:

* truncate table
* sstableload snapshot
* flush
* output data status
* restart to replay commitlogs
* output data status

====
See attached .py for a mostly automated reproduction scenario.  It expects DSE (and I found it with DSE 4.7.0-1), rather than ""actual"" Cassandra, but it's not using any DSE specific features.  The script looks for the configs in the DSE locations, but they're set at the top, and there's only 2 places where dse is restarted."
CASSANDRA-9194,Delete-only workloads crash Cassandra,"The size of a tombstone is not properly accounted for in the memtable. A memtable which has only tombstones will never get flushed. It will grow until the JVM runs out of memory. The following program easily demonstrates the problem.
{code}
		Cluster.Builder builder = Cluster.builder();
		
		Cluster c = builder.addContactPoints(""cas121.devf3.com"").build();
		
		Session s = c.connect();
			
		s.execute(""CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }"");

		s.execute(""CREATE TABLE IF NOT EXISTS test.test(id INT PRIMARY KEY)"");

		PreparedStatement stmt = s.prepare(""DELETE FROM test.test WHERE id = :id"");

		int id = 0;
		
		while (true)
		{
			s.execute(stmt.bind(id));
			
			id++;
		}{code}

This program should run forever, but eventually Cassandra runs out of heap and craps out. You needn't wait for Cassandra to crash. If you run ""nodetool cfstats test.test"" while it is running, you'll see Memtable cell count grow, but Memtable data size will remain 0.

This issue was fixed once before. I received a patch for version 2.0.5 (I believe), which contained the fix, but the fix has apparently been lost, because it is clearly broken, and I don't see the fix in the change logs."
CASSANDRA-9134,Fix leak detected errors in unit tests,"There are several of these errors when running unit tests on trunk:

{code}
    [junit] ERROR 01:09:36 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@317c884a) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@943674927:Memory@[7f1bcc0078e0..7f1bcc007908) was not released before the reference was garbage collected
    [junit] ERROR 01:09:36 LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@317c884a) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@943674927:Memory@[7f1bcc0078e0..7f1bcc007908) was not released before the reference was garbage collected
    [junit] ERROR 01:09:36 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@317c884a:
    [junit] Thread[CompactionExecutor:1,1,main]
    [junit] 	at java.lang.Thread.getStackTrace(Thread.java:1589)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
    [junit] 	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:33)
    [junit] 	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:31)
    [junit] 	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:112)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.<init>(BigTableWriter.java:491)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:83)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107)
    [junit] 	at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:89)
    [junit] 	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:53)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:253)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:153)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73)
    [junit] 	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:58)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:239)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    [junit] 	at java.lang.Thread.run(Thread.java:745)
    [junit] 
    [junit] ERROR 01:09:36 Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@317c884a:
    [junit] Thread[CompactionExecutor:1,1,main]
    [junit] 	at java.lang.Thread.getStackTrace(Thread.java:1589)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:200)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:133)
    [junit] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:60)
    [junit] 	at org.apache.cassandra.io.util.SafeMemory.<init>(SafeMemory.java:33)
    [junit] 	at org.apache.cassandra.io.util.SafeMemoryWriter.<init>(SafeMemoryWriter.java:31)
    [junit] 	at org.apache.cassandra.io.sstable.IndexSummaryBuilder.<init>(IndexSummaryBuilder.java:112)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.<init>(BigTableWriter.java:491)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:83)
    [junit] 	at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107)
    [junit] 	at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:89)
    [junit] 	at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.<init>(DefaultCompactionWriter.java:53)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:253)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:153)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:73)
    [junit] 	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:58)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:239)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    [junit] 	at java.lang.Thread.run(Thread.java:745)
    [junit] 
{code}"
CASSANDRA-9129,HintedHandoff in pending state forever after upgrading to 2.0.14 from 2.0.11 and 2.0.12,"Upgrading from Cassandra 2.0.11 or 2.0.12 to 2.0.14 I am seeing a pending hinted hand off that never clears.  New hinted hand offs that go into pending waiting for a node to come up clear as expected.  But 1 always remains.

I through the following steps.

1) stop cassandra
2) Upgrade cassandra to 2.0.14
3) Start cassandra
4) nodetool tpstats

There are no errors in the logs, to help with this issue.  I ran a few nodetool commands to get some data and pasted them below:

Below is what is shown after running nodetool status on each node in the ring
{code}Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens  Owns   Host ID   Rack
UN  <NODE1>  279.8 MB   256     34.9%  <HOSTID>       rack1
UN  <NODE2>  279.79 MB  256     33.0%  <HOSTID>       rack1
UN  <NODE3>  279.87 MB  256     32.1%  <HOSTID>       rack1
{code}

Below is what is shown after running nodetool tpstats on each node in the ring showing a single HintedHandoff in pending status that never clears
{code}
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
ReadStage                         0         0          14550         0                 0
RequestResponseStage              0         0         113040         0                 0
MutationStage                     0         0         168873         0                 0
ReadRepairStage                   0         0           1147         0                 0
ReplicateOnWriteStage             0         0              0         0                 0
GossipStage                       0         0         232112         0                 0
CacheCleanupExecutor              0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
MemoryMeter                       0         0              6         0                 0
FlushWriter                       0         0             38         0                 0
ValidationExecutor                0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
AntiEntropyStage                  0         0              0         0                 0
MemtablePostFlusher               0         0           1333         0                 0
MiscStage                         0         0              0         0                 0
PendingRangeCalculator            0         0              6         0                 0
CompactionExecutor                0         0            178         0                 0
commitlog_archiver                0         0              0         0                 0
HintedHandoff                     0         1            133         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
PAGED_RANGE                  0
BINARY                       0
READ                         0
MUTATION                     0
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
{code}

Below is what is shown after running nodetool cfstats system.hints on all 3 nodes.
{code}
Keyspace: system
	Read Count: 0
	Read Latency: NaN ms.
	Write Count: 0
	Write Latency: NaN ms.
	Pending Tasks: 0
		Table: hints
		SSTable count: 0
		Space used (live), bytes: 0
		Space used (total), bytes: 0
		Off heap memory used (total), bytes: 0
		SSTable Compression Ratio: 0.0
		Number of keys (estimate): 0
		Memtable cell count: 0
		Memtable data size, bytes: 0
		Memtable switch count: 0
		Local read count: 0
		Local read latency: 0.000 ms
		Local write count: 0
		Local write latency: 0.000 ms
		Pending tasks: 0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used, bytes: 0
		Bloom filter off heap memory used, bytes: 0
		Index summary off heap memory used, bytes: 0
		Compression metadata off heap memory used, bytes: 0
		Compacted partition minimum bytes: 0
		Compacted partition maximum bytes: 0
		Compacted partition mean bytes: 0
		Average live cells per slice (last five minutes): 0.0
		Average tombstones per slice (last five minutes): 0.0

----------------
{code}

Below is what is shown after running nodetool gossipinfo
{code}
/<NODE1>
  generation:1428349617
  heartbeat:238170
  HOST_ID:<NODE1ID>
  RELEASE_VERSION:2.0.14
  DC:<DCNAME>
  RPC_ADDRESS:<NODE1IP>
  SCHEMA:132878b7-a33b-3ca3-b83d-3cacf7fc2138
  STATUS:NORMAL,-1399780091502863826
  RACK:rack1
  SEVERITY:0.0
  LOAD:2.93383711E8
  NET_VERSION:7
/<NODE2>
  generation:1428349784
  heartbeat:237665
  HOST_ID:<NODE2ID>
  RELEASE_VERSION:2.0.14
  DC:app3-profiledata
  RPC_ADDRESS:<NODE2>
  SCHEMA:132878b7-a33b-3ca3-b83d-3cacf7fc2138
  STATUS:NORMAL,-1019261967377984057
  RACK:rack1
  SEVERITY:0.0
  LOAD:2.93393487E8
  NET_VERSION:7
/<NODE3>
  generation:1428348889
  heartbeat:240384
  HOST_ID:<NODE3ID>
  RELEASE_VERSION:2.0.14
  DC:app3-profiledata
  RPC_ADDRESS:<NODE3IP>
  SCHEMA:132878b7-a33b-3ca3-b83d-3cacf7fc2138
  STATUS:NORMAL,-1060333141359417961
  RACK:rack1
  SEVERITY:0.0
  LOAD:2.9345286E8
  NET_VERSION:7
{code}
  
  
Below is cassandra.yaml
{code}
cluster_name: '<Cluster Name>'

num_tokens: 256

auto_bootstrap: true

hinted_handoff_enabled: true

max_hint_window_in_ms: 345600000

hinted_handoff_throttle_in_kb: 1024

max_hints_delivery_threads: 2

authenticator: AllowAllAuthenticator

authorizer: AllowAllAuthorizer

permissions_validity_in_ms: 2000

partitioner: org.apache.cassandra.dht.Murmur3Partitioner

data_file_directories:
    - /mnt/cassandra/data

commitlog_directory: /mnt/cassandra/commitlog

disk_failure_policy: stop

key_cache_size_in_mb:

key_cache_save_period: 14400

row_cache_size_in_mb: 0

row_cache_save_period: 0

saved_caches_directory: /mnt/cassandra/saved_caches

commitlog_sync: batch

commitlog_sync_batch_window_in_ms: 50

commitlog_segment_size_in_mb: 32

seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: ""<NODE1>,<NODE2>,<NODE3>""

concurrent_reads: 32

concurrent_writes: 32

memtable_total_space_in_mb: 512

memtable_flush_queue_size: 4

trickle_fsync: false

trickle_fsync_interval_in_kb: 10240

storage_port: 7000

ssl_storage_port: 7001

listen_address: <LOCALIP>

start_native_transport: true

native_transport_port: 9042

start_rpc: true

rpc_address: <LOCALIP>

rpc_port: 9160

rpc_keepalive: true

rpc_server_type: hsha

rpc_min_threads: 16

rpc_max_threads: 256

thrift_framed_transport_size_in_mb: 15

incremental_backups: false

snapshot_before_compaction: false

auto_snapshot: true

column_index_size_in_kb: 64

in_memory_compaction_limit_in_mb: 64

multithreaded_compaction: false

compaction_throughput_mb_per_sec: 128

compaction_preheat_key_cache: true

read_request_timeout_in_ms: 10000

range_request_timeout_in_ms: 10000

write_request_timeout_in_ms: 10000

truncate_request_timeout_in_ms: 60000

request_timeout_in_ms: 10000

cross_node_timeout: false

phi_convict_threshold: 12

endpoint_snitch: PropertyFileSnitch

dynamic_snitch_update_interval_in_ms: 100

dynamic_snitch_reset_interval_in_ms: 600000

dynamic_snitch_badness_threshold: 0.2

request_scheduler: org.apache.cassandra.scheduler.NoScheduler

index_interval: 512

server_encryption_options:
    internode_encryption: none
    keystore: conf/.keystore
    keystore_password: cassandra
    truststore: conf/.truststore
    truststore_password: cassandra

client_encryption_options:
    enabled: false
    keystore: conf/.keystore
    keystore_password: cassandra

internode_compression: all

inter_dc_tcp_nodelay: true
{code}

I have stopped upgrading my other cassandra clusters until cause for this behavior is found.

Please let me know if more information is needed."
CASSANDRA-9128,Flush system.IndexInfo after index state changed,"We don't force a flush of {{system.IndexInfo}} after updating it by marking an index as built. This may lead to indexes being unnecessarily rebuilt following a disorderly shutdown.

We also don't update it after an index is removed, but that's probably less of an issue as we do flush {{system.schema_columns}} after removing the index, so those won't get rebuilt."
CASSANDRA-9122,CassandraDaemon.java:167 NullPointerException,"Repeated occurrance of null pointer exception in system.log after upgrading from 2.1.2 to 2.1.3. System.log section below.

{code:java}
ERROR [CompactionExecutor:40] 2015-04-05 18:34:36,413 CassandraDaemon.java:167 - Exception in thread Thread[CompactionExecutor:40,1,main]
java.lang.NullPointerException: null
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:475) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:463) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:274) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1152) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_11]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_11]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_11]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_11]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_11]
INFO  [ScheduledTasks:1] 2015-04-05 18:34:38,392 ColumnFamilyStore.java:877 - Enqueuing flush of sstable_activity: 35890 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:30] 2015-04-05 18:34:38,401 Memtable.java:339 - Writing Memtable-sstable_activity@388081044(3726 serialized bytes, 1656 ops, 0%/0% of on/off-heap limit)
{code}"
CASSANDRA-9117,"LEAK DETECTED during repair, startup","When running the {{incremental_repair_test.TestIncRepair.multiple_repair_test}} dtest, the following error logs show up:

{noformat}
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,491 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@83f047e) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1631580268:Memory@[7f354800bdc0..7f354800bde8) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,493 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@50bc8f67) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@191552666:Memory@[7f354800ba90..7f354800bdb0) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,493 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7fd10877) to class org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1954741807:Memory@[7f3548101190..7f3548101194) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2015-04-03 15:48:25,494 Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@578550ac) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@1903393047:[[OffHeapBitSet]] was not released before the reference was garbage collected
{noformat}

The test is being run against trunk (commit {{1dff098e}}).  I've attached a DEBUG-level log from the test run."
CASSANDRA-9111,SSTables originated from the same incremental repair session have different repairedAt timestamps,"CASSANDRA-7168 optimizes QUORUM reads by skipping incrementally repaired SSTables on other replicas that were repaired on or before the maximum repairedAt timestamp of the coordinating replica's SSTables for the query partition.

One assumption of that optimization is that SSTables originated from the same repair session in different nodes will have the same repairedAt timestamp, since the objective is to skip reading SSTables originated in the same repair session (or before).

However, currently, each node timestamps independently SSTables originated from the same repair session, so they almost never have the same timestamp.

Steps to reproduce the problem:
{code}
ccm create test
ccm populate -n 3
ccm start
ccm node1 cqlsh;
{code}

{code:sql}
CREATE KEYSPACE foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
CREATE TABLE foo.bar ( key int, col int, PRIMARY KEY (key) ) ;
INSERT INTO foo.bar (key, col) VALUES (1, 1);
exit;
{code}

{code}
ccm node1 flush;
ccm node2 flush;
ccm node3 flush;

nodetool -h 127.0.0.1 -p 7100 repair -par -inc foo bar

[2015-04-02 21:56:07,726] Starting repair command #1, repairing 3 ranges for keyspace foo (parallelism=PARALLEL, full=false)
[2015-04-02 21:56:07,816] Repair session 3655b670-d99c-11e4-b250-9107aba35569 for range (3074457345618258602,-9223372036854775808] finished
[2015-04-02 21:56:07,816] Repair session 365a4a50-d99c-11e4-b250-9107aba35569 for range (-9223372036854775808,-3074457345618258603] finished
[2015-04-02 21:56:07,818] Repair session 365bf800-d99c-11e4-b250-9107aba35569 for range (-3074457345618258603,3074457345618258602] finished
[2015-04-02 21:56:07,995] Repair command #1 finished

sstablemetadata ~/.ccm/test/node1/data/foo/bar-377b5540d99d11e49cc09107aba35569/foo-bar-ka-1-Statistics.db ~/.ccm/test/node2/data/foo/bar-377b5540d99d11e49cc09107aba35569/foo-bar-ka-1-Statistics.db ~/.ccm/test/node3/data/foo/bar-377b5540d99d11e49cc09107aba35569/foo-bar-ka-1-Statistics.db | grep Repaired

Repaired at: 1428023050318
Repaired at: 1428023050322
Repaired at: 1428023050340
{code}"
CASSANDRA-9107,More accurate row count estimates,"Currently the estimated row count from cfstats is the sum of the number of rows in all the sstables. This becomes very inaccurate with wide rows or heavily updated datasets since the same partition would exist in many sstables.  In example:

{code}
create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

create TABLE wide (key text PRIMARY KEY , value text) WITH compaction = {'class': 'SizeTieredCompactionStrategy', 'min_threshold': 30, 
'max_threshold': 100} ;
-------------------------------

insert INTO wide (key, value) VALUES ('key', 'value');
// flush
// cfstats output: Number of keys (estimate): 1  (128 in older version from index)

insert INTO wide (key, value) VALUES ('key', 'value');
// flush
// cfstats output: Number of keys (estimate): 2  (256 in older version from index)

... etc
{code}

previously it used the index but it still did it per sstable and summed them up which became inaccurate as there are more sstables (just by much worse). With new versions of sstables we can merge the cardinalities to resolve this with a slight hit to accuracy in the case of every sstable having completely unique partitions.

Furthermore I think it would be pretty minimal effort to include the number of rows in the memtables to this count. We wont have the cardinality merging between memtables and sstables but I would consider that a relatively minor negative."
CASSANDRA-9077,Deleting an element from a List which is null throws a NPE,"I am seeing an NPE on the latest 2.1 branch with this sequence of deletes from a list - first delete the entire list, then attempt to delete one element.

I expected to see {{List index 0 out of bound, list has size 0}} but instead got an NPE.

{noformat}
./bin/cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.3-SNAPSHOT | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh> use frozen_collections ;
cqlsh:frozen_collections> DROP TABLE IF EXISTS t;
cqlsh:frozen_collections> CREATE TABLE t (id text PRIMARY KEY, l list<text>, s set<text>);
cqlsh:frozen_collections> INSERT INTO t (id, l, s) VALUES ('user', ['1'], {'1'});
cqlsh:frozen_collections>
cqlsh:frozen_collections> DELETE l FROM t WHERE id ='user';
cqlsh:frozen_collections> //INSERT INTO t (id, l) VALUES ('user', ['1']);
cqlsh:frozen_collections> DELETE l[0] FROM t WHERE id = 'user';
ServerError: <ErrorMessage code=0000 [Server error] message=""java.lang.NullPointerException"">
cqlsh:frozen_collections>
cqlsh:frozen_collections> DELETE s FROM t WHERE id ='user';
cqlsh:frozen_collections> DELETE s['1'] FROM t WHERE id = 'user';
{noformat}

It appears the {{DELETE emails...}} directly followed by {{DELETE emails[0]...}} is the offending sequence. Either one alone works fine, as does adding an intervening insert/update.

The same sequence performed on a Set rather than List works (as shown above).
"
CASSANDRA-9070,Race in cancelling compactions,"seems we might have a race situation when cancelling compactions

currently we do the following to ensure that we don't start any new compactions when we try to do markAllCompacting()

# pause compactions - this makes sure we don't create any new compaction tasks from the compaction strategies
# cancel any ongoing compactions - compactions register themselves with the CompactionMetrics and then, when cancelling we get all compactions here, and tell them to stop

Problem is that there is a window between when the CompactionTask is created and when it is registered in CompactionMetrics meaning with a bit of bad luck, we could have a situation like this:
# we finish a compaction and create a new CompactionTask from the compaction strategy
# we pause the compaction strategies to not create any new CompactionTasks
# we cancel all ongoing compactions
# The CompactionTask created in #1 above registers itself in CompactionMetrics and misses that it should be cancelled"
CASSANDRA-9060,Anticompaction hangs on bloom filter bitset serialization,"I tried running an incremental repair against a 15-node vnode-cluster with roughly 500GB data running on 2.1.3-SNAPSHOT, without performing the suggested migration steps. I manually chose a small range for the repair (using --start/end-token). The actual repair part took almost no time at all, but the anticompactions took a lot of time (not surprisingly).

Obviously, this might not be the ideal way to run incremental repairs, but I wanted to look into what made the whole process so slow. The results were rather surprising. The majority of the time was spent serializing bloom filters.

The reason seemed to be two-fold. First, the bloom-filters generated were huge (probably because the original SSTables were large). With a proper migration to incremental repairs, I'm guessing this would not happen. Secondly, however, the bloom filters were being written to the output one byte at a time (with quite a few type-conversions on the way) to transform the little-endian in-memory representation to the big-endian on-disk representation.

I have implemented a solution where big-endian is used in-memory as well as on-disk, which obviously makes de-/serialization much, much faster. This introduces some slight overhead when checking the bloom filter, but I can't see how that would be problematic. An obvious alternative would be to still perform the serialization/deserialization using a byte array, but perform the byte-order swap there."
CASSANDRA-9051,Error in cqlsh command line while querying,"Aggregation queries (select count(*) from TABLE_NAME ) on Cassandra cluster results in the following error. Even after increasing the read_request_timeout_in_ms and range_request_timeout_in_ms parameters. For more information on the bug. You can refer the this stack overflow link.

http://stackoverflow.com/questions/29205005/error-in-cqlsh-command-line-while-querying

 errors={}, last_host=localhost
  Statement trace did not complete within 10 seconds"
CASSANDRA-9031,nodetool info -T throws ArrayOutOfBounds when the node has not joined the cluster,"To reproduce, bring up a node that does not join the cluster, either using -Dcassandra.write_survey=true or -Dcassandra.join_ring=false, then run 'nodetool info -T'. You'll get the following stack trace:

{code}ID                     : e384209f-f7a9-4cff-8fd5-03adfaa0d846
Gossip active          : true
Thrift active          : true
Native Transport active: true
Load                   : 76.69 KB
Generation No          : 1427229938
Uptime (seconds)       : 728
Heap Memory (MB)       : 109.93 / 826.00
Off Heap Memory (MB)   : 0.01
Exception in thread ""main"" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:635)
	at java.util.ArrayList.get(ArrayList.java:411)
	at org.apache.cassandra.tools.NodeProbe.getEndpoint(NodeProbe.java:676)
	at org.apache.cassandra.tools.NodeProbe.getDataCenter(NodeProbe.java:694)
	at org.apache.cassandra.tools.NodeCmd.printInfo(NodeCmd.java:666)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1277){code}

After applying the attached patch, the new error is:
{code}ID                     : a7d76a2a-82d2-4faa-94e1-a30df6663ebb
Gossip active          : true
Thrift active          : false
Native Transport active: false
Load                   : 89.36 KB
Generation No          : 1427231804
Uptime (seconds)       : 12
Heap Memory (MB)       : 135.49 / 826.00
Off Heap Memory (MB)   : 0.01
Exception in thread ""main"" java.lang.RuntimeException: This node does not have any tokens. Perhaps it is not part of the ring?
	at org.apache.cassandra.tools.NodeProbe.getEndpoint(NodeProbe.java:678)
	at org.apache.cassandra.tools.NodeProbe.getDataCenter(NodeProbe.java:698)
	at org.apache.cassandra.tools.NodeCmd.printInfo(NodeCmd.java:676)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1313){code}"
CASSANDRA-9021,AssertionError and Leak detected during sstable compaction,"After ~3 hours of data ingestion we see assertion errors and 'LEAK DETECTED' errors during what looks like sstable compaction.


system.log snippets (full log attached):
{code}
...
INFO  [CompactionExecutor:12] 2015-03-23 02:45:51,770  CompactionTask.java:267 - Compacted 4 sstables to [/mnt/cass_data_disks/data1/requests_ks/timeline-       9500fe40d0f611e495675d5ea01541b5/requests_ks-timeline-ka-185,].  65,916,594 bytes to 66,159,512 (~100% of original) in 26,554ms = 2.376087MB/s.  983 total       partitions merged to 805.  Partition merge counts were {1:627, 2:178, }
INFO  [CompactionExecutor:11] 2015-03-23 02:45:51,837  CompactionTask.java:267 - Compacted 4 sstables to [/mnt/cass_data_disks/data1/system/                     compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-119,].  426 bytes to 42 (~9% of original) in 82ms = 0.000488MB/s.  5  total partitions merged to 1.  Partition merge counts were {1:1, 2:2, }
ERROR [NonPeriodicTasks:1] 2015-03-23 02:45:52,251  CassandraDaemon.java:167 - Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.AssertionError: null
 at org.apache.cassandra.io.compress.CompressionMetadata$Chunk.<init>(CompressionMetadata.java:438) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:228) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.dropPageCache(CompressedPoolingSegmentedFile.java:80) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.sstable.SSTableReader$6.run(SSTableReader.java:923) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at org.apache.cassandra.io.sstable.SSTableReader$InstanceTidier$1.run(SSTableReader.java:2036) ~[cassandra-all-2.1.3.304.jar:2.1.3.304]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_45]
 at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_45]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178) ~[na:1.7.0_45]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292) ~[na:1.7.0_45]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_45]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
 at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
...
INFO  [MemtableFlushWriter:50] 2015-03-23 02:47:29,465  Memtable.java:378 - Completed flushing /mnt/cass_data_disks/data1/requests_ks/timeline-                  9500fe40d0f611e495675d5ea01541b5/requests_ks-timeline-ka-188-Data.db (16311981 bytes) for commitlog position ReplayPosition(segmentId=1427071574495,             position=4523631)
ERROR [Reference-Reaper:1] 2015-03-23 02:47:33,987  Ref.java:181 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2f59b10) to class org.apache.cassandra.io.sstable.SSTableReader$DescriptorTypeTidy@1251424500:/mnt/cass_data_disks/data1/requests_ks/timeline-9500fe40d0f611e495675d5ea01541b5/    requests_ks-timeline-ka-149 was not released before the reference was garbage collected
INFO  [Service Thread] 2015-03-23 02:47:40,158  GCInspector.java:142 - ConcurrentMarkSweep GC in 12247ms.  CMS Old Gen: 5318987136 -> 457655168; CMS Perm Gen:   44731264 -> 44699160; Par Eden Space: 8597912 -> 418006664; Par Survivor Space: 71865728 -> 59679584
...
{code}
"
CASSANDRA-8925,broadcast_rpc_address NPEs while using rpc_interface ,"Somewhat amusingly, it looks like my NPE on startup is the result of a copy-paste error in:
{code}
[clockfort@clockfort cassandra]$ git log --stat 3e5edb82
commit 3e5edb82c73b7b7c6e1d1e970fb764c3e3158da6
Author: Ariel Weisberg <ariel.weisberg@datastax.com>
Date:   Tue Jan 27 13:30:47 2015 +0100

    rpc_interface and listen_interface generate NPE on startup when specified interface doesn't exist
    
    Patch by Ariel Weisberg; reviewed by Robert Stupp for CASSANDRA-8677

 src/java/org/apache/cassandra/config/DatabaseDescriptor.java | 46 +++++++++++++++++++++++-----------------------
 1 file changed, 23 insertions(+), 23 deletions(-)
{code}

The log looks like:
{code}
INFO  18:51:13 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  18:51:13 Global memtable on-heap threshold is enabled at 2008MB
INFO  18:51:13 Global memtable off-heap threshold is enabled at 2008MB
ERROR 18:51:13 Fatal error during configuration loading
java.lang.NullPointerException: null
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:411) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:110) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:554) [apache-cassandra-2.1.3.jar:2.1.3]
null
Fatal error during configuration loading; unable to start. See log for stacktrace.
{code}

This is with a cassandra.yaml config snippet of:
{code}
start_rpc: true
# rpc_address: localhost
rpc_interface: eth0
# RPC address to broadcast to drivers and other Cassandra nodes. This cannot
# be set to 0.0.0.0. If left blank, this will be set to the value of
# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must
# be set.
# broadcast_rpc_address: 1.2.3.4
{code}

"
CASSANDRA-8853,adding existing table at node startup,"I get intermittent failures running [putget_test.TestPutGet|https://github.com/riptano/cassandra-dtest/blob/master/putget_test.py#L11] on trunk. The core of the failure is

{code}
Cannot add already existing table ""resource_role_permissons_index"" to keyspace ""system_auth""
{code}

I'll put in some time today seeing if it fails on previous versions.

Here are two gists with the stdout and stderr from failing runs:

https://gist.github.com/mambocab/b724a2c697416f21a621
https://gist.github.com/mambocab/adb5cb90c14cda5f87c8

Each of those were in an Ubuntu VM running under VirtualBox with 2 GB memory. Here's a third that reproduced with 4GB:

https://gist.github.com/mambocab/02ffa977eae2b5c3432b

and here are the same for a successful run:

https://gist.github.com/mambocab/de2a089e93bc4dff61cc

There's some noise about reading JMX metrics in the Java stack traces that can be ignored for this issue. This is in the traces for both failing runs, and not in the trace for the successful one:

{code}
java.lang.AssertionError: org.apache.cassandra.exceptions.AlreadyExistsException: Cannot add already existing table ""resource_role_permissons_index"" to keyspace ""system_auth""
    at org.apache.cassandra.service.StorageService.doAuthSetup(StorageService.java:897)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:832)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:579)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:469)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:357)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:492)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:599)
Caused by: org.apache.cassandra.exceptions.AlreadyExistsException: Cannot add already existing table ""resource_role_permissons_index"" to keyspace ""system_auth""
    at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:286)
    at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:275)
    at org.apache.cassandra.service.StorageService.doAuthSetup(StorageService.java:891)
    ... 6 more
{code}

The test command is

{code}
CASSANDRA_DIR=~/cstar_src/cassandra PRINT_DEBUG=true nosetests -x -s -v putget_test:TestPutGet >~/putget_test.stdout 2>~/putget_test.stderr
{code}

I'm running in Ubuntu under VirtualBox, which may be the problem:

{code}
$ uname -a
Linux dtest-VirtualBox 3.13.0-45-generic #74-Ubuntu SMP Tue Jan 13 19:36:28 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
{code}

dtest discussion [here|https://github.com/riptano/cassandra-dtest/issues/170]."
CASSANDRA-8851,"Uncaught exception on thread Thread[SharedPool-Worker-16,5,main] after upgrade to 2.1.3","Hi there,

after upgrading to 2.1.3 we've got the following error every few seconds:
{code}
WARN  [SharedPool-Worker-16] 2015-02-23 10:20:36,392 AbstractTracingAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-16,5,main]: {}
java.lang.AssertionError: null
	at org.apache.cassandra.io.util.Memory.size(Memory.java:307) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.utils.obs.OffHeapBitSet.capacity(OffHeapBitSet.java:61) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.utils.BloomFilter.indexes(BloomFilter.java:74) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.utils.BloomFilter.isPresent(BloomFilter.java:98) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:1366) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:1350) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:41) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:185) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:273) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1915) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1748) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:342) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:57) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:62) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_45]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) ~[apache-cassandra-2.1.3.jar:2.1.3]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-2.1.3.jar:2.1.3]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
{code}

This seems to crash the compactions and pushes up server load and piles up compactions.

Any idea / possible workaround?

Best,

Tobias
"
CASSANDRA-8842,Upgrade java-driver used for stress,"There are a number of java-driver issues I've been hitting while using stress on large clusters.  These issues are fixed in the later driver releases. Mainly race conditions.

https://github.com/datastax/java-driver/blob/2.0/driver-core/CHANGELOG.rst#2010"
CASSANDRA-8839,DatabaseDescriptor throws NPE when rpc_interface is used,"Copy from mail to dev mailinglist. 

When using

- listen_interface instead of listen_address
- rpc_interface instead of rpc_address

starting 2.1.3 throws an NPE:

{code}
ERROR [main] 2015-02-20 07:50:09,661 DatabaseDescriptor.java:144 - Fatal error during configuration loading
java.lang.NullPointerException: null
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:411) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:110) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465) [apache-cassandra-2.1.3.jar:2.1.3]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:554) [apache-cassandra-2.1.3.jar:2.1.3]
{code}

Occurs on debian package as well as in tar.gz distribution. 

{code}
/* Local IP, hostname or interface to bind RPC server to */
if(conf.rpc_address !=null&& conf.rpc_interface !=null)
{
    throw newConfigurationException(""Set rpc_address OR rpc_interface, not both"");
}
else if(conf.rpc_address !=null)
{
    try
    {
        rpcAddress = InetAddress.getByName(conf.rpc_address);
    }
    catch(UnknownHostException e)
    {
        throw newConfigurationException(""Unknown host in rpc_address ""+ conf.rpc_address);
    }
}
else if(conf.rpc_interface !=null)
{
    listenAddress = getNetworkInterfaceAddress(conf.rpc_interface,""rpc_interface"");
}
else
{
    rpcAddress = FBUtilities.getLocalAddress();
}
{code}

I think that listenAddress in the second else block is an error. In my case rpc_interface is eth0, so listenAddress gets set, and rpcAddress remains unset. The result is NPE in line 411:

{code}
if(rpcAddress.isAnyLocalAddress())
{code}

After changing rpc_interface to rpc_address everything works as expected.

"
CASSANDRA-8815,Race in sstable ref counting during streaming failures,"We have a seen a machine in Prod whose all read threads are blocked(spinning) on trying to acquire the reference lock on stables. There are also some stream sessions which are doing the same. 
On looking at the heap dump, we could see that a live sstable which is part of the View has a ref count = 0. This sstable is also not compacting or is part of any failed compaction. 

On looking through the code, we could see that if ref goes to zero and the stable is part of the View, all reader threads will spin forever. 

On further looking through the code of streaming, we could see that if StreamTransferTask.complete is called after closeSession has been called due to error in OutgoingMessageHandler, it will double decrement the ref count of an sstable. 

This race can happen and we see through exception in logs that closeSession was triggered by OutgoingMessageHandler. 

The fix for this is very simple i think. In StreamTransferTask.abort, we can remove a file from ""files” before decrementing the ref count. This will avoid this race. "
CASSANDRA-8812,JVM Crashes on Windows x86,"Under Windows (32 or 64 bit) with the 32-bit Oracle JDK, the JVM may crash due to EXCEPTION_ACCESS_VIOLATION. This happens inconsistently. The attached test project can recreate the crash - sometimes it works successfully, sometimes there's a Java exception in the log, and sometimes the hotspot JVM crash shows up (regardless of whether the JUnit test results in success - you can ignore that). Run it a bunch of times to see the various outcomes. It also contains a sample hotspot error log.

Note that both when the Java exception is thrown and when the JVM crashes, the stack trace is almost the same - they both eventually occur when the PERIODIC-COMMIT-LOG-SYNCER thread calls CommitLogSegment.sync and accesses the buffer (MappedByteBuffer): if it happens to be in buffer.force(), then the Java exception is thrown, and if it's in one of the buffer.put() calls before it, then the JVM crashes. This possibly exposes a JVM bug as well in this case. So it basically looks like a race condition which results in the buffer sometimes being used after it is no longer valid.

I recreated this on a PC with Windows 7 64-bit running the 32-bit Oracle JDK, as well as on a modern.ie virtualbox image of Windows 7 32-bit running the JDK, and it happens both with JDK 7 and JDK 8. Also defining an explicit dependency on cassandra 2.1.2 (as opposed to the cassandra-unit dependency on 2.1.0) doesn't make a difference. At some point in my testing I've also seen a Java-level exception on Linux, but I can't recreate it at the moment with this test project, so I can't guarantee it.
"
CASSANDRA-8802,Leaked reference on windows,"The dtest counter_tests.py:TestCounters.upgrade_test is failing on Windows with the following error:
{code}
ERROR [Reference-Reaper:1] 2015-02-13 11:06:17,802 Ref.java:167 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2d0bdc9) to class org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$1@669450811:[OffHeapBitSet] was not released before the reference was garbage collected
{code}

This exception is not occurring on Linux or OSX. The test is also erroring from CASSANDRA-8535.

/cc [~benedict] [~JoshuaMcKenzie]"
CASSANDRA-8800,Cassandra fails to start with OOM after disk space problem occurred,"It one point the Cassandra failed with 
org.apache.cassandra.io.FSWriteError: java.io.IOException: No space left on device

Which seems pretty normal situation. But after the disk space was increased, Cassandra refused to start up again. The memory snapshot shows a big ArrayList of org.apache.cassandra.io.sstable.SSTableScanner is allocated, holding around 2.8Gb.
It looks like the DB is corrupted, but we're not entirely sure.
"
CASSANDRA-8796,'nodetool info' prints exception against older node,"{{nodetool info}} from current 2.1 branch (2.1.3) issued against a 2.1.2 node prints the following:

{noformat}
bin/nodetool info
objc[57382]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined.
ID                     : 28b5c1dd-1822-412a-aa16-11f35438e464
Gossip active          : true
Thrift active          : true
Native Transport active: true
Load                   : 41,1 KB
Generation No          : 1423767989
Uptime (seconds)       : 258
Heap Memory (MB)       : 183,07 / 4016,00
error: org.apache.cassandra.metrics:type=ColumnFamily,keyspace=system,scope=IndexInfo,name=BloomFilterOffHeapMemoryUsed
-- StackTrace --
javax.management.InstanceNotFoundException: org.apache.cassandra.metrics:type=ColumnFamily,keyspace=system,scope=IndexInfo,name=BloomFilterOffHeapMemoryUsed
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:643)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$2.run(Transport.java:202)
	at sun.rmi.transport.Transport$2.run(Transport.java:199)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:198)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:567)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:828)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.access$400(TCPTransport.java:619)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$1.run(TCPTransport.java:684)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler$1.run(TCPTransport.java:681)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:681)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
	at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:276)
	at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:253)
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:162)
	at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
	at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
	at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:906)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:267)
	at com.sun.proxy.$Proxy19.getValue(Unknown Source)
	at org.apache.cassandra.tools.NodeProbe.getColumnFamilyMetric(NodeProbe.java:1047)
	at org.apache.cassandra.tools.NodeTool$Info.getOffHeapMemoryUsed(NodeTool.java:443)
	at org.apache.cassandra.tools.NodeTool$Info.execute(NodeTool.java:373)
	at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:250)
	at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:164)
{noformat}

From 2.1.2 it looks like:
{noformat}
bin/nodetool info
objc[57525]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined.
ID               : 28b5c1dd-1822-412a-aa16-11f35438e464
Gossip active    : true
Thrift active    : true
Native Transport active: true
Load             : 41,1 KB
Generation No    : 1423767989
Uptime (seconds) : 292
Heap Memory (MB) : 186,72 / 4016,00
Data Center      : datacenter1
Rack             : rack1
Exceptions       : 0
Key Cache        : entries 8, size 600 bytes, capacity 100 MB, 14 hits, 21 requests, 0,667 recent hit rate, 14400 save period in seconds
Row Cache        : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache    : entries 0, size 0 bytes, capacity 50 MB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Token            : (invoke with -T/--tokens to see all 256 tokens)
{noformat}

I.e. cache and token information is missing.
"
CASSANDRA-8792,Improve Memory assertions,Null pointers are valid returns if a size of zero is returned. We assume a null pointer implies resource mismanagement in a few places. We also don't properly check the bounds of all of our accesses; this patch attempts to tidy up both of these things.
CASSANDRA-8771,Remove commit log segment recycling,"For discussion

Commit log segment recycling introduces a lot of complexity in the existing code.

CASSANDRA-8729 is a side effect of commit log segment recycling and addressing it will require memory management code and thread coordination for memory that the filesystem will no longer handle for us.

There is some discussion about what storage configurations actually benefit from preallocated files. Fast random access devices like SSDs, or non-volatile write caches etc. make the distinction not that great. 

I haven't measured any difference in throughput for bulk appending vs overwriting although it was pointed out that I didn't test with concurrent IO streams.

What would it take to make removing commit log segment recycling acceptable? Maybe a benchmark on a spinning disk that measures the performance impact of preallocation when there are other IO streams?"
CASSANDRA-8758,CompressionMetadata.Writer should use a safer version of RefCountedMemory,Another small follow-on from CASSANDRA-8707 to finish off the memory and resource safety improvements.
CASSANDRA-8749,Cleanup SegmentedFile,"As a follow up to 8707 (building upon it for ease, since that edits these files), and a precursor to another follow up, this ticket cleans up the SegmentedFile hierarchy a little, and makes it encapsulate the construction of a new reader, so we implementation details don't leak into SSTableReader."
CASSANDRA-8748,Backport memory leak fix from CASSANDRA-8707 to 2.0,There are multiple elements in CASSANDRA-8707 but the memory leak is common to Cassandra 2.0.  This ticket is to fix the memory leak specifically for 2.0.
CASSANDRA-8747,Make SSTableWriter.openEarly behaviour more robust,"Currently openEarly does some fairly ugly looping back over the summary data we've collected looking for one we think should be fully covered in the Index and Data files, and that should have a safe boundary between it and the end of an IndexSummary entry so that when scanning across it we should not accidentally read an incomplete key. The approach taken is a little difficult to reason about though, and be confident of, and I now realise is also very subtly broken. Since we're cleaning up the behaviour around this code, it seemed worthwhile to improve its clarity and make its behaviour easier to reason about. The current behaviour can be characterised as:

# Take the current Index file length
# Find the IndexSummary boundary key (first key in an interval) that starts past this position
# Take the IndexSummary boundary key (first key) for the preceding interval as our initial boundary
# Construct a reader with this boundary
# Lookup our last key in the reader, and if its end position is past the end of the data file, take the prior summary boundary. Repeat until we find one starting before the end.

The bug may well be very hard to exhibit, or even impossible, but is that if we have a single very large partition followed by 127 very tiny partitions (or whatever the IndexSummary interval is configured as), our IndexSummary interval buffer may not guarantee the record we have selected as our end is fully readable.

The new approach is to track in the IndexSummary the safe and optimal boundary point (i.e. the last record in each summary interval) and its bounds in the index and data files. On flushing either file, we notify the summary builder to the new flush points, and it consults its map of these and selects the last such boundary that can safely be read in both. This is much easier to understand, and has no such subtle risk."
CASSANDRA-8741,Running a drain before a decommission apparently the wrong thing to do,"This might simply be a documentation issue. It appears that running ""nodetool drain"" is a very wrong thing to do before running a ""nodetool decommission"".

The idea was that I was going to safely shut off writes and flush everything to disk before beginning the decommission. What happens is the ""decommission"" call appears to fail very early on after starting, and afterwards, the node in question is stuck in state LEAVING, but all other nodes in the ring see that node as NORMAL, but down. No streams are ever sent from the node being decommissioned to other nodes.

The drain command does indeed shut down the ""BatchlogTasks"" executor (org/apache/cassandra/service/StorageService.java, line 3445 in git tag ""cassandra-2.0.11"") but the decommission process tries using that executor when calling the ""startBatchlogReplay"" function (org/apache/cassandra/db/BatchlogManager.java, line 123) called through org.apache.cassandra.service.StorageService.unbootstrap (see the stack trace pasted below).

This also failed in a similar way on Cassandra 1.2.13-ish (DSE 3.2.4).

So, either something is wrong with the drain/decommission commands, or it's very wrong to run a drain before a decommission. What's worse, there seems to be no way to recover this node once it is in this state; you need to shut it down and run ""removenode"".

My terminal output:
{code}
ubuntu@x:~$ nodetool drain
ubuntu@x:~$ tail /var/log/^C
ubuntu@x:~$ nodetool decommission
Exception in thread ""main"" java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@3008fa33 rejected from org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor@1d6242e8[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 52]
        at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
        at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)
        at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)
        at java.util.concurrent.ScheduledThreadPoolExecutor.submit(ScheduledThreadPoolExecutor.java:629)
        at org.apache.cassandra.db.BatchlogManager.startBatchlogReplay(BatchlogManager.java:123)
        at org.apache.cassandra.service.StorageService.unbootstrap(StorageService.java:2966)
        at org.apache.cassandra.service.StorageService.decommission(StorageService.java:2934)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745){code}"
CASSANDRA-8729,Commitlog causes read before write when overwriting,"The memory mapped commit log implementation writes directly to the page cache. If a page is not in the cache the kernel will read it in even though we are going to overwrite.

The way to avoid this is to write to private memory, and then pad the write with 0s at the end so it is page (4k) aligned before writing to a file.

The commit log would benefit from being refactored into something that looks more like a pipeline with incoming requests receiving private memory to write in, completed buffers being submitted to a  parallelized compression/checksum step, followed by submission to another thread for writing to a file that preserves the order."
CASSANDRA-8719,Using thrift HSHA with offheap_objects appears to corrupt data,"Copying my comment from CASSANDRA-6285 to a new issue since that issue is long closed and I'm not sure if they are related...

I am getting this exception using Thrift HSHA in 2.1.0:

{quote}
 INFO [CompactionExecutor:8] 2015-01-26 13:32:51,818 CompactionTask.java (line 138) Compacting [SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db'), SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db')]
 INFO [CompactionExecutor:8] 2015-01-26 13:32:51,890 ColumnFamilyStore.java (line 856) Enqueuing flush of compactions_in_progress: 212 (0%) on-heap, 20 (0%) off-heap
 INFO [MemtableFlushWriter:8] 2015-01-26 13:32:51,892 Memtable.java (line 326) Writing Memtable-compactions_in_progress@1155018639(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
 INFO [MemtableFlushWriter:8] 2015-01-26 13:32:51,896 Memtable.java (line 360) Completed flushing /tmp/cass_test/cassandra/TestCassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-2-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1422296630707, position=430226)
ERROR [CompactionExecutor:8] 2015-01-26 13:32:51,906 CassandraDaemon.java (line 166) Exception in thread Thread[CompactionExecutor:8,1,RMI Runtime]
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) >= current key DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-3-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_40]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
        at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
{quote}

I don't think it's caused by CASSANDRA-8211, because it happens during the first compaction that takes place between the first 2 SSTables to get flushed from an initially empty column family.

Also, I've only been able to reproduce it when using both *hsha* for the rpc server and *offheap_objects* for memtable allocation. If I switch either to sync or to offheap_buffers or heap_buffers then I cannot reproduce the problem. Also under the same circumstances I'm pretty sure I've seen incorrect data being returned to a client multiget_slice request before any SSTables had been flushed yet, so I presume this is corruption that happens before any flush/compaction takes place.

nodetool scrub yielded these errors:

{quote}
 INFO [CompactionExecutor:9] 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db') (168780 bytes)
 INFO [CompactionExecutor:10] 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db') (135024 bytes)
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,531 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,534 OutputHandler.java (line 57) Error reading row (stacktrace follows):
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) >= current key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-4-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.tryAppend(SSTableRewriter.java:141) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:186) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:592) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:100) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$3.execute(CompactionManager.java:315) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:270) [apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
        at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Row starting at position 25342 is unreadable; skipping to next
 WARN [CompactionExecutor:10] 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(29459452031265566667651334397450214244, 726f776b65793030355f31343232323936393033323837) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))

etc...
{quote}

I've able to reliably reproduce by creating a new empty table, writing a few partitions containing a few rows each, forcing a flush using nodetool, writing some more, and flushing again, and letting the first compaction happen (using LCS). If I read before flushing, sometimes I will get zero rows back (with quorum between writes and reads), or corrupt bytes, though sometimes reading would yield the correct results. I haven't been able to narrow down yet the cases when the data comes back corrupt vs not, though in the same test case (one set of of writes) the results of a read appear to be consistent (successive reads are either all correct or all incorrect).
"
CASSANDRA-8716,"""java.util.concurrent.ExecutionException: java.lang.AssertionError: Memory was freed"" when running cleanup","{code}Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.AssertionError: Memory was freed
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:234)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:272)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:1115)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(StorageService.java:2177)
        at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
        at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
        at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:259)
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:211)
        at org.apache.cassandra.io.sstable.IndexSummary.getIndex(IndexSummary.java:79)
        at org.apache.cassandra.io.sstable.IndexSummary.getKey(IndexSummary.java:84)
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:58)
        at org.apache.cassandra.io.sstable.SSTableReader.getIndexScanPosition(SSTableReader.java:602)
        at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:947)
        at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:910)
        at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:819)
        at org.apache.cassandra.db.ColumnFamilyStore.getExpectedCompactedFileSize(ColumnFamilyStore.java:1088)
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:564)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:63)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:281)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:225)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        ... 3 more{code}


The error continue to happen even after scrub.
If I retry a few time, it may run successfully.
I am attaching the server log.
"
CASSANDRA-8709,Convert SequentialWriter from using RandomAccessFile to nio channel,"For non-mmap'ed I/O on Windows, using nio channels will give us substantially more flexibility w/regards to renaming and moving files around while writing them.  This change in conjunction with CASSANDRA-4050 should allow us to remove the Windows bypass code in SSTableRewriter for non-memory-mapped I/O.

In general, migrating from instances of RandomAccessFile to nio channels will help make Windows and linux behavior more consistent."
CASSANDRA-8689,Assertion error in 2.1.2: ERROR [IndexSummaryManager:1],"After upgrading a 6 nodes cassandra from 2.1.0 to 2.1.2, start getting the following assertion error.

{noformat}
ERROR [IndexSummaryManager:1] 2015-01-26 20:55:40,451 CassandraDaemon.java:153 - Exception in thread Thread[IndexSummaryManager:1,1,main]
java.lang.AssertionError: null
        at org.apache.cassandra.io.util.Memory.size(Memory.java:307) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.sstable.IndexSummary.getOffHeapSize(IndexSummary.java:192) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.sstable.SSTableReader.getIndexSummaryOffHeapSize(SSTableReader.java:1070) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(IndexSummaryManager.java:292) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(IndexSummaryManager.java:238) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.io.sstable.IndexSummaryManager$1.runMayThrow(IndexSummaryManager.java:139) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:77) ~[apache-cassandra-2.1.2.jar:2.1.2]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_45]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) [na:1.7.0_45]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) [na:1.7.0_45]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.7.0_45]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
{noformat}

cassandra service is still running despite the issue. Node has total 8G memory with 2G allocated to heap. We are basically running read queries to retrieve data out of cassandra.


"
CASSANDRA-8670,Large columns + NIO memory pooling causes excessive direct memory usage,"If you provide a large byte array to NIO and ask it to populate the byte array from a socket it will allocate a thread local byte buffer that is the size of the requested read no matter how large it is. Old IO wraps new IO for sockets (but not files) so old IO is effected as well.

Even If you are using Buffered{Input | Output}Stream you can end up passing a large byte array to NIO. The byte array read method will pass the array to NIO directly if it is larger than the internal buffer.  

Passing large cells between nodes as part of intra-cluster messaging can cause the NIO pooled buffers to quickly reach a high watermark and stay there. This ends up costing 2x the largest cell size because there is a buffer for input and output since they are different threads. This is further multiplied by the number of nodes in the cluster - 1 since each has a dedicated thread pair with separate thread locals.

Anecdotally it appears that the cost is doubled beyond that although it isn't clear why. Possibly the control connections or possibly there is some way in which multiple 

Need a workload in CI that tests the advertised limits of cells on a cluster. It would be reasonable to ratchet down the max direct memory for the test to trigger failures if a memory pooling issue is introduced. I don't think we need to test concurrently pulling in a lot of them, but it should at least work serially.

The obvious fix to address this issue would be to read in smaller chunks when dealing with large values. I think small should still be relatively large (4 megabytes) so that code that is reading from a disk can amortize the cost of a seek. It can be hard to tell what the underlying thing being read from is going to be in some of the contexts where we might choose to implement switching to reading chunks."
CASSANDRA-8641,Repair causes a large number of tiny SSTables,"I have a 3 nodes cluster with RF = 3, quad core and 32 GB or RAM. I am running 2.1.2 with all the default settings. I'm seeing some strange behaviors during incremental repair (under write load).

Taking the example of one particular column family, before running an incremental repair, I have about 13 SSTables. After finishing the incremental repair, I have over 114000 SSTables.

{noformat}
Table: customers
SSTable count: 114688
Space used (live): 97203707290
Space used (total): 99175455072
Space used by snapshots (total): 0
SSTable Compression Ratio: 0.28281112416526505
Memtable cell count: 0
Memtable data size: 0
Memtable switch count: 1069
Local read count: 0
Local read latency: NaN ms
Local write count: 11548705
Local write latency: 0.030 ms
Pending flushes: 0
Bloom filter false positives: 0
Bloom filter false ratio: 0.00000
Bloom filter space used: 144145152
Compacted partition minimum bytes: 311
Compacted partition maximum bytes: 1996099046
Compacted partition mean bytes: 3419
Average live cells per slice (last five minutes): 0.0
Maximum live cells per slice (last five minutes): 0.0
Average tombstones per slice (last five minutes): 0.0
Maximum tombstones per slice (last five minutes): 0.0
{noformat}

Looking at the logs during the repair, it seems Cassandra is struggling to compact minuscule memtables (often just a few kilobytes):

{noformat}
INFO  [CompactionExecutor:337] 2015-01-17 01:44:27,011 CompactionTask.java:251 - Compacted 32 sstables to [/mnt/data/cassandra/data/business/customers-d9d42d209ccc11e48ca54553c90a9d45/business-customers-ka-228341,].  8,332 bytes to 6,547 (~78% of original) in 80,476ms = 0.000078MB/s.  32 total partitions merged to 32.  Partition merge counts were {1:32, }
INFO  [CompactionExecutor:337] 2015-01-17 01:45:35,519 CompactionTask.java:251 - Compacted 32 sstables to [/mnt/data/cassandra/data/business/customers-d9d42d209ccc11e48ca54553c90a9d45/business-customers-ka-229348,].  8,384 bytes to 6,563 (~78% of original) in 6,880ms = 0.000910MB/s.  32 total partitions merged to 32.  Partition merge counts were {1:32, }
INFO  [CompactionExecutor:339] 2015-01-17 01:47:46,475 CompactionTask.java:251 - Compacted 32 sstables to [/mnt/data/cassandra/data/business/customers-d9d42d209ccc11e48ca54553c90a9d45/business-customers-ka-229351,].  8,423 bytes to 6,401 (~75% of original) in 10,416ms = 0.000586MB/s.  32 total partitions merged to 32.  Partition merge counts were {1:32, }
{noformat}
 
Here is an excerpt of the system logs showing the abnormal flushing:

{noformat}
INFO  [AntiEntropyStage:1] 2015-01-17 15:28:43,807 ColumnFamilyStore.java:840 - Enqueuing flush of customers: 634484 (0%) on-heap, 2599489 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:06,823 ColumnFamilyStore.java:840 - Enqueuing flush of levels: 129504 (0%) on-heap, 222168 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:07,940 ColumnFamilyStore.java:840 - Enqueuing flush of chain: 4508 (0%) on-heap, 6880 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:08,124 ColumnFamilyStore.java:840 - Enqueuing flush of invoices: 1469772 (0%) on-heap, 2542675 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:09,471 ColumnFamilyStore.java:840 - Enqueuing flush of customers: 809844 (0%) on-heap, 3364728 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:24,368 ColumnFamilyStore.java:840 - Enqueuing flush of levels: 28212 (0%) on-heap, 44220 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:24,822 ColumnFamilyStore.java:840 - Enqueuing flush of chain: 860 (0%) on-heap, 1130 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:24,985 ColumnFamilyStore.java:840 - Enqueuing flush of invoices: 334480 (0%) on-heap, 568959 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:27,375 ColumnFamilyStore.java:840 - Enqueuing flush of customers: 221568 (0%) on-heap, 929962 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:35,755 ColumnFamilyStore.java:840 - Enqueuing flush of invoices: 7916 (0%) on-heap, 11080 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:36,239 ColumnFamilyStore.java:840 - Enqueuing flush of customers: 9968 (0%) on-heap, 33041 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:37,935 ColumnFamilyStore.java:840 - Enqueuing flush of invoices: 42108 (0%) on-heap, 69494 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:41,182 ColumnFamilyStore.java:840 - Enqueuing flush of customers: 40936 (0%) on-heap, 159099 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:49,573 ColumnFamilyStore.java:840 - Enqueuing flush of levels: 17236 (0%) on-heap, 27048 (0%) off-heap
INFO  [AntiEntropyStage:1] 2015-01-17 15:29:50,440 ColumnFamilyStore.java:840 - Enqueuing flush of chain: 548 (0%) on-heap, 630 (0%) off-heap
{noformat}

At the end of the repair, the cluster has become unusable."
CASSANDRA-8603,Cut tombstone memory footprint in half for cql deletes,"As CQL does not yet support range deletes every delete from CQL results in a ""Semi-RangeTombstone"" which actually has the same start and end values - but until today they are copies. Effectively doubling the required heap memory to store the RangeTombstone.

"
CASSANDRA-8559,OOM caused by large tombstone warning.,"When running with high amount of tombstones the error message generation from CASSANDRA-6117 can lead to out of memory situation with the default setting.

Attached a heapdump viewed in visualvm showing how this construct created two 777mb strings to print the error message for a read query and then crashed OOM.

{code}
        if (respectTombstoneThresholds() && columnCounter.ignored() > DatabaseDescriptor.getTombstoneWarnThreshold())
        {
            StringBuilder sb = new StringBuilder();
            CellNameType type = container.metadata().comparator;
            for (ColumnSlice sl : slices)
            {
                assert sl != null;

                sb.append('[');
                sb.append(type.getString(sl.start));
                sb.append('-');
                sb.append(type.getString(sl.finish));
                sb.append(']');
            }

            logger.warn(""Read {} live and {} tombstoned cells in {}.{} (see tombstone_warn_threshold). {} columns was requested, slices={}, delInfo={}"",
                        columnCounter.live(), columnCounter.ignored(), container.metadata().ksName, container.metadata().cfName, count, sb, container.deletionInfo());
        }
{code}

"
CASSANDRA-8546,RangeTombstoneList becoming bottleneck on tombstone heavy tasks,"I would like to propose a change of the data structure used in the RangeTombstoneList to store and insert tombstone ranges to something with at least O(log N) insert in the middle and at near O(1) and start AND end. Here is why:

When having tombstone heavy work-loads the current implementation of RangeTombstoneList becomes a bottleneck with slice queries.

Scanning the number of tombstones up to the default maximum (100k) can take up to 3 minutes of how addInternal() scales on insertion of middle and start elements.

The attached test shows that with 50k deletes from both sides of a range.

INSERT 1...110000
flush()
DELETE 1...50000
DELETE 110000...60000

While one direction performs ok (~400ms on my notebook):
{code}
SELECT * FROM timeseries WHERE name = 'a' ORDER BY timestamp DESC LIMIT 1
{code}

The other direction underperforms (~7seconds on my notebook)
{code}
SELECT * FROM timeseries WHERE name = 'a' ORDER BY timestamp ASC LIMIT 1
{code}
"
CASSANDRA-8499,Ensure SSTableWriter cleans up properly after failure,"In 2.0 we do not free a bloom filter, in 2.1 we do not free a small piece of offheap memory for writing compression metadata. In both we attempt to flush the BF despite having encountered an exception, making the exception slow to propagate."
CASSANDRA-8476,RE in writeSortedContents or replaceFlushed blocks compaction threads indefinitely.,"Encountered this problem while generating some test data, incremental backup failed to create hard-links to some of the of the system files (which is done at the end of each compaction):

Example of the RE stacktrace:
{noformat}
14/12/12 15:47:47 ERROR cassandra.SchemaLoader: Fatal exception in thread Thread[FlushWriter:5,5,main]
java.lang.RuntimeException: Tried to create duplicate hard link to <path>/cassandra/data/system/IndexInfo/backups/system-IndexInfo-jb-1-Index.db
	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:75)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1222)
	at org.apache.cassandra.db.DataTracker.maybeIncrementallyBackup(DataTracker.java:189)
	at org.apache.cassandra.db.DataTracker.replaceFlushed(DataTracker.java:166)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.replaceFlushed(AbstractCompactionStrategy.java:231)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceFlushed(ColumnFamilyStore.java:1141)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:343)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
14
{noformat}

jstack shows that CompactionExecutor threads are now blocked waiting on the flush future which will actually never decrement a latch.

{noformat}
""CompactionExecutor:125"" daemon prio=5 tid=0x00007fb3a10da800 nid=0x13c43 waiting on condition [0x000000012a900000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000071b669088> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:425)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:409)
	at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:457)
	at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:203)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

""CompactionExecutor:124"" daemon prio=5 tid=0x00007fb35cc09800 nid=0x13a2b waiting on condition [0x000000012934f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000007ce4bf918> (a java.util.concurrent.FutureTask)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:425)
	at java.util.concurrent.FutureTask.get(FutureTask.java:187)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:409)
	at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:457)
	at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:203)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}"
CASSANDRA-8463,Constant compaction under LCS,"It appears that tables configured with LCS will completely re-compact themselves over some period of time after upgrading from 2.0 to 2.1 (2.0.11 -> 2.1.2, specifically). It starts out with <10 pending tasks for an hour or so, then starts building up, now with 50-100 tasks pending across the cluster after 12 hours. These nodes are under heavy write load, but were easily able to keep up in 2.0 (they rarely had >5 pending compaction tasks), so I don't think it's LCS in 2.1 actually being worse, just perhaps some different LCS behavior that causes the layout of tables from 2.0 to prompt the compactor to reorganize them?

The nodes flushed ~11MB SSTables under 2.0. They're currently flushing ~36MB SSTables due to the improved memtable setup in 2.1. Before I upgraded the entire cluster to 2.1, I noticed the problem and tried several variations on the flush size, thinking perhaps the larger tables in L0 were causing some kind of cascading compactions. Even if they're sized roughly like the 2.0 flushes were, same behavior occurs. I also tried both enabling & disabling STCS in L0 with no real change other than L0 began to back up faster, so I left the STCS in L0 enabled.

Tables are configured with 32MB sstable_size_in_mb, which was found to be an improvement on the 160MB table size for compaction performance. Maybe this is wrong now? Otherwise, the tables are configured with defaults. Compaction has been unthrottled to help them catch-up. The compaction threads stay very busy, with the cluster-wide CPU at 45% ""nice"" time. No nodes have completely caught up yet. I'll update JIRA with status about their progress if anything interesting happens.

From a node around 12 hours ago, around an hour after the upgrade, with 19 pending compaction tasks:
SSTables in each level: [6/4, 10, 105/100, 268, 0, 0, 0, 0, 0]
SSTables in each level: [6/4, 10, 106/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [1, 16/10, 105/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [5/4, 10, 103/100, 272, 0, 0, 0, 0, 0]
SSTables in each level: [4, 11/10, 105/100, 270, 0, 0, 0, 0, 0]
SSTables in each level: [1, 12/10, 105/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [1, 14/10, 104/100, 267, 0, 0, 0, 0, 0]
SSTables in each level: [9/4, 10, 103/100, 265, 0, 0, 0, 0, 0]

Recently, with 41 pending compaction tasks:
SSTables in each level: [4, 13/10, 106/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [4, 12/10, 106/100, 273, 0, 0, 0, 0, 0]
SSTables in each level: [5/4, 11/10, 106/100, 271, 0, 0, 0, 0, 0]
SSTables in each level: [4, 12/10, 103/100, 275, 0, 0, 0, 0, 0]
SSTables in each level: [2, 13/10, 106/100, 273, 0, 0, 0, 0, 0]
SSTables in each level: [3, 10, 104/100, 275, 0, 0, 0, 0, 0]
SSTables in each level: [6/4, 11/10, 103/100, 269, 0, 0, 0, 0, 0]
SSTables in each level: [4, 16/10, 105/100, 264, 0, 0, 0, 0, 0]

More information about the use case: writes are roughly uniform across these tables. The data is ""sharded"" across these 8 tables by key to improve compaction parallelism. Each node receives up to 75,000 writes/sec sustained at peak, and a small number of reads. This is a pre-production cluster that's being warmed up with new data, so the low volume of reads (~100/sec per node) is just from automatic sampled data checks, otherwise we'd just use STCS :)"
CASSANDRA-8459,"""autocompaction"" on reads can prevent memtable space reclaimation","Memtable memory reclamation is dependent on reads always making progress, however on the collectTimeOrderedData critical path it is possible for the read to perform a _write_ inline, and for this write to block waiting for memtable space to be reclaimed. However the reclaimation is blocked waiting for this read to complete.

There are a number of solutions to this, but the simplest is to make the defragmentation happen asynchronously, so the read terminates normally."
CASSANDRA-8447,Nodes stuck in CMS GC cycle with very little traffic when compaction is enabled,"Behavior - If autocompaction is enabled, nodes will become unresponsive due to a full Old Gen heap which is not cleared during CMS GC.

Test methodology - disabled autocompaction on 3 nodes, left autocompaction enabled on 1 node.  Executed different Cassandra stress loads, using write only operations.  Monitored visualvm and jconsole for heap pressure.  Captured iostat and dstat for most tests.  Captured heap dump from 50 thread load.  Hints were disabled for testing on all nodes to alleviate GC noise due to hints backing up.

Data load test through Cassandra stress -  /usr/bin/cassandra-stress  write n=1900000000 -rate threads=<different threads tested> -schema  replication\(factor=3\)  keyspace=""Keyspace1"" -node <all nodes listed>

Data load thread count and results:
* 1 thread - Still running but looks like the node can sustain this load (approx 500 writes per second per node)
* 5 threads - Nodes become unresponsive due to full Old Gen Heap.  CMS measured in the 60 second range (approx 2k writes per second per node)
* 10 threads - Nodes become unresponsive due to full Old Gen Heap.  CMS measured in the 60 second range
* 50 threads - Nodes become unresponsive due to full Old Gen Heap.  CMS measured in the 60 second range  (approx 10k writes per second per node)
* 100 threads - Nodes become unresponsive due to full Old Gen Heap.  CMS measured in the 60 second range  (approx 20k writes per second per node)
* 200 threads - Nodes become unresponsive due to full Old Gen Heap.  CMS measured in the 60 second range  (approx 25k writes per second per node)

Note - the observed behavior was the same for all tests except for the single threaded test.  The single threaded test does not appear to show this behavior.

Tested different GC and Linux OS settings with a focus on the 50 and 200 thread loads.  

JVM settings tested:
#  default, out of the box, env-sh settings
#  10 G Max | 1 G New - default env-sh settings
#  10 G Max | 1 G New - default env-sh settings
#* JVM_OPTS=""$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=50""
#   20 G Max | 10 G New 
   JVM_OPTS=""$JVM_OPTS -XX:+UseParNewGC""
   JVM_OPTS=""$JVM_OPTS -XX:+UseConcMarkSweepGC""
   JVM_OPTS=""$JVM_OPTS -XX:+CMSParallelRemarkEnabled""
   JVM_OPTS=""$JVM_OPTS -XX:SurvivorRatio=8""
   JVM_OPTS=""$JVM_OPTS -XX:MaxTenuringThreshold=8""
   JVM_OPTS=""$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=75""
   JVM_OPTS=""$JVM_OPTS -XX:+UseCMSInitiatingOccupancyOnly""
   JVM_OPTS=""$JVM_OPTS -XX:+UseTLAB""
   JVM_OPTS=""$JVM_OPTS -XX:+CMSScavengeBeforeRemark""
   JVM_OPTS=""$JVM_OPTS -XX:CMSMaxAbortablePrecleanTime=60000""
   JVM_OPTS=""$JVM_OPTS -XX:CMSWaitDuration=30000""
   JVM_OPTS=""$JVM_OPTS -XX:ParallelGCThreads=12""
   JVM_OPTS=""$JVM_OPTS -XX:ConcGCThreads=12""
   JVM_OPTS=""$JVM_OPTS -XX:+UnlockDiagnosticVMOptions""
   JVM_OPTS=""$JVM_OPTS -XX:+UseGCTaskAffinity""
   JVM_OPTS=""$JVM_OPTS -XX:+BindGCTaskThreadsToCPUs""
   JVM_OPTS=""$JVM_OPTS -XX:ParGCCardsPerStrideChunk=32768""
   JVM_OPTS=""$JVM_OPTS -XX:-UseBiasedLocking""
# 20 G Max | 1 G New 
   JVM_OPTS=""$JVM_OPTS -XX:+UseParNewGC""
   JVM_OPTS=""$JVM_OPTS -XX:+UseConcMarkSweepGC""
   JVM_OPTS=""$JVM_OPTS -XX:+CMSParallelRemarkEnabled""
   JVM_OPTS=""$JVM_OPTS -XX:SurvivorRatio=8""
   JVM_OPTS=""$JVM_OPTS -XX:MaxTenuringThreshold=8""
   JVM_OPTS=""$JVM_OPTS -XX:CMSInitiatingOccupancyFraction=75""
   JVM_OPTS=""$JVM_OPTS -XX:+UseCMSInitiatingOccupancyOnly""
   JVM_OPTS=""$JVM_OPTS -XX:+UseTLAB""
   JVM_OPTS=""$JVM_OPTS -XX:+CMSScavengeBeforeRemark""
   JVM_OPTS=""$JVM_OPTS -XX:CMSMaxAbortablePrecleanTime=60000""
   JVM_OPTS=""$JVM_OPTS -XX:CMSWaitDuration=30000""
   JVM_OPTS=""$JVM_OPTS -XX:ParallelGCThreads=12""
   JVM_OPTS=""$JVM_OPTS -XX:ConcGCThreads=12""
   JVM_OPTS=""$JVM_OPTS -XX:+UnlockDiagnosticVMOptions""
   JVM_OPTS=""$JVM_OPTS -XX:+UseGCTaskAffinity""
   JVM_OPTS=""$JVM_OPTS -XX:+BindGCTaskThreadsToCPUs""
   JVM_OPTS=""$JVM_OPTS -XX:ParGCCardsPerStrideChunk=32768""
   JVM_OPTS=""$JVM_OPTS -XX:-UseBiasedLocking""

Linux OS settings tested:
# Disabled Transparent Huge Pages
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag
# Enabled Huge Pages
echo 21500000000 > /proc/sys/kernel/shmmax (over 20GB for heap)
echo 1536 > /proc/sys/vm/nr_hugepages (20GB/2MB page size)
# Disabled NUMA
numa-off in /etc/grub.confdatastax
# Verified all settings documented here were implemented
  http://www.datastax.com/documentation/cassandra/2.0/cassandra/install/installRecommendSettings.html

Attachments:
#  .yaml
#  fio output - results.tar.gz
#  50 thread heap dump - https://drive.google.com/a/datastax.com/file/d/0B4Imdpu2YrEbMGpCZW5ta2liQ2c/view?usp=sharing
#  100 thread - visual vm anonymous screenshot - visualvm_screenshot
#  dstat screen shot of with compaction - Node_with_compaction.png
#  dstat screen shot of without compaction -- Node_without_compaction.png
#  gcinspector messages from system.log
# gc.log output - gc.logs.tar.gz

Observations:
#  even though this is a spinning disk implementation, disk io looks good. 
#* output from Jshook perf monitor https://github.com/jshook/perfscripts is attached
#* note, we leveraged direct io for all tests by adding direct=1 to the .global config files
#  cpu usage is moderate until large GC events occur
#  once old gen heap fills up and cannot clean, memtable post flushers start to back up (show a lot pending) via tpstats
#  the node itself, i.e. ssh, is still responsive but the Cassandra instance becomes unresponsive
# once old gen heap fills up Cassandra stress starts to throw CL ONE errors stating there aren't enough replicas to satisfy....
#  heap dump from 50 thread, JVM scenario 1 is attached
#* appears to show a compaction thread consuming a lot of memory
#  sample system.log output for gc issues
#  strace -e futex -p $PID -f -c output during 100 thread load and during old gen ""filling"", just before full
% time    seconds  usecs/call    calls    errors syscall
100.00  244.886766        4992    49052      7507 futex
100.00  244.886766                49052      7507 total
#  htop during full gc cycle  - https://s3.amazonaws.com/uploads.hipchat.com/6528/480117/4ZlgcoNScb6kRM2/upload.png
#  nothing is blocked via tpstats on these nodes
#  compaction does have pending tasks, upwards of 20, on the nodes
#  Nodes without compaction achieved approximately 20k writes per second per node without errors or drops

Next Steps:
#  Will try to create a flame graph and update load here - http://www.brendangregg.com/blog/2014-06-12/java-flame-graphs.html
#  Will try to recreate in another environment"
CASSANDRA-8428,Nodetool Drain kills C* Process,"Nodetool Drain is documented at http://wiki.apache.org/cassandra/NodeTool and in the nodetool help to flush a node and stop accepting writes. This is the behavior I see with 2.1.2.

In 2.0.11 and 1.2.19, instead the Cassandra Process is killed. In the 1.2.19 logs, I see:
{code}
 INFO [RMI TCP Connection(2)-192.168.1.5] 2014-12-05 10:32:44,234 StorageService.java (line 964)
 DRAINING: starting drain process
 INFO [RMI TCP Connection(2)-192.168.1.5] 2014-12-05 10:32:44,235 ThriftServer.java (line 116) S
top listening to thrift clients
 INFO [RMI TCP Connection(2)-192.168.1.5] 2014-12-05 10:32:44,239 Server.java (line 159) Stop li
stening for CQL clients
 INFO [RMI TCP Connection(2)-192.168.1.5] 2014-12-05 10:32:44,239 Gossiper.java (line 1203) Announcing shutdown
 INFO [RMI TCP Connection(2)-192.168.1.5] 2014-12-05 10:32:46,240 MessagingService.java (line 696) Waiting for messaging service to quiesce
 INFO [ACCEPT-/127.0.0.1] 2014-12-05 10:32:46,241 MessagingService.java (line 919) MessagingService shutting down server thread.{code}
So it appears this in an intentional shut down, in which case the docs and help are wrong. I could not find a JIRA that described the change in behavior moving to 2.1.

Other users on IRC report that drain works as expected for them on 1.2.19. Attached are 2.0 logs."
CASSANDRA-8419,NPE in SelectStatement,"The dtest {{cql_tests.py:TestCQL.empty_in_test}} is failing in trunk with a Null Pointer Exception. The stack trace is:
{code}ERROR [SharedPool-Worker-1] 2014-12-03 16:24:16,274 ErrorMessage.java:243 - Unexpected exception
during request
java.lang.NullPointerException: null
        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:213) ~[guava-16.0
.jar:na]
        at com.google.common.collect.Lists$TransformingSequentialList.<init>(Lists.java:525) ~[gu
ava-16.0.jar:na]
        at com.google.common.collect.Lists.transform(Lists.java:508) ~[guava-16.0.jar:na]
        at org.apache.cassandra.db.composites.Composites.toByteBuffers(Composites.java:45) ~[main
/:na]
        at org.apache.cassandra.cql3.restrictions.SingleColumnPrimaryKeyRestrictions.values(Singl
eColumnPrimaryKeyRestrictions.java:257) ~[main/:na]
        at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getPartitionKeys(StatementRestrictions.java:362) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:296) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.getPageableCommand(SelectStatement.java:205) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:165) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:72) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:239) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:261) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:118) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_67]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]{code}

The error occurred while executing {{SELECT v FROM test_compact WHERE k1 IN ()}}."
CASSANDRA-8409,Node generating a huge number of tiny sstable_activity flushes,"On one of my nodes, I’m seeing hundreds per second of “INFO  21:28:05 Enqueuing flush of sstable_activity: 0 (0%) on-heap, 33 (0%) off-heap”. tpstats shows a steadily climbing # of pending MemtableFlushWriter/MemtablePostFlush until the node OOMs. When the flushes actually happen the sstable written is invariably 121 bytes. I’m writing pretty aggressively to one of my user tables (sev.mdb_group_pit), but that table's flushing behavior seems reasonable.

tpstats:
{quote}
frew@hostname:~/s_dist/apache-cassandra-2.1.0$ bin/nodetool -h hostname tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
MutationStage                   128      4429          36810         0                 0
ReadStage                         0         0           1205         0                 0
RequestResponseStage              0         0          24910         0                 0
ReadRepairStage                   0         0             26         0                 0
CounterMutationStage              0         0              0         0                 0
MiscStage                         0         0              0         0                 0
HintedHandoff                     2         2              9         0                 0
GossipStage                       0         0           5157         0                 0
CacheCleanupExecutor              0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
CompactionExecutor                4        28            429         0                 0
ValidationExecutor                0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
AntiEntropyStage                  0         0              0         0                 0
PendingRangeCalculator            0         0             11         0                 0
MemtableFlushWriter               8     38644           8987         0                 0
MemtablePostFlush                 1     38940           8735         0                 0
MemtableReclaimMemory             0         0           8987         0                 0

Message type           Dropped
READ                         0
RANGE_SLICE                  0
_TRACE                       0
MUTATION                 10457
COUNTER_MUTATION             0
BINARY                       0
REQUEST_RESPONSE             0
PAGED_RANGE                  0
READ_REPAIR                208
{quote}

I've attached one of the produced sstables."
CASSANDRA-8351,Running COPY FROM in cqlsh aborts with errors or segmentation fault,"Running Cassandra 2.1.2 binary tarball on a single instance.

Put together a script to try to reproduce this using data generated by cassandra-stress.

Reproduction steps: Download files and run cqlsh -f stress.cql
This may need to run a couple of times before errors are encountered. I've seen this work best when running after a fresh install.

Errors seen:

1.    Segmentation fault (core dumped)

2.    
{code}
       stress.cql:24:line contains NULL byte
       stress.cql:24:Aborting import at record #0. Previously-inserted values still present.
       71 rows imported in 0.100 seconds.{code}

3.   
{code}*** glibc detected *** python: corrupted double-linked list: 0x0000000001121ad0 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x7eb96)[0x7f80fe0cdb96]
/lib/x86_64-linux-gnu/libc.so.6(+0x7fead)[0x7f80fe0ceead]
python[0x42615d]
python[0x501dc8]
python[0x4ff715]
python[0x425d02]
python(PyEval_EvalCodeEx+0x1c4)[0x575db4]
python[0x577be2]
python(PyObject_Call+0x36)[0x4d91b6]
python(PyEval_EvalFrameEx+0x2035)[0x54d8a5]
python(PyEval_EvalCodeEx+0x1a2)[0x575d92]
python(PyEval_EvalFrameEx+0x7b8)[0x54c028]
python(PyEval_EvalCodeEx+0x1a2)[0x575d92]
python(PyEval_EvalFrameEx+0x7b8)[0x54c028]
python(PyEval_EvalFrameEx+0xa02)[0x54c272]
python(PyEval_EvalFrameEx+0xa02)[0x54c272]
python(PyEval_EvalFrameEx+0xa02)[0x54c272]
python(PyEval_EvalCodeEx+0x1a2)[0x575d92]
python(PyEval_EvalFrameEx+0x7b8)[0x54c028]
python(PyEval_EvalCodeEx+0x1a2)[0x575d92]
python(PyEval_EvalFrameEx+0x7b8)[0x54c028]
python(PyEval_EvalCodeEx+0x1a2)[0x575d92]
python[0x577be2]
python(PyObject_Call+0x36)[0x4d91b6]
python(PyEval_EvalFrameEx+0x2035)[0x54d8a5]
python(PyEval_EvalFrameEx+0xa02)[0x54c272]
python(PyEval_EvalFrameEx+0xa02)[0x54c272]
python(PyEval_EvalCodeEx+0x1a2)[0x575d92]
python[0x577ab0]
python(PyObject_Call+0x36)[0x4d91b6]
python[0x4c91fa]
python(PyObject_Call+0x36)[0x4d91b6]
python(PyEval_CallObjectWithKeywords+0x36)[0x4d97c6]
python[0x4f7f58]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x7e9a)[0x7f80ff369e9a]
/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f80fe1433fd]
======= Memory map: ========
00400000-00672000 r-xp 00000000 08:01 1447344                            /usr/bin/python2.7
00871000-00872000 r--p 00271000 08:01 1447344                            /usr/bin/python2.7
00872000-008db000 rw-p 00272000 08:01 1447344                            /usr/bin/python2.7
008db000-008ed000 rw-p 00000000 00:00 0 
0090e000-01260000 rw-p 00000000 00:00 0                                  [heap]
7f80ec000000-7f80ec0aa000 rw-p 00000000 00:00 0 
7f80ec0aa000-7f80f0000000 ---p 00000000 00:00 0 
7f80f0000000-7f80f0021000 rw-p 00000000 00:00 0 
7f80f0021000-7f80f4000000 ---p 00000000 00:00 0 
7f80f4000000-7f80f4021000 rw-p 00000000 00:00 0 
7f80f4021000-7f80f8000000 ---p 00000000 00:00 0 
7f80fa713000-7f80fa714000 ---p 00000000 00:00 0 
7f80fa714000-7f80faf14000 rw-p 00000000 00:00 0                          [stack:7493]
7f80faf14000-7f80faf15000 ---p 00000000 00:00 0 
7f80faf15000-7f80fb715000 rw-p 00000000 00:00 0                          [stack:7492]
7f80fb715000-7f80fb716000 ---p 00000000 00:00 0 
7f80fb716000-7f80fbf16000 rw-p 00000000 00:00 0                          [stack:7491]
7f80fbf16000-7f80fbf21000 r-xp 00000000 08:01 1456254                    /usr/lib/python2.7/lib-dynload/_json.so
7f80fbf21000-7f80fc120000 ---p 0000b000 08:01 1456254                    /usr/lib/python2.7/lib-dynload/_json.so
7f80fc120000-7f80fc121000 r--p 0000a000 08:01 1456254                    /usr/lib/python2.7/lib-dynload/_json.so
7f80fc121000-7f80fc122000 rw-p 0000b000 08:01 1456254                    /usr/lib/python2.7/lib-dynload/_json.so
7f80fc122000-7f80fc133000 r-xp 00000000 08:01 1585974                    /usr/local/lib/python2.7/dist-packages/blist/_blist.so
7f80fc133000-7f80fc332000 ---p 00011000 08:01 1585974                    /usr/local/lib/python2.7/dist-packages/blist/_blist.so
7f80fc332000-7f80fc333000 r--p 00010000 08:01 1585974                    /usr/local/lib/python2.7/dist-packages/blist/_blist.so
7f80fc333000-7f80fc335000 rw-p 00011000 08:01 1585974                    /usr/local/lib/python2.7/dist-packages/blist/_blist.so
7f80fc335000-7f80fc349000 r-xp 00000000 08:01 1456262                    /usr/lib/python2.7/lib-dynload/datetime.so
7f80fc349000-7f80fc548000 ---p 00014000 08:01 1456262                    /usr/lib/python2.7/lib-dynload/datetime.so
7f80fc548000-7f80fc549000 r--p 00013000 08:01 1456262                    /usr/lib/python2.7/lib-dynload/datetime.so
7f80fc549000-7f80fc54d000 rw-p 00014000 08:01 1456262                    /usr/lib/python2.7/lib-dynload/datetime.so
7f80fc54d000-7f80fc56c000 r-xp 00000000 08:01 1456251                    /usr/lib/python2.7/lib-dynload/_io.so
7f80fc56c000-7f80fc76b000 ---p 0001f000 08:01 1456251                    /usr/lib/python2.7/lib-dynload/_io.so
7f80fc76b000-7f80fc76c000 r--p 0001e000 08:01 1456251                    /usr/lib/python2.7/lib-dynload/_io.so
7f80fc76c000-7f80fc775000 rw-p 0001f000 08:01 1456251                    /usr/lib/python2.7/lib-dynload/_io.so
7f80fc775000-7f80fc77b000 r-xp 00000000 08:01 1456263                    /usr/lib/python2.7/lib-dynload/_multiprocessing.so
7f80fc77b000-7f80fc97a000 ---p 00006000 08:01 1456263                    /usr/lib/python2.7/lib-dynload/_multiprocessing.so
7f80fc97a000-7f80fc97b000 r--p 00005000 08:01 1456263                    /usr/lib/python2.7/lib-dynload/_multiprocessing.so
7f80fc97b000-7f80fc97c000 rw-p 00006000 08:01 1456263                    /usr/lib/python2.7/lib-dynload/_multiprocessing.so
7f80fc97c000-7f80fc99e000 r-xp 00000000 08:01 1704004                    /lib/x86_64-linux-gnu/libtinfo.so.5.9
7f80fc99e000-7f80fcb9e000 ---p 00022000 08:01 1704004                    /lib/x86_64-linux-gnu/libtinfo.so.5.9
7f80fcb9e000-7f80fcba2000 r--p 00022000 08:01 1704004                    /lib/x86_64-linux-gnu/libtinfo.so.5.9
7f80fcba2000-7f80fcba3000 rw-p 00026000 08:01 1704004                    /lib/x86_64-linux-gnu/libtinfo.so.5.9
7f80fcba3000-7f80fcbdc000 r-xp 00000000 08:01 1704181                    /lib/x86_64-linux-gnu/libreadline.so.6.2
7f80fcbdc000-7f80fcddc000 ---p 00039000 08:01 1704181                    /lib/x86_64-linux-gnu/libreadline.so.6.2
7f80fcddc000-7f80fcdde000 r--p 00039000 08:01 1704181                    /lib/x86_64-linux-gnu/libreadline.so.6.2
7f80fcdde000-7f80fcde4000 rw-p 0003b000 08:01 1704181                    /lib/x86_64-linux-gnu/libreadline.so.6.2
7f80fcde4000-7f80fcde5000 rw-p 00000000 00:00 0 
7f80fcde5000-7f80fcdea000 r-xp 00000000 08:01 1456232                    /usr/lib/python2.7/lib-dynload/readline.so
7f80fcdea000-7f80fcfe9000 ---p 00005000 08:01 1456232                    /usr/lib/python2.7/lib-dynload/readline.so
7f80fcfe9000-7f80fcfea000 r--p 00004000 08:01 1456232                    /usr/lib/python2.7/lib-dynload/readline.so
7f80fcfea000-7f80fcfec000 rw-p 00005000 08:01 1456232                    /usr/lib/python2.7/lib-dynload/readline.so
7f80fcfec000-7f80fcff0000 r-xp 00000000 08:01 1456240                    /usr/lib/python2.7/lib-dynload/termios.so
7f80fcff0000-7f80fd1ef000 ---p 00004000 08:01 1456240                    /usr/lib/python2.7/lib-dynload/termios.so
7f80fd1ef000-7f80fd1f0000 r--p 00003000 08:01 1456240                    /usr/lib/python2.7/lib-dynload/termios.so
7f80fd1f0000-7f80fd1f2000 rw-p 00004000 08:01 1456240                    /usr/lib/python2.7/lib-dynload/termios.so
7f80fd1f2000-7f80fd1f8000 r-xp 00000000 08:01 1456257                    /usr/lib/python2.7/lib-dynload/_csv.so
7f80fd1f8000-7f80fd3f7000 ---p 00006000 08:01 1456257                    /usr/lib/python2.7/lib-dynload/_csv.so
7f80fd3f7000-7f80fd3f8000 r--p 00005000 08:01 1456257                    /usr/lib/python2.7/lib-dynload/_csv.so
7f80fd3f8000-7f80fd3fa000 rw-p 00006000 08:01 1456257                    /usr/lib/python2.7/lib-dynload/_csv.so
7f80fd3fa000-7f80fd3fd000 r-xp 00000000 08:01 1456238                    /usr/lib/python2.7/lib-dynload/_heapq.so
7f80fd3fd000-7f80fd5fc000 ---p 00003000 08:01 1456238                    /usr/lib/python2.7/lib-dynload/_heapq.so
7f80fd5fc000-7f80fd5fd000 r--p 00002000 08:01 1456238                    /usr/lib/python2.7/lib-dynload/_heapq.so
7f80fd5fd000-7f80fd5ff000 rw-p 00003000 08:01 1456238                    /usr/lib/python2.7/lib-dynload/_heapq.so
7f80fd5ff000-7f80fd6c0000 rw-p 00000000 00:00 0 
7f80fd6c0000-7f80fd6e1000 r-xp 00000000 08:01 1456260                    /usr/lib/python2.7/lib-dynload/_ctypes.so
7f80fd6e1000-7f80fd8e0000 ---p 00021000 08:01 1456260                    /usr/lib/python2.7/lib-dynload/_ctypes.so
7f80fd8e0000-7f80fd8e1000 r--p 00020000 08:01 1456260                    /usr/lib/python2.7/lib-dynload/_ctypes.so
7f80fd8e1000-7f80fd8e5000 rw-p 00021000 08:01 1456260                    /usr/lib/python2.7/lib-dynload/_ctypes.so
7f80fd8e5000-7f80fd968000 rw-p 00000000 00:00 0 
7f80fd96b000-7f80fd96f000 r-xp 00000000 08:01 1704045                    /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f80fd96f000-7f80fdb6e000 ---p 00004000 08:01 1704045                    /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f80fdb6e000-7f80fdb6f000 r--p 00003000 08:01 1704045                    /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f80fdb6f000-7f80fdb70000 rw-p 00004000 08:01 1704045                    /lib/x86_64-linux-gnu/libuuid.so.1.3.0
7f80fdb70000-7f80fde39000 r--p 00000000 08:01 1447853                    /usr/lib/locale/locale-archive
7f80fde39000-7f80fde4e000 r-xp 00000000 08:01 1703980                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f80fde4e000-7f80fe04d000 ---p 00015000 08:01 1703980                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f80fe04d000-7f80fe04e000 r--p 00014000 08:01 1703980                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f80fe04e000-7f80fe04f000 rw-p 00015000 08:01 1703980                    /lib/x86_64-linux-gnu/libgcc_s.so.1
7f80fe04f000-7f80fe204000 r-xp 00000000 08:01 1703953                    /lib/x86_64-linux-gnu/libc-2.15.so
7f80fe204000-7f80fe404000 ---p 001b5000 08:01 1703953                    /lib/x86_64-linux-gnu/libc-2.15.so
7f80fe404000-7f80fe408000 r--p 001b5000 08:01 1703953                    /lib/x86_64-linux-gnu/libc-2.15.so
7f80fe408000-7f80fe40a000 rw-p 001b9000 08:01 1703953                    /lib/x86_64-linux-gnu/libc-2.15.so
7f80fe40a000-7f80fe40f000 rw-p 00000000 00:00 0 
7f80fe40f000-7f80fe50a000 r-xp 00000000 08:01 1703968                    /lib/x86_64-linux-gnu/libm-2.15.so
7f80fe50a000-7f80fe709000 ---p 000fb000 08:01 1703968                    /lib/x86_64-linux-gnu/libm-2.15.so
7f80fe709000-7f80fe70a000 r--p 000fa000 08:01 1703968                    /lib/x86_64-linux-gnu/libm-2.15.so
7f80fe70a000-7f80fe70b000 rw-p 000fb000 08:01 1703968                    /lib/x86_64-linux-gnu/libm-2.15.so
7f80fe70b000-7f80fe721000 r-xp 00000000 08:01 1704165                    /lib/x86_64-linux-gnu/libz.so.1.2.3.4
7f80fe721000-7f80fe920000 ---p 00016000 08:01 1704165                    /lib/x86_64-linux-gnu/libz.so.1.2.3.4
7f80fe920000-7f80fe921000 r--p 00015000 08:01 1704165                    /lib/x86_64-linux-gnu/libz.so.1.2.3.4
7f80fe921000-7f80fe922000 rw-p 00016000 08:01 1704165                    /lib/x86_64-linux-gnu/libz.so.1.2.3.4
7f80fe922000-7f80fead3000 r-xp 00000000 08:01 1710096                    /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
7f80fead3000-7f80fecd3000 ---p 001b1000 08:01 1710096                    /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
7f80fecd3000-7f80fecee000 r--p 001b1000 08:01 1710096                    /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
7f80fecee000-7f80fecf9000 rw-p 001cc000 08:01 1710096                    /lib/x86_64-linux-gnu/libcrypto.so.1.0.0
7f80fecf9000-7f80fecfd000 rw-p 00000000 00:00 0 
7f80fecfd000-7f80fed51000 r-xp 00000000 08:01 1710095                    /lib/x86_64-linux-gnu/libssl.so.1.0.0
7f80fed51000-7f80fef51000 ---p 00054000 08:01 1710095                    /lib/x86_64-linux-gnu/libssl.so.1.0.0
7f80fef51000-7f80fef54000 r--p 00054000 08:01 1710095                    /lib/x86_64-linux-gnu/libssl.so.1.0.0
7f80fef54000-7f80fef5a000 rw-p 00057000 08:01 1710095                    /lib/x86_64-linux-gnu/libssl.so.1.0.0
7f80fef5a000-7f80fef5b000 rw-p 00000000 00:00 0 
7f80fef5b000-7f80fef5d000 r-xp 00000000 08:01 1703952                    /lib/x86_64-linux-gnu/libutil-2.15.so
7f80fef5d000-7f80ff15c000 ---p 00002000 08:01 1703952                    /lib/x86_64-linux-gnu/libutil-2.15.so
7f80ff15c000-7f80ff15d000 r--p 00001000 08:01 1703952                    /lib/x86_64-linux-gnu/libutil-2.15.so
7f80ff15d000-7f80ff15e000 rw-p 00002000 08:01 1703952                    /lib/x86_64-linux-gnu/libutil-2.15.so
7f80ff15e000-7f80ff160000 r-xp 00000000 08:01 1703972                    /lib/x86_64-linux-gnu/libdl-2.15.so
7f80ff160000-7f80ff360000 ---p 00002000 08:01 1703972                    /lib/x86_64-linux-gnu/libdl-2.15.so
7f80ff360000-7f80ff361000 r--p 00002000 08:01 1703972                    /lib/x86_64-linux-gnu/libdl-2.15.so
7f80ff361000-7f80ff362000 rw-p 00003000 08:01 1703972                    /lib/x86_64-linux-gnu/libdl-2.15.so
7f80ff362000-7f80ff37a000 r-xp 00000000 08:01 1703966                    /lib/x86_64-linux-gnu/libpthread-2.15.so
7f80ff37a000-7f80ff579000 ---p 00018000 08:01 1703966                    /lib/x86_64-linux-gnu/libpthread-2.15.so
7f80ff579000-7f80ff57a000 r--p 00017000 08:01 1703966                    /lib/x86_64-linux-gnu/libpthread-2.15.so
7f80ff57a000-7f80ff57b000 rw-p 00018000 08:01 1703966                    /lib/x86_64-linux-gnu/libpthread-2.15.so
7f80ff57b000-7f80ff57f000 rw-p 00000000 00:00 0 
7f80ff57f000-7f80ff5a1000 r-xp 00000000 08:01 1703969                    /lib/x86_64-linux-gnu/ld-2.15.so
7f80ff655000-7f80ff6d7000 rw-p 00000000 00:00 0 
7f80ff708000-7f80ff790000 rw-p 00000000 00:00 0 
7f80ff79a000-7f80ff79e000 rw-p 00000000 00:00 0 
7f80ff79e000-7f80ff79f000 rwxp 00000000 00:00 0 
7f80ff79f000-7f80ff7a1000 rw-p 00000000 00:00 0 
7f80ff7a1000-7f80ff7a2000 r--p 00022000 08:01 1703969                    /lib/x86_64-linux-gnu/ld-2.15.so
7f80ff7a2000-7f80ff7a4000 rw-p 00023000 08:01 1703969                    /lib/x86_64-linux-gnu/ld-2.15.so
7fff01618000-7fff01639000 rw-p 00000000 00:00 0                          [stack]
7fff0174d000-7fff0174f000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
{code}"
CASSANDRA-8319,Allow LWT DELETE with column comparison,"Right now, the only way to use LWT with DELETE is to rely on the IF NOT EXISTS keyword

There may be some scenarios where using IF column=xxx with DELETE is relevant.

 I am preparing a hands-on with a chat application using C*. A chatroom is defined as:

{code:sql}
CREATE TABLE chatroom (
    room_id uuid PRIMARY KEY,
    name text,
    participants list<frozen <person>> // person is an UDT representing a subset of the users table);
{code}

 Right now, upon removing a participant from the room, I need to:

* count remaining participants in the room (read the participants list)
* remove the room (the whole partition) is there isn't anyone inside

 This is a read-before-write pattern, but even this does not prevent race conditions. Indeed, the last participant may leave the room at the same time a new one enters

 So using LWT with ""DELETE FROM chatroom IF participants = [] WHERE room_id= ..."" may help making the removal safe

 With this design, room creation/deletion as well as participants addition/removal should go through LWT to be consistent. It's slow but participant joining and leaving event frequency is low enough compared to people posting messages to make the trade off not too expensive in general"
CASSANDRA-8312,Use live sstables in snapshot repair if possible,"Snapshot repair can be very much slower than parallel repairs because of the overhead of opening the SSTables in the snapshot. This is particular true when using LCS, as you typically have many smaller SSTables then.

I compared parallel and sequential repair on a small range on one of our clusters (2*3 replicas). With parallel repair, this took 22 seconds. With sequential repair (default in 2.0), the same range took 330 seconds! This is an overhead of 330-22*6 = 198 seconds, just opening SSTables (there were 1000+ sstables). Also, opening 1000 sstables for many smaller rangers surely causes lots of memory churning.

The idea would be to list the sstables in the snapshot, but use the corresponding sstables in the live set if it's still available. For almost all sstables, the original one should still exist."
CASSANDRA-8280,Cassandra crashing on inserting data over 64K into indexed strings,"An attemtp to instert 65536 bytes in a field that is a primary index throws (correctly?) the cassandra.InvalidRequest exception. However, inserting the same data *in a indexed field that is not a primary index* works just fine. 

However, Cassandra will crash on next commit and never recover. So I rated it as Critical as it can be used for DoS attacks.

Reproduce: see the snippet below:
{code}
import uuid
from cassandra import ConsistencyLevel
from cassandra import InvalidRequest
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from cassandra.policies import ConstantReconnectionPolicy
from cassandra.cqltypes import UUID
 
# DROP KEYSPACE IF EXISTS cs;
# CREATE KEYSPACE cs WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
# USE cs;
# CREATE TABLE test3 (name text, value uuid, sentinel text, PRIMARY KEY (name));
# CREATE INDEX test3_sentinels ON test3(sentinel);             
 
class CassandraDemo(object):
 
    def __init__(self):
        ips = [""127.0.0.1""]
        ap = PlainTextAuthProvider(username=""cs"", password=""cs"")
        reconnection_policy = ConstantReconnectionPolicy(20.0, max_attempts=1000000)
        cluster = Cluster(ips, auth_provider=ap, protocol_version=3, reconnection_policy=reconnection_policy)
        self.session = cluster.connect(""cs"")
 
    def exec_query(self, query, args):
        prepared_statement = self.session.prepare(query)
        prepared_statement.consistency_level = ConsistencyLevel.LOCAL_QUORUM
        self.session.execute(prepared_statement, args)
 
    def bug(self):
        k1 = UUID( str(uuid.uuid4()) )       
        long_string = ""X"" * 65536
        query = ""INSERT INTO test3 (name, value, sentinel) VALUES (?, ?, ?);""
        args = (""foo"", k1, long_string)
 
        self.exec_query(query, args)
        self.session.execute(""DROP KEYSPACE IF EXISTS cs_test"", timeout=30)
        self.session.execute(""CREATE KEYSPACE cs_test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"")
         
c = CassandraDemo()

#first run
c.bug()

#second run, Cassandra crashes with java.lang.AssertionError
c.bug()
{code}

And here is the cassandra log:
{code}
ERROR [MemtableFlushWriter:3] 2014-11-06 16:44:49,263 CassandraDaemon.java:153 - Exception in thread Thread[MemtableFlushWriter:3,5,main]
java.lang.AssertionError: 65536
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithShortLength(ByteBufferUtil.java:290) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.db.ColumnIndex$Builder.maybeWriteRowHeader(ColumnIndex.java:214) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.db.ColumnIndex$Builder.add(ColumnIndex.java:201) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.db.ColumnIndex$Builder.build(ColumnIndex.java:142) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rawAppend(SSTableWriter.java:233) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:218) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:354) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:312) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) ~[guava-16.0.jar:na]
        at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1053) ~[apache-cassandra-2.1.1.jar:2.1.1]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) ~[na:1.7.0_60]
{code}"
CASSANDRA-8241,Use ecj [was: javac] instead of javassist,"Using JDK's built-in Java-Compiler API has some advantages over javassist.

Although compilation feels a bit slower, Java compiler API has some advantages:
* boxing + unboxing works
* generics work
* compiler error messages are better (or at least known) and have line/column numbers

The implementation does not use any temp files. Everything's in memory.

Patch attached to this issue."
CASSANDRA-8231,Wrong size of cached prepared statements,"Cassandra counts memory footprint of prepared statements for caching purposes. It seems, that there is problem with some statements, ie SelectStatement. Even simple selects is counted as 100KB object, updates, deletes etc have few hundreds or thousands bytes. Result is that cache - QueryProcessor.preparedStatements  - holds just fraction of statements..

I dig a little into the code, and it seems that problem is in jamm in class MemoryMeter. It seems that if instance contains reference to class, it counts size of whole class too. SelectStatement references EnumSet through ResultSet.Metadata and EnumSet holds reference to Enum class...

"
CASSANDRA-8192,Better error logging on corrupt compressed SSTables: currently AssertionError in Memory.java,"Since update of 1 of 12 nodes from 2.1.0-rel to 2.1.1-rel Exception during start up.
{panel:title=system.log}
ERROR [SSTableBatchOpen:1] 2014-10-27 09:44:00,079 CassandraDaemon.java:153 - Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.AssertionError: null
	at org.apache.cassandra.io.util.Memory.size(Memory.java:307) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.compress.CompressionMetadata.<init>(CompressionMetadata.java:135) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.compress.CompressionMetadata.create(CompressionMetadata.java:83) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.util.CompressedSegmentedFile$Builder.metadata(CompressedSegmentedFile.java:50) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile$Builder.complete(CompressedPoolingSegmentedFile.java:48) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:766) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:725) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:402) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:302) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:438) ~[apache-cassandra-2.1.1.jar:2.1.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_55]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_55]
	at java.lang.Thread.run(Unknown Source) [na:1.7.0_55]
{panel}
In the attached log you can still see as well CASSANDRA-8069 and CASSANDRA-6283."
CASSANDRA-8164,OOM due to slow memory meter,"Memory meter holds strong reference to memtable while it iterates over memtable cells. Because meter is not fast, it prevents memtable from being GCed after it has been flushed for some time.

If write rate is fast enough, this makes node OOM.

Fixed this by aborting metering if table becomes not active in datatracker, i.e. flushing or flushed."
CASSANDRA-8131,Short-circuited query results from collection index query,"After watching Jonathan's 2014 summit video, I wanted to give collection indexes a try as they seem to be a fit for a ""search by key/values"" usage pattern we have in our setup. Doing some test queries that I expect users would do against the table, a short-circuit behavior came up:

Here's the whole transcript:
{noformat}
CREATE TABLE by_sets (id int PRIMARY KEY, datakeys set<text>, datavars set<text>);
CREATE INDEX by_sets_datakeys ON by_sets (datakeys);
CREATE INDEX by_sets_datavars ON by_sets (datavars);
INSERT INTO by_sets (id, datakeys, datavars) VALUES (1, {'a'}, {'b'});
INSERT INTO by_sets (id, datakeys, datavars) VALUES (2, {'c'}, {'d'});
INSERT INTO by_sets (id, datakeys, datavars) VALUES (3, {'e'}, {'f'});
INSERT INTO by_sets (id, datakeys, datavars) VALUES (4, {'a'}, {'z'});
SELECT * FROM by_sets;

 id | datakeys | datavars
----+----------+----------
  1 |    {'a'} |    {'b'}
  2 |    {'c'} |    {'d'}
  4 |    {'a'} |    {'z'}
  3 |    {'e'} |    {'f'}

{noformat}
We then tried this query which short-circuited:
{noformat}
SELECT * FROM by_sets WHERE datakeys CONTAINS 'a' AND datakeys CONTAINS 'c';

 id | datakeys | datavars
----+----------+----------
  1 |    {'a'} |    {'b'}
  4 |    {'a'} |    {'z'}

(2 rows)
{noformat}
Instead of receveing 3 rows, which match the datakeys CONTAINS 'a' AND datakeys CONTAINS 'c' we only got the first.

Doing the same, but with CONTAINS 'c' first, ignores the second AND.
{noformat}
SELECT * FROM by_sets WHERE datakeys CONTAINS 'c' AND datakeys CONTAINS 'a' ;

 id | datakeys | datavars
----+----------+----------
  2 |    {'c'} |    {'d'}

(1 rows)
{noformat}
Also, on a side-note, I have two indexes on both datakeys and datavars. But when trying to run a query such as:
{noformat}
select * from by_sets WHERE datakeys CONTAINS 'a' AND datavars CONTAINS 'z';
code=2200 [Invalid query] message=""Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. 
If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING""
{noformat}
The second column, after AND (even if I inverse the order) requires an ""allow filtering"" clause yet the column is indexed an an in-memory ""join"" of the primary keys of these sets on the coordinator could build up the result.

Could anyone explain the short-circuit behavior?
And the requirement for ""allow-filtering"" on a secondly indexed column?

If they're not bugs but intended they should be documented better, at least their limitations."
CASSANDRA-8113,Gossip should ignore generation numbers too far in the future,"If a node sends corrupted gossip, it could set the generation numbers for other nodes to arbitrarily large values. This is dangerous since one bad node (e.g. with bad memory) could in theory bring down the cluster. Nodes should refuse to accept generation numbers that are too far in the future."
CASSANDRA-8105,NPE for null embedded UDT inside set,"An NPE is thrown parsing an INSERT statement when a embedded UDT inside another UDT is set to null inside a set.
This sounds very convoluted, but the examples below will hopefully make it clear...

With the following definitions:
CREATE TYPE ut1 (a int, b int);
CREATE TYPE ut2 (j frozen<ut1>, k int);
CREATE TYPE ut3 (i int, j frozen<ut1>);
CREATE TABLE tab1 (x int PRIMARY KEY, y set<frozen<ut2>>);
CREATE TABLE tab2 (x int PRIMARY KEY, y list<frozen<ut2>>);
CREATE TABLE tab3 (x int PRIMARY KEY, y set<frozen<ut3>>);

This query throws a NullPointerException:
INSERT INTO tab1 (x, y) VALUES (1, { { k: 1 } });

These however doesn't:
INSERT INTO tab2 (x, y) VALUES (1, [ { k: 1 } ]);
INSERT INTO tab3 (x, y) VALUES (1, { { i: 1 } });

So, the bug seems to be triggered only when the UDT is in a set, lists are fine. If the embedded UDT is after the value specified in the query, the bug  doesn't seem to trigger."
CASSANDRA-8104,frozen broken / frozen collections in frozen tuple type,"{{CREATE TABLE foo (key int primary key, tup frozen<tuple<double, list<double>, set<text>, map<int, boolean>>>)}}

Produces an NPE in {{CQL3Type.freeze()}}, because lists and sets have no keys.

{{CREATE TABLE bar (key int primary key, tup frozen<tuple<double, frozen<list<double>>, frozen<set<text>>, frozen<map<int, boolean>>>>)}}

Produces some NPEs that prevent the correct error message being printed.

Simple patch attached.
"
CASSANDRA-8099,Refactor and modernize the storage engine,"The current storage engine (which for this ticket I'll loosely define as ""the code implementing the read/write path"") is suffering from old age. One of the main problem is that the only structure it deals with is the cell, which completely ignores the more high level CQL structure that groups cell into (CQL) rows.

This leads to many inefficiencies, like the fact that during a reads we have to group cells multiple times (to count on replica, then to count on the coordinator, then to produce the CQL resultset) because we forget about the grouping right away each time (so lots of useless cell names comparisons in particular). But outside inefficiencies, having to manually recreate the CQL structure every time we need it for something is hindering new features and makes the code more complex that it should be.

Said storage engine also has tons of technical debt. To pick an example, the fact that during range queries we update {{SliceQueryFilter.count}} is pretty hacky and error prone. Or the overly complex ways {{AbstractQueryPager}} has to go into to simply ""remove the last query result"".

So I want to bite the bullet and modernize this storage engine. I propose to do 2 main things:
 # Make the storage engine more aware of the CQL structure. In practice, instead of having partitions be a simple iterable map of cells, it should be an iterable list of row (each being itself composed of per-column cells, though obviously not exactly the same kind of cell we have today).
 # Make the engine more iterative. What I mean here is that in the read path, we end up reading all cells in memory (we put them in a ColumnFamily object), but there is really no reason to. If instead we were working with iterators all the way through, we could get to a point where we're basically transferring data from disk to the network, and we should be able to reduce GC substantially.

Please note that such refactor should provide some performance improvements right off the bat but it's not its primary goal either. Its primary goal is to simplify the storage engine and adds abstraction that are better suited to further optimizations."
CASSANDRA-8090,NullPointerException when using prepared statements,"Due to the changes in CASSANDRA-4914, using a prepared statement from multiple threads leads to a race condition where the simple selection may be reset from a different thread, causing the following NPE:

{noformat}
java.lang.NullPointerException: null
	at org.apache.cassandra.cql3.ResultSet.addRow(ResultSet.java:63) ~[main/:na]
	at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.build(Selection.java:372) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1120) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:283) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:260) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:213) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:63) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:226) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:481) ~[main/:na]
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:133) ~[main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:438) [main/:na]
	at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:334) [main/:na]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_67]
	at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:163) [main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:103) [main/:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
{noformat}

Reproduced this using the stress tool:
{noformat}
 ./tools/bin/cassandra-stress user profile=tools/cqlstress-example.yaml ops\(insert=1,simple1=1\)
{noformat}

You'll need to change the {noformat}select:{noformat} line to be /1000 to prevent the illegal query exceptions."
CASSANDRA-8066,High Heap Consumption due to high number of SSTableReader,"Given a workload with quite a lot of reads, I recently encountered high heap memory consumption. Given 2GB of Heap, it appears I have 750.000+ tasks in SSTableReader.syncExecutor, consuming more than 1.2GB. These tasks have type SSTableReader$5, which I guess corresponds to :

{code}
readMeterSyncFuture = syncExecutor.scheduleAtFixedRate(new Runnable()
{
public void run()
{
if (!isCompacted.get())
{
meterSyncThrottle.acquire();
SystemKeyspace.persistSSTableReadMeter(desc.ksname, desc.cfname, desc.generation, readMeter);
}
}
}, 1, 5, TimeUnit.MINUTES);
{code}

I do not have have to the environment right now, but I could provide a threaddump later if necessary."
CASSANDRA-8019,Windows Unit tests and Dtests erroring due to sstable deleting task error,"Currently a large number of dtests and unit tests are erroring on windows with the following error in the node log:
{code}
ERROR [NonPeriodicTasks:1] 2014-09-29 11:05:04,383 SSTableDeletingTask.java:89 - Unable to delete c:\\users\\username\\appdata\\local\\temp\\dtest-vr6qgw\\test\\node1\\data\\system\\local-7ad54392bcdd35a684174e047860b377\\system-local-ka-4-Data.db (it will be removed on server restart; we'll also retry after GC)\n
{code}

git bisect points to the following commit:
{code}
0e831007760bffced8687f51b99525b650d7e193 is the first bad commit
commit 0e831007760bffced8687f51b99525b650d7e193
Author: Benedict Elliott Smith <benedict@apache.org>
Date:  Fri Sep 19 18:17:19 2014 +0100

    Fix resource leak in event of corrupt sstable

    patch by benedict; review by yukim for CASSANDRA-7932

:100644 100644 d3ee7d99179dce03307503a8093eb47bd0161681 f55e5d27c1c53db3485154cd16201fc5419f32df M      CHANGES.txt
:040000 040000 194f4c0569b6be9cc9e129c441433c5c14de7249 3c62b53b2b2bd4b212ab6005eab38f8a8e228923 M  src
:040000 040000 64f49266e328b9fdacc516c52ef1921fe42e994f de2ca38232bee6d2a6a5e068ed9ee0fbbc5aaebe M  test
{code}

You can reproduce this by running simple_bootstrap_test."
CASSANDRA-7992,Arithmetic overflow sorting commit log segments on replay,"When replaying a lot of commit logs aged several days commit log segments are sorted incorrectly due to arith overflow in CommitLogSegmentFileComparator
"
CASSANDRA-7983,nodetool repair triggers OOM,"Customer has a 3 node cluster with 500Mb data on each node

{noformat}
[cassandra@nbcqa-chc-a02 ~]$ nodetool status
Note: Ownership information does not include topology; for complete information, specify a keyspace
Datacenter: CH2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns   Host ID                               Rack
UN  162.150.4.234  255.26 MB  256     33.2%  4ad1b6a8-8759-4920-b54a-f059126900df  RAC1
UN  162.150.4.235  318.37 MB  256     32.6%  3eb0ec58-4b81-442e-bee5-4c91da447f38  RAC1
UN  162.150.4.167  243.7 MB   256     34.2%  5b2c1900-bf03-41c1-bb4e-82df1655b8d8  RAC1
[cassandra@nbcqa-chc-a02 ~]$
{noformat}

After we run repair command, system runs into OOM after some 45 minutes
Nothing else is running

{noformat}
[cassandra@nbcqa-chc-a02 ~]$ date
Fri Sep 19 15:55:33 UTC 2014
[cassandra@nbcqa-chc-a02 ~]$ nodetool repair -st -9220354588320251877 -et -9220354588320251873
Sep 19, 2014 4:06:08 PM ClientCommunicatorAdmin Checker-run
WARNING: Failed to check the connection: java.net.SocketTimeoutException: Read timed out
{noformat}

Here is when we run OOM

{noformat}
ERROR [ReadStage:28914] 2014-09-19 16:34:50,381 CassandraDaemon.java (line 199) Exception in thread Thread[ReadStage:28914,5,main]
java.lang.OutOfMemoryError: Java heap space
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:69)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
        at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
        at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1547)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1376)
        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:333)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1363)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1927)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{noformat}

Cassandra process pegs 1 of the 8 CPU's 100% 

{noformat}
top - 16:50:07 up 11 days,  2:01,  2 users,  load average: 0.54, 0.60, 0.65
Tasks: 175 total,   1 running, 174 sleeping,   0 stopped,   0 zombie
Cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu1  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu2  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu3  :  0.7%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu4  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu5  :  0.3%us,  0.3%sy,  0.0%ni, 99.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.3%st
Cpu6  :  0.0%us,  0.3%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Cpu7  :  0.3%us,  0.3%sy,  0.0%ni, 99.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
Mem:  16332056k total, 16167212k used,   164844k free,   149956k buffers
Swap:        0k total,        0k used,        0k free,  8360056k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 2161 cassandr  20   0 11.5g 6.9g 227m S 107.8 44.0 281:59.49 java
 9942 root      20   0  106m 2320 1344 S  1.0  0.0   0:00.03 dhclient-script
28969 opscente  20   0 4479m 188m  12m S  0.7  1.2  56:24.24 java
 5123 cassandr  20   0 1788m 107m  28m S  0.3  0.7   0:08.09 java
    1 root      20   0 19228 1352 1072 S  0.0  0.0   0:00.82 init
    2 root      20   0     0    0    0 S  0.0  0.0   0:00.02 kthreadd
    3 root      RT   0     0    0    0 S  0.0  0.0   0:05.52 migration/0
    4 root      20   0     0    0    0 S  0.0  0.0   0:13.15 ksoftirqd/0
    5 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/0
    6 root      RT   0     0    0    0 S  0.0  0.0   0:03.33 watchdog/0
    7 root      RT   0     0    0    0 S  0.0  0.0   0:04.88 migration/1
    8 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/1
    9 root      20   0     0    0    0 S  0.0  0.0   0:19.21 ksoftirqd/1
   10 root      RT   0     0    0    0 S  0.0  0.0   0:03.24 watchdog/1
   11 root      RT   0     0    0    0 S  0.0  0.0   0:05.46 migration/2
   12 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/2
   13 root      20   0     0    0    0 S  0.0  0.0   0:16.87 ksoftirqd/2
   14 root      RT   0     0    0    0 S  0.0  0.0   0:03.49 watchdog/2
   15 root      RT   0     0    0    0 S  0.0  0.0   0:05.31 migration/3
   16 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/3
   17 root      20   0     0    0    0 S  0.0  0.0   0:19.33 ksoftirqd/3
   18 root      RT   0     0    0    0 S  0.0  0.0   0:03.43 watchdog/3
   19 root      RT   0     0    0    0 S  0.0  0.0   0:05.36 migration/4
   20 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration/4
   21 root      20   0     0    0    0 S  0.0  0.0   0:17.64 ksoftirqd/4
   22 root      RT   0     0    0    0 S  0.0  0.0   0:03.18 watchdog/4
   23 root      RT   0     0    0    0 S  0.0  0.0   0:05.31 migration/5
{noformat}

There is a 12Gb HeapDump, the memory leak suspects show the following trace

{noformat}
 Thread Stack

RMI TCP Connection(1621)-162.150.4.235
  at org.apache.cassandra.service.StorageService.createRepairRangeFrom(Ljava/lang/String;Ljava/lang/String;)Ljava/util/Collection; (StorageService.java:2612)
  at org.apache.cassandra.service.StorageService.forceRepairRangeAsync(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;ZLjava/util/Collection;Ljava/util/Collection;[Ljava/lang/String;)I (StorageService.java:2541)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.misc.Trampoline.invoke(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.GeneratedMethodAccessor8.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.misc.MethodUtil.invoke(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Ljava/lang/reflect/Method;Ljava/lang/Object;[Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Ljava/lang/Object;Ljava/lang/Object;[Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Ljava/lang/Object;Ljava/lang/Object;[Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.mbeanserver.PerInterface.invoke(Ljava/lang/Object;Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Ljavax/management/ObjectName;Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;)Ljava/lang/Object; (Unknown Source)
  at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Ljavax/management/ObjectName;Ljava/lang/String;[Ljava/lang/Object;[Ljava/lang/String;)Ljava/lang/Object; (Unknown Source)
  at javax.management.remote.rmi.RMIConnectionImpl.doOperation(I[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at javax.management.remote.rmi.RMIConnectionImpl.access$300(Ljavax/management/remote/rmi/RMIConnectionImpl;I[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run()Ljava/lang/Object; (Unknown Source)
  at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(I[Ljava/lang/Object;Ljavax/security/auth/Subject;)Ljava/lang/Object; (Unknown Source)
  at javax.management.remote.rmi.RMIConnectionImpl.invoke(Ljavax/management/ObjectName;Ljava/lang/String;Ljava/rmi/MarshalledObject;[Ljava/lang/String;Ljavax/security/auth/Subject;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.GeneratedMethodAccessor37.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at java.lang.reflect.Method.invoke(Ljava/lang/Object;[Ljava/lang/Object;)Ljava/lang/Object; (Unknown Source)
  at sun.rmi.server.UnicastServerRef.dispatch(Ljava/rmi/Remote;Ljava/rmi/server/RemoteCall;)V (Unknown Source)
  at sun.rmi.transport.Transport$1.run()Ljava/lang/Void; (Unknown Source)
  at sun.rmi.transport.Transport$1.run()Ljava/lang/Object; (Unknown Source)
  at java.security.AccessController.doPrivileged(Ljava/security/PrivilegedExceptionAction;Ljava/security/AccessControlContext;)Ljava/lang/Object; (Native Method)
  at sun.rmi.transport.Transport.serviceCall(Ljava/rmi/server/RemoteCall;)Z (Unknown Source)
  at sun.rmi.transport.tcp.TCPTransport.handleMessages(Lsun/rmi/transport/Connection;Z)V (Unknown Source)
  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0()V (Unknown Source)
  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run()V (Unknown Source)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V (Unknown Source)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run()V (Unknown Source)
  at java.lang.Thread.run()V (Unknown Source)
{noformat}

The file is way too large to be attached here.  
It's currently held at my SCP server
Let us know if there is any other info you may need
I will be posting the other 2 system logs as soon as I get them "
CASSANDRA-7953,RangeTombstones not merging during compaction,"When performing a compaction on two sstables that contain the same RangeTombstone with different timestamps, the tombstones are not merged in the new sstable.

This has been tested using cassandra 2.1 with the following table:
{code}
CREATE TABLE test(
  key text,
  column text,
  data text,
  PRIMARY KEY(key, column)
);
{code}

And then doing the following:
{code}
INSERT INTO test (key, column, data) VALUES (""1"", ""1"", ""1""); // If the sstable only contains tombstones during compaction it seems that the sstable either gets removed or isn't created (but that could probably be a separate JIRA issue).
INSERT INTO test (key, column, data) VALUES (""1"", ""2"", ""2""); // The inserts are not actually needed, since the deletes create tombstones either way.
DELETE FROM test WHERE key=""1"" AND column=""2"";

nodetool flush

INSERT INTO test (key, column, data) VALUES (""1"", ""2"", ""2"");
DELETE FROM test WHERE key=""1"" AND column=""2"";

nodetool flush
nodetool compact
{code}

When checking with the SSTableExport tool two tombstones exists in the compacted sstable. This can be repeated, resulting in more and more tombstones."
CASSANDRA-7946,NPE when streaming data to a joining node and dropping table in cluster,"Summary

 The cluster has 3 nodes with Vnodes. We were adding a 4th one (in auto bootstrap mode). The 4 node has stream sessions, receiving data from the others.

 While the streaming was going on (it takes quite a while because there are 200Gb of data of worth), an existing table is dropped in the cluster and a new one is created.

 A few time after, we caught an NPE in the log file of the joining node (still not finished joining):

 The NPE is located here: https://github.com/apache/cassandra/blob/cassandra-2.0/src/java/org/apache/cassandra/streaming/compress/CompressedStreamReader.java#L63

 It can be a race condition where schema agreement has not reached the joining node."
CASSANDRA-7926,Stress can OOM on merging of timing samples,"{noformat}
Exception in thread ""StressMetrics:2"" java.lang.OutOfMemoryError: Java heap space
        at java.util.Arrays.copyOf(Arrays.java:2343)
        at org.apache.cassandra.stress.util.SampleOfLongs.merge(SampleOfLongs.java:76)
        at org.apache.cassandra.stress.util.TimingInterval.merge(TimingInterval.java:95)
        at org.apache.cassandra.stress.util.Timing.snapInterval(Timing.java:95)
        at org.apache.cassandra.stress.StressMetrics.update(StressMetrics.java:124)
        at org.apache.cassandra.stress.StressMetrics.access$200(StressMetrics.java:36)
        at org.apache.cassandra.stress.StressMetrics$1.run(StressMetrics.java:72)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

This is partially down to recently increasing the per-thread sample size, but also because we allocate temporary space linear in size to total sample size in all threads during merge. This can easily be avoided. We should also scale per-thread sample size based on total number of threads, so we limit total memory use."
CASSANDRA-7897,NodeTool command to display OffHeap memory usage,"Most of the highest memory consuming data structure in Cassandra is now off-heap. It will be nice to display the memory used by BF's, Index Summaries, FS Buffers, Caches and Memtables (when enabled)

This ticket is to track and display off heap memory allocation/used by running Cassandra process, this will help users to further tune the memory used by these data structures per CF."
CASSANDRA-7886,Coordinator should not wait for read timeouts when replicas hit Exceptions,"*Issue*
When you have TombstoneOverwhelmingExceptions occuring in queries, this will cause the query to be simply dropped on every data-node, but no response is sent back to the coordinator. Instead the coordinator waits for the specified read_request_timeout_in_ms.

On the application side this can cause memory issues, since the application is waiting for the timeout interval for every request.Therefore, if our application runs into TombstoneOverwhelmingExceptions, then (sooner or later) our entire application cluster goes down :-(

*Proposed solution*
I think the data nodes should send a error message to the coordinator when they run into a TombstoneOverwhelmingException. Then the coordinator does not have to wait for the timeout-interval.

"
CASSANDRA-7836,Data loss after moving tokens,"The dtest topology_test:TestTopology.movement_test is failing on 2.1.0 and 2.1-HEAD. The test, which has been attached, goes through the following workflow:
- Create an unbalanced three node cluster without vnodes
- Create a keyspace with RF=1
- Load data into the cluster
- Flush the cluster with nodetool flush
- Move the tokens around to balance the cluster, using nodetool move
- Run nodetool cleanup

At this point, the test checks that all 10,000 of the originally inserted rows are still there. In 2.0.10 that is true, however in 2.1, only ~4000 rows exist."
CASSANDRA-7810,tombstones gc'd before being locally applied,"# single node environment
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };
use test;
create table foo (a int, b int, primary key(a,b));
alter table foo with gc_grace_seconds = 0;
insert into foo (a,b) values (1,2);
select * from foo;
-- one row returned. so far, so good.
delete from foo where a=1 and b=2;
select * from foo;
-- 0 rows. still rainbows and kittens.
bin/nodetool flush;
bin/nodetool compact;
select * from foo;
 a | b
---+---
 1 | 2

(1 rows)

gahhh.

looks like the tombstones were considered obsolete and thrown away before being applied to the compaction?  gc_grace just means the interval after which they won't be available to remote nodes repair - they should still apply locally regardless (and do correctly in 2.0.9)"
CASSANDRA-7808,LazilyCompactedRow incorrectly handles row tombstones,"LazilyCompactedRow doesn’t handle row tombstones correctly, leading to an AssertionError (CASSANDRA-4206) in some cases, and the row tombstone being incorrectly dropped in others. It looks like this was introduced by CASSANDRA-5677.

To reproduce an AssertionError:

1. Hack a really small return value for DatabaseDescriptor.getInMemoryCompactionLimit() like 10 bytes to force large row compaction
2. Create a column family with gc_grace = 10
3. Insert a few columns in one row
4. Call nodetool flush
5. Delete the row
6. Call nodetool flush
7. Wait 10 seconds
8. Call nodetool compact and it will fail

To reproduce the row tombstone being dropped, do the same except, after the delete (in step 5), insert a column that sorts before the ones you inserted in step 3. E.g. if you inserted b, c, d in step 3, insert a now. After the compaction, which now succeeds, the full row will be visible, rather than just a.

The problem is two fold. Firstly, LazilyCompactedRow.Reducer.reduce() and getReduce() incorrectly call container.clear(). This clears the columns (as intended) but also removes the deletion times from container. This means no further columns are deleted if they are annihilated by the row tombstone.

Secondly, after the second pass, LazilyCompactedRow.isEmpty() is called which calls

{{ColumnFamilyStore.removeDeletedCF(emptyColumnFamily, controller.gcBefore(key.getToken()))}}

which unfortunately removes the last deleted time from emptyColumnFamily if it is earlier than gcBefore. Since this is only called after the second pass, the second pass doesn’t remove any columns that are removed by the row tombstone whereas the first pass removes just the first one.

This is pretty serious - no large rows can ever be compacted and row tombstones can go missing."
CASSANDRA-7787,2i index indexing the cql3 row marker throws NPE,"If you have a secondary index implementation that 'indexes()' the cql3 row marker you get a NPE in SecondaryIndexManager/deleteFromIndexes() as the call to getColumnDefinitionFromColumnName() returns null which is not checked for.

This has been detected in the context of inserting PK only rows, where the row marker is expected to be present. When 'indexes()' returned 'false', the row would mistakenly get deleted as the row marker didn't go through.

If 'indexes()' returns 'true' the row marker goes through but you get a NPE."
CASSANDRA-7784,"DROP table leaves the counter and row cache in a temporarily inconsistent state that, if saved during, will cause an exception to be thrown","It looks like this is quite a realistic race to hit reasonably often, since we forceBlockingFlush after removing from Schema.cfIdMap, so there could be a lengthy window to overlap with an auto-save"
CASSANDRA-7782,NPE autosaving cache,"With the machine just sitting idle for a while:

{noformat}
INFO  18:33:35 Writing Memtable-sstable_activity@1889719059(162 serialized bytes, 72 ops, 0%/0% of on/off-heap limit)
INFO  18:33:35 Completed flushing /srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-3-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1408116815479, position=129971)
ERROR 19:33:34 Exception in thread Thread[CompactionExecutor:12,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:464) ~[main/:na]
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.serialize(CacheService.java:452) ~[main/:na]
        at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:225) ~[main/:na]
        at org.apache.cassandra.db.compaction.CompactionManager$11.run(CompactionManager.java:1053) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_65]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_65]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_65]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
INFO  19:33:35 Enqueuing flush of sstable_activity: 1561 (0%) on-heap, 0 (0%) off-heap
INFO  19:33:35 Writing Memtable-sstable_activity@1705670040(162 serialized bytes, 72 ops, 0%/0% of on/off-heap limit)
INFO  19:33:35 Completed flushing /srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-4-Data.db (177 bytes) for commitlog position ReplayPosition(segmentId=1408116815479, position=134711)
INFO  19:33:35 Compacting [SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-4-Data.db'), SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-2-Data.db'), SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-3-Data.db'), SSTableReader(path='/srv/cassandra/bin/../data/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-1-Data.db')]
{noformat}

Looks similar to CASSANDRA-7632"
CASSANDRA-7772,"Windows - fsync-analog, flush data to disk",We currently use CLibrary fsync linux-native calls to flush to disk.  Given the role this plays in our SequentialWriter and data integrity in general we need some analog to this function on Windows.
CASSANDRA-7770,NPE when clean up compaction leftover if table is already dropped,"As reported in CASSANDRA-7759, Directories throws NPE when trying to remove compaction leftovers on already dropped table.

Attaching patch to check if table exists in schema before clean up."
CASSANDRA-7767,Expose sizes of off-heap data structures via JMX and `nodetool info`,"It would be very helpful for troubleshooting memory consumption to know the individual sizes of off-heap data structures such as bloom filters, index summaries, compression metadata, etc. Can we expose this over JMX? Also, since `nodetool info` already shows size of heap, key cache, etc. it seems like a natural place to show this."
CASSANDRA-7750,"Do not flush on truncate if ""durable_writes"" is false.","CASSANDRA-7511 changed truncate so it will always flush to fix commit log issues.  If durable_writes is false, then there will not be able data in the commit log for the table, so we can safely just drop the memtables and not flush."
CASSANDRA-7743,Possible C* OOM issue during long running test,"During a long running test, we ended up with a lot of ""java.lang.OutOfMemoryError: Direct buffer memory"" errors on the Cassandra instances.

Here is an example of stacktrace from system.log :
{code}
ERROR [SharedPool-Worker-1] 2014-08-11 11:09:34,610 ErrorMessage.java:218 - Unexpected exception during request
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:658) ~[na:1.7.0_25]
        at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[na:1.7.0_25]
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306) ~[na:1.7.0_25]
        at io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:434) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:179) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PoolArena.allocate(PoolArena.java:168) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PoolArena.allocate(PoolArena.java:98) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:251) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:155) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:146) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:107) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.AdaptiveRecvByteBufAllocator$HandleImpl.allocate(AdaptiveRecvByteBufAllocator.java:104) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:112) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:507) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:464) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:378) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:350) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]
{code}

The test consisted of a 3-nodes cluster of n1-standard-1 GCE instances (1 vCPU, 3.75 GB RAM) running cassandra-2.1.0-rc5, and a n1-standard-2 instance running the test.

After ~2.5 days, several requests start to fail and we see the previous stacktraces in the system.log file.

The output from linux ‘free’ and ‘meminfo’ suggest that there is still memory available.

{code}
$ free -m
total              used       free     shared    buffers     cached
Mem:          3702       3532        169          0        161        854
-/+ buffers/cache:       2516       1185
Swap:            0          0          0

$ head -n 4 /proc/meminfo
MemTotal:        3791292 kB
MemFree:          173568 kB
Buffers:          165608 kB
Cached:           874752 kB
{code}

These errors do not affect all the queries we run. The cluster is still responsive but is unable to display tracing information using cqlsh :

{code}
$ ./bin/nodetool --host 10.240.137.253 status duration_test
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.240.98.27    925.17 KB  256     100.0%            41314169-eff5-465f-85ea-d501fd8f9c5e  RAC1
UN  10.240.137.253  1.1 MB     256     100.0%            c706f5f9-c5f3-4d5e-95e9-a8903823827e  RAC1
UN  10.240.72.183   896.57 KB  256     100.0%            15735c4d-98d4-4ea4-a305-7ab2d92f65fc  RAC1


$ echo 'tracing on; select count(*) from duration_test.ints;' | ./bin/cqlsh 10.240.137.253
Now tracing requests.

 count
-------
  9486

(1 rows)

Statement trace did not complete within 10 seconds
{code}

"
CASSANDRA-7709,sstable2json has resource leaks,"Coverity found 3 resource leak within  SSTable2JsonExporter:
** CID 71971:  Resource leak  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 300 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, java.util.Collection, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()

** CID 71972:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 332 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.SSTableReader,
java.io.PrintStream, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()

** CID 71975:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 236 in
org.apache.cassandra.tools.SSTableExport.enumeratekeys(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, org.apache.cassandra.config.CFMetaData)()


________________________________________________________________________________________________________
*** CID 71971:  Resource leak  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 300 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, java.util.Collection, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()
294                 i++;
295                 serializeRow(deletionInfo, atomIterator,
sstable.metadata, decoratedKey, outs);
296             }
297
298             outs.println(""\n]"");
299             outs.flush();
>>>     CID 71971:  Resource leak  (RESOURCE_LEAK)
>>>     Variable ""dfile"" going out of scope leaks the resource it refers to.
300         }
301
302         // This is necessary to accommodate the test suite since
you cannot open a Reader more
303         // than once from within the same process.
304         static void export(SSTableReader reader, PrintStream outs,
String[] excludes, CFMetaData metadata) throws IOException
305         {

________________________________________________________________________________________________________
*** CID 71972:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 332 in
org.apache.cassandra.tools.SSTableExport.export(org.apache.cassandra.io.sstable.SSTableReader,
java.io.PrintStream, java.lang.String[],
org.apache.cassandra.config.CFMetaData)()
326                 if (excludeSet.contains(currentKey))
327                     continue;
328                 else if (i != 0)
329                     outs.println("","");
330
331                 serializeRow(row, row.getKey(), outs);
>>>     CID 71972:  Resource leak on an exceptional path  (RESOURCE_LEAK)
>>>     Variable ""scanner"" going out of scope leaks the resource it refers to.
332                 checkStream(outs);
333
334                 i++;
335             }
336
337             outs.println(""\n]"");

________________________________________________________________________________________________________
*** CID 71975:  Resource leak on an exceptional path  (RESOURCE_LEAK)
/src/java/org/apache/cassandra/tools/SSTableExport.java: 236 in
org.apache.cassandra.tools.SSTableExport.enumeratekeys(org.apache.cassandra.io.sstable.Descriptor,
java.io.PrintStream, org.apache.cassandra.config.CFMetaData)()
230             while (iter.hasNext())
231             {
232                 DecoratedKey key = iter.next();
233
234                 // validate order of the keys in the sstable
235                 if (lastKey != null && lastKey.compareTo(key) > 0)
>>>     CID 71975:  Resource leak on an exceptional path  (RESOURCE_LEAK)
>>>     Variable ""iter"" going out of scope leaks the resource it refers to.
236                     throw new IOException(""Key out of order! "" +
lastKey + "" > "" + key);
237                 lastKey = key;
238
239
outs.println(metadata.getKeyValidator().getString(key.getKey()));
240                 checkStream(outs); // flushes
241             }

"
CASSANDRA-7701,Infinite flush loop,"If you run the repro from CASSANDRA-7695, you can get an infinite flush loop:

{noformat}
NFO  20:46:34 Compacting [SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1442-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1443-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1440-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1441-Data.db')]
INFO  20:46:34 Enqueuing flush of compactions_in_progress: 180 (0%) on-heap, 0 (0%) off-heap
INFO  20:46:34 Writing Memtable-compactions_in_progress@842537816(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  20:46:34 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-954-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1407269935245, position=12803233)
INFO  20:46:34 Compacted 4 sstables to [/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1444,].  9,664 bytes to 2,416 (~25% of original) in 37ms = 0.062272MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
INFO  20:46:35 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:35 Writing Memtable-put_test@418061108(512021 serialized bytes, 356 ops, 33%/0% of on/off-heap limit)
INFO  20:46:35 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1445-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935247, position=30214640)
INFO  20:46:37 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:37 Writing Memtable-put_test@1715686331(512021 serialized bytes, 348 ops, 33%/0% of on/off-heap limit)
INFO  20:46:37 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1446-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935250, position=19460288)
INFO  20:46:39 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:39 Writing Memtable-put_test@295716698(512021 serialized bytes, 344 ops, 33%/0% of on/off-heap limit)
INFO  20:46:39 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1447-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935253, position=7681712)
INFO  20:46:39 Enqueuing flush of compactions_in_progress: 1333 (0%) on-heap, 0 (0%) off-heap
INFO  20:46:39 Writing Memtable-compactions_in_progress@833384128(143 serialized bytes, 9 ops, 0%/0% of on/off-heap limit)
INFO  20:46:39 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-955-Data.db (166 bytes) for commitlog position ReplayPosition(segmentId=1407269935253, position=8194154)
INFO  20:46:39 Compacting [SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1447-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1446-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1445-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1444-Data.db')]
INFO  20:46:39 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-952-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-955-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-953-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-954-Data.db')]
INFO  20:46:39 Enqueuing flush of compactions_in_progress: 180 (0%) on-heap, 0 (0%) off-heap
INFO  20:46:39 Writing Memtable-compactions_in_progress@1180400490(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
INFO  20:46:39 Compacted 4 sstables to [/var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-956,].  462 bytes to 166 (~35% of original) in 44ms = 0.003598MB/s.  5 total partitions merged to 1.  Partition merge counts were {1:3, 2:1, }
INFO  20:46:39 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-957-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1407269935253, position=12803233)
INFO  20:46:39 Compacted 4 sstables to [/var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1448,].  9,664 bytes to 2,416 (~25% of original) in 68ms = 0.033883MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
INFO  20:46:40 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:40 Writing Memtable-put_test@553655581(512021 serialized bytes, 358 ops, 33%/0% of on/off-heap limit)
INFO  20:46:40 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1449-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935255, position=32775200)
INFO  20:46:42 Enqueuing flush of put_test: 87040444 (33%) on-heap, 0 (0%) off-heap
INFO  20:46:42 Writing Memtable-put_test@1630754028(512021 serialized bytes, 354 ops, 34%/0% of on/off-heap limit)
INFO  20:46:42 Completed flushing /var/lib/cassandra/data/cass_test/put_test-3252eb901cde11e4a736517bcdb23258/cass_test-put_test-ka-1450-Data.db (2416 bytes) for commitlog position ReplayPosition(segmentId=1407269935258, position=23557184)
{noformat}

This keeps going long after the run is complete.  Restarting the node stops the loop."
CASSANDRA-7685,Prepared marker for collections inside UDT do not handle null values,"The fix for CASSANDRA-7472 does not handle null values. The following causes an NPE to be thrown:

{code}
// CREATE TYPE phone (alias text, number text)
// CREATE TYPE address (street text, phones set<phone>)
// CREATE TABLE user (id int PRIMARY KEY, addr address)
PreparedStatement ps = session.prepare(""INSERT INTO user (id, addr) VALUES (1, { street: 'foo', phones: ? })"");
BoundStatement bs = ps.bind().setSet(0, null);
session.execute(bs);
{code}"
CASSANDRA-7684,flush makes rows invisible to cluster key equality query,"{noformat}
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };
USE test;
CREATE TYPE point_t (x double, y double);
CREATE TABLE points (partitionkey int, b boolean static, clusteringkey point_t, PRIMARY KEY (partitionkey, clusteringkey) );
INSERT INTO points (partitionkey, clusteringkey) VALUES (1, {x:-104.9925100000,y:39.7476520000});
select * from points WHERE partitionkey=1 AND clusteringkey = {x:-104.9925100000,y:39.7476520000};
 partitionkey | clusteringkey           | b
--------------+-------------------------+------
            1 | {x: -104.99, y: 39.748} | null
(1 rows)
cqlsh:test> update points set b = true where partitionkey=1;
cqlsh:test> select * from points WHERE partitionkey=1 AND clusteringkey = {x:-104.9925100000,y:39.7476520000};
 partitionkey | clusteringkey           | b
--------------+-------------------------+------
            1 | {x: -104.99, y: 39.748} | True
(1 rows)
{noformat}

// run bin/nodetool flush here

{noformat}
cqlsh:test> select * from points WHERE partitionkey=1 AND clusteringkey = {x:-104.9925100000,y:39.7476520000};
(0 rows)
cqlsh:test> select * from points WHERE partitionkey=1;
 partitionkey | clusteringkey           | b
--------------+-------------------------+------
            1 | {x: -104.99, y: 39.748} | True
(1 rows)
{noformat}

i.e. the data is not lost, it's just invisible when read from sstable, but visible when read from memtable.


"
CASSANDRA-7670,selecting field from empty UDT cell using dot notation triggers exception,"Hopefully the title explains the issue.

Basically if a row has a user type which is undefined, using the ""dot notation"" syntax causes cqlsh to show a NPE, and an exception ends up in the log as well.

I would expect the behavior here to execute the query and return null, since the user type itself is null, then it's fields are null as well (the downside here is there would be no way to distinguish between a null UDT and a null UDT field, without making an initial query to check if the UDT is non-null).

Steps to repro:
{noformat}
create keyspace test with replication = {'class':'SimpleStrategy', 'replication_factor':1};

use test;

CREATE TYPE t_item (subitem text);

create table mytable (value1 text PRIMARY KEY, value2 text, item t_item);

insert into mytable (value1, value2 ) values ('foo', 'bar');

select item.subitem from mytable;
<ErrorMessage code=0000 [Server error] message=""java.lang.NullPointerException"">
{noformat}
Here's the exception logged:
{noformat}
ERROR [SharedPool-Worker-1] 2014-08-01 15:49:00,514 ErrorMessage.java:218 - Unexpected exception during request
java.lang.NullPointerException: null
        at org.apache.cassandra.db.marshal.TupleType.split(TupleType.java:146) ~[main/:na]
        at org.apache.cassandra.cql3.statements.Selection$FieldSelector.compute(Selection.java:469) ~[main/:na]
        at org.apache.cassandra.cql3.statements.Selection$SelectionWithFunctions.handleRow(Selection.java:537) ~[main/:na]
        at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.build(Selection.java:333) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:1149) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:283) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:225) ~[main/:na]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:60) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:187) ~[main/:na]
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:206) ~[main/:na]
        at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:118) ~[main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:421) [main/:na]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:318) [main/:na]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:103) [netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:332) [netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:31) [netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:323) [netty-all-4.0.20.Final.jar:4.0.20.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) [na:1.7.0_65]
        at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:163) [main/:na]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:103) [main/:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
{noformat}"
CASSANDRA-7640,Seeing commit log grow unbounded,"While running 2.0.6 saw the commit log size grow up to 30 GB before restarting the node, with the max set to 1 GB.

With DEBUG logging on CommitLog* we see segments are not being removed because they are still dirty.  But for some reason the force flush when the active size is too large isn't happening (The active size gets printed out, and is 20+GB).  The logs show normal ""high volume"" flushes happening, and force flushes of system.* stuff happening.  But there are no flushes occurring for the memtables that are keeping the segments dirty.

Upon restart the commit log replays fine with no errors.  There are no errors from flushing to the post flusher.  tpstats for both show 0 active 0 pending at times during.

The only config change around this is that the segment size is larger than normal, it is set to 128 MB, as there are some very large columns being inserted (30+MB)."
CASSANDRA-7638,Revisit GCInspector,"In CASSANDRA-2868 we had to change the api that GCI uses to avoid the native memory leak, but this caused GCI to be less reliable and more 'best effort' than before where it was 100% reliable.  Let's revisit this and see if the native memory leak is fixed in java7."
CASSANDRA-7632,NPE in AutoSavingCache$Writer.deleteOldCacheFiles,"Observed this NPE in one of our production cluster (2.0.9). Does not seem to be causing harm but good to resolve.

ERROR [CompactionExecutor:1188] 2014-07-27 21:57:08,225 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:1188,1,main] 
clusterName=clouddb_p03 
java.lang.NullPointerException 
at org.apache.cassandra.cache.AutoSavingCache$Writer.deleteOldCacheFiles(AutoSavingCache.java:265) 
at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:195) 
at org.apache.cassandra.db.compaction.CompactionManager$10.run(CompactionManager.java:862) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334) 
at java.util.concurrent.FutureTask.run(FutureTask.java:166) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603) 
at java.lang.Thread.run(Thread.java:722)"
CASSANDRA-7621,Use jvm server machine code compiler,"As far as I can tell, Cassandra is using the jvm's default client compiler. The attached patch adds a line to cassandra-env.sh to tell it to use the jvm's server compiler. The server compiler is meant for long running processes and is more aggressive in it's optimizations to the machine code it generates, at the expense of machine code size and initial compilation speed.

I've run {{cassandra-stress  --num-keys=2000000 -K 50}} and {{cassandra-stress  --num-keys=2000000 -K 50 --operation=READ}} with both settings, and the runs with the server flag are 2-5% faster on average on my Macbook Pro, with a 2.3GHz i7, 16GB of memory and an SSD.

http://www.oracle.com/technetwork/java/whitepaper-135217.html#2"
CASSANDRA-7608,StressD can't create keyspaces with Write Command,"It is impossible to run the default stress command via the dameon ./stress write
Because the column names are HeapByteBuffers so they get ignored during serilization (no error is thrown) and then when the object is deserialized on the server the settings.columns.names is null. This leads to a null pointer on the dameon for what would have worked had it run locally.

Settings object on the Local machine
{code}
columns = {org.apache.cassandra.stress.settings.SettingsColumn@1465}
maxColumnsPerKey = 5
names = {java.util.Arrays$ArrayList@1471} size = 5
[0] = {java.nio.HeapByteBuffer@1478}""java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]""
[1] = {java.nio.HeapByteBuffer@1483}""java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]""
[2] = {java.nio.HeapByteBuffer@1484}""java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]""
[3] = {java.nio.HeapByteBuffer@1485}""java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]""
[4] = {java.nio.HeapByteBuffer@1486}""java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]""
{code}

Setings object on the StressD Machine
{code}
columns = {org.apache.cassandra.stress.settings.SettingsColumn@810}
maxColumnsPerKey = 5
names = null
{code}


This leads to the null pointer in 
{code}
Exception in thread ""Thread-1"" java.lang.NullPointerException
    at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpacesThrift(SettingsSchema.java:94)
    at org.apache.cassandra.stress.settings.SettingsSchema.createKeySpaces(SettingsSchema.java:67)
    at org.apache.cassandra.stress.settings.StressSettings.maybeCreateKeyspaces(StressSettings.java:193)
    at org.apache.cassandra.stress.StressAction.run(StressAction.java:59)
    at java.lang.Thread.run(Thread.java:745)
{code}

Which refers to
{code}
       for (int i = 0; i < settings.columns.names.size(); i++)
            standardCfDef.addToColumn_metadata(new ColumnDef(settings.columns.names.get(i), ""BytesType""));
{code}


Possible solution:
Just use the settings.columns.namestr and convert them to byte buffers at this point in the code. "
CASSANDRA-7601,Data loss after nodetool taketoken,"The dtest consistent_bootstrap_test.py:TestBootstrapConsistency.consistent_reads_after_relocate_test is failing on HEAD of the git branches 2.1 and 2.1.0.

The test performs the following actions:
- Create a cluster of 3 nodes
- Create a keyspace with RF 2
- Take node 3 down
- Write 980 rows to node 2 with CL ONE
- Flush node 2
- Bring node 3 back up
- Run nodetool taketoken on node 3 to transfer 80% of node 1's tokens to node 3
- Check for data loss

When the check for data loss is performed, only ~725 rows can be read via CL ALL."
CASSANDRA-7554,Make CommitLogSegment sync/close asynchronous wrt each other,"There are a few minor issues with CLS I wanted to tidy up after working on nearby code a bit recently, namely:

1) We use synchronized() for sync() and for various minor accessors, meaning either can block on the other, which is bad since sync() is lengthy
2) Currently close() (and hence recycle()) must wait for a sync() to complete, which means even if we have room available in segments waiting to be recycled an ongoing sync might prevent us from reclaiming the space, prematurely bottlenecking on the disk here
3) recycle() currently depends on close(), which depends on sync(); if we've decided to recycle/close a file before it is synced, this means we do not care about the contents so can actually _avoid_ syncing to disk (which is great in cases where the flush writers get ahead of the CL sync)

To solve these problems I've introduced a new fairly simple concurrency primitive called AsyncLock, which only supports tryLock(), or tryLock(Runnable) - with the latter executing the provided runnable on the thread _currently owning the lock_ after it relinquishes it. I've used this to make close() take a Runnable to be executed _when the segment is actually ready to be disposed of_ - which is either immediately, or once any in progress sync has completed. This means the manager thread never blocks on a sync.

There is a knock on effect here, which is that we are even less inclined to obey the CL limit (which has always been a soft limit), so I will file a separate minor ticket to introduce a hard limit for CL size in case users want to control this."
CASSANDRA-7546,AtomicSortedColumns.addAllWithSizeDelta has a spin loop that allocates memory,"In order to preserve atomicity, this code attempts to read, clone/update, then CAS the state of the partition.

Under heavy contention for updating a single partition this can cause some fairly staggering memory growth (the more cores on your machine the worst it gets).

Whilst many usage patterns don't do highly concurrent updates to the same partition, hinting today, does, and in this case wild (order(s) of magnitude more than expected) memory allocation rates can be seen (especially when the updates being hinted are small updates to different partitions which can happen very fast on their own) - see CASSANDRA-7545

It would be best to eliminate/reduce/limit the spinning memory allocation whilst not slowing down the very common un-contended case."
CASSANDRA-7518,The In-Memory option,"There is an In-Memory option introduced in the commercial version of Cassandra by DataStax Enterprise 4.0:
http://www.datastax.com/documentation/datastax_enterprise/4.0/datastax_enterprise/inMemory.html
But with 1GB size limited for an in-memory table.

It would be great if the In-Memory option can be available to the community version of Cassandra, and extend to a large size of in-memory table, such as 64GB.
"
CASSANDRA-7507,OOM creates unreliable state - die instantly better,"I had a cassandra node run OOM. My heap had enough headroom, there was just something which either was a bug or some unfortunate amount of short-term memory utilization. This resulted in the following error:

 WARN [StorageServiceShutdownHook] 2014-06-30 09:38:38,251 StorageProxy.java (line 1713) Some hints were not written before shutdown.  This is not supposed to happen.  You should (a) run repair, and (b) file a bug report

There are no other messages of relevance besides the OOM error about 90 minutes earlier.

My (limited) understanding of the JVM and Cassandra says that when it goes OOM, it will attempt to signal cassandra to shut down ""cleanly"". The problem, in my view, is that with an OOM situation, nothing is guaranteed anymore. I believe it's impossible to reliably ""cleanly shut down"" at this point, and therefore it's wrong to even try. 

Yes, ideally things could be written out, flushed to disk, memory messages written, other nodes notified, etc. but why is there any reason to believe any of those steps could happen? Would happen? Couldn't bad data be written at this point to disk rather than good data? Some network messages delivered, but not others?

I think Cassandra should have the option to (and possibly default) to kill itself immediately upon the OOM condition happening in a hard way, and not rely on the java-based clean shutdown process. Cassandra already handles recovery from unclean shutdown, and it's not a big deal. My node, for example, kept in a sort-of alive state for 90 minutes where who knows what it was doing or not doing.

I don't know enough about the JVM and options for it to know the best exact implementation of ""die instantly on OOM"", but it should be something that's possible either with some flags or a C library (which doesn't rely on java memory to do something which it may not be able to get!)

Short version: a kill -9 of all C* processes in that instance without needing more java memory, when OOM is raised"
CASSANDRA-7503,Windows - better communication on start-up failure,"The Windows launch scripts report successful launch on certain failure conditions.  For instance, if you try to start a 2nd instance of Cassandra and it fails to bind to the JMX port:

{code:title=silent failure}
D:\src\cassandra\bin>cassandra.bat
Detected powershell execution permissions.  Running with enhanced startup scripts.
Setting up Cassandra environment
Starting cassandra server
Started cassandra successfully with pid: 6520
{code}

{code:title=-f failure}
D:\src\cassandra\bin>cassandra.bat -f
Detected powershell execution permissions.  Running with enhanced startup scripts.
Setting up Cassandra environment
Starting cassandra server
Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 7199; nested exception is:
        java.net.BindException: Address already in use: JVM_Bind
{code}

Reference CASSANDRA-7254 for linux-based solution to a similar NPE on startup due to duplicate JMX port-bind."
CASSANDRA-7497,Cassandra crash on startup,"I´m installing Cassandra 2.0.9 from scratch and when I startup it with cassandra -f doesn´t work and I obtain this info:

paradise@MASK1:~$ cassandra -f
 INFO 01:28:09,095 Logging initialized
 INFO 01:28:09,134 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 01:28:09,475 Data files directories: [/var/lib/cassandra/data]
 INFO 01:28:09,477 Commit log directory: /var/lib/cassandra/commitlog
 INFO 01:28:09,477 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 01:28:09,477 disk_failure_policy is stop
 INFO 01:28:09,477 commit_failure_policy is stop
 INFO 01:28:09,482 Global memtable threshold is enabled at 1988MB
 INFO 01:28:09,600 Not using multi-threaded compaction
 INFO 01:28:09,832 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_60
 INFO 01:28:09,832 Heap size: 8339324928/8340373504
 INFO 01:28:09,832 Code Cache Non-heap memory: init = 2555904(2496K) used = 662528(647K) committed = 2555904(2496K) max = 50331648(49152K)
 INFO 01:28:09,832 Par Eden Space Heap memory: init = 671088640(655360K) used = 120827640(117995K) committed = 671088640(655360K) max = 671088640(655360K)
 INFO 01:28:09,833 Par Survivor Space Heap memory: init = 83886080(81920K) used = 0(0K) committed = 83886080(81920K) max = 83886080(81920K)
 INFO 01:28:09,833 CMS Old Gen Heap memory: init = 7584350208(7406592K) used = 0(0K) committed = 7584350208(7406592K) max = 7585398784(7407616K)
 INFO 01:28:09,833 CMS Perm Gen Non-heap memory: init = 21757952(21248K) used = 14340320(14004K) committed = 21757952(21248K) max = 85983232(83968K)
 INFO 01:28:09,837 Classpath: /etc/cassandra:/usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang3-3.1.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.3.jar:/usr/share/cassandra/lib/disruptor-3.0.1.jar:/usr/share/cassandra/lib/guava-15.0.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.9.2.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jbcrypt-0.3m.jar:/usr/share/cassandra/lib/jline-1.0.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.9.1.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/lz4-1.2.0.jar:/usr/share/cassandra/lib/metrics-core-2.2.0.jar:/usr/share/cassandra/lib/netty-3.6.6.Final.jar:/usr/share/cassandra/lib/reporter-config-2.1.0.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.7.2.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.7.2.jar:/usr/share/cassandra/lib/snakeyaml-1.11.jar:/usr/share/cassandra/lib/snappy-java-1.0.5.jar:/usr/share/cassandra/lib/snaptree-0.1.jar:/usr/share/cassandra/lib/super-csv-2.1.0.jar:/usr/share/cassandra/lib/thrift-server-internal-only-0.3.3.jar:/usr/share/cassandra/apache-cassandra-2.0.9.jar:/usr/share/cassandra/apache-cassandra-thrift-2.0.9.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/cassandra/stress.jar::/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 01:28:09,839 JNA not found. Native methods will be disabled.
 INFO 01:28:09,855 Initializing key cache with capacity of 100 MBs.
 INFO 01:28:09,865 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 01:28:09,865 Initializing row cache with capacity of 0 MBs
 INFO 01:28:09,872 Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO 01:28:10,028 Initializing system.schema_triggers
ERROR 01:28:10,051 Exception encountered during startup
java.lang.RuntimeException: Incompatible SSTable found.  Current version jb is unable to read file: /var/lib/cassandra/data/system/compaction_history/system-compaction_history-ka-1.  Please run upgradesstables.
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:415)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:315)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:272)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:114)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:92)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:536)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:261)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:496)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:585)
java.lang.RuntimeException: Incompatible SSTable found.  Current version jb is unable to read file: /var/lib/cassandra/data/system/compaction_history/system-compaction_history-ka-1.  Please run upgradesstables.
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:415)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:315)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:272)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:114)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:92)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:536)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:261)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:496)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:585)
Exception encountered during startup: Incompatible SSTable found.  Current version jb is unable to read file: /var/lib/cassandra/data/system/compaction_history/system-compaction_history-ka-1.  Please run upgradesstables.

The /var/lib/cassandra/data/system/compaction_history/system-compaction_history-ka-1 file not exist and is imposible to run nodetool to do ugradesstables.

"
CASSANDRA-7484,Throw exception on unknown UDT field,"Currently, the following code:
{noformat}
CREATE TYPE foo (f : int);
CREATE TABLE test (k int PRIMARY KEY, v foo);

INSERT INTO test (k, v) VALUES (0, { s : ?})
{noformat}
will crash, because the {{s}} field is not part of type {{foo}} and it's not caught. The consequence being that the metadata for the bindMarker ends up being {{null}} and some NPE is thrown. We should throw a proper exception instead."
CASSANDRA-7477,JSON to SSTable import failing,"Issue affects C* version >= 2.1. Commit found by using git bisect. The previous commit to this one also fails, but due to other reasons (CCM server won't start). This commit is the one that give the same error as 2.1 HEAD:

{noformat}
02d1e7497a9930120fac367ce82a3b22940acafb is the first bad commit
commit 02d1e7497a9930120fac367ce82a3b22940acafb
Author: Brandon Williams <brandonwilliams@apache.org>
Date:   Mon Apr 21 14:42:29 2014 -0500

    Default flush dir to data dir.
    Patch by brandonwilliams, reviewed by yukim for CASSANDRA-7064

:040000 040000 c50a123f305b73583ccbfa9c455efc4e4cee228f 507a90290dccb8a929afadf1f833d926049c46ad M	conf
{noformat}

{noformat}
$ PRINT_DEBUG=true nosetests -x -s -v json_tools_test.py 
json_tools_test (json_tools_test.TestJson) ... cluster ccm directory: /tmp/dtest-8WVBq9
Starting cluster...
Version: 2.1.0
Getting CQLSH...
Inserting data...
Flushing and stopping cluster...
Exporting to JSON file...
-- test-users-ka-1-Data.db -----

Deleting cluster and creating new...
Inserting data...
Importing JSON file...
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
Importing 2 keys...
java.lang.ClassCastException: org.apache.cassandra.db.composites.Composites$EmptyComposite cannot be cast to org.apache.cassandra.db.composites.CellName
	at org.apache.cassandra.db.composites.AbstractCellNameType.cellFromByteBuffer(AbstractCellNameType.java:168)
	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:165)
	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:242)
	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:225)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:464)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:351)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:575)
ERROR: org.apache.cassandra.db.composites.Composites$EmptyComposite cannot be cast to org.apache.cassandra.db.composites.CellName
Verifying import...
data: [[u'gandalf', 1955, u'male', u'p@$$', u'WA']]
FAIL
removing ccm cluster test at: /tmp/dtest-8WVBq9
ERROR

======================================================================
ERROR: json_tools_test (json_tools_test.TestJson)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/kishan/git/cstar/cassandra-dtest/dtest.py"", line 214, in tearDown
    raise AssertionError('Unexpected error in %s node log: %s' % (node.name, errors))
AssertionError: Unexpected error in node1 node log: ['ERROR [SSTableBatchOpen:1] 2014-06-30 13:56:01,032 CassandraDaemon.java:166 - Exception in thread Thread[SSTableBatchOpen:1,5,main]\n']
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-8WVBq9
dtest: DEBUG: Starting cluster...
dtest: DEBUG: Version: 2.1.0
dtest: DEBUG: Getting CQLSH...
dtest: DEBUG: Inserting data...
dtest: DEBUG: Flushing and stopping cluster...
dtest: DEBUG: Exporting to JSON file...
dtest: DEBUG: Deleting cluster and creating new...
dtest: DEBUG: Inserting data...
dtest: DEBUG: Importing JSON file...
dtest: DEBUG: Verifying import...
dtest: DEBUG: data: [[u'gandalf', 1955, u'male', u'p@$$', u'WA']]
dtest: DEBUG: removing ccm cluster test at: /tmp/dtest-8WVBq9
--------------------- >> end captured logging << ---------------------

======================================================================
FAIL: json_tools_test (json_tools_test.TestJson)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/kishan/git/cstar/cassandra-dtest/json_tools_test.py"", line 91, in json_tools_test
    [u'gandalf', 1955, u'male', u'p@$$', u'WA'] ] )
AssertionError: Element counts were not equal:
First has 0, Second has 1:  [u'frodo', 1985, u'male', u'pass@', u'CA']
First has 0, Second has 1:  [u'sam', 1980, u'male', u'@pass', u'NY']
-------------------- >> begin captured logging << --------------------
dtest: DEBUG: cluster ccm directory: /tmp/dtest-8WVBq9
dtest: DEBUG: Starting cluster...
dtest: DEBUG: Version: 2.1.0
dtest: DEBUG: Getting CQLSH...
dtest: DEBUG: Inserting data...
dtest: DEBUG: Flushing and stopping cluster...
dtest: DEBUG: Exporting to JSON file...
dtest: DEBUG: Deleting cluster and creating new...
dtest: DEBUG: Inserting data...
dtest: DEBUG: Importing JSON file...
dtest: DEBUG: Verifying import...
dtest: DEBUG: data: [[u'gandalf', 1955, u'male', u'p@$$', u'WA']]
--------------------- >> end captured logging << ---------------------

----------------------------------------------------------------------
Ran 1 test in 28.636s

FAILED (errors=1, failures=1)
{noformat}"
CASSANDRA-7438,Serializing Row cache alternative (Fully off heap),"Currently SerializingCache is partially off heap, keys are still stored in JVM heap as BB, 

* There is a higher GC costs for a reasonably big cache.
* Some users have used the row cache efficiently in production for better results, but this requires careful tunning.
* Overhead in Memory for the cache entries are relatively high.

So the proposal for this ticket is to move the LRU cache logic completely off heap and use JNI to interact with cache. We might want to ensure that the new implementation match the existing API's (ICache), and the implementation needs to have safe memory access, low overhead in memory and less memcpy's (As much as possible).

We might also want to make this cache configurable."
CASSANDRA-7437,"Ensure writes have completed after dropping a table, before recycling commit log segments (CASSANDRA-7437)","I've noticed on unit test output that there are still assertions being raised here, so I've taken a torch to the code path to make damned certain it cannot happen in future 

# We now wait for all running reads on a column family or writes on the keyspace during a dropCf call
# We wait for all appends to the prior commit log segments before recycling them
# We pass the list of dropped Cfs into the CL.forceRecycle call so that they can be markedClean definitely after they have been marked finished
# Finally, to prevent any possibility of this still happening causing any negative consequences, I've suppressed the assertion in favour of an error log message, as the assertion would break correct program flow for the drop and potentially result in undefined behaviour

-(in actuality there is the slightest possibility still of a race condition on read of a secondary index that causes a repair driven write, but this is a really tiny race window, as I force wait for all reads after unlinking the CF, so it would have to be a read that grabbed the CFS reference before it was dropped, but hadn't quite started its read op yet).- In fact this is also safe, as these modifications all grab a write op from the Keyspace, which has to happen before they get the CFS, and also because we drop the data before waiting for reads to finish on the CFS."
CASSANDRA-7365,some compactions do not works under windows (file in use during rename),"compaction do not works under windows due to file rename fails: (Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces = process can not access file because its in use by another process). Not all compactions are broken. compactions done during server startup on system tables works fine.

INFO  18:30:27 Completed flushing c:\cassandra-2.1\data\system\compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-6-Dat.db (42 bytes) for commitlog position ReplayPosition(segmentId=1402165543361, psition=8024611)
ERROR 18:30:27 Exception in thread hread[CompactionExecutor:5,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted
INFO  18:30:27 Compacting [SSTableReader(path='c:\cassandra-2.1\data\system\com
actions_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_prog
ess-ka-3-Data.db'), SSTableReader(path='c:\cassandra-2.1\data\system\compaction
_in_progress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka
5-Data.db'), SSTableReader(path='c:\cassandra-2.1\data\system\compactions_in_pr
gress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-4-Data
db'), SSTableReader(path='c:\cassandra-2.1\data\system\compactions_in_progress-
5080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-6-Data.db')]
ERROR 18:30:27 Exception in thread Thread[CompactionExecutor:5,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-7-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-7-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted
INFO  18:30:27 Compacted 4 sstables to [].  423 bytes to 0 (~0% of original) in
39ms = 0,000000MB/s.  4 total partitions merged to 0.  Partition merge counts w
re {2:2, }
INFO  18:30:45 Enqueuing flush of compactions_in_progress: 1345 (0%) on-heap, 0
(0%) off-heap
INFO  18:30:45 Writing Memtable-compactions_in_progress@15659113(153 serialized
bytes, 10 ops, 0%/0% of on/off-heap limit)
INFO  18:30:45 Completed flushing c:\cassandra-2.1\data\system\compactions_in_p
ogress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-8-Dat
.db (173 bytes) for commitlog position ReplayPosition(segmentId=1402165543361,
osition=8025407)
INFO  18:30:45 Compacting [SSTableReader(path='c:\cassandra-2.1\data\test\sipdb
58f51090ee6511e3815625991ef2b954\test-sipdb-ka-3-Data.db'), SSTableReader(path=
c:\cassandra-2.1\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka
1-Data.db'), SSTableReader(path='c:\cassandra-2.1\data\test\sipdb-58f51090ee651
e3815625991ef2b954\test-sipdb-ka-4-Data.db'), SSTableReader(path='c:\cassandra-
.1\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-2-Data.db'),
STableReader(path='c:\cassandra-2.1\data\test\sipdb-58f51090ee6511e3815625991ef
b954\test-sipdb-ka-6-Data.db')]
ERROR 18:31:41 Unable to delete c:\cassandra-2.1\data\test\sipdb-58f51090ee6511
3815625991ef2b954\test-sipdb-ka-8-Data.db (it will be removed on server restart
 we'll also retry after GC)
ERROR 18:31:41 Missing component: c:\cassandra-2.1\data\test\sipdb-58f51090ee65
1e3815625991ef2b954\test-sipdb-tmp-ka-8-Digest.sha1
ERROR 18:31:41 Missing component: c:\cassandra-2.1\data\test\sipdb-58f51090ee65
1e3815625991ef2b954\test-sipdb-tmp-ka-8-Summary.db
ERROR 18:31:41 Missing component: c:\cassandra-2.1\data\test\sipdb-58f51090ee65
1e3815625991ef2b954\test-sipdb-tmp-ka-8-Statistics.db
INFO  18:31:41 Enqueuing flush of compactions_in_progress: 148 (0%) on-heap, 0
0%) off-heap
INFO  18:31:41 Writing Memtable-compactions_in_progress@6888852(0 serialized by
es, 1 ops, 0%/0% of on/off-heap limit)
INFO  18:31:41 Completed flushing c:\cassandra-2.1\data\system\compactions_in_p
ogress-55080ab05d9c388690a4acb25fe1f77b\system-compactions_in_progress-ka-9-Dat
.db (42 bytes) for commitlog position ReplayPosition(segmentId=1402165543361, p
sition=8025563)
ERROR 18:31:41 Exception in thread Thread[CompactionExecutor:6,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted
ERROR 18:31:41 Exception in thread Thread[CompactionExecutor:6,1,RMI Runtime]
java.lang.RuntimeException: Failed to rename c:\cassandra-2.1\data\test\sipdb-5
f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db to c:\cassandra-2.1
data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:167) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:151) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:512) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.j
va:504) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.close(SSTableWriter.ja
a:479) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:427) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SST
bleWriter.java:422) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:312) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.sstable.SSTableRewriter.finish(SSTableRewrit
r.java:306) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(Compaction
ask.java:188) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAware
unnable.java:48) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:
8) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(Co
pactionTask.java:74) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(Ab
tractCompactionTask.java:59) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompa
tionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0
rc1]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:4
1) ~[na:1.7.0_60]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_
0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor
java:1145) ~[na:1.7.0_60]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecuto
.java:615) [na:1.7.0_60]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.nio.file.FileSystemException: c:\cassandra-2.1\data\test\sipdb-
8f51090ee6511e3815625991ef2b954\test-sipdb-tmp-ka-8-Index.db -> c:\cassandra-2.
\data\test\sipdb-58f51090ee6511e3815625991ef2b954\test-sipdb-ka-8-Index.db: Pro
es nemß p°Ýstup k souboru, neboŁ jej prßvý vyu×Ývß jinř proces.

        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.
ava:86) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.ja
a:97) ~[na:1.7.0_60]
        at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301) ~[na:1.7.0
60]
        at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.
ava:287) ~[na:1.7.0_60]
        at java.nio.file.Files.move(Files.java:1347) ~[na:1.7.0_60]
        at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUt
ls.java:181) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.j
va:163) ~[apache-cassandra-2.1.0-rc1.jar:2.1.0-rc1]
        ... 19 common frames omitted"
CASSANDRA-7360,CQLSSTableWriter consumes all memory for table with compound primary key,"When using CQLSSTableWriter to write a table with compound primary key, if the partition key is identical for a huge amount of records, the sync() method is never called, and the memory usage keeps growing until the memory is exhausted. 

Could the code be improved to do sync() even when there is no new row  created? The relevant code is in SSTableSimpleUnsortedWriter.java and AbstractSSTableSimpleWriter.java. I am new to the code and cannot produce a reasonable patch for now.

The problem can be reproduced by the following test case:
{code}
import org.apache.cassandra.io.sstable.CQLSSTableWriter;
import org.apache.cassandra.exceptions.InvalidRequestException;

import java.io.IOException;
import java.util.UUID;

class SS {
    public static void main(String[] args) {
        String schema = ""create table test.t (x uuid, y uuid, primary key (x, y))"";


        String insert = ""insert into test.t (x, y) values (?, ?)"";
        CQLSSTableWriter writer = CQLSSTableWriter.builder()
            .inDirectory(""/tmp/test/t"")
            .forTable(schema).withBufferSizeInMB(32)
            .using(insert).build();

        UUID id = UUID.randomUUID();
        try {
            for (int i = 0; i < 50000000; i++) {
                UUID id2 = UUID.randomUUID();
                writer.addRow(id, id2);
            }

            writer.close();
        } catch (Exception e) {
            System.err.println(""hell"");
        }
    }
}
{code}"
CASSANDRA-7353,Java heap being set too large on Windows with 32-bit JVM,"On windows, the JVM settings for max heap size and new gen heap size are set based on the total system memory. When the system has 8G of RAM, the max heap size is set to 2048M. However, according to http://goo.gl/1ElbLm, the recommended max heap for a 32 bit JVM on Windows is 1.8G.

When cassandra is started on Windows under these conditions, the following error is seen:

Error occurred during initialization of VM
Could not reserve enough space for object heap
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

Switching to a 64-bit JVM on the same machine solves the issue. If a 32-bit JVM is being used, cassandra should be started up with a smaller heap than would be normally used to prevent the error."
CASSANDRA-7323,NPE in StreamTransferTask.createMessageForRetry,"seeing this NPE during repair:
at org.apache.cassandra.streaming.StreamTransferTask.createMessageForRetry(StreamTransferTask.java:106)
at org.apache.cassandra.streaming.StreamSession.retry(StreamSession.java:525)
at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:401)
at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:289)
at java.lang.Thread.run(Thread.java:744)"
CASSANDRA-7269,Make CqlInputFormat and CqlRecordReader consistent with comments,"Both the CqlInputFormat and CqlPagingInputFormat have the following comment:

{code}
/**
...
 *   the number of CQL rows per page
 *   CQLConfigHelper.setInputCQLPageRowSize. The default page row size is 1000. You 
 *   should set it to ""as big as possible, but no bigger."" It set the LIMIT for the CQL 
 *   query, so you need set it big enough to minimize the network overhead, and also
 *   not too big to avoid out of memory issue.
...
**/
{code}

The property is used in both classes, but the default is only set to 1000 in CqlPagingRecordReader explicitly.

We should either make the default part of the CqlConfigHelper so it's set in both places or update the comments in the CqlInputFormat to say that if it's not set, it will default to the java driver fetch size which is 5000."
CASSANDRA-7235,"ColumnStats min/max column names does not account for range thombstones, making deleted data resurrect at random","This issue can be reproduced in cqlsh:
{code}
create table test1( a int, b int, primary key(a,b));
INSERT INTO test1(a,b) values (1,1);
INSERT INTO test1(a,b) values (1,2);
INSERT INTO test1(a,b) values (1,3);
INSERT INTO test1(a,b) values (1,4);
INSERT INTO test1(a,b) values (1,5);
INSERT INTO test1(a,b) values (1,6);
-- flush memtable here
delete from test1 where a=1 and b=6;
INSERT INTO test1(a,b) values (2,2);
-- flush memtable here

select * from test1 where a=1 and b=6;

 a | b
---+---
 1 | 6

{code}

Voila!

The problem is in columnStats accounting for min and max columns names. range tombstones are not accounted there, so sstables get omitted on slice query read, if they have only range thombsones in some range.

Also while debugged this found a problem in LazilyCompactedRow: RangeThombstones processing of timestamps histogram were implemented wrong in CASSANDRA-6522 - no range thobmstones are actuallu accessible in deletionInfo().rangeIterator(), so fixed it as well.
"
CASSANDRA-7177,Starting threads in the OutboundTcpConnectionPool constructor causes race conditions,"The OutboundTcpConnectionPool starts connection threads in its constructor, causing race conditions when MessagingService#getConnectionPool is concurrently called for the first time for a given address.

I.e., here's one of the races:
{noformat}
 WARN 12:49:03,182 Error processing org.apache.cassandra.metrics:type=Connection,scope=127.0.0.1,name=CommandPendingTasks
javax.management.InstanceAlreadyExistsException: org.apache.cassandra.metrics:type=Connection,scope=127.0.0.1,name=CommandPendingTasks
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at com.yammer.metrics.reporting.JmxReporter.registerBean(JmxReporter.java:464)
	at com.yammer.metrics.reporting.JmxReporter.processGauge(JmxReporter.java:438)
	at com.yammer.metrics.reporting.JmxReporter.processGauge(JmxReporter.java:16)
	at com.yammer.metrics.core.Gauge.processWith(Gauge.java:28)
	at com.yammer.metrics.reporting.JmxReporter.onMetricAdded(JmxReporter.java:395)
	at com.yammer.metrics.core.MetricsRegistry.notifyMetricAdded(MetricsRegistry.java:516)
	at com.yammer.metrics.core.MetricsRegistry.getOrAdd(MetricsRegistry.java:491)
	at com.yammer.metrics.core.MetricsRegistry.newGauge(MetricsRegistry.java:79)
	at com.yammer.metrics.Metrics.newGauge(Metrics.java:70)
	at org.apache.cassandra.metrics.ConnectionMetrics.<init>(ConnectionMetrics.java:71)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.<init>(OutboundTcpConnectionPool.java:55)
	at org.apache.cassandra.net.MessagingService.getConnectionPool(MessagingService.java:498)
{noformat}"
CASSANDRA-7155,"Followup to 6914: null handling, duplicate column in resultSet and cleanup","The patch for CASSANDRA-6914 left a few stuffs not properly handled:
# A condition like {{IF m['foo'] = null}} is not handled and throw a NPE.
# It's using ByteBuffer.equals() to compare 2 collection values which is generally incorrect (the actual comparator should be used).
# If 2 conditions on 2 elements of the same collection were provided and the CAS failed, then the collection was duplicated in the resultSet.
# The ColumnCondition.WithVariables was generally a bit inefficient/ugly: it can lead to bind multiple times the same terms which is unnecessary. It's cleaner to directly create a condition with bound values."
CASSANDRA-7145,FileNotFoundException during compaction,"I can't finish any compaction because my nodes always throw a ""FileNotFoundException"". I've already tried the following but nothing helped:

1. nodetool flush
2. nodetool repair (ends with RuntimeException; see attachment)
3. node restart (via dse cassandra-stop)

Whenever I restart the nodes, another type of exception is logged (see attachment) somewhere near the end of startup process. This particular exception doesn't seem to be critical because the nodes still manage to finish the startup and become online.

I don't have specific steps to reproduce the problem that I'm experiencing with compaction and repair. I'm in the middle of migrating 4.8 billion rows from MySQL via SSTableLoader. 

Some things that may or may not be relevant:
1. I didn't drop and recreate the keyspace (so probably not related to CASSANDRA-4857)
2. I do the bulk-loading in batches of 1 to 20 millions rows. When a batch reaches 100% total progress (i.e. starts to build secondary index), I kill the sstableloader process and cancel the index build
3. I restart the nodes occasionally. It's possible that there is an on-going compaction during one of those restarts.

Related StackOverflow question (mine): http://stackoverflow.com/questions/23435847/filenotfoundexception-during-compaction"
CASSANDRA-7135,Better checks and error messages for data and log dirs on startup,"We used to have checks to make sure the data and logging dirs existed and had good permissions on startup.  In 2.0, it seems that's not the case any more.  If {{/var/lib/cassandra}} and {{/var/log/cassandra}} don't exist, you'll get a startup log like this:

{noformat}
~/cassandra $ bin/cassandra -f
log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /var/log/cassandra/system.log (No such file or directory)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:142)
	at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
	at org.apache.log4j.RollingFileAppender.setFile(RollingFileAppender.java:207)
	at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
	at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
	at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
	at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:809)
	at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:735)
	at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:615)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:502)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:395)
	at org.apache.log4j.PropertyWatchdog.doOnChange(PropertyConfigurator.java:922)
	at org.apache.log4j.helpers.FileWatchdog.checkAndConfigure(FileWatchdog.java:89)
	at org.apache.log4j.helpers.FileWatchdog.<init>(FileWatchdog.java:58)
	at org.apache.log4j.PropertyWatchdog.<init>(PropertyConfigurator.java:914)
	at org.apache.log4j.PropertyConfigurator.configureAndWatch(PropertyConfigurator.java:461)
	at org.apache.cassandra.service.CassandraDaemon.initLog4j(CassandraDaemon.java:133)
	at org.apache.cassandra.service.CassandraDaemon.<clinit>(CassandraDaemon.java:81)
 INFO 18:10:23,635 Logging initialized
 INFO 18:10:23,670 Loading settings from file:/home/thobbs/cassandra/conf/cassandra.yaml
 INFO 18:10:24,061 Data files directories: [/var/lib/cassandra/data]
 INFO 18:10:24,062 Commit log directory: /var/lib/cassandra/commitlog
 INFO 18:10:24,062 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 18:10:24,062 disk_failure_policy is stop
 INFO 18:10:24,062 commit_failure_policy is stop
 INFO 18:10:24,066 Global memtable threshold is enabled at 981MB
 INFO 18:10:24,186 Not using multi-threaded compaction
 INFO 18:10:24,393 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_40
 INFO 18:10:24,393 Heap size: 4116709376/4116709376
 INFO 18:10:24,393 Code Cache Non-heap memory: init = 2555904(2496K) used = 664064(648K) committed = 2555904(2496K) max = 50331648(49152K)
 INFO 18:10:24,393 Par Eden Space Heap memory: init = 671088640(655360K) used = 107378592(104861K) committed = 671088640(655360K) max = 671088640(655360K)
 INFO 18:10:24,393 Par Survivor Space Heap memory: init = 83886080(81920K) used = 0(0K) committed = 83886080(81920K) max = 83886080(81920K)
 INFO 18:10:24,393 CMS Old Gen Heap memory: init = 3361734656(3282944K) used = 0(0K) committed = 3361734656(3282944K) max = 3361734656(3282944K)
 INFO 18:10:24,394 CMS Perm Gen Non-heap memory: init = 21757952(21248K) used = 14180160(13847K) committed = 21757952(21248K) max = 85983232(83968K)
 INFO 18:10:24,394 Classpath: bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang3-3.1.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.3.jar:bin/../lib/disruptor-3.0.1.jar:bin/../lib/guava-15.0.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.9.2.jar:bin/../lib/jackson-mapper-asl-1.9.2.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jbcrypt-0.3m.jar:bin/../lib/jline-1.0.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.9.1.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/lz4-1.2.0.jar:bin/../lib/metrics-core-2.2.0.jar:bin/../lib/netty-3.6.6.Final.jar:bin/../lib/reporter-config-2.1.0.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.7.2.jar:bin/../lib/slf4j-log4j12-1.7.2.jar:bin/../lib/snakeyaml-1.11.jar:bin/../lib/snappy-java-1.0.5.jar:bin/../lib/snaptree-0.1.jar:bin/../lib/super-csv-2.1.0.jar:bin/../lib/thrift-server-0.3.3.jar:bin/../lib/jamm-0.2.5.jar
 INFO 18:10:24,395 JNA not found. Native methods will be disabled.
 INFO 18:10:24,410 Initializing key cache with capacity of 100 MBs.
 INFO 18:10:24,423 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 18:10:24,424 Initializing row cache with capacity of 0 MBs
 INFO 18:10:24,433 Scheduling row cache save to each 0 seconds (going to save all keys).
ERROR 18:10:24,461 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 18:10:24,468 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 18:10:24,469 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 18:10:24,469 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 18:10:24,470 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 18:10:24,470 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,471 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,471 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,472 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,472 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,473 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,474 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,474 Failed to create /var/lib/cassandra/data/system/compaction_history directory
ERROR 18:10:24,475 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 18:10:24,475 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 18:10:24,476 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 18:10:24,476 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 18:10:24,477 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 18:10:24,477 Failed to create /var/lib/cassandra/data/system/sstable_activity directory
ERROR 18:10:24,479 Failed to create /var/lib/cassandra/data/system/sstable_activity directory
ERROR 18:10:24,480 Failed to create /var/lib/cassandra/data/system/sstable_activity directory
ERROR 18:10:24,480 Failed to create /var/lib/cassandra/data/system/sstable_activity directory
ERROR 18:10:24,481 Failed to create /var/lib/cassandra/data/system/sstable_activity directory
ERROR 18:10:24,481 Failed to create /var/lib/cassandra/data/system/sstable_activity directory
ERROR 18:10:24,482 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 18:10:24,482 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 18:10:24,483 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 18:10:24,484 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 18:10:24,484 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 18:10:24,485 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 18:10:24,485 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 18:10:24,486 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 18:10:24,486 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 18:10:24,487 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 18:10:24,488 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 18:10:24,488 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 18:10:24,489 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 18:10:24,489 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 18:10:24,490 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 18:10:24,490 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 18:10:24,491 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 18:10:24,491 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 18:10:24,492 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 18:10:24,492 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 18:10:24,493 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 18:10:24,494 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,494 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,495 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,495 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,496 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,496 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,497 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,498 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,498 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,499 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,499 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,500 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,500 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,501 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,502 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,502 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,503 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,504 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,504 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,505 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,506 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,506 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,507 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,507 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,508 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,508 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,509 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,510 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,510 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 18:10:24,511 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 18:10:24,511 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 18:10:24,512 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 18:10:24,512 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 18:10:24,513 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,513 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,514 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,514 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,515 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,515 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,516 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,516 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 18:10:24,517 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,517 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,518 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,519 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,519 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,520 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,520 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,521 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,521 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,521 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 18:10:24,522 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 18:10:24,522 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 18:10:24,522 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 18:10:24,523 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 18:10:24,523 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,523 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,524 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,524 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,524 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,525 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,525 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,526 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,526 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,526 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 18:10:24,527 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,527 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,527 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,528 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,528 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,528 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,529 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,529 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,530 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,530 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,530 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,531 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,531 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,531 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,532 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,532 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 18:10:24,535 Fatal error: java.io.IOException: Failed to mkdirs /var/lib/cassandra/data
Failed to mkdirs /var/lib/cassandra/data; unable to start server
{noformat}

To summarize, we need to:
* Make sure the directories exist
** If not, suggest creating them or editing cassandra.yaml
* Make sure the directories have good permissions
** If not, say what permissions are needed and link to instructions for fixing them"
CASSANDRA-7113,Unhandled ClassCastException Around Migrations,"I'm having trouble reproducing this, but a run of the pycassa integration tests against the latest 2.1 left this in the logs:

{noformat}
INFO  [MigrationStage:1] 2014-04-29 18:42:22,088 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@52f225c1[cfId=e8c08650-cff7-11e3-8109-6b09a6cc3d5a,ksName=PycassaTestKeyspace,cfName=SingleComposite,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.IntegerType),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.BytesType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]=ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.BytesType, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=column1, type=org.apache.cassandra.db.marshal.IntegerType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=<null>,memtableFlushPeriod=0,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=NONE,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-29 18:42:22,090 ColumnFamilyStore.java:285 - Initializing PycassaTestKeyspace.SingleComposite
INFO  [CompactionExecutor:9] 2014-04-29 18:42:22,096 CompactionTask.java:252 - Compacted 4 sstables to [/var/lib/cassandra/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-169,].  18,036 bytes to 17,507 (~97% of original) in 23ms = 0.725912MB/s.  7 total partitions merged to 4.  Partition merge counts were {1:3, 4:1, }
ERROR [Thrift:24] 2014-04-29 18:42:22,109 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.ClassCastException: null
ERROR [Thrift:34] 2014-04-29 18:42:22,130 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.ClassCastException: null
ERROR [Thrift:25] 2014-04-29 18:42:22,173 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.ClassCastException: null
ERROR [Thrift:11] 2014-04-29 18:42:22,258 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.ClassCastException: null
ERROR [Thrift:16] 2014-04-29 18:42:22,422 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.ClassCastException: null
ERROR [Thrift:26] 2014-04-29 18:42:22,747 CustomTThreadPoolServer.java:219 - Error occurred during processing of message.
java.lang.ClassCastException: null
INFO  [Thrift:7] 2014-04-29 18:42:22,790 MigrationManager.java:220 - Create new ColumnFamily: org.apache.cassandra.config.CFMetaData@461a7048[cfId=e9380040-cff7-11e3-8109-6b09a6cc3d5a,ksName=PycassaTestKeyspace,cfName=UUIDComposite,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.IntegerType),org.apache.cassandra.db.marshal.TimeUUIDType),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.UTF8Type,keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=key, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=column2, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=5 cap=5]=ColumnDefinition{name=value, type=org.apache.cassandra.db.marshal.UTF8Type, kind=COMPACT_VALUE, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=column1, type=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.IntegerType), kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=<null>,memtableFlushPeriod=0,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=NONE,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-29 18:42:22,794 ColumnFamilyStore.java:857 - Enqueuing flush of schema_keyspaces: 510 (0%) on-heap, 0 (0%) off-heap
{noformat}

I'm not sure exactly where the errors happened, but it probably had something to do with the schema migration.  We need to fix the error logging here as well as fix whatever the original cause is."
CASSANDRA-7107,General minor tidying of CollationController path,"There is a lot of unnecessary boiler plate when grabbing an iterator from an in-memory column family. This patch:

* Removes FakeCellName
* Avoids wrapping a non-OnDiskAtomIterator as an OnDiskAtomIterator except when the wrapping is useful
* Removes ColumnSlice.NavigableSetIterator and creates a simpler more direct equivalent in ABTC
* Does not construct a SliceIterator in either ABSC or ABTC if only one slice is requested (just returns that slice as an Iterator)
* Does not construct multiple list indirections in ABSC when constructing a slice
* Shares forward/reverse iterators in ABSC between slices and full-iteration
* Avoids O(N) comparisons during collation of results into an ABSC, by using the knowledge that all columns are provided in insertion order from a merge iterator

"
CASSANDRA-7050,AbstractColumnFamilyInputFormat & AbstractColumnFamilyOutputFormat throw NPE if username is provided but password is null,If a username is provided to either of these classes but the password is null the thrift layer throws an NPE because it can't handle null values for the login.
CASSANDRA-7031,Increase default commit log total space + segment size,"I would like to increase the default commit log total space and segment size options for 64-bit JVMs:

The current default of 1Gb and 32Mb is quite constrained and can have some (very minor) negative performance implications, for no major benefit: 

# 32Mb files are actually quite small, and if during the 10s interval we have completely filled multiple of them (quite easy) it would be more efficient to write fewer larger files, as we can issue fewer fsyncs and permit the OS to schedule the writes more efficiently. On my box this has a small but noticeable impact. Although I would expect on decent server hardware this would be smaller still, since we immediately drop the pages from cache on writing there isn't a great deal of advantage to keeping the files so small. The only advantage I can see is that during a drop KS/CF or other event that forces log rollover we're wasting less space until log recycling. 128-256Mb are modest increases that seem more appropriate to me.
# 1Gb is too small for the default total log space. We can find that we force memtable flushes as a result of log utilisation instead of memtable occupancy quite often (esp. as a result of increased effective memtable space from recent improvements), especially on machines with more addressable memory. I suggest 8Gb as a minimum. The only disadvantage of having more log data is that replay on restart may be slightly slower, but since most of the events will be ignored it should be relatively benign, and I would rather take the penalty on startup instead of during running, no matter how small the running penalty.

"
CASSANDRA-7013,"sstable_generation_loading_test sstableloader_compression_* dtests fail in 1.2, 2.0, and 2.1","This looks like something fundamentally wrong with the sstable_generation_loading_test.py dtest - 1.2 and 2.0 look like:
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-sJDYKB
ok
remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-gRTcgb
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99.9th,elapsed_time
10000,1000,1000,11.5,80.3,280.6,5
END
ok
sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-xneocV
Testing sstableloader with pre_compression=Deflate and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-_iJ1tD
Testing sstableloader with pre_compression=Deflate and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-FZAci9
Testing sstableloader with pre_compression=Deflate and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-AsoepN
Testing sstableloader with pre_compression=None and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Cfiwh8
Testing sstableloader with pre_compression=None and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-tTuQQg
Testing sstableloader with pre_compression=None and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-lA0rXi
Testing sstableloader with pre_compression=Snappy and post_compression=Deflate
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-jJ5iub
Testing sstableloader with pre_compression=Snappy and post_compression=None
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL
sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-JTMBKg
Testing sstableloader with pre_compression=Snappy and post_compression=Snappy
creating keyspace and inserting
Making a copy of the sstables
Wiping out the data and restarting cluster
re-creating the keyspace and column families.
Calling sstableloader
No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]No sstables to stream

progress: [total: 100 - 0MB/s (avg: 0MB/s)]Reading data back
FAIL

======================================================================
FAIL: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 108, in sstableloader_compression_deflate_to_deflate_test
    self.load_sstable_with_configuration('Deflate', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 102, in sstableloader_compression_deflate_to_none_test
    self.load_sstable_with_configuration('Deflate', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 105, in sstableloader_compression_deflate_to_snappy_test
    self.load_sstable_with_configuration('Deflate', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 90, in sstableloader_compression_none_to_deflate_test
    self.load_sstable_with_configuration(None, 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 84, in sstableloader_compression_none_to_none_test
    self.load_sstable_with_configuration(None, None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 87, in sstableloader_compression_none_to_snappy_test
    self.load_sstable_with_configuration(None, 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 99, in sstableloader_compression_snappy_to_deflate_test
    self.load_sstable_with_configuration('Snappy', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 93, in sstableloader_compression_snappy_to_none_test
    self.load_sstable_with_configuration('Snappy', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

======================================================================
FAIL: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 96, in sstableloader_compression_snappy_to_snappy_test
    self.load_sstable_with_configuration('Snappy', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 197, in load_sstable_with_configuration
    read_and_validate_data(cursor)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 190, in read_and_validate_data
    self.assertEquals([str(i), 'col', str(i)], cursor.fetchone())
AssertionError: ['0', 'col', '0'] != None

----------------------------------------------------------------------
Ran 11 tests in 336.704s

FAILED (failures=9)
{noformat}

2.1 looks like:
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 sstable_generation_loading_test.py
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-gPkstl
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Vx3Bsj
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Ap0VwZ
Testing sstableloader with pre_compression=Deflate and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-Nsy44b
Testing sstableloader with pre_compression=Deflate and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-77_xIH
Testing sstableloader with pre_compression=Deflate and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-x3esB0
Testing sstableloader with pre_compression=None and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-HgJni9
Testing sstableloader with pre_compression=None and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-QkUEf5
Testing sstableloader with pre_compression=None and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-mgv9zG
Testing sstableloader with pre_compression=Snappy and post_compression=Deflate
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-e0mEVq
Testing sstableloader with pre_compression=Snappy and post_compression=None
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR
sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading) ... cluster ccm directory: /tmp/dtest-x3D6Bu
Testing sstableloader with pre_compression=Snappy and post_compression=Snappy
[node1 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node1 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node1 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node1 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node1 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node1 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node1 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node1 ERROR]   ... 8 more
[node1 ERROR] Caused by: java.lang.AssertionError
[node1 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node1 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node1 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node1 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node1 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node1 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node1 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node1 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node1 ERROR]   at java.lang.Thread.run(Thread.java:744)
[node2 ERROR] java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625)
[node2 ERROR]   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454)
[node2 ERROR]   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543)
[node2 ERROR] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
[node2 ERROR]   at java.util.concurrent.FutureTask.report(FutureTask.java:122)
[node2 ERROR]   at java.util.concurrent.FutureTask.get(FutureTask.java:188)
[node2 ERROR]   at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407)
[node2 ERROR]   ... 8 more
[node2 ERROR] Caused by: java.lang.AssertionError
[node2 ERROR]   at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340)
[node2 ERROR]   at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380)
[node2 ERROR]   at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188)
[node2 ERROR]   at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316)
[node2 ERROR]   at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
[node2 ERROR]   at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[node2 ERROR]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[node2 ERROR]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[node2 ERROR]   at java.lang.Thread.run(Thread.java:744)
ERROR

======================================================================
ERROR: incompressible_data_in_compressed_table_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 25, in incompressible_data_in_compressed_table_test
    cluster.populate(1).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: remove_index_file_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 55, in remove_index_file_test
    cluster.populate(1).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 108, in sstableloader_compression_deflate_to_deflate_test
    self.load_sstable_with_configuration('Deflate', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 102, in sstableloader_compression_deflate_to_none_test
    self.load_sstable_with_configuration('Deflate', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_deflate_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 105, in sstableloader_compression_deflate_to_snappy_test
    self.load_sstable_with_configuration('Deflate', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 90, in sstableloader_compression_none_to_deflate_test
    self.load_sstable_with_configuration(None, 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 84, in sstableloader_compression_none_to_none_test
    self.load_sstable_with_configuration(None, None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_none_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 87, in sstableloader_compression_none_to_snappy_test
    self.load_sstable_with_configuration(None, 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_deflate_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 99, in sstableloader_compression_snappy_to_deflate_test
    self.load_sstable_with_configuration('Snappy', 'Deflate')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_none_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 93, in sstableloader_compression_snappy_to_none_test
    self.load_sstable_with_configuration('Snappy', None)
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

======================================================================
ERROR: sstableloader_compression_snappy_to_snappy_test (sstable_generation_loading_test.TestSSTableGenerationAndLoading)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 96, in sstableloader_compression_snappy_to_snappy_test
    self.load_sstable_with_configuration('Snappy', 'Snappy')
  File ""/home/mshuler/git/cassandra-dtest/sstable_generation_loading_test.py"", line 127, in load_sstable_with_configuration
    cluster.populate(2).start()
  File ""/home/mshuler/git/ccm/ccmlib/cluster.py"", line 230, in start
    raise NodeError(""Error starting {0}."".format(node.name), p)
NodeError: Error starting node1.

----------------------------------------------------------------------
Ran 11 tests in 56.599s

FAILED (errors=11)
{noformat}

2.1 last ccm node1 log (node2 is the same):
{noformat}
INFO  [main] 2014-04-08 19:20:33,339 CassandraDaemon.java:102 - Hostname: hana.12.am
INFO  [main] 2014-04-08 19:20:33,396 YamlConfigurationLoader.java:80 - Loading settings from file:/tmp/dtest-x3D6Bu/test/node1/conf/cassandra.yaml
INFO  [main] 2014-04-08 19:20:33,514 YamlConfigurationLoader.java:123 - Node configuration:[authenticator=AllowAllAuthenticator; authorizer=AllowAllAuthorizer; auto_bootstrap=false; auto_snapshot=true; batchlog_replay_throttle_in_kb=1024; cas_contention_timeout_in_ms=1000; client_encryption_options=<REDACTED>; cluster_name=test; column_index_size_in_kb=64; commitlog_directory=/tmp/dtest-x3D6Bu/test/node1/commitlogs; commitlog_segment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_period_in_ms=10000; compaction_preheat_key_cache=true; compaction_throughput_mb_per_sec=16; concurrent_counter_writes=32; concurrent_reads=32; concurrent_writes=32; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=[/tmp/dtest-x3D6Bu/test/node1/data]; disk_failure_policy=stop; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dynamic_snitch_update_interval_in_ms=100; endpoint_snitch=SimpleSnitch; flush_directory=/tmp/dtest-x3D6Bu/test/node1/flush; hinted_handoff_enabled=true; hinted_handoff_throttle_in_kb=1024; in_memory_compaction_limit_in_mb=64; incremental_backups=false; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial_token=-9223372036854775808; inter_dc_tcp_nodelay=false; internode_compression=all; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=127.0.0.1; max_hint_window_in_ms=10800000; max_hints_delivery_threads=2; memtable_allocation_type=heap_buffers; memtable_cleanup_threshold=0.4; native_transport_port=9042; partitioner=org.apache.cassandra.dht.Murmur3Partitioner; permissions_validity_in_ms=2000; phi_convict_threshold=5; preheat_kernel_page_cache=false; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=10000; request_scheduler=org.apache.cassandra.scheduler.NoScheduler; request_timeout_in_ms=10000; row_cache_save_period=0; row_cache_size_in_mb=0; rpc_address=127.0.0.1; rpc_keepalive=true; rpc_port=9160; rpc_server_type=sync; saved_caches_directory=/tmp/dtest-x3D6Bu/test/node1/saved_caches; seed_provider=[{class_name=org.apache.cassandra.locator.SimpleSeedProvider, parameters=[{seeds=127.0.0.1}]}]; server_encryption_options=<REDACTED>; snapshot_before_compaction=false; ssl_storage_port=7001; start_native_transport=true; start_rpc=true; storage_port=7000; thrift_framed_transport_size_in_mb=15; tombstone_failure_threshold=100000; tombstone_warn_threshold=1000; trickle_fsync=false; trickle_fsync_interval_in_kb=10240; truncate_request_timeout_in_ms=10000; write_request_timeout_in_ms=10000]
INFO  [main] 2014-04-08 19:20:33,740 DatabaseDescriptor.java:197 - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  [main] 2014-04-08 19:20:33,746 DatabaseDescriptor.java:285 - Global memtable on-heap threshold is enabled at 249MB
INFO  [main] 2014-04-08 19:20:33,747 DatabaseDescriptor.java:289 - Global memtable off-heap threshold is enabled at 249MB
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:113 - JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_51
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:141 - Heap size: 1046937600/1046937600
INFO  [main] 2014-04-08 19:20:34,141 CassandraDaemon.java:143 - Code Cache Non-heap memory: init = 2555904(2496K) used = 675712(659K) committed = 2555904(2496K) max = 50331648(49152K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - Par Eden Space Heap memory: init = 214827008(209792K) used = 81668768(79754K) committed = 214827008(209792K) max = 214827008(209792K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - Par Survivor Space Heap memory: init = 26804224(26176K) used = 0(0K) committed = 26804224(26176K) max = 26804224(26176K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - CMS Old Gen Heap memory: init = 805306368(786432K) used = 0(0K) committed = 805306368(786432K) max = 805306368(786432K)
INFO  [main] 2014-04-08 19:20:34,142 CassandraDaemon.java:143 - CMS Perm Gen Non-heap memory: init = 21757952(21248K) used = 16716776(16324K) committed = 21757952(21248K) max = 85983232(83968K)
INFO  [main] 2014-04-08 19:20:34,143 CassandraDaemon.java:144 - Classpath: /home/mshuler/git/cassandra/build/cobertura/classes:/tmp/dtest-x3D6Bu/test/node1/conf:/home/mshuler/git/cassandra/build/classes/main:/home/mshuler/git/cassandra/build/classes/thrift:/home/mshuler/git/cassandra/lib/airline-0.6.jar:/home/mshuler/git/cassandra/lib/antlr-3.2.jar:/home/mshuler/git/cassandra/lib/commons-cli-1.1.jar:/home/mshuler/git/cassandra/lib/commons-codec-1.2.jar:/home/mshuler/git/cassandra/lib/commons-lang3-3.1.jar:/home/mshuler/git/cassandra/lib/commons-math3-3.2.jar:/home/mshuler/git/cassandra/lib/compress-lzf-0.8.4.jar:/home/mshuler/git/cassandra/lib/concurrentlinkedhashmap-lru-1.4.jar:/home/mshuler/git/cassandra/lib/disruptor-3.0.1.jar:/home/mshuler/git/cassandra/lib/guava-16.0.jar:/home/mshuler/git/cassandra/lib/high-scale-lib-1.1.2.jar:/home/mshuler/git/cassandra/lib/jackson-core-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jackson-mapper-asl-1.9.2.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar:/home/mshuler/git/cassandra/lib/javax.inject.jar:/home/mshuler/git/cassandra/lib/jbcrypt-0.3m.jar:/home/mshuler/git/cassandra/lib/jline-1.0.jar:/home/mshuler/git/cassandra/lib/jna-4.0.0.jar:/home/mshuler/git/cassandra/lib/json-simple-1.1.jar:/home/mshuler/git/cassandra/lib/libthrift-0.9.1.jar:/home/mshuler/git/cassandra/lib/logback-classic-1.1.2.jar:/home/mshuler/git/cassandra/lib/logback-core-1.1.12.jar:/home/mshuler/git/cassandra/lib/lz4-1.2.0.jar:/home/mshuler/git/cassandra/lib/metrics-core-2.2.0.jar:/home/mshuler/git/cassandra/lib/netty-all-4.0.17.Final.jar:/home/mshuler/git/cassandra/lib/reporter-config-2.1.0.jar:/home/mshuler/git/cassandra/lib/slf4j-api-1.7.2.jar:/home/mshuler/git/cassandra/lib/snakeyaml-1.11.jar:/home/mshuler/git/cassandra/lib/snappy-java-1.0.5.jar:/home/mshuler/git/cassandra/lib/stream-2.5.2.jar:/home/mshuler/git/cassandra/lib/super-csv-2.1.0.jar:/home/mshuler/git/cassandra/lib/thrift-server-0.3.3.jar:/home/mshuler/.m2/repository/net/sourceforge/cobertura/cobertura/1.9.4.1/cobertura-1.9.4.1.jar:/home/mshuler/git/cassandra/lib/jamm-0.2.6.jar
WARN  [main] 2014-04-08 19:20:34,204 CLibrary.java:130 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
INFO  [main] 2014-04-08 19:20:34,227 CacheService.java:111 - Initializing key cache with capacity of 49 MBs.
INFO  [main] 2014-04-08 19:20:34,237 CacheService.java:133 - Initializing row cache with capacity of 0 MBs
INFO  [main] 2014-04-08 19:20:34,244 CacheService.java:150 - Initializing counter cache with capacity of 24 MBs
INFO  [main] 2014-04-08 19:20:34,246 CacheService.java:161 - Scheduling counter cache save to every 7200 seconds (going to save all keys).
INFO  [main] 2014-04-08 19:20:34,360 ColumnFamilyStore.java:283 - Initializing system.schema_triggers
INFO  [main] 2014-04-08 19:20:35,608 ColumnFamilyStore.java:283 - Initializing system.compaction_history
INFO  [main] 2014-04-08 19:20:35,618 ColumnFamilyStore.java:283 - Initializing system.batchlog
INFO  [main] 2014-04-08 19:20:35,624 ColumnFamilyStore.java:283 - Initializing system.sstable_activity
INFO  [main] 2014-04-08 19:20:35,630 ColumnFamilyStore.java:283 - Initializing system.peer_events
INFO  [main] 2014-04-08 19:20:35,643 ColumnFamilyStore.java:283 - Initializing system.compactions_in_progress
INFO  [main] 2014-04-08 19:20:35,652 ColumnFamilyStore.java:283 - Initializing system.hints
INFO  [main] 2014-04-08 19:20:35,656 ColumnFamilyStore.java:283 - Initializing system.schema_keyspaces
INFO  [main] 2014-04-08 19:20:35,660 ColumnFamilyStore.java:283 - Initializing system.range_xfers
INFO  [main] 2014-04-08 19:20:35,664 ColumnFamilyStore.java:283 - Initializing system.schema_columnfamilies
INFO  [main] 2014-04-08 19:20:35,669 ColumnFamilyStore.java:283 - Initializing system.NodeIdInfo
INFO  [main] 2014-04-08 19:20:35,677 ColumnFamilyStore.java:283 - Initializing system.paxos
INFO  [main] 2014-04-08 19:20:35,682 ColumnFamilyStore.java:283 - Initializing system.schema_usertypes
INFO  [main] 2014-04-08 19:20:35,686 ColumnFamilyStore.java:283 - Initializing system.schema_columns
INFO  [main] 2014-04-08 19:20:35,691 ColumnFamilyStore.java:283 - Initializing system.IndexInfo
INFO  [main] 2014-04-08 19:20:35,695 ColumnFamilyStore.java:283 - Initializing system.peers
INFO  [main] 2014-04-08 19:20:35,700 ColumnFamilyStore.java:283 - Initializing system.local
INFO  [main] 2014-04-08 19:20:35,820 DatabaseDescriptor.java:587 - Couldn't detect any schema definitions in local storage.
INFO  [main] 2014-04-08 19:20:35,821 DatabaseDescriptor.java:592 - To create keyspaces and column families, see 'help create' in cqlsh.
INFO  [main] 2014-04-08 19:20:35,903 ColumnFamilyStore.java:853 - Enqueuing flush of local: 1109 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:35,921 Memtable.java:344 - Writing Memtable-local@1878619947(220 serialized bytes, 5 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:36,021 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/local-7ad54392bcdd35a684174e047860b377/system-local-ka-1-Data.db (171 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=403)
INFO  [main] 2014-04-08 19:20:36,038 CommitLog.java:108 - No commitlog files found; skipping replay
INFO  [main] 2014-04-08 19:20:36,535 StorageService.java:510 - Cassandra version: 2.1.0-beta1-SNAPSHOT
INFO  [main] 2014-04-08 19:20:36,535 StorageService.java:511 - Thrift API version: 19.39.0
INFO  [main] 2014-04-08 19:20:36,559 StorageService.java:512 - CQL supported versions: 2.0.0,3.1.5 (default: 3.1.5)
INFO  [main] 2014-04-08 19:20:36,595 IndexSummaryManager.java:99 - Initializing index summary manager with a memory pool size of 49 MB and a resize interval of 60 minutes
INFO  [main] 2014-04-08 19:20:36,607 StorageService.java:537 - Loading persisted ring state
INFO  [main] 2014-04-08 19:20:36,679 StorageService.java:858 - Saved tokens not found. Using configuration value: [-9223372036854775808]
INFO  [main] 2014-04-08 19:20:36,701 MigrationManager.java:206 - Create new Keyspace: KSMetaData{name=system_traces, strategyClass=SimpleStrategy, strategyOptions={replication_factor=2}, cfMetaData={sessions=org.apache.cassandra.config.CFMetaData@7892cf41[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(706172616d6574657273:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type))),comment=traced sessions,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=duration, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=parameters, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=started_at, type=org.apache.cassandra.db.marshal.TimestampType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=request, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=coordinator, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}], events=org.apache.cassandra.config.CFMetaData@3a437a40[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type),comment=,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=source, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=thread, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=activity, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=14 cap=14]=ColumnDefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=event_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]}, durableWrites=true, userTypes=org.apache.cassandra.config.UTMetaData@240f1da2}
INFO  [MigrationStage:1] 2014-04-08 19:20:36,826 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 990 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:36,834 Memtable.java:344 - Writing Memtable-schema_keyspaces@2107075397(251 serialized bytes, 7 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:36,915 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-1-Data.db (216 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:36,920 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 164066 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:36,921 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1732384250(30202 serialized bytes, 514 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 19:20:37,019 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-1-Data.db (6720 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:37,033 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 288881 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:37,034 Memtable.java:344 - Writing Memtable-schema_columns@985819426(46627 serialized bytes, 904 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 19:20:37,140 Memtable.java:378 - Completed flushing /tmp/dtest-x3D6Bu/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-1-Data.db (10835 bytes) for commitlog position ReplayPosition(segmentId=1397002835560, position=99535)
INFO  [MigrationStage:1] 2014-04-08 19:20:37,351 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@40e9da6[cfId=c5e99f16-8677-3914-b17e-960613512345,ksName=system_traces,cfName=sessions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(706172616d6574657273:org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type))),comment=traced sessions,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=duration, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=parameters, type=org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=started_at, type=org.apache.cassandra.db.marshal.TimestampType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=request, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=coordinator, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 19:20:37,357 ColumnFamilyStore.java:283 - Initializing system_traces.sessions
INFO  [MigrationStage:1] 2014-04-08 19:20:37,358 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@5b8fff5e[cfId=8826e8e9-e16a-3728-8753-3bc1fc713c25,ksName=system_traces,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.TimeUUIDType,org.apache.cassandra.db.marshal.UTF8Type),comment=,readRepairChance=0.0,dclocalReadRepairChance=0.0,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=session_id, type=org.apache.cassandra.db.marshal.UUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=source, type=org.apache.cassandra.db.marshal.InetAddressType, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=6 cap=6]=ColumnDefinition{name=thread, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=activity, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=14 cap=14]=ColumnDefinition{name=source_elapsed, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=event_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=3600000,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 19:20:37,363 ColumnFamilyStore.java:283 - Initializing system_traces.events
ERROR [MigrationStage:1] 2014-04-08 19:20:37,426 CassandraDaemon.java:166 - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340) ~[main/:na]
        at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [main] 2014-04-08 19:20:37,427 CassandraDaemon.java:471 - Exception encountered during startup
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:411) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:298) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.announceNewKeyspace(MigrationManager.java:207) ~[main/:na]
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:915) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:625) ~[main/:na]
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:505) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:335) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:454) [main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:543) [main/:na]
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.get(FutureTask.java:188) ~[na:1.7.0_51]
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:407) ~[main/:na]
        ... 8 common frames omitted
Caused by: java.lang.AssertionError: null
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1256) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:340) ~[main/:na]
        at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:380) ~[main/:na]
        at org.apache.cassandra.db.DefsTables.mergeSchema(DefsTables.java:188) ~[main/:na]
        at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:316) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
ERROR [StorageServiceShutdownHook] 2014-04-08 19:20:37,437 CassandraDaemon.java:166 - Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.gms.Gossiper.stop(Gossiper.java:1270) ~[main/:na]
        at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:581) ~[main/:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[main/:na]
        at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
{noformat}"
CASSANDRA-7011,auth_test system_auth_ks_is_alterable_test dtest hangs in 2.1 and 2.0,"This test hangs forever. When I hit ctl-c after running the test, then the ccm nodes actually continue running - I think ccm is looking for log lines that never occur until the test is killed(?).
{noformat}
$ export MAX_HEAP_SIZE=""1G""; export HEAP_NEWSIZE=""256M""; ENABLE_VNODES=true PRINT_DEBUG=true nosetests --nocapture --nologcapture --verbosity=3 auth_test.py:TestAuth.system_auth_ks_is_alterable_test
nose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']
system_auth_ks_is_alterable_test (auth_test.TestAuth) ... cluster ccm directory: /tmp/dtest-O3AAJr
^C
{noformat}
Search for (hanging here) below - I typed this prior to hitting ctl-c. Then the nodes start running again and I see ""Listening for thrift clients"" later on.
{noformat}
mshuler@hana:~$ tail -f /tmp/dtest-O3AAJr/test/node*/logs/system.log
==> /tmp/dtest-O3AAJr/test/node1/logs/system.log <==
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,599 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1792243696(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:2] 2014-04-08 16:45:12,603 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node1/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  14,454 bytes to 11,603 (~80% of original) in 105ms = 0.105386MB/s.  7 total partitions merged to 3.  Partition merge counts were {1:1, 2:1, 4:1, }
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,668 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node1/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504671, position=193292)
INFO  [MigrationStage:1] 2014-04-08 16:45:12,669 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6806 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,670 Memtable.java:344 - Writing Memtable-schema_columns@352928691(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:1] 2014-04-08 16:45:12,672 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node1/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  710 bytes to 233 (~32% of original) in 70ms = 0.003174MB/s.  6 total partitions merged to 3.  Partition merge counts were {1:2, 4:1, }
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,721 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node1/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504671, position=193830)
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,566 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.
INFO  [NonPeriodicTasks:1] 2014-04-08 16:45:20,570 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'
INFO  [NonPeriodicTasks:1] 2014-04-08 16:45:21,806 Auth.java:232 - Created default superuser 'cassandra'

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,214 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1004 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,215 Memtable.java:344 - Writing Memtable-schema_keyspaces@781373873(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,295 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,296 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 34190 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,297 Memtable.java:344 - Writing Memtable-schema_columnfamilies@2077216447(5746 serialized bytes, 108 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:12,369 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db (2088 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [InternalResponseStage:4] 2014-04-08 16:45:12,370 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 37408 (0%) on-heap, 0 (0%) off-heap
INFO  [CompactionExecutor:4] 2014-04-08 16:45:12,371 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db')]
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:12,371 Memtable.java:344 - Writing Memtable-schema_columns@2003573271(5173 serialized bytes, 119 ops, 0%/0% of on/off-heap limit)
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,248 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,654 Memtable.java:344 - Writing Memtable-schema_keyspaces@789549279(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,788 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=193958)
INFO  [InternalResponseStage:1] 2014-04-08 16:45:11,789 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 34192 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:11,790 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1695849916(5746 serialized bytes, 108 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:2] 2014-04-08 16:45:11,898 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db (2087 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=194091)
INFO  [InternalResponseStage:1] 2014-04-08 16:45:11,899 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 37410 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,900 Memtable.java:344 - Writing Memtable-schema_columns@2090391222(5173 serialized bytes, 119 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:1] 2014-04-08 16:45:11,998 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db (1700 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=194091)
INFO  [main] 2014-04-08 16:45:18,654 CassandraDaemon.java:533 - No gossip backlog; proceeding
WARN  [NonPeriodicTasks:1] 2014-04-08 16:45:20,131 FBUtilities.java:359 - Trigger directory doesn't exist, please create it and try again.

(hanging here)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [NonPeriodicTasks:1] 2014-04-08 16:49:02,863 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [NonPeriodicTasks:1] 2014-04-08 16:49:02,888 PasswordAuthenticator.java:220 - PasswordAuthenticator created default user 'cassandra'

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:02,919 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db (1699 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=243552)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:02,922 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db')]

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [InternalResponseStage:3] 2014-04-08 16:49:02,959 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1006 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:02,960 Memtable.java:344 - Writing Memtable-schema_keyspaces@44265998(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:02,970 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 501 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:02,972 Memtable.java:344 - Writing Memtable-schema_keyspaces@603519674(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [main] 2014-04-08 16:49:03,029 Server.java:159 - Starting listening for CQL clients on /127.0.0.3:9042...

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [CompactionExecutor:4] 2014-04-08 16:49:03,064 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-13,].  13,907 bytes to 7,643 (~54% of original) in 213ms = 0.034220MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,069 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db (179 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [InternalResponseStage:3] 2014-04-08 16:49:03,071 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 42850 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,071 Memtable.java:344 - Writing Memtable-schema_columnfamilies@666219518(7373 serialized bytes, 135 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,087 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=247475)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,090 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  16,411 bytes to 11,314 (~68% of original) in 165ms = 0.065393MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }
INFO  [CompactionExecutor:6] 2014-04-08 16:49:03,090 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-13-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db')]
INFO  [MigrationStage:1] 2014-04-08 16:49:03,091 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8799 (0%) on-heap, 0 (0%) off-heap

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [main] 2014-04-08 16:49:03,091 ThriftServer.java:119 - Binding thrift service to /127.0.0.3:9160

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,091 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1821762369(1609 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [Thread-8] 2014-04-08 16:49:03,098 ThriftServer.java:136 - Listening for thrift clients...
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,166 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db (2536 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [InternalResponseStage:3] 2014-04-08 16:49:03,167 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 44033 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,168 Memtable.java:344 - Writing Memtable-schema_columns@1052245443(6187 serialized bytes, 140 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:8] 2014-04-08 16:49:03,168 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-12-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-9-Data.db')]

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,203 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (949 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=248454)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,204 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6833 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,205 Memtable.java:344 - Writing Memtable-schema_columns@1698728310(1041 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:6] 2014-04-08 16:49:03,218 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node2/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  750 bytes to 233 (~31% of original) in 125ms = 0.001778MB/s.  8 total partitions merged to 3.  Partition merge counts were {1:1, 3:1, 4:1, }

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,278 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db (1990 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=213470)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,279 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-10-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-11-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-9-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-12-Data.db')]

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,310 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (431 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=248454)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [CompactionExecutor:8] 2014-04-08 16:49:03,313 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-13,].  14,821 bytes to 8,115 (~54% of original) in 140ms = 0.055279MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }
INFO  [MigrationStage:1] 2014-04-08 16:49:03,313 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 502 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,315 Memtable.java:344 - Writing Memtable-schema_keyspaces@901379409(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,332 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 501 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,333 Memtable.java:344 - Writing Memtable-schema_keyspaces@1181313624(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,392 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=217732)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,397 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8818 (0%) on-heap, 0 (0%) off-heap
INFO  [CompactionExecutor:7] 2014-04-08 16:49:03,397 CompactionTask.java:131 - Compacting [SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-14-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-13-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-15-Data.db'), SSTableReader(path='/tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-16-Data.db')]
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,400 Memtable.java:344 - Writing Memtable-schema_columnfamilies@2077765501(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
INFO  [CompactionExecutor:5] 2014-04-08 16:49:03,405 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-13,].  16,987 bytes to 11,600 (~68% of original) in 123ms = 0.089940MB/s.  9 total partitions merged to 3.  Partition merge counts were {1:1, 4:2, }

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,418 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-18-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252763)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,419 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8817 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,420 Memtable.java:344 - Writing Memtable-schema_columnfamilies@495883972(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,483 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-14-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=217732)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,484 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6807 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,485 Memtable.java:344 - Writing Memtable-schema_columns@365627698(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,519 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/system-schema_columnfamilies-ka-15-Data.db (956 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252896)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,520 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columns: 6806 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,521 Memtable.java:344 - Writing Memtable-schema_columns@1830789468(1014 serialized bytes, 21 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [CompactionExecutor:7] 2014-04-08 16:49:03,523 CompactionTask.java:287 - Compacted 4 sstables to [/tmp/dtest-O3AAJr/test/node3/data/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-17,].  750 bytes to 233 (~31% of original) in 122ms = 0.001821MB/s.  8 total partitions merged to 3.  Partition merge counts were {1:1, 3:1, 4:1, }
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,562 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-14-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=218351)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,581 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node2/flush/system/schema_columns-296e9c049bec3085827dc17d3df2122a/system-schema_columns-ka-15-Data.db (435 bytes) for commitlog position ReplayPosition(segmentId=1396993504760, position=252896)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,586 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 502 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,586 Memtable.java:344 - Writing Memtable-schema_keyspaces@543512535(138 serialized bytes, 3 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node2/logs/system.log <==
INFO  [MigrationStage:1] 2014-04-08 16:49:03,604 DefsTables.java:388 - Loading org.apache.cassandra.config.CFMetaData@1327e4ff[cfId=2d324e48-3275-3517-8dd5-9a2c5b0856c5,ksName=system_auth,cfName=permissions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7065726d697373696f6e73:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,gcGraceSeconds=7776000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.UTF8Type,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=permissions, type=org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type), kind=REGULAR, componentIndex=1, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=username, type=org.apache.cassandra.db.marshal.UTF8Type, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]=ColumnDefinition{name=resource, type=org.apache.cassandra.db.marshal.UTF8Type, kind=CLUSTERING_COLUMN, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionParameters={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtableFlushPeriod=0,caching={""keys"":""ALL"", ""rows_per_partition"":""NONE""},defaultTimeToLive=0,minIndexInterval=128,maxIndexInterval=2048,speculativeRetry=99.0PERCENTILE,populateIoCacheOnFlush=false,droppedColumns={},triggers={}]
INFO  [MigrationStage:1] 2014-04-08 16:49:03,610 ColumnFamilyStore.java:283 - Initializing system_auth.permissions
INFO  [main] 2014-04-08 16:49:03,622 CassandraDaemon.java:501 - Waiting for gossip to settle before accepting client requests...
INFO  [InternalResponseStage:5] 2014-04-08 16:49:03,625 ColumnFamilyStore.java:853 - Enqueuing flush of schema_keyspaces: 1004 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,626 Memtable.java:344 - Writing Memtable-schema_keyspaces@2064761560(276 serialized bytes, 6 ops, 0%/0% of on/off-heap limit)

==> /tmp/dtest-O3AAJr/test/node3/logs/system.log <==
INFO  [MemtableFlushWriter:3] 2014-04-08 16:49:03,664 Memtable.java:378 - Completed flushing /tmp/dtest-O3AAJr/test/node3/flush/system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/system-schema_keyspaces-ka-18-Data.db (159 bytes) for commitlog position ReplayPosition(segmentId=1396993504533, position=222660)
INFO  [MigrationStage:1] 2014-04-08 16:49:03,665 ColumnFamilyStore.java:853 - Enqueuing flush of schema_columnfamilies: 8818 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:1] 2014-04-08 16:49:03,666 Memtable.java:344 - Writing Memtable-schema_columnfamilies@1464552123(1627 serialized bytes, 27 ops, 0%/0% of on/off-heap limit)
<...>
{noformat}"
CASSANDRA-7007,Streaming Errors due to RejectedExecutionException,"With the same repair setup as CASSANDRA-6984, I'm now seeing repair failures due to some streaming tasks hitting RejectedExecutionException:

{noformat}
ERROR [STREAM-OUT-/127.0.0.3] 2014-04-08 13:50:15,424 StreamSession.java (line 420) [Stream #9f64e230-bf4e-11e3-b994-43eb3a328df9] Streaming error occurred
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1eb6b48a rejected from java.util.concurrent.ScheduledThreadPoolExecutor@6d06dfc6[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
    at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)
    at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)
    at java.util.concurrent.Executors$DelegatedScheduledExecutorService.schedule(Executors.java:689)
    at org.apache.cassandra.streaming.StreamTransferTask.scheduleTimeout(StreamTransferTask.java:125)
    at org.apache.cassandra.streaming.StreamSession.fileSent(StreamSession.java:468)
    at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:60)
    at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
    at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
    at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
    at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
    at java.lang.Thread.run(Thread.java:724)
 INFO [STREAM-OUT-/127.0.0.3] 2014-04-08 13:50:15,431 StreamResultFuture.java (line 186) [Stream #9f64e230-bf4e-11e3-b994-43eb3a328df9] Session with /127.0.0.3 is complete
 WARN [STREAM-OUT-/127.0.0.3] 2014-04-08 13:50:15,431 StreamResultFuture.java (line 215) [Stream #9f64e230-bf4e-11e3-b994-43eb3a328df9] Stream failed
ERROR [NonPeriodicTasks:1] 2014-04-08 13:50:15,449 CassandraDaemon.java (line 198) Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: Outgoing stream handler has been closed
    at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:170)
    at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSession.java:620)
    at org.apache.cassandra.streaming.StreamSession.taskCompleted(StreamSession.java:566)
    at org.apache.cassandra.streaming.StreamReceiveTask$OnCompletionRunnable.run(StreamReceiveTask.java:120)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
{noformat}

Since the ThreadPoolExecutor says its state is TERMINATED, I'm guessing this is due to a race condition where the timeout executor can be shutdown too early."
CASSANDRA-6993,Windows: remove mmap'ed I/O for index files and force standard file access,Memory-mapped I/O on Windows causes issues with hard-links; we're unable to delete hard-links to open files with memory-mapped segments even using nio.  We'll need to push for close to performance parity between mmap'ed I/O and buffered going forward as the buffered / compressed path offers other benefits.
CASSANDRA-6984,NullPointerException in Streaming During Repair,"In cassandra-2.0, I can trigger a NullPointerException with a repair.  These steps should reproduce the issue:
* create a three node ccm cluster (with vnodes)
* start a stress write (I'm using {{tools/bin/cassandra-stress --replication-factor=3 -n 10000000 -k -t 1}})
* stop node3 while stress is running, then wait a minute
* start node 3
* run ""ccm node3 repair""

In the logs for node1, I see this:
{noformat}


ERROR [STREAM-OUT-/127.0.0.3] 2014-04-04 17:40:08,547 CassandraDaemon.java (line 198) Exception in thread Thread[STREAM-OUT-/127.0.0.3,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler$MessageHandler.signalCloseDone(ConnectionHandler.java:249)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:375)
        at java.lang.Thread.run(Thread.java:724)
{noformat}

After applying Yuki's suggested patch:
{noformat}
diff --git a/src/java/org/apache/cassandra/streaming/ConnectionHandler.java b/src/java/org/apache/cassandra/streaming/ConnectionHandler.java
index 356138b..b06a818 100644
--- a/src/java/org/apache/cassandra/streaming/ConnectionHandler.java
+++ b/src/java/org/apache/cassandra/streaming/ConnectionHandler.java
@@ -366,7 +366,7 @@ public class ConnectionHandler
             {
                 throw new AssertionError(e);
             }
-            catch (IOException e)
+            catch (Throwable e)
             {
                 session.onError(e);
             }
{noformat}

I see a new NPE:
{noformat}
ERROR [STREAM-OUT-/127.0.0.3] 2014-04-04 18:12:35,912 StreamSession.java (line 420) [Stream #9b592af0-bc4e-11e3-a6f9-43eb3a328df9] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamSession.fileSent(StreamSession.java:465)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:60)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
        at java.lang.Thread.run(Thread.java:724)
{noformat}"
CASSANDRA-6977,attempting to create 10K column families fails with 100 node cluster,"During this test we are attempting to create a total of 1K keyspaces with 10 column families each to bring the total column families to 10K.  With a 5 node cluster this operation can be completed; however, it fails with 100 nodes.  Please see the two charts.  For the 5 node case the time required to create each keyspace and subsequent 10 column families increases linearly until the number of keyspaces is 1K.  For a 100 node cluster there is a sudden increase in latency between 450 keyspaces and 550 keyspaces.  The test ends when the test script times out.  After the test script times out it is impossible to reconnect to the cluster with the datastax python driver because it cannot connect to the host:
cassandra.cluster.NoHostAvailable: ('Unable to connect to any servers', {'10.199.5.98': OperationTimedOut()}

It was found that running the following stress command does work from the same machine the test script runs on.

cassandra-stress -d 10.199.5.98 -l 2 -e QUORUM -L3 -b -o INSERT

It should be noted that this test was initially done with DSE 4.0 and c* version 2.0.5.24 and in that case it was not possible to run stress against the cluster even locally on a node due to not finding the host.

Attached are system logs from one of the nodes, charts showing schema creation latency for 5 and 100 node clusters and virtualvm tracer data for cpu, memory, num_threads and gc runs, tpstat output and the test script.

The test script was on an m1.large aws instance outside of the cluster under test."
CASSANDRA-6962,examine shortening path length post-5202,"From CASSANDRA-5202 discussion:

{quote}
Did we give up on this?
Could we clean up the redundancy a little by moving the ID into the directory name? e.g., ks/cf-uuid/version-generation-component.db
I'm worried about path length, which is limited on Windows.
Edit: to give a specific example, for KS foo Table bar we now have
/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/foo-bar-ka-1-Data.db
I'm proposing
/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/ka-1-Data.db
{quote}"
CASSANDRA-6948,"After Bootstrap or Replace node startup, EXPIRING_MAP_REAPER is shutdown and cannot be restarted, causing callbacks to collect indefinitely","Since ExpiringMap.shutdown() shuts down the static executor service, it cannot be restarted (and in fact reset() makes no attempt to do so). As such callbacks that receive no response are never removed from the map, and eventually either than server will run out of memory or will loop around the integer space and start reusing messageids that have not been expired, causing assertions to be thrown and messages to fail to be sent. It appears that this situation only arises on bootstrap or node replacement, as MessagingService is shutdown before being attached to the listen address.

This can cause the following errors to begin occurring in the log:

ERROR [Native-Transport-Requests:7636] 2014-03-28 13:32:10,638 ErrorMessage.java (line 222) Unexpected exception during request
java.lang.AssertionError: Callback already exists for id -1665979622! (CallbackInfo(target=/10.106.160.84, callback=org.apache.cassandra.service.WriteResponseHandler@5d36d8ea, serializer=org.apache.cassandra.db.WriteResponse$WriteResponseSerializer@6ed37f0b))
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:549)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:601)
	at org.apache.cassandra.service.StorageProxy.mutateCounter(StorageProxy.java:984)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:449)
	at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:524)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:521)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:505)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:358)
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:131)
	at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:304)
	at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
	at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
ERROR [ReplicateOnWriteStage:102766] 2014-03-28 13:32:10,638 CassandraDaemon.java (line 196) Exception in thread Thread[ReplicateOnWriteStage:102766,5,main]
java.lang.AssertionError: Callback already exists for id -1665979620! (CallbackInfo(target=/10.106.160.84, callback=org.apache.cassandra.service.WriteResponseHandler@3bdb1a75, serializer=org.apache.cassandra.db.WriteResponse$WriteResponseSerializer@6ed37f0b))
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:549)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:601)
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:806)
	at org.apache.cassandra.service.StorageProxy$8$1.runMayThrow(StorageProxy.java:1074)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1896)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)"
CASSANDRA-6945,"Calculate liveRatio on per-memtable basis, non per-CF","Currently we recalculate live ratio every doubling of write ops to the CF, not to an individual memtable. The value itself is also CF-bound, not memtable-bound. This is causing at least several issues:

1. Depending on what stage the current memtable is, the live ratio calculated can vary *a lot*
2. That calculated live ratio will potentially stay that way for quite a while - the longer C* process is on, the longer it would stay incorrect
3. Incorrect live ratio means inefficient MeteredFlusher - flushing less or more often than needed, picking bad candidates for flushing, etc.
4. Incorrect live ratio means incorrect size returned to the metrics consumers
5. Compaction strategies that rely on memtable size estimation are affected
6. All of the above is slightly amplified by the fact that all the memtables pending flush would also use that one incorrect value

Depending on the stage the current memtable at the moment of live ratio recalculation is, the value calculated can be *extremely* wrong (say, a recently created, fresh memtable - would have a much higher than average live ratio).

The suggested fix is to bind live ratio to individual memtables, not column families as a whole, with some optimizations to make recalculations run less often by inheriting previous memtable's stats."
CASSANDRA-6935,Make clustering part of primary key a first order component in the storage engine,"It would be helpful for a number of upcoming improvements if the clustering part of the primary key were extracted from CellName, and if a ColumnFamily object could store multiple ClusteredRow (or similar) instances, within which each cell is keyed only by the column identifier.

This would also, by itself, reduce on comparison costs and also permit memory savings in memtables, by sharing the clustering part of the primary key across all cells in the same row. It might also make it easier to move more data off-heap, by constructing an off-heap clustered row, but keeping the partition level object on-heap.

"
CASSANDRA-6932,StorageServiceMbean needs to expose flush directory.,"Storage service currently exposes data dirs, commitlog dir, and saved caches dir. Should add the flush dir now that we have that as well."
CASSANDRA-6930,Dynamic Snitch isWorthMergingForRangeQuery Doesn't Handle Some Cases Optimally,"I was investigating slow responses for queries like {{select * from system.peers}} and noticed that the dynamic endpoint snitch was reporting that the query was _not_ worth merging.  In this case, the local host had a score of 0, so {{return maxMerged < maxL1 + maxL2}} was returning false.  I believe using a {{<=}} condition is the proper fix there.

Additionally, because scores are looked up three separate times, this method is a prone to race conditions.  I don't think it's worth fixing the race condition for a multi-node scenario, but at least in the single-node case, we can immediately return true and avoid any race conditions that would cause it to erroneously return false."
CASSANDRA-6920,LatencyMetrics can return infinity,"There is a race condition when updating the recentLatency metrics exposed from LatencyMetrics.

Attaching a patch with a test that exposes the issue and a potential fix."
CASSANDRA-6916,Preemptive opening of compaction result,"Related to CASSANDRA-6812, but a little simpler: when compacting, we mess quite badly with the page cache. One thing we can do to mitigate this problem is to use the sstable we're writing before we've finished writing it, and to drop the regions from the old sstables from the page cache as soon as the new sstables have them (even if they're only written to the page cache). This should minimise any page cache churn, as the old sstables must be larger than the new sstable, and since both will be in memory, dropping the old sstables is at least as good as dropping the new.

The approach is quite straight-forward. Every X MB written:
# grab flushed length of index file;
# grab second to last index summary record, after excluding those that point to positions after the flushed length;
# open index file, and check that our last record doesn't occur outside of the flushed length of the data file (pretty unlikely)
# Open the sstable with the calculated upper bound

Some complications:
# must keep running copy of compression metadata for reopening with
# we need to be able to replace an sstable with itself but a different lower bound
# we need to drop the old page cache only when readers have finished

"
CASSANDRA-6912,SSTableReader.isReplaced does not allow for safe resource cleanup,"There are a number of possible race conditions on resource cleanup from the use of cloneWithNewSummarySamplingLevel, because the replacement sstable can be itself replaced/obsoleted while the prior sstable is still referenced (this is actually quite easy with compaction, but can happen in other circumstances less commonly)."
CASSANDRA-6890,Standardize on a single read path,"Since we actively unmap unreferenced SSTR's and also copy data out of those readers on the read path, the current memory mapped i/o is a lot of complexity for very little payoff.  Clean out the mmapp'ed i/o on the read path."
CASSANDRA-6879,ConcurrentModificationException while doing range slice query.,"The paging read request (either from thrift or native) would sporadically fail due to a race condition between read repair and requesting thread waiting for read repair results list. The READ_REPAIR is queued in ReadCallback.maybeResolveForRepair(), and it does not seem to have guarantee that its resolve() method (which internally create RangeSliceResponseResolver.Reducer and doing repairResults.addAll inside RangeSliceResponseResolver.Reducer) would be invoked before the requesting thread starts waiting on resolver.repairResults. So, there is a small window that the list is partially populated, while requesting thread starts waiting on repairResults. I believe for the most of the time, the requesting thread is either wait for the entire repair results or not waiting for repair results at all. The original intent here seems to be waiting for repair results always (if the repair is triggered by repair chance).

{code}
ERROR [Native-Transport-Requests:70827] 2014-03-18 05:00:12,774 ErrorMessage.java (line 222) Unexpected exception during request
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)
        at java.util.ArrayList$Itr.next(ArrayList.java:831)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:423)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:1583)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:188)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:163)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:58)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:358)
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:131)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:304)
        at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
        at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

{code}
ERROR [Thrift:1] 2014-03-18 07:18:02,434 CustomTThreadPoolServer.java (line 212) Error occurred during processing of message.
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)
        at java.util.ArrayList$Itr.next(ArrayList.java:831)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:423)
        at org.apache.cassandra.service.StorageProxy.getRangeSlice(StorageProxy.java:1583)
        at org.apache.cassandra.service.pager.RangeSliceQueryPager.queryNextPage(RangeSliceQueryPager.java:85)
        at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:71)
        at org.apache.cassandra.service.pager.RangeSliceQueryPager.fetchPage(RangeSliceQueryPager.java:36)
        at org.apache.cassandra.cql3.statements.SelectStatement.pageCountQuery(SelectStatement.java:202)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:169)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:58)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:212)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1958)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}"
CASSANDRA-6860,Race condition in Batch CLE,"CASSANDRA-6759 exposed a bug in CLE, where we are signalling the waiting segments before setting the value they're waiting on. This is occurring much more readily now as we perform a slow call in the middle of the race. Attaching a quick and simple patch."
CASSANDRA-6841,ConcurrentModificationException in commit-log-writer after local schema reset,"{code}
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,013 MigrationManager.java (line 329) Starting local schema reset...
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,016 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@394448776(114/1140 serialized/live bytes, 3 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,016 Memtable.java (line 331) Writing Memtable-local@394448776(114/1140 serialized/live bytes, 3 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,182 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-398-Data.db (145 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33159822)
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,185 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@1087210140(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,185 Memtable.java (line 331) Writing Memtable-local@1087210140(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,357 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-399-Data.db (96 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33159959)
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,361 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@768887091(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,361 Memtable.java (line 331) Writing Memtable-local@768887091(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,516 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-400-Data.db (96 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33160096)
 INFO [CompactionExecutor:38] 2014-03-12 11:37:54,517 CompactionTask.java (line 115) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-398-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-400-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-399-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-jb-397-Data.db')]
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,519 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-local@271993477(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,519 Memtable.java (line 331) Writing Memtable-local@271993477(62/620 serialized/live bytes, 1 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:54,794 Memtable.java (line 371) Completed flushing /var/lib/cassandra/data/system/local/system-local-jb-401-Data.db (96 bytes) for commitlog position ReplayPosition(segmentId=1394620057452, position=33160233)
 INFO [RMI TCP Connection(38)-192.168.36.171] 2014-03-12 11:37:54,799 MigrationManager.java (line 357) Local schema reset is complete.
 INFO [CompactionExecutor:38] 2014-03-12 11:37:54,848 CompactionTask.java (line 275) Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-jb-402,].  6,099 bytes to 5,821 (~95% of original) in 330ms = 0.016822MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
 INFO [OptionalTasks:1] 2014-03-12 11:37:55,110 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-schema_columnfamilies@106276050(181506/509164 serialized/live bytes, 3276 ops)
 INFO [FlushWriter:6] 2014-03-12 11:37:55,110 Memtable.java (line 331) Writing Memtable-schema_columnfamilies@106276050(181506/509164 serialized/live bytes, 3276 ops)
 INFO [OptionalTasks:1] 2014-03-12 11:37:55,110 ColumnFamilyStore.java (line 785) Enqueuing flush of Memtable-schema_columns@252242773(185191/630698 serialized/live bytes, 3614 ops)
ERROR [COMMIT-LOG-WRITER] 2014-03-12 11:37:55,111 CassandraDaemon.java (line 196) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)
        at java.util.HashMap$KeyIterator.next(HashMap.java:960)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.flushOldestKeyspaces(CommitLogAllocator.java:309)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.fetchSegment(CommitLogAllocator.java:147)
        at org.apache.cassandra.db.commitlog.CommitLog.activateNextSegment(CommitLog.java:299)
        at org.apache.cassandra.db.commitlog.CommitLog.access$100(CommitLog.java:49)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:350)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:51)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:744)

{code}"
CASSANDRA-6838,FileCacheService overcounting its memoryUsage,"On investigating why I was seeing dramatically worse performance for counter updates over prepared CQL3 statements compred to unprepared CQL2 statements, I stumbled upon a bug in FileCacheService wherein, on returning a cached reader back to the pool, its memory is counted again towards the total memory usage, but is not matched by a decrement when checked out. So we effectively are probably not caching readers most of the time.
"
CASSANDRA-6832,File handle leak in StreamWriter.java,"Reference CASSANDRA-6283 where this first came up.  nodetool.bat repair -par on 2.0.5 pops up the following stack:

ERROR [Finalizer] 2014-02-17 09:21:52,922 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for C:\var\lib\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-jb-41-CRC.db allocated
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:106)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:98)
        at org.apache.cassandra.io.util.DataIntegrityMetadata$ChecksumValidator.<init>(DataIntegrityMetadata.java:53)
        at org.apache.cassandra.io.util.DataIntegrityMetadata.checksumValidator(DataIntegrityMetadata.java:40)
        at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:76)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:59)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
        at java.lang.Thread.run(Thread.java:744)

This leak doesn't look like it's breaking anything but is still worth fixing."
CASSANDRA-6818,SSTable references not released if stream session fails before it starts,"I observed a large number of 'orphan' SSTables - SSTables that are in the data directory but not loaded by Cassandra - on a 1.1.12 node that had a large stream fail before it started. These orphan files are particularly dangerous because if the node is restarted and picks up these SSTables it could bring data back to life if tombstones have been GCed. To confirm the SSTables are orphan, I created a snapshot and it didn't contain these files. I can see in the logs that they have been compacted so should have been deleted.

The log entries for the stream are:

{{INFO [StreamStage:1] 2014-02-21 19:41:48,742 StreamOut.java (line 115) Beginning transfer to /10.0.0.1}}
{{INFO [StreamStage:1] 2014-02-21 19:41:48,743 StreamOut.java (line 96) Flushing memtables for [CFS(Keyspace='ks', ColumnFamily='cf1'), CFS(Keyspace='ks', ColumnFamily='cf2')]...}}
{{ERROR [GossipTasks:1] 2014-02-21 19:41:49,239 AbstractStreamSession.java (line 113) Stream failed because /10.0.0.1 died or was restarted/removed (streams may still be active in background, but further streams won't be started)}}
{{INFO [StreamStage:1] 2014-02-21 19:41:51,783 StreamOut.java (line 161) Stream context metadata [...] 2267 sstables.}}
{{INFO [StreamStage:1] 2014-02-21 19:41:51,789 StreamOutSession.java (line 182) Streaming to /10.0.0.1}}
{{INFO [Streaming to /10.0.0.1:1] 2014-02-21 19:42:02,218 FileStreamTask.java (line 99) Found no stream out session at end of file stream task - this is expected if the receiver went down}}

After digging in the code, here's what I think the issue is:

1. StreamOutSession.transferRanges() creates a streaming session, which is registered with the failure detector in AbstractStreamSession's constructor.
2. Memtables are flushed, potentially taking a long time.
3. The remote node fails, convict() is called and the StreamOutSession is closed. However, at this time StreamOutSession.files is empty because it's still waiting for the memtables to flush.
4. Memtables finish flusing, references are obtained to SSTables to be streamed and the PendingFiles are added to StreamOutSession.files.
5. The first stream fails but the StreamOutSession isn't found so is never closed and the references are never released.

This code is more or less the same on 1.2 so I would expect it to reproduce there. I looked at 2.0 and can't even see where SSTable references are released when the stream fails.

Some possible fixes for 1.1/1.2:

1. Don't register with the failure detector until after the PendingFiles are set up. I think this is the behaviour in 2.0 but I don't know if it was done like this to avoid this issue.
2. Detect the above case in (e.g.) StreamOutSession.begin() by noticing the session has been closed with care to avoid double frees.
3. Add some synchronization so closeInternal() doesn't race with setting up the session."
CASSANDRA-6810,SSTable and Index Layout Improvements/Modifications,"Right now SSTables are somewhat inefficient in their storage of composite keys. I propose resolving this by merging (some of) the index functionality with the storage of keys, through introducing a composite btree/trie structure (e.g. string b-tree) to represent the key, and for this structure to index into the cell position in the file. This structure can then serve as both an efficient index and the key data itself. 

If we then offer the option of (possibly automatically decided for you at flush) storing this either packed into the same file directly prepending the data, or in a separate key file (with small pages), with an uncompressed page cache we can get good performance for wide rows by storing it separately and relying on the page cache for CQL row index lookups, whereas storing it inline will allow very efficient lookups of small rows where index lookups aren't particularly helpful. This removal of extra data from the index file, however, will allow CASSANDRA-6709 to massively scale up the efficiency of the key cache, whilst also reducing the total disk footprint of sstables and (most likely) offering better indexing capability in similar space"
CASSANDRA-6802,Row cache improvements,"There are a few things we could do;

* Start using the native memory constructs from CASSANDRA-6694 to avoid serialization/deserialization costs and to minimize the on-heap overhead
* Stop invalidating cached rows on writes (update on write instead).
"
CASSANDRA-6797,compaction and scrub data directories race on startup," 
Hi,  

On doing a rolling restarting of a 2.0.5 cluster in several environments I'm seeing the following error:
{code}

 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,549 CompactionTask.java (line 115) Compacting [SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compactio
n_race/node1/data/system/local/system-local-jb-15-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-16-Data.db'), SSTableReader(path='/Users/Matthew/.ccm/compaction_race/node1/data/system/local/syst
em-local-jb-14-Data.db')]
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,557 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,560 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 WARN [main] 2014-03-03 17:11:07,608 ColumnFamilyStore.java (line 473) Removing orphans for /Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-13: [CompressionInfo.db, Filter.db, Index.db, TOC.txt, Summary.db, Data.db, Statistics.
db]
ERROR [main] 2014-03-03 17:11:07,609 CassandraDaemon.java (line 479) Exception encountered during startup
java.lang.AssertionError: attempted to delete non-existing file system-local-jb-13-CompressionInfo.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:111)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:106)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:476)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
 INFO [CompactionExecutor:1] 2014-03-03 17:11:07,612 CompactionTask.java (line 275) Compacted 4 sstables to [/Users/Matthew/.ccm/compaction_race/node1/data/system/local/system-local-jb-17,].  10,963 bytes to 5,572 (~50% of original) in 57ms = 0.093226MB/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }

{code}
Seems like a potential race, since compactions are occurring whilst the existing data directories are being scrubbed.
Probably an in progress compaction looks like an incomplete one and results in it being attempted to be scrubbed whilst in progress. 
On the attempt to delete in the scrubDataDirectories we discover that it no longer exists, presumably because it has now been compacted away. 
This then causes an assertion error and the node fails to start up. 

Here is a ccm script which just stops and starts a 3 node 2.0.5 cluster repeatedly. 
It seems to fairly reliably reproduce the problem, in less than ten iterations: 

{code}
#!/bin/bash

ccm create compaction_race -v 2.0.5
ccm populate -n 3
ccm start

for i in $(seq 0 1000); do 
    echo $i;
    ccm stop
    ccm start
    grep ERR ~/.ccm/compaction_race/*/logs/system.log;
done

{code}
 
Someone else should probably confirm that this is what is going wrong,  
however if it is, the solution might be as simple as to disable autocompactions slightly earlier in CassandraDaemon.setup. 
 
Or alternatively if there isn't a good reason why we are first scrubbing the system tables and then scrubbing all keyspaces (including the system keyspace), you could perhaps just scrub solely the non system keyspaces on the second scrub.

Please let me know if there is anything else I can provide.
Thanks,
Matt
"
CASSANDRA-6793,NPE in Hadoop Word count example,"The partition keys requested in WordCount.java do not match the primary key set up in the table output_words. It looks this patch was not merged properly from [CASSANDRA-5622|https://issues.apache.org/jira/browse/CASSANDRA-5622].The attached patch addresses the NPE and uses the correct keys defined in #5622.

I am assuming there is no need to fix the actual NPE like throwing an InvalidRequestException back to user to fix the partition keys, as it would be trivial to get the same from the TableMetadata using the driver API.

java.lang.NullPointerException
	at org.apache.cassandra.dht.Murmur3Partitioner.getToken(Murmur3Partitioner.java:92)
	at org.apache.cassandra.dht.Murmur3Partitioner.getToken(Murmur3Partitioner.java:40)
	at org.apache.cassandra.client.RingCache.getRange(RingCache.java:117)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:163)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:63)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:587)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at WordCount$ReducerToCassandra.reduce(Unknown Source)
	at WordCount$ReducerToCassandra.reduce(Unknown Source)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)"
CASSANDRA-6788,Race condition silently kills thrift server,"There's a race condition in CustomTThreadPoolServer that can cause the thrift server to silently stop listening for connections. 

It happens when the executor service throws a RejectedExecutionException, which is not caught.
 
Silent in the sense that OpsCenter doesn't notice any problem since JMX is still running fine."
CASSANDRA-6787,assassinate should continue when the endpoint vanishes,"Assassinate can NPE in various situations, most notably if the endpoint vanishes during the sleep-for-safety check."
CASSANDRA-6776,Secondary Index on Map type isn't returning the expected result,"I am attaching the cql script I use to test. The output I get from the query returns zero rows.

cqlsh:testks> select * from testks.test2 where keywords CONTAINS KEY 'cool';

(0 rows)

cqlsh:testks>

When I run a query for set<text> it works fine.
cqlsh:testks> select * from testks.test2 where nicknames CONTAINS 'barb';

 user_name | birth_year | gender | keywords                   | nicknames                     | password | session_token | state
-----------+------------+--------+----------------------------+-------------------------------+----------+---------------+-------
    user10 |       1968 |      f | {'cool': '3', 'lame': '1'} | {'barb', 'barbara', 'barbie'} |     null |          null |    co

(1 rows)

I exit cqlsh and run nodetool flush. When I look at the index file for keywords index, it doesn't look right. Seems to be missing entries. The file is also attached."
CASSANDRA-6769,Static columns break IN clauses,"If you use static columns, as implemented in CASSANDRA-6561, then very simple SELECT...WHERE...IN queries fail with an internal NPE.

create table foo (x text, y text, s text static, primary key (x,y));
insert into foo (x,y,s) values ('a','b','c');
select * from foo where x='a' and y in ('b','c');
Request did not complete within rpc_timeout.

ERROR [ReadStage:190] 2014-02-25 14:19:16,400 CassandraDaemon.java (line 196) Exception in thread Thread[ReadStage:190,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1900)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:141)
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:162)
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:162)
	at org.apache.cassandra.db.filter.ColumnSlice$NavigableMapIterator.computeNext(ColumnSlice.java:117)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter$1.hasNext(SliceQueryFilter.java:148)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:157)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:140)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:185)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:122)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1550)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1341)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1896)
"
CASSANDRA-6753,Cassandra2.1~beta1 Stall at Boot,"I was trying out the new release for several perf. improvements that I am very interested in. After upgrading my cassandra from 2.0.5 to the beta version, cassandra is stalled while init the column families.

I might misconfigure something, but it seems it is suck in a loop. I added a couple debug statements, but, on second thought, I think I should just leave it to the experts...

It's looping in the following over and over:

{code:title=src/java/org/apache/cassandra/utils/memory/Pool.java#needsCleaning}
0 >= -858993472 && true && true
{code}

{code:title=Log}
INFO  [HeapSlabPoolCleaner] 2014-02-21 22:28:40,073 Keyspace.java:77 - java.lang.Thread.getStackTrace(Unknown Source),
org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:77),
org.apache.cassandra.db.Keyspace$1.apply(Keyspace.java:74),
com.google.common.collect.Iterators$8.transform(Iterators.java:794),
com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48),
org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:2278),
org.apache.cassandra.db.ColumnFamilyStore$FlushLargestColumnFamily.run(ColumnFamilyStore.java:1043),
org.apache.cassandra.utils.memory.PoolCleanerThread.run(PoolCleanerThread.java:70)
{code}

They may be totally unrelated or a normal behavior. Let me know if there is any other info I should provide."
CASSANDRA-6694,Slightly More Off-Heap Memtables,"The Off Heap memtables introduced in CASSANDRA-6689 don't go far enough, as the on-heap overhead is still very large. It should not be tremendously difficult to extend these changes so that we allocate entire Cells off-heap, instead of multiple BBs per Cell (with all their associated overhead).

The goal (if possible) is to reach an overhead of 16-bytes per Cell (plus 4-6 bytes per cell on average for the btree overhead, for a total overhead of around 20-22 bytes). This translates to 8-byte object overhead, 4-byte address (we will do alignment tricks like the VM to allow us to address a reasonably large memory space, although this trick is unlikely to last us forever, at which point we will have to bite the bullet and accept a 24-byte per cell overhead), and 4-byte object reference for maintaining our internal list of allocations, which is unfortunately necessary since we cannot safely (and cheaply) walk the object graph we allocate otherwise, which is necessary for (allocation-) compaction and pointer rewriting.

The ugliest thing here is going to be implementing the various CellName instances so that they may be backed by native memory OR heap memory."
CASSANDRA-6692,AtomicBTreeColumns Improvements,"There are two improvements to make to the BTree code that should help:

1) It turns out Stack Allocation is more rubbish than we had hoped, and so the fast route actually allocates garbage. It's unlikely this reduces throughput, but the increased young-gen pressure is probably unwelcome. I propose to remove the fast route for now.

2) It is not uncommon to race to perform an update, so that the new values are actually out-of-date when we come to modify the tree. In this case the update should recognise that the original (portion of) the tree has not been modified, and simply return it, without allocating a new one."
CASSANDRA-6690,Replace EmptyColumns with updated ArrayBackedSortedColumns,"Now that UnsortedColumns is gone (CASSANDRA-6630) and TreeMapBackedSortedColumns is also gone (CASSANDRA-6662), we can also get rid of EmptyColumns and, further, inline AbstractThreadUnsafeSortedColumns into ABSC, leaving us with just two ColumnFamily implementations - AtomicBTreeColumns for the memtable and ABSC for everything else.

The modified ABSC in the patch starts with the empty static array, so in terms of memory it's about the same as EmptyColumns used to be."
CASSANDRA-6678,Unwanted schema pull while upgrading nodes from 1.2 to 2.0,"While upgrading from 1.2 to 2.0, the 1.2 nodes are not supposed to pull schemas from upgraded 2.0 nodes to avoid conflicts.

This relies on network version checks between the two nodes, but there's a bit of a race between the Gossiper, which is activated first, and the MessagingService, which is activated after the Gossiper and handles network version exchange: if a 1.2 node Gossiper gets a gossip message from a newly 2.0 node *before* opening connections from the MessagingService, the version will still be 1.2, and the schema will be pulled from the new node.

A possible solution may be to have the Gossiper update the network version upon receiving the first gossip message of an upgraded node: thoughts?"
CASSANDRA-6655,Writing mostly deletes to a Memtable results in undercounting the table's occupancy so it may not flush,"In the extreme case of only deletes the memtable will never flush, and we will OOM."
CASSANDRA-6648,Race condition during node bootstrapping,"When bootstrapping a new node, data is ""missing"" as if the new node didn't actually bootstrap, which I tracked down to the following scenario:

1) New node joins token ring and waits for schema to be settled before actually bootstrapping.
2) The schema scheck somewhat passes and it starts bootstrapping.
3) Bootstrapping doesn't find the ks/cf that should have received from the other node.
4) Queries at this point cause NPEs, until when later they ""recover"" but data is missed.

The problem seems to be caused by a race condition between the migration manager and the bootstrapper, with the former running after the latter.
I think this is supposed to protect against such scenarios:
{noformat}
            while (!MigrationManager.isReadyForBootstrap())
            {
                setMode(Mode.JOINING, ""waiting for schema information to complete"", true);
                Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);
            }
{noformat}

But MigrationManager.isReadyForBootstrap() implementation is quite fragile and doesn't take into account ""slow"" schema propagation."
CASSANDRA-6645,upgradesstables causes NPE for secondary indexes without an underlying column family,"SecondaryIndex#getIndexCfs is allowed to return null by contract, if the index is not backed by a column family, but this causes an NPE as StorageService#getValidColumnFamilies and StorageService#upgradeSSTables do not check for null values."
CASSANDRA-6628,Cassandra crashes on Solaris sparcv9 using java 64bit,"When running cassandra 2.0.4 (and other versions) on Solaris and java 64 bit, JVM crashes. Issue is described once in CASSANDRA-4646 but closed as invalid.

The reason for this crash is some memory allignment related problems and incorrect sun.misc.Unsafe usage. If you look into DirectByteBuffer in jdk, you will see that it checks os.arch before using getLong methods.

I have a patch, which check for the os.arch and if it is not one of the known, it reads longs and ints byte by byte.

Although patch fixes the problem in cassandra, it will still crash without similar fixes in the lz4 library. I already provided the patch for Unsafe usage in lz4."
CASSANDRA-6619,Race condition issue during upgrading 1.1 to 1.2,"There is a race condition during upgrading a C* 1.1x cluster to C* 1.2.
One issue is that OutboundTCPConnection can't establish from a 1.2 node to some 1.1x nodes.  Because of this, a live cluster during the upgrading will suffer in high read latency and be unable to fulfill some write requests.  It won't be a problem if there is a small cluster but it is a problem in a large cluster (100+ nodes) because the upgrading process takes 10+ hours to 1+ day(s) to complete.


Acknowledging about CASSANDRA-5692, however, it does not fully fix the issue.  We already have a patch for this and will attach shortly for feedback.


"
CASSANDRA-6602,Compaction improvements to optimize time series data,"There are some unique characteristics of many/most time series use cases that both provide challenges, as well as provide unique opportunities for optimizations.

One of the major challenges is in compaction. The existing compaction strategies will tend to re-compact data on disk at least a few times over the lifespan of each data point, greatly increasing the cpu and IO costs of that write.

Compaction exists to
1) ensure that there aren't too many files on disk
2) ensure that data that should be contiguous (part of the same partition) is laid out contiguously
3) deleting data due to ttls or tombstones

The special characteristics of time series data allow us to optimize away all three.

Time series data
1) tends to be delivered in time order, with relatively constrained exceptions
2) often has a pre-determined and fixed expiration date
3) Never gets deleted prior to TTL
4) Has relatively predictable ingestion rates

Note that I filed CASSANDRA-5561 and this ticket potentially replaces or lowers the need for it. In that ticket, jbellis reasonably asks, how that compaction strategy is better than disabling compaction.

Taking that to heart, here is a compaction-strategy-less approach that could be extremely efficient for time-series use cases that follow the above pattern.

(For context, I'm thinking of an example use case involving lots of streams of time-series data with a 5GB per day ingestion rate, and a 1000 day retention with TTL, resulting in an eventual steady state of 5TB per node)

1) You have an extremely large memtable (preferably off heap, if/when doable) for the table, and that memtable is sized to be able to hold a lengthy window of time. A typical period might be one day. At the end of that period, you flush the contents of the memtable to an sstable and move to the next one. This is basically identical to current behaviour, but with thresholds adjusted so that you can ensure flushing at predictable intervals. (Open question is whether predictable intervals is actually necessary, or whether just waiting until the huge memtable is nearly full is sufficient)
2) Combine the behaviour with CASSANDRA-5228 so that sstables will be efficiently dropped once all of the columns have. (Another side note, it might be valuable to have a modified version of CASSANDRA-3974 that doesn't bother storing per-column TTL since it is required that all columns have the same TTL)
3) Be able to mark column families as read/write only (no explicit deletes), so no tombstones.
4) Optionally add back an additional type of delete that would delete all data earlier than a particular timestamp, resulting in immediate dropping of obsoleted sstables.

The result is that for in-order delivered data, Every cell will be laid out optimally on disk on the first pass, and over the course of 1000 days and 5TB of data, there will ""only"" be 1000 5GB sstables, so the number of filehandles will be reasonable.

For exceptions (out-of-order delivery), most cases will be caught by the extended (24 hour+) memtable flush times and merged correctly automatically. For those that were slightly askew at flush time, or were delivered so far out of order that they go in the wrong sstable, there is relatively low overhead to reading from two sstables for a time slice, instead of one, and that overhead would be incurred relatively rarely unless out-of-order delivery was the common case, in which case, this strategy should not be used.

Another possible optimization to address out-of-order would be to maintain more than one time-centric memtables in memory at a time (e.g. two 12 hour ones), and then you always insert into whichever one of the two ""owns"" the appropriate range of time. By delaying flushing the ahead one until we are ready to roll writes over to a third one, we are able to avoid any fragmentation as long as all deliveries come in no more than 12 hours late (in this example, presumably tunable).

Anything that triggers compactions will have to be looked at, since there won't be any. The one concern I have is the ramificaiton of repair. Initially, at least, I think it would be acceptable to just write one sstable per repair and not bother trying to merge it with other sstables."
CASSANDRA-6598,upgradesstables does not upgrade indexes causing startup error.,"Upgrading a cluster from 1.1.12 -> 1.2.13 -> 2.0 HEAD fails due to upgradesstables not upgrading the index files.

To reproduce:

{code}
# Make sure ccm has all the versions we need:
ccm create -v git:cassandra-2.0 test
ccm remove 
ccm create -v git:cassandra-1.2.13 test
ccm remove

# Create a 1.1.12 cluster:
ccm create -v git:cassandra-1.1.12 test
# Set cluster partitioner:
perl -p -i -e 's/partitioner: null/partitioner: RandomPartitioner/gi' ~/.ccm/test/cluster.conf

ccm populate -n 1
ccm start
ccm node1 stress -- --operation=INSERT --family-type=Standard --num-keys=10000 --create-index=KEYS --compression=SnappyCompressor --compaction-strategy=LeveledCompactionStrategy
ccm flush

ccm node1 drain
ccm status
# Wait until node1 shows DOWN.

# Set cluster version:
perl -p -i -e 's/git_cassandra-1.1.12/git_cassandra-1.2.13/gi' ~/.ccm/test/cluster.conf

# Upgrade node1:
ccm node1 updateconf
ccm node1 start

# Upgrade sstables:
~/.ccm/test/node1/bin/nodetool -p 7100 upgradesstables

ls ~/.ccm/test/node1/data/Keyspace1/Standard1/

# Note the versions on files. Data has been upgraded to version *ic* but indexes are left on version *hf*.


# Upgrade to 2.0:
ccm flush
ccm node1 drain
ccm status
# Wait until node1 shows DOWN.
# Set cluster version:
perl -p -i -e 's/git_cassandra-1.2.13/git_cassandra-2.0/gi' ~/.ccm/test/cluster.conf
ccm node1 updateconf
ccm node1 start
{code}

On this last upgrade attempt, cassandra 2.0 complains that the version for the indexes is incorrect:

{code}
java.lang.RuntimeException: Can't open incompatible SSTable! Current version jb, found file: /home/ryan/.ccm/test/node1/data/Keyspace1/Standard1/Keyspace1-Standard1.Idx1-hf-1
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:393)
        at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.init(AbstractSimplePerColumnSecondaryIndex.java:52)
        at org.apache.cassandra.db.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:274)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:279)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:416)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:388)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:309)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:266)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:273)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:549)
{code}

The same test can be done starting from 1.2 upgrading to 2.0. The index files do not upgrade in this scenario either, however, there is not the same error, possibly because 2.0 is tolerant of version 1.2 indexes?"
CASSANDRA-6569,Batchlog replays copy the entire batchlog table into the heap,"The current batchlog replay path will read the entire batchlog table into the heap. This is pretty bad. This was compounded by CASSANDRA-5762, which caused the SELECT statement used by the batchlog replay to bring the entire row into memory instead of just the selected columns."
CASSANDRA-6557,CommitLogSegment may be duplicated in unlikely race scenario,"In the unlikely event that the thread that switched to a new CLS has not finished executing the cleanup of its switch by the time the CLS has finished being used, it is possible for the same segment to be 'switched' in again. This would be benign except that it is added to the activeSegments queue a second time also, which would permit it to be recycled twice, creating two different CLS objects in memory pointing to the same CLS on disk, after which all bets are off.

The issue is highly unlikely to occur, but highly unlikely means it will probably happen eventually. I've fixed this based on my patch for CASSANDRA-5549, using the NonBlockingQueue I introduce there to simplify the logic and make it more obviously correct."
CASSANDRA-6544,Reduce GC activity during compaction,"We are noticing increase in P99 while the compactions are running at full stream. Most of it is because of the increased GC activity (followed by full GC).

The obvious solution/work around is to throttle the compactions, but with SSD's we can get more disk bandwidth for reads and compactions.

It will be nice to move the compaction object allocations off heap. First thing to do might be create a Offheap Slab allocator with the size as the compaction in memory size and recycle it. 

Also we might want to make it configurable so folks can disable it when they don't have off-heap memory to reserve."
CASSANDRA-6534,Slow inserts with collections into a single partition (Pathological GC behavior),"We noticed extremely slow insertion rates to a single partition key, using composite column with a collection value. We were not able to replicate the issue using the same schema, but with a non-colleciton value and using much larger values.  During the collection insertion tests we have tons of these messages in the system.log:
""GCInspector.java (line 119) GC for ConcurrentMarkSweep: 1287 ms for 2 collections, 1233256368 used; max is 8375238656""

We are inserting a tiny amounts of data 32-64 bytes and seeing the issue after only a couple 10k inserts. The amount of memory being used by C*/JVM is no where near proportional to the amount data being inserted. Why is C* consuming so much memory?

Attached is a picture of the GC under one of the pathological tests. Keep in mind we are only inserting 128KB - 256KB of data and we are almost hitting the limit of the heap.

GC flags:
-XX:+UseThreadPriorities
-XX:ThreadPriorityPolicy=42
-Xms8192M
-Xmx8192M
-Xmn2048M
-XX:+HeapDumpOnOutOfMemoryError
-Xss180k
-XX:+UseParNewGC
-XX:+UseConcMarkSweepGC
-XX:+CMSParallelRemarkEnabled
-XX:SurvivorRatio=8
-XX:MaxTenuringThreshold=1
-XX:CMSInitiatingOccupancyFraction=75
-XX:+UseCMSInitiatingOccupancyOnly
-XX:+UseTLAB

Example schemas:

Note: The type of collection or primitive type in the collection doesn't seem to matter.

{code}
CREATE TABLE test.test (
row_key text, 
column_key uuid,
 column_value list<int>, 
PRIMARY KEY(row_key, column_key));

CREATE TABLE test.test (
row_key text, 
column_key uuid, 
column_value map<text, text>, 
PRIMARY KEY(row_key, column_key));
{code}

Example inserts:

Note: This issue is able to be replicated with extremely small inserts (a well as larger ~1KB)

{code}
INSERT INTO test.test 
(row_key, column_key, column_value)
VALUES 
('0000000001', e0138677-7246-11e3-ac78-016ae7083d37, [0, 1, 2, 3]);

INSERT INTO test.test 
(row_key, column_key, column_value) 
VALUES
('0000000022', 1ac5770a-7247-11e3-80e4-016ae7083d37, { 'a': '0123456701234567012345670',  'b': '0123456701234567012345670' });
{code}

As a comparison, I was able to run the same tests with the following schema with no issue:

Note: This test was able to run at a much faster insertion speed, for much longer and much bigger column sizes (1KB) without any GC issues.

{code}
CREATE TABLE test.test (
row_key text, 
column_key uuid, 
column_value text, 
PRIMARY KEY(row_key, column_key) )
{code}
"
CASSANDRA-6527,Random tombstones after adding a CF with sstableloader,"I've marked this bug as critical since it results in a data loss without any warnings.

Here's the scenario:

- create a fresh one-node cluster with cassandra 1.2.11
- add a sample row:

{code}
CREATE KEYSPACE keyspace1 WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};
use keyspace1;
create table table1 (key text primary key, value1 text);
update table1 set value1 = 'some-value' where key = 'some-key';
{code}

- flush, drain, shutdown the cluster - you should have a single sstable:

{code}
root@l1:~# ls /var/lib/cassandra/data/keyspace1/table1/
keyspace1-table1-ic-1-CompressionInfo.db  
keyspace1-table1-ic-1-Filter.db  
keyspace1-table1-ic-1-Statistics.db  
keyspace1-table1-ic-1-TOC.txt
keyspace1-table1-ic-1-Data.db             
keyspace1-table1-ic-1-Index.db   
keyspace1-table1-ic-1-Summary.db
{code}

with a perfectly correct content:

{code}
root@l1:~# sstable2json /var/lib/cassandra/data/keyspace1/table1/keyspace1-table1-ic-1-Data.db
[
{""key"": ""736f6d652d6b6579"",""columns"": [["""","""",1387822268786000], [""value1"",""some-value"",1387822268786000]]}
]
{code}

- create a new cluster with 2.0.3 (we've used 3 nodes with replication=2, but I guess it doesn't matter)

- copy sstable from the machine in the old cluster to one of the machines in the new cluster (we do not want to use old sstableloader)

- load sstables with sstableloader:

{code}
sstableloader -d 172.16.9.12 keyspace1/table1
{code}

- analyze the content of newly loaded sstable:

{code}
root@l13:~# sstable2json /var/lib/cassandra/data/keyspace1/table1/keyspace1-table1-jb-1-Data.db
[
{""key"": ""736f6d652d6b6579"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":294205259775,""localDeletionTime"":0}},""columns"": [["""","""",1387824835597000], [""value1"",""some-value"",1387824835597000]]}
]
{code}

There's a random tombstone inserted!

We've hit this bug in production. We never use delete for this column family, but the tombstones appeared for each row. The timestamp looks random. In our case it was mostly in past, but sometimes (about 3% rows) it was in the future. That's even worse than missing a row. In that case you cannot simply add it again - tombstone from the future will hide it.

Fortunately, we have noticed that quickly and canceled the migration. However, we were quite lucky. There are no warnings or errors during the whole process. Losing less than 3% of data may be hard to noticed at first sight for many kind of apps.
"
CASSANDRA-6526,"CQLSSTableWriter addRow(Map<String, Object> values) does not work as documented.","There are 2 bugs in the method
{code}
addRow(Map<String, Object> values)
{code}
First issue is that the map <b>must</b> contain all the column names as keys in the map otherwise the addRow fails (with InvalidRequestException ""Invalid number of arguments, expecting %d values but got %d"").

Second Issue is that the keys in the map must be in lower-case otherwise they may not be found in the map, which will result in a NPE during decompose.
h6. SUGGESTED SOLUTION:
Fix the addRow method with:
{code}
public CQLSSTableWriter addRow(Map<String, Object> values)
    throws InvalidRequestException, IOException
{
    int size = boundNames.size();
    Map<String, ByteBuffer> rawValues = new HashMap<>(size);
    for (int i = 0; i < size; i++) {
        ColumnSpecification spec = boundNames.get(i);
        String colName = spec.name.toString();
        rawValues.put(colName, values.get(colName) == null ? null : ((AbstractType)spec.type).decompose(values.get(colName)));
    }
    return rawAddRow(rawValues);
}
{code}
When creating the new Map for the insert we need to go over all columns and apply null to missing columns.

Fix the method documentation add this line:
{code}
     * <p>
     * Keys in the map <b>must</b> be in lower case, otherwise their value will be null.
     *
{code}"
CASSANDRA-6517,Loss of secondary index entries if nodetool cleanup called before compaction,"From time to time we had the feeling of not getting all results that should have been returned using secondary indexes. Now we tracked down some situations and found out, it happened:

1) To primary keys that were already deleted and have been re-created later on

2) After our nightly maintenance scripts were running

We can reproduce now the following szenario:

- create a row entry with an indexed column included
- query it and use the secondary index criteria -> Success
- delete it, query again -> entry gone as expected
- re-create it with the same key, query it -> success again

Now use in exactly that sequence

nodetool cleanup
nodetool flush
nodetool compact

When issuing the query now, we don't get the result using the index. The entry is indeed available in it's table when I just ask for the key. Below is the exact copy-paste output from CQL when I reproduced the problem with an example entry on on of our tables.

mwerrch@mstc01401:/opt/cassandra$ current/bin/cqlsh Connected to 14-15-Cluster at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3 | CQL spec 3.1.1 | Thrift protocol 19.38.0] Use HELP for help.
cqlsh> use mwerrch;
cqlsh:mwerrch> desc tables;

B4Container_Demo

cqlsh:mwerrch> desc table ""B4Container_Demo"";

CREATE TABLE ""B4Container_Demo"" (
  key uuid,
  archived boolean,
  bytes int,
  computer int,
  deleted boolean,
  description text,
  doarchive boolean,
  filename text,
  first boolean,
  frames int,
  ifversion int,
  imported boolean,
  jobid int,
  keepuntil bigint,
  nextchunk text,
  node int,
  recordingkey blob,
  recstart bigint,
  recstop bigint,
  simulationid bigint,
  systemstart bigint,
  systemstop bigint,
  tapelabel bigint,
  version blob,
  PRIMARY KEY (key)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='demo' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=604800 AND
  index_interval=128 AND
  read_repair_chance=1.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX mwerrch_Demo_computer ON ""B4Container_Demo"" (computer);

CREATE INDEX mwerrch_Demo_node ON ""B4Container_Demo"" (node);

CREATE INDEX mwerrch_Demo_recordingkey ON ""B4Container_Demo"" (recordingkey);

cqlsh:mwerrch> INSERT INTO ""B4Container_Demo"" (key,computer,node) VALUES (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

 key                                  | node | computer
--------------------------------------+------+----------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |   50 |       50

(1 rows)

cqlsh:mwerrch> DELETE FROM ""B4Container_Demo"" WHERE key=78c70562-1f98-3971-9c28-2c3d8e09c10f;
cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)

cqlsh:mwerrch> INSERT INTO ""B4Container_Demo"" (key,computer,node) VALUES (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

 key                                  | node | computer
--------------------------------------+------+----------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |   50 |       50

(1 rows)

**********************************
Now we execute (maybe from a different shell so we don't have to close this session) from /opt/cassandra/current/bin directory:
./nodetool cleanup
./nodetool flush
./nodetool compact


Going back to our CQL session the result will no longer be available if queried via the index:
*********************************

cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)
"
CASSANDRA-6498,Null pointer exception in custom secondary indexes,"StorageProxy#estimateResultRowsPerRange raises a null pointer exception when using a custom 2i implementation that not uses a column family as underlying storage:
{code}
resultRowsPerRange = highestSelectivityIndex.getIndexCfs().getMeanColumns();
{code}
According to the documentation, the method SecondaryIndex#getIndexCfs should return null when no column family is used."
CASSANDRA-6491,Timeout can send confusing information as to what their cause is,"We can race between the time we ""detect"" a timeout and the time we build the actual exception, so that it's possible to have a timeout exception that pretends enough replica have actually acknowledged the operation, which is thus slightly confusing to the user as to why it got a timeout.

That kind of race is rather unlikely in a healthy environment, but https://datastax-oss.atlassian.net/browse/JAVA-227 shows that it's at least possible to trigger in a test environment.

Note that it's definitively not worth synchronizing to avoid that that, but it could maybe be simple enough to detect the race when building the exception and ""correct"" the ack count. Attaching simple patch to show what I have in mind.

Note that I don't entirely disagree that it's not ""perfect"", but as said above, proper synchronization is just not worth it and it seems to me that it's not worth confusing users over that."
CASSANDRA-6405,"When making heavy use of counters, neighbor nodes occasionally enter spiral of constant memory consumpion","We're randomly running into an interesting issue on our ring. When making use of counters, we'll occasionally have 3 nodes (always neighbors) suddenly start immediately filling up memory, CMSing, fill up again, repeat. This pattern goes on for 5-20 minutes. Nearly all requests to the nodes time out during this period. Restarting one, two, or all three of the nodes does not resolve the spiral; after a restart the three nodes immediately start hogging up memory again and CMSing constantly.

When the issue resolves itself, all 3 nodes immediately get better. Sometimes it reoccurs in bursts, where it will be trashed for 20 minutes, fine for 5, trashed for 20, and repeat that cycle a few times.

There are no unusual logs provided by cassandra during this period of time, other than recording of the constant dropped read requests and the constant CMS runs. I have analyzed the log files prior to multiple distinct instances of this issue and have found no preceding events which are associated with this issue.

I have verified that our apps are not performing any unusual number or type of requests during this time.

This behaviour occurred on 1.0.12, 1.1.7, and now on 1.2.11.

The way I've narrowed this down to counters is a bit naive. It started happening when we started making use of counter columns, went away after we rolled back use of counter columns. I've repeated this attempted rollout on each version now, and it consistently rears its head every time. I should note this incident does _seem_ to happen more rarely on 1.2.11 compared to the previous versions.

This incident has been consistent across multiple different types of hardware, as well as major kernel version changes (2.6 all the way to 3.2). The OS is operating normally during the event.


I managed to get an hprof dump when the issue was happening in the wild. Something notable in the class instance counts as reported by jhat. Here are the top 5 counts for this one node:

{code}
5967846 instances of class org.apache.cassandra.db.CounterColumn 
1247525 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue 
1247310 instances of class org.apache.cassandra.cache.KeyCacheKey 
1246648 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node 
1237526 instances of class org.apache.cassandra.db.RowIndexEntry 
{code}

Is it normal or expected for CounterColumn to have that number of instances?

The data model for how we use counters is as follows: between 50-20000 counter columns per key. We currently have around 3 million keys total, but this issue also replicated when we only had a few thousand keys total. Average column count is around 1k, and 90th is 18k. New columns are added regularly, and columns are incremented regularly. No column or key deletions occur. We probably have 1-5k ""hot"" keys at any given time, spread across the entire ring. R:W ratio is typically around 50:1. This is the only CF we're using counters on, at this time. CF details are as follows:

{code}
    ColumnFamily: CommentTree
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.01
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.LeveledCompactionStrategy
      Compaction Strategy Options:
        sstable_size_in_mb: 160



                Column Family: CommentTree
                SSTable count: 30
                SSTables in each level: [1, 10, 19, 0, 0, 0, 0, 0, 0]
                Space used (live): 4656930594
                Space used (total): 4677221791
                SSTable Compression Ratio: 0.0
                Number of Keys (estimate): 679680
                Memtable Columns Count: 8289
                Memtable Data Size: 2639908
                Memtable Switch Count: 5769
                Read Count: 185479324
                Read Latency: 1.786 ms.
                Write Count: 5377562
                Write Latency: 0.026 ms.
                Pending Tasks: 0
                Bloom Filter False Positives: 2914204
                Bloom Filter False Ratio: 0.56403
                Bloom Filter Space Used: 523952
                Compacted row minimum size: 30
                Compacted row maximum size: 4866323
                Compacted row mean size: 7742
                Average live cells per slice (last five minutes): 39.0
                Average tombstones per slice (last five minutes): 0.0

{code}


Please let me know if I can provide any further information. I can provide the hprof if desired, however it is 3GB so I'll need to provide it outside of JIRA."
CASSANDRA-6403,Division by zero Exception in HintedHandoff and CompactionExecutor,"In write load testing I'm getting division by zero exceptions after running for a while:

ERROR [HintedHandoff:2] 2013-11-23 20:44:41,411 CassandraDaemon.java (line 187) Exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:464)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:309)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92)
        at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:530)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:460)
        ... 6 more

ERROR [CompactionExecutor:8] 2013-11-23 21:34:01,493 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:8,1,RMI Runtime]
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable.<init>(ParallelCompactionIterable.java:59)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


The nodes that encounter this error seem to hold onto a lot of memory which is not freed even after the write load is stopped. With the write load continuing they eventually run out of heap. nodetool compact dies with the same exception."
CASSANDRA-6391,Expose a total memtable size metric for a CF,"We currently expose the size of the current memtable, but that metric doesn't included memtables that are pending a flush and the memtables for indexes. It would be nice to expose all of these as a single gauge metric to get a picture of a single column families heap usage.

"
CASSANDRA-6369,Fix prepared statement size computation,"When computed the size of CQLStatement to limit the prepared statement cache (CASSANDRA-6107), we overestimate the actual memory used because the statement include a reference to the table CFMetaData which measureDeep counts. And as it happens, that reference is big: on a simple test preparing a very trivial select statement, I was able to only prepare 87 statements before some started to be evicted because each statement was more than 93K big and more than 92K of that was the CFMetaData object. As it happens there is no reason to account the CFMetaData object at all since it's in memory anyway whether or not there is prepared statements or not.

Attaching a simple (if not extremely elegant) patch to remove what we don't care about of the computation. Another solution would be to use the MemoryMeter.withTrackerProvider option as we do in Memtable, but in the QueryProcessor case we currently use only one MemoryMeter, not one per CF, so it didn't felt necessarilly cleaner. We could create one-shot MemoryMeter object each time we need to measure a CQLStatement but that doesn't feel a lot simpler/cleaner either. But if someone feels religious about some other solution, I don't care.
"
CASSANDRA-6364,There should be different disk_failure_policies for data and commit volumes or commit volume failure should always cause node exit,"We're doing fault testing on a pre-production Cassandra cluster.  One of the tests was to simulation failure of the commit volume/disk, which in our case is on a dedicated disk.  We expected failure of the commit volume to be handled somehow, but what we found was that no action was taken by Cassandra when the commit volume fail.  We simulated this simply by pulling the physical disk that backed the commit volume, which resulted in filesystem I/O errors on the mount point.

What then happened was that the Cassandra Heap filled up to the point that it was spending 90% of its time doing garbage collection.  No errors were logged in regards to the failed commit volume.  Gossip on other nodes in the cluster eventually flagged the node as down.  Gossip on the local node showed itself as up, and all other nodes as down.

The most serious problem was that connections to the coordinator on this node became very slow due to the on-going GC, as I assume uncommitted writes piled up on the JVM heap.  What we believe should have happened is that Cassandra should have caught the I/O error and exited with a useful log message, or otherwise done some sort of useful cleanup.  Otherwise the node goes into a sort of Zombie state, spending most of its time in GC, and thus slowing down any transactions that happen to use the coordinator on said node.

A limit on in-memory, unflushed writes before refusing requests may also work.  Point being, something should be done to handle the commit volume dying as doing nothing results in affecting the entire cluster.  I should note, we are using: disk_failure_policy: best_effort"
CASSANDRA-6359,sstableloader does not free off-heap memory for index summary,"Although sstableloader tells {{SSTableReaders}} to release their references to the {{IndexSummary}} objects, the summary's {{Memory}} is never {{free()}}'d, causing an off-heap memory leak."
CASSANDRA-6357,Flush memtables to separate directory,"Flush writers are a critical element for keeping a node healthy. When several compactions run on systems with low performing data directories, IO becomes a premium. Once the disk subsystem is saturated, write IO is blocked which will cause flush writer threads to backup. Since memtables are large blocks of memory in the JVM, too much blocking can cause excessive GC over time degrading performance. In the worst case causing an OOM.

Since compaction is running on the data directories. My proposal is to create a separate directory for flushing memtables. Potentially we can use the same methodology of keeping the commit log separate and minimize disk contention against the critical function of the flushwriter. "
CASSANDRA-6356,Proposal: Statistics.db (SSTableMetadata) format change,"We started to distinguish what's loaded to heap, and what's not from Statistics.db. For now, ancestors are loaded as they needed.

Current serialization format is so adhoc that adding new metadata that are not permanently hold onto memory is somewhat difficult and messy. I propose to change serialization format so that a group of stats can be loaded as needed.
"
CASSANDRA-6297,Gossiper blocks when updating tokens and turns node down,"The GossipStage call to SystemTable.updateTokens causes a blocking memtable flush that may get stuck in the postFlushExecutor queue while waiting for other memtables to flush; as a consequence, the Gossiper itself ""blocks"" and the node is turned down."
CASSANDRA-6275,2.0.x leaks file handles,"Looks like C* is leaking file descriptors when doing lots of CAS operations.

{noformat}
$ sudo cat /proc/15455/limits
Limit                     Soft Limit           Hard Limit           Units    
Max cpu time              unlimited            unlimited            seconds  
Max file size             unlimited            unlimited            bytes    
Max data size             unlimited            unlimited            bytes    
Max stack size            10485760             unlimited            bytes    
Max core file size        0                    0                    bytes    
Max resident set          unlimited            unlimited            bytes    
Max processes             1024                 unlimited            processes
Max open files            4096                 4096                 files    
Max locked memory         unlimited            unlimited            bytes    
Max address space         unlimited            unlimited            bytes    
Max file locks            unlimited            unlimited            locks    
Max pending signals       14633                14633                signals  
Max msgqueue size         819200               819200               bytes    
Max nice priority         0                    0                   
Max realtime priority     0                    0                   
Max realtime timeout      unlimited            unlimited            us 
{noformat}

Looks like the problem is not in limits.

Before load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
166

cassandra-test1 ~]$ lsof -n | grep java | wc -l
164

cassandra-test2 ~]$ lsof -n | grep java | wc -l
180
{noformat}

After load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
967

cassandra-test1 ~]$ lsof -n | grep java | wc -l
1766

cassandra-test2 ~]$ lsof -n | grep java | wc -l
2578
{noformat}

Most opened files have names like:
{noformat}
java      16890 cassandra 1636r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1637r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1638r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1639r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1640r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1641r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1642r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1643r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1644r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1645r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1646r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1647r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1648r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1649r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1650r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1651r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1652r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1653r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1654r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1655r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1656r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
{noformat}

Also, when that happens it's not always possible to shutdown server process via SIGTERM. Have to use SIGKILL.

p.s. See mailing thread for more context information https://www.mail-archive.com/user@cassandra.apache.org/msg33035.html"
CASSANDRA-6271,Replace SnapTree in AtomicSortedColumns,"On the write path a huge percentage of time is spent in GC (>50% in my tests, if accounting for slow down due to parallel marking). SnapTrees are both GC unfriendly due to their structure and also very expensive to keep around - each column name in AtomicSortedColumns uses > 100 bytes on average (excluding the actual ByteBuffer).

I suggest using a sorted array; changes are supplied at-once, as opposed to one at a time, and if < 10% of the keys in the array change (and data equal to < 10% of the size of the key array) we simply overlay a new array of changes only over the top. Otherwise we rewrite the array. This method should ensure much less GC overhead, and also save approximately 80% of the current memory overhead.

TreeMap is similarly difficult object for the GC, and a related task might be to remove it where not strictly necessary, even though we don't keep them hanging around for long. TreeMapBackedSortedColumns, for instance, seems to be used in a lot of places where we could simply sort the columns."
CASSANDRA-6255,Exception count not incremented on OutOfMemoryError (HSHA),"One of our nodes decided to stop listening on 9160 (netstat -l was showing nothing and telnet was reporting connection refused). Nodetool status showed no hosts down and on the offending node nodetool info gave the following:

{noformat}
nodetool info
Token            : (invoke with -T/--tokens to see all 256 tokens)
ID               : (removed)
Gossip active    : true
Thrift active    : true
Native Transport active: false
Load             : 2.05 TB
Generation No    : 1382536528
Uptime (seconds) : 432970
Heap Memory (MB) : 8098.05 / 14131.25
Data Center      : DC1
Rack             : RAC2
Exceptions       : 0
Key Cache        : size 536854996 (bytes), capacity 536870912 (bytes), 41383646 hits, 1710831591 requests, 0.024 recent hit rate, 0 save period in seconds
Row Cache        : size 0 (bytes), capacity 0 (bytes), 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
{noformat}

After looking at the cassandra log, I saw a bunch of the following:

{noformat}
ERROR [Selector-Thread-16] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
ERROR [Selector-Thread-7] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
{noformat}

There wasn't anything else overtly suspicious in the logs except for the occasional 
{noformat}
ERROR [Selector-Thread-0] 2013-10-27 17:35:58,662 TNonblockingServer.java (line 468) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
{noformat}
but  that periodically comes up - I have looked into it before but it has never seemed to have any serious impact.

This ticket is not about *why* an OutOfMemoryError occurred - which is bad but I don't think I have enough information to reproduce or speculate on a cause. This ticket is about the fact that an OutOfMemoryError occurred and nodetool info was reporting Thrift active : true and Exceptions : 0. 

Our monitoring systems and investigation processes are both starting to rely on on the exception count. The fact that it was not accurate here is disconcerting."
CASSANDRA-6249,Keep memtable data size updated even during flush; add a method to calculate total memtables size (incl pending flush),Keep memtable data size updated even during flush. Add a method to calculate total memtables size (including pending flush ones).
CASSANDRA-6191,Add a warning for small sstable size setting in LCS,"Not sure ""bug"" is the right description, because I can't say for sure that the large number of SSTables is the cause of the memory issues. I'll share my research so far:

Under high read-load with a very large number of compressed SSTables (caused by the initial default 5mb sstable_size in LCS) it seems memory is exhausted, without any room for GC to fix this. It tries to GC but doesn't reclaim much.

The node first hits the ""emergency valves"" flushing all memtables, then reducing caches. And finally logs 0.99+ heap usages and hangs with GC failure or crashes with OutOfMemoryError.

I've taken a heapdump and started analysis to find out what's wrong. The memory seems to be used by the byte[] backing the HeapByteBuffer in the ""compressed"" field of org.apache.cassandra.io.compress.CompressedRandomAccessReader. The byte[] are generally 65536 byes in size, matching the block-size of the compression.

Looking further in the heap-dump I can see that these readers are part of the pool in org.apache.cassandra.io.util.CompressedPoolingSegmentedFile. Which is linked to the ""dfile"" field of org.apache.cassandra.io.sstable.SSTableReader. The dump-file lists 45248 instances of CompressedRandomAccessReader.

Is this intended to go this way? Is there a leak somewhere? Or should there be an alternative strategy and/or warning for cases where a node is trying to read far too many SSTables?

EDIT:
Searching through the code I found that PoolingSegmentedFile keeps a pool of RandomAccessReader for re-use. While the CompressedRandomAccessReader allocates a ByteBuffer in it's constructor and (to make things worse) enlarges it if it's reasing a large chunk. This (sometimes enlarged) ByteBuffer is then kept alive because it becomes part of the CompressedRandomAccessReader which is in turn kept alive as part of the pool in the PoolingSegmentedFile."
CASSANDRA-6180,NPE in CqlRecordWriter: Related to AbstractCassandraStorage handling null values,"I encountered an issue with the {{CqlStorage}} and it's handling of null values. The {{CqlRecordWriter}} throws an NPE when a value is null. I found a related ticket CASSANDRA-5885 and applied the there stated fix to the {{AbstractCassandraStorage}}.
Instead of converting {{null}} values to {{ByteBuffer.wrap(new byte[0])}} {{AbstractCassandraStorage}} returns {{(ByteBuffer)null}}

This issue can be reproduced with the attached files: {{test_null.cql}}, {{test_null_data}}, {{null_test.pig}}

A fix can be found in the attached patch.

{code}
java.io.IOException: java.lang.NullPointerException
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:248)
Caused by: java.lang.NullPointerException
	at org.apache.thrift.protocol.TBinaryProtocol.writeBinary(TBinaryProtocol.java:194)
	at org.apache.cassandra.thrift.Cassandra$execute_prepared_cql3_query_args.write(Cassandra.java:41253)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
	at org.apache.cassandra.thrift.Cassandra$Client.send_execute_prepared_cql3_query(Cassandra.java:1683)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_prepared_cql3_query(Cassandra.java:1673)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:232)
{code}"
CASSANDRA-6126,Set MALLOC_ARENA_MAX in cassandra-env.sh for new glibc per-thread allocator,"Cassandra does not take into account particular GLIBC environment variables and the nature of the JVM.  Cassandra should set the MALLOC_ARENA_MAX variable, ideally in cassandra-env.sh, to have something like the following:
{noformat}
export MALLOC_ARENA_MAX=${MALLOC_ARENA_MAX:-4}
{noformat}

This will limit the number of per-thread memory allocation arenas (ie separate memory ranges dedicated to each thread for memory allocation) for the new per-thread GLIBC malloc found in distribution like Enterprise Linux 6 and newer.  The net effect is a performance gain, specifically through reduced page-table size and kernel overhead of memory management.  Without the setting, Cassandra will occupy more than double the amount of virtual memory space, and due to increased pagetable size, the resident amount of memory will also be somewhat larger (~10% or so).  For more discussion, see HADOOP-7154.  Bottom line, cassandra-env.sh need to be setting MALLOC_ARENA_MAX to better use system resources.

In general it can be stated all JVMs should use this environment setting as JVMs tend to be highly threaded and manage their own heap.  Cassandra and Hadoop in particular seem to benefit from my testing.

More background: http://siddhesh.in/journal/2012/10/24/malloc-per-thread-arenas-in-glibc/"
CASSANDRA-6125,Race condition in Gossip propagation,"Gossip propagation has a race when concurrent VersionedValues are created and submitted/propagated, causing some updates to be lost, even if happening on different ApplicationStatuses.
That's what happens basically:
1) A new VersionedValue V1 is created with version X.
2) A new VersionedValue V2 is created with version Y = X + 1.
3) V2 is added to the endpoint state map and propagated.
4) Nodes register Y as max version seen.
5) At this point, V1 is added to the endpoint state map and propagated too.
6) V1 version is X < Y, so nodes do not ask for his value after digests.

A possible solution would be to propagate/track per-ApplicationStatus versions, possibly encoding them to avoid network overhead."
CASSANDRA-6107,CQL3 Batch statement memory leak,"We are doing large volume insert/update tests on a CASS via CQL3. 


Using 4GB heap, after roughly 750,000 updates create/update 75,000 row keys, we run out of heap, and it never dissipates, and we begin getting this infamous error which many people seem to be encountering:

WARN [ScheduledTasks:1] 2013-09-26 16:17:10,752 GCInspector.java (line 142) Heap is 0.9383457210434385 full.  You may need to reduce memtable and/or cache sizes.  Cassandra will now flush up to the two largest memtables to free up memory.  Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically
 INFO [ScheduledTasks:1] 2013-09-26 16:17:10,753 StorageService.java (line 3614) Unable to reduce heap usage since there are no dirty column families


8 and 12 GB heaps appear to delay the problem by roughly proportionate amounts of 75,000 - 100,000 rowkeys per 4GB. Each run of 50,000 row key creations sees the heap grow and never shrink again. 

We have attempted to no effect:
- removing all secondary indexes to see if that alleviates overuse of bloom filters 
- adjusted parameters for compaction throughput
- adjusted memtable flush thresholds and other parameters 

By examining heapdumps, it seems apparent that the problem is perpetual retention of CQL3 BATCH statements. We have even tried dropping the keyspaces after the updates and the CQL3 statement are still visible in the heapdump, and after many many many CMS GC runs. G1 also showed this issue.

The 750,000 statements are broken into batches of roughly 200 statements."
CASSANDRA-6087,Node OOMs on commit log replay when starting up,"After some activity on batchlogs and hints and CFs restarted nodes without finished draining. On startup it OOMs. 

Investigating it found, that memtables occupied all available RAM, because MeteredFlusher is started later in setup code, so memtables cannot flush. 

Fixed by starting metered flusher before commit log replay starts"
CASSANDRA-6079,"Memtables flush is delayed when having a lot of batchlog activity, making node OOM","Both MeteredFlusher and BatchlogManager share the same OptionalTasks thread. So, when batchlog manager processes its tasks no flushes can occur. Even more, batchlog manager waits for batchlog CF compaction to finish.

On a lot of batchlog activity this prevents memtables from flush for a long time, making the node OOM.

Fixed this by moving batchlog to its own thread and not waiting for batchlog compaction to finish."
CASSANDRA-6072,NPE in Pig CassandraStorage,"key_alias can be null for tables created from thrift.

Which causes an NPE here:
https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/hadoop/pig/AbstractCassandraStorage.java#L633"
CASSANDRA-6047,Memory leak when using snapshot repairs,"Running nodetool repair repeatedly with the -snapshot parameter results in a native memory leak. The JVM process will take up more and more physical memory until it is killed by the Linux OOM killer.

The command used was as follows:

nodetool repair keyspace -local -snapshot -pr -st start_token -et end_token

Removing the -snapshot flag prevented the memory leak.  The subrange repair necessitated multiple repairs, so it made the problem noticeable, but I believe the problem would be reproducible even if you ran repair repeatedly without specifying a start and end token.

Notes from [~yukim]:

Probably the cause is too many snapshots. Snapshot sstables are opened during validation, but memories used are freed when releaseReferences called. But since snapshots never get marked compacted, memories never freed.

We only cleanup mmap'd memories when sstable is mark compacted. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/SSTableReader.java#L974

Validation compaction never marks snapshots compacted.
"
CASSANDRA-6025,Deleted row resurrects if was not compacted in GC Grace Timeout due to thombstone read optimization in CollactionController,"How to reproduce:
1. Insert column
2. Flush, so you'll have sstable-1
3. Delete just inserted column
4. Flush, now you have sstable-2 as well
5. Left it uncompacted for more then gc grace time or just use 0, so you dont have to wait
6. Read data form column. You'll read just deleted column


{code}
            /* add the SSTables on disk */
// This sorts sstables in the order sstable-2, sstable-1
            Collections.sort(view.sstables, SSTable.maxTimestampComparator);
//...
            for (SSTableReader sstable : view.sstables)
            {
//...
                if (iter.getColumnFamily() != null)
//...
                    while (iter.hasNext())
                    {
                        OnDiskAtom atom = iter.next();
// the problem is here. reading atom after gc grace time
// makes this condition false. so tombstone from sstable-2
// is not placed to temp container and is just thrown away.
// On next iteration of outer for statement an original
// data inserted in step 1 from sstable-1 will be read and
// placed to temp.
                        if (atom.getLocalDeletionTime() >= gcBefore)
                            temp.addAtom(atom);
//
                    }

// .. so at the end of the for statemet we resolve data from temp. which
// do not have tombstone at all -> data are resurrected.
               container.addAll(temp, HeapAllocator.instance);
 
}
{code}
"
CASSANDRA-5982,OutOfMemoryError when writing text blobs to a very large number of tables,"This test goes outside the norm for Cassandra, creating ~2000 column families, and writing large text blobs to them. 

The process goes like this:

Bring up a 6 node m2.2xlarge cluster on EC2. This instance type has enough memory (34.2GB) so that Cassandra will allocate a full 8GB heap without tuning cassandra-env.sh. However, this instance type only has a single drive, so data and commitlog are comingled. (This test has also been run m1.xlarge instances which have four drives (but lower memory) and has exhibited similar results when assigning one to commitlog and 3 to datafile_directories.)

Use the 'memtable_allocator: HeapAllocator' setting from CASSANDRA-5935.

Create 2000 CFs:
{code}
CREATE KEYSPACE cf_stress WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}
CREATE COLUMNFAMILY cf_stress.tbl_00000 (id timeuuid PRIMARY KEY, val1 text, val2 text, val3 text ) ;
# repeat for tbl_00001, tbl_00002 ... tbl_02000
{code}

This process of creating tables takes a long time, about 5 hours, but for anyone wanting to create that many tables, presumably they only need to do this once, so this may be acceptable.

Write data:

The test dataset consists of writing 100K, 1M, and 10M documents to these tables:

{code}
INSERT INTO {table_name} (id, val1, val2, val3) VALUES (?, ?, ?, ?)
{code}

With 5 threads doing these inserts across the cluster, indefinitely, randomly choosing a table number 1-2000, the cluster eventually topples over with 'OutOfMemoryError: Java heap space'.

A heap dump analysis indicates that it's mostly memtables:

!2000CF_memtable_mem_usage.png!

Best current theory is that this is commitlog bound and that the memtables cannot flush fast enough due to locking issues. But I'll let [~jbellis] comment more on that.
"
CASSANDRA-5968,Nodetool info throws NPE when connected to a booting instance,"When an instance is newly added to the cluster and it's still streaming stuff, trying to call nodetool info on it throws NPE. Stack trace below.

To replicate: add a new node to the cluster, run nodetool info before bootstrap is complete.

Expected behaviour: is nice and just says RPC server is not running.

{noformat}
$ nodetool info
Token            : (invoke with -T/--tokens to see all 0 tokens)
ID               : cc7bcf48-4a54-48af-97f6-99c82bce76f2
Gossip active    : true
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.isRPCServerRunning(StorageService.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}
"
CASSANDRA-5957,Cannot drop keyspace Keyspace1 after running cassandra-stress,"Steps to reproduce:
# Set MAX_HEAP=""2G"", HEAP_NEWSIZE=""400M""
# Run ./cassandra-stress -n 50000 -c 400 -S 256
# The test should complete despite several warnings about low heap memory.
# Try to drop keyspace:
{noformat}
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
{noformat}

system.log:
{noformat}
 INFO 15:10:46,516 Enqueuing flush of Memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,516 Writing Memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,690 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-6-Data.db (38 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794574)
 INFO 15:10:46,692 Enqueuing flush of Memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,693 Writing Memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,857 Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-ic-6-Data.db (38 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794574)
 INFO 15:10:46,897 Enqueuing flush of Memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)
 INFO 15:10:46,898 Writing Memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)
 INFO 15:10:47,064 Completed flushing /var/lib/cassandra/data/system/local/system-local-ic-12-Data.db (139 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794845)
 INFO 15:10:48,956 Enqueuing flush of Memtable-local@432522279(46/46 serialized/live bytes, 1 ops)
 INFO 15:10:48,957 Writing Memtable-local@432522279(46/46 serialized/live bytes, 1 ops)
 INFO 15:10:49,132 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 400882073/1094043713)bytes
 INFO 15:10:49,175 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 147514075/645675954)bytes
 INFO 15:10:49,185 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 223249644/609072261)bytes
 INFO 15:10:49,202 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 346471085/990388210)bytes
 INFO 15:10:49,215 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 294748503/2092376617)bytes
 INFO 15:10:49,257 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 692722235/739328646)bytes
 INFO 15:10:49,285 Completed flushing /var/lib/cassandra/data/system/local/system-local-ic-13-Data.db (82 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794974)
 INFO 15:10:49,286 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-13-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-12-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-11-Data.db')]
ERROR 15:10:49,287 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:378)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:262)
	at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:718)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:775)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1668)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4048)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4036)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:374)
	... 13 more
Caused by: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:354)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:325)
	at org.apache.cassandra.db.DataTracker.unreferenceSSTables(DataTracker.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:302)
	at org.apache.cassandra.db.Table.unloadCf(Table.java:314)
	at org.apache.cassandra.db.Table.dropCf(Table.java:296)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:607)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:469)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
ERROR 15:10:49,287 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:354)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:325)
	at org.apache.cassandra.db.DataTracker.unreferenceSSTables(DataTracker.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:302)
	at org.apache.cassandra.db.Table.unloadCf(Table.java:314)
	at org.apache.cassandra.db.Table.dropCf(Table.java:296)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:607)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:469)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
 INFO 15:10:49,471 Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ic-14,].  829 bytes to 501 (~60% of original) in 184ms = 0,002597MB/s.  4 total rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
{noformat}

"
CASSANDRA-5955,The native protocol server can trigger a Netty bug,"The patch from CASSANDRA-5926 did fix the original deadlock, but unfortunately we can now run into a netty bug (with MemoryAwareThreadPoolExecutor): https://github.com/netty/netty/issues/1310.

That bug has been fixed in netty 3.6.6 but we're currently using an older version (3.5.9). So we should just upgrade our dependency to 3.6.6. "
CASSANDRA-5910,Most CQL3 functions should handle null gracefully,"Currently, we don't allow null parameters for functions. So
{noformat}
UPDATE test SET d=dateOf(null) WHERE k=0
{noformat}
is basically an invalid query. Unfortunately, there's at least one case where we don't validate correctly, namely if we do:
{noformat}
SELECT k, dateOf(t) FROM test
{noformat}
In that case, if for any of the row {{t}} is null, we end up with a server side NPE. But more importantly, throwing an InvalidException in that case would be pretty inconvenient and actually somewhat wrong since the query is not invalid in itself. So, at least in that latter case, we want {{dateOf(t) == null}} when {{t == null}}. And if we do that, I suggest making it always the case (i.e. make the first query valid but assigning {{null}} to {{d}}).
"
CASSANDRA-5906,Avoid allocating over-large bloom filters,"We conservatively estimate the number of partitions post-compaction to be the total number of partitions pre-compaction.  That is, we assume the worst-case scenario of no partition overlap at all.

This can result in substantial memory wasted in sstables resulting from highly overlapping compactions."
CASSANDRA-5903,Integer overflow in OffHeapBitSet when bloomfilter > 2GB,"In org.apache.cassandra.utils.obs.OffHeapBitSet.

byteCount overflows and causes an IllegalArgument exception in Memory.allocate when bloomfilter is > 2GB.

Suggest changing byteCount to long.

{code:title=OffHeapBitSet.java}
    public OffHeapBitSet(long numBits)
    {
        // OpenBitSet.bits2words calculation is there for backward compatibility.
        int byteCount = OpenBitSet.bits2words(numBits) * 8;
        bytes = RefCountedMemory.allocate(byteCount);
        // flush/clear the existing memory.
        clear();
    }

{code}"
CASSANDRA-5885,NPE in CqlRecordWriter when sinking null values,"null values cause a Thrift NPE when sinking to CqlRecordWriter (replacing the null values with an appropriate non-null sentinel works) :

	at java.lang.Thread.run(Thread.java:680) ~[na:1.6.0_51]
java.io.IOException: java.lang.NullPointerException
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:245) ~[cassandra-all-1.2.8.jar:1.2.8]
java.lang.NullPointerException: null
	at org.apache.thrift.protocol.TBinaryProtocol.writeBinary(TBinaryProtocol.java:194) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.Cassandra$execute_prepared_cql3_query_args.write(Cassandra.java:41253) ~[cassandra-thrift-1.2.8.jar:1.2.8]
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.Cassandra$Client.send_execute_prepared_cql3_query(Cassandra.java:1683) ~[cassandra-thrift-1.2.8.jar:1.2.8]
	at org.apache.cassandra.thrift.Cassandra$Client.execute_prepared_cql3_query(Cassandra.java:1673) ~[cassandra-thrift-1.2.8.jar:1.2.8]
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:229) ~[cassandra-all-1.2.8.jar:1.2.8]"
CASSANDRA-5882,Changing column type from int to bigint or vice versa causes decoding errors.,"cqlsh:dbsite> create table testint (id uuid, bestof bigint, primary key (id) );
cqlsh:dbsite> insert into testint (id, bestof ) values (49d30f84-a409-4433-ad60-eb9c1a06b7bb, 1376399966);
cqlsh:dbsite> insert into testint (id, bestof ) values (6cab4798-ad29-4419-bd59-308f9ec3bc44, 1376389800);
cqlsh:dbsite> insert into testint (id, bestof ) values (685bb9ff-a4fe-4e47-95eb-f6a353d9e179, 1376390400);
cqlsh:dbsite> insert into testint (id, bestof ) values (a848f832-5ded-4ef7-bf4b-7db561564c57, 1376391000);
cqlsh:dbsite> select * from testint ;
 id                                   | bestof
--------------------------------------+------------
 a848f832-5ded-4ef7-bf4b-7db561564c57 | 1376391000
 49d30f84-a409-4433-ad60-eb9c1a06b7bb | 1376399966
 6cab4798-ad29-4419-bd59-308f9ec3bc44 | 1376389800
 685bb9ff-a4fe-4e47-95eb-f6a353d9e179 | 1376390400

cqlsh:dbsite> alter table testint alter bestof TYPE int;
cqlsh:dbsite> select * from testint ;
 id                                   | bestof
--------------------------------------+-----------------------------
 a848f832-5ded-4ef7-bf4b-7db561564c57 |  '\x00\x00\x00\x00R\n\x0fX'
 49d30f84-a409-4433-ad60-eb9c1a06b7bb |     '\x00\x00\x00\x00R\n2^'
 6cab4798-ad29-4419-bd59-308f9ec3bc44 | '\x00\x00\x00\x00R\n\n\xa8'
 685bb9ff-a4fe-4e47-95eb-f6a353d9e179 | '\x00\x00\x00\x00R\n\r\x00'

Failed to decode value '\x00\x00\x00\x00R\n\x0fX' (for column 'bestof') as int: unpack requires a string argument of length 4
Failed to decode value '\x00\x00\x00\x00R\n2^' (for column 'bestof') as int: unpack requires a string argument of length 4
2 more decoding errors suppressed.


I realize that going from BIGINT to INT would cause overflow if a column contained a number larger than 2^31-1, it is at least technically possible to go in the other direction.  I also understand that rewriting all the data in the correct format would be a very expensive operation on a large column family, but if that's not something we want to allow we should explicitly disallow changing data types if the table has any rows."
CASSANDRA-5865,NPE when you mistakenly set listen_address to 0.0.0.0,"It's clearly stated that setting {{listen_address}} to {{0.0.0.0}} is always wrong. But if you mistakenly do it anyway you end up with an NPE on 1.2.8 while it's not the case on 2.0.0-rc1. See bellow:

{code}
 INFO 16:34:43,598 JOINING: waiting for ring information
 INFO 16:34:44,505 Handshaking version with /127.0.0.1
 INFO 16:34:44,533 Handshaking version with /0.0.0.0
 INFO 16:35:13,626 JOINING: schema complete, ready to bootstrap
 INFO 16:35:13,631 JOINING: getting bootstrap token
ERROR 16:35:13,633 Exception encountered during startup
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
ERROR 16:35:13,668 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:519)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{code}"
CASSANDRA-5852,json2sstable breaks on data exported from sstable2json.,"Attached is a JSON formatted sstable generated by sstable2json.

This file cannot be loaded back into Cassandra via json2sstable; it outputs this error:

{code}
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
Importing 16 keys...
java.lang.NumberFormatException: Non-hex characters in value6
	at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:60)
	at org.apache.cassandra.utils.ByteBufferUtil.hexToBytes(ByteBufferUtil.java:503)
	at org.apache.cassandra.tools.SSTableImport.stringAsType(SSTableImport.java:578)
	at org.apache.cassandra.tools.SSTableImport.access$000(SSTableImport.java:59)
	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:154)
	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:231)
	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:214)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:432)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:319)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:543)
ERROR: Non-hex characters in value6
{code}

Steps to reproduce:

{code}
$ ccm create -v git:trunk test-json-import
Fetching Cassandra updates...
Current cluster is now: test-json-import
$ ccm populate -n 1
$ ccm start
$ ccm node1 cqlsh
Connected to test-json-import at 127.0.0.1:9160.
[cqlsh 4.0.0 | Cassandra 2.0.0-rc1-SNAPSHOT | CQL spec 3.1.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> CREATE TABLE test.test (key varchar PRIMARY KEY, value varchar);
cqlsh> INSERT INTO test.test (key, value) VALUES ('ryan', 'ryan');
cqlsh> 
$ ccm node1 flush
$ ccm stop
$ ~/.ccm/test-json-import/node1/bin/json2sstable -s -K test -c test ~/Downloads/import_error/r.json ~/.ccm/test-json-import/node1/data/test/test/test-test-ja-1-Data.db 
{code}"
CASSANDRA-5821,Test new ConcurrentLinkedHashMap implementation (1.4RC),"There are some [improvements being made to CLHM| https://code.google.com/p/concurrentlinkedhashmap/source/detail?r=888ad7cebe5b509e5e713b00836f5df9f50baf32] that we should test.

Create a small enough dataset that it can fit in memory and devise a test that has a high key cache hit rate, and compare results to CLHM 1.3 we already use."
CASSANDRA-5815,NPE from migration manager,"In one of our production clusters we see this error often. Looking through the source, Gossiper.instance.getEndpointStateForEndpoint(endpoint) is returning null for some end point. De we need any config change on our end to resolve this? In any case, cassandra should be updated to protect against this NPE.

{noformat}
ERROR [OptionalTasks:1] 2013-07-24 13:40:38,972 AbstractCassandraDaemon.java (line 132) Exception in thread Thread[OptionalTasks:1,5,main] 
java.lang.NullPointerException 
at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:134) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) 
at java.util.concurrent.FutureTask.run(FutureTask.java:138) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)
{noformat}

It turned out that the reason for NPE was we bootstrapped a node with the same token as another node. Cassandra should not throw an NPE here but log a meaningful error message. "
CASSANDRA-5805,CQL 'set' returns incorrect value,"CQL 'set' returns incorrect value after flush.
Create the following table:

{code}
CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
USE ks;
CREATE TABLE cf ( k int PRIMARY KEY , s set<int> );
{code}

Insert data:

{code}
INSERT INTO cf (k, s) VALUES (1, {1});
INSERT INTO cf (k, s) VALUES (1, {2});
{code}

This should return:

{code}
cqlsh:ks> SELECT * FROM cf;

 k | s
---+--------
 1 | {2}
{code}

and it does when no flush has happened.

But when I do flush after each insert, it starts returning:

{code}
cqlsh:ks> SELECT * FROM cf;

 k | s
---+--------
 1 | {1, 2}
{code}

'system.local' table flushes every time it inserts(updates) tokens, and this behavior is causing 'nodetool move' to act weirdly."
CASSANDRA-5750,CLI can show bad DESCRIBE for CQL3 CF if given the CF explicitly,"The CLI omits CQL3 tables if you do a regular describe command. It also emits a nice warning about it. However, if you do a describe with an explicit CF name, it does something a bit unintuitive:

{code}
[default@ryan] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: null
{code}

In this case it emitted the WARNING message, but it still showed the table anyway, and many of the CF settings are incorrect because of this. Better to show nothing than incorrect values."
CASSANDRA-5749,DESC TABLE omits some column family settings,"In CQL I can create a table with settings introduced in 2.0:

{code}
cqlsh:Keyspace1> CREATE TABLE r1 ( key int PRIMARY KEY, value varchar) WITH speculative_retry='ALWAYS';
{code}

But the settings don't show up when I DESC TABLE:
{code}
cqlsh:Keyspace1> DESC TABLE r1;

CREATE TABLE r1 (
  key int PRIMARY KEY,
  value text
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

For comparison, here is the same table viewed from cassandra-cli:

{code}
[default@Keyspace1] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

WARNING: Could not connect to the JMX on 127.0.0.1:7199 - some information won't be shown.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Default time to live: 0
      Bloom Filter FP chance: default
      Index interval: default
      Speculative Retry: NONE
      Compaction Strategy: null
{code}

Ideally, all of these values that cli shows would be shown by cqlsh.
"
CASSANDRA-5704,"add cassandra.unsafesystem property (Truncate flushes to disk again in 1.2, even with durable_writes=false)","I just upgraded my dev-environment to C* 1.2. Unfortunetaly 1.2 makes my JUnit tests slow again, due to a blocking-flush in saveTruncationRecord().

With Cassandra 1.1 truncate was very fast due to: CASSANDRA-4153


My proposal is to make saveTruncationRecord() only flush when durableWrites are enabled.

My assumption is that if somebody turn off durable writes then he does not mind if truncate is not guaranteed to be durable either.


I successfully tested the following patch with my testsuite. Its as fast as it was with 1.1 (maybe even faster!):
{code}
@@ -186,5 +186,8 @@ public class SystemTable
         String req = ""UPDATE system.%s SET truncated_at = truncated_at + %s WHERE key = '%s'"";
         processInternal(String.format(req, LOCAL_CF, truncationAsMapEntry(cfs, truncatedAt, position), LOCAL_KEY));
-        forceBlockingFlush(LOCAL_CF);
+        
+        KSMetaData ksm = Schema.instance.getKSMetaData(cfs.table.name);
+        if (ksm.durableWrites) // flush only when durable_writes are enabled
+            forceBlockingFlush(LOCAL_CF);
     }
{code}
"
CASSANDRA-5699,Streaming (2.0) can deadlock,"The new streaming implementation (CASSANDRA-5286) creates 2 threads per host for streaming, one for the incoming stream and one for the outgoing one. However, both currently share the same socket, but since we use synchronous I/O, a read can block a write, which can result in a deadlock if 2 nodes are both blocking on a read a the same time, thus blocking their respective writes (this is actually fairly easy to reproduce with a simple repair).

So instead attaching a patch that uses one socket per thread.

The patch also correct the stream throughput throttling calculation that was 8000 times lower than what it should be."
CASSANDRA-5692,Race condition in detecting version on a mixed 1.1/1.2 cluster,"On a mixed 1.1 / 1.2 cluster, starting 1.2 nodes fires sometimes a race condition in version detection, where the 1.2 node wrongly detects version 6 for a 1.1 node.

It works as follows:
1) The just started 1.2 node quickly opens an OutboundTcpConnection toward a 1.1 node before receiving any messages from the latter.
2) Given the version is correctly detected only when the first message is received, the version is momentarily set at 6.
3) This opens an OutboundTcpConnection from 1.2 to 1.1 at version 6, which gets stuck in the connect() method.

Later, the version is correctly fixed, but all outbound connections from 1.2 to 1.1 are stuck at this point.

Evidence from 1.2 logs:
TRACE 13:48:31,133 Assuming current protocol version for /127.0.0.2
DEBUG 13:48:37,837 Setting version 5 for /127.0.0.2"
CASSANDRA-5670,running compact on an index did not compact two index files into one,"With a data directory containing secondary index files ending in -1 and -2, I expected that when I ran compact against the index that they would compact down to a set of -3 files.  This column family uses SizeTieredCompactionStrategy.

Using our standard CQL example, the compact command used was: 
$ ./nodetool compact test1 test1-playlists.playlists_artist_idx

Please note: reproducing this test on 1.1.12 (using a single primary key), you will see that running compact on the keyspace also does not compact the index file.  There is no option to compact the index, so I could not compare that.

{noformat}
CREATE KEYSPACE test1 WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};

use test1;

CREATE TABLE playlists (
  id uuid,
  song_order int,
  song_id uuid,
  title text,
  album text,
  artist text,
  PRIMARY KEY  (id, song_order ) );

INSERT INTO playlists (id, song_order, song_id, title, artist, album)
  VALUES (62c36092-82a1-3a00-93d1-46196ee77204, 1,
  a3e64f8f-bd44-4f28-b8d9-6938726e34d4, 'La Grange', 'ZZ Top', 'Tres Hombres');

select * from playlists;

=====================================
./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db				
test1-playlists-ic-1-Data.db	
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					

=====================================

CREATE INDEX ON playlists(artist );
select * from playlists;
select * from playlists where artist = 'ZZ Top';

=====================================
$ ./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db			
test1-playlists-ic-1-Data.db					
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					
	
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt

=====================================

delete artist from playlists where id = 62c36092-82a1-3a00-93d1-46196ee77204 and song_order = 1;
select * from playlists;
select * from playlists where artist = 'ZZ Top';

=====================================
$ ./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db	
test1-playlists-ic-1-Data.db					
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					
test1-playlists-ic-2-CompressionInfo.db				
test1-playlists-ic-2-Data.db					
test1-playlists-ic-2-Filter.db					
test1-playlists-ic-2-Index.db					
test1-playlists-ic-2-Statistics.db				
test1-playlists-ic-2-Summary.db					
test1-playlists-ic-2-TOC.txt
			
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt

=====================================

./nodetool compact test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-3-CompressionInfo.db
test1-playlists-ic-3-Data.db
test1-playlists-ic-3-Filter.db
test1-playlists-ic-3-Index.db
test1-playlists-ic-3-Statistics.db
test1-playlists-ic-3-Summary.db
test1-playlists-ic-3-TOC.txt
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt

=====================================

$ ./nodetool compact test1 test1-playlists.playlists_artist_idx

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-3-CompressionInfo.db
test1-playlists-ic-3-Data.db
test1-playlists-ic-3-Filter.db
test1-playlists-ic-3-Index.db
test1-playlists-ic-3-Statistics.db
test1-playlists-ic-3-Summary.db
test1-playlists-ic-3-TOC.txt
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt


=====================================
cqlsh:test1> describe keyspace test1;

CREATE KEYSPACE test1 WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE test1;

CREATE TABLE playlists (
  id uuid,
  song_order int,
  album text,
  artist text,
  song_id uuid,
  title text,
  PRIMARY KEY (id, song_order)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX playlists_artist_idx ON playlists (artist);

{noformat}
"
CASSANDRA-5661,Discard pooled readers for cold data,"Reader pooling was introduced in CASSANDRA-4942 but pooled RandomAccessReaders are never cleaned up until the SSTableReader is closed.  So memory use is ""the worst case simultaneous RAR we had open for this file, forever.""

We should introduce a global limit on how much memory to use for RAR, and evict old ones."
CASSANDRA-5619,CAS UPDATE for a lost race: save round trip by returning column values,"Looking at the new CAS CQL3 support examples [1], if one lost a race for an UPDATE, to save a round trip to get the current values to decide if you need to perform your work, could the columns that were used in the IF clause also be returned to the caller?  Maybe the columns values as part of the SET part could also be returned.

I don't know if this is generally useful though.

In the case of creating a new user account with a given username which is the partition key, if one lost the race to another person creating an account with the same username, it doesn't matter to the loser what the column values are, just that they lost.

I'm new to Cassandra, so maybe there's other use cases, such as doing incremental amount of work on a row.  In pure Java projects I've done while loops around AtomicReference.html#compareAndSet() until the work was done on the referenced object to handle multiple threads each making forward progress in updating the references object.

[1] https://github.com/riptano/cassandra-dtest/blob/master/cql_tests.py#L3044"
CASSANDRA-5605,Crash caused by insufficient disk space to flush,"A few times now I have seen our Cassandra nodes crash by running themselves out of memory. It starts with the following exception:

{noformat}
ERROR [FlushWriter:13000] 2013-05-31 11:32:02,350 CassandraDaemon.java (line 164) Exception in thread Thread[FlushWriter:13000,5,main]
java.lang.RuntimeException: Insufficient disk space to write 8042730 bytes
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat} 

After which, it seems the MemtablePostFlusher stage gets stuck and no further memtables get flushed: 

{noformat} 
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,467 StatusLogger.java (line 68) MemtablePostFlusher               1        32         0
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,469 StatusLogger.java (line 73) CompactionManager                 1         2
{noformat} 

What makes this ridiculous is that, at the time, the data directory on this node had 981GB free disk space (as reported by du). We primarily use STCS and at the time the aforementioned exception occurred, at least one compaction task was executing which could have easily involved 981GB (or more) worth of input SSTables. Correct me if I am wrong but but Cassandra counts data currently being compacted against available disk space. In our case, this is a significant overestimation of the space required by compaction since a large portion of the data being compacted has expired or is an overwrite.

More to the point though, Cassandra should not crash because its out of disk space unless its really actually out of disk space (ie, dont consider 'phantom' compaction disk usage when flushing). I have seen one of our nodes die in this way before our alerts for disk space even went off."
CASSANDRA-5584,Incorrect use of System.nanoTime(),"From System.nanoTime() JavaDoc:
{noformat}
For example, to measure how long some code takes to execute:
 long startTime = System.nanoTime();
 // ... the code being measured ...
 long estimatedTime = System.nanoTime() - startTime; 

To compare two nanoTime values
 long t0 = System.nanoTime();
 ...
 long t1 = System.nanoTime();
one should use t1 - t0 < 0, not t1 < t0, because of the possibility of numerical overflow.
{noformat}
I found one place with such incorrect use that can result in overflow and in incorrect timeout handling. See attached patch."
CASSANDRA-5562,sstablescrub should respect MAX_HEAP_SIZE,"sstablescrub has Xmx hardcoded to 256MB.  This is not enough in my installation and causes an OOM.

Since it's meant to be run offline, the memory usually allocated to the daemon can be safely given to the tool.

Attached is a patch that makes it respect $MAX_HEAP_SIZE (with fallback to 256MB if not defined)"
CASSANDRA-5521,move IndexSummary off heap,IndexSummary can still use a lot of heap for narrow-row sstables.  (It can also contribute to memory fragmentation because of the large arrays it creates.)
CASSANDRA-5508,Expose whether jna is enabled and memory is locked via JMX,"This may not be possible, but it would be very useful.  Currently the only definitive way to determine whether JNA is enabled and that it's able to lock the memory it needs is to look at the startup log.

It would be great if there was a way to store whether it is enabled so that jmx (or nodetool) could easily tell if JNA was enabled and whether it was able to lock the memory."
CASSANDRA-5506,Reduce memory consumption of IndexSummary,"I am evaluating cassandra for a use case with many tiny rows which would result in a node with 1-3TB of storage having billions of rows. Before loading that much data I am hitting GC issues and when looking at the heap dump I noticed that 70+% of the memory was used by IndexSummaries. 

The two major issues seem to be:

1) that the positions are stored as an ArrayList<Long> which results in each position taking 24 bytes (class + flags + 8 byte long). This might make sense when the file is initially written but once it has been serialized it would be a lot more memory efficient to just have an long[] (really a int[] would be fine unless 2GB sstables are allowed).

2) The DecoratedKey for a byte[16] key takes 195 bytes -- this is for the overhead of the ByteBuffer in the key and overhead in the token.

To somewhat ""work around"" the problem I have increased index_sample but will this many rows that didn't really help starts to have diminishing returns. 


NOTE: This heap dump was from linux with a 64bit oracle vm. 
"
CASSANDRA-5498,Possible NPE on EACH_QUORUM writes,"When upgrading from 1.0 to 1.1, we observed that DatacenterSyncWriteResponseHandler.assureSufficientLiveNodes() can throw an NPE if one of the writeEndpoints has a DC that is not listed in the keyspace while one of the nodes is down. We observed this while running in EC2, and using the Ec2Snitch. The exception typically was was brief, but a certain segment of writes (using EACH_QUORUM) failed during that time.

This ticket will address the NPE in DSWRH, while a followup ticket will be created once we get to the bottom of the incorrect DC being reported from Ec2Snitch.
"
CASSANDRA-5488,CassandraStorage throws NullPointerException (NPE) when widerows is set to 'true',"CassandraStorage throws NPE when widerows is set to 'true'. 

2 problems in getNextWide:
1. Creation of tuple without specifying size
2. Calling addKeyToTuple on lastKey instead of key

java.lang.NullPointerException
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:34)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:26)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.addKeyToTuple(CassandraStorage.java:313)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:196)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:224)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
    at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)
2013-04-16 12:28:03,671 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task"
CASSANDRA-5444,TTL does not compact rows ,"I find that the rows are not compacted when gc_grace_seconds is set through alter table command. Following were the steps performed:

1. Created the following table:
CREATE COLUMNFAMILY MirrorSmokeTest (
    id uuid PRIMARY KEY,
    test_message varchar
);

2. Ran an alter command:
alter table MirrorSmokeTest WITH gc_grace_seconds = 60;

3. Insert a row into the above table:
insert into MirrorSmokeTest (id, test_message) Values ('4a7fad82-8298-4d91-85de-8255b7c7e4f5', '2') USING ttl 60;

4. Waited for 60 seconds and ran a query that results in the following output:

 select * from MirrorSmokeTest;
 id
--------------------------------------
 4a7fad82-8298-4d91-85de-8255b7c7e4f5

5. Ran a flush and compact and reran the query that results in the following:

 select * from MirrorSmokeTest;
 id
--------------------------------------
 4a7fad82-8298-4d91-85de-8255b7c7e4f5"
CASSANDRA-5417,Push composites support in the storage engine,"CompositeType happens to be very useful and is now widely used: CQL3 heavily rely on it, and super columns are now using it too internally. Besides, CompositeType has been advised as a replacement of super columns on the thrift side for a while, so it's safe to assume that it's generally used there too.

CompositeType has initially been introduced as just another AbstractType.  Meaning that the storage engine has no nothing whatsoever of composites being, well, composite. This has the following drawbacks:
* Because internally a composite value is handled as just a ByteBuffer, we end up doing a lot of extra work. Typically, each time we compare 2 composite value, we end up ""deserializing"" the components (which, while it doesn't copy data per-se because we just slice the global ByteBuffer, still waste some cpu cycles and allocate a bunch of ByteBuffer objects). And since compare can be called *a lot*, this is likely not negligible.
* This make CQL3 code uglier than necessary. Basically, CQL3 makes extensive use of composites, and since it gets backs ByteBuffer from the internal columns, it always have to check if it's actually a compositeType or not, and then split it and pick the different parts it needs. It's only an API problem, but having things exposed as composites directly would definitively make thinks cleaner. In particular, in most cases, CQL3 don't care whether it has a composite with only one component or a non-really-composite value, but we still always distinguishes both cases.  Lastly, if we do expose composites more directly internally, it's not a lot more work to ""internalize"" better the different parts of the cell name that CQL3 uses (what's the clustering key, what's the actuall CQL3 column name, what's the collection element), making things cleaner. Last but not least, there is currently a bunch of places where methods take a ByteBuffer as argument and it's hard to know whether it expects a cell name or a CQL3 column name. This is pretty error prone.
* It makes it hard (or impossible) to do a number of performance improvements.  Consider CASSANDRA-4175, I'm not really sure how you can do it properly (in memory) if cell names are just ByteBuffer (since CQL3 column names are just one of the component in general). But we also miss oportunities of sharing prefixes. If we were able to share prefixes of composite names in memory we would 1) lower the memory footprint and 2) potentially speed-up comparison (of the prefixes) by checking reference equality first (also, doing prefix sharing on-disk, which is a separate concern btw, might be easier to do if we do prefix sharing in memory).

So I suggest pushing CompositeType support inside the storage engine. What I mean by that concretely would be change the internal {{Column.name}} from ByteBuffer to some CellName type. A CellName would API-wise just be a list of ByteBuffer. But in practice, we'd have a specific CellName implementation for not-really-composite names, and the truly composite implementation will allow some prefix sharing. From an external API however, nothing would change, we would pack the composite as usual before sending it back to the client, but at least internally, comparison won't have to deserialize the components every time, and CQL3 code will be cleaner.
"
CASSANDRA-5342,ancestors are not cleared in SSTableMetadata after compactions are done and old SSTables are removed,"We are using LCS and have total of 38000 SSTables for one CF. During LCS, there could be over a thousand SSTable involved. All those SSTable IDs are stored in ancestors field of SSTableMetatdata for the new table. In our case, it consumes more than 1G of heap memory for those field. Put it in perspective, the ancestors consume 2 - 3 times more memory than bloomfilter (fp = 0.1 by default) in LCS. 
We should remove those ancestors from SSTableMetadata after the compaction is finished and the old SSTable is removed. It  might be a big deal for Sized Compaction since there are small number of SSTable involved. But it consumes a lot of memory for LCS. 
At least, we shouldn't load those ancestors to the memory during startup if the files are removed. 
I would love to contribute and provide patch. Please let me know how to start. "
CASSANDRA-5308,Cassandra with leveled compaction quickly runs out of free space with tons of tmp files,"We were performing a massive upload of data to our cluster and found, that one of the nodes is ruuning out of free space

nodetool ring didn't show any extra usage, but df -h / on the problamatic node shown us 95% of used space, while other nodes were all around 50%

Eventually, we discovered that there's an abnormous peak of tmp files in one of our CF's: 753'906'319'257 bytes of data in 26811 files.

Rebooting the node caused all that files to get deleted and node went back to normal.

One of the symptoms: when the node started to misbehave and collect the temp files, it started to flush the memtables due to lot of heap used.

See attached info

"
CASSANDRA-5273,Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,"On out of memory exception, there is an uncaughtexception handler that is calling System.exit(). However, multiple threads are calling this handler causing a deadlock and the server cannot stop working. See http://www.mail-archive.com/user@cassandra.apache.org/msg27898.html. And see stack trace in attachement."
CASSANDRA-5257,Compaction race allows sstables to be in multiple compactions simultaneously,"Reported by [~cscotta] on Twitter.  Here is a log fragment showing the 2110 sstable pulled into two compactions:

{noformat}
 INFO [CompactionExecutor:41495] 2013-02-14 14:19:26,621 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2110-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-1711-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2068-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-1391-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2115-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2052-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2089-Data.db')]
 INFO [OptionalTasks:1] 2013-02-14 14:20:28,978 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399218463 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:20:28,979 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@347404907(60626496/399218463 serialized/live bytes, 2165232 ops)
 INFO [FlushWriter:1590] 2013-02-14 14:20:28,980 Memtable.java (line 447) Writing Memtable-Metrics@347404907(60626496/399218463 serialized/live bytes, 2165232 ops)
 INFO [FlushWriter:1590] 2013-02-14 14:20:30,606 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2117-Data.db (21066414 bytes) for commitlog position ReplayPosition(segmentId=1360737579813, position=4969777)
 INFO [OptionalTasks:1] 2013-02-14 14:21:51,046 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399221413 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:21:51,046 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@663159049(60626944/399221413 serialized/live bytes, 2165248 ops)
 INFO [FlushWriter:1591] 2013-02-14 14:21:51,047 Memtable.java (line 447) Writing Memtable-Metrics@663159049(60626944/399221413 serialized/live bytes, 2165248 ops)
 INFO [FlushWriter:1591] 2013-02-14 14:21:52,692 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2118-Data.db (21071657 bytes) for commitlog position ReplayPosition(segmentId=1360737579815, position=14067099)
 INFO [OptionalTasks:1] 2013-02-14 14:23:13,059 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:23:13,060 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@480704694(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1592] 2013-02-14 14:23:13,061 Memtable.java (line 447) Writing Memtable-Metrics@480704694(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1592] 2013-02-14 14:23:14,677 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2119-Data.db (21074605 bytes) for commitlog position ReplayPosition(segmentId=1360737579817, position=23128798)
 INFO [OptionalTasks:1] 2013-02-14 14:24:35,073 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:24:35,073 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@2075924408(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1593] 2013-02-14 14:24:35,074 Memtable.java (line 447) Writing Memtable-Metrics@2075924408(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1593] 2013-02-14 14:24:36,683 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2120-Data.db (21044055 bytes) for commitlog position ReplayPosition(segmentId=1360737579819, position=32199135)
 INFO [CompactionExecutor:41571] 2013-02-14 14:24:36,684 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2120-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2119-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2117-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2118-Data.db')]
 INFO [CompactionExecutor:41571] 2013-02-14 14:25:12,234 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2121,].  84,256,731 bytes to 84,334,086 (~100% of original) in 35,549ms = 2.262434MB/s.  14,432 total rows, 14,432 unique.  Row merge counts were {1:14432, 2:0, 3:0, 4:0, }
 INFO [OptionalTasks:1] 2013-02-14 14:25:57,115 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399232107 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:25:57,117 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@258318328(60628568/399232107 serialized/live bytes, 2165306 ops)
 INFO [FlushWriter:1594] 2013-02-14 14:25:57,118 Memtable.java (line 447) Writing Memtable-Metrics@258318328(60628568/399232107 serialized/live bytes, 2165306 ops)
 INFO [FlushWriter:1594] 2013-02-14 14:25:58,705 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2122-Data.db (21059731 bytes) for commitlog position ReplayPosition(segmentId=1360737579822, position=7758494)
 INFO [OptionalTasks:1] 2013-02-14 14:27:19,129 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399218832 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:27:19,130 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@2026189692(60626552/399218832 serialized/live bytes, 2165234 ops)
 INFO [FlushWriter:1595] 2013-02-14 14:27:19,131 Memtable.java (line 447) Writing Memtable-Metrics@2026189692(60626552/399218832 serialized/live bytes, 2165234 ops)
 INFO [FlushWriter:1595] 2013-02-14 14:27:20,726 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2123-Data.db (21081156 bytes) for commitlog position ReplayPosition(segmentId=1360737579824, position=16830476)
 INFO [OptionalTasks:1] 2013-02-14 14:28:41,143 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:28:41,143 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@835031438(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1596] 2013-02-14 14:28:41,144 Memtable.java (line 447) Writing Memtable-Metrics@835031438(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1596] 2013-02-14 14:28:42,775 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2124-Data.db (21050393 bytes) for commitlog position ReplayPosition(segmentId=1360737579826, position=25891158)
 INFO [OptionalTasks:1] 2013-02-14 14:30:03,156 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:30:03,157 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@1604541794(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1597] 2013-02-14 14:30:03,158 Memtable.java (line 447) Writing Memtable-Metrics@1604541794(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1597] 2013-02-14 14:30:04,869 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2125-Data.db (21050877 bytes) for commitlog position ReplayPosition(segmentId=1360737579829, position=1405790)
 INFO [CompactionExecutor:41657] 2013-02-14 14:30:04,870 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2123-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2122-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2125-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2124-Data.db')]
 INFO [CompactionExecutor:41657] 2013-02-14 14:30:40,416 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2126,].  84,242,157 bytes to 84,305,839 (~100% of original) in 35,545ms = 2.261930MB/s.  14,432 total rows, 14,432 unique.  Row merge counts were {1:14432, 2:0, 3:0, 4:0, }
 INFO [OptionalTasks:1] 2013-02-14 14:31:25,169 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399240957 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:31:25,171 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@960476318(60629912/399240957 serialized/live bytes, 2165354 ops)
 INFO [FlushWriter:1598] 2013-02-14 14:31:25,172 Memtable.java (line 447) Writing Memtable-Metrics@960476318(60629912/399240957 serialized/live bytes, 2165354 ops)
 INFO [FlushWriter:1598] 2013-02-14 14:31:26,783 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2127-Data.db (21084189 bytes) for commitlog position ReplayPosition(segmentId=1360737579831, position=10480922)
 INFO [OptionalTasks:1] 2013-02-14 14:32:47,183 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399238376 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:32:47,184 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@852152579(60629520/399238376 serialized/live bytes, 2165340 ops)
 INFO [FlushWriter:1599] 2013-02-14 14:32:47,185 Memtable.java (line 447) Writing Memtable-Metrics@852152579(60629520/399238376 serialized/live bytes, 2165340 ops)
 INFO [FlushWriter:1599] 2013-02-14 14:32:48,794 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2128-Data.db (21052579 bytes) for commitlog position ReplayPosition(segmentId=1360737579833, position=19561582)
 INFO [OptionalTasks:1] 2013-02-14 14:34:09,197 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399229710 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:34:09,197 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@600711801(60628204/399229710 serialized/live bytes, 2165293 ops)
 INFO [FlushWriter:1600] 2013-02-14 14:34:09,198 Memtable.java (line 447) Writing Memtable-Metrics@600711801(60628204/399229710 serialized/live bytes, 2165293 ops)
 INFO [FlushWriter:1600] 2013-02-14 14:34:10,818 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2129-Data.db (21067262 bytes) for commitlog position ReplayPosition(segmentId=1360737579835, position=28636059)
 INFO [OptionalTasks:1] 2013-02-14 14:35:31,238 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399221598 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:35:31,239 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@2104370804(60626972/399221598 serialized/live bytes, 2165249 ops)
 INFO [FlushWriter:1601] 2013-02-14 14:35:31,240 Memtable.java (line 447) Writing Memtable-Metrics@2104370804(60626972/399221598 serialized/live bytes, 2165249 ops)
 INFO [FlushWriter:1601] 2013-02-14 14:35:32,844 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2130-Data.db (21078893 bytes) for commitlog position ReplayPosition(segmentId=1360737579838, position=4177813)
 INFO [CompactionExecutor:41742] 2013-02-14 14:35:32,845 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2128-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2129-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2130-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2127-Data.db')]
 INFO [CompactionExecutor:41742] 2013-02-14 14:36:08,384 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2131,].  84,282,923 bytes to 84,299,440 (~100% of original) in 35,538ms = 2.262204MB/s.  14,432 total rows, 14,432 unique.  Row merge counts were {1:14432, 2:0, 3:0, 4:0, }
 INFO [CompactionExecutor:41767] 2013-02-14 14:36:08,385 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2126-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2131-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2115-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2121-Data.db')]
 INFO [OptionalTasks:1] 2013-02-14 14:36:53,252 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399229895 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:36:53,253 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@562526332(60628232/399229895 serialized/live bytes, 2165294 ops)
 INFO [FlushWriter:1602] 2013-02-14 14:36:53,254 Memtable.java (line 447) Writing Memtable-Metrics@562526332(60628232/399229895 serialized/live bytes, 2165294 ops)
 INFO [FlushWriter:1602] 2013-02-14 14:36:55,065 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2133-Data.db (21065439 bytes) for commitlog position ReplayPosition(segmentId=1360737579840, position=13247694)
 INFO [OptionalTasks:1] 2013-02-14 14:38:15,266 MeteredFlusher.java (line 58) flushing high-traffic column family CFS(Keyspace='jetpack', ColumnFamily='Metrics') (estimated 399214407 bytes)
 INFO [OptionalTasks:1] 2013-02-14 14:38:15,267 ColumnFamilyStore.java (line 640) Enqueuing flush of Memtable-Metrics@1048911247(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1603] 2013-02-14 14:38:15,268 Memtable.java (line 447) Writing Memtable-Metrics@1048911247(60625880/399214407 serialized/live bytes, 2165210 ops)
 INFO [FlushWriter:1603] 2013-02-14 14:38:16,899 Memtable.java (line 481) Completed flushing /home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2134-Data.db (21065175 bytes) for commitlog position ReplayPosition(segmentId=1360737579842, position=22318066)
 INFO [CompactionExecutor:41767] 2013-02-14 14:38:32,197 CompactionTask.java (line 273) Compacted 4 sstables to [/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2132,].  337,297,607 bytes to 337,468,452 (~100% of original) in 143,810ms = 2.237918MB/s.  57,728 total rows, 57,728 unique.  Row merge counts were {1:57728, 2:0, 3:0, 4:0, }
 INFO [CompactionExecutor:41811] 2013-02-14 14:38:32,197 CompactionTask.java (line 118) Compacting [SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2110-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2132-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2068-Data.db'), SSTableReader(path='/home/cscotta/jetpack/cassandra/data/jetpack/Metrics/jetpack-Metrics-ib-2089-Data.db')]
ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}"
CASSANDRA-5256,"""Memory was freed"" AssertionError During Major Compaction","When initiating a major compaction with `./nodetool -h localhost compact`, an AssertionError is thrown in the CompactionExecutor from o.a.c.io.util.Memory:

ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

---

I've invoked the `nodetool compact` three times; this occurred after each. The node has been up for a couple days accepting writes and has not been restarted.

Here's the server's log since it was started a few days ago: https://gist.github.com/cscotta/4956472/raw/95e7cbc68de1aefaeca11812cbb98d5d46f534e8/cassandra.log

Here's the code being used to issue writes to the datastore: https://gist.github.com/cscotta/20cbd36c2503c71d06e9

---

Configuration: One node, one keyspace, one column family. ~60 writes/second of data with a TTL of 86400, zero reads. Stock cassandra.yaml.

Keyspace DDL:

create keyspace jetpack;
use jetpack;
create column family Metrics with key_validation_class = 'UTF8Type' and comparator = 'IntegerType';"
CASSANDRA-5244,Compactions don't work while node is bootstrapping,"It seems that there is a race condition in StorageService that prevents compactions from completing while node is in a bootstrap state.

I have been able to reproduce this multiple times by throttling streaming throughput to extend the bootstrap time while simultaneously inserting data to the cluster.

The problems lies in the synchronization of initServer(int delay) and reportSeverity(double incr) methods as they both try to acquire the instance lock of StorageService through the use of synchronized keyword. As initServer does not return until the bootstrap has completed, all calls to reportSeverity will block until that. However, reportSeverity is called when starting compactions in CompactionInfo and thus all compactions block until bootstrap completes. 

This might severely degrade node's performance after bootstrap as it might have lots of compactions pending while simultaneously starting to serve reads.

I have been able to solve the issue by adding a separate lock for reportSeverity and removing its class level synchronization. This of course is not a valid approach if we must assume that any of Gossiper's IEndpointStateChangeSubscribers could potentially end up calling back to StorageService's synchronized methods. However, at least at the moment, that does not seem to be the case.

Maybe somebody with more experience about the codebase comes up with a better solution?

(This might affect DynamicEndpointSnitch as well, as it also calls to reportSeverity in its setSeverity method)"
CASSANDRA-5241,Fix forceBlockingFlush,"ForceBlockingFlush doesn't guarantee that after the call, every that the thread has written prior to the call will be fully flushed. At least not in the case of concurrent flushes, because if 2 threads flush roughly at the same time, one will have it's forceBlockingFlush call return immediately because the memtable will be clean (even though some of the thread writes may have not be fully flushed yet).

I think this is very fragile and make it easy to have hard to find races and so we should fix it. Typically a forceFlush that see a clean memtable could submit a dummy task in the postFlushExecutor and wait for that."
CASSANDRA-5214,AE in DataTracker.markCompacting,"On 1.2 branch:

{noformat}
 INFO [main] 2013-02-01 05:50:07,709 CassandraDaemon.java (line 103) Logging initialized
 INFO [main] 2013-02-01 05:50:07,730 CassandraDaemon.java (line 125) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO [main] 2013-02-01 05:50:07,731 CassandraDaemon.java (line 126) Heap size: 1046937600/1046937600
 INFO [main] 2013-02-01 05:50:07,731 CassandraDaemon.java (line 127) Classpath: /tmp/dtest-4ju3_j/test/node1/conf:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/build/classes/main:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/build/classes/thrift:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/antlr-3.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/avro-1.4.0-fixes.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/avro-1.4.0-sources-fixes.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-cli-1.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-codec-1.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-lang-2.6.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/compress-lzf-0.8.4.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/concurrentlinkedhashmap-lru-1.3.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/guava-13.0.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/high-scale-lib-1.1.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jackson-core-asl-1.9.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jackson-mapper-asl-1.9.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jamm-0.2.5.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jline-1.0.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/json-simple-1.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/libthrift-0.7.0.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/log4j-1.2.16.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/metrics-core-2.0.3.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/netty-3.5.9.Final.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/servlet-api-2.5-20081211.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/slf4j-api-1.7.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/slf4j-log4j12-1.7.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snakeyaml-1.6.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snappy-java-1.0.4.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snaptree-0.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jamm-0.2.5.jar
 INFO [main] 2013-02-01 05:50:07,733 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2013-02-01 05:50:07,748 DatabaseDescriptor.java (line 131) Loading settings from file:/tmp/dtest-4ju3_j/test/node1/conf/cassandra.yaml
 INFO [main] 2013-02-01 05:50:08,168 DatabaseDescriptor.java (line 190) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2013-02-01 05:50:08,168 DatabaseDescriptor.java (line 204) disk_failure_policy is stop
 INFO [main] 2013-02-01 05:50:08,174 DatabaseDescriptor.java (line 265) Global memtable threshold is enabled at 332MB
 INFO [main] 2013-02-01 05:50:08,911 CacheService.java (line 111) Initializing key cache with capacity of 49 MBs.
 INFO [main] 2013-02-01 05:50:08,923 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-02-01 05:50:08,924 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-02-01 05:50:08,931 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [main] 2013-02-01 05:50:09,438 DatabaseDescriptor.java (line 542) Couldn't detect any schema definitions in local storage.
 INFO [main] 2013-02-01 05:50:09,440 DatabaseDescriptor.java (line 545) Found table data in data directories. Consider using the CLI to define your schema.
 INFO [CompactionExecutor:1] 2013-02-01 05:50:09,579 ColumnFamilyStore.java (line 678) Enqueuing flush of Memtable-local@524805736(133/133 serialized/live bytes, 6 ops)
 INFO [FlushWriter:1] 2013-02-01 05:50:09,592 Memtable.java (line 447) Writing Memtable-local@524805736(133/133 serialized/live bytes, 6 ops)
 INFO [FlushWriter:1] 2013-02-01 05:50:09,670 Memtable.java (line 481) Completed flushing /tmp/dtest-4ju3_j/test/node1/data/system/local/system-local-ib-1-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1359719409398, position=425)
ERROR [CompactionExecutor:2] 2013-02-01 05:50:09,681 CassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError
    at org.apache.cassandra.db.DataTracker.markCompacting(DataTracker.java:183)
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:128)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:185)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}"
CASSANDRA-5207,Validate login for USE queries,"CASSANDRA-5144 added login validation to Thrift set_keyspace method. Same should be done for CQL2 and CQL3 USE queries, otherwise C* will leak keyspace existence to strangers even when the configured authenticator requires login."
CASSANDRA-5202,"CFs should have globally and temporally unique CF IDs to prevent ""reusing"" data from earlier incarnation of same CF name","Attached is a driver that sequentially:

1. Drops keyspace
2. Creates keyspace
4. Creates 2 column families
5. Seeds 1M rows with 100 columns
6. Queries these 2 column families

The above steps are repeated 1000 times.

The following exception is observed at random (race - SEDA?):

ERROR [ReadStage:55] 2013-01-29 19:24:52,676 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:55,5,main]
java.lang.AssertionError: DecoratedKey(-1, ) != DecoratedKey(62819832764241410631599989027761269388, 313a31) in C:\var\lib\cassandra\data\user_role_reverse_index\business_entity_role\user_role_reverse_index-business_entity_role-hf-1-Data.db
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:60)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:67)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:79)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:256)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:64)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1367)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1229)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1164)
	at org.apache.cassandra.db.Table.getRow(Table.java:378)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:69)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:822)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1271)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


This exception appears in the server at the time of client submitting a query request (row slice) and not at the time data is seeded. The client times out and this data can no longer be queried as the same exception would always occur from there on.

Also on iteration 201, it appears that dropping column families failed and as a result their recreation failed with unique column family name violation (see exception below). Note that the data files are actually gone, so it appears that the server runtime responsible for creating column family was out of sync with the piece that dropped them:

Starting dropping column families
Dropped column families
Starting dropping keyspace
Dropped keyspace
Starting creating column families
Created column families
Starting seeding data
Total rows inserted: 1000000 in 5105 ms
Iteration: 200; Total running time for 1000 queries is 232; Average running time of 1000 queries is 0 ms
Starting dropping column families
Dropped column families
Starting dropping keyspace
Dropped keyspace
Starting creating column families
Created column families
Starting seeding data
Total rows inserted: 1000000 in 5361 ms
Iteration: 201; Total running time for 1000 queries is 222; Average running time of 1000 queries is 0 ms
Starting dropping column families
Starting creating column families
Exception in thread ""main"" com.netflix.astyanax.connectionpool.exceptions.BadRequestException: BadRequestException: [host=127.0.0.1(127.0.0.1):9160, latency=2468(2469), attempts=1]InvalidRequestException(why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""))
	at com.netflix.astyanax.thrift.ThriftConverter.ToConnectionPoolException(ThriftConverter.java:159)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:60)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:27)
	at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl$1.execute(ThriftSyncConnectionFactoryImpl.java:140)
	at com.netflix.astyanax.connectionpool.impl.AbstractExecuteWithFailoverImpl.tryOperation(AbstractExecuteWithFailoverImpl.java:69)
	at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:255)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl.createKeyspace(ThriftKeyspaceImpl.java:569)
	at com.nuance.mca.astyanax.App.recreateKeyspaceSchema(App.java:139)
	at com.nuance.mca.astyanax.App.main(App.java:88)
Caused by: InvalidRequestException(why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""))
	at org.apache.cassandra.thrift.Cassandra$system_add_keyspace_result.read(Cassandra.java:30010)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_add_keyspace(Cassandra.java:1285)
	at org.apache.cassandra.thrift.Cassandra$Client.system_add_keyspace(Cassandra.java:1272)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl$14.internalExecute(ThriftKeyspaceImpl.java:584)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl$14.internalExecute(ThriftKeyspaceImpl.java:572)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:55)
	... 7 more



"
CASSANDRA-5195,Offline scrub does not migrate the directory structure on migration from 1.0.x to 1.1.x and causes the keyspace to disappear,"Due to CASSANDRA-4411, upon migration from 1.0.x to 1.1.x containing LCS-compacted sstables, an offline scrub should be run before Cassandra 1.1.x is started. But Cassandra 1.1.x uses a new directory structure (CASSANDRA-2749) that offline scrubber doesn't detect or try to migrate.

How to reproduce:

1- Run cassandra 1.0.12.
2- Run stress tool, let Cassandra flush Keyspace1 or flush manually.
3- Stop cassandra 1.0.12
4- Run ./bin/sstablescrub Keyspace1 Standard1
  which returns ""Unknown keyspace/columnFamily Keyspace1.Standard1"" and notice the data directory isn't migrated.
5- Run cassandra 1.1.9. Keyspace1 doesn't get loaded and Cassandra doesn't try to migrate the directory structure. Also commitlog entries get skipped: ""Skipped XXXXX mutations from unknown (probably removed) CF with id 1000""

Without the unsuccessful step 4, Cassandra 1.1.9 loads and migrates the Keyspace correctly.

  "
CASSANDRA-5185,symlinks to data directories are broken in 1.2.0,"symlinks to data directories is broken in 1.2.0
{noformat}
cd ~
tar xzf apache-cassandra-1.2.0-bin.tar.gz
cd apache-cassandra-1.2.0/conf
vim cassandra.yaml
#set data/commitlog/savecache dirs to
#~/apache-cassandra-1.2.0/var/...
cd ../bin
# start once to make folders
./cassandra -f
#cntrl-c
cd ..
mkdir var/lib/cassandra2
mv var/lib/cassandra/data/system var/lib/cassandra2/system
cd var/lib/cassandra/data
ln -s ../../cassandra2/system .
cd ~/apache-cassandra-1.2.0/bin/
cassandra -f
#get lots of assertion errors see attached log file
{noformat}

{noformat}
 INFO 21:59:44,883 Enqueuing flush of Memtable-local@1578022692(52/52 serialized/live bytes, 2 ops)
 INFO 21:59:44,890 Writing Memtable-local@1578022692(52/52 serialized/live bytes, 2 ops)
ERROR 21:59:44,892 Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.io.sstable.Descriptor.<init>(Descriptor.java:190)
	at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:593)
	at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:588)
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:428)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:417)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

I traced it back some, I think it is coming from:
{noformat}
org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:428)
which calls: cfs.directories.getLocationForDisk(dataDirectory)

    public File getLocationForDisk(File dataDirectory)
    {
        for (File dir : sstableDirectories)
        {
            if (FileUtils.getCanonicalPath(dir).startsWith(FileUtils.getCanonicalPath(dataDirectory)))
                return dir;
        }
        return null;
    }
{noformat}

My guess is that the FileUtils.getCanonicalPath calls aren't matching because of the symlinks.  So null is being returned there."
CASSANDRA-5157,mac friendly cassandra-env.sh,"by default my mac launches with 1024mb but this fix will allow it to start up at 25% total memory
{code}
Darwin)
    system_memory_in_bytes=`sysctl hw.memsize | awk '{print $2}'`
    system_memory_in_mb=`expr $system_memory_in_bytes / 1024 / 1024`
    system_cpu_cores=`sysctl hw.ncpu | awk '{print $2}'`
;;
{code}"
CASSANDRA-5146,repair -pr hangs,"while running a repair -pr the repair seems to hang after getting a merkle tree

{code}
 INFO [AntiEntropySessions:9] 2013-01-10 18:23:01,652 AntiEntropyService.java (line 652) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] new session: will sync /10.8.25.101, /10.8.30.14 on range (28356863910078205288614550619314017620,42535295865117307932921825928971026436] for evidence.[fingerprints, messages]
 INFO [AntiEntropySessions:9] 2013-01-10 18:23:01,653 AntiEntropyService.java (line 857) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] requesting merkle trees for fingerprints (to [/10.8.30.14, /10.8.25.101])
 INFO [ValidationExecutor:7] 2013-01-10 18:23:01,654 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-fingerprints@500862962(12960712/12960712 serialized/live bytes, 469 ops)
 INFO [FlushWriter:25] 2013-01-10 18:23:01,655 Memtable.java (line 424) Writing Memtable-fingerprints@500862962(12960712/12960712 serialized/live bytes, 469 ops)
 INFO [FlushWriter:25] 2013-01-10 18:23:02,058 Memtable.java (line 458) Completed flushing /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-192-Data.db (11413718 bytes) for commitlog position ReplayPosition(segmentId=1357767160463, position=8921654)
 INFO [AntiEntropyStage:1] 2013-01-10 18:25:52,735 AntiEntropyService.java (line 214) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] Received merkle tree for fingerprints from /10.8.25.101
{code}"
CASSANDRA-5062,Support CAS,"""Strong"" consistency is not enough to prevent race conditions.  The classic example is user account creation: we want to ensure usernames are unique, so we only want to signal account creation success if nobody else has created the account yet.  But naive read-then-write allows clients to race and both think they have a green light to create.
"
CASSANDRA-5020,Time to switch back to byte[] internally?,"We switched to ByteBuffer for column names and values back in 0.7, which gave us a short term performance boost on mmap'd reads, but we gave that up when we switched to refcounted sstables in 1.0.  (refcounting all the way up the read path would be too painful, so we copy into an on-heap buffer when reading from an sstable, then release the reference.)

A HeapByteBuffer wastes a lot of memory compared to a byte[] (5 more ints, a long, and a boolean).

The hard problem here is how to do the arena allocation we do on writes, which has been very successful in reducing STW CMS from heap fragmentation.  ByteBuffer is a good fit there."
CASSANDRA-5014,Convert timeout from milli second to micro second.,"Convert all the timeouts to microseconds.

Jonathan's comment from CASSANDRA-4705

{quote}
millis may be too coarse a grain here, especially for Custom settings. Currently an in-memory read will typically be under 2ms and it's quite possible we can get that down to 1 if we can purge some of the latency between stages. Might as well use micros since Timer gives it to us for free
{quote}"
CASSANDRA-4934,assertion error in offheap bloom filter,"Saw this while running the dtests:

{noformat}
 INFO [CompactionExecutor:2] 2012-11-08 09:35:18,206 CompactionTask.java (line 116) Compacting [SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-9-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-11-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-10-Data.db'), SSTableReader(path='/tmp/dtest-n2D_fM/test/node2/data/system/local/system-local-ia-13-Data.db')]
ERROR [CompactionExecutor:2] 2012-11-08 09:35:18,257 CassandraDaemon.java (line 132) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError: Memory was freed
    at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:134)
    at org.apache.cassandra.io.util.Memory.getByte(Memory.java:104)
    at org.apache.cassandra.utils.obs.OffHeapBitSet.get(OffHeapBitSet.java:60)
    at org.apache.cassandra.utils.BloomFilter.isPresent(BloomFilter.java:71)
    at org.apache.cassandra.db.compaction.CompactionController.shouldPurge(CompactionController.java:106)
    at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:64)
    at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:95)
    at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:151)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:72)
    at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
    at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at com.google.common.collect.Iterators$8.computeNext(Iterators.java:734)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:146)
    at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:69)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:179)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}"
CASSANDRA-4876,Make bloom filters optional by default for LCS,"Since LCS gives us only 10% chance we will need to merge data from multiple sstables, bloom filters are a waste of memory (unless you have a workload where you request lots of keys that don't exist)."
CASSANDRA-4846,BulkLoader throws NPE at start up,"BulkLoader in trunk throws below exception at start up and exit abnormally.

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:87)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:180)
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:148)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:96)
	at java.io.File.list(File.java:1010)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:117)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:63)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.CacheService.initRowCache(CacheService.java:154)
	at org.apache.cassandra.service.CacheService.<init>(CacheService.java:102)
	at org.apache.cassandra.service.CacheService.<clinit>(CacheService.java:83)
	... 8 more
{code}

This comes from CASSANDRA-4732, which moved keyCache in SSTableReader initialization at instance creation. This causes access to CacheService that did not happen for v1.1 and ends up NPE because BulkLoader does not load cassandra.yaml."
CASSANDRA-4820,Add a close() method to CRAR to prevent leaking file descriptors.,"The problem is that under heavy load Finalizer daemon is unable to keep up with number of ""source"" and ""channel"" fields from CRAR to finalize (as FileInputStream has custom finalize() which calls close inside) which creates memory/cpu pressure on the machine."
CASSANDRA-4802,"Regular startup log has confusing ""Bootstrap/Replace/Move completed!"" without boostrap, replace, or move","A regular startup completes successfully, but it has a confusing message the end of the startup:

""  INFO 15:19:29,137 Bootstrap/Replace/Move completed! Now serving reads.""

This happens despite no bootstrap, replace, or move.

While purely cosmetic, this makes you wonder what the node just did - did it just bootstrap?!  It should simply read something like ""Startup completed! Now serving reads"" unless it actually has done one of the actions in the error message.



Complete log at the end:


INFO 15:13:30,522 Log replay complete, 6274 replayed mutations
 INFO 15:13:30,527 Cassandra version: 1.0.12
 INFO 15:13:30,527 Thrift API version: 19.20.0
 INFO 15:13:30,527 Loading persisted ring state
 INFO 15:13:30,541 Starting up server gossip
 INFO 15:13:30,542 Enqueuing flush of Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,543 Writing Memtable-LocationInfo@1828864224(29/36 serialized/live bytes, 1 ops)
 INFO 15:13:30,550 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-274-Data.db (80 bytes)
 INFO 15:13:30,563 Starting Messaging Service on port 7000
 INFO 15:13:30,571 Using saved token 31901471898837980949691369446728269823
 INFO 15:13:30,572 Enqueuing flush of Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,573 Writing Memtable-LocationInfo@294410307(53/66 serialized/live bytes, 2 ops)
 INFO 15:13:30,579 Completed flushing /data2/data-cassandra/system/LocationInfo-hd-275-Data.db (163 bytes)
 INFO 15:13:30,581 Node kaos-cass02.xxxxxxx/1.2.3.4 state jump to normal
 INFO 15:13:30,598 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 15:13:30,600 Will not load MX4J, mx4j-tools.jar is not in the classpath
"
CASSANDRA-4708,StorageProxy slow-down and memory leak,"I am consistently observing slow-downs in StorageProxy caused by the NonBlockingHashMap used indirectly by MessagingService via the callbacks ExpiringMap.

This seems do be due to NBHM having unbounded memory usage in the face of workloads with high key churn. As monotonically increasing integers are used as callback id's by MessagingService, the backing NBHM eventually ends up growing the backing store unboundedly. This causes it to also do very large and expensive backing store reallocation and migrations, causing throughput to drop to tens of operations per second, lasting seconds or even minutes. 

This behavior is especially noticable for high throughput workloads where the dataset is completely in ram and I'm doing up to a hundred thousand reads per second.

Replacing NBHM in ExpiringMap with the java standard library ConcurrentHashMap resolved the issue and allowed me to keep a consistent high throughput.

An open issue on NBHM can be seen here: http://sourceforge.net/tracker/?func=detail&aid=3563980&group_id=194172&atid=948362"
CASSANDRA-4694,populate_io_cache_on_flush option should be configurable for each column family independently,"I suggest to configure populate_io_cache_on_flush option for each column family. It should be configurable from cassandra-cli and should be stored in System keyspace. 

That could be useful if you have a few column families inside single keyspace and you need to fit in memory only one of them.

Patch has been attached. I've been testing it on pseudo-cluster using ccm. So I don't have fully confidence about lack of bugs. Please carefully review that code."
CASSANDRA-4675,NPE in NTS when using LQ against a node (DC) that doesn't have replica,"in a NetworkTopologyStrategy where there are 2 DC:

{panel}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
127.0.0.1       dc1         r1          Up     Normal  115.78 KB       50.00%  0                                           
127.0.0.2       dc2         r1          Up     Normal  129.3 KB        50.00%  85070591730234615865843651857942052864  
{panel}
I have a KS that has replica is 1 of the dc (dc1):

{panel}
[default@unknown] describe Keyspace3;                                                                                                                     
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

But if I connect to a node in dc2, using LOCAL_QUORUM, I get NPE in the Cassandra node's log:

{panel}
[default@unknown] consistencylevel as LOCAL_QUORUM;                       
Consistency level is set to 'LOCAL_QUORUM'.
[default@unknown] use Keyspace3;                                          
Authenticated to keyspace: Keyspace3
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                     
Internal error processing get
org.apache.thrift.TApplicationException: Internal error processing get
        at org.apache.thrift.TApplicationException.read(TApplicationException.java:108)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:511)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}

node2's log:
{panel}
ERROR [Thrift:3] 2012-09-17 18:15:16,868 Cassandra.java (line 2999) Internal error processing get
java.lang.NullPointerException
        at org.apache.cassandra.locator.NetworkTopologyStrategy.getReplicationFactor(NetworkTopologyStrategy.java:142)
        at org.apache.cassandra.service.DatacenterReadCallback.determineBlockFor(DatacenterReadCallback.java:90)
        at org.apache.cassandra.service.ReadCallback.<init>(ReadCallback.java:67)
        at org.apache.cassandra.service.DatacenterReadCallback.<init>(DatacenterReadCallback.java:63)
        at org.apache.cassandra.service.StorageProxy.getReadCallback(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:609)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:564)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.internal_get(CassandraServer.java:383)
        at org.apache.cassandra.thrift.CassandraServer.get(CassandraServer.java:401)
        at org.apache.cassandra.thrift.Cassandra$Processor$get.process(Cassandra.java:2989)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
{panel}

I could workaround it by adding dc2:0 to the option:

{panel}
[default@Keyspace3] describe Keyspace3;                                           
Keyspace: Keyspace3:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [dc2:0, dc1:1]
  Column Families:
    ColumnFamily: testcf
{panel}

Now you get UA:

{panel}
[default@Keyspace3] get testcf[utf8('k1')][utf8('c1')];                           
null
UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6506)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:519)
        at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:492)
        at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:648)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:348)
{panel}


On a side note, is there a thought on having a CL.LOCAL_ONE? Ie if local node (wrt the dc) does not have replica, on a LOCAL_ONE, it won't try to go across DC to try to get it. It would be similar to LOCAL_QUORUM."
CASSANDRA-4602,Stack Size on Sun JVM 1.6.0_33 must be at least 160k,"I started a fresh Cassandra 1.1.4 install with Sun JVM 1.6.35.

On startup I got this in output.log

{noformat}
The stack size specified is too small, Specify at least 160k
Cannot create Java VM
Service exit with a return value of 1
{noformat}

Remembering CASSANDRA-4275 I monkeyed around and started the JVM with -Xss160k the same as Java 7. I then got

{code:java}
ERROR [WRITE-/192.168.1.12] 2012-08-31 01:43:29,865 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[WRITE-/192.168.1.12,5,main]
java.lang.StackOverflowError
	at java.net.SocketOutputStream.socketWrite0(Native Method)
	at java.net.SocketOutputStream.socketWrite(Unknown Source)
	at java.net.SocketOutputStream.write(Unknown Source)
	at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
	at java.io.BufferedOutputStream.flush(Unknown Source)
	at java.io.DataOutputStream.flush(Unknown Source)
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:156)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:126)
{code}

Same as CASSANDRA-4442

At which point I dropped back to Java 6.33. 

CASSANDRA-4457 bumped the stack size to 180 for java 7, should we also do this for Java 6.33+ ?"
CASSANDRA-4597,Impossible to set LeveledCompactionStrategy to a column family.,"CFPropDefs.applyToCFMetadata() does not set the compaction class on CFM

When altering the compaction strategy of a column family to LeveledCompactionStrategy, the compaction strategy is not changed (the describe command shows that the SizeTieredCompactionStrategy is still set to the CF)
When creating a column family WITH compaction_strategy_class='LeveledCompactionStrategy', the compaction strategy class used is  SizeTieredCompactionStrategy

Ex : 
jal@jal-VirtualBox:~/cassandra/apache-cassandra-1.1.1/bin$ ./cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 2.2.0 | Cassandra 1.1.1 | CQL spec 3.0.0 | Thrift protocol 19.32.0]
Use HELP for help.
cqlsh> use test1;
cqlsh:test1> describe table pns_credentials;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

I want to set the LeveledCompaction strategy for this table, so I execute the following ALTER TABLE :

cqlsh:test1> alter table pns_credentials 
         ... WITH compaction_strategy_class='LeveledCompactionStrategy'
         ... AND compaction_strategy_options:sstable_size_in_mb=10;

In Cassandra logs, I see some informations :
 INFO 10:23:52,532 Enqueuing flush of Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,533 Writing Memtable-schema_columnfamilies@965212657(1391/1738 serialized/live bytes, 20 ops)
 INFO 10:23:52,629 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-94-Data.db (1442 bytes) for commitlog position ReplayPosition(segmentId=3556583843054, position=1987)


However, when I look at the description of the table, the table is still with the SizeTieredCompactionStrategy
cqlsh:test1> describe table pns_credentials ;

CREATE TABLE pns_credentials (
  ise text PRIMARY KEY,
  isnew int,
  ts timestamp,
  mergestatus int,
  infranetaccount text,
  user_level int,
  msisdn bigint,
  mergeusertype int
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';
 
In the schema_columnfamilies table (in system keyspace), the table pns_credentials is still using the SizeTieredCompactionStrategy
cqlsh:test1> use system;
cqlsh:system> select * from schema_columnfamilies ;
...
         test1 |   pns_credentials |                   null | KEYS_ONLY |                        [] |         | org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy |                          {} |                                                                                                                                                                           org.apache.cassandra.db.marshal.UTF8Type | {""sstable_compression"":""org.apache.cassandra.io.compress.SnappyCompressor""} |          org.apache.cassandra.db.marshal.UTF8Type |           864000 | 1029 |       ise |     org.apache.cassandra.db.marshal.UTF8Type |                        0 |                       32 |                        4 |                0.1 |               True |          null | Standard |        null
... 


Same behaviour using cqlsh or command-cli.
"
CASSANDRA-4590,"""The system cannot find the path specified"" when creating hard link on Windows","When upgrading from Cassandra 1.0.5 to 1.1.3, we have a test case (uses embedded Cassandra) that started failing as shown below. Other than the upgrade, no changes were made to the code or config. I believe this MAY be related to the change made in CASSANDRA-3101.

We verified that the file it is trying to create the hard link to does exist - so it is purely the creation of the link that is failing.

Here is the basic failure:

# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.


Here is a more complete log output:


# [11:30:59.975] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.976] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Digest.sha1]
# [11:30:59.977] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Index.db]
# [11:30:59.978] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Filter.db]
# [11:30:59.978] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.979] [DEBUG] [o.a.c.i.u.FileUtils] [deleteWithConfirm] [Deleting system-schema_columnfamilies-he-4-Statistics.db]
# [11:30:59.979] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.d.CollationController] [collectAllData] [collectAllData]
# [11:30:59.980] [DEBUG] [o.a.c.i.s.SSTable] [delete] [Deleted target\test\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-he-4]
# [11:30:59.981] [INFO ] [o.a.c.d.ColumnFamilyStore] [maybeSwitchMemtable] [Enqueuing flush of Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.981] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Writing Memtable-PropertyProductDefaultInventoryCounts@2002512083(74/92 serialized/live bytes, 1 ops)]
# [11:30:59.992] [DEBUG] [o.a.c.d.Directories] [getLocationWithMaximumAvailableSpace] [expected data files size is 134; largest free partition (target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts) has 82645161984 bytes free]
# [11:31:00.012] [INFO ] [o.a.c.d.Memtable] [writeSortedContents] [Completed flushing target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db (123 bytes) for commitlog position ReplayPosition(segmentId=592725621297887, position=6701)]
# [11:31:00.013] [DEBUG] [o.a.c.u.I.IntervalNode] [<init>] [Creating IntervalNode from [Interval(DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334), DecoratedKey(70791399548943621833439300945136455431, 50726f706572747950726f6475637431323334))]]
# [11:31:00.013] [DEBUG] [o.a.c.d.DataTracker] [addNewSSTablesSize] [adding target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1 to list of files tracked for RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [submitBackground] [Scheduling a background task check for RevKeyspace.PropertyProductDefaultInventoryCounts with SizeTieredCompactionStrategy]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [Checking RevKeyspace.PropertyProductDefaultInventoryCounts]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [discard completed log segments for ReplayPosition(segmentId=592725621297887, position=6701), column family 1001]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.SizeTieredCompactionStrategy] [getNextBackgroundTask] [Compaction buckets are [[SSTableReader(path='target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-Data.db')]]]
# [11:31:00.014] [DEBUG] [o.a.c.d.c.CommitLog] [call] [Not safe to delete commit log CommitLogSegment(target\test\cassandra\commitlog\CommitLog-592725621297887.log); dirty is Versions (7), ; hasNext: false]
# [11:31:00.015] [DEBUG] [o.a.c.d.c.CompactionManager] [runMayThrow] [No tasks available]
# [11:31:00.307] [ERROR] [o.a.c.u.CLibrary] [createHardLinkWithExec] [Unable to create hard link]
java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) [cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) [cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.308] [ERROR] [o.a.c.s.AbstractCassandraDaemon] [uncaughtException] [Exception in thread Thread[MigrationStage:1,5,main]]
java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [ERROR] [o.a.c.t.CustomTThreadPoolServer] [run] [Error occurred during processing of message.]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:170) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.CassandraServer.system_drop_keyspace(CassandraServer.java:1008) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3476) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_keyspace.getResult(Cassandra.java:3464) ~[cassandra-thrift-1.1.3.jar:1.1.3]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.get(FutureTask.java:83) ~[na:1.6.0_33]
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 11 common frames omitted
Caused by: java.io.IOError: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1526) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1564) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:517) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:386) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) ~[na:1.6.0_33]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) ~[na:1.6.0_33]
	... 3 common frames omitted
Caused by: java.io.IOException: Exception while executing the command: cmd /c mklink /H C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\snapshots\1346340659980-PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db C:\XYZ\source_code\s7-t1\crs-inventory\inventory-core\target\test\cassandra\data\RevKeyspace\PropertyProductDefaultInventoryCounts\RevKeyspace-PropertyProductDefaultInventoryCounts-he-1-CompressionInfo.db, command error Code: 1, command output: The system cannot find the path specified.

	at org.apache.cassandra.utils.FBUtilities.exec(FBUtilities.java:573) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLinkWithExec(CLibrary.java:188) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:151) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:905) ~[cassandra-all-1.1.3.jar:1.1.3]
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1515) ~[cassandra-all-1.1.3.jar:1.1.3]
	... 10 common frames omitted
# [11:31:00.309] [DEBUG] [o.a.c.s.ClientState] [logout] [logged out: #<User allow_all groups=[]>]
# [11:31:00.310] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-5>]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [MARK HOST AS DOWN TRIGGERED for host 127.0.0.1(127.0.0.1):9162]
# [11:31:00.310] [ERROR] [m.p.c.c.HConnectionManager] [markHostAsDown] [Pool state on shutdown: <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}; IsActive?: true; Active: 1; Blocked: 0; Idle: 15; NumBeforeExhausted: 49]
# [11:31:00.311] [INFO ] [m.p.c.c.ConcurrentHClientPool] [shutdown] [Shutdown triggered on <ConcurrentCassandraClientPoolByHost>:{127.0.0.1(127.0.0.1):9162}]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-6>]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-15>]
# [11:31:00.311] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.311] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-14>]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [o.a.c.t.CustomTThreadPoolServer] [run] [Thrift transport error occurred during processing of message.]
org.apache.thrift.transport.TTransportException: Cannot read. Remote side has closed. Tried to read 4 bytes, but only got 0 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:22) ~[libthrift-0.7.0.jar:0.7.0]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186) ~[cassandra-all-1.1.3.jar:1.1.3]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_33]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_33]
	at java.lang.Thread.run(Thread.java:662) [na:1.6.0_33]
# [11:31:00.312] [DEBUG] [m.p.c.c.c.HThriftClient] [close] [Closing client CassandraClient<127.0.0.1:9162-13>]
"
CASSANDRA-4576,Error in non-upgraded node's log when upgrading another node to trunk,"I'm upgrading a 2-node cluster from cassandra-1.1 to trunk. On node1 I flush, stop the node, upgrade it to trunk, and start it. The following error gets written once a second in the log for node2:

{code}
ERROR [GossipStage:10] 2012-08-24 11:03:36,293 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[GossipStage:10,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.UnknownHostException: addr is of illegal length
	at java.net.InetAddress.getByAddress(InetAddress.java:935)
	at java.net.InetAddress.getByAddress(InetAddress.java:1318)
	at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
	at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
	at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
	... 4 more
{code}


Here is the exact code I ran to produce the error:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.partitioner = 'org.apache.cassandra.dht.RandomPartitioner'
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        (node1, node2) = cluster.nodelist()
        node1.watch_log_for('Listening for thrift clients...')
        node2.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
        import pdb; pdb.set_trace()  # <-- pause here and tail -f the node2.logfilename()
{code}"
CASSANDRA-4567,Error in log related to Murmur3Partitioner,"Start a 2-node cluster on cassandra-1.1. Bring down one node, upgrade it to trunk, start it back up. The following error shows up in the log:
{code}
...
 INFO [main] 2012-08-22 10:44:40,012 CacheService.java (line 170) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 (148 bytes)
 INFO [SSTableBatchOpen:2] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 (226 bytes)
 INFO [SSTableBatchOpen:3] 2012-08-22 10:44:40,106 SSTableReader.java (line 164) Opening /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 (89 bytes)
ERROR [SSTableBatchOpen:3] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:3,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-3 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:2] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:2,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-1 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [SSTableBatchOpen:1] 2012-08-22 10:44:40,114 CassandraDaemon.java (line 131) Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.RuntimeException: Cannot open /tmp/dtest-IYHWfV/test/node1/data/system/LocationInfo/system-LocationInfo-he-2 because partitioner does not match org.apache.cassandra.dht.Murmur3Partitioner
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:175)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:149)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:236)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [main] 2012-08-22 10:44:40,486 DatabaseDescriptor.java (line 522) Couldn't detect any schema definitions in local storage.
 INFO [main] 2012-08-22 10:44:40,487 DatabaseDescriptor.java (line 525) Found table data in data directories. Consider using the CLI to define your schema.
...
{code}

Note that the error does not happen when upgrading from cassandra-1.0 to cassandra-1.1, or when ""upgrading"" from trunk to trunk.

This is the exact dtest I used:
{code}
from dtest import Tester, debug

FROM_VERSION = 'git:cassandra-1.1'
TO_VERSION = 'git:trunk'

class TestUpgradeOneNode(Tester):

    def upgrade_test(self):
        # Start a cluster
        cluster = self.cluster
        cluster.set_cassandra_dir(cassandra_version=FROM_VERSION)
        cluster.populate(2).start()
        node1 = cluster.nodelist()[0]
        node1.watch_log_for('Listening for thrift clients...')

        # Bring one node down and upgrade it.
        node1.flush()
        node1.stop(wait_other_notice=True)
        node1.set_cassandra_dir(cassandra_version=TO_VERSION)
        node1.start(wait_other_notice=True)
{code}"
CASSANDRA-4536,Ability for CQL3 to list partition keys,"It can be useful to know the set of in-use partition keys (storage engine row keys).  One example given to me was where application data was modeled as a few 10s of 1000s of wide rows, where the app required presenting these rows to the user sorted based on information in the partition key.  The partition count is small enough to do the sort client-side in memory, which is what the app did with the Thrift API--a range slice with an empty columns list.

This was a problem when migrating to CQL3.  {{SELECT mykey FROM mytable}} includes all the logical rows, which makes the resultset too large to make this a reasonable approach, even with paging.

One way to add support would be to allow DISTINCT in the special case of {{SELECT DISTINCT mykey FROM mytable}}."
CASSANDRA-4526,Issue with CQL and ALTER TABLE DROP,"Creating a CF in cqlsh -3

{noformat}
CREATE COLUMNFAMILY ads_config (c_id uuid, ct_id uuid, pt_id uuid, c_type int, creat blob, start timestamp, end timestamp, total int, pending int, PRIMARY KEY ( campaign_id, creat_id, placement_id));
{noformat}

INSERT INTO works fine. SELECT * works fine.

{noformat}
ALTER TABLE ads_config add cost int;
{noformat}

SELECT * works fine, new field is null.

{noformat}
ALTER TABLE ads_config drop cost;
{noformat}

Gives a:
{noformat}
TSocket read 0 bytes
{noformat}

Closing and reopening cql works fine. The column key exists and can't be dropped.

{noformat}
Error log:
 INFO 12:08:40,632 Enqueuing flush of Memtable-schema_columnfamilies@445620464(1428/1785 serialized/live bytes, 20 ops)
 INFO 12:08:40,633 Writing Memtable-schema_columnfamilies@445620464(1428/1785 serialized/live bytes, 20 ops)
 INFO 12:08:40,696 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-4-Data.db (1487 bytes) for commitlog position ReplayPosition(segmentId=180414134565599, position=11928)
 INFO 12:08:40,697 Enqueuing flush of Memtable-schema_columns@1158801519(222/277 serialized/live bytes, 4 ops)
 INFO 12:08:40,697 Writing Memtable-schema_columns@1158801519(222/277 serialized/live bytes, 4 ops)
 INFO 12:08:40,704 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-4-Data.db')]
 INFO 12:08:40,731 Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-4-Data.db (273 bytes) for commitlog position ReplayPosition(segmentId=180414134565599, position=11928)
 INFO 12:08:40,761 Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-5-Data.db,].  5,892 to 2,855 (~48% of original) bytes for 1 keys at 0.050421MB/s.  Time: 54ms.
ERROR 12:08:40,780 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:396)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 12:08:40,779 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:188)
	at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:161)
	at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:148)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:99)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:108)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:121)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1237)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3542)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:3530)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:369)
	... 15 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.ColumnDefinition.fromSchema(ColumnDefinition.java:256)
	at org.apache.cassandra.config.CFMetaData.addColumnDefinitionSchema(CFMetaData.java:1293)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1225)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:294)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:396)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:271)
	at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:211)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
 INFO 12:08:40,791 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-4-Data.db')]
 INFO 12:08:40,856 Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-5-Data.db,].  2,973 to 2,537 (~85% of original) bytes for 1 keys at 0.037223MB/s.  Time: 65ms.
{noformat}

(It's my first issue reported, so please, let me know if I'm missing any relevant data :))."
CASSANDRA-4479,JMX attribute setters not consistent with cassandra.yaml,"If a setting is configurable both via cassandra.yaml and JMX, the two should be consistent. If that doesn't hold, then the JMX setter can't be trusted. 

Here I present the example of phi_convict_threshold.

I'm trying to set phi_convict_threshold via JMX, which sets FailureDetector.phiConvictThreshold_, but this doesn't update Config.phi_convict_threshold, which gets its value from cassandra.yaml when starting up.

Some places, such as FailureDetector.interpret(InetAddress), use FailureDetector.phiConvictThreshold_; others, such as AntiEntropyService.line 813 in cassandra-1.1.2, use Config.phi_convict_threshold:
{code}
            // We want a higher confidence in the failure detection than usual because failing a repair wrongly has a high cost.
            if (phi < 2 * DatabaseDescriptor.getPhiConvictThreshold())
                return;
{code}

where DatabaseDescriptor.getPhiConvictThreshold() returns Conf.phi_convict_threshold.


So, it looks like there are cases where a value is stored in multiple places, and setting the value via JMX doesn't set all of them. I'd say there should only be a single place where a configuration parameter is stored, and that single field:
* should read in the value from cassandra.yaml, optionally falling back to a sane default
* should be the field that the JMX attribute reads and sets, and
* any place that needs the current global setting should get it from that field. However, there could be cases where you read in a global value at the start of a task and keep that value locally until the end of the task.

Also, anything settable via JMX should be volatile or set via a synchronized setter, or else according to the Java memory model other threads may be stuck with the old setting.


So, I'm requesting the following:
* Setting up guidelines for how to expose a configuration parameter both via cassandra.yaml and JMX, based on what I've mentioned above
* Going through the list of configuration parameters and fixing any that don't match those guidelines


I'd also recommend logging any changes to configuration parameters."
CASSANDRA-4474,Respect five-minute flush moratorium after initial CL replay,"As noted in CASSANDRA-1967, the post-replay flush can kick off compactions before the five minute grace period introduced in CASSANDRA-3181 to avoid i/o contention while server is warming up."
CASSANDRA-4464,expose 2I CFs to the rest of nodetool,"This was begun in CASSANDRA-4063.  We should extend it to scrub as well, and probably compact since any sane way to do it for scrub should give the other for free.

Not sure how easy these will be since they go through CompactionManager via StorageProxy.  I think getValidColumnFamilies could be updated to check for index CFs with ""dot notation.""

(Other operations like flush or snapshot don't make sense for 2I CFs in isolation of their parent.)"
CASSANDRA-4462,upgradesstables strips active data from sstables,"From the discussion here: http://mail-archives.apache.org/mod_mbox/cassandra-user/201207.mbox/%3CCAOac0GCtyDqS6ocuHOuQqre4re5wKj3o-ZpUZGkGsjCHzDVbTA%40mail.gmail.com%3E

We are trying to migrate a 0.8.8 cluster to 1.1.2 by migrating the sstables from the 0.8.8 ring to a parallel 1.1.2 ring. However, every time we run the `nodetool upgradesstables` step we find it removes active data from our CFs -- leading to lost data in our application.

The steps we took were:


1. Bring up a 1.1.2 ring in the same AZ/data center configuration with
tokens matching the corresponding nodes in the 0.8.8 ring.
2. Create the same keyspace on 1.1.2.
3. Create each CF in the keyspace on 1.1.2.
4. Flush each node of the 0.8.8 ring.
5. Rsync each non-compacted sstable from 0.8.8 to the corresponding node in
1.1.2.
6. Move each 0.8.8 sstable into the 1.1.2 directory structure by renaming the file to the  /cassandra/data/<keyspace>/<cf>/<keyspace>-<cf>... format. For example, for the keyspace ""Metrics"" and CF ""epochs_60"" we get:
""cassandra/data/Metrics/epochs_60/Metrics-epochs_60-g-941-Data.db"".
7. On each 1.1.2 node run `nodetool -h localhost refresh Metrics <CF>` for each CF in the keyspace. We notice that storage load jumps accordingly.
8. On each 1.1.2 node run `nodetool -h localhost upgradesstables`.

Afterwards we would test the validity of the data by comparing it with data from the original 0.8.8 ring. After an upgradesstables command the data was always incorrect.

With further testing we found that we could successfully use scrub to convert our sstables without data loss. However, any invocation of upgradesstables causes active data to be culled from the sstables:

 INFO [CompactionExecutor:4] 2012-07-24 04:27:36,837 CompactionTask.java (line 109) Compacting [SSTableReader(path='/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-51-Data.db')]
 INFO [CompactionExecutor:4] 2012-07-24 04:27:51,090 CompactionTask.java (line 221) Compacted to [/raid0/cassandra/data/Metrics/metrics_900/Metrics-metrics_900-hd-58-Data.db,].  60,449,155 to 2,578,102 (~4% of original) bytes for 4,002 keys at 0.172562MB/s.  Time: 14,248ms.

These are the steps we've tried:

WORKS		refresh -> scrub
WORKS		refresh -> scrub -> major compaction
WORKS		refresh -> scrub -> cleanup
WORKS		refresh -> scrub -> repair

FAILS		refresh -> upgradesstables
FAILS		refresh -> scrub -> upgradesstables
FAILS		refresh -> scrub -> repair -> upgradesstables
FAILS		refresh -> scrub -> major compaction -> upgradesstables

We have fewer than 143 million row keys in the CFs we're testing and none
of the *-Filter.db files are > 10MB, so I don't believe this is our
problem: https://issues.apache.org/jira/browse/CASSANDRA-3820

The keyspace is defined as:

Keyspace: Metrics:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [us-east:3]

And the column family that we tested with is defined as:

    ColumnFamily: metrics_900
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.1
      DC Local Read repair chance: 0.0
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor

All rows have a TTL of 30 days and a gc_grace=0 so it's possible that a small number of older columns would be removed during a compaction/scrub/upgradesstables step. However, the majority should still be kept as their TTL's have not expired yet."
CASSANDRA-4446,nodetool drain sometimes doesn't mark commitlog fully flushed,"I recently wiped a customer's QA cluster. I drained each node and verified that they were drained. When I restarted the nodes, I saw the commitlog replay create a memtable and then flush it. I have attached a sanitized log snippet from a representative node at the time. 

It appears to show the following :
1) Drain begins
2) Drain triggers flush
3) Flush triggers compaction
4) StorageService logs DRAINED message
5) compaction thread excepts
6) on restart, same CF creates a memtable
7) and then flushes it [1]

The columnfamily involved in the replay in 7) is the CF for which the compaction thread excepted in 5). This seems to suggest a timing issue whereby the exception in 5) prevents the flush in 3) from marking all the segments flushed, causing them to replay after restart.

In case it might be relevant, I did an online change of compaction strategy from Leveled to SizeTiered during the uptime period preceding this drain.

[1] Isn't commitlog replay not supposed to automatically trigger a flush in modern cassandra?"
CASSANDRA-4396,Subcolumns not removed when compacting tombstoned super column,"When we compact a tombstone for a super column with the old data for that super column, we end up writing the deleted super column and all the subcolumn data that is now worthless to the new sstable. This is especially inefficient when reads need to scan tombstones during a slice.

Here is the output of a simple test I ran to confirm:

insert supercolumn, then flush
{noformat}
Nicks-MacBook-Pro:12:20:52 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-1-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": -9223372036854775808, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
{noformat}

delete supercolumn, flush again

{noformat}
[Nicks-MacBook-Pro:12:20:59 cassandra-1.0] cassandra$ bin/nodetool -h localhost flush
[Nicks-MacBook-Pro:12:22:41 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-2-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": []}}
}
{noformat}

compact and check resulting sstable

{noformat}
[Nicks-MacBook-Pro:12:22:55 cassandra-1.0] cassandra$ bin/nodetool -h localhost compact 
[Nicks-MacBook-Pro:12:23:09 cassandra-1.0] cassandra$ bin/sstable2json ~/.ccm/1node/node1/data/Keyspace2/Super4-hd-3-Data.db 
{
""6b657931"": {""supercol1"": {""deletedAt"": 1340990544005000, ""subColumns"": [[""737562636f6c31"",""7468697320697320612074657374"",1340990212532000]]}}
}
[Nicks-MacBook-Pro:12:23:20 cassandra-1.0] cassandra$ 
{noformat}"
CASSANDRA-4393,Reduce default BF size,With improvements like leveled compaction and CASSANDRA-2503 it's no longer worth throwing quite so much memory at BF to avoid false positives.
CASSANDRA-4338,Experiment with direct buffer in SequentialWriter,Using a direct buffer instead of a heap-based byte[] should let us avoid a copy into native memory when we flush the buffer.
CASSANDRA-4328,CQL client timeout when inserting data after creating index,"After creating index on table inserts fails.
steps (from cqlsh -3)
create table myapp (pidh text, cn text, tn text, s text, m text, ts bigint, PRIMARY KEY (pidh, ts));
INSERT INTO myapp(pidh, cn, tn, s, m, ts) VALUES ('4274@localhost','Test.tests','main','text','bzzzzz',2231897614162493);
create index idx_cn on myapp(cn);

Next insert from cql client time outs without showing error.
Each insert in systemlog gives ERROR [MutationStage:xx] ....
from log file:

 INFO [MigrationStage:1] 2012-06-11 12:28:35,715 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,716 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@1502301540(1259/1573 serialized/live bytes, 20 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:35,868 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db (1312 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [MigrationStage:1] 2012-06-11 12:28:35,869 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:35,869 CompactionTask.java (line 109) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-141-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-142-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-140-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-143-Data.db')]
 INFO [FlushWriter:4] 2012-06-11 12:28:35,869 Memtable.java (line 266) Writing Memtable-schema_columns@1756291746(280/350 serialized/live bytes, 5 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,104 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hd-65-Data.db (325 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [CompactionExecutor:26] 2012-06-11 12:28:36,130 CompactionTask.java (line 221) Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hd-144-Data.db,].  42,461 to 38,525 (~90% of original) bytes for 3 keys at 0.140767MB/s.  Time: 261ms.
 INFO [MigrationStage:1] 2012-06-11 12:28:36,140 SecondaryIndexManager.java (line 208) Creating new index : ColumnDefinition{name=636e, validator=org.apache.cassandra.db.marshal.UTF8Type, index_type=KEYS, index_name='idx_cn', component_index=1}
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,141 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,141 Memtable.java (line 266) Writing Memtable-myapp@207814912(171/213 serialized/live bytes, 4 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,255 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db (170 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8651)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,256 SecondaryIndex.java (line 159) Submitting index build of myapp.idx_cn for data in SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Logging/myapp/Logging-myapp-hd-2-Data.db')
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,258 ColumnFamilyStore.java (line 633) Enqueuing flush of Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,258 Memtable.java (line 266) Writing Memtable-IndexInfo@409882274(39/48 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-06-11 12:28:36,390 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/IndexInfo/system-IndexInfo-hd-14-Data.db (84 bytes) for commitlog position ReplayPosition(segmentId=2232837134267994, position=8744)
 INFO [Creating index: myapp.idx_cn] 2012-06-11 12:28:36,390 SecondaryIndex.java (line 200) Index build of myapp.idx_cn complete
ERROR [MutationStage:37] 2012-06-11 12:28:39,657 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:37,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more


ERROR [MutationStage:39] 2012-06-11 12:29:39,876 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MutationStage:39,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1254)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:267)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:369)
	at java.util.TreeMap.getEntry(TreeMap.java:340)
	at java.util.TreeMap.containsKey(TreeMap.java:227)
	at java.util.TreeMap$KeySet.contains(TreeMap.java:1045)
	at org.apache.cassandra.db.Table.apply(Table.java:415)
	at org.apache.cassandra.db.Table.apply(Table.java:380)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:294)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:453)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1250)
	... 3 more
"
CASSANDRA-4321,stackoverflow building interval tree & possible sstable corruptions,"After upgrading to 1.1.1 (from 1.1.0) I have started experiencing StackOverflowError's resulting in compaction backlog and failure to restart. 

The ring currently consists of 6 DC's and 22 nodes using LCS & compression.  This issue was first noted on 2 nodes in one DC and then appears to have spread to various other nodes in the other DC's.  

When the first occurrence of this was found I restarted the instance but it failed to start so I cleared its data and treated it as a replacement node for the token it was previously responsible for.  This node successfully streamed all the relevant data back but failed again a number of hours later with the same StackOverflowError and again was unable to restart. 

The initial stack overflow error on a running instance looks like this:

ERROR [CompactionExecutor:314] 2012-06-07 09:59:43,017 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[CompactionExecutor:314,1,main]
java.lang.StackOverflowError
        at java.util.Arrays.mergeSort(Arrays.java:1157)
        at java.util.Arrays.sort(Arrays.java:1092)
        at java.util.Collections.sort(Collections.java:134)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:114)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:49)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow.  Compactions stop from this point onwards]


I restarted this failing instance with DEBUG logging enabled and it throws the following exception part way through startup:

ERROR 11:37:51,046 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.db.Table$2.apply(Table.java:574)
        at org.apache.cassandra.db.Table$2.apply(Table.java:571)
        at com.google.common.collect.Iterators$8.next(Iterators.java:751)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1625)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG 11:37:51,052 Initializing ksU.cfS


And then finally fails with the following:

DEBUG 11:49:03,752 Creating IntervalNode from [Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b)), Interval(DecoratedKey(104860264640932324846851821824650966808, 4fcc88eb0218216164673394), DecoratedKey(93975306025956344620001177071135439009, 4fc8fb042c98458c7a58bc3b))]
java.lang.reflect.InvocationTargetException
DEBUG 11:49:03,753 Configured datacenter replicas are dc1:2, dc2:2, dc3:2, dc4:2, dc5:0, dc6:2, dc7:0, dc8:0, dc9:2
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.StackOverflowError
        at org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:307)
        at org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:276)
        at org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:230)
        at org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)
        at org.slf4j.impl.Log4jLoggerAdapter.debug(Log4jLoggerAdapter.java:228)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:45)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)

[snip - this repeats until stack overflow]

        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:64)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:62)
        at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:39)
        at org.apache.cassandra.db.DataTracker.buildIntervalTree(DataTracker.java:560)
        at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:617)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
        at org.apache.cassandra.db.DataTracker.addInitialSSTables(DataTracker.java:259)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:234)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:331)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:309)
        at org.apache.cassandra.db.Table.initCf(Table.java:367)
        at org.apache.cassandra.db.Table.<init>(Table.java:299)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:254)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3

Running with assertions enabled allows me to start the instance but when doing so I get errors such as:

ERROR 01:22:22,753 Exception in thread Thread[SSTableBatchOpen:2,5,main]java.lang.AssertionError: SSTable first key DecoratedKey(100294972947100949193477090306072672386, 4fcf051ef5067d7f17d9fc35) > last key DecoratedKey(90250429663386465697464050082134975058, 4fce996e3c1eed8c4b17dd66)
at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:412)
at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:187)
at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:225)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:636)

and:

ERROR 01:27:58,946 Exception in thread Thread[CompactionExecutor:9,1,main]
java.lang.AssertionError: Last written key DecoratedKey(81958437188197992567937826278457419048, 4fa1aebad23f81e4321d344d) >= current key DecoratedKey(64546479828744423263742604083767363606, 4fcafc0f19f6a8092d4d4f94) writing into /var/lib/XX/data/cassandra/ks1/cf1/ks1-cf1-tmp-hd-657317-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:134)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:159)
        at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:50)
        at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:150)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

Just like the initial errors compactions appear to stop occurring after this point.  

Given the above this looks like sstables are getting corrupted.  By restarting nodes I am able to identify several hundred sstables exhibiting the same problem and this appears to be growing.

I have tried scrubbing those affected nodes but the problem continues to occur.  If this is due to sstable corruptions is there another way of validating sstables for correctness?  Given that it has spread to various servers in other DC's it looks like this is directly related to the 1.1.1 upgrade recently performed on the ring."
CASSANDRA-4314,Index CF tombstones can cause OOM,"My database (now at 1.0.10) is in a state in which it goes out of memory with hardly any activity at all.  A key slice nothing more.

The logs attached are this including verbose gc in stdout.  I started up cassandra and waited a bit to ensure that it was unperturbed.

Then (about 15:46) I ran this slice (using Pelops), which in this case should return NO data.  My client times out and the database goes OOM.

                  ConsistencyLevel cl = ConsistencyLevel.TWO;//TWO nodes in my cluster
                  Selector s = new Selector(this.pool);
                  List<IndexExpression> indexExpressions = new ArrayList<IndexExpression>();
                  IndexExpression e = new IndexExpression(
                              ByteBuffer.wrap(""encryptionSettingsID"".getBytes(ASCII)), IndexOperator.EQ,
                              ByteBuffer.wrap(encryptionSettingsID.getBytes(Utils.ASCII)));
                  indexExpressions.add(e);
                  IndexClause indexClause = new IndexClause(indexExpressions,
                              ByteBuffer.wrap(EMPTY_BYTE_ARRAY), 1);
                  SlicePredicate predicate = new SlicePredicate();
                  predicate.setColumn_names(Arrays.asList(new ByteBuffer[]
                        { ByteBuffer.wrap(COL_PAN_ENC_BYTES) }));
                  List<KeySlice> slices = s.getKeySlices(CF_TOKEN, indexClause, predicate, cl);

Note that “encryptionSettingsID” is an indexed column.  When this is executed there should be no columns with the supplied value.

I suppose I may have some kind of blatant error in this query but it is not obvious to me.  I’m relatively new to cassandra.

My key space is defined as follows:

KsDef(name:TB_UNIT, strategy_class:org.apache.cassandra.locator.SimpleStrategy, strategy_options:{replication_factor=3}, 
cf_defs:[

CfDef(keyspace:TB_UNIT, name:token, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:70 61 6E 45 6E 63, validation_class:BytesType), ColumnDef(name:63 72 65 61 74 65 54 73, validation_class:DateType), ColumnDef(name:63 72 65 61 74 65 44 61 74 65, validation_class:DateType, index_type:KEYS, index_name:TokenCreateDate), ColumnDef(name:65 6E 63 72 79 70 74 69 6F 6E 53 65 74 74 69 6E 67 73 49 44, validation_class:UTF8Type, index_type:KEYS, index_name:EncryptionSettingsID)], caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:pan_d721fd40fd9443aa81cc6f59c8e047c6, column_type:Standard, comparator_type:BytesType, caching:keys_only), 

CfDef(keyspace:TB_UNIT, name:counters, column_type:Standard, comparator_type:BytesType, column_metadata:[ColumnDef(name:75 73 65 43 6F 75 6E 74, validation_class:CounterColumnType)], default_validation_class:CounterColumnType, caching:keys_only)

])


tpstats show pending tasks many minutes after time out:


[root@r610-lb6 bin]# ../cassandra/bin/nodetool -h 127.0.0.1 tpstats
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
ReadStage                         3         3            107         0                 0
RequestResponseStage              0         0             56         0                 0
MutationStage                     0         0              6         0                 0
ReadRepairStage                   0         0              0         0                 0
ReplicateOnWriteStage             0         0              0         0                 0
GossipStage                       0         0           2231         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
MemtablePostFlusher               0         0              3         0                 0
StreamStage                       0         0              0         0                 0
FlushWriter                       0         0              3         0                 0
MiscStage                         0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0              9         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
BINARY                       0
READ                         0
MUTATION                     0
REQUEST_RESPONSE             0

cfstats:

Keyspace: keyspace
        Read Count: 118
        Read Latency: 0.14722033898305084 ms.
        Write Count: 0
        Write Latency: NaN ms.
        Pending Tasks: 0
                Column Family: token
                SSTable count: 7
                Space used (live): 4745885584
                Space used (total): 4745885584
                Number of Keys (estimate): 18626048
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 118
                Read Latency: 0.147 ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 55058352
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 150
                Compacted row maximum size: 258
                Compacted row mean size: 201

                Column Family: pan_2fef6478b62242dd94aecaa049b9d7bb
                SSTable count: 7
                Space used (live): 1987147156
                Space used (total): 1987147156
                Number of Keys (estimate): 14955264
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 28056224
                Key cache: disabled
                Row cache: disabled
                Compacted row minimum size: 104
                Compacted row maximum size: 124
                Compacted row mean size: 124

                Column Family: counters
                SSTable count: 11
                Space used (live): 3433469364
                Space used (total): 3433469364
                Number of Keys (estimate): 21475328
                Memtable Columns Count: 0
                Memtable Data Size: 0
                Memtable Switch Count: 0
                Read Count: 0
                Read Latency: NaN ms.
                Write Count: 0
                Write Latency: NaN ms.
                Pending Tasks: 0
                Bloom Filter False Postives: 0
                Bloom Filter False Ratio: 0.00000
                Bloom Filter Space Used: 40271696
                Key cache capacity: 4652
                Key cache size: 4652
                Key cache hit rate: NaN
                Row cache: disabled
                Compacted row minimum size: 125
                Compacted row maximum size: 179
                Compacted row mean size: 150

"
CASSANDRA-4303,Compressed bloomfilters,"Very commonly, people encountering an OOM need to increase their bloom filter false positive ratio to reduce memory pressure, since BFs tend to be the largest shareholder.  It would make sense if we could alleviate the memory pressure from BFs with compression while maintaining the FP ratio (at the cost of a bit of cpu) that some users have come to expect.  One possible implementation is at http://code.google.com/p/javaewah/"
CASSANDRA-4296,CQL3: create table don't always validate access to the right keyspace,"Create table allows (like other queries) to override the currently set keyspace ({{CREATE TABLE foo.bar ...}}). However, when we do that, the access check is done on the wrong keyspace. In particular if no keyspace was set, this end up in a NPE."
CASSANDRA-4292,Improve JBOD loadbalancing and reduce contention,"As noted in CASSANDRA-809, we have a certain amount of flush (and compaction) threads, which mix and match disk volumes indiscriminately.  It may be worth creating a tight thread -> disk affinity, to prevent unnecessary conflict at that level.

OTOH as SSDs become more prevalent this becomes a non-issue.  Unclear how much pain this actually causes in practice in the meantime."
CASSANDRA-4279,kick off background compaction when min/max changed,"When the threshold changes, we may be eligible for a compaction immediately (without waiting for a flush to trigger the eligibility check)."
CASSANDRA-4265,Limit total open connections (HSHA server),"When using the rpc_server_type: hsha there seems to be no limit on the number of open connections that cassandra accepts / on the total memory consumed by them. This can lead to OOM errors since the HSHA server assigns a FrameBuffer per connection which is only cleaned up when the connection is closed.

Setup:
I wrote a simple test App using Hector which iterated through my rows retrieving data. If my Hector connection pool size was set high (in this case 100) then after a while cassandra would crash with OOM. The program was sequential so only 1 connection was actually in use at any one time but from what I can tell (and from MAT analysis) all the open connections were consuming memory as well (the FrameBuffers).

At the moment all the solutions to this OOM problem seem to rest with the client. The memory consumed (on a node) is equal to [open connections]*[max request/response size] (it is max because of https://issues.apache.org/jira/browse/THRIFT-1457).

This means the client needs to know how much memory each node has spare for it to use up with its connection pool. If you have a distributed set of clients then they would have to co-ordinate on how many open connections they have per node.

I was just testing on a dev machine with small heap sizes (512mb-2GB) but the memory consumed as stated is based off connections and buffer size so this problem would scale for larger heap sizes.

Solutions:

The simplest would be a limit on the number of connections the HSHA server accepts. I only started looking into cassandra a few days ago but I tried a very simple connection limit mechanism (I will attach the patch) which seemed to work. I'm sure it can be done much cleaner than my version.

This means the client or clients only have to have a hard limit on their max request size (lets say 2mb). Then for each node you know that allowing 100 connections to this node would potentially use up 200mb of memory. You can then tune this number per node. (This isn't perfect since clients can't always tell exactly how big a serialised response will be so you can go above the 200mb)

A more complex solution might be able to remove the burden from the client completely. Thrift doesn't have streaming support but I assume when cassandra reads data from disk / memtables streaming can be done at that point. If this is the case then you can monitor how much memory client connections are consuming in total. If a request comes in and buffering the request / buffering the response would push cassandra over the limit then you can send an error back instead of servicing the request."
CASSANDRA-4261,[patch] Support consistency-latency prediction in nodetool,"h3. Introduction

Cassandra supports a variety of replication configurations: ReplicationFactor is set per-ColumnFamily and ConsistencyLevel is set per-request. Setting {{ConsistencyLevel}} to {{QUORUM}} for reads and writes ensures strong consistency, but {{QUORUM}} is often slower than {{ONE}}, {{TWO}}, or {{THREE}}. What should users choose?

This patch provides a latency-consistency analysis within {{nodetool}}. Users can accurately predict Cassandra's behavior in their production environments without interfering with performance.

What's the probability that we'll read a write t seconds after it completes? What about reading one of the last k writes? This patch provides answers via {{nodetool predictconsistency}}:

{{nodetool predictconsistency ReplicationFactor TimeAfterWrite Versions}}
\\ \\
{code:title=Example output|borderStyle=solid}

//N == ReplicationFactor
//R == read ConsistencyLevel
//W == write ConsistencyLevel

user@test:$ nodetool predictconsistency 3 100 1
Performing consistency prediction
100ms after a given write, with maximum version staleness of k=1
N=3, R=1, W=1
Probability of consistent reads: 0.678900
Average read latency: 5.377900ms (99.900th %ile 40ms)
Average write latency: 36.971298ms (99.900th %ile 294ms)

N=3, R=1, W=2
Probability of consistent reads: 0.791600
Average read latency: 5.372500ms (99.900th %ile 39ms)
Average write latency: 303.630890ms (99.900th %ile 357ms)

N=3, R=1, W=3
Probability of consistent reads: 1.000000
Average read latency: 5.426600ms (99.900th %ile 42ms)
Average write latency: 1382.650879ms (99.900th %ile 629ms)

N=3, R=2, W=1
Probability of consistent reads: 0.915800
Average read latency: 11.091000ms (99.900th %ile 348ms)
Average write latency: 42.663101ms (99.900th %ile 284ms)

N=3, R=2, W=2
Probability of consistent reads: 1.000000
Average read latency: 10.606800ms (99.900th %ile 263ms)
Average write latency: 310.117615ms (99.900th %ile 335ms)

N=3, R=3, W=1
Probability of consistent reads: 1.000000
Average read latency: 52.657501ms (99.900th %ile 565ms)
Average write latency: 39.949799ms (99.900th %ile 237ms)
{code}

h3. Demo

Here's an example scenario you can run using [ccm|https://github.com/pcmanus/ccm]. The prediction is fast:

{code:borderStyle=solid}
cd <cassandra-source-dir with patch applied>
ant

ccm create consistencytest --cassandra-dir=. 
ccm populate -n 5
ccm start

# if start fails, you might need to initialize more loopback interfaces
# e.g., sudo ifconfig lo0 alias 127.0.0.2

# use stress to get some sample latency data
tools/bin/stress -d 127.0.0.1 -l 3 -n 10000 -o insert
tools/bin/stress -d 127.0.0.1 -l 3 -n 10000 -o read

bin/nodetool -h 127.0.0.1 -p 7100 predictconsistency 3 100 1
{code}

h3. What and Why

We've implemented [Probabilistically Bounded Staleness|http://pbs.cs.berkeley.edu/#demo], a new technique for predicting consistency-latency trade-offs within Cassandra. Our [paper|http://arxiv.org/pdf/1204.6082.pdf] will appear in [VLDB 2012|http://www.vldb2012.org/], and, in it, we've used PBS to profile a range of Dynamo-style data store deployments at places like LinkedIn and Yammer in addition to profiling our own Cassandra deployments. In our experience, prediction is both accurate and much more lightweight than profiling and manually testing each possible replication configuration (especially in production!).

This analysis is important for the many users we've talked to and heard about who use ""partial quorum"" operation (e.g., non-{{QUORUM}} {{ConsistencyLevel}}). Should they use CL={{ONE}}? CL={{TWO}}? It likely depends on their runtime environment and, short of profiling in production, there's no existing way to answer these questions. (Keep in mind, Cassandra defaults to CL={{ONE}}, meaning users don't know how stale their data will be.)

We outline limitations of the current approach after describing how it's done. We believe that this is a useful feature that can provide guidance and fairly accurate estimation for most users.

h3. Interface

This patch allows users to perform this prediction in production using {{nodetool}}.

Users enable tracing of latency data by calling {{enableConsistencyPredictionLogging()}} in the {{PBSPredictorMBean}}.

Cassandra logs a variable number of latencies (controllable via JMX ({{setMaxLoggedLatenciesForConsistencyPrediction(int maxLogged)}}, default: 10000). Each latency is 8 bytes, and there are 4 distributions we require, so the space overhead is {{32*logged_latencies}} bytes of memory for the predicting node.

{{nodetool predictconsistency}} predicts the latency and consistency for each possible {{ConsistencyLevel}} setting (reads and writes) by running {{setNumberTrialsForConsistencyPrediction(int numTrials)}} Monte Carlo trials per configuration (default: 10000).

Users shouldn't have to touch these parameters, and the defaults work well. The more latencies they log, the better the predictions will be.

h3. Implementation

This patch is fairly lightweight, requiring minimal changes to existing code. The high-level overview is that we gather empirical latency distributions then perform Monte Carlo analysis using the gathered data.

h4. Latency Data

We log latency data in {{service.PBSPredictor}}, recording four relevant distributions:
 * *W*: time from when the coordinator sends a mutation to the time that a replica begins to serve the new value(s)
 * *A*: time from when a replica accepting a mutation sends an
 * *R*: time from when the coordinator sends a read request to the time that the replica performs the read
* *S*: time from when the replica sends a read response to the time when the coordinator receives it

We augment {{net.MessageIn}} and {{net.MessageOut}} to store timestamps along with every message (8 bytes overhead required for millisecond {{long}}). In {{net.MessagingService}}, we log the start of every mutation and read, and, in {{net.ResponseVerbHandler}}, we log the end of every mutation and read. Jonathan Ellis mentioned that [1123|https://issues.apache.org/jira/browse/CASSANDRA-1123] had similar latency tracing, but, as far as we can tell, these latencies aren't in that patch. We use an LRU policy to bound the number of latencies we track for each distribution.

h4. Prediction

When prompted by {{nodetool}}, we call {{service.PBSPredictor.doPrediction}}, which performs the actual Monte Carlo analysis based on the provided data. It's straightforward, and we've commented this analysis pretty well but can elaborate more here if required.

h4. Testing

We've modified the unit test for {{SerializationsTest}} and provided a new unit test for {{PBSPredictor}} ({{PBSPredictorTest}}). You can run the {{PBSPredictor}} test with {{ant pbs-test}}.

h4. Overhead

This patch introduces 8 bytes of overhead per message. We could reduce this overhead and add timestamps on-demand, but this would require changing {{net.MessageIn}} and {{net.MessageOut}} serialization at runtime, which is messy.

If enabled, consistency tracing requires {{32*logged_latencies}} bytes of memory on the node on which tracing is enabled.

h3. Caveats

 The predictions are conservative, or worst-case, meaning we may predict more staleness than in practice in the following ways:
 * We do not account for read repair. 
 * We do not account for Merkle tree exchange.
 * Multi-version staleness is particularly conservative.

The predictions are optimistic in the following ways:
 * We do not predict the impact of node failure.
 * We do not model hinted handoff.

We simulate non-local reads and writes. We assume that the coordinating Cassandra node is not itself a replica for a given key. (See discussion below.)

Predictions are only as good as the collected latencies. Generally, the more latencies that are collected, the better, but if the environment or workload changes, things might change. Also, we currently don't distinguish between column families or value sizes. This is doable, but it adds complexity to the interface and possibly more storage overhead.

Finally, for accurate results, we require replicas to have synchronized clocks (Cassandra requires this from clients anyway). If clocks are skewed/out of sync, this will bias predictions by the magnitude of the skew.

We can potentially improve these if there's interest, but this is an area of active research.
----
Peter Bailis and Shivaram Venkataraman
[pbailis@cs.berkeley.edu|mailto:pbailis@cs.berkeley.edu]
[shivaram@cs.berkeley.edu|mailto:shivaram@cs.berkeley.edu]"
CASSANDRA-4230,Deleting a CF always produces an error and that CF remains in an unknown state,"From the CLI perspective:

[default@Disco] drop column family client; 
null
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_column_family(Cassandra.java:1222)
	at org.apache.cassandra.thrift.Cassandra$Client.system_drop_column_family(Cassandra.java:1209)
	at org.apache.cassandra.cli.CliClient.executeDelColumnFamily(CliClient.java:1301)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:234)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:219)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

Log:

 INFO [MigrationStage:1] 2012-05-09 11:25:35,686 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,687 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@225225949(978/1222 serialized/live bytes, 21 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,748 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-34-Data.db (1041 bytes)
 INFO [MigrationStage:1] 2012-05-09 11:25:35,749 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,750 Memtable.java (line 266) Writing Memtable-schema_columns@213209572(586/732 serialized/live bytes, 12 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,812 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db (649 bytes)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,814 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-27-Data.db'), SSTableReader
(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-26-Data.db'), SSTableReader(path
='/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-28-Data.db')]
 INFO [MigrationStage:1] 2012-05-09 11:25:35,918 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [FlushWriter:3] 2012-05-09 11:25:35,919 Memtable.java (line 266) Writing Memtable-Client@864320066(372/465 serialized/live bytes, 6 ops)
 INFO [CompactionExecutor:20] 2012-05-09 11:25:35,945 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-29-Data.db,].  22,486 to 20,621 (~91% of orig
inal) bytes for 2 keys at 0.150120MB/s.  Time: 131ms.
 INFO [FlushWriter:3] 2012-05-09 11:25:36,013 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db (407 bytes)
ERROR [MigrationStage:1] 2012-05-09 11:25:36,043 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
        at org.apache.cassandra.utils.CLibrary.link(Native Method)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:150)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

ERROR [Thrift:3] 2012-05-09 11:25:36,048 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
        at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
        at org.apache.cassandra.service.MigrationManager.announceColumnFamilyDrop(MigrationManager.java:182)
        at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:948)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3348)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3336)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
        ... 11 more
Caused by: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
ERROR [MigrationStage:1] 2012-05-09 11:25:36,051 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
        at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
        at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)

        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336559135918-Client/Client.json (errno 17)
        at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
        at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
        ... 10 more
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,052 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-4-Data.db')]
 INFO [CompactionExecutor:22] 2012-05-09 11:25:36,187 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/Disco/Client/Disco-Client-hc-6-Data.db,].  728 to 458 (~62% of original) bytes for 8 keys at 0.003235MB/s.  Time: 135ms.

Schema:

CREATE COLUMN FAMILY Client WITH
       key_validation_class = UUIDType AND
       comparator = UTF8Type AND
       column_metadata = [ { column_name: key, validation_class: BytesType }
                           { column_name: name, validation_class: UTF8Type }
                           { column_name: userid, validation_class: UUIDType, index_type: KEYS }
                         ] AND
       compression_options = { sstable_compression:SnappyCompressor, chunk_length_kb:64 } AND
       compaction_strategy = LeveledCompactionStrategy AND
       compaction_strategy_options = { sstable_size_in_mb: 10 } AND
       gc_grace = 432000;

State of data dir after deletion attempt:

# ls -lah /var/lib/cassandra/data/Disco/Client/ 
total 76K
drwxr-xr-x  3 cassandra cassandra 4.0K May  9 11:25 .
drwxr-xr-x 17 cassandra cassandra 4.0K May  3 12:34 ..
-rw-r--r--  2 cassandra cassandra  420 May  9 11:25 Client-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx-old.json
-rw-r--r--  1 cassandra cassandra  418 May  7 18:04 Client.Client_userid_idx.json
-rw-r--r--  1 cassandra cassandra  418 May  9 11:25 Client.json
-rw-r--r--  1 cassandra cassandra   46 May  9 11:25 Disco-Client-hc-6-CompressionInfo.db
-rw-r--r--  1 cassandra cassandra  458 May  9 11:25 Disco-Client-hc-6-Data.db
-rw-r--r--  1 cassandra cassandra  976 May  9 11:25 Disco-Client-hc-6-Filter.db
-rw-r--r--  1 cassandra cassandra  208 May  9 11:25 Disco-Client-hc-6-Index.db
-rw-r--r--  1 cassandra cassandra 4.3K May  9 11:25 Disco-Client-hc-6-Statistics.db
-rw-r--r--  4 cassandra cassandra   46 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-CompressionInfo.db
-rw-r--r--  4 cassandra cassandra   92 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Data.db
-rw-r--r--  4 cassandra cassandra  496 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Filter.db
-rw-r--r--  4 cassandra cassandra   26 May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Index.db
-rw-r--r--  4 cassandra cassandra 4.3K May  7 18:04 Disco-Client.Client_userid_idx-hc-2-Statistics.db
drwxr-xr-x  6 cassandra cassandra 4.0K May  9 11:25 snapshots
"
CASSANDRA-4222,Improve serializing off-heap cache by not using finalizers,"We have a cluster that takes alot of reads and we have some issues with the off-heap caching, seeing several G of Finalizer objects on the heap

My conclusion is that the finalizer thread is too slow to run the finalize method on the FreeableMemory (and Memory) objects.

Simply removing the finalize() methods improves performance _alot_ (using finalizers is 50% slower in my micro benchmarks) and does not leak memory during ""normal"" use as far as i can see

im not sure about the implications for other use cases though"
CASSANDRA-4205,SSTables are not updated with max timestamp on upgradesstables/compaction leading to non-optimal performance.,"We upgraded from 0.7.9 to 1.0.7 on a cluster with a heavy update load. After converting all the reads to named column reads instead of get_slice calls, we noticed that we still weren't getting the performance improvements implemented in CASSANDRA-2498. A single named column read was still touching multiple SSTables according to nodetool cfhistograms. 

To verify whether or not this was a reporting issue or a real issue, we ran multiple tests with stress and noticed that it worked as expected. After changing stress so that it ran the read/write test directly in the CF having issues (3 times stress & flush), we noticed that stress also touched multiple SSTables (according to cfhistograms).

So, the root of the problem is ""something"" left over from our pre-1.0 days. All SSTables were upgraded with upgradesstables, and have been written and compacted many times since the upgrade (4 months ago). The usage pattern for this CF is that it is constantly read and updated (overwritten), but no deletes. 

After discussing the problem with Brandon Williams on #cassandra, it seems the problem might be because a max timestamp has never been written for the old SSTables that were upgraded from pre 1.0. They have only been compacted, and the max timestamp is not recorded during compactions. 

A suggested fix is to special case this in upgradesstables so that a max timestamp always exists for all SSTables. 

{panel}
06:08 < driftx> thorkild_: tx.  The thing is we don't record the max timestamp on compactions, but we can do it specially for upgradesstables.
06:08 < driftx> so, nothing in... nothing out.
06:10 < thorkild_> driftx: ah, so when you upgrade from before the metadata was written, and that data is only feed through upgradesstables and compactions -> never properly written?
06:10 < thorkild_> that makes sense.
06:11 < driftx> right, we never create it, we just reuse it :(
{panel}
"
CASSANDRA-4195,error in log when upgrading multi-node cluster to 1.1,"I upgraded a cluster from 1.0.9 to 1.1.0. The following message shows up in the logs for all but the first node.
{code}
ERROR [GossipStage:1] 2012-04-30 07:37:06,986 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.UnsupportedOperationException: Not a time-based UUID                  
    at java.util.UUID.timestamp(UUID.java:331)                                  
    at org.apache.cassandra.service.MigrationManager.updateHighestKnown(MigrationManager.java:121)
    at org.apache.cassandra.service.MigrationManager.rectify(MigrationManager.java:99)
    at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:83)
    at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:806)           
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:908)   
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:62)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:679)
{code}

this dtest demonstrates the issue. It was added to the cassandra-dtest repository as upgrade_to_11_test:
{code}
from dtest import Tester, debug 
from tools import * 
 
class TestUpgradeTo1_1(Tester): 
 
    def upgrade_test(self): 
        self.num_rows = 0 
        cluster = self.cluster 
 
        # Forcing cluster version on purpose 
        cluster.set_cassandra_dir(cassandra_version='1.0.9') 
 
        cluster.populate(3).start() 
        time.sleep(1) 
 
        for node in cluster.nodelist():     
            node.flush() 
            time.sleep(.5) 
            node.stop(wait_other_notice=True) 
            node.set_cassandra_dir(cassandra_version='1.1.0') 
            node.start(wait_other_notice=True) 
            time.sleep(.5)
{code}"
CASSANDRA-4179,"Add more general support for composites (to row key, column value)","Currently CQL3 have a nice syntax for using composites in the column name (it's more than that in fact, it creates a whole new abstraction but let's say I'm talking implementation here). There is however 2 other place where composites could be used (again implementation wise): the row key and the column value. This ticket proposes to explore which of those make sense for CQL3 and how.

For the row key, I really think that CQL support makes sense. It's very common (and useful) to want to stuff composite information in a row key. Sharding a time serie (CASSANDRA-4176) is probably the best example but there is other.

For the column value it is less clear. CQL3 makes it very transparent and convenient to store multiple related values into multiple columns so maybe composites in a column value is much less needed. I do still see two cases for which it could be handy:
# to save some disk/memory space, if you do know it makes no sense to insert/read two value separatly.
# if you want to enforce that two values should not be inserted separatly. I.e. to enforce a form of ""constraint"" to avoid programatic error.

Those are not widely useful things, but my reasoning is that if whatever syntax we come up for ""grouping"" row key in a composite trivially extends to column values, why not support it.


As for syntax I have 3 suggestions (that are just that, suggestions):
# If we only care about allowing grouping for row keys:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value text,
    PRIMARY KEY ((name, month), ts)
)
{noformat}
# A syntax that could work for both grouping in row key and colum value:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value1 text,
    value2 text,
    GROUP (name, month) as key,
    GROUP (value1, value2),
    PRIMARY KEY (key, ts)
)
{noformat}
# An alternative to the preceding one:
{noformat}
CREATE TABLE timeline (
    name text,
    month int,
    ts timestamp,
    value1 text,
    value2 text,
    GROUP (name, month) as key,
    GROUP (value1, value2),
    PRIMARY KEY (key, ts)
) WITH GROUP (name, month) AS key
   AND GROUP (value1, value2)
{noformat}"
CASSANDRA-4170,cql3 ALTER TABLE ALTER TYPE has no effect,"running the following with cql3:

{noformat}
CREATE TABLE test (foo text PRIMARY KEY, bar int);
ALTER TABLE test ALTER bar TYPE float;
{noformat}

does not actually change the column type of bar. It does under cql2.

Note that on the current cassandra-1.1.0 HEAD, this causes an NPE, fixed by CASSANDRA-4163. But even with that applied, the ALTER shown here has no effect."
CASSANDRA-4153,Optimize truncate when snapshots are disabled or keyspace not durable,"My goal is to make truncate to be less IO intensive so that my junit tests run faster (as already explained in CASSANDRA-3710). I think I have now a solution which does not change too much:

I created a patch that optimizes three things within truncate:
- Skip the whole Commitlog.forceNewSegment/discardCompletedSegments, if durable_writes are disabled for the keyspace.
- With CASSANDRA-3710 implemented, truncate does not need to flush memtables to disk when snapshots are disabled.
- Reduce the sleep interval

The patch works nicely for me. Applying it and disabling durable_writes/autoSnapshot increased the speed of my testsuite vastly. I hope I did not overlook something.

Let me know if my patch needs cleanup. I'd be glad to change it, if it means the patch will get accepted."
CASSANDRA-4142,OOM Exception during repair session with LeveledCompactionStrategy,"We encountered an OOM Exception on 2 nodes during repair session.
Our CF are set up to use LeveledCompactionStrategy and SnappyCompressor.
These two options used together maybe the key to the problem.

Despite of setting XX:+HeapDumpOnOutOfMemoryError, no dump have been generated.
Nonetheless a memory analysis on a live node doing a repair reveals an hotspot: an ArrayList of SSTableBoundedScanner which appears to contain as many objects as there are SSTables on disk. 
This ArrayList consumes 786 MB of the heap space for 5757 objects. Therefore each object is about 140 KB.

Eclipse Memory Analyzer's denominator tree shows that 99% of a SSTableBoundedScanner object's memory is consumed by a CompressedRandomAccessReader which contains two big byte arrays.

Cluster information:
9 nodes
Each node handles 35 GB (RandomPartitioner)

This JIRA was created following this discussion:
http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Why-so-many-SSTables-td7453033.html

"
CASSANDRA-4116,check most recent TS values in SSTables when a row tombstone has already been encountered,"once C* comes across a row tombstone, C* should check the TS on the tombstone against all SSTables.  If the most recent TS in an SST is older than the row tombstone, that entire SST (or the remainder of it) can be safely ignored.

There are two drivers for this.

* avoid checking column values that could not possibly be in the result set

* avoid OOMing because all the tombstones are temporarily kept in memory."
CASSANDRA-4111,Serializing cache can cause Segfault in 1.1,"Rare but this can happen per sure, looks like this issue is after CASSANDRA-3862 hence affectes only 1.1

        FreeableMemory old = map.get(key);
        if (old == null)
            return false;

        // see if the old value matches the one we want to replace
        FreeableMemory mem = serialize(value);
        if (mem == null)
            return false; // out of memory.  never mind.
        V oldValue = deserialize(old);
        boolean success = oldValue.equals(oldToReplace) && map.replace(key, old, mem);

        if (success)
            old.unreference();
        else
            mem.unreference();
        return success;

in the above code block we deserialize(old) without taking reference to the old memory, this can case seg faults when the old is reclaimed (free is called)
Fix is to get the reference just for deserialization

        V oldValue;
        // reference old guy before de-serializing
        old.reference();
        try
        {
             oldValue = deserialize(old);
        }
        finally
        {
            old.unreference();
        }"
CASSANDRA-4065,Bogus MemoryMeter liveRatio calculations,"I get strange cfs.liveRatios.

A couple of mem meter runs seem to calculate bogus results: 

{noformat}
Tue 09:14:48 dd@blnrzh045:~$ grep 'setting live ratio to maximum of 64 instead of' /var/log/cassandra/system.log
 WARN [MemoryMeter:1] 2012-03-20 08:08:07,253 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:09,160 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:13,274 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:08:22,032 Memtable.java (line 193) setting live ratio to maximum of 64 instead of Infinity
 WARN [MemoryMeter:1] 2012-03-20 08:12:41,057 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 67.11787351054079
 WARN [MemoryMeter:1] 2012-03-20 08:13:50,877 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 112.58547951925435
 WARN [MemoryMeter:1] 2012-03-20 08:15:29,021 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 193.36945063589877
 WARN [MemoryMeter:1] 2012-03-20 08:17:50,716 Memtable.java (line 193) setting live ratio to maximum of 64 instead of 348.45008340969434
{noformat}

Because meter runs never decrease liveRatio in Memtable (Which seems strange to me. If past calcs should be included for any reason wouldn't averaging make more sense?):

{noformat}
cfs.liveRatio = Math.max(cfs.liveRatio, newRatio);
{noformat}

Memtables are flushed every couple of secs:

{noformat}
ColumnFamilyStore.java (line 712) Enqueuing flush of Memtable-BlobStore@935814661(1874540/149963200 serialized/live bytes, 202 ops)
{noformat}

Even though a saner liveRatio has been calculated after the bogus runs:

{noformat}
INFO [MemoryMeter:1] 2012-03-20 08:19:55,934 Memtable.java (line 198) CFS(Keyspace='SmeetBlob', ColumnFamily='BlobStore') 
   liveRatio is 64.0 (just-counted was 2.97165811895841).  calculation took 124ms for 58 columns
{noformat}"
CASSANDRA-4056,[patch] guard against npe due to null sstable,SSTableIdentityIterator ctor can be called from sibling ctor with a null sstable. So catch block's markSuspect should be npe guarded.
CASSANDRA-4019,java.util.ConcurrentModificationException in Gossiper,"I have never seen this one before. Might be triggered by a race condition under heavy load. This error was triggered on 0.8.9


ERROR [GossipTasks:1] 2012-03-05 04:16:55,263 Gossiper.java (line 162) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayDeque$DeqIterator.next(ArrayDeque.java:605)
        at org.apache.cassandra.utils.AbstractStatsDeque.sum(AbstractStatsDeque.java:37)
        at org.apache.cassandra.utils.AbstractStatsDeque.mean(AbstractStatsDeque.java:60)
        at org.apache.cassandra.gms.ArrivalWindow.mean(FailureDetector.java:259)
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:282)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:155)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:538)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:157)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [GossipStage:1] 2012-03-05 04:16:55,263 Gossiper.java (line 737) Node /192.168.3.18 has restarted, now UP again
 INFO [GossipStage:1] 2012-03-05 04:16:55,264 Gossiper.java (line 705) InetAddress /192.168.3.18 is now UP
"
CASSANDRA-3991,Investigate importance of jsvc in debian packages,"jsvc seems to be buggy at best.  For instance, if you set a small heap like 128M it seems to completely ignore this and use as much memory as it wants.  I don't know what this is buying us over launching /usr/bin/cassandra directly like the redhat scripts do, but I've seen multiple complaints about its memory usage."
CASSANDRA-3989,nodetool cleanup/scrub/upgradesstables promotes all sstables to next level (LeveledCompaction),"1.0.7 + LeveledCompactionStrategy
If you run nodetool cleanup, scrub, or upgradesstables, Cassandra execute compaction for each sstable. During the compaction, it put the new sstable to next level of the original sstable. If you run cleanup many times, sstables will reached to the highest level, and CASSANDRA-3608 will happens at next cleanup.

Reproduce procedure:
# create column family CF1 with compaction_strategy=LeveledCompactionStrategy and compaction_strategy_options={sstable_size_in_mb: 5};
# Insert some data into CF1.
# nodetool flush
# Verify the sstable is created at L1 in CF1.json
# nodetool cleanup
# Verify sstable in L1 is removed and new sstable is created at L2 in CF1.json
# repeat nodetool cleanup some times"
CASSANDRA-3985,Ensure a directory is selected for Compaction,"From http://www.mail-archive.com/user@cassandra.apache.org/msg20757.html

CompactionTask.execute() checks if there is a valid compactionFileLocation only if partialCompactionsAcceptable() . upgradesstables results in a CompactionTask with userdefined set, so the valid location check is not performed. 

The result is a NPE, partial stack 

{code:java}
$ nodetool -h localhost upgradesstables
Error occured while upgrading the sstables for keyspace MyKeySpace
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
        at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
        at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:995)
        at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1648)
<snip>
Caused by: java.lang.NullPointerException
        at java.io.File.<init>(File.java:222)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:641)
        at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:652)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1888)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
{code}

(night time here, will fix tomorrow, anyone else feel free to fix it.)"
CASSANDRA-3961,Make index_interval configurable per column family,"After various experiments with mixing OLTP a OLAP workload running on single cassandra cluster i discovered that lot of memory is wasted on holding index samples for CF which are rarely accessed or index is not much used for CF access because slices over keys are used. 

There is per column family setting for configuring bloom filters - bloom_filter_fp_chance. Please add setting index_interval configurable per CF as well. If this setting is not set or it is zero, default from cassandra.yaml will be used."
CASSANDRA-3946,BulkRecordWriter shouldn't stream any empty data/index files that might be created at end of flush,"If by chance, we flush sstables during BulkRecordWriter (we have seen it happen), I want to make sure we don't try to stream them."
CASSANDRA-3939,occasional failure of CliTest,"{{CliTest}} will occasionally fail with an NPE.

{noformat}
[junit] Testcase: testCli(org.apache.cassandra.cli.CliTest):	Caused an ERROR
[junit] java.lang.NullPointerException
[junit] java.lang.RuntimeException: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1039)
[junit] 	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:228)
[junit] 	at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:213)
[junit] 	at org.apache.cassandra.cli.CliTest.testCli(CliTest.java:241)
[junit] Caused by: java.lang.NullPointerException
[junit] 	at org.apache.cassandra.cli.CliClient.validateSchemaIsSettled(CliClient.java:2855)
[junit] 	at org.apache.cassandra.cli.CliClient.executeAddColumnFamily(CliClient.java:1030)
{noformat}

This occurs because no default for {{schema_mwt}} is applied unless {{main()}} is invoked.

(Trivial )patch to follow."
CASSANDRA-3906,BulkRecordWriter throws NPE for counter columns,"Using BulkRecordWriter, fails with counters due to an NPE (we used column instead of counter_column). I also noticed this broke for super columns too."
CASSANDRA-3882,avoid distributed deadlock in migration stage,"This is follow-up work for the remainders of CASSANDRA-3832 which was only a partial fix. The deadlock in the migration stage needs to be fixed, as it can cause bootstrap (at least) to take potentially a very very long time to complete, and might also cause a lack of schema propagation until otherwise ""poked""."
CASSANDRA-3862,RowCache misses Updates,"While performing stress tests to find any race problems for CASSANDRA-2864 I guess I (re-)found one for the standard on-heap row cache.

During my stress test I hava lots of threads running with some of them only reading other writing and re-reading the value.

This seems to happen:

- Reader tries to read row A for the first time doing a getTopLevelColumns
- Row A which is not in the cache yet is updated by Writer. The row is not eagerly read during write (because we want fast writes) so the writer cannot perform a cache update
- Reader puts the row in the cache which is now missing the update

I already asked this some time ago on the mailing list but unfortunately didn't dig after I got no answer since I assumed that I just missed something. In a way I still do but haven't found any locking mechanism that makes sure that this should not happen.

The problem can be reproduced with every run of my stress test. When I restart the server the expected column is there. It's just missing from the cache.

To test I have created a patch that merges memtables with the row cache. With the patch the problem is gone.

I can also reproduce in 0.8. Haven't checked 1.1 but I haven't found any relevant change their either so I assume the same aplies there."
CASSANDRA-3849,Saved CF row cache breaks when upgrading to 1.1,"Enabled row and key caching. Used stress to insert some data. ran nodetool flush, then nodetool compact. Then read the data back to populate the cache. Turned row_cache_save_period and key_cache_save_period really low to force saving the cache data. I verified that the row and key cache files existed in /var/lib/cassandra/saved_caches/.

I then killed cassandra, checked out branch cassandra-1.1, compiled and tried to start the node. The node failed to start, and I got this error:
{code}
 INFO 01:33:30,893 reading saved cache /var/lib/cassandra/saved_caches/Keyspace1-Standard1-RowCache
ERROR 01:33:31,009 Exception encountered during startup
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.AssertionError: Row cache is not enabled on column family [Standard1]
	at org.apache.cassandra.db.ColumnFamilyStore.cacheRow(ColumnFamilyStore.java:1050)
	at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:383)
	at org.apache.cassandra.db.Table.open(Table.java:122)
	at org.apache.cassandra.db.Table.open(Table.java:100)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:204)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:353)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: Row cache is not enabled on column family [Standard1]
{code}"
CASSANDRA-3847,Pig should throw a useful error when the destination CF doesn't exist,"When trying to store data to nonexistent CF, no good error is returned.

Instead you get a message like:

{noformat}
[main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2042: Error in new logical plan. Try -Dpig.usenewlogicalplan=false.
{noformat}

Which, if you follow its advice, will eventually lead you to an NPE in initSchema."
CASSANDRA-3838,Repair Streaming hangs between multiple regions,"Streaming hangs between datacenters, though there might be multiple reasons for this, a simple fix will be to add the Socket timeout so the session can retry.

The following is the netstat of the affected node (the below output remains this way for a very long period).
[test_abrepairtest@test_abrepair--euwest1c-i-1adfb753 ~]$ nt netstats
Mode: NORMAL
Streaming to: /50.17.92.159
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1523325354/2475291786 - 61%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%



""Streaming:1"" daemon prio=10 tid=0x00002aaac2060800 nid=0x1676 runnable [0x000000006be85000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketOutputStream.socketWrite0(Native Method)
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at com.sun.net.ssl.internal.ssl.OutputRecord.writeBuffer(OutputRecord.java:297)
        at com.sun.net.ssl.internal.ssl.OutputRecord.write(OutputRecord.java:286)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:743)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:731)
        at com.sun.net.ssl.internal.ssl.AppOutputStream.write(AppOutputStream.java:59)
        - locked <0x00000006afea1bd8> (a com.sun.net.ssl.internal.ssl.AppOutputStream)
        at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
        at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
        at com.ning.compress.lzf.LZFOutputStream.flush(LZFOutputStream.java:117)
        at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:152)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Streaming from: /46.51.141.51
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2241-Data.db sections=7231 progress=0/1548922508 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2231-Data.db sections=4730 progress=0/296474156 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2244-Data.db sections=7650 progress=0/1580417610 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2217-Data.db sections=7682 progress=0/196689250 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2220-Data.db sections=7149 progress=0/478695185 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2171-Data.db sections=443 progress=0/78417320 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2235-Data.db sections=6631 progress=0/2270344837 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2222-Data.db sections=4590 progress=0/1310718798 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2233-Data.db sections=4581 progress=0/595026085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-g-2226-Data.db sections=7682 progress=0/2933920085 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2213-Data.db sections=7876 progress=0/3308781588 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2216-Data.db sections=7386 progress=0/2868167170 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2240-Data.db sections=7874 progress=0/587439833 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2254-Data.db sections=4618 progress=0/215989758 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2221-Data.db sections=7002 progress=1542191546/2475291786 - 62%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2239-Data.db sections=6266 progress=0/2190197091 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2210-Data.db sections=6698 progress=0/2304563183 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2230-Data.db sections=7662 progress=0/3082087770 - 0%
   abtests: /mnt/data/cassandra070/data/abtests/cust_allocs-hc-2229-Data.db sections=7386 progress=0/1324787539 - 0%


""Thread-198896"" prio=10 tid=0x00002aaac0e00800 nid=0x4710 runnable [0x000000004251b000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
        at com.sun.net.ssl.internal.ssl.InputRecord.readV3Record(InputRecord.java:405)
        at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:360)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:798)
        - locked <0x00000005e220a170> (a java.lang.Object)
        at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:755)
        at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
        - locked <0x00000005e220a1b8> (a com.sun.net.ssl.internal.ssl.AppInputStream)
        at com.ning.compress.lzf.LZFDecoder.readFully(LZFDecoder.java:392)
        at com.ning.compress.lzf.LZFDecoder.decompressChunk(LZFDecoder.java:190)
        at com.ning.compress.lzf.LZFInputStream.readyBuffer(LZFInputStream.java:254)
        at com.ning.compress.lzf.LZFInputStream.read(LZFInputStream.java:129)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at java.io.DataInputStream.readLong(DataInputStream.java:399)
        at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:115)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:119)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:37)
        at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:244)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:148)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:90)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:185)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
"
CASSANDRA-3823,[patch] remove bogus assert - never false,"code asserts that SSTableScanner is Closeable

             final SSTableScanner scanner = sstable.getScanner(filter);
             scanner.seekTo(startWith);
-            assert scanner instanceof Closeable; // otherwise we leak FDs

always is, unless null, but of course the line before would throw NPE. 

Just confusing.
"
CASSANDRA-3821,Counters in super columns don't preserve correct values after cluster restart,"Set up a 3-node cluster with rf=3. Create a counter super column family and increment a bunch of subcolumns 100 times each, with cf=QUORUM. Then wait a few second, restart the cluster, and read the values back. They almost all come back different (and higher) then they are supposed to be.

Here are some extra things I've noticed:
 - Reading back the values before the restart always produces correct results.
 - Doing a nodetool flush before killing the cluster greatly improves the results, though sometimes a value will still be incorrect. You might have to run the test several times to see an incorrect value after a flush.
 - This problem doesn't happen on C* 1.0.7, unless you don't sleep between doing the increments and killing the cluster. Then it sometimes happens to a lesser degree.

A dtest has been added to demonstrate this issue. It is called ""super_counter_test.py""."
CASSANDRA-3743,Lower memory consumption used by index sampling,"currently j.o.a.c.io.sstable.indexsummary is implemented as ArrayList of KeyPosition (RowPosition key, long offset)i propose to change it to:

RowPosition keys[]
long offsets[]

and use standard binary search on it. This will lower number of java objects used per entry from 2 (KeyPosition + RowPosition) to 1 (RowPosition).

For building these arrays convenient ArrayList class can be used and then call to .toArray() on it.

This is very important because index sampling uses a lot of memory on nodes with billions rows"
CASSANDRA-3712,Can't cleanup after I moved a token.,"Before cleanup failed, I moved one node's token.
My cluster had 10GB data on 2 nodes. Data repartition was bad, tokens were 165[...] and 155[...].
I moved 155 to 075[...], then adjusted to 076[...]. The moves were correctly processed, with no exception.
But then, when I wanted to cleanup, it failed and keeps failing, on both nodes.

Other maintenance procedures like repair, compact or scrub work.
All the data is in the URLs CF.

Example session log:
nodetool cleanup fails:
$ ./nodetool --host cnode1 cleanup
Error occured during cleanup
java.util.concurrent.ExecutionException: java.lang.AssertionError
 at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
 at java.util.concurrent.FutureTask.get(FutureTask.java:83)
 at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
 at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(CompactionManager.java:237)
 at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilyStore.java:958)
 at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1604)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
 at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
 at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
 at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
 at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
 at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
 at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
 at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
 at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
 at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
 at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
 at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
 at java.lang.reflect.Method.invoke(Method.java:597)
 at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
 at sun.rmi.transport.Transport$1.run(Transport.java:159)
 at java.security.AccessController.doPrivileged(Native Method)
 at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
 at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
 at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError
 at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
 at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
 at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
 at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
 at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
 at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
 at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
 at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 ... 3 more


The server log looks like this:
 INFO [CompactionExecutor:260] 2012-01-09 14:08:41,716 CompactionManager.java (line 702) Cleaning up SSTableReader(path='/ke/cassandra/data/kev3/URLs-hc-457-Data.db')
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,220 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 156787206 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:47,226 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:47,236 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@1347180703(16324791/156973287 serialized/live bytes, 173288 ops)
 INFO [pool-1-thread-1] 2012-01-09 14:08:51,003 Memtable.java (line 180) CFS(Keyspace='kev3', ColumnFamily='URLs.URLs_1_idx') liveRatio is 7.692510757866615 (just-counted was 4.512127842861816).  calculation took 8648ms for 97329 columns
 INFO [FlushWriter:23] 2012-01-09 14:08:54,360 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-143-Data.db (26375495 bytes)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:55,566 GCInspector.java (line 123) GC for ParNew: 206 ms for 1 collections, 934108624 used; max is 2034237440
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,289 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 188842513 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:08:57,297 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [FlushWriter:23] 2012-01-09 14:08:57,297 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@164871630(19662738/189069779 serialized/live bytes, 208494 ops)
 INFO [ScheduledTasks:1] 2012-01-09 14:08:57,619 GCInspector.java (line 123) GC for ParNew: 402 ms for 2 collections, 981893424 used; max is 2034237440
 INFO [FlushWriter:23] 2012-01-09 14:09:05,944 Memtable.java (line 277) Completed flushing /ke/cassandra/data/kev3/URLs.URLs_1_idx-hc-144-Data.db (31755390 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 MeteredFlusher.java (line 62) flushing high-traffic column family CFS(Keyspace='kev3', ColumnFamily='URLs') (estimated 174605041 bytes)
 INFO [OptionalTasks:1] 2012-01-09 14:09:06,447 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
 INFO [FlushWriter:23] 2012-01-09 14:09:06,447 Memtable.java (line 240) Writing Memtable-URLs.URLs_1_idx@284469330(18158445/174605041 serialized/live bytes, 192702 ops)
ERROR [CompactionExecutor:260] 2012-01-09 14:09:06,448 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:260,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.Memtable.put(Memtable.java:136)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:780)
	at org.apache.cassandra.db.index.keys.KeysIndex.deleteColumn(KeysIndex.java:82)
	at org.apache.cassandra.db.index.SecondaryIndexManager.deleteFromIndexes(SecondaryIndexManager.java:438)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:754)
	at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:241)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)




"
CASSANDRA-3690,Streaming CommitLog backup,"Problems with the current SST backups
1) The current backup doesn't allow us to restore point in time (within a SST)
2) Current SST implementation needs the backup to read from the filesystem and hence additional IO during the normal operational Disks
3) in 1.0 we have removed the flush interval and size when the flush will be triggered per CF, 
          For some use cases where there is less writes it becomes increasingly difficult to time it right.
4) Use cases which needs BI which are external (Non cassandra), needs the data in regular intervals than waiting for longer or unpredictable intervals.

Disadvantages of the new solution
1) Over head in processing the mutations during the recover phase.
2) More complicated solution than just copying the file to the archive.

Additional advantages:
Online and offline restore.
Close to live incremental backup.

Note: If the listener agent gets restarted, it is the agents responsibility to Stream the files missed or incomplete.

There are 3 Options in the initial implementation:
1) Backup -> Once a socket is connected we will switch the commit log and send new updates via the socket.
2) Stream -> will take the absolute path of the file and will read the file and send the updates via the socket.
3) Restore -> this will get the serialized bytes and apply's the mutation.

Side NOTE: (Not related to this patch as such) The agent which will take incremental backup is planned to be open sourced soon (Name: Priam)."
CASSANDRA-3681,Multiple threads can attempt hint handoff to the same target,"HintedHandOffManager attempts to prevent multiple threads sending hints to the same target with the queuedDeliveries set, but the code is buggy.  If two handoffs *do* occur concurrently, the second thread can use an arbitrarily large amount of memory skipping tombstones when it starts paging from the beginning of the hint row, looking for the first live hint.  (This is not a problem with a single thread, since it always pages starting with the last-seen hint column name, effectively skipping the tombstones.  Then it compacts when it's done.)

Technically this bug is present in all older Cassandra releases, but it only causes problems in 1.0.x since the hint rows tend to be much larger (since there is one hint per write containing the entire mutation, instead of just one per row consisting of just the key)."
CASSANDRA-3675,ship with -XX:+ExplicitGCInvokesConcurrent by default,"It's so much easier if you can safely tell people to trigger a full GC to discover their live set (see CASSANDRA-3574), instead of explaining the behavior of CMS and what the memory usage graph looks like etc etc. Shipping with {{-XX:+ExplicitGCInvokesConcurrent}} means this is by default safe.

For people that have special needs like some kind of rolling compacting GC with disablegossip, they are special enough that they can just change the VM options."
CASSANDRA-3655,NPE when running upgradesstables,"Running a test upgrade from 0.7(version f sstables) to 1.0.
upgradesstables runs for about 40 minutes and then NPE's when trying to retrieve a key.

No files have been succesfully upgraded. Likely related is that scrub (without having run upgrade) consumes all RAM and OOMs.

Possible theory is that a lot of paths call IPartitioner's decorateKey, and, at least in the randompartitioner's implementation, if any of those callers pass a null ByteBuffer, they key will be null in the stack trace below.


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:970)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1540)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:65)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:92)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:137)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
"
CASSANDRA-3624,Hinted Handoff - related OOM,"One of our nodes had collected alot of hints for another node, so when the dead node came back and the row mutations were read back from disk, the node died with an OOM-exception (and kept dying after restart, even with increased heap (from 8G to 12G)). The heap dump contained alot of SuperColumns and our application does not use those (but HH does). 

I'm guessing that each mutation is big so that PAGE_SIZE*<mutation_size> does not fit in memory (will check this tomorrow)

A simple fix (if my assumption above is correct) would be to reduce the PAGE_SIZE in HintedHandOffManager.java to something like 10 (or even 1?) to reduce the memory pressure. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every *mutation* anyway (not every page), thoughts?

If anyone runs in to the same problem, I got the node started again by simply removing the HintsColumnFamily* files."
CASSANDRA-3620,"Proposal for distributed deletes - fully automatic ""Reaper Model"" rather than GCSeconds and manual repairs","Proposal for an improved system for handling distributed deletes, which removes the requirement to regularly run repair processes to maintain performance and data integrity. 

h2. The Problem

There are various issues with repair:

* Repair is expensive to run
* Repair jobs are often made more expensive than they should be by other issues (nodes dropping requests, hinted handoff not working, downtime etc)
* Repair processes can often fail and need restarting, for example in cloud environments where network issues make a node disappear from the ring for a brief moment
* When you fail to run repair within GCSeconds, either by error or because of issues with Cassandra, data written to a node that did not see a later delete can reappear (and a node might miss a delete for several reasons including being down or simply dropping requests during load shedding)
* If you cannot run repair and have to increase GCSeconds to prevent deleted data reappearing, in some cases the growing tombstone overhead can significantly degrade performance

Because of the foregoing, in high throughput environments it can be very difficult to make repair a cron job. It can be preferable to keep a terminal open and run repair jobs one by one, making sure they succeed and keeping and eye on overall load to reduce system impact. This isn't desirable, and problems are exacerbated when there are lots of column families in a database or it is necessary to run a column family with a low GCSeconds to reduce tombstone load (because there are many write/deletes to that column family). The database owner must run repair within the GCSeconds window, or increase GCSeconds, to avoid potentially losing delete operations. 

It would be much better if there was no ongoing requirement to run repair to ensure deletes aren't lost, and no GCSeconds window. Ideally repair would be an optional maintenance utility used in special cases, or to ensure ONE reads get consistent data. 

h2. ""Reaper Model"" Proposal

# Tombstones do not expire, and there is no GCSeconds
# Tombstones have associated ACK lists, which record the replicas that have acknowledged them
# Tombstones are deleted (or marked for compaction) when they have been acknowledged by all replicas
# When a tombstone is deleted, it is added to a ""relic"" index. The relic index makes it possible for a reaper to acknowledge a tombstone after it is deleted
# The ACK lists and relic index are held in memory for speed
# Background ""reaper"" threads constantly stream ACK requests to other nodes, and stream back ACK responses back to requests they have received (throttling their usage of CPU and bandwidth so as not to affect performance)
# If a reaper receives a request to ACK a tombstone that does not exist, it creates the tombstone and adds an ACK for the requestor, and replies with an ACK. This is the worst that can happen, and does not cause data corruption. 

ADDENDUM

The proposal to hold the ACK and relic lists in memory was added after the first posting. Please see comments for full reasons. Furthermore, a proposal for enhancements to repair was posted to comments, which would cause tombstones to be scavenged when repair completes (the author had assumed this was the case anyway, but it seems at time of writing they are only scavenged during compaction on GCSeconds timeout). The proposals are not exclusive and this proposal is extended to include the possible enhancements to repair described.

NOTES

* If a node goes down for a prolonged period, the worst that can happen is that some tombstones are recreated across the cluster when it restarts, which does not corrupt data (and this will only occur with a very small number of tombstones)
* The system is simple to implement and predictable 
* With the reaper model, repair would become an optional process for optimizing the database to increase the consistency seen by ConsistencyLevel.ONE reads, and for fixing up nodes, for example after an sstable was lost

h3. Planned Benefits

* Reaper threads can utilize ""spare"" cycles to constantly scavenge tombstones in the background thereby greatly reducing tombstone load, improving query performance, reducing the system resources needed by processes such as compaction, and making performance generally more predictable 
* The reaper model means that GCSeconds is no longer necessary, which removes the threat of data corruption if repair can't be run successfully within that period (for example if repair can't be run because of a new adopter's lack of Cassandra expertise, a cron script failing, or Cassandra bugs or other technical issues)
* Reaper threads are fully automatic, work in the background and perform finely grained operations where interruption has little effect. This is much better for database administrators than having to manually run and manage repair, whether for the purposes of preventing data corruption or for optimizing performance, which in addition to wasting operator time also often creates load spikes and has to be restarted after failure.  "
CASSANDRA-3548,NPE in AntiEntropyService$RepairSession.completed(),"This may be related to CASSANDRA-3519 (cluster it was observed on is still 1.0.1), however i think there is still a race condition.

Observed on a 2 DC cluster, during a repair that spanned the DC's.  

{noformat}
INFO [AntiEntropyStage:1] 2011-11-28 06:22:56,225 StreamingRepairTask.java (line 136) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] Forwarding streaming repair of 8602 
ranges to /10.6.130.70 (to be streamed with /10.37.114.10)
...
 INFO [AntiEntropyStage:66] 2011-11-29 11:20:57,109 StreamingRepairTask.java (line 253) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] task succeeded
ERROR [AntiEntropyStage:66] 2011-11-29 11:20:57,109 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropyStage:66,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.completed(AntiEntropyService.java:712)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer$1.run(AntiEntropyService.java:912)
        at org.apache.cassandra.streaming.StreamingRepairTask$2.run(StreamingRepairTask.java:186)
        at org.apache.cassandra.streaming.StreamingRepairTask$StreamingRepairResponse.doVerb(StreamingRepairTask.java:255)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{noformat}

One of the nodes involved in the repair session failed, e.g. (Not sure if this is from the same repair session as the streaming task above, but it illustrates the issue)

{noformat}
ERROR [AntiEntropySessions:1] 2011-11-28 19:39:52,507 AntiEntropyService.java (line 688) [repair #2bf19860-197f-11e1-0000-5ff37d368cb6] session completed with the following error
java.io.IOException: Endpoint /10.29.60.10 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
ERROR [GossipTasks:1] 2011-11-28 19:39:52,507 StreamOutSession.java (line 232) StreamOutSession /10.29.60.10 failed because {} died or was restarted/removed
ERROR [GossipTasks:1] 2011-11-28 19:39:52,571 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:782)
        at java.util.ArrayList$Itr.next(ArrayList.java:754)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

{noformat}

When a node is marked as failed AntiEntropyService.RepairSession.forceShutdown() clears the activejobs map. But the jobs to other nodes will continue, and will eventually call completed(). 

RepairSession.terminated should stop completed() from checking the map, but there is a race between the map been cleared and if there is an error in finally block it wont be set. 
"
CASSANDRA-3547,Race between cf flush and  its secondary indexes flush,"When a CF with indexes is flushed, it's indexes are flushed too. In particular their memtable is switched, but without making the old memtable frozen. This can conflict with a concurrent flush of the index itself, as reported on the user list by Michael Vaknine:
{noformat}
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 java.lang.AssertionError
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:671)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:745)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:750)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.keys.KeysIndex.forceBlockingFlush(KeysIndex.java:119)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:258)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.maybeBuildSecondaryIndexes(SecondaryIndexManager.java:123)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:151)
{noformat}"
CASSANDRA-3544,NPE on startup when there are permissions issues with directories,"If the directories used by cassandra for data, commitlog, and saved caches aren't readable due to permissions, you get an NPE on startup.  In particular, if none of them are readable, you'll see something like this:

{noformat}
ERROR 14:50:11,945 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: null
{noformat}

This traceback happens when the saved_caches directory isn't readable, but you can get different ones if only the data or commitlog directories aren't readable.

We should check the permissions of these directories before trying to list their contents."
CASSANDRA-3537,ExpiringMap timer is not exception-proof,"I have 4 cassandra nodes ,and I put about 30G data to db for every nodes . It's just 4 days before I start the cluster ,but now every 4 nodes have the same problem ,JVM heap is full  ,and  GC take no effect ,There must be some memory leak . Jmap the memory as follow:

Object Histogram:

num 	  #instances	#bytes	Class description
--------------------------------------------------------------------------
1:		15793606	758093088	java.nio.HeapByteBuffer
2:		2153811	320138208	java.lang.Object[]
3:		6163192	197222144	org.apache.cassandra.db.Column
4:		2543836	175890256	int[]
5:		2168816	155397192	long[]
6:		2078123	116374888	org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
7:		1847111	73884440	java.math.BigInteger
8:		1234243	59243664	java.util.Hashtable
9:		1770829	58233000	char[]
10:		1770627	56660064	java.lang.String
11:		1665886	39981264	org.apache.cassandra.db.DecoratedKey
12:		692706	38791536	org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
13:		1234274	37172088	java.util.Hashtable$Entry[]
14:		1133541	36273312	java.net.Inet4Address
15:		738528	35449344	org.apache.cassandra.service.ReadCallback
16:		2078118	33249888	org.cliffc.high_scale_lib.Counter
17:		1373886	32973264	org.apache.cassandra.db.ReadResponse
18:		1234023	29616552	org.apache.cassandra.net.Message
19:		1234019	29616456	org.apache.cassandra.net.Header
20:		1846185	29538960	org.apache.cassandra.dht.BigIntegerToken
21:		891378	28524096	org.apache.cassandra.utils.ExpiringMap$CacheableObject
22:		692706	27708240	org.cliffc.high_scale_lib.NonBlockingHashMap
23:		1148252	27558048	java.util.Collections$SynchronizedSet
24:		541977	26014896	org.apache.cassandra.db.SliceFromReadCommand
25:		998001	23952024	java.util.concurrent.ConcurrentSkipListMap$Node
26:		928792	22291008	java.util.ArrayList
27:		692715	22166880	java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
28:		891378	21393072	org.apache.cassandra.net.CallbackInfo
29:		1148247	18371952	java.util.Hashtable$KeySet
30:		731859	17564616	org.apache.cassandra.db.Row
31:		529991	16959712	org.apache.cassandra.db.ArrayBackedSortedColumns
32:		691425	16594200	org.apache.cassandra.db.AbstractColumnContainer$DeletionInfo
33:		648580	15565920	org.apache.cassandra.db.filter.QueryPath
34:		648338	15560112	org.apache.cassandra.service.RowDigestResolver
35:		971376	15542016	java.util.concurrent.atomic.AtomicInteger
36:		837418	13398688	org.apache.cassandra.utils.SimpleCondition
37:		535614	12854736	org.apache.cassandra.db.ColumnFamily
38:		725634	11610144	java.util.concurrent.atomic.AtomicReference
39:		195117	9365616	org.apache.cassandra.db.ThreadSafeSortedColumns
40:		281921	9021472	java.util.concurrent.ConcurrentSkipListMap$HeadIndex
41:		277679	8885728	java.util.concurrent.locks.ReentrantLock$NonfairSync
42:		314424	7546176	java.util.concurrent.ConcurrentSkipListMap$Index
43:		275186	6604464	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
44:		270280	6486720	java.util.concurrent.LinkedBlockingQueue$Node
45:		219553	5269272	org.apache.cassandra.io.sstable.IndexSummary$KeyPosition
46:		106436	5108928	java.util.TreeMap
47:		122185	4887400	org.apache.cassandra.db.ExpiringColumn
48:		189968	4559232	org.apache.cassandra.db.SuperColumn
49:		275659	4410544	java.util.concurrent.locks.ReentrantLock
50:		90213	4330224	java.util.concurrent.LinkedBlockingQueue
51:		107026	4281040	java.util.TreeMap$Entry
52:		30501	4222056	* ConstMethodKlass"
CASSANDRA-3536,Assertion error during bootstraping cassandra," I have a 3 node cassandra cluster. I have RF set to 3 and do reads
and writes using QUORUM.

Here is my initial ring configuration

[root@CAP4-CNode1 ~]# /root/cassandra/bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load
Owns    Token

       113427455640312821154458202477256070484
10.19.104.11    datacenter1 rack1       Up     Normal  1.66 GB
33.33%  0
10.19.104.12    datacenter1 rack1       Up     Normal  1.06 GB
33.33%  56713727820156410577229101238628035242
10.19.104.13    datacenter1 rack1       Up     Normal  1.61 GB
33.33%  113427455640312821154458202477256070484

I want to add 10.19.104.14 to the cluster.

I edited the 10.19.104.14 cassandra.yaml file and set the token to
127605887595351923798765477786913079296 and set auto_bootstrap to
true.

When I started cassandra I am getting Assertion Error.  

thanks
Ramesh




[root@CAP4-CNode4 cassandra]#  INFO 10:29:46,093 Logging initialized
 INFO 10:29:46,099 JVM vendor/version: Java HotSpot(TM) 64-Bit Server
VM/1.6.0_25
 INFO 10:29:46,100 Heap size: 8304721920/8304721920
 INFO 10:29:46,100 Classpath:
bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/apache-cassandra-1.0.2.jar:bin/../lib/apache-cassandra-clientutil-1.0.2.jar:bin/../lib/apache-cassandra-thrift-1.0.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/mx4j-examples.jar:bin/../lib/mx4j-impl.jar:bin/../lib/mx4j.jar:bin/../lib/mx4j-jmx.jar:bin/../lib/mx4j-remote.jar:bin/../lib/mx4j-rimpl.jar:bin/../lib/mx4j-rjmx.jar:bin/../lib/mx4j-tools.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar
 INFO 10:29:48,713 JNA mlockall successful
 INFO 10:29:48,726 Loading settings from
file:/root/apache-cassandra-1.0.2/conf/cassandra.yaml
 INFO 10:29:48,883 DiskAccessMode 'auto' determined to be mmap,
indexAccessMode is mmap
 INFO 10:29:48,898 Global memtable threshold is enabled at 2640MB
 INFO 10:29:49,203 Couldn't detect any schema definitions in local storage.
 INFO 10:29:49,204 Found table data in data directories. Consider
using the CLI to define your schema.
 INFO 10:29:49,220 Creating new commitlog segment
/var/lib/cassandra/commitlog/CommitLog-1321979389220.log
 INFO 10:29:49,227 No commitlog files found; skipping replay
 INFO 10:29:49,230 Cassandra version: 1.0.2
 INFO 10:29:49,230 Thrift API version: 19.18.0
 INFO 10:29:49,230 Loading persisted ring state
 INFO 10:29:49,235 Starting up server gossip
 INFO 10:29:49,259 Enqueuing flush of
Memtable-LocationInfo@122130810(192/240 serialized/live bytes, 4 ops)
 INFO 10:29:49,260 Writing Memtable-LocationInfo@122130810(192/240
serialized/live bytes, 4 ops)
 INFO 10:29:49,317 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db (300 bytes)
 INFO 10:29:49,340 Starting Messaging Service on port 7000
 INFO 10:29:49,349 JOINING: waiting for ring and schema information
 INFO 10:29:50,759 Applying migration
4b0e20f0-1511-11e1-0000-c11bc95834d7 Add keyspace: MSA, rep
strategy:SimpleStrategy{}, durable_writes: true
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Migrations@1507565381(6744/8430 serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Writing Memtable-Migrations@1507565381(6744/8430
serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Schema@1498835564(2889/3611 serialized/live bytes, 3 ops)
 INFO 10:29:50,776 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-1-Data.db (6808 bytes)
 INFO 10:29:50,777 Writing Memtable-Schema@1498835564(2889/3611
serialized/live bytes, 3 ops)
 INFO 10:29:50,797 Completed flushing
/var/lib/cassandra/data/system/Schema-h-1-Data.db (3039 bytes)
 INFO 10:29:50,814 Applying migration
4b6f2cb0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1639d811[cfId=1000,ksName=MSA,cfName=modseq,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2f984f7d,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,815 Enqueuing flush of
Memtable-Migrations@948613108(7482/9352 serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Writing Memtable-Migrations@948613108(7482/9352
serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Enqueuing flush of
Memtable-Schema@421910828(3294/4117 serialized/live bytes, 3 ops)
 INFO 10:29:50,831 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-2-Data.db (7546 bytes)
 INFO 10:29:50,832 Writing Memtable-Schema@421910828(3294/4117
serialized/live bytes, 3 ops)
 INFO 10:29:50,846 Completed flushing
/var/lib/cassandra/data/system/Schema-h-2-Data.db (3444 bytes)
 INFO 10:29:50,854 Applying migration
4b8c9fc0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1bd97d0d[cfId=1001,ksName=MSA,cfName=msgid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@63a0eec3,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,855 Enqueuing flush of
Memtable-Migrations@1520138062(7750/9687 serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Writing Memtable-Migrations@1520138062(7750/9687
serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Enqueuing flush of
Memtable-Schema@347459675(3630/4537 serialized/live bytes, 3 ops)
 INFO 10:29:50,878 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-3-Data.db (7814 bytes)
 INFO 10:29:50,879 Writing Memtable-Schema@347459675(3630/4537
serialized/live bytes, 3 ops)
 INFO 10:29:50,894 Completed flushing
/var/lib/cassandra/data/system/Schema-h-3-Data.db (3780 bytes)
 INFO 10:29:50,900 Applying migration
4ba1ae60-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@6a095b8a[cfId=1002,ksName=MSA,cfName=participants,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@c58f769,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,900 Enqueuing flush of
Memtable-Migrations@618337492(8194/10242 serialized/live bytes, 1 ops)
 INFO 10:29:50,901 Writing Memtable-Migrations@618337492(8194/10242
serialized/live bytes, 1 ops)
 INFO 10:29:50,902 Enqueuing flush of
Memtable-Schema@724860211(4020/5025 serialized/live bytes, 3 ops)
 INFO 10:29:50,917 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-4-Data.db (8258 bytes)
 INFO 10:29:50,918 Writing Memtable-Schema@724860211(4020/5025
serialized/live bytes, 3 ops)
 INFO 10:29:50,925 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-3-Data.db')]
 INFO 10:29:50,934 Completed flushing
/var/lib/cassandra/data/system/Schema-h-4-Data.db (4170 bytes)
 INFO 10:29:50,935 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-3-Data.db')]
 INFO 10:29:50,940 Applying migration
4bb4e840-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@318c69a9[cfId=1003,ksName=MSA,cfName=subinfo,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=5000.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=14400,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@796cefa8,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Migrations@1682081063(8618/10772 serialized/live bytes, 1
ops)
 INFO 10:29:50,941 Writing Memtable-Migrations@1682081063(8618/10772
serialized/live bytes, 1 ops)
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Schema@1083461053(4427/5533 serialized/live bytes, 3 ops)
 INFO 10:29:50,977 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-5-Data.db (8682 bytes)
 INFO 10:29:50,978 Writing Memtable-Schema@1083461053(4427/5533
serialized/live bytes, 3 ops)
 INFO 10:29:50,991 Compacted to
[/var/lib/cassandra/data/system/Schema-h-5-Data.db,].  14,433 to
14,106 (~97% of original) bytes for 5 keys at 0.269051MB/s.  Time:
50ms.
 INFO 10:29:50,995 Completed flushing
/var/lib/cassandra/data/system/Schema-h-7-Data.db (4577 bytes)
 INFO 10:29:51,000 Applying migration
4bc6e9a0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@20b00ec2[cfId=1004,ksName=MSA,cfName=transactions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@698f352,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,001 Enqueuing flush of
Memtable-Migrations@596545504(9027/11283 serialized/live bytes, 1 ops)
 INFO 10:29:51,002 Writing Memtable-Migrations@596545504(9027/11283
serialized/live bytes, 1 ops)
 INFO 10:29:51,003 Enqueuing flush of
Memtable-Schema@1686621532(4835/6043 serialized/live bytes, 3 ops)
 INFO 10:29:51,029 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-7-Data.db (9091 bytes)
 INFO 10:29:51,029 Writing Memtable-Schema@1686621532(4835/6043
serialized/live bytes, 3 ops)
 INFO 10:29:51,031 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-6-Data.db,].  30,426 to
30,234 (~99% of original) bytes for 1 keys at 0.272013MB/s.  Time:
106ms.
 INFO 10:29:51,044 Completed flushing
/var/lib/cassandra/data/system/Schema-h-8-Data.db (4985 bytes)
 INFO 10:29:51,049 Applying migration
4bd76460-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@4ab4faeb[cfId=1005,ksName=MSA,cfName=uid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1500000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2fc5809e,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,050 Enqueuing flush of
Memtable-Migrations@1333730706(9421/11776 serialized/live bytes, 1
ops)
 INFO 10:29:51,050 Writing Memtable-Migrations@1333730706(9421/11776
serialized/live bytes, 1 ops)
 INFO 10:29:51,051 Enqueuing flush of
Memtable-Schema@577668356(5236/6545 serialized/live bytes, 3 ops)
 INFO 10:29:51,065 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-9-Data.db (9485 bytes)
 INFO 10:29:51,066 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-6-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-7-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-5-Data.db')]
 INFO 10:29:51,066 Writing Memtable-Schema@577668356(5236/6545
serialized/live bytes, 3 ops)
 INFO 10:29:51,081 Completed flushing
/var/lib/cassandra/data/system/Schema-h-9-Data.db (5386 bytes)
 INFO 10:29:51,083 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-5-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-8-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-7-Data.db')]
 INFO 10:29:51,114 Compacted to
[/var/lib/cassandra/data/system/Schema-h-10-Data.db,].  29,054 to
28,727 (~98% of original) bytes for 8 keys at 0.913207MB/s.  Time:
30ms.
 INFO 10:29:51,144 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-10-Data.db,].  57,492 to
57,300 (~99% of original) bytes for 1 keys at 0.700584MB/s.  Time:
78ms.
 INFO 10:29:51,410 Node /10.19.104.13 is now part of the cluster
 INFO 10:29:51,412 InetAddress /10.19.104.13 is now UP
 INFO 10:29:51,414 Enqueuing flush of
Memtable-LocationInfo@709342045(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,415 Writing Memtable-LocationInfo@709342045(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,428 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db (89 bytes)
 INFO 10:29:51,439 Node /10.19.104.12 is now part of the cluster
 INFO 10:29:51,439 InetAddress /10.19.104.12 is now UP
 INFO 10:29:51,441 Enqueuing flush of
Memtable-LocationInfo@1292444743(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,441 Writing Memtable-LocationInfo@1292444743(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,455 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db (89 bytes)
 INFO 10:29:51,456 Node /10.19.104.11 is now part of the cluster
 INFO 10:29:51,457 InetAddress /10.19.104.11 is now UP
 INFO 10:29:51,459 Enqueuing flush of
Memtable-LocationInfo@1891328597(20/25 serialized/live bytes, 1 ops)
 INFO 10:29:51,459 Writing Memtable-LocationInfo@1891328597(20/25
serialized/live bytes, 1 ops)
 INFO 10:29:51,471 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db (74 bytes)
 INFO 10:29:51,473 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db')]
 INFO 10:29:51,497 Compacted to
[/var/lib/cassandra/data/system/LocationInfo-h-5-Data.db,].  552 to
444 (~80% of original) bytes for 3 keys at 0.018410MB/s.  Time: 23ms.
 INFO 10:30:19,349 JOINING: getting bootstrap token
 INFO 10:30:19,352 Enqueuing flush of
Memtable-LocationInfo@225265367(36/45 serialized/live bytes, 1 ops)
 INFO 10:30:19,353 Writing Memtable-LocationInfo@225265367(36/45
serialized/live bytes, 1 ops)
 INFO 10:30:19,364 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-7-Data.db (87 bytes)
 INFO 10:30:19,374 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:30:49,375 JOINING: Starting to bootstrap...
ERROR 10:31:13,444 Fatal exception in thread Thread[Thread-49,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
       at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
       at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
       at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:922)
       at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
       at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:102)
       at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
       at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)"
CASSANDRA-3521,sstableloader throwing exceptions when loading snapshot data from compressed CFs,"Loaded data from snapshot then enabled  `sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor`
Then flush, scrub and compact. I can see actual CompressionRatio in JMX Console and access my data without problems..

Now I snapshot compressed keyspace and when trying to load snapshot to another single node or different Keyspace (the same super CF structure with compression options enabled, even try to truncate snapshoted CFs.) I cant retrieve any records . 


sstableloader command with debug mode dont throw any errors and shows its streaming 

{quote}
sstableloader-cassandra_2/bin/sstableloader --debug Impressions_compressed/
{quote}


Node logs contains repeating the errors bellow.

{quote}
ERROR [Thread-319] 2011-11-22 09:56:01,931 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-319,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-260-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
 INFO [Thread-320] 2011-11-22 09:56:02,492 StreamInSession.java (line 120) Streaming of file Impressions_compressed/HidSaid-hb-9-Data.db sections=1 progress=0/5616749 - 0% from org.apache.cassandra.streaming.StreamInSession@3cc62c07 failed: requesting a retry.
ERROR [Thread-320] 2011-11-22 09:56:02,493 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-320,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-261-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
{quote}

Hope its enough if you need more info just tell me what you need to reproduce this bug."
CASSANDRA-3497,BloomFilter FP ratio should be configurable or size-restricted some other way,"When you have a live dc and purely analytical dc, in many situations you can have less nodes on the analytical side, but end up getting restricted by having the BloomFilters in-memory, even though you have absolutely no use for them.  It would be nice if you could reduce this memory requirement by tuning the desired FP ratio, or even just disabling them altogether."
CASSANDRA-3495,capture BloomFilter memory size in Cassandra (JMX),"Maybe this could be done in https://issues.apache.org/jira/browse/CASSANDRA-3347 also, but not sure what's the scope in that jira. It'd be great if the BF memory size can be captured in the JMX monitoring. 

Though not sure how you would capture the ""heap size"" (easily that is) of the object. There is a BF.serializedSize() , can this be exposed to the BloomFilterTracker and DataTracker... and use this instead? Anyway, will let the implementor to decide/design "
CASSANDRA-3489,EncryptionOptions should be instantiated,"As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix."
CASSANDRA-3446,Problem SliceByNamesReadCommand on super column family after flush operation,"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: ....."
CASSANDRA-3432,Avoid large array allocation for compressed chunk offsets,"For each compressed file we keep the chunk offsets in memory (a long[]). The size of this array is directly proportional to the sstable file and the chunk_length_kb used, but say for a 64GB sstable, we're talking ~8MB in memory by default.

Without being absolutely huge, this probably makes the life of the GC harder than necessary for the same reasons than CASSANDRA-2466, and this ticket proposes the same solution, i.e. to break down those big array into smaller ones to ease fragmentation.

Note that this is only a concern for size tiered compaction. But until leveled compaction is battle tested, the default and we know nobody uses size tiered anymore, it's probably worth making the optimization."
CASSANDRA-3427,"CompressionMetadata is not shared across threads, we create a new one for each read","The CompressionMetada holds the compressed block offsets in memory. Without being absolutely huge, this is still of non-negligible size as soon as you have a bit of data in the DB. Reallocating this for each read is a very bad idea.

Note that this only affect range queries, since ""normal"" queries uses CompressedSegmentedFile that does reuse a unique CompressionMetadata instance.

( Background: http://thread.gmane.org/gmane.comp.db.cassandra.user/21362 )"
CASSANDRA-3417,InvocationTargetException ConcurrentModificationException at startup,"I was starting up the new DataStax AMI where the seed starts first and 34 nodes would latch on together. So far things have been working decently for launching, but right now I just got this during startup.


{CODE}
ubuntu@ip-10-40-190-143:~$ sudo cat /var/log/cassandra/output.log 
 INFO 09:24:38,453 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:24:38,456 Heap size: 1936719872/1937768448
 INFO 09:24:38,457 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:24:39,891 JNA mlockall successful
 INFO 09:24:39,901 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:24:40,057 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:24:40,069 Global memtable threshold is enabled at 616MB
 INFO 09:24:40,159 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:24:40,475 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:24:40,486 Couldn't detect any schema definitions in local storage.
 INFO 09:24:40,486 Found table data in data directories. Consider using the CLI to define your schema.
 INFO 09:24:40,497 No commitlog files found; skipping replay
 INFO 09:24:40,501 Cassandra version: 1.0.0
 INFO 09:24:40,502 Thrift API version: 19.18.0
 INFO 09:24:40,502 Loading persisted ring state
 INFO 09:24:40,506 Starting up server gossip
 INFO 09:24:40,529 Enqueuing flush of Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,530 Writing Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,600 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-1-Data.db (298 bytes)
 INFO 09:24:40,613 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:24:40,621 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:24:40,628 Joining: waiting for ring and schema information
 INFO 09:24:43,389 InetAddress /10.194.29.156 is now dead.
 INFO 09:24:43,391 InetAddress /10.85.11.38 is now dead.
 INFO 09:24:43,392 InetAddress /10.34.42.28 is now dead.
 INFO 09:24:43,393 InetAddress /10.77.63.49 is now dead.
 INFO 09:24:43,394 InetAddress /10.194.22.191 is now dead.
 INFO 09:24:43,395 InetAddress /10.34.74.58 is now dead.
 INFO 09:24:43,395 Node /10.34.33.16 is now part of the cluster
 INFO 09:24:43,396 InetAddress /10.34.33.16 is now UP
 INFO 09:24:43,397 Enqueuing flush of Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,398 Writing Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,417 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-2-Data.db (74 bytes)
 INFO 09:24:43,418 InetAddress /10.202.67.43 is now dead.
 INFO 09:24:43,419 InetAddress /10.116.215.81 is now dead.
 INFO 09:24:43,420 InetAddress /10.99.39.242 is now dead.
 INFO 09:24:43,421 InetAddress /10.80.110.28 is now dead.
 INFO 09:24:43,422 InetAddress /10.118.233.198 is now dead.
 INFO 09:24:43,423 InetAddress /10.40.177.173 is now dead.
 INFO 09:24:43,424 InetAddress /10.205.23.34 is now dead.
 INFO 09:24:43,425 InetAddress /10.101.41.8 is now dead.
 INFO 09:24:43,669 InetAddress /10.118.230.219 is now dead.
 INFO 09:24:43,670 InetAddress /10.80.41.192 is now dead.
 INFO 09:24:43,671 InetAddress /10.40.22.224 is now dead.
 INFO 09:24:43,672 InetAddress /10.39.107.114 is now dead.
 INFO 09:24:46,164 InetAddress /10.118.185.68 is now dead.
 INFO 09:24:46,166 InetAddress /10.84.205.93 is now dead.
 INFO 09:24:46,167 InetAddress /10.116.134.183 is now dead.
 INFO 09:24:46,670 InetAddress /10.118.179.67 is now dead.
 INFO 09:24:46,671 InetAddress /10.116.241.250 is now dead.
 INFO 09:24:48,441 InetAddress /10.118.94.62 is now dead.
 INFO 09:24:48,442 InetAddress /10.99.86.251 is now dead.
 INFO 09:24:50,176 InetAddress /10.113.42.21 is now dead.
 INFO 09:24:50,177 InetAddress /10.34.159.72 is now dead.
 INFO 09:24:50,178 InetAddress /10.32.79.134 is now dead.
 INFO 09:24:50,179 InetAddress /10.80.210.38 is now dead.
 INFO 09:24:50,180 InetAddress /10.34.70.73 is now dead.
 INFO 09:24:50,181 InetAddress /10.196.79.240 is now dead.
 INFO 09:25:01,713 InetAddress /10.82.210.172 is now dead.
 INFO 09:25:06,202 InetAddress /10.80.110.28 is now UP
 INFO 09:25:06,908 InetAddress /10.99.39.242 is now UP
 INFO 09:25:07,696 InetAddress /10.118.233.198 is now UP
 INFO 09:25:07,697 InetAddress /10.205.23.34 is now UP
 INFO 09:25:08,704 InetAddress /10.194.22.191 is now UP
 INFO 09:25:08,705 InetAddress /10.40.177.173 is now UP
 INFO 09:25:08,706 InetAddress /10.101.41.8 is now UP
 INFO 09:25:09,489 InetAddress /10.202.67.43 is now UP
 INFO 09:25:09,698 InetAddress /10.77.63.49 is now UP
 INFO 09:25:10,628 Joining: getting bootstrap token
 INFO 09:25:10,631 Enqueuing flush of Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,631 Writing Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,647 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-3-Data.db (87 bytes)
 INFO 09:25:10,649 Joining: sleeping 30000 ms for pending range setup
 INFO 09:25:10,689 InetAddress /10.85.11.38 is now UP
 INFO 09:25:10,708 InetAddress /10.34.74.58 is now UP
 INFO 09:25:10,912 InetAddress /10.194.29.156 is now UP
 INFO 09:25:11,261 Applying migration bb843dd0-0146-11e1-0000-b877c09da5ff Add keyspace: OpsCenter, rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@55e29b99[cfId=1000,ksName=OpsCenter,cfName=pdps,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=300.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@105585dc,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ec736e4[cfId=1004,ksName=OpsCenter,cfName=rollups86400,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@68e4e358,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@b09dc35[cfId=1003,ksName=OpsCenter,cfName=rollups7200,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@3458213c,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ee04fd[cfId=1002,ksName=OpsCenter,cfName=rollups300,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=16,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@4d898115,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e79b177[cfId=1005,ksName=OpsCenter,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter raw event storage,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@67723c7f,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@540523be[cfId=1006,ksName=OpsCenter,cfName=events_timeline,cfType=Standard,comparator=org.apache.cassandra.db.marshal.LongType,subcolumncomparator=<null>,comment=OpsCenter event timelines,rowCacheSize=0.0,keyCacheSize=5.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@1d6dba0a,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@ed0f59e[cfId=1007,ksName=OpsCenter,cfName=settings,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter settings,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@38ad5fab,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e63f09e[cfId=1001,ksName=OpsCenter,cfName=rollups60,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@534a55e5,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]}, durable_writes: true
 INFO 09:25:11,273 Enqueuing flush of Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,273 Writing Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,274 Enqueuing flush of Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,358 Completed flushing /raid0/cassandra/data/system/Migrations-h-1-Data.db (12989 bytes)
 INFO 09:25:11,358 Writing Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,390 Completed flushing /raid0/cassandra/data/system/Schema-h-1-Data.db (5970 bytes)
 INFO 09:25:11,727 InetAddress /10.116.215.81 is now UP
 INFO 09:25:11,744 InetAddress /10.34.42.28 is now UP
 INFO 09:25:11,750 InetAddress /10.40.22.224 is now UP
 INFO 09:25:12,023 InetAddress /10.80.41.192 is now UP
 INFO 09:25:12,712 InetAddress /10.39.107.114 is now UP
 INFO 09:25:12,717 InetAddress /10.118.185.68 is now UP
 INFO 09:25:12,721 InetAddress /10.116.134.183 is now UP
 INFO 09:25:13,322 InetAddress /10.118.230.219 is now UP
 INFO 09:25:13,632 InetAddress /10.84.205.93 is now UP
 INFO 09:25:14,713 InetAddress /10.118.179.67 is now UP
 INFO 09:25:14,717 InetAddress /10.116.241.250 is now UP
 INFO 09:25:17,468 InetAddress /10.34.159.72 is now UP
 INFO 09:25:17,476 InetAddress /10.118.94.62 is now UP
 INFO 09:25:17,480 InetAddress /10.80.210.38 is now UP
 INFO 09:25:17,716 InetAddress /10.32.79.134 is now UP
 INFO 09:25:17,721 InetAddress /10.99.86.251 is now UP
 INFO 09:25:18,717 InetAddress /10.196.79.240 is now UP
 INFO 09:25:18,727 InetAddress /10.34.70.73 is now UP
 INFO 09:25:19,596 InetAddress /10.113.42.21 is now UP
 INFO 09:25:25,750 InetAddress /10.82.210.172 is now UP
 INFO 09:25:37,743 Enqueuing flush of Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,744 Writing Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,764 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-4-Data.db (89 bytes)
 INFO 09:25:37,773 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-1-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-3-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-4-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-2-Data.db')]
 INFO 09:25:37,776 Enqueuing flush of Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,777 Writing Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,821 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-5-Data.db (89 bytes)
 INFO 09:25:37,869 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-6-Data.db,].  548 to 443 (~80% of original) bytes for 3 keys at 0.006500MB/s.  Time: 65ms.
 INFO 09:25:38,740 Enqueuing flush of Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,740 Writing Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,757 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-8-Data.db (89 bytes)
 INFO 09:25:38,766 Enqueuing flush of Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,767 Writing Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,814 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-9-Data.db (89 bytes)
 INFO 09:25:38,816 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-6-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-9-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-8-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-5-Data.db')]
 INFO 09:25:38,823 Enqueuing flush of Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,823 Writing Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,893 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-10-Data.db (89 bytes)
 INFO 09:25:38,916 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-11-Data.db,].  710 to 548 (~77% of original) bytes for 3 keys at 0.005226MB/s.  Time: 100ms.
 INFO 09:25:39,538 Enqueuing flush of Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,539 Writing Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,555 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-13-Data.db (89 bytes)
 INFO 09:25:39,578 Enqueuing flush of Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,578 Writing Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,594 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-14-Data.db (89 bytes)
 INFO 09:25:39,596 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-11-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-10-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-14-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-13-Data.db')]
 INFO 09:25:39,613 Enqueuing flush of Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,614 Writing Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,652 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-16-Data.db (89 bytes)
 INFO 09:25:39,692 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-15-Data.db,].  815 to 653 (~80% of original) bytes for 3 keys at 0.006487MB/s.  Time: 96ms.
 INFO 09:25:39,731 Enqueuing flush of Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,731 Writing Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-18-Data.db (89 bytes)
 INFO 09:25:40,649 Starting to bootstrap...
 INFO 09:25:40,701 Finished streaming session 304272969286 from /10.205.23.34
 INFO 09:25:40,703 Enqueuing flush of Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,703 Writing Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,721 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-19-Data.db (163 bytes)
 INFO 09:25:40,722 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db')]
 INFO 09:25:40,726 Node /10.40.190.143 state jump to normal
 INFO 09:25:40,734 Enqueuing flush of Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:40,735 Writing Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    at java.util.HashMap$EntryIterator.next(HashMap.java:834)
    at java.util.HashMap$EntryIterator.next(HashMap.java:832)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:301)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:293)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1127)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1084)
    at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:920)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:805)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:880)
    at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1027)
    at org.apache.cassandra.service.StorageService.setToken(StorageService.java:226)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:573)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:460)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:381)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:215)
    at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:238)
    ... 5 more
Cannot load daemon
Service exit with a return value of 3
 INFO 09:35:35,156 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:35:35,159 Heap size: 1936719872/1937768448
 INFO 09:35:35,160 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:35:36,626 JNA mlockall successful
 INFO 09:35:36,636 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:35:36,757 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:35:36,769 Global memtable threshold is enabled at 616MB
 INFO 09:35:36,811 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:35:37,030 Opening /raid0/cassandra/data/system/Schema-h-1 (5970 bytes)
 INFO 09:35:37,067 Opening /raid0/cassandra/data/system/Migrations-h-1 (12989 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-19 (163 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-18 (89 bytes)
 INFO 09:35:37,083 Opening /raid0/cassandra/data/system/LocationInfo-h-15 (653 bytes)
 INFO 09:35:37,085 Opening /raid0/cassandra/data/system/LocationInfo-h-16 (89 bytes)
 INFO 09:35:37,131 Loading schema version bb843dd0-0146-11e1-0000-b877c09da5ff
 INFO 09:35:37,372 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319794537372.log
 INFO 09:35:37,384 Replaying /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,416 Finished reading /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,422 Enqueuing flush of Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,423 Writing Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,424 Enqueuing flush of Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,472 Completed flushing /raid0/cassandra/data/OpsCenter/events-h-1-Data.db (230 bytes)
 INFO 09:35:37,479 Writing Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,497 Completed flushing /raid0/cassandra/data/system/Versions-h-1-Data.db (247 bytes)
 INFO 09:35:37,497 Log replay complete, 4 replayed mutations
 INFO 09:35:37,509 Cassandra version: 1.0.0
 INFO 09:35:37,510 Thrift API version: 19.18.0
 INFO 09:35:37,510 Loading persisted ring state
 INFO 09:35:37,528 Starting up server gossip
 INFO 09:35:37,530 Enqueuing flush of Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,530 Writing Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,554 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-20-Data.db (80 bytes)
 INFO 09:35:37,555 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:35:37,562 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db')]
 INFO 09:35:37,566 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:35:37,592 Using saved token 19444706681196483626478548996101040654
 INFO 09:35:37,593 Enqueuing flush of Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,593 Writing Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,616 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-22-Data.db (163 bytes)
 INFO 09:35:37,620 Node /10.40.190.143 state jump to normal
 INFO 09:35:37,639 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 09:35:37,640 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 09:35:37,684 Binding thrift service to /0.0.0.0:9160
 INFO 09:35:37,687 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-21-Data.db,].  1,074 to 799 (~74% of original) bytes for 4 keys at 0.007620MB/s.  Time: 100ms.
 INFO 09:35:37,688 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 09:35:37,692 Using synchronous/threadpool thrift server on /0.0.0.0 : 9160
 INFO 09:35:37,695 Listening for thrift clients...
 INFO 09:35:37,706 Node /10.118.230.219 is now part of the cluster
 INFO 09:35:37,707 InetAddress /10.118.230.219 is now UP
 INFO 09:35:37,708 Enqueuing flush of Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,709 Writing Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,725 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-24-Data.db (89 bytes)
 INFO 09:35:37,726 Node /10.34.42.28 is now part of the cluster
 INFO 09:35:37,727 InetAddress /10.34.42.28 is now UP
 INFO 09:35:37,729 Enqueuing flush of Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,729 Writing Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-25-Data.db (89 bytes)
 INFO 09:35:37,748 Node /10.77.63.49 has restarted, now UP
 INFO 09:35:37,748 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-24-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-22-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-25-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-21-Data.db')]
 INFO 09:35:37,748 InetAddress /10.77.63.49 is now UP
 INFO 09:35:37,749 Node /10.77.63.49 state jump to normal
 INFO 09:35:37,750 Node /10.34.70.73 is now part of the cluster
 INFO 09:35:37,750 InetAddress /10.34.70.73 is now UP
 INFO 09:35:37,752 Enqueuing flush of Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,752 Writing Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,789 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-26-Data.db,].  1,140 to 877 (~76% of original) bytes for 4 keys at 0.020399MB/s.  Time: 41ms.
 INFO 09:35:37,801 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-27-Data.db (89 bytes)
 INFO 09:35:37,801 Node /10.99.86.251 is now part of the cluster
 INFO 09:35:37,802 InetAddress /10.99.86.251 is now UP
 INFO 09:35:37,803 Enqueuing flush of Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,804 Writing Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,825 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-29-Data.db (89 bytes)
 INFO 09:35:37,826 Node /10.202.67.43 has restarted, now UP
 INFO 09:35:37,827 InetAddress /10.202.67.43 is now UP
 INFO 09:35:37,827 Node /10.202.67.43 state jump to normal
 INFO 09:35:37,828 Node /10.116.134.183 is now part of the cluster
 INFO 09:35:37,828 InetAddress /10.116.134.183 is now UP
 INFO 09:35:37,829 Enqueuing flush of Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,830 Writing Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,850 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-30-Data.db (89 bytes)
 INFO 09:35:37,852 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-30-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-27-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-26-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-29-Data.db')]
 INFO 09:35:37,853 Node /10.118.94.62 is now part of the cluster
 INFO 09:35:37,853 InetAddress /10.118.94.62 is now UP
 INFO 09:35:37,855 Enqueuing flush of Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,855 Writing Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,885 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-31-Data.db (89 bytes)
 INFO 09:35:37,886 Node /10.116.215.81 is now part of the cluster
 INFO 09:35:37,887 InetAddress /10.116.215.81 is now UP
 INFO 09:35:37,888 Enqueuing flush of Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,888 Writing Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,909 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-33-Data.db (89 bytes)
 INFO 09:35:37,910 Node /10.80.110.28 has restarted, now UP
 INFO 09:35:37,911 InetAddress /10.80.110.28 is now UP
 INFO 09:35:37,911 Node /10.80.110.28 state jump to normal
 INFO 09:35:37,912 Node /10.80.210.38 is now part of the cluster
 INFO 09:35:37,912 InetAddress /10.80.210.38 is now UP
 INFO 09:35:37,914 Enqueuing flush of Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,914 Writing Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,925 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-32-Data.db,].  1,144 to 982 (~85% of original) bytes for 4 keys at 0.014190MB/s.  Time: 66ms.
 INFO 09:35:37,927 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-35-Data.db (89 bytes)
 INFO 09:35:37,928 Node /10.40.177.173 has restarted, now UP
 INFO 09:35:37,929 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-31-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-32-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-33-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-35-Data.db')]
 INFO 09:35:37,929 InetAddress /10.40.177.173 is now UP
 INFO 09:35:37,929 Node /10.40.177.173 state jump to normal
 INFO 09:35:37,930 Node /10.101.41.8 has restarted, now UP
 INFO 09:35:37,931 InetAddress /10.101.41.8 is now UP
 INFO 09:35:37,931 Node /10.101.41.8 state jump to normal
 INFO 09:35:37,931 Node /10.205.23.34 has restarted, now UP
 INFO 09:35:37,932 InetAddress /10.205.23.34 is now UP
 INFO 09:35:37,932 Node /10.205.23.34 state jump to normal
 INFO 09:35:37,933 Node /10.118.185.68 is now part of the cluster
 INFO 09:35:37,933 InetAddress /10.118.185.68 is now UP
 INFO 09:35:37,934 Enqueuing flush of Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,935 Writing Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,970 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-36-Data.db (89 bytes)
 INFO 09:35:37,971 Node /10.116.241.250 is now part of the cluster
 INFO 09:35:37,972 InetAddress /10.116.241.250 is now UP
 INFO 09:35:37,973 Enqueuing flush of Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,974 Writing Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,003 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-38-Data.db (89 bytes)
 INFO 09:35:38,004 Node /10.113.42.21 is now part of the cluster
 INFO 09:35:38,005 InetAddress /10.113.42.21 is now UP
 INFO 09:35:38,007 Enqueuing flush of Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,008 Writing Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,014 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-37-Data.db,].  1,249 to 1,087 (~87% of original) bytes for 4 keys at 0.012196MB/s.  Time: 85ms.
 INFO 09:35:38,024 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-40-Data.db (89 bytes)
 INFO 09:35:38,024 Node /10.194.29.156 is now part of the cluster
 INFO 09:35:38,025 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-37-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-40-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-36-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-38-Data.db')]
 INFO 09:35:38,025 InetAddress /10.194.29.156 is now UP
 INFO 09:35:38,026 Enqueuing flush of Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,027 Writing Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,042 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-41-Data.db (89 bytes)
 INFO 09:35:38,043 Node /10.85.11.38 has restarted, now UP
 INFO 09:35:38,044 InetAddress /10.85.11.38 is now UP
 INFO 09:35:38,044 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,045 Node /10.34.159.72 is now part of the cluster
 INFO 09:35:38,046 InetAddress /10.34.159.72 is now UP
 INFO 09:35:38,047 Enqueuing flush of Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,048 Writing Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,065 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-42-Data.db (89 bytes)
 INFO 09:35:38,067 Node /10.194.22.191 is now part of the cluster
 INFO 09:35:38,067 InetAddress /10.194.22.191 is now UP
 INFO 09:35:38,069 Enqueuing flush of Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,069 Writing Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,092 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-44-Data.db (89 bytes)
 INFO 09:35:38,093 Node /10.34.74.58 is now part of the cluster
 INFO 09:35:38,097 InetAddress /10.34.74.58 is now UP
 INFO 09:35:38,098 Enqueuing flush of Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,099 Writing Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,105 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-43-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-41-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-44-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-42-Data.db')]
 INFO 09:35:38,106 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-43-Data.db,].  1,354 to 1,192 (~88% of original) bytes for 4 keys at 0.014034MB/s.  Time: 81ms.
 INFO 09:35:38,144 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-46-Data.db (89 bytes)
 INFO 09:35:38,145 Node /10.40.22.224 is now part of the cluster
 INFO 09:35:38,146 InetAddress /10.40.22.224 is now UP
 INFO 09:35:38,147 Enqueuing flush of Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,148 Writing Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,155 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-47-Data.db,].  1,459 to 1,297 (~88% of original) bytes for 4 keys at 0.024738MB/s.  Time: 50ms.
 INFO 09:35:38,164 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-49-Data.db (89 bytes)
 INFO 09:35:38,165 Node /10.32.79.134 is now part of the cluster
 INFO 09:35:38,166 InetAddress /10.32.79.134 is now UP
 INFO 09:35:38,167 Enqueuing flush of Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,168 Writing Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,199 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-50-Data.db (89 bytes)
 INFO 09:35:38,200 Node /10.118.179.67 is now part of the cluster
 INFO 09:35:38,200 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-50-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-47-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-49-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-46-Data.db')]
 INFO 09:35:38,200 InetAddress /10.118.179.67 is now UP
 INFO 09:35:38,202 Enqueuing flush of Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,202 Writing Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,248 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-51-Data.db (89 bytes)
 INFO 09:35:38,249 Node /10.84.205.93 is now part of the cluster
 INFO 09:35:38,249 InetAddress /10.84.205.93 is now UP
 INFO 09:35:38,251 Enqueuing flush of Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,251 Writing Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,262 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-52-Data.db,].  1,564 to 1,402 (~89% of original) bytes for 4 keys at 0.021919MB/s.  Time: 61ms.
 INFO 09:35:38,294 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-54-Data.db (89 bytes)
 INFO 09:35:38,294 Node /10.34.33.16 has restarted, now UP
 INFO 09:35:38,295 InetAddress /10.34.33.16 is now UP
 INFO 09:35:38,296 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,296 Node /10.39.107.114 is now part of the cluster
 INFO 09:35:38,297 InetAddress /10.39.107.114 is now UP
 INFO 09:35:38,298 Enqueuing flush of Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,299 Writing Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,311 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-55-Data.db (89 bytes)
 INFO 09:35:38,312 Node /10.196.79.240 is now part of the cluster
 INFO 09:35:38,312 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-52-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-55-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-54-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-51-Data.db')]
 INFO 09:35:38,313 InetAddress /10.196.79.240 is now UP
 INFO 09:35:38,314 Enqueuing flush of Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,315 Writing Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,354 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-56-Data.db (89 bytes)
 INFO 09:35:38,355 Node /10.99.39.242 has restarted, now UP
 INFO 09:35:38,356 InetAddress /10.99.39.242 is now UP
 INFO 09:35:38,356 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,357 Node /10.118.233.198 has restarted, now UP
 INFO 09:35:38,358 InetAddress /10.118.233.198 is now UP
 INFO 09:35:38,358 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,359 Node /10.82.210.172 is now part of the cluster
 INFO 09:35:38,359 InetAddress /10.82.210.172 is now UP
 INFO 09:35:38,364 Enqueuing flush of Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,364 Writing Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,439 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-58-Data.db (89 bytes)
 INFO 09:35:38,440 Node /10.80.41.192 is now part of the cluster
 INFO 09:35:38,440 InetAddress /10.80.41.192 is now UP
 INFO 09:35:38,442 Enqueuing flush of Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,442 Writing Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,451 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-57-Data.db,].  1,669 to 1,515 (~90% of original) bytes for 4 keys at 0.010470MB/s.  Time: 138ms.
 INFO 09:35:38,459 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-60-Data.db (89 bytes)
 INFO 09:35:38,459 Node /10.76.243.129 is now part of the cluster
 INFO 09:35:38,460 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-56-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-58-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-57-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-60-Data.db')]
 INFO 09:35:38,460 InetAddress /10.76.243.129 is now UP
 INFO 09:35:38,462 Enqueuing flush of Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,462 Writing Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,478 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-61-Data.db (89 bytes)
 INFO 09:35:38,486 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,487 Node /10.34.70.73 state jump to normal
 INFO 09:35:38,488 Node /10.99.86.251 state jump to normal
 INFO 09:35:38,489 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,489 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,490 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,491 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,493 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,493 Node /10.113.42.21 state jump to normal
 INFO 09:35:38,494 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,495 Node /10.34.159.72 state jump to normal
 INFO 09:35:38,496 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,497 Node /10.84.205.93 state jump to normal
 INFO 09:35:38,497 Node /10.118.179.67 state jump to normal
 INFO 09:35:38,498 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,499 Node /10.196.79.240 state jump to normal
 INFO 09:35:38,500 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,501 Node /10.80.41.192 state jump to normal
 INFO 09:35:38,502 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,508 Node /10.118.185.68 state jump to normal
 INFO 09:35:38,524 Node /10.118.230.219 state jump to normal
 INFO 09:35:38,536 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-62-Data.db,].  1,782 to 1,620 (~90% of original) bytes for 4 keys at 0.020328MB/s.  Time: 76ms.
 INFO 09:35:38,537 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,537 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,538 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,539 Node /10.116.241.250 state jump to normal
 INFO 09:35:38,540 Node /10.194.29.156 state jump to normal
 INFO 09:35:38,540 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,541 Node /10.40.22.224 state jump to normal
 INFO 09:35:38,542 Node /10.32.79.134 state jump to normal
 INFO 09:35:38,543 Node /10.39.107.114 state jump to normal
 INFO 09:35:38,543 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,550 Node /10.77.63.49 state jump to normal
 INFO 09:35:38,550 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,551 Node /10.116.134.183 state jump to normal
 INFO 09:35:38,553 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,557 Node /10.202.67.43 state jump to normal
 INFO 09:35:38,558 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,562 Node /10.116.215.81 state jump to normal
 INFO 09:35:38,563 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,564 Node /10.205.23.34 state jump to normal
 INFO 09:35:38,565 Node /10.39.107.114 state jump to normal
{CODE}"
CASSANDRA-3358,2GB row size limit in ColumnIndex offset calculation,"Index offset is calculated using int instead of long resulting in overflow at 2GB row size. As a result affected columns can not be retrieved. 

Fix: use long instead of int"
CASSANDRA-3349,NPE on malformed CQL,"It's not clear why, but the CQL grammar specification in Cql.g allows for an empty WHERE clause on DELETE, i.e.:

{noformat}
DELETE FROM someCF WHERE;
{noformat}

When this is used, with or without a column list, it causes an NPE on the node processing the CQL. Traceback on a recent 1.0.0 build:

{noformat}
ERROR [pool-2-thread-1] 2011-10-11 15:45:25,655 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
        at org.apache.cassandra.cql.CqlParser.deleteStatement(CqlParser.java:1994)
        at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:292)
        at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:984)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:500)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1268)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

The CQL client gets an error with the message, ""Internal application error"".

It might be better to allow leaving off the ""WHERE"" as well as the condition, to match SQL semantics, although fixing that probably won't solve this problem."
CASSANDRA-3334,dropping index causes some inflight mutations to fail,"dropping index causes some inflight mutations to fail. hector on client side didnt throw any exception

 INFO [MigrationStage:1] 2011-10-07 23:11:53,742 Migration.java (line 119) Applying migration fb1a8540-f128-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@786669[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@8bb33c,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=461 lim=464 cap=466],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=null, index_name='null'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:11:53,805 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:11:53,820 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:53,820 Memtable.java (line 237) Writing Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-14-Data.db (7924 bytes)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 237) Writing Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [CompactionExecutor:4] 2011-10-07 23:11:55,008 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-11-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-12-Data.db')]
 INFO [FlushWriter:3] 2011-10-07 23:11:56,430 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-14-Data.db (3470 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:11:56,446 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-12-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-11-Data.db')]
ERROR [MutationStage:56] 2011-10-07 23:11:56,508 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:56,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:51] 2011-10-07 23:11:56,539 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:51,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:38] 2011-10-07 23:11:56,633 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:38,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:57] 2011-10-07 23:11:56,664 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:57,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
"
CASSANDRA-3257,Enabling SSL on a fairly light cluster leaks Open files.,"To reproduce:

Enable SSL encryption and let the server be idle for a day or so you will see the below....

[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
16333
Sun Sep 25 17:23:29 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7501 java.lang:type=Memory gc
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
64
Sun Sep 25 17:23:53 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ 

After running GC manually the issue goes away."
CASSANDRA-3248,CommitLog writer should call fdatasync instead of fsync,"CommitLogSegment uses SequentialWriter to flush the buffered data to log device. It depends on FileDescriptor#sync() which invokes fsync() as it force the file attributes to disk.

However, at least on Linux, fdatasync() is good enough for commit log flush:

bq. fdatasync() is similar to fsync(), but does not flush modified metadata unless that metadata is needed in order to allow a subsequent data retrieval to be  correctly handled.  For example, changes to st_atime or st_mtime (respectively, time of last access and time of last modification; see stat(2)) do not require flushing because they are not necessary for a subsequent data read to be handled correctly.  On the other hand, a change to the file size (st_size,  as  made  by  say  ftruncate(2)),  would require a metadata flush.

File size is synced to disk by fdatasync() either. Although the commit log recovery logic sorts the commit log segements on their modify timestamp, it can be removed safely, IMHO.

I checked the native code of JRE 6. On Linux and Solaris, FileChannel#force(false) invokes fdatasync(). On windows, the false flag does not have any impact.

On my log device (commodity SATA HDD, write cache disabled), there is large performance gap between fsync() and fdatasync():
{quote}
$sysbench --test=fileio --num-threads=1  --file-num=1 --file-total-size=10G --file-fsync-all=on --file-fsync-mode={color:red}fdatasync{color} --file-test-mode=seqwr --max-time=600 --file-block-size=2K  --max-requests=0 run

{color:blue}54.90{color} Requests/sec executed

   per-request statistics:
         min:                                  8.29ms
         avg:                                 18.18ms
         max:                                108.36ms
         approx.  95 percentile:              25.02ms


$ sysbench --test=fileio --num-threads=1  --file-num=1 --file-total-size=10G --file-fsync-all=on --file-fsync-mode={color:red}fsync{color} --file-test-mode=seqwr --max-time=600 --file-block-size=2K  --max-requests=0 run

{color:blue}28.08{color} Requests/sec executed

    per-request statistics:
         min:                                 33.28ms
         avg:                                 35.61ms
         max:                                911.87ms
         approx.  95 percentile:              41.69ms
{quote}

I do think this is a very critical performance improvement.

"
CASSANDRA-3246,memtable_total_space_in_mb does not accept the value 0 in Cassandra 1.0,"This affects 1.0 beta1.

From the key explanation in cassandra.yaml it looks like it should accept the value ""0""

# Total memory to use for memtables. Cassandra will flush the largest
# memtable when this much memory is used.
# If omitted, Cassandra will set it to 1/3 of the heap.
# If set to 0, only the old flush thresholds are used.
memtable_total_space_in_mb: 0

However in the code I could see the following:

if (conf.memtable_total_space_in_mb <= 0)
throw new ConfigurationException(""memtable_total_space_in_mb must be positive"");
logger.info(""Global memtable threshold is enabled at {}MB"", conf.memtable_total_space_in_mb);"
CASSANDRA-3234,LeveledCompaction has several performance problems,"Two main problems:

- BF size calculation doesn't take into account LCS breaking the output apart into ""bite sized"" sstables, so memory use is much higher than predicted
- ManyToMany merging is slow.  At least part of this is from running the full reducer machinery against single input sources, which can be optimized away."
CASSANDRA-3223,probably don't need to do full copy to row cache after un-mmap() change,"3179  changes from directly using the bytebuffer from mmap(), to copying that buffer,

CFS.cacheRow() https://github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/ColumnFamilyStore.java   line 1126
says it makes a deep copy exactly to prevent issues from unmmap().

maybe this deep copy is not needed now given 3179


if so, maybe slightly better performance in both speed and memory"
CASSANDRA-3210,memtables do not need to be flushed on the Table.apply() path anymore after 2449,"2449 removes auto-flush from Table.apply(), but the data structure is still there, no harm, but better remove it:

in
https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/Table.java

line 399 and 470"
CASSANDRA-3203,Odd flush behavior,"Given the same workload against 0.8, trunk is creating more than twice the amount of sstables.  Even though a uniform stress workload is being generated, flush size degrades quickly:

{noformat}
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:22,878 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2058235391(7741
035/110172631 serialized/live bytes, 151785 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:24,888 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1520390052(3887
220/72403158 serialized/live bytes, 76220 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:26,890 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1868496516(4097
085/76255481 serialized/live bytes, 80335 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:28,893 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@498232521(43513
20/80922269 serialized/live bytes, 85320 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:29,895 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1592308290(2310
810/44514839 serialized/live bytes, 45310 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:30,897 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@775439677(22684
80/64984390 serialized/live bytes, 44480 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:31,899 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@928217914(26741
85/76231422 serialized/live bytes, 52435 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:32,901 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@158103119(27511
95/77317732 serialized/live bytes, 53945 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:33,903 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2035169258(3132
420/88934701 serialized/live bytes, 61420 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:34,905 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1097314626(2979
675/83651699 serialized/live bytes, 58425 ops)
{noformat}

The serialized to live size ratio appears completely out of whack."
CASSANDRA-3193,Gossip teardown causes test failures,Just look at any recent jenkins or buildbot attempt for the semi-nonsensical NBHM.remove NPE
CASSANDRA-3183,Make SerializingCacheProvider the default if JNA is available,"The serializing cache is a better choice for most users:

- Lower total memory usage (serialized data is usually 8x-12x smaller than ""live"" data in the JVM with all the overhead that involves) means you can cache more rows for a given memory footprint
- Moving the serialized rows off-heap means you can use smaller heaps, reducing the impact of GC pauses"
CASSANDRA-3168,Arena allocation causes excessive flushing on small heaps,"adding allocator.size() to Memtable.getLiveSize has two problems:

1) it double-counts allocated parts of regions
2) it makes the size of an empty memtable the size of a single region

(2) is a particular problem because flushing a nearly-empty memtable will not actually free up much memory -- we just trade one almost-empty region, for another.  In testing, I even saw this happening to the low-traffic system tables like LocationInfo."
CASSANDRA-3155,Secondary index should report it's memory consumption,Non-CFS backed secondary indexes will consume RAM which should be reported back to Cassandra to be factored into it's flush by RAM amount.
CASSANDRA-3122,SSTableSimpleUnsortedWriter take long time when inserting big rows,"In SSTableSimpleUnsortedWriter, when dealing with rows having a lot of columns, if we call newRow several times (to flush data as soon as possible), the time taken by the newRow() call is increasing non linearly. This is because when newRow is called, we merge the size increasing existing CF with the new one."
CASSANDRA-3061,Optionally skip log4j configuration,"from this thread http://groups.google.com/group/brisk-users/browse_thread/thread/3a18f4679673bea8

When brisk accesses cassandra classes inside of a Hadoop Task JVM the AbstractCassandraDaemon uses a log4j PropertyConfigurator to setup cassandra logging. This closes all the existing appenders, including the TaskLogAppender for the hadoop task. They are not opened again because they are not in the config. 

log4j has Logger Repositories to handle multiple configs in the same process, but there is a bit of suck involved in making a RepositorySelector. 

Two examples...
http://www.mail-archive.com/log4j-dev@jakarta.apache.org/msg02972.html
http://docs.redhat.com/docs/en-US/JBoss_Enterprise_Application_Platform/4.2/html/Getting_Started_Guide/logging.log4j.reposelect.html

Basically all the selector has access to thread local storage, and it looks like normally people get the class loader from the current thread. A thread will inherit it's class loader from the thread that created it, unless otherwise specified. 

We have code in the same thread the uses hadoop and cassandra classes, so this could be a dead end.  

As a work around i've added cassandra.log4j.configure JVM param and made the AbstractCassandraServer skip the log4j config if it's false. My job completes and I can see the cassandra code logging an extra message I put in into the Hadoop task log file...

2011-08-19 15:56:06,442 WARN org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-maptask.properties, hadoop-metrics2.properties
2011-08-19 15:56:06,776 INFO org.apache.cassandra.service.AbstractCassandraDaemon: Logging initialized externally
2011-08-19 15:56:07,332 INFO org.apache.hadoop.mapred.MapTask: numReduceTasks: 0
 
The param has to be passed to the task JVM, so need to modify Haddop mapred-site.xml as follows 

<property>
  <name>mapred.child.java.opts</name>
  <value>-Xmx256m -Dcassandra.log4j.configure=false</value>
  <description>
    Tune your mapred jvm arguments for best performance. 
    Also see documentation from jvm vendor.
  </description>
</property>

It's not pretty but it works. In my extra log4j logging I can see the second reset() call is gone.  


Change the to Hadoop TaskLogAppender also stops the NPE but there may also be some lost log messages 
https://issues.apache.org/jira/browse/HADOOP-7556"
CASSANDRA-3057,secondary index on a column that has a value of size > 64k will fail on flush,"exception seen on flush when an indexed column contain size > 64k:

granted that having a value > 64k possibly mean something that shouldn't be indexed as it most likely would have a high cardinality, but i think there would still be some valid use case for it.

test case:
simply run the stress test with 
-n 1 -u 0 -c 2  -y Standard  -o INSERT  -S 65536 -x KEYS

then call a flush

exception:
 INFO [FlushWriter:8] 2011-08-18 21:49:33,214 Memtable.java (line 218) Writing Memtable-Standard1.Idx1@1652462853(16/20 serialized/live bytes, 1 ops)
Standard1@980087547(196659/245823 serialized/live bytes, 3 ops)
ERROR [FlushWriter:8] 2011-08-18 21:49:33,230 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:8,5,RMI Runtime]
java.lang.AssertionError: 65536
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithShortLength(ByteBufferUtil.java:330)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:245)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

"
CASSANDRA-3022,Failures in cassandra long test: LongCompactionSpeedTest,"*The failing test case*
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
{code}


*The following error is repeated in the console output*
{code}
    [junit] ERROR 04:02:20,654 Error in ThreadPoolExecutor
    [junit] java.util.MissingFormatArgumentException: Format specifier 's'
    [junit] 	at java.util.Formatter.format(Formatter.java:2432)
    [junit] 	at java.util.Formatter.format(Formatter.java:2367)
    [junit] 	at java.lang.String.format(String.java:2769)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:136)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:559)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:141)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:107)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
{code}

*Cassandra Revision List at time of failure
{code}
Summary
* log ks and cf of large rows being compacted patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
* revert r1156772
* cache invalidate removes saved cache files patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
* make sure truncate clears out the commitlog patch by jbellis; reviewed by slebresne for CASSANDRA-2950
* include column name in validation failure exceptions patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
* fix NPE when encryption_options is unspecified patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
* update CHANGES
* update CHANGES

Revision 1156830 by jbellis: 
log ks and cf of large rows being compacted
patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java

Revision 1156791 by jbellis: 
revert r1156772
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156772 by jbellis: 
cache invalidate removes saved cache files
patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156763 by jbellis: 
make sure truncate clears out the commitlog
patch by jbellis; reviewed by slebresne for CASSANDRA-2950
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
	/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156753 by jbellis: 
include column name in validation failure exceptions
patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1156749 by jbellis: 
fix NPE when encryption_options is unspecified
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java

Revision 1156695 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt

Revision 1156694 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt
{code}

"
CASSANDRA-3003,Trunk single-pass streaming doesn't handle large row correctly,"For normal column family, trunk streaming always buffer the whole row into memory. In uses
{noformat}
  ColumnFamily.serializer().deserializeColumns(in, cf, true, true);
{noformat}
on the input bytes.
We must avoid this for rows that don't fit in the inMemoryLimit.

Note that for regular column families, for a given row, there is actually no need to even recreate the bloom filter of column index, nor to deserialize the columns. It is enough to filter the key and row size to feed the index writer, but then simply dump the rest on disk directly. This would make streaming more efficient, avoid a lot of object creation and avoid the pitfall of big rows.

Counters column family are unfortunately trickier, because each column needs to be deserialized (to mark them as 'fromRemote'). However, we don't need to do the double pass of LazilyCompactedRow for that. We can simply use a SSTableIdentityIterator and deserialize/reserialize input as it comes."
CASSANDRA-2929,Don't include tmp files as sstable when create column families,"When we open a column family and populate the SSTableReader, we happen to include -tmp files. This has no change to actually happen in a real life situation, but that is what was triggering a race in the unit tests triggering spurious assertion failure in estimateRowsFromIndex."
CASSANDRA-2901,Allow taking advantage of multiple cores while compacting a single CF,"Moved from CASSANDRA-1876:

There are five stages: read, deserialize, merge, serialize, and write. We probably want to continue doing read+deserialize and serialize+write together, or you waste a lot copying to/from buffers.

So, what I would suggest is: one thread per input sstable doing read + deserialize (a row at a time). A thread pool (one per core?) merging corresponding rows from each input sstable. One thread doing serialize + writing the output (this has to wait for the merge threads to complete in-order, obviously). This should take us from being CPU bound on SSDs (since only one core is compacting) to being I/O bound.

This will require roughly 2x the memory, to allow the reader threads to work ahead of the merge stage. (I.e. for each input sstable you will have up to one row in a queue waiting to be merged, and the reader thread working on the next.) Seems quite reasonable on that front.  You'll also want a small queue size for the serialize-merged-rows executor.

Multithreaded compaction should be either on or off. It doesn't make sense to try to do things halfway (by doing the reads with a
threadpool whose size you can grow/shrink, for instance): we still have compaction threads tuned to low priority, by default, so the impact on the rest of the system won't be very different. Nor do we expect to have so many input sstables that we lose a lot in context switching between reader threads.

IMO it's acceptable to punt completely on rows that are larger than memory, and fall back to the old non-parallel code there. I don't see any sane way to parallelize large-row compactions."
CASSANDRA-2894,add paging to get_count,"It is non-intuitive that get_count materializes the entire slice-to-count on the coordinator node (to perform read repair and > CL.ONE consistency).  Even experienced users have been known to cause memory problems by requesting large counts.

The user cannot page the count himself, because you need a start and stop column to do that, and get_count only returns an integer.

So the best fix is for us to do the paging under the hood, in CassandraServer.  Add a limit to the slicepredicate they specify, and page through it.

We could add a global setting for count_slice_size, and document that counts of more columns than that will have higher latency (because they make multiple calls through StorageProxy for the pages)."
CASSANDRA-2868,Native Memory Leak,"We have memory issues with long running servers. These have been confirmed by several users in the user list. That's why I report.

The memory consumption of the cassandra java process increases steadily until it's killed by the os because of oom (with no swap)

Our server is started with -Xmx3000M and running for around 23 days.

pmap -x shows

Total SST: 1961616 (mem mapped data and index files)
Anon  RSS: 6499640
Total RSS: 8478376

This shows that > 3G are 'overallocated'.

We will use BRAF on one of our less important nodes to check wether it is related to mmap and report back."
CASSANDRA-2864,Alternative Row Cache Implementation,"we have been working on an alternative implementation to the existing row cache(s)

We have 2 main goals:

- Decrease memory -> get more rows in the cache without suffering a huge performance penalty
- Reduce gc pressure

This sounds a lot like we should be using the new serializing cache in 0.8. 
Unfortunately our workload consists of loads of updates which would invalidate the cache all the time.

*Note: Updated Patch Description (Please check history if you're interested where this was comming from)*

h3. Rough Idea

- Keep serialized row (ByteBuffer) in mem which represents unfiltered but collated columns of all ssts but not memtable columns
- Writes dont affect the cache at all. They go only to the memtables
- Reads collect columns from memtables and row cache
- Serialized Row is re-written (merged) with mem tables when flushed


h3. Some Implementation Details

h4. Reads

- Basically the read logic differ from regular uncached reads only in that a special CollationController which is deserializing columns from in memory bytes
- In the first version of this cache the serialized in memory format was the same as the fs format but test showed that performance sufferd because a lot of unnecessary deserialization takes place and that columns seeks are O( n ) whithin one block
- To improve on that a different in memory format was used. It splits length meta info and data of columns so that the names can be binary searched. 

{noformat}

===========================
Header (24)                    
===========================
MaxTimestamp:        long  
LocalDeletionTime:   int   
MarkedForDeleteAt:   long  
NumColumns:          int   
===========================
Column Index (num cols * 12)              
===========================
NameOffset:          int   
ValueOffset:         int   
ValueLength:         int   
===========================
Column Data                
===========================
Name:                byte[]
Value:               byte[]
SerializationFlags:  byte  
Misc:                ?     
Timestamp:           long  
---------------------------
Misc Counter Column        
---------------------------
TSOfLastDelete:      long  
---------------------------
Misc Expiring Column       
---------------------------
TimeToLive:          int   
LocalDeletionTime:   int   
===========================

{noformat}

- These rows are read by 2 new column interators which correspond to SSTableNamesIterator and SSTableSliceIterator. During filtering only columns that actually match are constructed. The searching / skipping is performed on the raw ByteBuffer and does not create any objects.
- A special CollationController is used to access and collate via cache and said new iterators. It also supports skipping the cached row by max update timestamp

h4. Writes

- Writes dont update or invalidate the cache.
- In CFS.replaceFlushed memtables are merged before the data view is switched. I fear that this is killing counters because they would be overcounted but my understading of counters is somewhere between weak and non-existing. I guess that counters if one wants to support them here would need an additional unique local identifier in memory and in serialized cache to be able to filter duplicates or something like that.

{noformat}
    void replaceFlushed(Memtable memtable, SSTableReader sstable)
    {
        if (sstCache.getCapacity() > 0) {
            mergeSSTCache(memtable);
        }
        data.replaceFlushed(memtable, sstable);
        CompactionManager.instance.submitBackground(this);
    }
{noformat}


Test Results: See comments below

"
CASSANDRA-2863,NPE when writing SSTable generated via repair,"A NPE is generated during repair when closing an sstable generated via SSTable build. It doesn't happen always. The node had been scrubbed and compacted before calling repair.

 INFO [CompactionExecutor:2] 2011-07-06 11:11:32,640 SSTableReader.java (line 158) Opening /d2/cassandra/data/sbs/walf-g-730
ERROR [CompactionExecutor:2] 2011-07-06 11:11:34,327 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:2,1,main] 
java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
"
CASSANDRA-2849,InvalidRequestException when validating column data includes entire column value,"If the column value fails to validate, then ThriftValidation.validateColumnData() calls bytesToHex() on the entire column value and puts this string in the Exception. Since the value may be up to 2GB, this is potentially a lot of extra memory. The value is likely to be logged (and presumably returned to the thrift client over the network?). This could cause a lot of slowdown or an unnecessary OOM crash, and is unlikely to be useful (the client has access to the full value anyway if required for debugging).

Also, the reason for the exception (extracted from the MarshalException) is printed _after_ the data, so if there's any truncation in the logging system at any point, the reason will be lost. 

The reason should be displayed before the column value, and the column value should be truncated in the Exception message."
CASSANDRA-2845,Cassandra uses 100% system CPU on Ubuntu Natty (11.04),"Step 1. Boot up a brand new, default Ubuntu 11.04 Server install
Step 2. Install Cassandra from Apache APT Respository (deb http://www.apache.org/dist/cassandra/debian 08x main)
Step 3. apt-get install cassandra, as soon as it cassandra starts it will freeze the machine

What's happening is that as soon as cassandra starts up it immediately sucks up 100% of CPU and starves the machine. This effectively bricks the box until you boot into single user mode and disable the cassandra init.d script.

Under htop, the CPU usage shows up as ""system"" cpu, not user.

The machine I'm testing this on is a Quad-Core Sandy Bridge w/ 16GB of Memory, so it's not a system resource issue. I've also tested this on completely different hardware (Dual 64-Bit Xeons & AMD X4) and it has the same effect.

Ubuntu 10.10 does not exhibit the same issue. I have only tested 0.8 and 0.8.1.

root@cassandra01:/# java -version
java version ""1.6.0_22""
OpenJDK Runtime Environment (IcedTea6 1.10.2) (6b22-1.10.2-0ubuntu1~11.04.1)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)

root@cassandra:/# uname -a
Linux cassandra01 2.6.38-8-generic #42-Ubuntu SMP Mon Apr 11 03:31:24 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux

/proc/cpu
Intel(R) Xeon(R) CPU E31270 @ 3.40GHz

/proc/meminfo
MemTotal:       16459776 kB
MemFree:        14190708 kB"
CASSANDRA-2843,better performance on long row read,"currently if a row contains > 1000 columns, the run time becomes considerably slow (my test of 
a row with 30 00 columns (standard, regular) each with 8 bytes in name, and 40 bytes in value, is about 16ms.
this is all running in memory, no disk read is involved.

through debugging we can find
most of this time is spent on 
[Wall Time]  org.apache.cassandra.db.Table.getRow(QueryFilter)
[Wall Time]  org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(QueryFilter, ColumnFamily)
[Wall Time]  org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(QueryFilter, int, ColumnFamily)
[Wall Time]  org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(QueryFilter, int, ColumnFamily)
[Wall Time]  org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(ColumnFamily, Iterator, int)
[Wall Time]  org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(IColumnContainer, Iterator, int)
[Wall Time]  org.apache.cassandra.db.ColumnFamily.addColumn(IColumn)

ColumnFamily.addColumn() is slow because it inserts into an internal concurrentSkipListMap() that maps column names to values.
this structure is slow for two reasons: it needs to do synchronization; it needs to maintain a more complex structure of map.

but if we look at the whole read path, thrift already defines the read output to be List<ColumnOrSuperColumn> so it does not make sense to use a luxury map data structure in the interium and finally convert it to a list. on the synchronization side, since the return CF is never going to be shared/modified by other threads, we know the access is always single thread, so no synchronization is needed.

but these 2 features are indeed needed for ColumnFamily in other cases, particularly write. so we can provide a different ColumnFamily to CFS.getTopLevelColumnFamily(), so getTopLevelColumnFamily no longer always creates the standard ColumnFamily, but take a provided returnCF, whose cost is much cheaper.

the provided patch is for demonstration now, will work further once we agree on the general direction. 
CFS, ColumnFamily, and Table  are changed; a new FastColumnFamily is provided. the main work is to let the FastColumnFamily use an array  for internal storage. at first I used binary search to insert new columns in addColumn(), but later I found that even this is not necessary, since all calling scenarios of ColumnFamily.addColumn() has an invariant that the inserted columns come in sorted order (I still have an issue to resolve descending or ascending  now, but ascending works). so the current logic is simply to compare the new column against the end column in the array, if names not equal, append, if equal, reconcile.

slight temporary hacks are made on getTopLevelColumnFamily so we have 2 flavors of the method, one accepting a returnCF. but we could definitely think about what is the better way to provide this returnCF.


this patch compiles fine, no tests are provided yet. but I tested it in my application, and the performance improvement is dramatic: it offers about 50% reduction in read time in the 3000-column case.


thanks
Yang
"
CASSANDRA-2829,memtable with no post-flush activity can leave commitlog permanently dirty,"Only dirty Memtables are flushed, and so only dirty memtables are used to discard obsolete commit log segments. This can result it log segments not been deleted even though the data has been flushed.  

Was using a 3 node 0.7.6-2 AWS cluster (DataStax AMI's) with pre 0.7 data loaded and a running application working against the cluster. Did a rolling restart and then kicked off a repair, one node filled up the commit log volume with 7GB+ of log data, there was about 20 hours of log files. 

{noformat}
$ sudo ls -lah commitlog/
total 6.9G
drwx------ 2 cassandra cassandra  12K 2011-06-24 20:38 .
drwxr-xr-x 3 cassandra cassandra 4.0K 2011-06-25 01:47 ..
-rw------- 1 cassandra cassandra 129M 2011-06-24 01:08 CommitLog-1308876643288.log
-rw------- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308876643288.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 01:36 CommitLog-1308877711517.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308877711517.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 02:20 CommitLog-1308879395824.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308879395824.log.header
...
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 20:38 CommitLog-1308946745380.log
-rw-r--r-- 1 cassandra cassandra   36 2011-06-24 20:47 CommitLog-1308946745380.log.header
-rw-r--r-- 1 cassandra cassandra 112M 2011-06-24 20:54 CommitLog-1308947888397.log
-rw-r--r-- 1 cassandra cassandra   44 2011-06-24 20:47 CommitLog-1308947888397.log.header
{noformat}

The user KS has 2 CF's with 60 minute flush times. System KS had the default settings which is 24 hours. Will create another ticket see if these can be reduced or if it's something users should do, in this case it would not have mattered. 

I grabbed the log headers and used the tool in CASSANDRA-2828 and most of the segments had the system CF's marked as dirty.

{noformat}
$ bin/logtool dirty /tmp/logs/commitlog/

Not connected to a server, Keyspace and Column Family names are not available.

/tmp/logs/commitlog/CommitLog-1308876643288.log.header
Keyspace Unknown:
	Cf id 0: 444
/tmp/logs/commitlog/CommitLog-1308877711517.log.header
Keyspace Unknown:
	Cf id 1: 68848763
...
/tmp/logs/commitlog/CommitLog-1308944451460.log.header
Keyspace Unknown:
	Cf id 1: 61074
/tmp/logs/commitlog/CommitLog-1308945597471.log.header
Keyspace Unknown:
	Cf id 1000: 43175492
	Cf id 1: 108483
/tmp/logs/commitlog/CommitLog-1308946745380.log.header
Keyspace Unknown:
	Cf id 1000: 239223
	Cf id 1: 172211

/tmp/logs/commitlog/CommitLog-1308947888397.log.header
Keyspace Unknown:
	Cf id 1001: 57595560
	Cf id 1: 816960
	Cf id 1000: 0
{noformat}

CF 0 is the Status / LocationInfo CF and 1 is the HintedHandof CF. I dont have it now, but IIRC CFStats showed the LocationInfo CF with dirty ops. 

I was able to repo a case where flushing the CF's did not mark the log segments as obsolete (attached unit-test patch). Steps are:

1. Write to cf1 and flush.
2. Current log segment is marked as dirty at the CL position when the flush started, CommitLog.discardCompletedSegmentsInternal()
3. Do not write to cf1 again.
4. Roll the log, my test does this manually. 
5. Write to CF2 and flush.
6. Only CF2 is flushed because it is the only dirty CF. cfs.maybeSwitchMemtable() is not called for cf1 and so log segment 1 is still marked as dirty from cf1.

Step 5 is not essential, just matched what I thought was happening. I thought SystemTable.updateToken() was called which does not flush, and this was the last thing that happened.  

The expired memtable thread created by Table uses the same cfs.forceFlush() which is a no-op if the cf or it's secondary indexes are clean. 
    
I think the same problem would exist in 0.8. "
CASSANDRA-2823,NPE during range slices with rowrepairs,"Doing some heavy testing of relatively fast feeding (5000+ mutations/sec) + repair on all node + range slices.
Then occasionally killing a node here and there and restarting it.

Triggers the following NPE
 ERROR [pool-2-thread-3] 2011-06-24 20:56:27,289 Cassandra.java (line 3210) Internal error processing get_range_slices
java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.maybeScheduleRepairs(RowRepairResolver.java:109)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:112)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:83)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:161)
	at org.apache.cassandra.utils.MergeIterator.computeNext(MergeIterator.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:120)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:43)

Looking at the code in getReduced:

{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
{noformat}
seems like resolved becomes null when this happens and versions.size is larger than 1.

RowRepairResolver.resolveSuperset() does actually return null if it cannot resolve anything, so there is definately a case here which can occur and is not handled.

It may also be an interesting question if it is guaranteed that                
versions.add(current.left.cf);
can never return null?

Jonathan suggested on IRC that maybe 
{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
                if (resolved == null)
                      return new Row(key, resolved);
{noformat}

could be a fix.
"
CASSANDRA-2817,Expose number of threads blocked on submitting a memtable for flush,"Writes can be blocked by a thread trying to submit a memtable while the flush queue is full. While this is the expected behavior (the goal being to prevent OOMing), it is worth exposing when that happens so that people can monitor it and modify settings accordingly if that happens too often."
CASSANDRA-2811,Repair doesn't stagger flushes,"When you do a nodetool repair (with no options), the following things occured:
* For each keyspace, a call to SS.forceTableRepair is issued
* In each of those calls: for each token range the node is responsible for, a repair session is created and started
* Each of these session will request one merkle tree by column family (to each node for which it makes sense, which includes the node the repair is started on)

All those merkle tree requests are done basically at the same time. And now that compaction is multi-threaded, this means that usually more than one validation compaction will be started at the same time. The problem is that a validation compaction starts by a flush. Given that by default the flush_queue_size is 4 and the number of compaction thread is the number of processors and given that on any recent machine the number of core will be >= 4, this means that this will easily end up blocking write for some period of time.

It turns out to also have a more subtle problem for repair itself. If two validation compaction for the same column family (but different range) are started in a very short time interval, the first validation will block on the flush, but the second one may not block at all if the memtable is clean when it request it's own flush. In which case that second validation will be executed on data older than it should.

I think the simpler fix is to make sure we only ever do one validation compaction at a time. It's probably a better use of resources anyway. "
CASSANDRA-2792,Bootstrapping node stalls. Bootstrapper thinks it is still streaming some sstables. The source nodes do not. Caused by IllegalStateException on source nodes.,"I am bootstrapping a node into a 4 node cluster with RF3 (1 node is currently down due to sstable issues, but the cluster is running without issues). 

There are two keyspaces FightMyMonster and FMM_Studio. The first keyspace successfully streams and the whole operation is probably at 99% when it stalls on some sstables in the much smaller FMM_Studio keyspace.

Netstats on the bootstrapping node reports it is still streaming:

Mode: Bootstrapping
Not sending any streams.
Streaming from: /192.168.1.4
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-101-Data.db sections=1 progress=0/76453 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-103-Data.db sections=1 progress=0/90475 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-102-Data.db sections=1 progress=0/4304182 - 0%
Streaming from: /192.168.1.3
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-158-Data.db sections=2 progress=0/146990 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-81-Data.db sections=1 progress=0/3992 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-70-Data.db sections=1 progress=0/1776 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-159-Data.db sections=2 progress=0/136829 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-157-Data.db sections=2 progress=0/5779597 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-82-Data.db sections=1 progress=0/161 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-71-Data.db sections=1 progress=0/135 - 0%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0            334
Responses                       n/a         0         421957

However, running netstats on the source nodes reports they are not streaming:

Mode: Normal
 Nothing streaming to /192.168.1.9
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0        1949476
Responses                       n/a         1        1778768

Examination of the logs on the source nodes show an IllegalStateException that has likely interrupted/broken the streaming process.

17 22:27:05,924 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:05,925 StreamOut.java (line 100) Flushing memtables for FMM_Studio...
 INFO [StreamStage:1] 2011-06-17 22:27:06,004 StreamOut.java (line 173) Stream context metadata [/var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db sections=1 progress=0/1585378 - 0%, /var/opt/cas
sandra/data/FMM_Studio/PartsData-f-100-Data.db sections=1 progress=0/76453 - 0%, /var/opt/cassandra/data/FMM_Studio/PartsData-f-98-Data.db sections=1 progress=0/4309514 - 0%, /var/opt/cassandra/data/FMM
_Studio/PartsData-f-99-Data.db sections=1 progress=0/90475 - 0%], 11 sstables.
 INFO [StreamStage:1] 2011-06-17 22:27:06,005 StreamOutSession.java (line 174) Streaming to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,006 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 StreamOut.java (line 100) Flushing memtables for FightMyMonster...
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,007 Memtable.java (line 157) Writing Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Tribes@1510979736(18318 bytes, 703 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-ColumnViews_TimeUUID@864545260(2073 bytes, 63 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_0@537829218(2600 bytes, 129 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/MonsterMarket_1-f-3799-Data.db (1774 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 157) Writing Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,070 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserSigninLog@1692186117(4043 bytes, 137 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/UserFights-f-8192-Data.db (1179202 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 157) Writing Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [CompactionExecutor:1] 2011-06-17 22:27:06,161 CompactionManager.java (line 395) Compacting [SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8189-Data.db'),SSTableReader(pa
th='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8190-Data.db'),SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8191-Data.db'),SSTableReader(path='/var/opt/cassandra/data/
FightMyMonster/UserFights-f-8192-Data.db')]
 INFO [StreamStage:1] 2011-06-17 22:27:06,162 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-TribeFights@321579649(138 bytes, 3 operations)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) 

There are two problems. Firstly the source nodes should report to the bootstrapping node that there has been a problem, and/or the bootstrapping node should timeout and report the the issue. 

Secondly, there is an issue with what is causing IllegalStateException."
CASSANDRA-2755,ColumnFamilyRecordWriter fails to throw a write exception encountered after the user begins to close the writer,"There appears to be a race condition in {{ColumnFamilyRecordWriter}} that can result in the loss of an exception. Here is how it can happen (W stands for the {{RangeClient}}'s worker thread; U stands for the {{ColumnFamilyRecordWriter}} user's thread):

# W: {{RangeClient}}'s {{run}} method catches an exception originating in the Thrift client/socket, but doesn't get a chance to set it on the {{lastException}} field before it the thread is preempted.
# U: The user calls {{close}} which calls {{stopNicely}}. Because the {{lastException}} field is null, {{stopNicely}} does not throw anything. {{close}} then joins on the worker thread.
# W: The {{RangeClient}}'s {{run}} method sets the {{lastException}} field and exits.
# U: Although the thread in {{close}} is waiting for the worker thread to exit, it has already checked the {{lastException}} field so it doesn't detect the presence of the last exception. Instead, {{close}} returns without throwing anything.

This race condition means that intermittently write failures will go undetected."
CASSANDRA-2746,CliClient does not log root cause exception when catch it from executeCLIStatement,"When executing a statement from the cassandra-cli (with --debug) , if an exception is thrown from one of the cases in side the executeCLIStatement method, the root cause is swallowed. For specific case such as the InvalidRequestException or the SchemaDisagreementException, just the message itself maybe enough, but for the general Exception case, without the root cause, it could be difficult to debug the issue. 

For example, we have seen exception like:
{noformat}
null
java.lang.RuntimeException
at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
{noformat}

the null there would most likely indicate this is a NPE (though it could still be any Exception with null message). By adding a initCause to the caught exception, we could see the root cause, eg:

{noformat}
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:212)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1336)
        at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1166)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:170)
        ... 2 more
{noformat}

submitting a patch here that would add the initCause to all caught exceptions here. But the most important one is the general Exception case."
CASSANDRA-2718,NPE in SSTableWriter when no ReplayPosition availible,"The following NPE occurs when durable_writes is set to false

{noformat}
ERROR 09:20:30,378 Fatal exception in thread Thread[FlushWriter:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.serialize(ReplayPosition.java:127)
	at org.apache.cassandra.io.sstable.SSTableWriter.writeMetadata(SSTableWriter.java:209)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:187)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:173)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}"
CASSANDRA-2713,Null strategy_options on a KsDef leads to an NPE.,"For add/update keyspace, a KsDef with null strategy_options will cause an NPE."
CASSANDRA-2677,Optimize streaming to be single-pass,"Streaming currently is a two-pass operation: one to write the Data component do disk from the socket, then another to build the index and bloom filter from it.  This means we do about 2x the i/o we would if we created the index and BF during the original write.

For node movement this was not considered to be a Big Deal because the stream target is not a member of the ring, so we can be inefficient without hurting live queries.  But optimizing node movement to not require un/rebootstrap (CASSANDRA-1427) and bulk load (CASSANDRA-1278) mean we can stream to live nodes too.

The main obstacle here is we don't know how many keys will be in the new sstable ahead of time, which we need to size the bloom filter correctly. We can solve this by including that information (or a close approximation) in the stream setup -- the source node can calculate that without hitting disk from the in-memory index summary."
CASSANDRA-2660,BRAF.sync() bug can cause massive commit log write magnification,"This was discovered, fixed and tested on 0.7.5. Cursory examination shows it should still be an issue on trunk/0.8. If people otherwise agree with the patch I can rebase if necessary.

Problem:

BRAF.flush() is actually broken in the sense that it cannot be called without close co-operation with the caller. rebuffer() does the co-op by adjusting bufferOffset and validateBufferBytes appropriately, by sync() doesn't. This means sync() is broken, and sync() is used by the commit log.

The attached patch moves the bufferOffset/validateBufferBytes handling out into resetBuffer() and has both flush() and rebuffer() call that. This makes sync() safe.

What happened was that for batched commit log mode, every time sync() was called the data buffered so far would get written to the OS and fsync():ed. But until rebuffer() is called for other reasons as part of the write path, all subsequent sync():s would result in the very same data (plus whatever was written since last time) being re-written and fsync():ed again. So first you write+fsync N bytes, then N+N1, then N+N1+N2... (each N being a batch), until at some point you trigger a rebuffer() and it starts all over again.

The result is that you see *a lot* more writes to the commit log than are in fact written to the BRAF. And these writes translate into actual real writes to the underlying storage device due to fsync(). We had crazy numbers where we saw spikes upwards of 80 mb/second where the actual throughput was more like ~ 1 mb second of data to the commit log.

(One can make a possibly weak argument that it is also functionally incorrect as I can imagine implementations where re-writing the same blocks does copy-on-write in such a way that you're not necessarily guaranteed to see before-or-after data, particularly in case of partial page writes. However that's probably not a practical issue.)

Worthy of noting is that this probably causes added difficulties in fsync() latencies since the average fsync() will contain a lot more data. Depending on I/O scheduler and underlying device characteristics, the extra writes *may* not have a detrimental effect, but I think it's pretty easy to point to cases where it will be detrimental - in particular if the commit log is on a non-battery backed drive. Even with a nice battery backed RAID with the commit log on, the size of the writes probably contributes to difficulty in making the write requests propagate down without being starved by reads (but this is speculation, not tested, other than that I've observed commit log writer starvation that seemed excessive).

This isn't the first subtle BRAF bug. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)
"
CASSANDRA-2654,Work around native heap leak in sun.nio.ch.Util affecting IncomingTcpConnection,"NIO's leaky, per-thread caching of direct buffers in combination with IncomingTcpConnection's eager buffering of messages leads to leakage of large amounts of native heap. Details in [1]. More on the root cause in [2]. Even though it doesn't fix the leak, attached patch has been found to alleviate the problem by keeping the size of each direct buffer modest.

"
CASSANDRA-2635,make cache skipping optional,"We've applied this patch locally in order to turn of page skipping; not completely but only for compaction/repair situations where it can be directly detrimental in the sense of causing data to become cold even though your entire data set fits in memory.

It's better than completely disabling DONTNEED because the cache skipping does make sense and has no relevant (that I can see) detrimental effects in some cases, like when dumping caches.

The patch is against 0.7.5 right now but if the change is desired I can make a patch for trunk. Also, the name of the configuration option is dubious since saying 'false' does not actually turn it off completely. I wasn't able to figure out a good name that conveyed the functionality in a short brief name however.

A related concern as discussed in CASSANDRA-1902 is that the cache skipping isn't fsync:ing and so won't work reliably on writes. If the feature is to be retained that's something to fix in a different ticket.

A question is also whether to retain the default to true or change it to false. I'm kinda leaning to false since it's detrimental in the ""easy"" cases of little data. In ""big"" cases with lots of data people will have to think and tweak anyway, so better to put the burden on that end.
"
CASSANDRA-2619,secondary index not dropped until restart,"when dropping the secondary index (via cassandra-cli), the describe keyspace still shows the Built index entry. Only after a restart of the CassandraDaemon then the Built Index entry is gone. This seems indicate a problem with the index not really been dropped completed.

to test, use a single node, create an index, then drop it from the cli (issue an update column family ... with metadata fields but not the index info)

below is the original:

  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: path
          Index Type: KEYS{color}
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: sentinel
          Index Type: KEYS{color}

issue an update:
{noformat}

[default@unknown] use cfs;
Authenticated to keyspace: cfs
[default@cfs] update column family inode with comparator=BytesType and column_metadata=[{column_name:70617468, validation_class:BytesType}, {column_name:73656e74696e656c,validation_class:BytesType}];
fca46d00-783c-11e0-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

describe the keyspace again:
Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType

*notice the red line on Built Indexes*

restart CassandraDaemon, describe again:

Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: []{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType


on another note, upon re-create the index, it does not appear the index is actually rebuilt. There is no need to restart CassandraDaemon for the Built Index to show up from the describe. But the update goes very fast. We could tell the index is not being rebuilt because we were getting NPE from:

{noformat}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1647)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1594)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
{noformat}
and after re-create the index, the exception resurface (the exception does not surface upon drop).

If we drop the index files and remove them, then re-create the index, the NPE is resolved: 

{noformat}
$ find /var/lib/cassandra/data/cfs -name ""*path*"" -o -name ""*sentinel* -exec rm {} \;""
{noformat}"
CASSANDRA-2594,run cassandra under numactl --interleave=all,"By default, Linux attempts to be smart about memory allocations such that data is close to the NUMA node on which it runs. For big database type of applications, this is not the best thing to do if the priority is to avoid disk I/O. In particular with Cassandra, we're heavily multi-threaded anyway and there is no particular reason to believe that one NUMA node is ""better"" than another.

Consequences of allocating unevenly among NUMA nodes can include excessive page cache eviction when the kernel tries to allocate memory - such as when restarting the JVM.

With that briefly stated background, I propse the following patch to make the Cassandra script run Cassandra with numactl --interleave=all if numactl seems to be available.
"
CASSANDRA-2590,row delete breaks read repair,"related to CASSANDRA-2589 

Working at CL ALL can get inconsistent reads after row deletion. Reproduced on the 0.7 and 0.8 source. 

Steps to reproduce:

# two node cluster with rf 2 and HH turned off
# insert rows via cli 
# flush both nodes 
# shutdown node 1
# connect to node 2 via cli and delete one row
# bring up node 1
# connect to node 1 via cli and issue get with CL ALL 
# first get returns the deleted row, second get returns zero rows.

RowRepairResolver.resolveSuperSet() resolves a local CF with the old row columns, and the remote CF which is marked for deletion. CF.resolve() does not pay attention to the deletion flags and the resolved CF has both markedForDeletion set and a column with a lower timestamp. The return from resolveSuperSet() is used as the return for the read without checking if the cols are relevant. 

Also when RowRepairResolver.mabeScheduleRepairs() runs it sends two mutations. Node 1 is given the row level deletation, and Node 2 is given a mutation to write the old (and now deleted) column from node 2. I have some log traces for this if needed. 

A quick fix is to check for relevant columns in the RowRepairResolver, will attach shortly.    "
CASSANDRA-2589,row deletes do not remove columns,"When a row delete is issued CF.delete() sets the localDeletetionTime and markedForDeleteAt values but does not remove columns which have a lower time stamp. As a result:

# Memory which could be freed is held on to (prob not too bad as it's already counted)
# The deleted columns are serialised to disk, along with the CF info to say they are no longer valid. 
# NamesQueryFilter and SliceQueryFilter have to do more work as they filter out the irrelevant columns using QueryFilter.isRelevant()
# Also columns written with a lower time stamp after the deletion are added to the CF without checking markedForDeletionAt.


This can cause RR to fail, will create another ticket for that and link. This ticket is for a fix to removing the columns. 

Two options I could think of:

# Check for deletion when serialising to SSTable and ignore columns if the have a lower timestamp. Otherwise leave as is so dead columns stay in memory. 
# Ensure at all times if the CF is deleted all columns it contains have a higher timestamp. 
## I *think* this would include all column types (DeletedColumn as well) as the CF deletion has the same effect. But not sure.
## Deleting (potentially) all columns in delete() will take time. Could track the highest timestamp in the CF so the normal case of deleting all cols does not need to iterate. 
 


"
CASSANDRA-2554,Move gossip heartbeats [back] to its own thread,"Gossip heartbeat *really* needs to run every 1s or other nodes may mark us down. But gossip currently shares an executor thread with other tasks.

I see at least two of these could cause blocking: hint cleanup post-delivery and flush-expired-memtables, both of which call forceFlush which will block if the flush queue + threads are full.

We've run into this before (CASSANDRA-2253); we should move Gossip back to its own dedicated executor or it will keep happening whenever someone accidentally puts something on the ""shared"" executor that can block."
CASSANDRA-2528,NPE from PrecompactedRow,"received a NPE from trunk (0.8) on PrecompactedRow:

ERROR [CompactionExecutor:2] 2011-04-21 17:21:31,610 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:86)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


size of data in /var/lib/cassandra is 11G on this, but there is also report that 1.7G also see the same.

data was previously populated from 0.7.4 cassandra

added debug logging, not sure how much this help (this is logged before the exception.)

 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,588 CompactionManager.java (line 534) Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-7-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-6-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-9-Data.db')]
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-10-Data.db   : 256
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-7-Data.db   : 512
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-6-Data.db   : 768
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-8-Data.db   : 1024
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-9-Data.db   : 1280
 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,609 CompactionIterator.java (line 185) Major@1181554512(cfs, inode.path, 523/10895) now compacting at 16777 bytes/ms.
"
CASSANDRA-2463,Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers,"Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate [n] contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the ""release valve"" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).

This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.

I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.

Here's the base 0.7.4 release:
http://cl.ly/413g2K06121z252e2t10

Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.

Here's the 0.7.4 release with this patch applied:
http://cl.ly/050I1g26401B1X0w3s1f

This graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.

The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions."
CASSANDRA-2459,move SSTableScanner to use SSTableReader.dfile instead of BRAF,"That can give us the following benefits:

a). don't need to skip a cache because we will be operating on memory mappings
b). better performance (not copying data between kernel and user buffers as effect gained from using memory mapped segments, avoiding time operating on the kernel mode (+ time for switching context and read-ahead pressure) which BRAF involves)
c). less impact on the live-reads
d). less garbage will be created
e). less file descriptors opened
"
CASSANDRA-2454,Possible deadlock for counter mutations,"{{StorageProxy.applyCounterMutation}} is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. If there are more than a few concurrent mutations, this can lead to deadlock."
CASSANDRA-2451,Make clean compactions cleanup the row cache,"We uselessly keep in cache keys that are cleanup, which is not a big deal because they will get expunged eventually but there is no point in wasting the memory in the meantime."
CASSANDRA-2449,Deprecate or modify per-cf memtable sizes in favor of the global threshold,"The new memtable_total_space_in_mb setting is an excellent way to cap memory usage for memtables, and one could argue that it should replace the per-cf memtable sizes entirely. On the other hand, people may still want a knob to tune to flush certain cfs less frequently.

I think a best of both worlds approach might be to deprecate the memtable_(throughput|operations) settings, and replace them with a preference value, which controls the relative memory usage of one CF versus another (all CFs at 1 would mean equal preference). For backwards compatibility, we could continue to read from the _throughput value and treat it as the preference value, while logging a warning."
CASSANDRA-2444,Remove checkAllColumnFamilies on startup,"We've ran into many times where we do not want compaction to run right away against CFs when booting up a node. If the node needs to compact, it will do so at the first flush"
CASSANDRA-2427,Heuristic or hard cap to prevent fragmented commit logs from bringing down the server,"Widely divergent write rates on column families can cause the commit log segments to fragment. In some cases we have seen the commit log partition overrun.

One solution here would be to create a heuristic for segment fragmentation to trigger a flush (commit log segments/memtable) or simply track the free disk space and force a global flush when the disk gets to 80% capacity."
CASSANDRA-2424,Failed compactation doesn't delete temporary files causing hd to fill,"Hi,

We seem to have a few incorrect keys in one of our columns (Seems to be a memory bit flip). Cassandra will try to compact and compact those tables again but won't delete any temporary files.



 INFO [CompactionExecutor:1] 2011-04-05 17:52:45,494 SSTableWriter.java (line 108) Last written key : DecoratedKey(dsearch_1300960988716456<398199492444161_1301855754714_4564397053271578441, 647365617263685f313330303936303938383731363435363c3339383139393439323434343136315f313330313835353735343731345f34353634333937303533323731353738343431)
 INFO [CompactionExecutor:1] 2011-04-05 17:52:45,521 SSTableWriter.java (line 109) Current key : DecoratedKey(dsearch_13009609887164564398199492444161_1301855754760_4564395937046043595, 647365617263685f31333030393630393838373136343536343339383139393439323434343136315f313330313835353735343736305f34353634333935393337303436303433353935)
 INFO [CompactionExecutor:1] 2011-04-05 17:52:45,521 SSTableWriter.java (line 110) Writing into file /cassandra/data/table_userentries/table_userentries-tmp-f-3550-Data.db
ERROR [CompactionExecutor:1] 2011-04-05 17:52:45,522 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:452)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)

		

ls -l /cassandra/data/table_userentries/table_userentries-tmp*
-rw-r--r-- 1 root root  13G 2011-04-05 16:09 /cassandra/data/table_userentries/table_userentries-tmp-f-3507-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:09 /cassandra/data/table_userentries/table_userentries-tmp-f-3507-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:21 /cassandra/data/table_userentries/table_userentries-tmp-f-3512-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:21 /cassandra/data/table_userentries/table_userentries-tmp-f-3512-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:33 /cassandra/data/table_userentries/table_userentries-tmp-f-3517-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:33 /cassandra/data/table_userentries/table_userentries-tmp-f-3517-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3521-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3521-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 16:55 /cassandra/data/table_userentries/table_userentries-tmp-f-3527-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 16:55 /cassandra/data/table_userentries/table_userentries-tmp-f-3527-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:05 /cassandra/data/table_userentries/table_userentries-tmp-f-3531-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:06 /cassandra/data/table_userentries/table_userentries-tmp-f-3531-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3535-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3535-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3540-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3540-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:39 /cassandra/data/table_userentries/table_userentries-tmp-f-3545-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:40 /cassandra/data/table_userentries/table_userentries-tmp-f-3545-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 17:52 /cassandra/data/table_userentries/table_userentries-tmp-f-3550-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 17:52 /cassandra/data/table_userentries/table_userentries-tmp-f-3550-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:04 /cassandra/data/table_userentries/table_userentries-tmp-f-3556-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:04 /cassandra/data/table_userentries/table_userentries-tmp-f-3556-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:15 /cassandra/data/table_userentries/table_userentries-tmp-f-3562-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:15 /cassandra/data/table_userentries/table_userentries-tmp-f-3562-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3566-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3566-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:40 /cassandra/data/table_userentries/table_userentries-tmp-f-3572-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:40 /cassandra/data/table_userentries/table_userentries-tmp-f-3572-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 18:53 /cassandra/data/table_userentries/table_userentries-tmp-f-3577-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 18:53 /cassandra/data/table_userentries/table_userentries-tmp-f-3577-Index.db
-rw-r--r-- 1 root root  13G 2011-04-05 19:06 /cassandra/data/table_userentries/table_userentries-tmp-f-3582-Data.db
-rw-r--r-- 1 root root 512M 2011-04-05 19:07 /cassandra/data/table_userentries/table_userentries-tmp-f-3582-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 19:29 /cassandra/data/table_userentries/table_userentries-tmp-f-3592-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 19:29 /cassandra/data/table_userentries/table_userentries-tmp-f-3592-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 19:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3598-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 19:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3598-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 19:56 /cassandra/data/table_userentries/table_userentries-tmp-f-3602-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 19:56 /cassandra/data/table_userentries/table_userentries-tmp-f-3602-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 20:12 /cassandra/data/table_userentries/table_userentries-tmp-f-3606-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 20:12 /cassandra/data/table_userentries/table_userentries-tmp-f-3606-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 20:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3612-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 20:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3612-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 20:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3617-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 20:44 /cassandra/data/table_userentries/table_userentries-tmp-f-3617-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 21:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3622-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 21:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3622-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 21:23 /cassandra/data/table_userentries/table_userentries-tmp-f-3627-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 21:23 /cassandra/data/table_userentries/table_userentries-tmp-f-3627-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 21:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3635-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 21:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3635-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 22:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3646-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 22:16 /cassandra/data/table_userentries/table_userentries-tmp-f-3646-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 22:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3658-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 22:48 /cassandra/data/table_userentries/table_userentries-tmp-f-3658-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 23:22 /cassandra/data/table_userentries/table_userentries-tmp-f-3672-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 23:21 /cassandra/data/table_userentries/table_userentries-tmp-f-3672-Index.db
-rw-r--r-- 1 root root  19G 2011-04-05 23:53 /cassandra/data/table_userentries/table_userentries-tmp-f-3685-Data.db
-rw-r--r-- 1 root root 736M 2011-04-05 23:52 /cassandra/data/table_userentries/table_userentries-tmp-f-3685-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 00:29 /cassandra/data/table_userentries/table_userentries-tmp-f-3698-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 00:28 /cassandra/data/table_userentries/table_userentries-tmp-f-3698-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 01:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3712-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 01:01 /cassandra/data/table_userentries/table_userentries-tmp-f-3712-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 01:36 /cassandra/data/table_userentries/table_userentries-tmp-f-3723-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 01:36 /cassandra/data/table_userentries/table_userentries-tmp-f-3723-Index.db
-rw-r--r-- 1 root root  19G 2011-04-06 02:08 /cassandra/data/table_userentries/table_userentries-tmp-f-3735-Data.db
-rw-r--r-- 1 root root 736M 2011-04-06 02:08 /cassandra/data/table_userentries/table_userentries-tmp-f-3735-Index.db
-rw-r--r-- 1 root root  13G 2011-04-06 02:32 /cassandra/data/table_userentries/table_userentries-tmp-f-3742-Data.db
-rw-r--r-- 1 root root 512M 2011-04-06 02:32 /cassandra/data/table_userentries/table_userentries-tmp-f-3742-Index.db
-rw-r--r-- 1 root root 4.5G 2011-04-06 02:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3748-Data.db
-rw-r--r-- 1 root root 168M 2011-04-06 02:42 /cassandra/data/table_userentries/table_userentries-tmp-f-3748-Index.db


	"
CASSANDRA-2419,Risk of counter over-count when recovering commit log,"When a memtable was flush, there is a small delay before the commit log replay position gets updated. If the node fails during this delay, all the updates of this memtable will be replay during commit log recovery and will end-up being over-counts."
CASSANDRA-2413,Reduce default memtable size,"I'm going to wimp out on targeting CASSANDRA-2006 for 0.7.5 so to mitigate OOMing by newcomers let's reduce the default memtable size -- what we have now predates indexes, which can dramatically increase memory requirements."
CASSANDRA-2404,if out of disk space reclaim compacted SSTables during memtable flush,"During compaction if there is not enough disk space we invoke GC to reclaim unused space.

During memtable and binary memtable flush we just error out if there is not enough disk space to flush the table. 

Can we make cfs.createFlushWriter() use the same logic as Table.getDataFileLocation() to reclaim space if needed?"
CASSANDRA-2381,orphaned data files may be created during migration race,"We try to prevent creating orphans by locking Table.flusherLock in maybeSwitchMemtable and the Migration process, but since the actual writing is done asynchronously in Memtable.writeSortedContents there is a race window, where we acquire lock in maybeSwitch, we're not dropped so we queue the flush and release the lock, Migration does the drop, then Memtable writes itself out."
CASSANDRA-2350,Races between schema changes and StorageService operations,"I only tested this on 0.7.0, but it judging by the 0.7.3 code (latest I've looked at) the same thing should happen.

The case in particular that I ran into is this: I force a compaction for all CFs in a keyspace, and while the compaction is happening I add another CF to the keyspace. I get the following exception because the underlying set of CFs has changed while being iterated over.

{noformat}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
        at java.util.HashMap$ValueIterator.next(Unknown Source)
        at java.util.Collections$UnmodifiableCollection$1.next(Unknown Source)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source) 
{noformat}

The problem is a little more fundamental than that, though, as I believe any schema change of CFs in the keyspace during one of these operations (e.g. flush, compaction, etc) has the potential to cause a race. I'm not sure what would happen if the set of CFs to compact was acquired and one of them was dropped before it had been compacted."
CASSANDRA-2330,Queue indexes for flush before the parent,"Secondary indexes flush when the parent does.  This has an unfortunate side effect: a single CF flushing with a single secondary index fills the flush queue and blocks further writes until the first one completes.  A simple but naive optimization here would be to queue the indexes before the parent since they are generally going to be smaller, and thus flush faster, reducing the amount of time writes are blocked."
CASSANDRA-2327,Table.flusherLock is static final.. remove static.,"I see read and write latency spike when the system tables are flushing(according to opscentral).... 

Only reason which i can come-up with is probably because of the Table.flusherLock is static final... i think it should not be static because the flush is per keyspace and this lock will lock all the read operations because one table is going through a flush."
CASSANDRA-2317,Column family deletion time is not always reseted after gc_grace,"Follow up of CASSANDRA-2305.
Reproducible (thanks to Jeffrey Wang) by: 

Create a CF with gc_grace_seconds = 0 and no row cache.
Insert row X, col A with timestamp 0.
Insert row X, col B with timestamp 2.
Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
Wait 1 second.
Force flush and compaction.
Insert row X, col A with timestamp 0.
Read row X, col A (see nothing)."
CASSANDRA-2313,CommutativeRowIndexer always read full row in memory,"CommutativeRowIndexer use CFSerializer.deserializeColumns() that read the full row in memory. We should use PreCompactedRow/LazilyCompactedRow instead to avoid this on huge row.

As an added benefit, using PreCompactedRow will avoid a current seek back to write the row size."
CASSANDRA-2305,Tombstoned rows not purged from cache after gcgraceseconds,"From email to list:

I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):

insert row X with timestamp T
delete row X with timestamp T+1
force flush + compaction
insert row X with timestamp T

My understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).

I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks."
CASSANDRA-2297,UnsupportedOperationException: Overflow in bytesPastMark(..),"I hit the following exception on a row that was more than 60GB.  
The row has column families of super column type.

This problem is discussed by the following thread.  
http://www.mail-archive.com/dev@cassandra.apache.org/msg01881.html

{code}
ERROR [HintedHandoff:1] 2011-02-26 18:49:35,708 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.bytesPastMark(BufferedRandomAccessFile.java:477)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:120)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:1)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1290)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$1(HintedHandOffManager.java:262)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}"
CASSANDRA-2289,Replicate on write NPE for empty row,Replicate on write will throw a NPE for the first write to a row.
CASSANDRA-2279,Tombstones not collected post-repair,"The keys would only show up in sstables2json and look like this:

(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:34am 
===> /opt/cassandra/bin/sstable2json Queue-2527-Data.db -k waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b 
2011-02-23 07:24:43,710 INFO [org.apache.cassandra.config.DatabaseDescriptor] - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap 
2011-02-23 07:24:43,972 INFO [org.apache.cassandra.io.SSTableReader] - Opening /opt/cassandra/storage/queue/data/Panama/Queue-2527-Data.db 
{ 
""waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b"": [] 
} 
(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:44am 
===>

The steps that I took to reproduce it were:
Create a keyspace, column family, and a key
Delete the key on Node 1 using the cli (del cf['key'];)
Flush 
Repair on a cluster with more than 1 node
Wait GCSeconds 
Compact
And the empty row would appear on Node 2

However, when I was able to get rid of the empty rows, I was following these steps on a single machine: 
Create a keyspace, column family, and a key
Delete the key
Flush
Sample write (writing to some temporary key)
Deleting the attribute to that temporary key (not the entire key)
Flush
Compact

or these steps:
Create a keyspace, column family, and a key
Delete the key
Flush 
Wait GCseconds
Compact

"
CASSANDRA-2270,nodetool info NPE when node isn't fully booted,"Running ""nodetool -h 127.0.0.1 info"" when the node is not yet ready throw a NPE.

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.getCurrentGenerationNumber(Gossiper.java:313)
        at org.apache.cassandra.service.StorageService.getCurrentGenerationNumber(StorageService.java:1239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
"
CASSANDRA-2261,"During Compaction, Corrupt SSTables with rows that cause failures should be identified and blacklisted.","When a compaction of a set of SSTables fails because of corruption it will continue to try to compact that SSTable causing pending compactions to build up.

One way to mitigate this problem would be to log the error, then identify the specific SSTable that caused the failure, subsequently blacklisting that SSTable and ensuring that it is no longer included in future compactions. For this we could simply store the problematic SSTable's name in memory.

If it's not possible to identify the SSTable that caused the issue, then perhaps blacklisting the (ordered) permutation of SSTables to be compacted together is something that can be done to solve this problem in a more general case, and avoid issues where two (or more) SSTables have trouble compacting a particular row. For this option we would probably want to store the lists of the bad combinations in the system table somewhere s.t. these can survive a node failure (there have been a few cases where I have seen a compaction cause a node failure).

"
CASSANDRA-2228,Race conditions when reinitialisating nodes (OOM + Nullpointer),"I had a corrupt system table which wouldn't compact anymore and I deleted the files and restarted cassandra and let it take the same token/ip address.

I experienced the same errors when I'm adding a newly installed node under the same token/ip address before calling repair.

1)
After a few seconds/minutes, I get a OOM error:


 INFO [FlushWriter:1] 2011-02-23 16:40:28,958 Memtable.java (line 164) Completed flushing /cassandra/data/system/Schema-f-15-Data.db (8037 bytes)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 Migration.java (line 133) Applying migration 3e30e76b-1e3f-11e0-8369-5a9c1faed4ae Add keyspace: table_userentriesrep factor:3rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@58925d9[cfId=1024,tableName=table_userentries,cfName=table_userentries,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}], org.apache.cassandra.config.CFMetaData@11ab7246[cfId=1025,tableName=table_userentries,cfName=table_userentries_meta,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}]}
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Migrations at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-23 16:40:28,966 Memtable.java (line 157) Writing Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Schema at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,967 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Schema@139610466(8370 bytes, 15 operations)
 INFO [ScheduledTasks:1] 2011-02-23 16:40:28,972 StatusLogger.java (line 89) table_sourcedetection.table_sourcedetection                 0,0                 0/0            0/200000
ERROR [FlushWriter:1] 2011-02-23 16:41:01,240 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:75)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:158)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:51)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:176)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)





2) If I restart then, I'm getting an Nullpointer exception. The OOM error will only appear once.

ERROR [main] 2011-02-23 16:42:32,782 AbstractCassandraDaemon.java (line 333) Exception encountered during startup.
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:925)
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:105)
        at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:161)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:185)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)


Killing and restarting the node multiple times will eventually ""fix"" these errors.


Steps to reproduce. Remove complete data directory and restart node with same token/ip.

"
CASSANDRA-2183,memtable_flush_after_mins setting not working,"We have observed the behavior that memtable_flush_after_mins setting not working occasionally.   After some testing and code digging, we finally figured out what going on.
The memtable_flush_after_mins won't work on certain condition with current implementation in Cassandra.

In org.apache.cassandra.db.Table,  the scheduled flush task is setup by the following code during construction.

------------------------------------------------------------------------------------------------------------------
int minCheckMs = Integer.MAX_VALUE;
       
for (ColumnFamilyStore cfs : columnFamilyStores.values())  
{
    minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
}

Runnable runnable = new Runnable()
{
   public void run()
   {
       for (ColumnFamilyStore cfs : columnFamilyStores.values())
       {
           cfs.forceFlushIfExpired();
       }
   }
};
flushTask = StorageService.scheduledTasks.scheduleWithFixedDelay(runnable, minCheckMs, minCheckMs, TimeUnit.MILLISECONDS);
------------------------------------------------------------------------------------------------------------------------------

Now for our application, we will create a keyspacewithout without any columnfamily first.  And only add needed columnfamily later depends on request.

However, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with Integer.MAX_VALUE ms
since there is no columnfamily yet.

Later when you add columnfamily to this empty keyspace, the initCf() method in Table.java doesn't check whether the scheduled flush check task interval need
to be updated or not.   To fix this, we'd need to restart the Cassandra after columnfamily added into the keyspace. 

I would suggest that add additional logic in initCf() method to recreate a scheduled flush check task if needed.
"
CASSANDRA-2178,Memtable Flush writers doesn't actually flush in parallel,"The flushWriter JMXEnabledThreadPoolExecutor sets the core pool min to 1, and sets the LBQ to DatabaseDescriptor.getFlushWriters(). Increasing memtable_flush_writers should allow us to flush more in parallel. The pool will not grow until LBQ fills up to DatabaseDescriptor.getFlushWriters(). "
CASSANDRA-2175,make key cache preheating use less memory,"CASSANDRA-1878 pre-heats the key cache post-compaction so latency doesn't suffer while warming the cache back up.  This can double the memory used temporarily; for a large key cache, this can have a substantial impact.

For now a boolean on/off is probably the best we can do.  With http://code.google.com/p/concurrentlinkedhashmap/issues/detail?id=21 though, we could say ""preheat the hottest X keys."""
CASSANDRA-2171,Record and expose flush rate per CF,"In order to automatically throttle compaction to some multiple of the flush rate, we need to record the flush rate across the system. Since this might be useful information on a per CF basis, this ticket will deal with recording the flush rate in the CFStore object, and exposing it via JMX."
CASSANDRA-2169,user created with debian packaging is unable to increase memlock,"To reproduce:
- Install a fresh copy of ubuntu 10.04.
- Install sun's java6 jdk.
- Install libjna-java 3.2.7 into /usr/share/java.
- Install cassandra 0.7.0 from the apache debian packages.
- Start cassandra using /etc/init.d/cassandra
In the output.log there will be the following error:
{quote}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
{quote}
This shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'.

I tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf.  I was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but I could never get the init.d script to work and get beyond that error.

Based on all the trial and error, I think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'."
CASSANDRA-2156,Compaction Throttling,"Compaction is currently relatively bursty: we compact as fast as we can, and then we wait for the next compaction to be possible (""hurry up and wait"").

Instead, to properly amortize compaction, you'd like to compact exactly as fast as you need to to keep the sstable count under control.

For every new level of compaction, you need to increase the rate that you compact at: a rule of thumb that we're testing on our clusters is to determine the maximum number of buckets a node can support (aka, if the 15th bucket holds 750 GB, we're not going to have more than 15 buckets), and then multiply the flush throughput by the number of buckets to get a minimum compaction throughput to maintain your sstable count.

Full explanation: for a min compaction threshold of {{T}}, the bucket at level {{N}} can contain {{SsubN = T^N}} 'units' (unit == memtable's worth of data on disk). Every time a new unit is added, it has a {{1/SsubN}} chance of causing the bucket at level N to fill. If the bucket at level N fills, it causes {{SsubN}} units to be compacted. So, for each active level in your system you have {{SubN * 1 / SsubN}}, or {{1}} amortized unit to compact any time a new unit is added."
CASSANDRA-2152,Encryption options are not validated correctly in DatabaseDescriptor,Missing configuration for encryption_options introduced via CASSANDRA-1567 result in an obtuse NPE from MessagingService
CASSANDRA-2144,Don't slurp rows into memory during export,"SSTableExport uses SSTableIdentityIterator.getColumnFamilyWithColumns, which reads a full row into memory."
CASSANDRA-2105,Fix the read race condition in CFStore for counters ,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
"
CASSANDRA-2072,Race condition during decommission,"Occasionally when decommissioning a node, there is a race condition that occurs where another node will never remove the token and thus propagate it again with a state of down.  With CASSANDRA-1900 we can solve this, but it shouldn't occur in the first place.

Given nodes A, B, and C, if you decommission B it will stream to A and C.  When complete, B will decommission and receive this stacktrace:

ERROR 00:02:40,282 Fatal exception in thread Thread[Thread-5,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91

At this point A will show it is removing B's token, but C will not and instead its failure detector will report that B is dead, and nodetool ring on C shows B in a leaving/down state.  In another gossip round, C will propagate this state back to A."
CASSANDRA-2043,remove memtable throughput configuration,"Now that we can set memtable thresholds on a per-CF basis, having two sizing thresholds is more confusing than helpful, especially for new users who see the throughput threshold and think that it accurately represents how much memory a memtable will use"
CASSANDRA-2008,CLI help incorrect in places,"Found some errors in the CLI help, such as these for create column family.

- memtable_operations: Flush memtables after this many operations
- memtable_throughput: ... or after this many bytes have been written
- memtable_flush_after: ... or after this many seconds

Should be millions of ops, MB's written and minutes not seconds.  Have confirmed thats how the values are used. Will check all the help. "
CASSANDRA-1991,"CFS.maybeSwitchMemtable() calls CommitLog.instance.getContext(), which may block, under flusher lock write lock","While investigate CASSANDRA-1955 I realized I was seeing very poor latencies for reasons that had nothing to do with flush_writers, even when using periodic commit log mode (and flush writers set ridiculously high, 500).

It turns out writes blocked were slow because Table.apply() was spending lots of time (I can easily trigger seconds on moderate work-load) trying to acquire a flusher lock read lock (""flush lock millis"" log printout in the logging patch I'll attach).

That in turns is caused by CFS.maybeSwitchMemtable() which acquires the flusher lock write lock.

Bisecting further revealed that the offending line of code that blocked was:

                    final CommitLogSegment.CommitLogContext ctx = writeCommitLog ? CommitLog.instance.getContext() : null;

Indeed, CommitLog.getContext() simply returns currentSegment().getContext(), but does so by submitting a callable on the service executor. So independently of flush writers, this can block all (global, for all cf:s) writes very easily, and does.

I'll attach a file that is an independent Python script that triggers it on my macos laptop (with an intel SSD, which is why I was particularly surprised) (it assumes CPython, out-of-the-box-or-almost Cassandra on localhost that isn't in a cluster, and it will drop/recreate a keyspace called '1955').

I'm also attaching, just FYI, the patch with log entries that I used while tracking it down.

Finally, I'll attach a patch with a suggested solution of keeping track of the latest commit log with an AtomicReference (as an alternative to synchronizing all access to segments). With that patch applied, latencies are not affected by my trigger case like they were before. There are some sub-optimal > 100 ms cases on my test machine, but for other reasons. I'm no longer able to trigger the extremes.

"
CASSANDRA-1990,Store GC info once per SSTable,"Deleted columns are currently marked with the server time when they were deleted, but since it is not possible to perform GC at a resolution lower than an entire SSTable, it would probably make sense to remove the ""markedForDeleteAt"" field and use the SSTable flush time to indicate it.

It would make the most sense to tackle this during or soon after CASSANDRA-674."
CASSANDRA-1969,Use BB for row cache - To Improve GC performance.,"Java BB.allocateDirect() will allocate native memory out of the JVM and will help reducing the GC pressure in the JVM with a large Cache.
From some of the basic tests it shows around 50% improvement than doing a normal Object cache.

In addition this patch provide the users an option to choose BB.allocateDirect or store everything in the heap."
CASSANDRA-1911,write path should call MessagingService.removeRegisteredCallback,"it would reduce memory overhead to pre-emptively clear the callback when done the way the read path does.

(other IAsyncCallbacks could do this too, but only read/write have enough volume to matter.)"
CASSANDRA-1898,json2sstable should support streaming,"json2sstable loads the entire json file into memory. This is so it can sort the file before creating an sstable. If the file was created using sstable2json and the partitioner isn't changing, this isn't necessary.  For very large files this means json2sstable requires a huge amount of memory.

There should be an option to stream the file. A simple check for out of order keys will prevent writing bad sstables.

This should be possible with the SAX style parser available in our current json library."
CASSANDRA-1896,Improve throughput by adding buffering to the inter-node TCP communication,"The inbound and outbound TCP implementation under org.apache.cassandra.net  does not buffer the socket streams. A simple change in IncomingTcpConnection and OutboundTcpConnection may give a rather big throughput increase. In my tests, I got up t o 30% more out of my cluster. Below is the diff of these two files with buffering included. The diff is over release 0.6.5 but can be quite simply applied also to 0.7. I suggest perhaps to limit the buffered input stream I added in IncomingTcpConnection to 4K. The Outbound implementation can surely be implemented a bit better (remove some of the code I duplicated there).


diff -r apache-cassandra-0.6.5-src/src/java/org/apache/cassandra/net/IncomingTcpConnection.java fix/apache-cassandra-0.6.5-src/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
44c44
<             input = new DataInputStream(new BufferedInputStream(socket.getInputStream()));
---
>             input = new DataInputStream(socket.getInputStream());


diff -r apache-cassandra-0.6.5-src/src/java/org/apache/cassandra/net/OutboundTcpConnection.java fix/apache-cassandra-0.6.5-src/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
77d76
<     	byte[] buf = new byte[4096];
80,112c79,89
<         	int l = 0;
<         	ByteBuffer bb;
<         	while ((bb = queue.peek()) != null && l+bb.limit() < buf.length) { 
<         		bb = take();
<         		System.arraycopy(bb.array(), 0, buf, l, bb.limit());
<         		l += bb.limit();
<         	}
<         	if (l == 0) {
< 	        	bb = take();
< 	            if (bb == CLOSE_SENTINEL)
< 	            {
< 	                disconnect();
< 	                continue;
< 	            }
< 	            if (socket != null || connect())
< 	                writeConnected(bb);
< 	            else
< 	                // clear out the queue, else gossip messages back up.
< 	                queue.clear();
<         	} else {
<         		if (socket != null || connect()) {
< 	        		try {
< 		        		output.write(buf, 0, l);
< 		                if (queue.peek() == null)
< 		                    output.flush();
< 	        		} catch (IOException e) {
< 	                    logger.info(""error writing to "" + endpoint);
< 	                    disconnect();
< 	        		}
<         		} else {
<         			queue.clear();
<         		}
<         	}
---
>             ByteBuffer bb = take();
>             if (bb == CLOSE_SENTINEL)
>             {
>                 disconnect();
>                 continue;
>             }
>             if (socket != null || connect())
>                 writeConnected(bb);
>             else
>                 // clear out the queue, else gossip messages back up.
>                 queue.clear();            

"
CASSANDRA-1883,NPE in get_slice quorum read,"Getting this NPE as of the 2010-12-17 0.7 trunk.  Some data may be corrupt somewhere on a node.  It could be a null key somewhere.

ERROR [pool-1-thread-28] 2010-12-18 12:53:20,411 Cassandra.java (line 2707) Internal error processing get_slice
java.lang.NullPointerException
        at org.apache.cassandra.service.DigestMismatchException.<init>(DigestMismatchException.java:30)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:92)
        at org.apache.cassandra.service.ReadResponseResolver.resolve(ReadResponseResolver.java:43)
        at org.apache.cassandra.service.QuorumResponseHandler.get(QuorumResponseHandler.java:91)
        at org.apache.cassandra.service.StorageProxy.strongRead(StorageProxy.java:362)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:229)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:128)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:225)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:301)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:263)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:2699)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)"
CASSANDRA-1867,sstable2json runs out of memory when trying to export huge rows,"Currently, sstable2json can run out of memory if it encounters a huge row. The problem is that it creates an in-memory String for each row. Proposed solution is to pass the output PrintStream to the serializeRow() and serializeColumns() methods and write to the stream incrementally."
CASSANDRA-1837,Deleted columns are resurrected after a flush,"Easily reproduced with the cli:

{noformat}
[default@unknown] create keyspace testks;
2785d67c-02df-11e0-ac09-e700f669bcfc
[default@unknown] use testks;
Authenticated to keyspace: testks
[default@testks] create column family testcf;
2fbad20d-02df-11e0-ac09-e700f669bcfc
[default@testks] set testcf['test']['foo'] = 'foo';
Value inserted.
[default@testks] set testcf['test']['bar'] = 'bar';
Value inserted.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
[default@testks] del testcf['test'];
row removed.
[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test

1 Row Returned.
{noformat}

Now flush testks and look again:

{noformat}

[default@testks] list testcf;
Using default limit of 100
-------------------
RowKey: test
=> (column=626172, value=626172, timestamp=1291821869120000)
=> (column=666f6f, value=666f6f, timestamp=1291821857320000)

1 Row Returned.
{noformat}"
CASSANDRA-1836,cassandra-cli NullPointerException on EOF,"When using ctrl-D to exit cassandra-cli (a common sort of thing for a prompt-based command-line tool), cassandra-cli does exit, but throws an NPE as well, which does not inspire confidence in its stability:

{noformat}
% cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to cassandra CLI.

Type 'help' or '?' for help. Type 'quit' or 'exit' to quit.
[default@unknown] (*) Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:328)
{noformat}

(the ctrl-D was sent at the position marked by (*).)  The error shows up whether or not one has connected to a keyspace or done work in the cli."
CASSANDRA-1789,Clean up (and make sane) key/row cache loading logspam,"//Start
 INFO 19:18:03,362 Heap size: 1935147008/1994063872
 INFO 19:18:03,366 JNA not found. Native methods will be disabled.
 INFO 19:18:03,376 Loading settings from file:/home/hermes/work/c/cass7/conf/cassandra.yaml
 INFO 19:18:03,533 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 19:18:03,612 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1291079883612.log

//Keycache loading
 *INFO 19:18:03,659 read 0 from saved key cache*
 *INFO 19:18:03,663 read 0 from saved key cache*
 *INFO 19:18:03,664 read 0 from saved key cache*
 *INFO 19:18:03,666 read 0 from saved key cache*
 *INFO 19:18:03,668 read 0 from saved key cache*

//Rowcache loading
 *INFO 19:18:03,671 loading row cache for LocationInfo of system*
 *INFO 19:18:03,671 completed loading (0 ms; 0 keys)  row cache for LocationInfo of system*
 *INFO 19:18:03,672 loading row cache for HintsColumnFamily of system*
 *INFO 19:18:03,672 completed loading (0 ms; 0 keys)  row cache for HintsColumnFamily of system*
 *INFO 19:18:03,673 loading row cache for Migrations of system*
 *INFO 19:18:03,673 completed loading (0 ms; 0 keys)  row cache for Migrations of system*
 *INFO 19:18:03,676 loading row cache for Schema of system*
 *INFO 19:18:03,676 completed loading (0 ms; 0 keys)  row cache for Schema of system*
 *INFO 19:18:03,676 loading row cache for IndexInfo of system*
 *INFO 19:18:03,677 completed loading (0 ms; 0 keys)  row cache for IndexInfo of system*

//The rest
 INFO 19:18:03,730 Couldn't detect any schema definitions in local storage.
 INFO 19:18:03,731 Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
 INFO 19:18:03,735 No commitlog files found; skipping replay
 INFO 19:18:03,783 Upgrading to 0.7. Purging hints if there are any. Old hints will be snapshotted.
 INFO 19:18:03,786 Cassandra version: 0.7.0-rc1-SNAPSHOT
 INFO 19:18:03,786 Thrift API version: 19.4.0
 INFO 19:18:03,795 Loading persisted ring state
 INFO 19:18:03,796 Starting up server gossip
 INFO 19:18:03,803 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1291079883612.log', position=700)
 INFO 19:18:03,804 Enqueuing flush of Memtable-LocationInfo@1249086728(227 bytes, 4 operations)
 INFO 19:18:03,805 Writing Memtable-LocationInfo@1249086728(227 bytes, 4 operations)
 INFO 19:18:03,992 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-1-Data.db (473 bytes)
 WARN 19:18:04,058 Generated random token 109302658160365096146210744235544448283. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
 INFO 19:18:04,059 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1291079883612.log', position=996)
 INFO 19:18:04,060 Enqueuing flush of Memtable-LocationInfo@1940835386(53 bytes, 2 operations)
 INFO 19:18:04,060 Writing Memtable-LocationInfo@1940835386(53 bytes, 2 operations)
 INFO 19:18:04,258 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-2-Data.db (301 bytes)
 INFO 19:18:04,269 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 19:18:04,301 Binding thrift service to localhost/127.0.0.1:9160
 INFO 19:18:04,304 Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO 19:18:04,307 Listening for thrift clients...

The logging here is annoying (and a bit schizophrenic).
Either the keycache loading logging should include as much info as the rowcache loading (time duration, CF/KS names) or it should be a much smaller snippet for both.
The best fix would probably be the line:
 *INFO XX:XX:XX,XXX completed loading (time; keys) row/key cache for CF in KS.*
 ...which would be a log line per CF per saved key/row cache (with more logging on error). 

I don't know if logging that ""0 rows (key or row cache) successfully loaded"" is worth it either, but I could be swayed by an argument for this."
CASSANDRA-1756,DatabaseDescriptor static initialization circular reference when initialized through call to StorageService.instance.initClient ,"In trunk, attempting to invoke StorageService.instance.initClient results in an NPE due to static definition field ordering in StorageService and a circular reference from DatabaseDescriptor back into an uninitialized field (scheduledTasks). Changing the ordering of the static fields such that scheduledTasks is defined before the static partitioner fixes the issue.

I've also marked the scheduledTasks executor as final as it doesn't seem to make sense changing it.

All tests pass with this change locally.

I suspect this hasn't surfaced in tests as calling initServer first in the same JVM will allow later calls to initClient to see the correctly defined scheduledTasks fields.

I'm following the recommended way to do this from ClientOnlyExample, if this isn't the right way to initialize things let me know.
"
CASSANDRA-1748,Flush before repair,"We don't currently flush before beginning a validation compaction, meaning that depending on the state of the memtables, we might end up with content on disk that is as different as a single memtable can make it (potentially, very different)."
CASSANDRA-1715,More schema migration race conditions,"Related to CASSANDRA-1631.

This is still a bug with schema updates to an existing CF, since reloadCf is doing a unload/init cycle. So flushing + compaction is an issue there as well. Here is a stacktrace from during an index creation where it stubbed its toe on an incomplete sstable from an in-progress compaction (path names anonymized):
{code}
INFO [CompactionExecutor:1] 2010-11-02 16:31:00,553 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-8-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='Standard1-e-9-Data.db')]
...
ERROR [MigrationStage:1] 2010-11-02 16:31:10,939 ColumnFamilyStore.java (line 244) Corrupt sstable Standard1-tmp-e-10-<>=[Data.db, Index.db]; skipped
java.io.EOFException
        at org.apache.cassandra.utils.FBUtilities.skipShortByteArray(FBUtilities.java:308)
        at org.apache.cassandra.io.sstable.SSTable.estimateRowsFromIndex(SSTable.java:231)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:286)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:202)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:235)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:443)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:431)
        at org.apache.cassandra.db.Table.initCf(Table.java:335)
        at org.apache.cassandra.db.Table.reloadCf(Table.java:343)
        at org.apache.cassandra.db.migration.UpdateColumnFamily.applyModels(UpdateColumnFamily.java:89)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:158)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:672)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
...
 INFO [CompactionExecutor:1] 2010-11-02 16:31:31,970 CompactionManager.java (line 303) Compacted to Standard1-tmp-e-10-Data.db.  213,657,983 to 213,657,983 (~100% of original) bytes for 626,563 keys.  Time: 31,416ms.
{code}

There is also a race between schema modification and streaming."
CASSANDRA-1702,handle skipping bad rows in LazilyCompacted path,it's easy to handle skipping bad rows during compation in the PreCompacted (merged-in-memory) path and we have done this for a long time.  It is harder in the LazilyCompacted path since we have already started writing data when we discover that some of the source rows cannot be deserialized.  This adds mark/reset to SSTableWriter so compaction can skip back to the beginning in these circumstances.
CASSANDRA-1695,Bootstrapping new node Error in row mutation UnserializableColumnFamilyException Couldn't find cfId,"When auto_bootstraping a new node into an existing cluster with data (auto_bootstrap: true) the new node throws UnserializableColumnFamilyException: Couldn't find cfId. Started with beta3 and then tried latest trunk r1029823.  Created a cluster of 5 nodes all using default settings other than ip addresses, OP partitioner, and data dir locations. started writing data to the cluster. On 6th node changed auto_bootstrap setting to true and started 6th node. Received the cfid row mutation error after gossiper finishes and schema is being shared with new node. 


INFO [main] 2010-11-02 08:56:09,150 AbstractCassandraDaemon.java (line 72) Heap size: 10719985664/10719985664
 INFO [main] 2010-11-02 08:56:09,160 CLibrary.java (line 43) JNA not found. Native methods will be disabled.
 INFO [main] 2010-11-02 08:56:09,167 DatabaseDescriptor.java (line 122) Loading settings from file:/opt/cassandra/0.7.0-beta3/conf/cassandra.yaml
 INFO [main] 2010-11-02 08:56:09,262 DatabaseDescriptor.java (line 173) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2010-11-02 08:56:09,321 CommitLogSegment.java (line 50) Creating new commitlog segment /var/log/cassandra/commitlog/CommitLog-1288706169321.log
 INFO [main] 2010-11-02 08:56:09,406 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [main] 2010-11-02 08:56:09,408 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [main] 2010-11-02 08:56:09,409 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [main] 2010-11-02 08:56:09,410 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [main] 2010-11-02 08:56:09,411 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [main] 2010-11-02 08:56:09,413 ColumnFamilyStore.java (line 504) loading row cache for LocationInfo of system
 INFO [main] 2010-11-02 08:56:09,414 ColumnFamilyStore.java (line 509) completed loading (1 ms; 0 keys)  row cache for LocationInfo of system
 INFO [main] 2010-11-02 08:56:09,414 ColumnFamilyStore.java (line 504) loading row cache for HintsColumnFamily of system
 INFO [main] 2010-11-02 08:56:09,414 ColumnFamilyStore.java (line 509) completed loading (0 ms; 0 keys)  row cache for HintsColumnFamily of system
 INFO [main] 2010-11-02 08:56:09,415 ColumnFamilyStore.java (line 504) loading row cache for Migrations of system
 INFO [main] 2010-11-02 08:56:09,415 ColumnFamilyStore.java (line 509) completed loading (0 ms; 0 keys)  row cache for Migrations of system
 INFO [main] 2010-11-02 08:56:09,417 ColumnFamilyStore.java (line 504) loading row cache for Schema of system
 INFO [main] 2010-11-02 08:56:09,417 ColumnFamilyStore.java (line 509) completed loading (0 ms; 0 keys)  row cache for Schema of system
 INFO [main] 2010-11-02 08:56:09,418 ColumnFamilyStore.java (line 504) loading row cache for IndexInfo of system
 INFO [main] 2010-11-02 08:56:09,418 ColumnFamilyStore.java (line 509) completed loading (1 ms; 0 keys)  row cache for IndexInfo of system
 INFO [main] 2010-11-02 08:56:09,442 DatabaseDescriptor.java (line 408) Couldn't detect any schema definitions in local storage.
 INFO [main] 2010-11-02 08:56:09,442 DatabaseDescriptor.java (line 434) Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
 INFO [main] 2010-11-02 08:56:09,445 CommitLog.java (line 187) No commitlog files found; skipping replay
 INFO [main] 2010-11-02 08:56:09,461 SystemTable.java (line 80) Upgrading to 0.7. Purging hints if there are any. Old hints will be snapshotted.
 INFO [main] 2010-11-02 08:56:09,465 StorageService.java (line 347) Cassandra version: 0.7.0-beta3
 INFO [main] 2010-11-02 08:56:09,465 StorageService.java (line 348) Thrift API version: 19.4.0
 INFO [main] 2010-11-02 08:56:09,472 StorageService.java (line 370) Loading persisted ring state
 INFO [main] 2010-11-02 08:56:09,473 StorageService.java (line 378) Starting up server gossip
 INFO [main] 2010-11-02 08:56:09,478 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=700)
 INFO [main] 2010-11-02 08:56:09,479 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-LocationInfo@648734830(236 bytes, 4 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:09,480 Memtable.java (line 167) Writing Memtable-LocationInfo@648734830(236 bytes, 4 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:09,705 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/LocationInfo-e-1-Data.db
 INFO [main] 2010-11-02 08:56:09,735 StorageService.java (line 394) This node will not auto bootstrap because it is configured to be a seed node.
 WARN [main] 2010-11-02 08:56:09,740 StorageService.java (line 434) Generated random token 4buaxINos8oCXqym. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
 INFO [main] 2010-11-02 08:56:09,740 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=848)
 INFO [main] 2010-11-02 08:56:09,741 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-LocationInfo@1159466336(36 bytes, 1 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:09,741 Memtable.java (line 167) Writing Memtable-LocationInfo@1159466336(36 bytes, 1 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:09,979 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/LocationInfo-e-2-Data.db
 INFO [MigrationStage:1] 2010-11-02 08:56:09,999 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=11469)
 INFO [MigrationStage:1] 2010-11-02 08:56:09,999 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@1518579727(6094 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,000 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=11469)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,000 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@1386432387(2192 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:10,000 Memtable.java (line 167) Writing Memtable-Migrations@1518579727(6094 bytes, 1 operations)
 INFO [main] 2010-11-02 08:56:10,005 Mx4jTool.java (line 73) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [main] 2010-11-02 08:56:10,055 CassandraDaemon.java (line 77) Binding thrift service to /192.168.1.94:9160
 INFO [main] 2010-11-02 08:56:10,057 CassandraDaemon.java (line 91) Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO [main] 2010-11-02 08:56:10,061 CassandraDaemon.java (line 119) Listening for thrift clients...
 INFO [FlushWriter:1] 2010-11-02 08:56:10,168 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/Migrations-e-1-Data.db
 INFO [FlushWriter:1] 2010-11-02 08:56:10,169 Memtable.java (line 167) Writing Memtable-Schema@1386432387(2192 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:10,404 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Schema-e-1-Data.db
 INFO [MigrationStage:1] 2010-11-02 08:56:10,444 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=22551)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,445 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@668531077(6407 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,445 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=22551)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,445 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@1709782292(2375 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:10,450 Memtable.java (line 167) Writing Memtable-Migrations@668531077(6407 bytes, 1 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:10,680 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Migrations-e-2-Data.db
 INFO [FlushWriter:1] 2010-11-02 08:56:10,681 Memtable.java (line 167) Writing Memtable-Schema@1709782292(2375 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:10,851 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/Schema-e-2-Data.db
 INFO [MigrationStage:1] 2010-11-02 08:56:10,853 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [MigrationStage:1] 2010-11-02 08:56:10,855 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=33669)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,855 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@1180026905(6591 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,856 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=33669)
 INFO [FlushWriter:1] 2010-11-02 08:56:10,856 Memtable.java (line 167) Writing Memtable-Migrations@1180026905(6591 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:10,856 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@700712290(2558 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:11,010 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/Migrations-e-3-Data.db
 INFO [FlushWriter:1] 2010-11-02 08:56:11,011 Memtable.java (line 167) Writing Memtable-Schema@700712290(2558 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:11,206 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Schema-e-3-Data.db
 INFO [MigrationStage:1] 2010-11-02 08:56:11,208 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [MigrationStage:1] 2010-11-02 08:56:11,210 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=44966)
 INFO [MigrationStage:1] 2010-11-02 08:56:11,210 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@1711957716(6770 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:11,210 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=44966)
 INFO [FlushWriter:1] 2010-11-02 08:56:11,210 Memtable.java (line 167) Writing Memtable-Migrations@1711957716(6770 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:11,211 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@887430306(2739 bytes, 3 operations)
 INFO [GossipStage:1] 2010-11-02 08:56:11,329 Gossiper.java (line 577) Node /192.168.1.217 is now part of the cluster
 INFO [GossipStage:1] 2010-11-02 08:56:11,337 Gossiper.java (line 577) Node /192.168.1.216 is now part of the cluster
 INFO [GossipStage:1] 2010-11-02 08:56:11,338 Gossiper.java (line 577) Node /192.168.1.219 is now part of the cluster
 INFO [GossipStage:1] 2010-11-02 08:56:11,338 Gossiper.java (line 577) Node /192.168.1.218 is now part of the cluster
 INFO [GossipStage:1] 2010-11-02 08:56:11,338 Gossiper.java (line 577) Node /192.168.1.215 is now part of the cluster
 INFO [FlushWriter:1] 2010-11-02 08:56:11,414 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Migrations-e-4-Data.db
 INFO [FlushWriter:1] 2010-11-02 08:56:11,415 Memtable.java (line 167) Writing Memtable-Schema@887430306(2739 bytes, 3 operations)
 INFO [CompactionExecutor:1] 2010-11-02 08:56:11,420 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data1/system/Migrations-e-1-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Migrations-e-2-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data1/system/Migrations-e-3-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Migrations-e-4-Data.db')]
 INFO [GossipStage:1] 2010-11-02 08:56:11,727 Gossiper.java (line 569) InetAddress /192.168.1.216 is now UP
 INFO [HintedHandoff:1] 2010-11-02 08:56:11,727 HintedHandOffManager.java (line 190) Started hinted handoff for endpoint /192.168.1.216
 INFO [FlushWriter:1] 2010-11-02 08:56:11,727 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/Schema-e-4-Data.db
 INFO [GossipStage:1] 2010-11-02 08:56:11,728 Gossiper.java (line 569) InetAddress /192.168.1.215 is now UP
 INFO [HintedHandoff:1] 2010-11-02 08:56:11,729 HintedHandOffManager.java (line 246) Finished hinted handoff of 0 rows to endpoint /192.168.1.216
 INFO [HintedHandoff:1] 2010-11-02 08:56:11,729 HintedHandOffManager.java (line 190) Started hinted handoff for endpoint /192.168.1.215
 INFO [MigrationStage:1] 2010-11-02 08:56:11,729 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [HintedHandoff:1] 2010-11-02 08:56:11,729 HintedHandOffManager.java (line 246) Finished hinted handoff of 0 rows to endpoint /192.168.1.215
 INFO [MigrationStage:1] 2010-11-02 08:56:11,731 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=57186)
 INFO [MigrationStage:1] 2010-11-02 08:56:11,732 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@1100275801(6953 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:11,732 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=57186)
 INFO [FlushWriter:1] 2010-11-02 08:56:11,732 Memtable.java (line 167) Writing Memtable-Migrations@1100275801(6953 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:11,732 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@353703090(2921 bytes, 3 operations)
 INFO [CompactionExecutor:1] 2010-11-02 08:56:11,745 CompactionManager.java (line 303) Compacted to /var/lib/cassandra/data1/system/Migrations-tmp-e-5-Data.db.  26,394 to 25,995 (~98% of original) bytes for 1 keys.  Time: 315ms.
 INFO [CompactionExecutor:1] 2010-11-02 08:56:11,746 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Schema-e-1-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data1/system/Schema-e-2-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Schema-e-3-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data1/system/Schema-e-4-Data.db')]
 INFO [GossipStage:1] 2010-11-02 08:56:11,938 Gossiper.java (line 569) InetAddress /192.168.1.219 is now UP
 INFO [HintedHandoff:1] 2010-11-02 08:56:11,938 HintedHandOffManager.java (line 190) Started hinted handoff for endpoint /192.168.1.219
 INFO [HintedHandoff:1] 2010-11-02 08:56:11,939 HintedHandOffManager.java (line 246) Finished hinted handoff of 0 rows to endpoint /192.168.1.219
 INFO [FlushWriter:1] 2010-11-02 08:56:12,056 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Migrations-e-6-Data.db
 INFO [FlushWriter:1] 2010-11-02 08:56:12,057 Memtable.java (line 167) Writing Memtable-Schema@353703090(2921 bytes, 3 operations)
ERROR [MutationStage:1] 2010-11-02 08:56:12,135 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1016
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:62)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
 INFO [CompactionExecutor:1] 2010-11-02 08:56:12,140 CompactionManager.java (line 303) Compacted to /var/lib/cassandra/data2/system/Schema-tmp-e-5-Data.db.  11,016 to 10,482 (~95% of original) bytes for 5 keys.  Time: 391ms.
 INFO [FlushWriter:1] 2010-11-02 08:56:12,269 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/Schema-e-6-Data.db
 INFO [MigrationStage:1] 2010-11-02 08:56:12,270 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [MigrationStage:1] 2010-11-02 08:56:12,272 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=68838)
 INFO [MigrationStage:1] 2010-11-02 08:56:12,273 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@1869820356(7125 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:12,273 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=68838)
 INFO [FlushWriter:1] 2010-11-02 08:56:12,273 Memtable.java (line 167) Writing Memtable-Migrations@1869820356(7125 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:12,273 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@1942516741(3098 bytes, 3 operations)
 INFO [GossipStage:1] 2010-11-02 08:56:12,330 Gossiper.java (line 569) InetAddress /192.168.1.218 is now UP
 INFO [HintedHandoff:1] 2010-11-02 08:56:12,330 HintedHandOffManager.java (line 190) Started hinted handoff for endpoint /192.168.1.218
 INFO [HintedHandoff:1] 2010-11-02 08:56:12,331 HintedHandOffManager.java (line 246) Finished hinted handoff of 0 rows to endpoint /192.168.1.218
 INFO [FlushWriter:1] 2010-11-02 08:56:12,473 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Migrations-e-7-Data.db
 INFO [FlushWriter:1] 2010-11-02 08:56:12,474 Memtable.java (line 167) Writing Memtable-Schema@1942516741(3098 bytes, 3 operations)
ERROR [MutationStage:2] 2010-11-02 08:56:12,584 RowMutationVerbHandler.java (line 83) Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1004
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.db.RowMutationSerializer.defreezeTheMaps(RowMutation.java:368)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:378)
        at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:336)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:52)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:62)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
 INFO [FlushWriter:1] 2010-11-02 08:56:12,619 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data1/system/Schema-e-7-Data.db
 INFO [MigrationStage:1] 2010-11-02 08:56:12,621 ColumnFamilyStore.java (line 228) read 0 from saved key cache
 INFO [MigrationStage:1] 2010-11-02 08:56:12,622 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Migrations at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=80679)
 INFO [MigrationStage:1] 2010-11-02 08:56:12,623 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Migrations@461456971(7314 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:12,623 ColumnFamilyStore.java (line 631) switching in a fresh Memtable for Schema at CommitLogContext(file='/var/log/cassandra/commitlog/CommitLog-1288706169321.log', position=80679)
 INFO [FlushWriter:1] 2010-11-02 08:56:12,623 Memtable.java (line 167) Writing Memtable-Migrations@461456971(7314 bytes, 1 operations)
 INFO [MigrationStage:1] 2010-11-02 08:56:12,623 ColumnFamilyStore.java (line 934) Enqueuing flush of Memtable-Schema@1143862280(3281 bytes, 3 operations)
 INFO [FlushWriter:1] 2010-11-02 08:56:12,815 Memtable.java (line 174) Completed flushing /var/lib/cassandra/data2/system/Migrations-e-8-Data.db
 INFO [CompactionExecutor:1] 2010-11-02 08:56:12,816 CompactionManager.java (line 224) Compacting [org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data1/system/Migrations-e-5-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Migrations-e-6-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Migrations-e-7-Data.db'),org.apache.cassandra.io.sstable.SSTableReader(path='/var/lib/cassandra/data2/system/Migrations-e-8-Data.db')]"
CASSANDRA-1675,log which memtable threshold has been hit,"There are three different tunable settings available for memtable sizing. Tuning these is an important task to operations centric people, because it relates to JVM memory management. Currently, the code logs when you have hit one of the three thresholds, but it does not tell you which of the three you have hit.

Attached patches against 0.6 and 0.7 branches add a log message for each of the thresholds, indicating which one has been reached. If you were to find yourself in the somewhat unlikely case of simultaneously hitting more than one, the new code would only tell you which one you hit first, because that's all the current codepath cares about."
CASSANDRA-1665,JMX threads leak in NodeProbe,There is a JMX threads leak in NodeProbe.  It creates and uses a JMXConnector but never calls its close() method.  I am working on a patch which add a close() method to NodeProbe  that calls JMXConnector.close().
CASSANDRA-1608,Redesigned Compaction,"After seeing the I/O issues in CASSANDRA-1470, I've been doing some more thinking on this subject that I wanted to lay out.

I propose we redo the concept of how compaction works in Cassandra. At the moment, compaction is kicked off based on a write access pattern, not read access pattern. In most cases, you want the opposite. You want to be able to track how well each SSTable is performing in the system. If we were to keep statistics in-memory of each SSTable, prioritize them based on most accessed, and bloom filter hit/miss ratios, we could intelligently group sstables that are being read most often and schedule them for compaction. We could also schedule lower priority maintenance on SSTable's not often accessed.

I also propose we limit the size of each SSTable to a fix sized, that gives us the ability to  better utilize our bloom filters in a predictable manner. At the moment after a certain size, the bloom filters become less reliable. This would also allow us to group data most accessed. Currently the size of an SSTable can grow to a point where large portions of the data might not actually be accessed as often.


"
CASSANDRA-1568,nodeprobe help message is missing option to compact specific keyspace,"{noformat}
-                ""%nAvailable commands: ring, info, version, cleanup, compact, cfstats, snapshot [snapshotname], clearsnapshot, "" +
-                ""tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken [status|force]|[token], "" +
+                ""%nAvailable commands: ring, info, version, cleanup, compact [keyspacename], cfstats, snapshot [snapshotname], "" +
+                ""clearsnapshot, tpstats, flush, drain, repair, decommission, move, loadbalance, removetoken [status|force]|[token], "" +
{noformat}"
CASSANDRA-1557,ColumnFamilyStore masking IOException from FileUtils as IOError,"The code in ColumnFamilyStore.snapshot() line 1368 is catching an IOException from the call to FileUtils.createHardLink() and wrapping it in an IOError. However the code in TruncateVerbHandler:56 is looking for the IOException. This can result  in the client not getting a response to a truncate() API call. 

When running on a machine with very low memory I attempted to truncate a CF with few rows, the following error occurred in the logs.

ERROR [MUTATION_STAGE:25] 2010-09-29 16:44:39,341 AbstractCassandraDaemon.java (line 88) Fatal exception in thread Thread[MUTATION_STAGE:25,5,main]
java.io.IOError: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1368)
        at org.apache.cassandra.db.ColumnFamilyStore.truncate(ColumnFamilyStore.java:1511)
        at org.apache.cassandra.db.Table.truncate(Table.java:633)
        at org.apache.cassandra.db.TruncateVerbHandler.doVerb(TruncateVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:50)
        at javautil.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Cannot run program ""ln"": java.io.IOException: error=12, Cannot allocate memory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)
        at org.apache.cassandra.io.util.FileUtils.createHardLinkWithExec(FileUtils.java:263)
        at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:229)
        at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1360)
        ... 7 more
Caused by: java.io.IOException: java.io.IOException: error=12, Cannot allocate memory
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
        at java.lang.ProcessImpl.start(ProcessImpl.java:65)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)
        ... 10 more

On the client I got this:

  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandrapy"", line 846, in truncate
    self.recv_truncate()
  File ""/tech/home//git_home/trojan/trojan/cassandra/Cassandra.py"", line 857, in recv_truncate
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/tech/home//git_home/trojan/trojan/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
<snip>
    chunk = self.read(sz-have)
  File ""/tech/home//git_home/trojan/trojan/thrift/transport/TSocket.py"", line 92, in read
    buff = self.handle.recv(sz)
timeout: timed out"
CASSANDRA-1555,Considerations for larger bloom filters,"To (optimally) support SSTables larger than 143 million keys, we need to support bloom filters larger than 2^31 bits, which java.util.BitSet can't handle directly.

A few options:
* Switch to a BitSet class which supports 2^31 * 64 bits (Lucene's OpenBitSet)
* Partition the java.util.BitSet behind our current BloomFilter
** Straightforward bit partitioning: bit N is in bitset N // 2^31
** Separate equally sized complete bloom filters for member ranges, which can be used independently or OR'd together under memory pressure.

All of these options require new approaches to serialization."
CASSANDRA-1488,Allow index sampling ratio to be configured in storage-conf,"Cassandra keeps a sample of row keys in memory to look up the right block of index entries, to look up row data with.  This is kept in the SSTableReader.indexSummary field.  (See Google's BigTable paper for a high-level description of how this index works; cassandra's is slightly different in the details but the idea is the same.)

We'd like to make IndexSummary.INDEX_INTERVAL configurable in storage-conf.xml, i.e., in the DatabaseDescriptor class.  For 0.6, a global setting is fine."
CASSANDRA-1467,"replication factor exceeds number of endpoints, when attempting to join a new node (but cluster has enough running nodes to fulfill RF)","What happens here the following:
* given a healthy running cluster of 2 nodes (it used to be 3 but I manually killed one down)
* with a Keyspace having a ReplicationFactor of 2
* (this cluster is operational, loaded with data and working well)

 as soon as I want to bring up a new 3rd node:
* the node is detected by the current cluster
* but as soon as it tries to initiate bootstrap sequence it dies with:  replication factor (2) exceeds number of endpoints (1)

I believe this is similar to #1343, but not quite the same, since the keyspace and everything is already created, and I'm not attempting any modification. This is purely bringing a new node up.

One extra tidbit of information, in case it's important...maybe it is not: 
I have only set 1 seed node configured (this is a test setup). And when the new node starts coming up (before the crash) its nodetool ring only reports the seed one.

From the new node coming up (before it crashes):
root@node1-3:~# nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
10.250.106.111  Up     Normal  116.3 GB        0          < = this the only configured seed node

From one of the other running nodes (this is the real status of the cluster):
root@node1-2:~# nodetool -h localhost -p 8080 ring
Address         Status State   Load            Token                                       
                                       113416112894748789872342756657008344878    
10.250.106.111  Up     Normal  116.3 GB        0                                           
10.215.195.81   Up     Normal  116.31 GB       56713727820156410577229101238628035242      
10.246.65.221   Up     Joining 7.63 KB         113416112894748789872342756657008344878     


Here's the full boot trace of the node is trying to join:
Java HotSpot(TM) Client VM warning: Can't detect initial thread stack location - find_vma failed
Create RMI registry on port 8081
Get the platform's MBean server
Initialize the environment map
Create an RMI connector server
Start the RMI connector server on port 8081
 INFO 22:32:50,448 Loading settings from /etc/cassandra/cassandra.yaml
DEBUG 22:32:50,537 Syncing log with a period of 10000
 INFO 22:32:50,538 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
DEBUG 22:32:50,548 setting auto_bootstrap to true
DEBUG 22:32:50,702 Starting CFS Statistics
DEBUG 22:32:50,710 key cache capacity for Statistics is 1
DEBUG 22:32:50,711 Starting CFS Schema
DEBUG 22:32:50,712 key cache capacity for Schema is 1
DEBUG 22:32:50,712 Starting CFS Migrations
DEBUG 22:32:50,713 key cache capacity for Migrations is 1
DEBUG 22:32:50,713 Starting CFS LocationInfo
DEBUG 22:32:50,713 key cache capacity for LocationInfo is 1
DEBUG 22:32:50,714 Starting CFS HintsColumnFamily
DEBUG 22:32:50,714 key cache capacity for HintsColumnFamily is 1
 INFO 22:32:50,736 Couldn't detect any schema definitions in local storage.
 INFO 22:32:50,737 Found table data in data directories. Consider using JMX to call org.apache.cassandra.service.StorageService.loadSchemaFromYaml().
DEBUG 22:32:50,738 opening keyspace system
 INFO 22:32:50,757 Cassandra version: 
 INFO 22:32:50,757 Thrift API version: 10.0.0
 INFO 22:32:50,758 Saved Token not found. Using 113416112894748789872342756657008344878
 INFO 22:32:50,758 Saved ClusterName not found. Using SOMETHING 
 INFO 22:32:50,763 Creating new commitlog segment /mnt/cassandra/commitlog/CommitLog-1283553170763.log
DEBUG 22:32:50,767 Estimating compactions for LocationInfo
DEBUG 22:32:50,768 Estimating compactions for HintsColumnFamily
DEBUG 22:32:50,768 Estimating compactions for Migrations
DEBUG 22:32:50,768 Estimating compactions for Schema
DEBUG 22:32:50,768 Estimating compactions for Statistics
DEBUG 22:32:50,769 Checking to see if compaction of LocationInfo would be useful
DEBUG 22:32:50,769 Checking to see if compaction of HintsColumnFamily would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Migrations would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Schema would be useful
DEBUG 22:32:50,769 Checking to see if compaction of Statistics would be useful
 INFO 22:32:50,779 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=276)
 INFO 22:32:50,782 Enqueuing flush of Memtable-LocationInfo@18721294(192 bytes, 4 operations)
 INFO 22:32:50,783 Writing Memtable-LocationInfo@18721294(192 bytes, 4 operations)
 INFO 22:32:50,897 Completed flushing /mnt/ebs/data/system/LocationInfo-e-1-Data.db
DEBUG 22:32:50,898 Checking to see if compaction of LocationInfo would be useful
DEBUG 22:32:50,898 Discarding 0
DEBUG 22:32:50,899 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=276), column family 0.
DEBUG 22:32:50,899 Marking replay position 276 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
 INFO 22:32:50,908 Starting up server gossip
 INFO 22:32:50,928 Joining: getting load information
 INFO 22:32:50,928 Sleeping 90000 ms to wait for load information...
DEBUG 22:32:50,940 attempting to connect to node1-1.domain.com/10.250.106.111
DEBUG 22:32:51,044 attempting to connect to node1-1.domain.com/10.250.106.111
DEBUG 22:32:51,045 attempting to connect to /10.215.195.81
 INFO 22:32:51,051 Node /10.215.195.81 is now part of the cluster
DEBUG 22:32:51,051 Resetting pool for /10.215.195.81
DEBUG 22:32:51,052 Token 113416112894748789872342756657008344877 removed manually (endpoint was unknown)
 INFO 22:32:51,052 Node /10.250.106.111 is now part of the cluster
DEBUG 22:32:51,053 Resetting pool for /10.250.106.111
DEBUG 22:32:51,053 Node /10.250.106.111 state normal, token 0
DEBUG 22:32:51,053 clearing cached endpoints
DEBUG 22:32:51,171 Applying AddKeyspace from /10.250.106.111
DEBUG 22:32:51,188 Applying migration 77e97d6c-b625-11df-8596-318df8b646e8
 INFO 22:32:51,189 switching in a fresh Memtable for Migrations at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417)
 INFO 22:32:51,189 Enqueuing flush of Memtable-Migrations@18093512(4938 bytes, 1 operations)
 INFO 22:32:51,189 Writing Memtable-Migrations@18093512(4938 bytes, 1 operations)
 INFO 22:32:51,194 switching in a fresh Memtable for Schema at CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417)
 INFO 22:32:51,194 Enqueuing flush of Memtable-Schema@27402470(1784 bytes, 3 operations)
 INFO 22:32:51,274 Completed flushing /mnt/ebs/data/system/Migrations-e-1-Data.db
DEBUG 22:32:51,274 Checking to see if compaction of Migrations would be useful
DEBUG 22:32:51,275 Discarding 2
DEBUG 22:32:51,275 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417), column family 2
.
DEBUG 22:32:51,275 Marking replay position 7417 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
 INFO 22:32:51,275 Writing Memtable-Schema@27402470(1784 bytes, 3 operations)
 INFO 22:32:51,388 Completed flushing /mnt/ebs/data/system/Schema-e-1-Data.db
DEBUG 22:32:51,389 Checking to see if compaction of Schema would be useful
DEBUG 22:32:51,389 Discarding 3
DEBUG 22:32:51,389 discard completed log segments for CommitLogContext(file='/mnt/cassandra/commitlog/CommitLog-1283553170763.log', position=7417), column family 3
.
DEBUG 22:32:51,389 Marking replay position 7417 on commit log CommitLogSegment(/mnt/cassandra/commitlog/CommitLog-1283553170763.log)
DEBUG 22:32:51,406 Starting CFS MyColumnFamily
DEBUG 22:32:51,406 key cache capacity for MyColumnFamily is 200000
 INFO 22:32:51,407 Creating new commitlog segment /mnt/cassandra/commitlog/CommitLog-1283553171407.log
DEBUG 22:32:51,604 attempting to connect to node1-1.domain.com/10.250.106.111
 INFO 22:32:51,622 InetAddress /10.215.195.81 is now UP
 INFO 22:32:51,623 InetAddress /10.250.106.111 is now UP
 INFO 22:32:51,623 Started hinted handoff for endpoint /10.215.195.81
 INFO 22:32:51,631 Finished hinted handoff of 0 rows to endpoint /10.215.195.81
 INFO 22:32:51,632 Started hinted handoff for endpoint /10.250.106.111
 INFO 22:32:51,632 Finished hinted handoff of 0 rows to endpoint /10.250.106.111
DEBUG 22:32:51,918 GC for ParNew: 14 ms, 20561696 reclaimed leaving 149257720 used; max is 902627328
DEBUG 22:32:51,919 GC for ConcurrentMarkSweep: 73 ms, 4178480 reclaimed leaving 8520856 used; max is 902627328
DEBUG 22:32:52,924 Disseminating load info ...
DEBUG 22:32:53,929 attempting to connect to /10.215.195.81
DEBUG 22:32:54,925 GC for ConcurrentMarkSweep: 73 ms, 212034048 reclaimed leaving 15214608 used; max is 902627328
DEBUG 22:33:52,931 Disseminating load info ...
DEBUG 22:34:20,929 ... got load info
 INFO 22:34:20,929 Joining: getting bootstrap token
DEBUG 22:34:20,931 token manually specified as 113416112894748789872342756657008344878
 INFO 22:34:20,932 Joining: sleeping 30000 ms for pending range setup
 INFO 22:34:50,935 Bootstrapping
DEBUG 22:34:50,935 Beginning bootstrap process
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)
        at org.apache.cassandra.locator.RackUnawareStrategy.calculateNaturalEndpoints(RackUnawareStrategy.java:57)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:195)
        at org.apache.cassandra.dht.BootStrapper.getRangesWithSources(BootStrapper.java:155)
        at org.apache.cassandra.dht.BootStrapper.startBootstrap(BootStrapper.java:73)
        at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:467)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:408)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:134)
        at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:57)
        ... 5 more
Cannot load daemon
Service exit with a return value of 3
"
CASSANDRA-1450,"Memtable flush causes bad ""reversed"" get_slice","If columns are inserted into a row before and after a memtable flush, a get_slice() after the flush with reversed=True will return incorrect results.  See attached patch to reproduce."
CASSANDRA-1432,java.util.NoSuchElementException when returning a node to the cluster,"I'm running the v0.7-beta1 in a 4 nodes cluster and just doing some simple testing. One of the nodes had been down (machine off, unclean shutdown) for an hour or so not sure how many writes were going on, when I bought it back up this message appears in the other 3 nodes...


INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 584) Node /192.168.34.27 has restarted, now UP again
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,200 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.27
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 636) Node /192.168.34.27 state jump to normal
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,201 StorageService.java (line 643) Will not change my token ownership to /192.168.34.27
ERROR [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,640 CassandraDaemon.java (line 82) Uncaught exception in thread Thread[HINTED-HANDOFF-POOL:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.NoSuchElementException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:87)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.util.NoSuchElementException
        at orgapache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
Caused by: java.util.NoSuchElementException
        at java.util.concurrent.ConcurrentSkipListMap.lastKey(ConcurrentSkipListMap.java:1981)
        at java.util.concurrent.ConcurrentSkipListMap$KeySet.last(ConcurrentSkipListMap.java:2331)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:121)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:218)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:78)
        at org.apache.cassandra.db.HintedHandOffManager$1.runMayThrow(HintedHandOffManager.java:296) not sure how many writes were going on
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

On the machine that was off (34.27) there are no errors in the logs, and here are the entries for around the same time...

 INFO [main] 2010-08-25 19:29:50,679 CommitLog.java (line 340) Recovery complete
 INFO [main] 2010-08-25 19:29:50,769 CommitLog.java (line 180) Log replay complete
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 342) Cassandra version: 0.7.0-beta1-SNAPSHOT
 INFO [main] 2010-08-25 19:29:50,797 StorageService.java (line 343) Thrift API version: 10.0.0
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 240) Saved Token found: 85070591730234615865843651857942052864
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 257) Saved ClusterName found: FOO
 INFO [main] 2010-08-25 19:29:50,813 SystemTable.java (line 272) Saved partitioner not found. Using org.apache.cassandra.dht.RandomPartitioner
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 422) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/local1/junkbox/cassandra/commitlog/CommitLog-12827213897
70.log', position=41336)
 INFO [main] 2010-08-25 19:29:50,814 ColumnFamilyStore.java (line 706) Enqueuing flush of Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,815 Memtable.java (line 150) Writing Memtable-LocationInfo@916236367(95 bytes, 2 operations)
 INFO [FLUSH-WRITER-POOL:1] 2010-08-25 19:29:50,873 Memtable.java (line 157) Completed flushing /local1/junkbox/cassandra/data/system/LocationInfo-e-6-Data.db
 INFO [main] 2010-08-25 19:29:50,917 StorageService.java (line 374) Starting up server gossip
 INFO [main] 2010-08-25 19:29:51,093 ColumnFamilyStore.java (line 1239) Loaded 0 rows into the Super2 cache
 INFO [main] 2010-08-25 19:29:51,170 CassandraDaemon.java (line 153) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2010-08-25 19:29:51,174 CassandraDaemon.java (line 167) Using TFramedTransport with a max frame size of 15728640 bytes.
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,198 Gossiper.java (line 578) Node /192.168.34.28 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.29 is now part of the cluster
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,199 Gossiper.java (line 578) Node /192.168.34.26 is now part of the cluster
 INFO [main] 2010-08-25 19:29:51,204 CassandraDaemon.java (line 208) Listening for thrift clients...
 INFO [main] 2010-08-25 19:29:51,210 Mx4jTool.java (line 73) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,417 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.28
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,417 Gossiper.java (line 570) InetAddress /192.168.34.28 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,418 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.28
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,855 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.29
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:51,855 Gossiper.java (line 570) InetAddress /192.168.34.29 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:51,860 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.29
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 191) Started hinted handoff for endpoint /192.168.34.26
 INFO [GOSSIP_STAGE:1] 2010-08-25 19:29:52,930 Gossiper.java (line 570) InetAddress /192.168.34.26 is now UP
 INFO [HINTED-HANDOFF-POOL:1] 2010-08-25 19:29:52,930 HintedHandOffManager.java (line 247) Finished hinted handoff of 0 rows to endpoint /192.168.34.26

I ran a repair on all the nodes and this was all that they each logged 
 INFO [manual-repair-fe7c5abb-bb0a-4415-aa75-0d72ba4e7f1b] 2010-08-25 19:49:24,194 AntiEntropyService.java (line 803) Waiting for repair requests to: []

The cluster seemed OK and kept on working."
CASSANDRA-1382,Race condition leads to FileNotFoundException on startup,"On startup LocationInfo file is deleted then attempted to be read from.

Steps to reproduce: Kill then quickly restart

Switching to ParallelGC to avoid CMS/CompressedOops incompatibility
INFO 17:05:08,680 DiskAccessMode isstandard, indexAccessMode is mmap
 INFO 17:05:08,786 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,797 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,807 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,833 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,834 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,839 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,862 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,864 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,876 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,885 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,892 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,893 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,897 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,901 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,906 Sampling index for /var/lib/cassandra/data/system/Schema-e-1-<>
 INFO 17:05:08,909 Sampling index for /var/lib/cassandra/data/system/Schema-e-2-<>
 INFO 17:05:08,918 Sampling index for /var/lib/cassandra/data/system/Migrations-e-1-<>
 INFO 17:05:08,922 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,928 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:08,933 Deleted /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db
 INFO 17:05:08,936 Deleted /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db
 INFO 17:05:08,936 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,937 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,947 Deleted /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db
 INFO 17:05:08,947 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,948 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,948 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,950 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,951 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:196)
    at org.apache.cassandra.db.StatisticsTable.deleteSSTableStatistics(StatisticsTable.java:81)
    at org.apache.cassandra.io.sstable.SSTable.deleteIfCompacted(SSTable.java:136)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:202)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,970 Deleted /var/lib/cassandra/data/system/LocationInfo-e-13-Data.db
 INFO 17:05:08,971 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-14-<>
ERROR 17:05:08,971 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-14-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-14-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,972 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-17-<>
 INFO 17:05:08,973 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-15-<>
ERROR 17:05:08,973 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-15-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-15-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,974 Sampling index for /var/lib/cassandra/data/system/LocationInfo-e-16-<>
ERROR 17:05:08,974 Corrupt file /var/lib/cassandra/data/system/LocationInfo-e-16-Data.db; skipped
java.io.FileNotFoundException: /var/lib/cassandra/data/system/LocationInfo-e-16-Index.db (No such file or directory)
    at java.io.RandomAccessFile.open(Native Method)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
    at java.io.RandomAccessFile.<init>(RandomAccessFile.java:98)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:142)
    at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:137)
    at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:289)
    at org.apache.cassandra.io.sstable.SSTableReader.internalOpen(SSTableReader.java:197)
    at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:176)
    at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:208)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:342)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:308)
    at org.apache.cassandra.db.Table.<init>(Table.java:245)
    at org.apache.cassandra.db.Table.open(Table.java:102)
    at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:121)
    at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:93)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:90)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
 INFO 17:05:08,996 Loading schema version acc5646a-a59d-11df-83fb-e700f669bcfc
 WARN 17:05:09,158 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO 17:05:09,164 Replaying /var/lib/cassandra/commitlog/CommitLog-1281571453475.log, /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571453475.log
 INFO 17:05:09,172 Finished reading /var/lib/cassandra/commitlog/CommitLog-1281571508928.log
 INFO 17:05:09,173 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,183 Enqueuing flush of Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,183 Writing Memtable-LocationInfo@137493297(17 bytes, 1 operations)
 INFO 17:05:09,184 switching in a fresh Memtable for Statistics at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1281571508928.log', position=592)
 INFO 17:05:09,184 Enqueuing flush of Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,265 Completed flushing /var/lib/cassandra/data/system/LocationInfo-e-18-Data.db
 INFO 17:05:09,273 Writing Memtable-Statistics@86823325(0 bytes, 0 operations)
 INFO 17:05:09,352 Completed flushing /var/lib/cassandra/data/system/Statistics-e-1-Data.db
 INFO 17:05:09,353 Recovery complete "
CASSANDRA-1377,NPE aborts streaming operations for keyspaces with hyphens ('-') in their names,"When streaming starts for operations such as repair or bootstrap, it will fail due to an NPE if they rows are in a keyspace that has a hyphen in its name.  One workaround for this issue would be to not use keyspace names containing hyphens.  It would be even nicer if streaming worked for keyspace names with hyphens, since keyspaces named like that seem to be fine in all other ways.

To reproduce:
 1. With a multi-node ring, load up a keyspace with a hyphen in its name
 2. Add some data to that keyspace
 3. nodetool repair

Expected results:
Repair operations complete normally

Actual results:
Repair operations don't complete normally.  The stacktrace below is correlated with the repair request.  

 INFO [AE-SERVICE-STAGE:1] 2010-06-30 14:11:29,744 AntiEntropyService.java (line 619) Performing streaming repair of 1 ranges to /10.255.0.20 for (my-keyspace,AColumnFamily)
ERROR [MESSAGE-DESERIALIZER-POOL:1] 2010-06-30 14:11:30,034 DebuggableThreadPoolExecutor.java (line 101) Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.getNewNames(StreamInitiateVerbHandler.java:154)
        at org.apache.cassandra.streaming.StreamInitiateVerbHandler.doVerb(StreamInitiateVerbHandler.java:76)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)"
CASSANDRA-1370,TokenMetaData.getPendingRangesMM() is unnecessarily synchronized,"TokenMetaData.getPendingRangesMM() is currently synchronized to avoid a race condition where multiple threads might create a multimap for the given table.  However, the pendingRanges instance variable that's the subject of the race condition is already a ConcurrentHashMap, and the race condition can be avoided by using putIfAbsent, leaving the case where the table's map is already initialized lock-free:

    private Multimap<Range, InetAddress> getPendingRangesMM(String table)
    {
        Multimap<Range, InetAddress> map = pendingRanges.get(table);
        if (map == null)
        {
            map = HashMultimap.create();
            Multimap<Range, InetAddress> fasterHorse 
            	= pendingRanges.putIfAbsent(table, map);
            if(fasterHorse != null) {
            	//another thread beat us to creating the map, oh well.
            	map = fasterHorse;
            }
        }
        return map;
    }
"
CASSANDRA-1330,AssertionError: discard at CommitLogContext(file=...) is not after last flush at  ...,"Looks related to CASSANDRA-936?

ERROR [MEMTABLE-POST-FLUSHER:1] 2010-07-28 11:39:36,909 CassandraDaemon.java (line 83) Uncaught exception in thread Thread[MEMTABLE-POST-FLUSHER:1,5,main]
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:86)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1118)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        ... 2 more
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:373)
        at org.apache.cassandra.db.ColumnFamilyStore$1.runMayThrow(ColumnFamilyStore.java:371)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegments(CommitLog.java:365)
        ... 8 more
Caused by: java.lang.AssertionError: discard at CommitLogContext(file='/srv/cassandra/commitlog/CommitLog-1280331567364.log', position=181) is not after last flush at 563
        at org.apache.cassandra.db.commitlog.CommitLog.discardCompletedSegmentsInternal(CommitLog.java:394)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:70)
        at org.apache.cassandra.db.commitlog.CommitLog$6.call(CommitLog.java:359)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:52)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
"
CASSANDRA-1327,page within a single row during hinted handoff,"currently HintedHandoffManager.sendMessage reads the entire hinted row into memory, then sends it in a single message.  while this is [barely] acceptable in 0.6, where the row size limit of 2GB keeps this from getting _too_ out of hand, we need explicit paging in 0.7"
CASSANDRA-1314,snitch that prefers a single replica for all reads to a given key,"motivation is, if you reduce or eliminate read repair (CASSANDRA-930) then you can cache about RF times as much data in memory, than if you're scattering reads across all the replicas equally.

making this data-center aware would be a nice improvement, but i'm just targetting a single DC here."
CASSANDRA-1293,add locking around row cache accesses,"CASSANDRA-1267 means we need to lock around removeDeleted on the row cache entry and the write path where we merge in new columns (otherwise there can be a race where we incorrectly continue to remove a column, that has been updated by the writer thread to be newly relevant)"
CASSANDRA-1282,NumericType: comparator for integers of fixed and arbitrary length,"Patch introduces a new column comparator: NumericType. It can compare signed integer values of fixed (int, long) and arbitrary length (BigInteger). Both can be mixed. Leading zero bytes or 0xFF bytes will be ignored and can safely be stripped on compaction.

The trivial version uses ""new BigInteger(byte[]).compareTo(new BigInteger(byte[]))"" but it's slower and fills up memory.
The faster version operates completely on stack and should even obsolete LongType."
CASSANDRA-1255,Explore interning keys and column names,"With multiple Memtables, key caches and row caches holding DecoratedKey references, it could potentially be a huge memory savings (and relief to GC) to intern DecoratedKeys. Taking the idea farther, for the skinny row pattern, and for certain types of wide row patterns, interning of column names could be very beneficial as well (although we would need to wrap the byte[]s in something for hashCode/equals).

This ticket should explore the benefits and overhead of interning.

Google collections/guava MapMaker is a very convenient way to create this type of cache: example call: http://stackoverflow.com/questions/2865026/use-permgen-space-or-roll-my-own-intern-method/2865083#2865083"
CASSANDRA-1247,Convert type of ColumnFamily.id and CFMetaData.cfId to Integer,"The column family id is boxed and unboxed so many times it should be somewhat faster to just pass around the same Integer. Memory won't fill up with Integers.

The following patch sets type of ColumnFamily.id and CFMetaData.cfId to Integer and includes some obvious auto-boxing optimizations."
CASSANDRA-1221,loadbalance operation never completes on a 3 node cluster,"Arya Goudarzi reports:

Please confirm if this is an issue and should be reported or I am doing something wrong. I could not find anything relevant on JIRA:

Playing with 0.7 nightly (today's build), I setup a 3 node cluster this way:

 - Added one node;
 - Loaded default schema with RF 1 from YAML using JMX;
 - Loaded 2M keys using py_stress;
 - Bootstrapped a second node;
 - Cleaned up the first node;
 - Bootstrapped a third node;
 - Cleaned up the second node;

I got the following ring:

Address       Status     Load          Range                                      Ring
                                      154293670372423273273390365393543806425
10.50.26.132  Up         518.63 MB     69164917636305877859094619660693892452     |<--|
10.50.26.134  Up         234.8 MB      111685517405103688771527967027648896391    |   |
10.50.26.133  Up         235.26 MB     154293670372423273273390365393543806425    |-->|

Now I ran:

nodetool --host 10.50.26.132 loadbalance

It's been going for a while. I checked the streams

nodetool --host 10.50.26.134 streams
Mode: Normal
Not sending any streams.
Streaming from: /10.50.26.132
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-3-Data.db/[(0,22206096), (22206096,27271682)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-4-Data.db/[(0,15180462), (15180462,18656982)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-5-Data.db/[(0,353139829), (353139829,433883659)]
  Keyspace1: /var/lib/cassandra/data/Keyspace1/Standard1-tmp-d-6-Data.db/[(0,366336059), (366336059,450095320)]

nodetool --host 10.50.26.132 streams
Mode: Leaving: streaming data to other nodes
Streaming to: /10.50.26.134
  /var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]
Not receiving any streams.

These have been going for the past 2 hours.

I see in the logs of the node with 134 IP address and I saw this:

INFO [GOSSIP_STAGE:1] 2010-06-22 16:30:54,679 StorageService.java (line 603) Will not change my token ownership to /10.50.26.132

So, to my understanding from wikis loadbalance supposed to decommission and re-bootstrap again by sending its tokens to other nodes and then bootstrap again. It's been stuck in streaming for the past 2 hours and the size of ring has not changed. The log in the first node says it has started streaming for the past hours:

INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 72) Beginning transfer process to /10.50.26.134 for ranges (154293670372423273273390365393543806425,69164917636305877859094619660693892452]
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,255 StreamOut.java (line 82) Flushing memtables for Keyspace1...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,266 StreamOut.java (line 128) Stream context metadata [/var/lib/cassandra/data/Keyspace1/Standard1-d-48-Data.db/[(0,366336059), (366336059,450095320)]] 1 sstables.
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 135) Sending a stream initiate message to /10.50.26.134 ...
 INFO [STREAM-STAGE:1] 2010-06-22 16:35:56,267 StreamOut.java (line 140) Waiting for transfer to /10.50.26.134 to complete
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 359) LocationInfo has reached its threshold; switching in a fresh Memtable at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1277249454413.log', position=720)
 INFO [FLUSH-TIMER] 2010-06-22 17:36:53,370 ColumnFamilyStore.java (line 622) Enqueuing flush of Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,370 Memtable.java (line 149) Writing Memtable(LocationInfo)@1637794189
 INFO [FLUSH-WRITER-POOL:1] 2010-06-22 17:36:53,528 Memtable.java (line 163) Completed flushing /var/lib/cassandra/data/system/LocationInfo-d-9-Data.db
 INFO [MEMTABLE-POST-FLUSHER:1] 2010-06-22 17:36:53,529 ColumnFamilyStore.java (line 374) Discarding 1000


Nothing more after this line.

Am I doing something wrong?"
CASSANDRA-1214,Force linux to not swap the JVM,"The way mmap()'d IO is handled in cassandra is dangerous. It allocates potentially massive buffers without any care for bounding the total size of the program's buffers. As the node's dataset grows, this *will* lead to swapping and instability.

This is a dangerous and wrong default for a couple of reasons.

1) People are likely to test cassandra with the default settings. This issue is insidious because it only appears when you have sufficient data in a certain node, there is absolutely no way to control it, and it doesn't at all respect the memory limits that you give to the JVM.

That can all be ascertained by reading the code, and people should certainly do their homework, but nevertheless, cassandra should ship with sane defaults that don't break down when you cross some magic unknown threshold.

2) It's deceptive. Unless you are extremely careful with capacity planning, you will get bit by this. Most people won't really be able to use this in production, so why get them excited about performance that they can't actually have?"
CASSANDRA-1179,split commitlog into header + mutations files,"As mentioned in CASSANDRA-1119, it seems possible that a commitlog header could be corrupted by a power loss during update of the header, post-flush.  We could try to make it more robust (by writing the size of the commitlogheader first, and skipping to the end if we encounter corruption) but it seems to me that the most foolproof method would be to split the log into two files: the header, which we'll overwrite, and the data, which is truly append only.  If If the header is corrupt on reply, we just reply the data from the beginning; the header allows us to avoid replaying data redundantly, but it's strictly an optimization and not required for correctness."
CASSANDRA-1155,keep persistent row statistics,"during flush and compaction we should keep row size statistics using EstimatedHistogram (column count, and row size), replacing min/max/total sizes in CFS.

having this detail will let us estimate, given an index CF, how many nodes we need to query to get the number of matching rows requested by the client."
CASSANDRA-1101,A Hadoop Output Format That Targets Cassandra,"Currently, there exists a Hadoop-specific input format (viz., ColumnFamilyInputFormat) that allows one to iterate over the rows in a given Cassandra column family and treat it as the input to a Hadoop map task. By the same token, one may need to feed the output of a Hadoop reduce task into a Cassandra column family, for which no mechanism exists today. This calls for the definition of a Hadoop-specific output format which accepts a pair of key and columns, and writes it out to a given column family.

Here, we describe an output format known as ColumnFamilyOutputFormat, which allows reduce tasks to persist keys and their associated columns as Cassandra rows in a given column family.  By default, it prevents overwriting existing rows in the column family, by ensuring at initialization time that it contains no rows in the given slice predicate. For the sake of speed, it employs a lazy write-back caching mechanism, where its record writer batches mutations created based on the reduce's inputs (in a task-specific map) but stops short of actually mutating the rows. The latter responsibility falls on its output committer, which makes the changes official by sending a batch mutate request to Cassandra.  

The record writer, which is called ColumnFamilyRecordWriter, maps the input <key, value> pairs to a Cassandra column family. In particular, it creates mutations for each column in the value, which it then associates with the key, and in turn the responsible endpoint.  Note that, given that round trips to the server are fairly expensive, it merely batches the mutations in-memory, and leaves it on the output committer to send the batched mutations to the server.  Furthermore, the writer groups the mutations by the endpoint responsible for the rows being affected. This allows the output committer to execute the mutations in parallel, on an endpoint-by-endpoint basis.

The output committer, which is called ColumnFamilyOutputCommitter, traverses the mutations collected by the record writer, and sends them to the endpoints responsible for them. Since the total set of mutations is partitioned by their endpoints, each of which can be performed in parallel, it allows us to commit the mutations using multiple threads, one per endpoint. As a result, it reduces the time it takes to propagate the mutations to the server considering that (a) the client eliminates one network hop that the server would otherwise have had to make and (b) each endpoint node has to deal with but a sub-set of the total set of mutations.

For convenience, we also define a default reduce task, called ColumnFamilyOutputReducer, which collects the columns in the input value and maps them to a data structure expected by Cassandra. By default, it assumes the input value to be in the form of a ColumnWritable, which denotes a name value pair corresponding to a certain column. This reduce task is in turn used by the attached test case, which maps every <key, value> pair in a sample input sequence file to a <key, column> pair, and then reduces them by aggregating columns corresponding to the same key. Eventually, the batched <key, columns> pairs are written to the column family associated with the output format."
CASSANDRA-1081,Thrift sockets leak in 0.6 hadoop interface,"Thrift connections appear not to be closed properly in 0.6 in ColumnFamilyRecordReader, which causes a file descriptor leak on the server and may eventually cause jobs to fail.

This appear to be fixed in 0.7 https://issues.apache.org/jira/browse/CASSANDRA-1017 so it may be worth backporting the patch or add a quick fix to close the Tsockets."
CASSANDRA-1040,read failure during flush,"Joost Ouwerkerk writes:
	
On a single-node cassandra cluster with basic config (-Xmx:1G)
loop {
  * insert 5,000 records in a single columnfamily with UUID keys and
random string values (between 1 and 1000 chars) in 5 different columns
spanning two different supercolumns
  * delete all the data by iterating over the rows with
get_range_slices(ONE) and calling remove(QUORUM) on each row id
returned (path containing only columnfamily)
  * count number of non-tombstone rows by iterating over the rows
with get_range_slices(ONE) and testing data.  Break if not zero.
}

while this is running, call ""bin/nodetool -h localhost -p 8081 flush KeySpace"" in the background every minute or so.  When the data hits some critical size, the loop will break."
CASSANDRA-1018,Ability to close tables for embeddable cassandra,"I would like to make Cassandra more embeddable in order to use it from within application unit tests.  Unfortunately, just nuking directories with CassandraServiceDataCleaner doesn't completely isolate test cases within a single running JVM.  In-memory table objects are kept in a static hash map.  This patch adds a close method that will remove table instances from the static hash map."
CASSANDRA-1014,"GC storming, possible memory leak","There appears to be a GC issue due to memory pressure in the 0.6 branch.  You can see this by starting the server and performing many inserts.  Quickly the jvm will consume most of its heap, and pauses for stop-the-world GC will begin.  With verbose GC turned on, this can be observed as follows:

[GC [ParNew (promotion failed): 79703K->79703K(84544K), 0.0622980 secs][CMS[CMS-concurrent-mark: 3.678/5.031 secs] [Times: user=10.35 sys=4.22, real=5.03 secs]
 (concurrent mode failure): 944529K->492222K(963392K), 2.8264480 secs] 990745K->492222K(1047936K), 2.8890500 secs] [Times: user=2.90 sys=0.04, real=2.90 secs]

After enough inserts (around 75-100 million) the server will GC storm and then OOM.

jbellis and I narrowed this down to patch 0001 in CASSANDRA-724.  Switching LBQ with ABQ made no difference, however using batch mode instead of periodic for the commitlog does prevent the issue from occurring.  The attached screenshot shows the heap usage in jconsole first when the issue is exhibiting, a restart, and then the same amount of inserts when it does not."
CASSANDRA-1007,Make memtable flush thresholds per-CF instead of global,"This is particularly useful in the scenario where you have a few CFs with a high volume of overwrite operations; increasing the memtable size/op count means that you can do the overwrite in memory before it ever hits disk.  Once on disk compaction is much more work for the system.

But, you don't want to give _all_ your CFs that high of a threshold because the memory is better used elsewhere, and because it makes commitlog replay unnecessarily painful."
CASSANDRA-998,Replace SuperColumn and ColumnFamily structures,"The most important concepts to come out of CASSANDRA-674 were the Slice, Metadata and ColumnKey structures.

As a replacement for SuperColumns and ColumnFamilies in the current codebase, Slices can addtionally:
* Represent arbitrarily nested columns
* Allow for eventually consistent range deletes (equivalent to a CF containing multiple 'markedForDelete' values)
* Be used for memory efficient compactions, since they encapsulate nesting and can be iterated
* Eliminate code duplication for SuperColumn handling

This is an umbrella ticket for the task of replacing the SuperColumn and ColumnFamily datastructures in the codebase, preferably with something like Slice."
CASSANDRA-946,Add a configuration and implementation to populate the data into memory,"Proactively load data into the memory when the node is started, there will be a configuration to enable this function and will be per Columnfamily. The requirement is to speed up the reads for data which can reside 100% of in the memory.... In addition to enabling the RowCache to 100% we can do this so as upgrades or any other means of restart will not clear the cache in the server."
CASSANDRA-934,NPE in sstable2json,"When sstable2json is not passed any excluded keys via -x, an NPE is raised."
CASSANDRA-924,incorrect neighbor calculation in repair,"With Replicationfactor=2, if a server is brought down and its data directory wiped out, it doesn't restore its data replica after restart and nodeprobe repair.
Steps to reproduce:
1) Bring up a cluster with three servers cs1,2,3, with their initial token set to 'foo3', 'foo6', and 'foo9', respectively. ReplicationFactor is set to 2 on all 3.
2) Insert 9 columns with keys from 'foo1' to 'foo9', and flush. Now I have foo1,2,3,7,8,9 on cs1, foo1,2,3,4,5,6, on cs2, and foo4,5,6,7,8,9
on cs3. So far so good
3) Bring down cs3 and wipe out its data directory
4) Bring up cs3
5) run nodeprobe repair Keyspace1 on cs3, the flush
At this point I expect to see cs3 getting its data back. But there's nothing in its data directory. I also tried getting all columns with
ConsistencyLevel::ALL to see if that'll do a read pair. But still cs3's data directory is empty.
"
CASSANDRA-880,"add ""drain"" command","""drain"" will flush and not accept more writes, leaving the commitlog empty on restart.

this is important to make the future upgrade to 0.7 smooth."
CASSANDRA-863,"Default cache levels should be explicit, not percentage","Now that we can specify  explicit numbers for row and key caches, the default config should use this.  The reason is that it's very easy to grow your keys and not realize a percentage is going to get you eventually.  It's better to have a system that has a low cache hit rate and keeps going than a system that encounters memory pressure from overpopulating its cache and slows to a crawl or dies."
CASSANDRA-847,Make the reading half of compactions memory-efficient,"This issue is the next on the road to finally fixing CASSANDRA-16. To make compactions memory efficient, we have to be able to perform the compaction process on the smallest possible chunks that might intersect and contend one-another, meaning that we need a better abstraction for reading from SSTables."
CASSANDRA-801,The Key Cache should be configurable by absolute numbers,"Just like the row cache, the key cache should be configurable by absolute numbers (not just a fraction). It would be nice to configure it by either memory size or entries (like the row cache)."
CASSANDRA-800,Spurious Gossip Up/Down and IO Errors,"We're seeing a lot of nodes flapping. It appears to possibly be a race condition in Gossip.

on 10.209.23.110

WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:52720]
WARN [MESSAGING-SERVICE-POOL:1] 2010-02-13 01:18:22,976 TcpConnection.java (line 484) Problem reading from socket connected to : java.nio.channels.SocketChannel[connected local=/10.209.23.110:7000 remote=/10.209.23.80:36128]
 WARN [MESSAGING-SERVICE-POOL:2] 2010-02-13 01:18:22,977 TcpConnection.java (line 485) Exception was generated at : 02/13/2010 01:18:22 on thread MESSAGING-SERVICE-POOL:2
Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
java.io.IOException: Reached an EOL or something bizzare occured. Reading from: /10.209.23.80 BufferSizeRemaining: 16
    at org.apache.cassandra.net.io.StartState.doRead(StartState.java:44)
    at org.apache.cassandra.net.io.ProtocolState.read(ProtocolState.java:39)
    at org.apache.cassandra.net.io.TcpReader.read(TcpReader.java:95)
    at org.apache.cassandra.net.TcpConnection$ReadWorkItem.run(TcpConnection.java:445)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


on 10.209.23.80 about the same time


ERROR [pool-1-thread-4751] 2010-02-13 01:17:12,261 Cassandra.java (line 1096) Internal error processing batch_insert
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:848)
    at java.util.HashMap$KeyIterator.next(HashMap.java:883)
    at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
    at java.util.HashSet.<init>(HashSet.java:100)
    at org.apache.cassandra.gms.Gossiper.getLiveMembers(Gossiper.java:173)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedMapForEndpoints(AbstractReplicationStrategy.java:120)
    at org.apache.cassandra.locator.AbstractReplicationStrategy.getHintedEndpoints(AbstractReplicationStrategy.java:78)
    at org.apache.cassandra.service.StorageService.getHintedEndpointMap(StorageService.java:1186)
    at org.apache.cassandra.service.StorageProxy.insertBlocking(StorageProxy.java:169)
    at org.apache.cassandra.service.CassandraServer.doInsert(CassandraServer.java:466)
    at org.apache.cassandra.service.CassandraServer.batch_insert(CassandraServer.java:445)
    at org.apache.cassandra.service.Cassandra$Processor$batch_insert.process(Cassandra.java:1088)
    at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:817)
    at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


just before that:

INFO [Timer-1] 2010-02-13 01:17:12,070 Gossiper.java (line 194) InetAddress /10.209.21.223 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.217 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,257 Gossiper.java (line 194) InetAddress /10.209.21.216 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.21.215 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,258 Gossiper.java (line 194) InetAddress /10.209.23.82 is now dead.


and just after that:

INFO [Timer-1] 2010-02-13 01:17:12,261 Gossiper.java (line 194) InetAddress /10.209.23.81 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,293 Gossiper.java (line 194) InetAddress /10.209.23.79 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,304 Gossiper.java (line 194) InetAddress /10.209.21.204 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,307 Gossiper.java (line 194) InetAddress /10.209.21.197 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,308 Gossiper.java (line 194) InetAddress /10.209.21.245 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,309 Gossiper.java (line 194) InetAddress /10.209.21.242 is now dead.
INFO [Timer-1] 2010-02-13 01:17:12,310 Gossiper.java (line 194) InetAddress /10.209.23.106 is now dead.
INFO [GMFD:1] 2010-02-13 01:17:26,780 Log4jLogger.java (line 41) 02/13/2010 01:17:26 - Remaining bytes zero. Stopping deserialization in EndPointState.
INFO [GMFD:1] 2010-02-13 01:17:26,784 Gossiper.java (line 543) InetAddress /10.209.21.204 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,785 Gossiper.java (line 543) InetAddress /10.209.23.106 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,786 Gossiper.java (line 543) InetAddress /10.209.21.197 is now UP
INFO [GMFD:1] 2010-02-13 01:17:26,800 Gossiper.java (line 543) InetAddress /10.209.21.216 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,808 Gossiper.java (line 543) InetAddress /10.209.21.217 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.223 is now UP
INFO [GMFD:1] 2010-02-13 01:17:41,823 Gossiper.java (line 543) InetAddress /10.209.21.215 is now UP


We're on 298a0e66ba66c5d2a1e5d4a70f2f619ae3fbf72a from git.apache.org, which claims to be:

git-svn-id: https://svn.apache.org/repos/asf/incubator/cassandra/branches/cassandra-0.5@9035"
CASSANDRA-790,SSTables limited to (2^31)/15 keys,"The current BloomFilter implementation requires a BitSet of (bucket_count * num_keys) in size, and that calculation is currently performed in an integer, which causes overflow for around 140 million keys in one SSTable.

Short term fix: perform the calculation in a long, and cap the value to the maximum size of a BitSet.
Long term fix: begin partitioning BitSets, perhaps using Linear Bloom Filters."
CASSANDRA-789,"Add configurable range sizes, paging to hadoop range queries","For very large (billions) numbers of keys, the current hardcoded 4096 keys per InputSplit could cause the split generator to OOM, since all splits are held in memory at once.  So we want to make 2 changes:

 1) make the number of keys configurable*
 2) make record reader page instead of assuming it can read all rows into memory at once

Note: going back to specifying number of splits instead of number of keys is bad for two reasons.  First, it does not work with the standard hadoop mapred.min.split.size configuration option.  Second, it means we have no way of measuring progress in the record reader, since we have no idea how many keys are in the split.  If we specify number of keys, then even if we page we know (to within a small margin of error) how many keys to expect, even if we page.

See CASSANDRA-775, CASSANDRA-342 for background."
CASSANDRA-778,Gossiper thread deadlock,"Found this while attempting to bootstrap a node with more than a trivial amount of data:

Found one Java-level deadlock:
=============================
""GMFD:1"":
  waiting to lock monitor 0x0000000100861d60 (object 0x00000001066a7ed8, a org.apache.cassandra.service.StorageService),
  which is held by ""main""
""main"":
  waiting to lock monitor 0x0000000100860710 (object 0x0000000106c7c968, a org.apache.cassandra.gms.Gossiper),
  which is held by ""GMFD:1""

Java stack information for the threads listed above:
===================================================
""GMFD:1"":
	at org.apache.cassandra.service.StorageService.getReplicationStrategy(StorageService.java:226)
	- waiting to lock <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:634)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:502)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:445)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:812)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:607)
	at org.apache.cassandra.gms.Gossiper.handleNewJoin(Gossiper.java:582)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
	- locked <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.gms.Gossiper$GossipDigestAck2VerbHandler.doVerb(Gossiper.java:1061)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:40)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:637)
""main"":
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:861)
	- waiting to lock <0x0000000106c7c968> (a org.apache.cassandra.gms.Gossiper)
	at org.apache.cassandra.service.StorageService.startBootstrap(StorageService.java:347)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:318)
	- locked <0x00000001066a7ed8> (a org.apache.cassandra.service.StorageService)
	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:99)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:174)

Found 1 deadlock.

main acquires SS lock and doesn't release it before attempting to acquire the Gossiper lock.  Meanwhile, the gossip stage acquires the Gossiper lock and then attempts to acquire the SS lock.

Solution is to have finer-grained locking on the resource in SS (map of replication strategies), or to move the collection to a different class (DD maybe?).  This was introduced in CASSANDRA-620."
CASSANDRA-740,Create InProcessCassandraServer for unit tests,"I've been personally using an in-process cassandra server and found it useful so I was ask to make it available publicly, here goes.
When unit-testing with cassandra I create an in process cassandra instance. That's nice since it lets you isolate tests, and you don't have to worry about a server being available for your unit tests.
The code goes more or less like this (I'll attach a patch when the work is done after cleanup etc)

/**
 * An in-memory cassandra storage service that listens to the thrift interface.
 * Useful for unit testing,
 *
 * @author Ran Tavory (rantav@gmail.com)
 *
 */
public class InProcessCassandraServer implements Runnable {

  private static final Logger log = LoggerFactory.getLogger(InProcessCassandraServer.class);

  CassandraDaemon cassandraDaemon;

  public void init() {
    try {
      prepare();
    } catch (IOException e) {
      log.error(""Cannot prepare cassandra."", e);
    }
    try {
      cassandraDaemon = new CassandraDaemon();
      cassandraDaemon.init(null);
    } catch (TTransportException e) {
      log.error(""TTransportException"", e);
    } catch (IOException e) {
      log.error(""IOException"", e);
    }
  }

  @Override
  public void run() {
    cassandraDaemon.start();
  }

  public void stop() {
    cassandraDaemon.stop();
    rmdir(""tmp"");
  }


  /**
   * Creates all files and directories needed
   * @throws IOException
   */
  private void prepare() throws IOException {
    // delete tmp dir first
    rmdir(""tmp"");
    // make a tmp dir and copy storag-conf.xml and log4j.properties to it
    copy(""/cassandra/storage-conf.xml"", ""tmp"");
    copy(""/cassandra/log4j.properties"", ""tmp"");
    System.setProperty(""storage-config"", ""tmp"");

    // make cassandra directories.
    for (String s: DatabaseDescriptor.getAllDataFileLocations()) {
      mkdir(s);
    }
    mkdir(DatabaseDescriptor.getBootstrapFileLocation());
    mkdir(DatabaseDescriptor.getLogFileLocation());
  }

  /**
   * Copies a resource from within the jar to a directory.
   *
   * @param resourceName
   * @param directory
   * @throws IOException
   */
  private void copy(String resource, String directory) throws IOException {
    mkdir(directory);
    InputStream is = getClass().getResourceAsStream(resource);
    String fileName = resource.substring(resource.lastIndexOf(""/"") + 1);
    File file = new File(directory + System.getProperty(""file.separator"") + fileName);
    OutputStream out = new FileOutputStream(file);
    byte buf[] = new byte[1024];
    int len;
    while ((len = is.read(buf)) > 0) {
      out.write(buf, 0, len);
    }
    out.close();
    is.close();
  }

  /**
   * Creates a directory
   * @param dir
   * @throws IOException
   */
  private void mkdir(String dir) throws IOException {
    FileUtils.createDirectory(dir);
  }

  /**
   * Removes a directory from file system
   * @param dir
   */
  private void rmdir(String dir) {
    FileUtils.deleteDir(new File(dir));
  }
}


And test code using this class looks like this:

public class XxxTest {

  private static InProcessCassandraServer cassandra;

  @BeforeClass
  public static void setup() throws TTransportException, IOException, InterruptedException {
    cassandra = new InProcessCassandraServer();
    cassandra.init();
    Thread t = new Thread(cassandra);
    t.setDaemon(true);
    t.start();
  }

  @AfterClass
  public static void shutdown() {
    cassandra.stop();
  }

  public void testX() {
    // connect to cassandra at localhost:9160
  }
}


note: I've set Fix Version to 6.0, hope it's correct"
CASSANDRA-696,Bootstrapping doesn't work on new clusters,"This is an edge case.

1. start a clean 3 node cluster with autobootstrap on.
2. load some data.
3. bootstrap in a 4th node.

the logs in the 4th node will indicate that data was not received.  If you restart the cluster in between steps 1 and 2, or 2 and 3, boot strapping works fine.  

I find that waiting on the table flush when making the streaming request solves the problem (see patch)."
CASSANDRA-694,Failure to flush commit log,"The following exception occurs consistently on at least node (note did not occur on other same-configured nodes) during startup:

INFO - Replaying /var/lib/cassandra/commitlog/CommitLog-1262855754427.log, /var/lib/cassandra/commitlog/CommitLog-1262832689989.log, /var/lib/cassandra/commitlog/CommitLog-1262885833186.log, /var/lib/cassandra/commitlog/CommitLog-1262900845019.log, /var/lib/cassandra/commitlog/CommitLog-1262913267844.log, /var/lib/cassandra/commitlog/CommitLog-1262927898170.log, /var/lib/cassandra/commitlog/CommitLog-1262961421039.log, /var/lib/cassandra/commitlog/CommitLog-1262977175175.log, /var/lib/cassandra/commitlog/CommitLog-1262989588783.log, /var/lib/cassandra/commitlog/CommitLog-1263000573676.log, /var/lib/cassandra/commitlog/CommitLog-1263013691393.log, /var/lib/cassandra/commitlog/CommitLog-1263044706108.log, /var/lib/cassandra/commitlog/CommitLog-1263060004191.log, /var/lib/cassandra/commitlog/CommitLog-1263071446342.log, /var/lib/cassandra/commitlog/CommitLog-1263082950154.log, /var/lib/cassandra/commitlog/CommitLog-1263095400814.log, /var/lib/cassandra/commitlog/CommitLog-1263118331046.log, /var/lib/cassandra/commitlog/CommitLog-1263143402963.log, /var/lib/cassandra/commitlog/CommitLog-1263155294308.log, /var/lib/cassandra/commitlog/CommitLog-1263166154352.log, /var/lib/cassandra/commitlog/CommitLog-1263178359247.log, /var/lib/cassandra/commitlog/CommitLog-1263202112017.log, /var/lib/cassandra/commitlog/CommitLog-1263230932274.log, /var/lib/cassandra/commitlog/CommitLog-1263250726505.log, /var/lib/cassandra/commitlog/CommitLog-1263264159438.log, /var/lib/cassandra/commitlog/CommitLog-1263289964249.log, /var/lib/cassandra/commitlog/CommitLog-1263317974387.log, /var/lib/cassandra/commitlog/CommitLog-1263331989090.log, /var/lib/cassandra/commitlog/CommitLog-1263344147667.log, /var/lib/cassandra/commitlog/CommitLog-1263359751527.log, /var/lib/cassandra/commitlog/CommitLog-1263395707008.log, /var/lib/cassandra/commitlog/CommitLog-1263397833524.log, /var/lib/cassandra/commitlog/CommitLog-1263398736183.log, /var/lib/cassandra/commitlog/CommitLog-1263399753707.log, /var/lib/cassandra/commitlog/CommitLog-1263401667504.log, /var/lib/cassandra/commitlog/CommitLog-1263404640782.log, /var/lib/cassandra/commitlog/CommitLog-1263405827234.log, /var/lib/cassandra/commitlog/CommitLog-1263406901115.log
INFO - LocationInfo has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(LocationInfo)@25934689
INFO - HintsColumnFamily has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(HintsColumnFamily)@4766820
INFO - AdXRequestStatistics has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(AdXRequestStatistics)@21521158
INFO - TokenGoogleIDCF has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(TokenGoogleIDCF)@22889075
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.db.Table.flush(Table.java:464)
        at org.apache.cassandra.db.CommitLog.recover(CommitLog.java:397)
        at org.apache.cassandra.db.RecoveryManager.doRecovery(RecoveryManager.java:65)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:90)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:135)
        ... 5 more

And the same exception occurs intermittently on other node (running) nodes during 'nodeprobe flush':

root@domU-12-31-38-00-26-31:~# nodeprobe -host localhost -port 8080 flush Logger
Exception in thread ""main"" java.lang.AssertionError: Blocking serialized executor is not yet implemented
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:84)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:78)
        at org.apache.cassandra.db.ColumnFamilyStore.submitFlush(ColumnFamilyStore.java:1045)
        at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:395)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:448)
        at org.apache.cassandra.service.StorageService.forceTableFlush(StorageService.java:984)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)"
CASSANDRA-685,add backpressure to StorageProxy,Now that we have CASSANDRA-401 and CASSANDRA-488 there is one last piece: we need to stop the target node from pulling mutations out of MessagingService as fast as it can only to take up space in the mutation queue and eventually fill up memory.
CASSANDRA-678,row-level cache,"We have a key cache but that doesn't help mitigate the expensive deserialization of the actual data to return.

Adding a row-level cache should be fairly simple using a ConcurrentLinkedHashMap<String [key], ColumnFamily> structure.  (We will only cache whole rows at a time, since already know how to query on those in-memory.  This limits us to CFs full of narrow rows but that is a common enough use case to be worth tackling if it can be done simply enough.)"
CASSANDRA-667,Avoid causing IO contention when cleaning up compacted SSTables,"Post-CASSANDRA-408 compacted sstables are deleted in parallel, one thread per sstable; this can cause IO contention with sstable flush and compaction.  Serialize this instead."
CASSANDRA-621,"GC compacted sstables as part of cleanup, compaction","""my disk space never goes down until i restart"" seems to be common for people testing things out who give their JVMs too much memory

let's help them out by calling system.gc on cleanup and compaction (which are rare enough that performance hit should be negligible)"
CASSANDRA-618,json2sstable/sstable2json don't export/import correct column names when the column family is of BytesType ordering,"Easy to reproduce.
1- start with an empty node.
2- run: client.insert(""Keyspace1"",
                          key_user_id,
                          new ColumnPath(""Standard1"", null, ""name"".getBytes(""UTF-8"")),
                          ""Ramzi"".getBytes(""UTF-8""),
                          timestamp,
                          ConsistencyLevel.ONE)
3- flush to get sstable
4- sstable2json and export the sstable to a file
5- delete sstable
6- json2sstable and import the json into a new sstable.
7- sstable2json on new sstable, you will see that the name is different than the name in the original json file. 
Also do a get on the column and it will return no result. "
CASSANDRA-599,The pending tasks listed for the compaction pool is confusing,"Using the build dated 29 Nov 09, the output from ""nodeprobe tpstats"" is confusing.  The following observations were made with an idle server that was compacting to clean up after some writes that were done in the past.  The number of pending tasks for COMPACTION-POOL starts at 1673, and after doing some work the number of pending tasks has increased to 1675.  I'd prefer that the amount of pending work reported by tpstats decreases as work is done.

> sh nodeprobe.sh tpstats
Pool Name                    Active   Pending      Completed
FILEUTILS-DELETE-POOL             0         0            872
MESSAGING-SERVICE-POOL            0         0              0
STREAM-STAGE                      0         0              0
RESPONSE-STAGE                    0         0         662309
ROW-READ-STAGE                    0         0         662309
LB-OPERATIONS                     0         0              0
COMMITLOG                         1         0        1189659
MESSAGE-DESERIALIZER-POOL         0         0              0
GMFD                              0         0              0
LB-TARGET                         0         0              0
CONSISTENCY-MANAGER               0         0              0
ROW-MUTATION-STAGE                0         0        1101916
MESSAGE-STREAMING-POOL            0         0              0
LOAD-BALANCER-STAGE               0         0              0
FLUSH-SORTER-POOL                 0         0           1723
MEMTABLE-POST-FLUSHER             0         0           1723
COMPACTION-POOL                   1      1673            136
FLUSH-WRITER-POOL                 0         0           1723
HINTED-HANDOFF-POOL               0         0             20
> sh nodeprobe.sh tpstats
Pool Name                    Active   Pending      Completed
FILEUTILS-DELETE-POOL             0         0            872
MESSAGING-SERVICE-POOL            0         0              0
STREAM-STAGE                      0         0              0
RESPONSE-STAGE                    0         0         662309
ROW-READ-STAGE                    0         0         662309
LB-OPERATIONS                     0         0              0
COMMITLOG                         1         0        1191173
MESSAGE-DESERIALIZER-POOL         0         0              0
GMFD                              0         0              0
LB-TARGET                         0         0              0
CONSISTENCY-MANAGER               0         0              0
ROW-MUTATION-STAGE                0         0        1101916
MESSAGE-STREAMING-POOL            0         0              0
LOAD-BALANCER-STAGE               0         0              0
FLUSH-SORTER-POOL                 0         0           1723
MEMTABLE-POST-FLUSHER             0         0           1723
COMPACTION-POOL                   1      1675            139
FLUSH-WRITER-POOL                 0         0           1723
HINTED-HANDOFF-POOL               0         0             20
(^[[01;32mroot@localhost^[[00m)^[[01;34m~/rackspace-stuff/cassandra-0.5-pre^[[00m> "
CASSANDRA-578,get_range_slice NPE,"If I call get_range_slice with arguments in the SliceRange structure, then it seems to NPE.  I think it only does it when there is nothing in the range specified in the column slice start and end.


ERROR - Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
ERROR - Fatal exception in thread Thread[ROW-READ-STAGE:8,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:55)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:38)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Row.addColumnFamily(Row.java:96)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1469)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:41)
        ... 4 more
"
CASSANDRA-562,Make range changes more fully automatic,"I think there are currently some problems with bootstrapping & leaving. The inherent problem is that a node one-sidedly announces that it is going to leave/take a token without making sure it will not cause conflict, and we do not have proper mechanism to clean up after a conflict. There are currently ways a simultaneous bootstrap or leaving can leave the cluster (tokenmetadata) in an inconsistent state and we'll need either a mechanism to resolve which operation wins or make sure only one operation is in process for affected ranges at one time.

1st option (resolve & clean conflicts as they happen):

We could add local timestamp to bootstrap/leave gossip and resolve conflicts based on that. This would allow us to choose one of the operations unambiguously and reject all but the one that was first. Theoretically, if different data centers are not in perfect clock sync, this might always favor one DC over the other in race situations, but this would hardly create any noticeable bias. Problem is, this approach would probably end up being horrendously complex (if not impossible) to do properly.

2nd option (make sure only one operation is in process at one time for affected ranges):

Add new messaging ""channel"" to be used to agree beforehand who is going to move. (1) Before a node can start bootstrapping, it must ask permission from the node whose range it is going to bootstrap to. The request will be accepted if no other node is currently bootstrapping there. Nodes of course check their own token metadata before bootstrapping, but this does not guard against two nodes bootstrapping simultaneously (that is, before they see each others' gossip). The only node able to answer this reliably is bootstrap source. (2) For leaving, the node should first check with all nodes that are going to have pending ranges if it is OK to leave. If it receives ""OK"" from all of them, then it can leave. If bootstrapping or leaving operation is rejected, the node will wait for random time and start over. Eventually all nodes will be able to bootstrap.

Good in this approach would be that with a relatively small change (adding one messaging exchange before actual move) we could make sure that all parties involved agree that the operation is OK. This is IMHO also the ""cleanest"" way. Downside is that we will need token lease times (how long the node owns ""rights"" to the token) and timers to make sure that we do not end in a deadlock (or lock ranges) in case the node will not complete the operation. Network partitions, delays and clock skews might create very difficult border cases to handle.

3rd option (another approach to preventing conflicts from happening):

(3) We might take a simplistic approach and add two new states: ABOUT_TO_BOOTSTRAP and ABOUT_TO_LEAVE. Whenever a node wants to bootstrap or leave, it will first gossip this state with token info and wait for some time. After the wait, it will check if it is OK to carry on with the operation (that is, it has not received any bootstrap/leaving gossip that would contradict with its own plans). Also in this case, if the operation cannot be done due to conflict, the node waits for random period and start over.

This would be easiest to implement, but it would not completely remove the problem as network partitions and other delays might still cause two nodes to clash without knowledge of each others' intentions. Also, adding two more gossip states will expand the state machine and make it more fragile.


Don't know if I'm thinking about this in a wrong way, but to me it seems resolving conflicts is very difficult, so the only option is to avoid them by some mechinism that makes sure only one node is moving within affected ranges.
"
CASSANDRA-542,stress.py enhancements,"this patch makes the following improvements over the existing stress.py (with the patch in CASSANDRA-514):

1) uses a shared memory array to rectify the fact that with the multiproc change in CASSANDRA-514 the updates to the op counts are not getting pooled correctly 
2) optionally allows the client to do round robin requests thus preventing cassandra from having to take on extra load balancing requests
3) refactors code so there is no duplicate code making it easier to support any new operations people may want to add
4) stress.py can now be used to generate performance graphs. It adds one new field which is the total number of operations completed for each interval which is required to show aggregate performance across intervals. It also does an append so that data points can be collected for each interval. Furthermore, the date foramt is now CSV so for easy import into Excel, R, etc.

Generating performance graphs can be generated in 5 easy steps:
1. nosetests --tests=system.stress:Stress.insert &
2. tail -f /tmp/progress_insert (just to watch and make sure it's working)
3. fire up R (www.r-project.org/)
4. w = read.table(""/tmp/progress_insert"",sep="","")
5. plot(w[,3]/60,w[,2],ylim=c(0,10000),xlab=""Elapsed time (minutes)"",ylab=""Writes / sec"", type='l')"
CASSANDRA-539,thread flushes from recoverymanager,"this will improve recovery speed

the intent appears to have been to avoid opening up the server to extra load while we're still cleaning up after recovery, but now that we return Future from flush we can have our cake and eat it too"
CASSANDRA-532,Flush creates empty SSTables if nothing exists in that CF,"When calling flush() through nodeprobe, we see SSTables being created that are empty for CFs:

 INFO [COMPACTION-POOL:1] 2009-11-05 22:58:09,515 ColumnFamilyStore.java (line 850) Compacting [org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-9-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-10-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-7-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-11-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-8-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-5-Data.db'),org.apache.cassandra.io.SSTableReader(path='/mnt/cassandra/data/Digg/Buries-6-Data.db')]
ERROR [COMPACTION-POOL:1] 2009-11-05 22:58:09,516 DebuggableThreadPoolExecutor.java (line 120) Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExecute(DebuggableThreadPoolExecutor.java:112)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:888)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.SSTableReader.getApproximateKeyCount(SSTableReader.java:102)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.doFileCompaction(ColumnFamilyStore.java:830)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompactionInternal(ColumnFamilyStore.java:673)
        at org.apache.cassandra.db.ColumnFamilyStore.doMajorCompaction(ColumnFamilyStore.java:645)
        at org.apache.cassandra.db.CompactionManager$OnDemandCompactor.run(CompactionManager.java:123)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        ... 2 more
"
CASSANDRA-505,turn nodeprobe flush_binary into nodeprobe flush,"want it to look like

nodeprobe flush (cf)*

i.e., zero or more CF names as arguments

then it will flush normal and binary memtables for the given CFs; if no CFs are given it should do all of them

this is important for the 0.4 -> 0.5 upgrade since the commitlog format has changed (so we want to let people just blow away the commitlog, after flushing)"
CASSANDRA-501,Bootstrap broken in 0.4.1,Bootstrap fails with NPE in 0.4.1 when you start a node with the -b option
CASSANDRA-481,describe_keyspace fails on the system table,"When the thrift call describe_keyspace is called on the system table, an NPE is raised:

ERROR - Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:174)
        at org.apache.cassandra.service.Cassandra$describe_keyspace_result.write(Cassandra.java:11109)
        at org.apache.cassandra.service.Cassandra$Processor$describe_keyspace.process(Cassandra.java:959)
        at org.apache.cassandra.service.Cassandra$Processor.process(Cassandra.java:627)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
CASSANDRA-459,Commitlog segments don't get deleted,"System table is not created with a periodic flush, so any update there (such as storing token info) can prevent commitlog segments from being deleted."
CASSANDRA-458,Null pointer exception in doIndexing(ColumnIndexer.java:142),"INFO - Saved Token not found. Using 17570558338530880605478324248305304996
INFO - Cassandra starting up...
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@15830327
INFO - Sorting Memtable(Standard1)@15830327
INFO - Writing Memtable(Standard1)@15830327
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-1-Data.db
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@22655307
INFO - Sorting Memtable(Standard1)@22655307
INFO - Writing Memtable(Standard1)@22655307
ERROR - Error in executor futuretask
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logFutur
eExceptions(DebuggableThreadPoolExecutor.java:95)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.afterExe
cute(DebuggableThreadPoolExecutor.java:82)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:887)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.ColumnIndexer.doIndexing(ColumnIndexer.java:1
07)
        at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:62
)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(C
olumnFamilySerializer.java:78)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:22
2)
        at org.apache.cassandra.db.ColumnFamilyStore$2$1.run(ColumnFamilyStore.j
ava:934)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:885)
        ... 2 more
INFO - Standard1 has reached its threshold; switching in a fresh Memtable
INFO - Enqueuing flush of Memtable(Standard1)@14600171
INFO - Sorting Memtable(Standard1)@14600171
INFO - Writing Memtable(Standard1)@14600171
INFO - Completed flushing /spool/cassandra/data/Keyspace1/Standard1-3-Data.db 

How to reproduce: Run perl script pointed below, three at once.  In short, script just inserts a row and immediately removes it.
#!/usr/local/bin/perl
use lib qw(/usr/local/cassandra/interface/gen-perl/Cassandra /usr/local/cassandra/interface/gen-perl);
use strict;

use Cassandra;

use Thrift::Socket;
use Thrift::BinaryProtocol;
use Thrift::FramedTransport;
use Thrift::BufferedTransport;

use Data::Dumper;
use Time::HiRes qw( gettimeofday tv_interval );
use Getopt::Std;
my %opt;
getopts('iu:t:rn:', \%opt);

my $socket = Thrift::Socket->new('localhost', 9160);
   $socket->setSendTimeout(1000);
   $socket->setRecvTimeout(5000);
my $transport =  Thrift::BufferedTransport->new($socket, 1024, 1024);
my $protocol = Thrift::BinaryProtocol->new($transport);
my $client = Cassandra::CassandraClient->new($protocol);

$transport->open();


eval {
    my $id=0;
    for(;;) {
        $id++;
        my $PID = sprintf(""%040lld"", int(1000000 * rand()));
        $client->batch_insert(
            'Keyspace1',
            $PID,
            {
                'Standard1' => _makeColumnList ({
                    map {
                        $_=>'0'x(int(1 + 100 * rand()))
                    } (0..int(1+10*rand()))
                })
            },
            Cassandra::ConsistencyLevel::ONE
        );
 
        $client->remove(
            'Keyspace1',
            $PID,
            Cassandra::ColumnPath->new({
                column_family=>'Standard1',
            }),
            time(),
            Cassandra::ConsistencyLevel::ONE
        );
        print ""$id\n"" if ($id%100 == 0);
    }
};
 
die Dumper($@) if ($@);
 
$transport->close();
sub _makeColumnList($$) {
    my ($row) = @_;
 
    my @cfmap;
 
    foreach my $k (keys %$row) {
        push @cfmap, Cassandra::ColumnOrSuperColumn->new({
            column=>Cassandra::Column->new({
                name=>$k,
                value=>$row->{$k},
                timestamp=>time(),
            })
        });
    }
    die if $#cfmap < 0;
    return \@cfmap;
}

"
CASSANDRA-446,"Use DecoratedKey objects in Memtable, SSTableReader/Writer objects","See CASSANDRA-420 for introduction of DecoratedKey.

IMO we should move Memtable and SSTR objects to using DK objects, too.  This will help Memtable flushes similar to how this helps BMt, and using DK objects in SSTR will avoid a ton of potential confusion about whether a String is a key or a decorated key.

As a side benefit, moving the decoration operation out of memtable flush and into the insert path will help make better use of multicore systems, since we can have multiple write threads but only one core can flush at a time.  (See CASSANDRA-445.)"
CASSANDRA-445,"commitlog may consider writes flushed, that are not yet","Jun Rao explains:

Suppose there are 3 updates u1, u2, and u3. They are written to commit log in that order. If u1 and u3 are applied to memtable first and at that point, a flush is triggered. After the flush completes, it will move the commit log restarting position based on the log for u3. However, u2 hasn't been persisted on disk yet. This means that if the node dies now, the recovery logic won't replay u2 from the log."
CASSANDRA-433,Remove item flush limit in BinaryMemtable,"The BinaryMemtable flushes in memory data to disk when the size of the data reaches a certain limit. There is also a hard coded limit that initiates the flush when more then 50000 items have been inserted. That causes issues if a lot of small items are inserted, we should remove or make the limit configurable."
CASSANDRA-405,Race condition with ConcurrentLinkedHashMap,We are seeing a race condition with ConcurrentLinkedHashMap using appendToTail. We could remove the ConcurrentLinkedHashMap for now until that's resolved.
CASSANDRA-392,Deadlock with SelectorManager.doProcess and TcpConnection.write,"We ran into a deadlock last night:
Name: MESSAGE-SERIALIZER-POOL:2
State: BLOCKED on sun.nio.ch.SelectionKeyImpl@2e257f1b owned by: TCP Selector Manager
Total blocked: 1  Total waited: 1

Stack trace: 
org.apache.cassandra.net.SelectionKeyHandler.turnOnInterestOps(SelectionKeyHandler.java:73)
org.apache.cassandra.net.TcpConnection.write(TcpConnection.java:186)
   - locked org.apache.cassandra.net.TcpConnection@5ab9f791
org.apache.cassandra.net.MessageSerializationTask.run(MessageSerializationTask.java:67)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
java.lang.Thread.run(Thread.java:619)



Name: TCP Selector Manager
State: BLOCKED on org.apache.cassandra.net.TcpConnection@5ab9f791 owned by: MESSAGE-SERIALIZER-POOL:2
Total blocked: 2  Total waited: 0

Stack trace: 
org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:360)
org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:131)
   - locked sun.nio.ch.SelectionKeyImpl@2e257f1b
org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:98)


The SelectionManager.doProcess acquires a monitor on the SelectionKey and then calls methods such as TcpConnection.connect(SelectionKey key) which obtains a monitor for the TcpConnection object itself.  Another task eg: MessageSerializationTask can come along and call write(Message message) which obtains a monitor for the TCPConnection first and then on calls to turnOnInterestOps tries to obtain the monitor for the SelectionKey which causes the deadlock.


"
CASSANDRA-385,intellibootstrap,"ideally bootstrap mode should determine its new position on the ring by examining the Load of the existing nodes in the cluster.  (currently Load is just disk space used but making this pluggable is another ticket.)  having found the highest-load-node-that-is-not-participating-in-bootstrap, it should ask that node for a Token which would move half the keys over to the new node.

This is easily computed since we have a periodic sampling of keys in memory of all the keys on disk, and even SSTable.getIndexedKeys that merges all such keys.  So pick the midpoint, and turn it into a token (these are decorated keys so that is always possible)."
CASSANDRA-367,CommitLog does not flush on writes,"When you write data to CommitLog, we are not calling a flush() in class LogRecordAdder.

Is this acceptable? We added flush() and its working now. This bug was introduced when things were consolidated in r799942"
CASSANDRA-360,Expose out each threadpool's pendingtasks via nodeprobe,"Added this feature will allow administrator to give a rough idea how busy cassandra is handling various tasks.  As some of these tasks are being queues up they may cause increases in memory (eg, BMT). 
"
CASSANDRA-358,SystemTable.initMetadata throws an NPE when called twice,"While this is not the expected use case of it, SystemTable.initMetadata throws an NPE when called twice in the same process.  The error points to tokenColumn being null even through cf is not at line 111."
CASSANDRA-313,File descriptor leak in CommitLog,"There is a file descriptor leak in CommitLog.java.  On systems with a default ulimit of 1024, this causes Cassandra to eventually crash due to too many open files.  A descriptor appears to be leaked at each memtable rotation."
CASSANDRA-287,Make iterator-based read code the One True Path,"Since CASSANDRA-172 we've had two read paths; the old, ad-hoc path based on the faulty assumption that we could skip checking older sstables if we got a hit earlier in the path (fixed in CASSANDRA-223 but still bearing the marks of its origin) and the new iterator-based path.

This makes all read operations go through the iterator path, which cleans things up enormously and sets the stage for CASSANDRA-139.

I introduce a new QueryFilter interface, which has 3 main methods:

    /**
     * returns an iterator that returns columns from the given memtable
     * matching the Filter criteria in sorted order.
     */
    public abstract ColumnIterator getMemColumnIterator(Memtable memtable);

    /**
     * returns an iterator that returns columns from the given SSTable
     * matching the Filter criteria in sorted order.
     */
    public abstract ColumnIterator getSSTableColumnIterator(SSTableReader sstable) throws IOException;

    /**
     * subcolumns of a supercolumn are unindexed, so to pick out parts of those we operate in-memory.
     * @param superColumn
     */
    public abstract void filterSuperColumn(SuperColumn superColumn);

The first two are for pulling out indexed top-level columns, from a memtable or an sstable, respectively.

If the query is on subcolumns of a supercolumn, which are unindexed, CFS.getColumnFamily does an indexed Name filter on the supercolumn, then asks filterSuperColumn on the primary QueryFilter to pick out the parts the user is requesting."
CASSANDRA-230,Race in ChecksumManager.instance(),There is a minor race condition in ChecksumManager.instance(). Patch attached.
CASSANDRA-228,Improve the readability of the DBManager.instance() method,"The DBManager class implements the singleton pattern using a Lock object, but it could replace the Lock object by declaring the instance() method as synchronized. Therefore, it avoids the race condition (as with Lock), but improves the readability and reduces the size of the method. Below is the new version of the method:

	public static synchronized DBManager instance() throws IOException
	{
		if (dbMgr_ == null)
			dbMgr_ = new DBManager();
		return dbMgr_;
	}
"
CASSANDRA-226,Make time-sorted CFs behave consistently,"Time-sorted CFs are indexed and sorted by time in the SSTable.  So lookup by name is inefficient, and compaction sucks (i.e., we can run out of memory), because we combine by name even though we sort by time. (see CASSANDRA-16)

The right fix is, for time-sorted columns we need to recognize that the right ""key"" as it were is (time, name) not name.  We should allow slice by time and lookup by time or time,name but not just name.  (Similarly, we should not allow lookup by time on name-sorted CFs.)  This means that name will not necessarily be unique in time-sorted CFs but that is the right behavior!  Time-based CFs are more like appending to a list than putting to a map.

(As part of this we could track the most recent append in time-sorted CFs to prevent the bug in CASSANDRA-223.  I am not sure that is the ""right"" way to go but I do mention it as a possibility that this change enables.)

In the code, this will allow more regular treatment of CFs and less special casing.  (It will also allow using only a single map for memtable columns, reducing memory usage -- see CASSANDRA-51.)

Implementation notes: we may be able to represent both types of CF with a pluggable Comparator + Indexer combination, which would solve CASSANDRA-185 and CASSANDRA-189 at the same time."
CASSANDRA-208,OOM intermittently during compaction,"jvm crashes intermittently during compaction. Our test data set is not that big, less than 10 GB.
When jvm is about to crash, we see that it consumes a lot of memory (exceeding the max heap size).

The excessive memory usage during compaction is caused by the maintenance of blockIndexes_ in SSTable. this blockIndexes_ was only introduced to the apache version."
CASSANDRA-98,Reads (get_column) miss data or return stale values if a memtable is being flushed,"Reads can return missing values (null/exception) or find stale copies of a column if the read happens during an SSTable flush.

The get_column can go in, and not find the data in the current memtable. When it looks in the ""historical"" memtable, if that CF has already been flushed, then  it gets cleared from the historical memtable. As a result, the read looks for the column in older SSTables and finds a stale value (if it exists) or returns with null.

It can be tricky to reproduce this problem, but the reason is pretty easy to see.

While subsequent reads might return the correct value (from disk), this behavior makes it very difficult for apps that expect to ""read your writes"", at least in the absence of failures."
CASSANDRA-76,Don't rely on flushkey_ special value to force flush,We can force flush programatically w/o needing this workaround.
CASSANDRA-51, Memory footprint for memtable,"The implementation of EfficientBidiMap(EBM) today stores the column in two place, a map and a sorted set. Both data structures store exactly the same values.

I assume we're storing this twice so that the map can give us O(1) reads while the sortedset is important for efficient flush. Is this tradeoff important ? Do we want to store the data twice to get O(1) reads over O(log(n)) reads from sortedset? Is the sortedset implementation broken? Perhaps we should consider a configuration option that turns off the map -- write performance will be slightly improved, read performance will be somewhat worse, and the memory footprint will probably be about half. Certainly sounds like a good alternative tradeoff.
"
CASSANDRA-44,It is difficult to modify the set of ColumnFamliies in an existing cluster,"ColumnFamilies may be added when cassandr is not running by editing the configuration file.

If you need to delete or re-order CFs, you must

1) kill cassandra
2) start it again and wait for log replay to finish
3) kill cassandra AGAIN

Alternatively on Cassandra 0.4.2 or later:
1) run nodeprobe flush and wait for it to finish
2) kill cassandra

Then:
4) make your edits (now there is no data in the commitlog)
5) manually remove the sstable files (-Data.db, -Index.db, and -Filter.db) for the CFs you removed, and rename files for CFs you renamed
6) start cassandra and your edits should take effect"
CASSANDRA-36,Log runtime stats for analysis by ops,"We need to log stats that indicate node / cluster health.  Some that I can think of are:

 - executor pool pending operation (queue) length [memtable per CF, memtablemanager, storageservice.bootstraper, storageservice.consistencymanager]
 - memtable size (which is more useful: per CF or total?)
 - sstable size, per CF
 - number of unmerged SSTables, per CF
 - size of sstable indexes (this is the other major semi-permanent memory chunk)
 - writes, reads per second (throughput)
 - average seconds per write / read (latency)
 - percent of reads that have to hit a SSTable (we don't know if this is in the OS cache or not, so is this actually useful?)
 - commitlog on-disk size (want to make sure these are getting cleaned out regularly)

Currently some of these are logged in an ad-hoc manner, e.g. time to read in ReadVerbHandler, but aggregation is not done and logs on a per-op basis are going to get quite spammy.  I'd like one thread to be in charge of logging, to dump aggregate data (at level INFO) every minute or so.

Might also be nice to expose this on the web console."
CASSANDRA-16,Memory efficient compactions ,"The basic idea is to allow rows to get large enough that they don't have to fit in memory entirely, but can easily fit on a disk. The compaction algorithm today de-serializes the entire row in memory before writing out the compacted SSTable (see ColumnFamilyStore.doCompaction() and associated methods).

The requirement is to have a compaction method with a lower memory requirement so we can support rows larger than available main memory. To re-use the old FB example, if we stored a user's inbox in a row, we'd want the inbox to grow bigger than memory so long as it fit on disk."
