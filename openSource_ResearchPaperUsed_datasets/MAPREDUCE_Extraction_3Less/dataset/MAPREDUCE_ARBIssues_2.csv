Bug_ID,Bug_Summary,Bug_Description
MAPREDUCE-7441,Race condition in closing FadvisedFileRegion,"This issue is similar to the one described in MAPREDUCE-7095, just for FadvisedFileRegion.transferSuccessful. There are warning messages when multiple threads are calling the transferSuccessful method:

{code:java}
2023-05-25 08:41:57,288 WARN org.apache.hadoop.mapred.FadvisedFileRegion: Failed to manage OS cache for /hadoop/data04/yarn/nm/usercache/hive/appcache/application_1684916804740_8245/output/attempt_1684916804740_8245_1_00_001154_0_10003/file.out
EBADF: Bad file descriptor
at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:271)
at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:148)
at org.apache.hadoop.mapred.FadvisedFileRegion.transferSuccessful(FadvisedFileRegion.java:163)
at org.apache.hadoop.mapred.ShuffleChannelHandler.lambda$sendMapOutput$0(ShuffleChannelHandler.java:516)
at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)
at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)


{code}
"
MAPREDUCE-7319,Log list of mappers at trace level in ShuffleHandler audit log,"[MAPREDUCE-6958] added the content length to ShuffleHandler audit log, which is logged at DEBUG level.  After enabling it, we found that the list of mappers for large jobs was filling up our audit logs.  It would be good to move the list of mappers to TRACE level to reduce the logging impact without disabling the log message entirely.

For example a log message like this:
{noformat}
2018-01-25 23:43:02,669 [New I/O worker #1] DEBUG ShuffleHandler.audit: shuffle for job_1512479762132_1318600 reducer 241 length 482072 mappers: [attempt_1512479762132_1318600_1_00_004852_0_10003,
attempt_1512479762132_1318600_1_00_004190_0_10003, attempt_1512479762132_1318600_1_00_004393_0_10003, attempt_1512479762132_1318600_1_00_005057_0_10003, attempt_1512479762132_1318600_1_00_004855_0_10002,
attempt_1512479762132_1318600_1_00_003976_0_10003, attempt_1512479762132_1318600_1_00_004058_0_10003, attempt_1512479762132_1318600_1_00_004355_0_10003, attempt_1512479762132_1318600_1_00_004436_0_10002,
attempt_1512479762132_1318600_1_00_004854_0_10003, attempt_1512479762132_1318600_1_00_005174_0_10004, attempt_1512479762132_1318600_1_00_003972_0_10002, attempt_1512479762132_1318600_1_00_004853_0_10002,
attempt_1512479762132_1318600_1_00_004856_0_10002]
{noformat}
Would become this with {{log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=DEBUG}}:
{noformat}
2018-01-25 23:43:02,669 [New I/O worker #1] DEBUG ShuffleHandler.audit: shuffle for job_1512479762132_1318600 reducer 241 length 482072
{noformat}
And this with {{log4j.logger.org.apache.hadoop.mapred.ShuffleHandler.audit=TRACE}}:
{noformat}
2018-01-25 23:43:02,669 [New I/O worker #1] DEBUG ShuffleHandler.audit: shuffle for job_1512479762132_1318600 reducer 241 length 482072
2018-01-25 23:43:02,669 [New I/O worker #1] TRACE ShuffleHandler.audit: shuffle for job_1512479762132_1318600 mappers: [attempt_1512479762132_1318600_1_00_004852_0_10003,
attempt_1512479762132_1318600_1_00_004190_0_10003, attempt_1512479762132_1318600_1_00_004393_0_10003, attempt_1512479762132_1318600_1_00_005057_0_10003, attempt_1512479762132_1318600_1_00_004855_0_10002,
attempt_1512479762132_1318600_1_00_003976_0_10003, attempt_1512479762132_1318600_1_00_004058_0_10003, attempt_1512479762132_1318600_1_00_004355_0_10003, attempt_1512479762132_1318600_1_00_004436_0_10002,
attempt_1512479762132_1318600_1_00_004854_0_10003, attempt_1512479762132_1318600_1_00_005174_0_10004, attempt_1512479762132_1318600_1_00_003972_0_10002, attempt_1512479762132_1318600_1_00_004853_0_10002,
attempt_1512479762132_1318600_1_00_004856_0_10002]
{noformat}
One question is whether there are any downstream consumers of this audit log that might have a problem with this change?
"
MAPREDUCE-7307,Potential thread leak in LocatedFileStatusFetcher,"We see that when using LocatedFileStatusFetcher to get file infos In parallel, if the listStatus thread is interrupted,  the  executor service in LocatedFileStatusFetcher is left unclosed,  the thread stack will like this:
{noformat}
""GetFileInfo #63"" #125 daemon prio=5 os_prio=0 tid=0x00007f6198106800 nid=0x881 waiting on condition [0x00007f60d9fde000]
java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for <0x0000000082e810a8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){noformat}
This caused by if condition.await() throws InterruptedException,  the method `shutDownNow` for the executor service would not be called as a result, should move such resource releasing call into the finally block."
MAPREDUCE-7241,FileInputFormat listStatus with less memory footprint,"This case sometimes sees in hive when user issues queries over all partitions by mistakes. The file status cached when listing status could accumulate to over 3g.  After digging into the  dumped memory, the LocatedBlock occupies about 50%(sometimes over 60%) memory that retained by LocatedFileStatus, as shows followed,

!filestatus.png!

Right now we only extract the block locations info from LocatedFileStatus,  the datanode infos(types) or block token are not taken into account. So there is no need to cache LocatedBlock, as do like this:

BlockLocation[] blockLocations = dedup(stat.getBlockLocations());
 LocatedFileStatus shrink = new LocatedFileStatus(stat, blockLocations);

private static BlockLocation[] dup(BlockLocation[] blockLocations) {
     BlockLocation[] copyLocs = new BlockLocation[blockLocations.length];
     int i = 0;
     for (BlockLocation location : blockLocations)

{         copyLocs[i++] = new BlockLocation(location);     }

    return copyLocs;
 }

 "
MAPREDUCE-7158,Inefficient Flush Logic in JobHistory EventWriter,"In HDFS, if the flush is implemented to send server request to actually commit the pending writes on the storage service side, we could observe in the benchmark runs that the MR jobs are taking much longer. From investigation we see the current implementation for writing events doesn't look right:
EventWriter# write()
This flush is redundant and this statement should be removed. It defeats the purpose of having a separate flush function itself.
Encoder.flush calls flush of the underlying output stream
After patching with the fix the MR jobs could complete normally, please kindly find the patch in attached."
MAPREDUCE-7150,Optimize collections used by MR JHS to reduce its memory,"We analyzed, using jxray (www.jxray.com) a heap dump of JHS running with big heap in a large clusters, handling large MapReduce jobs. The heap is large (over 32GB) and 21.4% of it is wasted due to various suboptimal Java collections, mostly maps and lists that are either empty or contain only one element. In such under-populated collections considerable amount of memory is still used by just the internal implementation objects. See the attached excerpt from the jxray report for the details. If certain collections are almost always empty, they should be initialized lazily. If others almost always have just 1 or 2 elements, they should be initialized with the appropriate initial capacity of 1 or 2 (the default capacity is 16 for HashMap and 10 for ArrayList).

Based on the attached report, we should do the following:
 # {{FileSystemCounterGroup.map}} - initialize lazily
 # {{CompletedTask.attempts}} - initialize with  capacity 2, given most tasks only have one or two attempts
 # {{JobHistoryParser$TaskInfo.attemptsMap}} - initialize with capacity
 # {{CompletedTaskAttempt.diagnostics}} - initialize with capacity 1 since it contains one diagnostic message most of the time
 # {{CompletedTask.reportDiagnostics}} - switch to ArrayList (no reason to use the more wasteful LinkedList here) and initialize with capacity 1."
MAPREDUCE-7131,Job History Server has race condition where it moves files from intermediate to finished but thinks file is in intermediate,"This is the race condition that can occur:

# during the first *scanIntermediateDirectory()*, *HistoryFileInfo.moveToDone()* is scheduled for job j1
# during the second *scanIntermediateDirectory()*, j1 is found again and put in the *fileStatusList* to process
# *HistoryFileInfo.moveToDone()* is processed in another thread and history files are moved to the finished directory
# the *HistoryFileInfo* for j1 is removed from *jobListCache*
# the j1 in *fileStatusList* is processed and a new *HistoryFileInfo* for j1 is created (history, conf, and summary files will point to the intermediate user directory, and state will be IN_INTERMEDIATE) and added to the *jobListCache*
# *moveToDone()* is scheduled for this new j1
# *moveToDone()* fails during *moveToDoneNow()* for the history file because the source path in the intermediate directory does not exist

From this point on, while the new j1 *HistoryFileInfo* is in the *jobListCache*, the JobHistoryServer will think the history file is in the intermediate directory. If a user queries this job in the JobHistoryServer UI, they will get

{code}
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Could not load history file <scheme>://<host>:<port>/mr-history/intermediate/<user>/job_1529348381246_27275711-1535123223269-<user>-<jobname>-1535127026668-1-0-SUCCEEDED-<queue>-1535126980787.jhist
{code}

Noticed this issue while running 2.7.4, but the race condition seems to still exist in trunk."
MAPREDUCE-7095,Race conditions in closing FadvisedChunkedFile ,"When a file is closed multiple times by multiple threads, all but the first close will generate a WARNING message.
{code:java}
11:04:33.605 AM	WARN	FadvisedChunkedFile	
Failed to manage OS cache for /var/run/100/yarn/nm/usercache/systest/appcache/application_1521665017379_0062/output/attempt_1521665017379_0062_m_012797_0/file.out
EBADF: Bad file descriptor
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
	at org.apache.hadoop.mapred.FadvisedChunkedFile.close(FadvisedChunkedFile.java:76)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.closeInput(ChunkedWriteHandler.java:303)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.discard(ChunkedWriteHandler.java:163)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.flush(ChunkedWriteHandler.java:192)
	at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:137)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.channelClosed(SimpleChannelUpstreamHandler.java:225)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:570)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.channelClosed(FrameDecoder.java:371)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:493)
	at org.jboss.netty.handler.codec.frame.FrameDecoder.channelClosed(FrameDecoder.java:371)
	at org.jboss.netty.handler.ssl.SslHandler.channelClosed(SslHandler.java:1667)
	at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
	at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
	at org.jboss.netty.channel.Channels.fireChannelClosed(Channels.java:468)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:375)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748){code}"
MAPREDUCE-7094,"LocalDistributedCacheManager leaves classloaders open, which leaks FDs","When a user starts a local mapred task from Hive's beeline, it will leave open file descriptors on the HS2 process (which runs the mapred task).

I debugged this and saw that it is caused by LocalDistributedCacheManager class, which creates a new URLClassLoader, with a classpath for the two jars seen below. Somewhere down the line Loaders will be created in this URLClassLoader for these files effectively creating the FD's on the OS level.

This is never cleaned up after execution, although LocalDistributedCacheManager removes the files, it will not close the ClassLoader, so FDs are left open although they point to deleted files at that time:
{code:java}
[root@host-1 ~]# lsof -p 14439 | grep hadoop-hive
java    14439 hive  DEL       REG                8,1             3348748 /tmp/hadoop-hive/mapred/local/1525789796610/hive-exec-core.jar
java    14439 hive  DEL       REG                8,1             3348750 /tmp/hadoop-hive/mapred/local/1525789796609/hive-exec-1.1.0-cdh5.13.4-SNAPSHOT-core.jar
java    14439 hive  649r      REG                8,1   8112438   3348750 /tmp/hadoop-hive/mapred/local/1525789796609/hive-exec-1.1.0-cdh5.13.4-SNAPSHOT-core.jar (deleted)
java    14439 hive  650r      REG                8,1   8112438   3348748 /tmp/hadoop-hive/mapred/local/1525789796610/hive-exec-core.jar (deleted)

{code}"
MAPREDUCE-7058,Race Condition When Stopping DelegationTokenRenewer,"[https://github.com/apache/hadoop/blob/69fa81679f59378fd19a2c65db8019393d7c05a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java]
{code:java}
  private ThreadPoolExecutor renewerService;

  private void processDelegationTokenRenewerEvent(
      DelegationTokenRenewerEvent evt) {
    serviceStateLock.readLock().lock();
    try {
      if (isServiceStarted) {
        renewerService.execute(new DelegationTokenRenewerRunnable(evt));
      } else {
        pendingEventQueue.add(evt);
      }
    } finally {
      serviceStateLock.readLock().unlock();
    }
  }

  @Override
  protected void serviceStop() {
    if (renewalTimer != null) {
      renewalTimer.cancel();
    }
    appTokens.clear();
    allTokens.clear();
    this.renewerService.shutdown();
{code}
{code:java}
2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)
	at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)
	at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)
{code}
What I think is going on here is that the {{serviceStop}} method is not setting the {{isServiceStarted}} flag to 'false'.

Please update so that the {{serviceStop}} method grabs the {{serviceStateLock}} and sets {{isServiceStarted}} to _false_, before shutting down the {{renewerService}} thread pool, to avoid this condition."
MAPREDUCE-7028,Concurrent task progress updates causing NPE in Application Master,"Concurrent task progress updates can cause a NullPointerException in the Application Master (stack trace is with code at current trunk):

{quote}
2017-12-20 06:49:42,369 INFO [IPC Server handler 9 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883
2017-12-20 06:49:42,369 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883
2017-12-20 06:49:42,383 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)
        at java.lang.Thread.run(Thread.java:748)
2017-12-20 06:49:42,385 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883
2017-12-20 06:49:42,386 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
{quote}

This happened naturally in several big wordcount runs, and I could reproduce this reliably by artificially making task updates more frequent."
MAPREDUCE-7025,Avg and Max of memory could be record in TaskCount,"MapReduce Counter is very helpful tool to do statistics, analysis and tuning in industry. One popular way is analysing job historical status of running to optimise continuously. But when the job completed, the memory usage in counter is a snapshot value {{PHYSICAL_MEMORY_BYTES}}. So if we can also record the average value and the max value instead of only the last snapshot value, it could be much helpful.

If you think it' ok. I will contribute the code."
MAPREDUCE-7015,Possible race condition in JHS if the job is not loaded,"There could be a race condition inside JHS. In our build environment, {{TestMRJobClient.testJobClient()}} failed with this exception:

{noformat}
ava.io.FileNotFoundException: File does not exist: hdfs://localhost:32836/tmp/hadoop-yarn/staging/history/done_intermediate/jenkins/job_1509975084722_0001_conf.xml
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1266)
	at org.apache.hadoop.hdfs.DistributedFileSystem$20.doCall(DistributedFileSystem.java:1258)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1258)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:340)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:292)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2123)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2092)
	at org.apache.hadoop.fs.FileSystem.copyToLocalFile(FileSystem.java:2068)
	at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:460)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at org.apache.hadoop.mapreduce.TestMRJobClient.runTool(TestMRJobClient.java:94)
	at org.apache.hadoop.mapreduce.TestMRJobClient.testConfig(TestMRJobClient.java:551)
	at org.apache.hadoop.mapreduce.TestMRJobClient.testJobClient(TestMRJobClient.java:167)
{noformat}

Root cause:
1. MapReduce job completes
2. CLI calls {{cluster.getJob(jobid)}}
3. The job is finished and the client side gets redirected to JHS
4. The job data is missing from {{CachedHistoryStorage}} so JHS tries to find the job
5. First it scans the intermediate directory and finds the job
6. The call {{moveToDone()}} is scheduled for execution on a separate thread inside {{moveToDoneExecutor}} and it starts to run immediately
7. RPC invocation returns with the path pointing to {{/tmp/hadoop-yarn/staging/history/done_intermediate}}
8. The call to {{moveToDone()}} completes which moves the contents of {{done_intermediate}} to {{done}}
9. Hadoop CLI tries to download the config file from done_intermediate but it's no longer there

Usually step #6 is slow enough to complete after #7, but sometimes it's faster, causing this race condition."
MAPREDUCE-6992,Race for temp dir in LocalDistributedCacheManager.java,"When localizing distributed cache files in ""local"" mode, LocalDistributedCacheManager.java chooses a ""unique"" directory based on a millisecond time stamp. When running code with some parallelism, it's possible to run into this.

The error message looks like 
{code}
bq. java.io.FileNotFoundException: jenkins/mapred/local/1508958341829_tmp does not exist
{code}

I ran into this in Impala's data loading. There, we run a HiveServer2 which runs in MapReduce. If multiple queries are submitted simultaneously to the HS2, they conflict on this directory. Googling found that StreamSets ran into something very similar looking at https://issues.streamsets.com/browse/SDC-5473.

I believe the buggy code is (link: https://github.com/apache/hadoop/blob/2da654e34a436aae266c1fbdec5c1067da8d854e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalDistributedCacheManager.java#L94)
{code}
    // Generating unique numbers for FSDownload.
    AtomicLong uniqueNumberGenerator =
        new AtomicLong(System.currentTimeMillis());
{code}

Notably, a similar code path uses an actual random number generator ({{LocalJobRunner.java}}, https://github.com/apache/hadoop/blob/2da654e34a436aae266c1fbdec5c1067da8d854e/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java#L912).
{code}
  public String getStagingAreaDir() throws IOException {
    Path stagingRootDir = new Path(conf.get(JTConfig.JT_STAGING_AREA_ROOT,
        ""/tmp/hadoop/mapred/staging""));
    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
    String user;
    randid = rand.nextInt(Integer.MAX_VALUE);
{code}"
MAPREDUCE-6974,"Add standard configuration keys for HTrace values, propagate across to MR committers if set","HDFS &c support HTrace logging; HBase sets up spans.

What doesn't do spans is MR jobs or other frameworks with use the MR committers.

That can be addressed by defining some standard configuration keys for HTrace hi/low numbers, setting them in job submission, then passing them over the wire in the Configuration, where setupJob and setupTask can extract them & use for the tracing span. They could also add their own span for taskCommit and jobCommit for performance measurement there.

Although the core code would be in MR, I'd propose putting the keys into Hadoop common,
 with some code in {{org.apache.hadoop.tracing.TraceUtils}}, to set it up. That way its possible to use more broadly."
MAPREDUCE-6968,Staging directory erasure coding config property has a typo,"TestMapreduceConfigFields has been failing since MAPREDUCE-6954. MRJobConfig#MR_AM_STAGING_DIR_ERASURECODING_ENABLED is defined as ""yarn.app.mapreduce.am.staging-direrasurecoding.enabled""  but the property is listed as ""yarn.app.mapreduce.am.staging-dir.erasurecoding.enabled"" in mapred-default.xml."
MAPREDUCE-6960,Shuffle Handler prints disk error stack traces for every read failure.,"{code}
 } catch (IOException e) {
          LOG.error(""Shuffle error :"", e);
{code}
In cases where the read from a disk fails and throws a DiskErrorException, the shuffle handler prints the entire stack trace for each and every one of the failures causing the nodemanager logs to quickly fill up the disk. "
MAPREDUCE-6954,Disable erasure coding for files that are uploaded to the MR staging area,"Depending on the encoder/decoder used and the type or MR workload, EC might negatively affect the performance of an MR job if too many files are localized.

In such a scenario, users might want to disable EC in the staging area to speed up the execution."
MAPREDUCE-6852,Job#updateStatus() failed with NPE due to race condition,"Like MAPREDUCE-6762, we found this issue in a cluster where Pig query occasionally failed on NPE - ""Pig uses JobControl API to track MR job status, but sometimes Job History Server failed to flush job meta files to HDFS which caused the status update failed."" Beside NPE in o.a.h.mapreduce.Job.getJobName, we also get NPE in Job.updateStatus() and the exception is as following:
{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:323)
	at org.apache.hadoop.mapreduce.Job$1.run(Job.java:320)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1833)
	at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:320)
	at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:604)
{noformat}
We found state here is null. However, we already check the job state to be RUNNING as code below:
{noformat}
  public boolean isComplete() throws IOException {
    ensureState(JobState.RUNNING);
    updateStatus();
    return status.isJobComplete();
  }
{noformat}
The only possible reason here is two threads are calling here for the same time: ensure state first, then one thread update the state to null while the other thread hit NPE issue here.
We should fix this NPE exception."
MAPREDUCE-6829,Add peak memory usage counter for each task,"Each task has counters PHYSICAL_MEMORY_BYTES and VIRTUAL_MEMORY_BYTES, which are snapshots of memory usage of that task. They are not sufficient for users to understand peak memory usage by that task, e.g. in order to diagnose task failures, tune job parameters or change application design. This new feature will add two more counters for each task: PHYSICAL_MEMORY_BYTES_MAX and VIRTUAL_MEMORY_BYTES_MAX.

This JIRA has the same feature from MAPREDUCE-4710.  I file this new YARN JIRA since MAPREDUCE-4710 is pretty old one from MR 1.x era, it more or less assumes a branch-1 architecture, should be close at this point."
MAPREDUCE-6792,Allow user's full principal name as owner of MapReduce staging directory in JobSubmissionFiles#JobStagingDir(),"Background - 
Currently, {{JobSubmissionFiles#JobStagingDir()}} assumes that file owner returned as part of {{FileSystem#getFileStatus()}} is always user's short principal name, which is true for HDFS. But, some file systems which are HDFS compatible like [Azure Data Lake Store (ADLS) |https://azure.microsoft.com/en-in/services/data-lake-store/] and work in multi tenant environment can have users with same names belonging to different domains. For example, {{user1@company1.com}} and {{user1@company2.com}}. It will be ambiguous, if {{FileSystem#getFileStatus()}} returns only the user's short principal name (without domain name) as the owner of the file/directory. 

The following code block allows only short user principal name as owner. It simply fails saying that ownership on the staging directory is not as expected, if owner returned by the {{FileStatus#getOwner()}} is not equal to short principal name of the current user.
{code}
    String realUser;
    String currentUser;
    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
    realUser = ugi.getShortUserName();
    currentUser = UserGroupInformation.getCurrentUser().getShortUserName();
    if (fs.exists(stagingArea)) {
      FileStatus fsStatus = fs.getFileStatus(stagingArea);
      String owner = fsStatus.getOwner();
      if (!(owner.equals(currentUser) || owner.equals(realUser))) {
         throw new IOException(""The ownership on the staging directory "" +
                      stagingArea + "" is not as expected. "" +
                      ""It is owned by "" + owner + "". The directory must "" +
                      ""be owned by the submitter "" + currentUser + "" or "" +
                      ""by "" + realUser);
      }
	  {code}
The proposal is to remove the strict restriction on short principal name by allowing the user's full principal name as owner of staging area directory in {{JobSubmissionFiles#JobStagingDir()}}."
MAPREDUCE-6768,TestRecovery.testSpeculative failed with NPE,"1 tests failed.
REGRESSION:  org.apache.hadoop.mapreduce.v2.app.TestRecovery.testSpeculative

Error Message:
null

Stack Trace:
java.lang.NullPointerException: null
        at org.apache.hadoop.mapreduce.v2.app.TestRecovery.testSpeculative(TestRecovery.java:1201)"
MAPREDUCE-6762,ControlledJob#toString failed with NPE when job status is not successfully updated,"This issue was found from a cluster where Pig query occasionally failed on NPE. Pig uses JobControl API to track MR job status, but sometimes Job History Server failed to flush job meta files to HDFS which caused the status update failed. Then we get NPE in {{org.apache.hadoop.mapreduce.Job.getJobName}}. The result of this situation is quite confusing: Pig query failed, job history is missing, but the job status on Yarn is succeed."
MAPREDUCE-6724,Single shuffle to memory must not exceed Integer#MAX_VALUE,"When shuffle is done in memory, MergeManagerImpl converts the requested size to an int to allocate an instance of InMemoryMapOutput. This results in an overflow if the requested size is bigger than Integer.MAX_VALUE and eventually causes the reducer to fail."
MAPREDUCE-6721,mapreduce.reduce.shuffle.memory.limit.percent=0.0 should be legal to enforce shuffle to disk,"We are potentially hitting an in-memory-shuffle-related reservation starvation resembling MAPREDUCE-6445. To work it around, we wanted to disable in memory shuffle via mapreduce.reduce.shuffle.memory.limit.percent=0.0 that turned out to be disallowed by the current logic. So we had to resort to another small float value such as 0.0001. However, zero is more logical imo.

"
MAPREDUCE-6673,Add a test example job that grows in memory usage over time,"While working on YARN-1011, I needed to put together an example that would have tasks increase their resource usage deterministically over time. It would be useful for any other utilization related work or stress tests. "
MAPREDUCE-6628,Potential memory leak in CryptoOutputStream,"There is a potential memory leak in {{CryptoOutputStream.java.}}  It allocates two direct byte buffers ({{inBuffer}} and {{outBuffer}}) that get freed when {{close()}} method is called.  Most of the time, {{close()}} method is called.  However, when writing to intermediate Map output file or the spill files in {{MapTask}}, {{close()}} is never called since calling so  would close the underlying stream which is not desirable.  There is a single underlying physical stream that contains multiple logical streams one per partition of Map output.  

By default the amount of memory allocated per byte buffer is 128 KB and  so the total memory allocated is 256 KB,  This may not sound much.  However, if the number of partitions (or number of reducers) is large (in the hundreds) and/or there are spill files created in {{MapTask}}, this can grow into a few hundred MB. 

I can think of two ways to address this issue:

h2. Possible Fix - 1
According to JDK documentation:
{quote}
The contents of direct buffers may reside outside of the normal garbage-collected heap, and so their impact upon the memory footprint of an application might not be obvious.  It is therefore recommended that direct buffers be allocated primarily for large, long-lived buffers that are subject to the underlying system's native I/O operations.  In general it is best to allocate direct buffers only when they yield a measureable gain in program performance.
{quote}
It is not clear to me whether there is any benefit of allocating direct byte buffers in {{CryptoOutputStream.java}}.  In fact, there is a slight CPU overhead in moving data from {{outBuffer}} to a temporary byte array as per the following code in {{CryptoOutputStream.java}}.
{code}
    /*
     * If underlying stream supports {@link ByteBuffer} write in future, needs
     * refine here. 
     */
    final byte[] tmp = getTmpBuf();
    outBuffer.get(tmp, 0, len);
    out.write(tmp, 0, len);
{code}
Even if the underlying stream supports direct byte buffer IO (or direct IO in OS parlance), it is not clear whether it will yield any measurable performance gain.

The fix would be to allocate a ByteBuffer on the heap for inBuffer and wrap a byte array in a {{ByteBuffer}} for {{outBuffer}}.  By the way, the {{inBuffer}} and {{outBuffer}} have to be {{ByteBuffer}} as demanded by the {{encrypt()}} method in {{Encryptor}}.

h2. Possible Fix - 2
Assuming that we want to keep the buffers as direct byte buffers, we can create a new constructor to {{CryptoOutputStream}} and pass a boolean flag {{ownOutputStream}} to indicate whether the underlying stream will be owned by {{CryptoOutputStream}}. If it is true, then calling the {{close()}} method will close the underlying stream.  Otherwise, when {{close()}} is called only the direct byte buffers will be freed and the underlying stream will not be closed.

The scope of changes for this fix will be somewhat wider.  We need to modify {{MapTask.java}}, {{CryptoUtils.java}}, and {{CryptoFSDataOutputStream.java}} as well to pass the ownership flag mentioned above.

I can post a patch for either of the above.  I welcome any other ideas from developers to fix this issue.
"
MAPREDUCE-6621,Memory Leak in JobClient#submitJobInternal(),"In JobClient:
{code}
public RunningJob submitJobInternal(final JobConf conf)
      throws FileNotFoundException, IOException {
    try {
      conf.setBooleanIfUnset(""mapred.mapper.new-api"", false);
      conf.setBooleanIfUnset(""mapred.reducer.new-api"", false);
      Job job = clientUgi.doAs(new PrivilegedExceptionAction<Job> () {
        @Override
        public Job run() throws IOException, ClassNotFoundException, 
          InterruptedException {
          Job job = Job.getInstance(conf);
          job.submit();
          return job;
        }
      });

      // update our Cluster instance with the one created by Job for submission
      // (we can't pass our Cluster instance to Job, since Job wraps the config
      // instance, and the two configs would then diverge)
      cluster = job.getCluster();

      return new NetworkedJob(job);
    } catch (InterruptedException ie) {
      throw new IOException(""interrupted"", ie);
    }
  }
{code}
We will replace the cluster object with the cluster object from Job, but the previous old cluster object would never be closed."
MAPREDUCE-6618,YarnClientProtocolProvider leaking the YarnClient thread. ,"YarnClientProtocolProvider creates YarnRunner which includes ResourceMgrDelegate. In ResourceMgrDelegate, we would initiate and start yarnclient. The yarnClient thread would be leaked due to
{code}
  @Override
  public void close(ClientProtocol clientProtocol) throws IOException {
    // nothing to do
  }
{code} in YarnClientProtocolProvider"
MAPREDUCE-6593,TestJobHistoryEventHandler.testTimelineEventHandling fails on trunk because of NPE,"https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Java8/824/

{code}
Tests run: 13, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.163 sec <<< FAILURE! - in org.apache.hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler
testTimelineEventHandling(org.apache.hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler)  Time elapsed: 5.115 sec  <<< ERROR!
java.lang.NullPointerException: null
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:331)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processEventForTimelineServer(JobHistoryEventHandler.java:1015)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:586)
	at org.apache.hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler.handleEvent(TestJobHistoryEventHandler.java:722)
	at org.apache.hadoop.mapreduce.jobhistory.TestJobHistoryEventHandler.testTimelineEventHandling(TestJobHistoryEventHandler.java:510)
{code}"
MAPREDUCE-6554,MRAppMaster servicestart failing  with NPE in MRAppMaster#parsePreviousJobHistory,"Create scenario so that MR app master gets preempted.
On next MRAppMaster launch tried to recover previous job history file {{MRAppMaster#parsePreviousJobHistory}}


{noformat}
2015-11-21 13:52:27,722 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: java.lang.NullPointerException
java.lang.NullPointerException
        at java.io.StringReader.<init>(StringReader.java:50)
        at org.apache.avro.Schema$Parser.parse(Schema.java:917)
        at org.apache.avro.Schema.parse(Schema.java:966)
        at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)
        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)
        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)
2015-11-21 13:52:27,725 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0

{noformat}

EventReader(EventReader stream)
{noformat}
 this.version = in.readLine();
...
    Schema myschema = new SpecificData(Event.class.getClassLoader()).getSchema(Event.class);
    this.schema = Schema.parse(in.readLine());
{noformat}

"
MAPREDUCE-6541,Exclude scheduled reducer memory when calculating available mapper slots from headroom to avoid deadlock ,"We saw a MR deadlock recently:

- When NM restarted by framework without enable recovery, containers running on these nodes will be identified as ""ABORTED"", and MR AM will try to reschedule ""ABORTED"" mapper containers.
- Since such lost mappers are ""ABORTED"" container, MR AM gives normal mapper priority (priority=20) to such mapper requests. If there's any pending reducer (priority=10) at the same time, mapper requests need to wait for reducer requests satisfied.
- In our test, one mapper needs 700+ MB, reducer needs 1000+ MB, and RM available resource = mapper-request = (700+ MB), only one job was running in the system so scheduler cannot allocate more reducer containers AND MR-AM thinks there're enough headroom for mapper so reducer containers will not be preempted.

MAPREDUCE-6302 can solve most of the problems, but in the other hand, I think we may need to exclude scheduled reducers resource when calculating #available-mapper-slots from headroom. Which we can avoid excessive reducer preemption."
MAPREDUCE-6535,TaskID default constructor results in NPE on toString(),"This code will reproduce the issue:

{code}
new TaskAttemptID().toString();
{code}

The issue is that the default constructor leaves the type {{null}}.  The {{get()}} in {{CharTaskTypesMaps.getRepresentingCharacter()}} then throws an NPE on the null type key.

The simplest solution would be to only call the {{get()}} on line 288 of {{TaskID.java}} if {{type}} is not {{null}} and return some other literal otherwise.  Since no part of the code is tripping on the NPE, what we choose for the literal shouldn't matter.  How about ""x""?"
MAPREDUCE-6528,Memory leak for HistoryFileManager.getJobSummary(),"We meet memory leak issues for JHS in a large cluster which is caused by code below doesn't release FSDataInputStream in exception case. MAPREDUCE-6273 should fix most cases that exceptions get thrown. However, we still need to fix the memory leak for occasional case.

{code} 
private String getJobSummary(FileContext fc, Path path) throws IOException {
    Path qPath = fc.makeQualified(path);
    FSDataInputStream in = fc.open(qPath);
    String jobSummaryString = in.readUTF();
    in.close();
    return jobSummaryString;
  }
{code}"
MAPREDUCE-6521,MiniMRYarnCluster should not create /tmp/hadoop-yarn/staging on local filesystem in unit test,MiniMRYarnCluster create /tmp/hadoop-yarn/staging/history/done by default. It should be under {{testWorkDir}} if the file system is localFs in order to make it to be removed by {{mvn clean}}. It would also avoid issues under parallel unit testing.
MAPREDUCE-6492,AsyncDispatcher exit with NPE on TaskAttemptImpl#sendJHStartEventForAssignedFailTask,"For {{TaskAttemptImpl#DeallocateContainerTransition}} {{sendJHStartEventForAssignedFailTask}} is send for TaskAttemptStateInternal.UNASSIGNED also .


Causing NPE on {{taskAttempt.container.getNodeHttpAddress()}} 


{noformat}
2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)
	at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)
	at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)
	at java.lang.Thread.run(Thread.java:745)
2015-09-28 18:01:48,660 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..
2015-09-28 18:01:48,660 INFO [ContainerLauncher #6] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_e04_1443430524957_0006_01_000059 taskAttempt attempt_1443430524957_0006_m_000000_9
{noformat}

Log aggregation fail for mapreduce application.
"
MAPREDUCE-6452,NPE when intermediate encrypt enabled for LocalRunner,"Enable the below properties try running mapreduce job

mapreduce.framework.name=local
mapreduce.job.encrypted-intermediate-data=true

{code}
2015-08-14 16:27:25,248 WARN  [Thread-21] mapred.LocalJobRunner (LocalJobRunner.java:run(561)) - job_local473843898_0001
java.lang.Exception: java.lang.NullPointerException
        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)
        at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)
        at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

{code}

Jobs are failing always"
MAPREDUCE-6442,Stack trace is missing when error occurs in client protocol provider's constructor,when provider creation fail dump the stack trace rather than just print out the message
MAPREDUCE-6361,NPE issue in shuffle caused by concurrent issue between copySucceeded() in one thread and copyFailed() in another thread on the same host,"The failure in log:
2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25
         at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)
         at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)
         at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
         at java.security.AccessController.doPrivileged(Native Method)
         at javax.security.auth.Subject.doAs(Subject.java:415)
         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
         at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.NullPointerException
         at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)
         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)
         at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
MAPREDUCE-6348,JobHistoryEventHandler could not flush every 30 secondes,"JobHistoryEventHandler could not flush the event every 30 seconds.
cause the var isTimerActive is never set to true.
"
MAPREDUCE-6339,Job history file is not flushed correctly because isTimerActive flag is not set true when flushTimerTask is scheduled.,"Job history file is not flushed correctly because isTimerActive flag is not set true when flushTimerTask is scheduled.
It looks like we should set isTimerActive to true when flushTimerTask is scheduled. Otherwise if a new qualified event comes before the current flush timer is expired,  flushTimerTask will be canceled and rescheduled.
Also I didn't find any code which set isTimerActive flag to true, So isTimerActive is useless in current code."
MAPREDUCE-6334,Fetcher#copyMapOutput is leaking usedMemory upon IOException during InMemoryMapOutput shuffle handler,"We are seeing this happen when
- an NM's disk goes bad during the creation of map output(s)
- the reducer's fetcher can read the shuffle header and reserve the memory
- but gets an IOException when trying to shuffle for InMemoryMapOutput
- shuffle fetch retry is enabled
"
MAPREDUCE-6308,Remove mapreduce.tasktracker.taskmemorymanager.monitoringinterval from trunk,"mapreduce.tasktracker.taskmemorymanager.monitoringinterval is not used anywhere, should be removed."
MAPREDUCE-6307,Remove property mapreduce.tasktracker.taskmemorymanager.monitoringinterval,mapreduce.tasktracker.taskmemorymanager.monitoringinterval is not used anywhere. We should remove the property.
MAPREDUCE-6295,Fix MR resource counter to handle negative value for getting memory resource after YARN-3304,"After YARN-3304, we will get negative value for memory resource if resource data is unavailable. MR resource counter shouldn't put negative value there so a simple fix is required."
MAPREDUCE-6275,Race condition in FileOutputCommitter v2 for user-specified task output subdirs,
MAPREDUCE-6261,NullPointerException if MapOutputBuffer.flush invoked twice,"MapOutputBuffer.flush will throw an NPE if it is invoked twice, since it blindly assumes kvbuffer is not null yet sets kvbuffer to null towards the end of the method."
MAPREDUCE-6198,NPE from JobTracker#resolveAndAddToTopology in MR1 cause initJob and heartbeat failure.,"NPE from JobTracker#resolveAndAddToTopology in MR1 cause initJob and heartbeat failure. The NPE is caused by dnsToSwitchMapping.resolve return null at the following:
{code}
    List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);
    String rName = rNameList.get(0);
{code}
I check the code in MR2, MR2 handle it correctly in coreResolve  of RackResolver.java
{code}
    List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);
    String rName = null;
    if (rNameList == null || rNameList.get(0) == null) {
      rName = NetworkTopology.DEFAULT_RACK;
      LOG.info(""Couldn't resolve "" + hostName + "". Falling back to ""
          + NetworkTopology.DEFAULT_RACK);
    } else {
      rName = rNameList.get(0);
      LOG.info(""Resolved "" + hostName + "" to "" + rName);
    }
{code}

We should do the same in MR1, if dnsToSwitchMapping.resolve return null, use NetworkTopology.DEFAULT_RACK."
MAPREDUCE-6174,Combine common stream code into parent class for InMemoryMapOutput and OnDiskMapOutput.,"Per MAPREDUCE-6166, both InMemoryMapOutput and OnDiskMapOutput will be doing similar things with regards to IFile streams.

In order to make it explicit that InMemoryMapOutput and OnDiskMapOutput are different from 3rd-party implementations, this JIRA will make them subclass a common class (see https://issues.apache.org/jira/browse/MAPREDUCE-6166?focusedCommentId=14223368&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14223368)"
MAPREDUCE-6169,MergeQueue should release reference to the current item from key and value at the end of the iteration to save memory.,"MergeQueue should release reference to the current item from key and value at the end of the iteration to save memory.
these buffers referenced by key and value can be large, which may cause an OOM error."
MAPREDUCE-6135,Job staging directory remains if MRAppMaster is OOM,"If MRAppMaster attempts run out of memory, it won't go through the normal job clean up process to move history files to history server location. When customers try to find out why the job failed, the data won't be available on history server webUI.

The work around is to extract the container id and NM id from the jhist file in the job staging directory; then use ""yarn logs"" command to get the AM logs.

It would be great the platform can take care of it by moving these hist files automatically to history server if AM attempts don't exit properly.

We discuss ideas on how to address this and would like get suggestions from others. Not sure if timeline server design covers this scenario.

1. Define some protocol for YARN to tell AppMaster ""you have exceeded AM max attempt, please clean up"". For example, YARN can launch AppMaster one more time after AM max attempt and MRAppMaster use that as the indication this is clean-up-only attempt.

2. Have some program periodically check job statuses and move files from job staging directory to history server for those finished jobs."
MAPREDUCE-6131,Integer overflow in RMContainerAllocator results in starvation of applications,"When processing large datasets, Hadoop encounters a scenario where all
 containers run reduce tasks and no map tasks are scheduled. The 
application does not fail but rather remains in this state without making 
any forward progress. It then has to be manually terminated. 

This bug is due to integer overflow in scheduleReduces() of 
RMContainerAllocator. The variable netScheduledMapMem overflows for 
large data sizes, takes negative value, and results in a large 
finalReduceMemLimit and a large rampup value. In almost all cases, this 
large rampup value is greater than the total number of reduce tasks. 
Therefore, the AM tries to assign all reduce tasks. And if the total number 
of reduce tasks is greater than the total container slots, then all slots are 
taken up by reduce tasks, leaving none for maps. 

With 128MB block size and 2GB map container size, overflow occurs with 128 TB data size. An example scenario for the reproduction is: 

- Input data size of 32TB, block size 128MB, Map container size = 10GB,
reduce container size = 10GB, #reducers = 50,  cluster mem capacity =  7 x 40GB, slowstart=0.0

Better resolution might be to change the variables used in 
RMContainerAllocator from int to long. A simpler fix instead would be to 
only change the local variables of scheduleReduces() to long data types. 
Patch is attached for 2.2.0. 

"
MAPREDUCE-6108,ShuffleError OOM while reserving memory by MergeManagerImpl,"Shuffle has OOM issue from time to time.  

Such as this email reported.
http://mail-archives.apache.org/mod_mbox/hadoop-mapreduce-dev/201408.mbox/%3CCABWXXjNK-on0XTrMuriJD8SDGJjTAMSvQW2CZpm3oEkJ3YM8YQ@mail.gmail.com%3E"
MAPREDUCE-6049,AM JVM does not exit if MRClientService gracefull shutdown fails,"Eventhough job got FAILED, AM process still not exiting

ThreadDump of AM process is below
{noformat}
""Job Fail Wait Timeout Monitor #0"" daemon prio=10 tid=0x0000000000aa9000 nid=0x41fa waiting on condition [0x00007f0e0d1d0000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000c104c688> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1079)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{noformat}"
MAPREDUCE-5928,Deadlock allocating containers for mappers and reducers,"I have a small cluster consisting of 8 desktop class systems (1 master + 7 workers).
Due to the small memory of these systems I configured yarn as follows:
{quote}
yarn.nodemanager.resource.memory-mb = 2200
yarn.scheduler.minimum-allocation-mb = 250
{quote}
On my client I did
{quote}
mapreduce.map.memory.mb = 512
mapreduce.reduce.memory.mb = 512
{quote}
Now I run a job with 27 mappers and 32 reducers.
After a while I saw this deadlock occur:
-	All nodes had been filled to their maximum capacity with reducers.
-	1 Mapper was waiting for a container slot to start in.

I tried killing reducer attempts but that didn't help (new reducer attempts simply took the existing container).

*Workaround*:
I set this value from my job. The default value is 0.05 (= 5%)
{quote}
mapreduce.job.reduce.slowstart.completedmaps = 0.99f
{quote}
"
MAPREDUCE-5896,InputSplits should indicate which locations have the block cached in memory,
MAPREDUCE-5867,Possible NPE in KillAMPreemptionPolicy related to ProportionalCapacityPreemptionPolicy,"I configured KillAMPreemptionPolicy for My Application Master and tried to check preemption of queues.
In one scenario I have seen below NPE in my AM

014-04-24 15:11:08,860 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy.preempt(KillAMPreemptionPolicy.java:57)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.getResources(RMContainerAllocator.java:662)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:246)
	at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:267)
	at java.lang.Thread.run(Thread.java:662)

I was using 2.2.0 and merged MAPREDUCE-5189 to see how AM preemption works."
MAPREDUCE-5823,TestTaskAttempt fails in trunk and branch-2 with NPE,"Here is the console output I got

{noformat}
java.lang.NullPointerException: null
	at org.apache.hadoop.security.token.Token.write(Token.java:221)
	at org.apache.hadoop.mapred.ShuffleHandler.serializeServiceData(ShuffleHandler.java:272)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createCommonContainerLaunchContext(TaskAttemptImpl.java:715)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.createContainerLaunchContext(TaskAttemptImpl.java:801)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1516)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1493)
	at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)
	at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)
	at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)
	at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)
	at org.apache.hadoop.mapreduce.v2.app.job.impl.TestTaskAttempt.testTooManyFetchFailureAfterKill(TestTaskAttempt.java:660)
{noformat}"
MAPREDUCE-5815,Fix NPE in TestMRAppMaster,Working MAPREDUCE-5813 I stumbled on NPE's in TestMRAppMaster. They seem to be introduced by MAPREDUCE-5805.
MAPREDUCE-5785,Derive heap size or mapreduce.*.memory.mb automatically,"Currently users have to set 2 memory-related configs per Job / per task type.  One first chooses some container size map reduce.\*.memory.mb and then a corresponding maximum Java heap size Xmx < map reduce.\*.memory.mb. This makes sure that the JVM's C-heap (native memory + Java heap) does not exceed this mapreduce.*.memory.mb. If one forgets to tune Xmx, MR-AM might be 
- allocating big containers whereas the JVM will only use the default -Xmx200m.
- allocating small containers that will OOM because Xmx is too high.

With this JIRA, we propose to set Xmx automatically based on an empirical ratio that can be adjusted. Xmx is not changed automatically if provided by the user.
"
MAPREDUCE-5748,Potential null pointer dereference in ShuffleHandler#Shuffle#messageReceived(),"Starting around line 510:
{code}
      ChannelFuture lastMap = null;
      for (String mapId : mapIds) {
...
      }
      lastMap.addListener(metrics);
      lastMap.addListener(ChannelFutureListener.CLOSE);
{code}
If mapIds is empty, lastMap would remain null, leading to NPE in addListener() call."
MAPREDUCE-5729,mapred job -list throws NPE,"mapred job -list throws the following NPE:
{noformat}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.TypeConverter.fromYarn(TypeConverter.java:445)
	at org.apache.hadoop.mapreduce.TypeConverter.fromYarnApps(TypeConverter.java:460)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.getAllJobs(ResourceMgrDelegate.java:125)
	at org.apache.hadoop.mapred.YARNRunner.getAllJobs(YARNRunner.java:164)

{noformat}"
MAPREDUCE-5693,Restore MRv1 behavior for log flush,"to improve log consistency and completeness for diagnostics in the case of JVM crashes and SIGTERMing by NM this JIRA proposes to restore the MRv1 behavior of periodic log syncing (every 5s) and having log sync as part of a shutdown hook.
"
MAPREDUCE-5688,TestStagingCleanup fails intermittently with JDK7,"Due to random ordering ordering in JDK7, the test TestStagingCleanup#testDeletionofStagingOnKillLastTry is failing

{noformat}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.231 sec <<< FAILURE!
test(org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup)  Time elapsed: 3882 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:349)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)
	at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)
	at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:159)
	at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:132)
	at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)
	at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$MRAppMasterShutdownHook.run(MRAppMaster.java:1399)
	at org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup.testDeletionofStagingOnKillLastTry(TestStagingCleanup.java:239)
	at org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup.test(TestStagingCleanup.java:82)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:243)
	at junit.framework.TestSuite.run(TestSuite.java:238)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:242)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:137)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
{noformat}"
MAPREDUCE-5687,TestYARNRunner#testResourceMgrDelegate fails with NPE after YARN-1446,"On trunk, I got:
{code}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1.049 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestYARNRunner
testResourceMgrDelegate(org.apache.hadoop.mapred.TestYARNRunner)  Time elapsed: 0.782 sec  <<< ERROR!
java.lang.NullPointerException: null
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.killApplication(YarnClientImpl.java:201)
	at org.apache.hadoop.mapred.ResourceMgrDelegate.killApplication(ResourceMgrDelegate.java:284)
	at org.apache.hadoop.mapred.TestYARNRunner.testResourceMgrDelegate(TestYARNRunner.java:212)
{code}"
MAPREDUCE-5679,TestJobHistoryParsing has race condition,"org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing can fail because of race condition.
{noformat}
testHistoryParsingWithParseErrors(org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing)  Time elapsed: 4.102 sec  <<< ERROR!
java.io.IOException: Unable to initialize History Viewer
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:520)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:398)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:137)
        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:339)
        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:798)
        at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.<init>(JobHistoryParser.java:86)
        at org.apache.hadoop.mapreduce.jobhistory.HistoryViewer.<init>(HistoryViewer.java:85)
        at org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing.checkHistoryParsing(TestJobHistoryParsing.java:339)
        at org.apache.hadoop.mapreduce.v2.hs.TestJobHistoryParsing.testHistoryParsingWithParseErrors(TestJobHistoryParsing.java:125)
{noformat}

In the checkHistoryParsing() function, after 
{code}
HistoryFileInfo fileInfo = jobHistory.getJobFileInfo(jobId);
{code}
a thread named MoveIntermediateToDone will be launched to move history file from done_intermediate to done directory.
If the history file is moved, 
{code}
      HistoryViewer viewer = new HistoryViewer(fc.makeQualified(
          fileInfo.getHistoryFile()).toString(), conf, true);
{code}
will throw IOException，because the history file is not found."
MAPREDUCE-5660,Log info about possible thrashing (when using memory-based scheduling in Capacity Scheduler) is not printed,"There is a tiny, but confusing when troubleshooting, bug in TaskTracker code:
{code}
if (totalMemoryAllottedForTasks > totalPhysicalMemoryOnTT) {
  LOG.info(""totalMemoryAllottedForTasks > totalPhysicalMemoryOnTT.""
      + "" Thrashing might happen."");
} else if (totalMemoryAllottedForTasks > totalVirtualMemoryOnTT) {
  LOG.info(""totalMemoryAllottedForTasks > totalVirtualMemoryOnTT.""
      + "" Thrashing might happen."");
}
{code}
totalMemoryAllottedForTasks is calculated in megabytes, while totalPhysicalMemoryOnTT (and totalVirtualMemoryOnTT) is calculated in bytes. totalMemoryAllottedForTasks should be converted to bytes for a correct comparison."
MAPREDUCE-5653,"DistCp does not honour config-overrides for mapreduce.[map,reduce].memory.mb","When a DistCp job is run through Oozie (through a Java action that launches DistCp), one sees that mapred.child.java.opts as set from the caller is honoured by DistCp. But, DistCp doesn't seem to honour any overrides for configs mapreduce.[map,reduce].memory.mb.

Problem has been identified. I'll post a patch shortly."
MAPREDUCE-5649,Reduce cannot use more than 2G memory  for the final merge,"In the org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.java file, in the finalMerge method: 

 int maxInMemReduce = (int)Math.min(
        Runtime.getRuntime().maxMemory() * maxRedPer, Integer.MAX_VALUE);
 
This means no matter how much memory user has, reducer will not retain more than 2G data in memory before the reduce phase starts.
"
MAPREDUCE-5623,TestJobCleanup fails because of RejectedExecutionException and NPE.,"org.apache.hadoop.mapred.TestJobCleanup can fail because of RejectedExecutionException by NonAggregatingLogHandler. This problem is described in YARN-1409. TestJobCleanup can still fail after fixing RejectedExecutionException, because of NPE by Job#getCounters()'s returning null.

{code}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.mapred.TestJobCleanup
-------------------------------------------------------------------------------
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 140.933 sec <<< FAILURE! - in org.apache.hadoop.mapred.TestJobCleanup
testCustomAbort(org.apache.hadoop.mapred.TestJobCleanup)  Time elapsed: 31.068 sec  <<< ERROR!
java.lang.NullPointerException: null
        at org.apache.hadoop.mapred.TestJobCleanup.testFailedJob(TestJobCleanup.java:199)
        at org.apache.hadoop.mapred.TestJobCleanup.testCustomAbort(TestJobCleanup.java:296)
{code}"
MAPREDUCE-5607,Backport MAPREDUCE-5086 - MR app master deletes staging dir when sent a reboot command from the RM,"If the RM is restarted when the MR job is running, then it sends a reboot command to the job. The job ends up deleting the staging dir and that causes the next attempt to fail."
MAPREDUCE-5603,Ability to disable FileInputFormat listLocatedStatus optimization to save client memory,It would be nice if users had the option to disable the listLocatedStatus optimization in FileInputFormat to save client memory.
MAPREDUCE-5584,ShuffleHandler becomes unresponsive during gridmix runs and can leak file descriptors,While running gridmix on 2.3 we noticed that jobs are running much slower than normal.  We tracked this down to reducers having difficulties shuffling data from maps.  Details to follow.
MAPREDUCE-5582,Setting mapred.job.reduce.memory.mb to 0 from a job with CapacityTracker leads to inconsistent state in JVMManager,"If a job sets mapred.job.reduce.memory.mb to 0 the capacity scheduler incorrectly allocates resources eventually causing ""Inconsistent state!!! JVM Manager reached an unstable state while reaping a JVM for task"" errors from the JVMManager killing all TaskTrackers that have been used for the job."
MAPREDUCE-5571,allow access to the DFS job submission + staging directory by members of the job submitters group,"The job submission and staging directories are explicitly given 0700 permissions restricting access of job submission files only to the submitter UID. this prevents hadoop daemon services running under different UIDs from reading the job submitters files.  it is common unix practice to run daemon services under their own UIDs for security purposes.

This bug can be demonstrated by creating a single node configuration, which runs LocalFileSystem and not HDFS.  Create two users and add them to a 'hadoop' group.  Start the hadoop services with one of the users, then submit a map/reduce job with the other user (or run one of the examples).  Job submission ultimately fails and the M/R job doesn't execute.

The fix is simple enough and secure-- change the staging directory permissions to 2750.  i have demonstrated the patch against 2.0.5 (along  with another fix for an incorrect decimal->octal conversion) and will attach the patch.

this bug is present since very early versions.  i would like to fix it at the lowest level as  it's a simple file mode change in all versions, and localized to one file.  is this possible?"
MAPREDUCE-5543,In-memory map outputs can be leaked after shuffle completes in 0.23,"MergeManagerImpl#close adds the contents of inMemoryMergedMapOutputs and inMemoryMapOutputs to a list of map outputs that is subsequently processed, but it does not clear those sets.  This prevents some of the map outputs from being garbage collected and significantly reduces the memory available for the subsequent reduce phase.

This was fixed for trunk and branch-2 by MAPREDUCE-5493, but that has since been closed after 2.1.1 released.  This JIRA tracks backporting the fix to branch-0.23 as well."
MAPREDUCE-5542,Killing a job just as it finishes can generate an NPE in client,If a client tries to kill a job just as the job is finishing then the client can crash with an NPE.
MAPREDUCE-5517,enabling uber mode with 0 reducer still requires mapreduce.reduce.memory.mb to be less than yarn.app.mapreduce.am.resource.mb,"Since there is no reducer, the memory allocated to reducer is irrelevant to enable uber mode of a job"
MAPREDUCE-5508,JobTracker memory leak caused by unreleased FileSystem objects in JobInProgress#cleanupJob,"MAPREDUCE-5351 fixed a memory leak problem but introducing another filesystem object (see ""tempDirFs"") that is not properly released.
{code} JobInProgress#cleanupJob()

  void cleanupJob() {
...
          tempDirFs = jobTempDirPath.getFileSystem(conf);
          CleanupQueue.getInstance().addToQueue(
              new PathDeletionContext(jobTempDirPath, conf, userUGI, jobId));
...
 if (tempDirFs != fs) {
      try {
        fs.close();
      } catch (IOException ie) {
...
}
{code}
"
MAPREDUCE-5493,In-memory map outputs can be leaked after shuffle completes,"MergeManagerImpl#close adds the contents of inMemoryMergedMapOutputs and inMemoryMapOutputs to a list of map outputs that is subsequently processed, but it does not clear those sets.  This prevents some of the map outputs from being garbage collected and significantly reduces the memory available for the subsequent reduce phase."
MAPREDUCE-5476,Job can fail when RM restarts after staging dir is cleaned but before MR successfully unregister with RM,
MAPREDUCE-5446,TestJobHistoryEvents and TestJobHistoryParsing have race conditions,"TestJobHistoryEvents and TestJobHistoryParsing are not properly waiting for MRApp to finish.  Currently they are polling the service state looking for Service.STATE.STOPPED, but the service can appear to be in that state *before* it is fully stopped.  This causes tests to finish with MRApp threads still in-flight, and those threads can conflict with subsequent tests when they collide in the filesystem.
"
MAPREDUCE-5429,App Master throw OutOfMemoryErrors.,"While running job , got OOM in app master and exitted the app master jvm.

{noformat}
2013-07-28 13:45:21,937 ERROR [IPC Server handler 14 on 59522] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:job_1374920247399_0422 (auth:TOKEN) cause:java.io.IOException: java.lang.OutOfMemoryError: Java heap space
2013-07-28 13:45:21,937 INFO [IPC Server handler 22 on 59522] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1374920247399_0422_r_000384_0
2013-07-28 13:45:46,100 INFO [IPC Server handler 22 on 59522] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1374920247399_0422_r_000384_0 is : 0.22976667
2013-07-28 13:45:21,937 ERROR [IPC Server handler 15 on 59522] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:job_1374920247399_0422 (auth:TOKEN) cause:java.io.IOException: java.lang.OutOfMemoryError: Java heap space
2013-07-28 13:45:21,937 ERROR [IPC Server handler 13 on 59522] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:job_1374920247399_0422 (auth:TOKEN) cause:java.io.IOException: java.lang.OutOfMemoryError: Java heap space
2013-07-28 13:45:54,522 INFO [IPC Server handler 15 on 59522] org.apache.hadoop.ipc.Server: IPC Server handler 15 on 59522, call statusUpdate(attempt_1374920247399_0422_r_000225_0, org.apache.hadoop.mapred.ReduceTaskStatus@dd89c26), rpc version=2, client version=19, methodsFingerPrint=937413979 from 10.71.115.238:59691: error: java.io.IOException: java.lang.OutOfMemoryError: Java heap space
java.io.IOException: java.lang.OutOfMemoryError: Java heap space
2013-07-28 13:45:21,937 INFO [IPC Server handler 19 on 59522] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1374920247399_0422_r_000307_0
2013-07-28 13:45:21,937 INFO [IPC Server handler 16 on 59522] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Status update from attempt_1374920247399_0422_r_000552_0
2013-07-28 13:46:09,900 INFO [IPC Server handler 16 on 59522] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1374920247399_0422_r_000552_0 is : 0.17983334
2013-07-28 13:45:14,870 ERROR [IPC Server handler 6 on 59522] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:job_1374920247399_0422 (auth:TOKEN) cause:java.io.IOException: java.lang.OutOfMemoryError: Java heap space
2013-07-28 13:45:14,870 FATAL [ResponseProcessor for block BP-myhacluster-25656:blk_-2026966945468195799_12352] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[ResponseProcessor for block BP-myhacluster-25656:blk_-2026966945468195799_12352,5,main] threw an Error.  Shutting down now...
java.lang.OutOfMemoryError: Java heap space
{noformat}"
MAPREDUCE-5423,Rare deadlock situation when reducers try to fetch map output,"During our cluster deployment, we found there is a very rare deadlock situation when reducers try to fetch map output. We had 5 fetchers and log snippet illustrates this problem is below (all fetchers went into a wait state after they can't acquire more RAM beyond the memoryLimit and no fetcher is releasing memory):

2013-07-18 04:32:28,135 INFO [main] org.apache.hadoop.mapreduce.task.reduce.MergeManager: MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2013-07-18 04:32:28,138 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events
2013-07-18 04:32:28,146 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0: Got 1 new map-outputs
2013-07-18 04:32:28,146 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 1 to fetcher#1
2013-07-18 04:32:28,146 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to 101-09-04.sc1.verticloud.com:8080 to fetcher#1
2013-07-18 04:32:28,319 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000017_0 sent hash and receievd reply
2013-07-18 04:32:28,320 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1373902166027_0622_m_000017_0 decomp: 27 len: 31 to MEMORY
2013-07-18 04:32:28,325 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 27 bytes from map-output for attempt_1373902166027_0622_m_000017_0
2013-07-18 04:32:28,325 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 27, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;27
2013-07-18 04:32:28,325 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: 101-09-04.sc1.verticloud.com:8080 freed by fetcher#1 in 179s
2013-07-18 04:32:33,158 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0: Got 1 new map-outputs
2013-07-18 04:32:33,158 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 1 to fetcher#1
2013-07-18 04:32:33,158 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to 101-09-04.sc1.verticloud.com:8080 to fetcher#1
2013-07-18 04:32:33,161 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000016_0 sent hash and receievd reply
2013-07-18 04:32:33,200 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1373902166027_0622_m_000016_0 decomp: 55841282 len: 55841286 to MEMORY
2013-07-18 04:32:33,322 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 55841282 bytes from map-output for attempt_1373902166027_0622_m_000016_0
2013-07-18 04:32:33,323 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 55841282, inMemoryMapOutputs.size() -&gt; 2, commitMemory -&gt; 27, usedMemory -&gt;55841309
2013-07-18 04:32:39,594 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 118022137 bytes from map-output for attempt_1373902166027_0622_m_000015_0
2013-07-18 04:32:39,594 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 118022137, inMemoryMapOutputs.size() -&gt; 3, commitMemory -&gt; 55841309, usedMemory -&gt;173863446
2013-07-18 04:32:39,594 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: 101-09-04.sc1.verticloud.com:8080 freed by fetcher#1 in 413s
2013-07-18 04:32:42,188 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0: Got 1 new map-outputs
2013-07-18 04:32:42,188 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 1 to fetcher#1
2013-07-18 04:32:42,188 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to 101-09-04.sc1.verticloud.com:8080 to fetcher#1
2013-07-18 04:32:42,190 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000014_0 sent hash and receievd reply
2013-07-18 04:32:42,277 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1373902166027_0622_m_000014_0 decomp: 140715962 len: 140715966 to MEMORY
2013-07-18 04:32:42,493 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 140715962 bytes from map-output for attempt_1373902166027_0622_m_000014_0
2013-07-18 04:32:42,493 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 140715962, inMemoryMapOutputs.size() -&gt; 4, commitMemory -&gt; 173863446, usedMemory -&gt;314579408
2013-07-18 04:32:42,494 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: 101-09-04.sc1.verticloud.com:8080 freed by fetcher#1 in 306s
2013-07-18 04:32:43,192 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0: Got 1 new map-outputs
2013-07-18 04:32:43,192 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 1 to fetcher#1
2013-07-18 04:32:43,192 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to 101-09-04.sc1.verticloud.com:8080 to fetcher#1
2013-07-18 04:32:43,195 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000013_0 sent hash and receievd reply
2013-07-18 04:32:43,280 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1373902166027_0622_m_000013_0 decomp: 141243082 len: 141243086 to MEMORY
2013-07-18 04:32:43,506 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 141243082 bytes from map-output for attempt_1373902166027_0622_m_000013_0
2013-07-18 04:32:43,506 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 141243082, inMemoryMapOutputs.size() -&gt; 5, commitMemory -&gt; 314579408, usedMemory -&gt;455822490
2013-07-18 04:32:43,507 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: 101-09-04.sc1.verticloud.com:8080 freed by fetcher#1 in 315s
2013-07-18 04:32:44,195 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0: Got 1 new map-outputs
2013-07-18 04:32:44,195 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 1 to fetcher#1
2013-07-18 04:32:44,195 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 1 of 1 to 101-09-04.sc1.verticloud.com:8080 to fetcher#1
2013-07-18 04:32:44,198 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000011_0 sent hash and receievd reply
2013-07-18 04:32:44,305 INFO [fetcher#1] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#1 about to shuffle output of map attempt_1373902166027_0622_m_000011_0 decomp: 173528412 len: 173528416 to MEMORY
...
2013-07-18 04:32:56,901 INFO [fetcher#2] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 282474777 bytes from map-output for attempt_1373902166027_0622_m_000001_0
2013-07-18 04:32:56,901 INFO [fetcher#2] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 282474777, inMemoryMapOutputs.size() -&gt; 5, commitMemory -&gt; 1179552807, usedMemory -&gt;1462027584
2013-07-18 04:32:56,901 INFO [fetcher#2] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: 101-09-04.sc1.verticloud.com:8080 freed by fetcher#2 in 2682s
2013-07-18 04:32:56,901 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 4 to fetcher#4
2013-07-18 04:32:56,902 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 4 of 4 to 101-09-04.sc1.verticloud.com:8080 to fetcher#4
2013-07-18 04:32:56,904 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000006_0,attempt_1373902166027_0622_m_000002_0,attempt_1373902166027_0622_m_000003_0,attempt_1373902166027_0622_m_000005_0 sent hash and receievd reply
2013-07-18 04:32:57,336 INFO [EventFetcher for fetching Map Completion Events] org.apache.hadoop.mapreduce.task.reduce.EventFetcher: attempt_1373902166027_0622_r_000001_0: Got 1 new map-outputs
2013-07-18 04:32:57,414 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#4 about to shuffle output of map attempt_1373902166027_0622_m_000006_0 decomp: 280156692 len: 280156696 to MEMORY
2013-07-18 04:32:57,867 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.Fetcher: Read 280156692 bytes from map-output for attempt_1373902166027_0622_m_000006_0
2013-07-18 04:32:57,867 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.MergeManager: closeInMemoryFile -&gt; map-output of size: 280156692, inMemoryMapOutputs.size() -&gt; 6, commitMemory -&gt; 1462027584, usedMemory -&gt;1742184276
2013-07-18 04:32:57,900 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#4 - MergerManager returned Status.WAIT ...
2013-07-18 04:32:57,901 INFO [fetcher#4] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: 101-09-04.sc1.verticloud.com:8080 freed by fetcher#4 in 999s
2013-07-18 04:32:57,901 INFO [fetcher#3] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: Assiging 101-09-04.sc1.verticloud.com:8080 with 4 to fetcher#3
2013-07-18 04:32:57,901 INFO [fetcher#3] org.apache.hadoop.mapreduce.task.reduce.ShuffleScheduler: assigned 4 of 4 to 101-09-04.sc1.verticloud.com:8080 to fetcher#3
2013-07-18 04:32:57,903 INFO [fetcher#3] org.apache.hadoop.mapreduce.task.reduce.Fetcher: for url=8080/mapOutput?job=job_1373902166027_0622&amp;reduce=1&amp;map=attempt_1373902166027_0622_m_000000_0,attempt_1373902166027_0622_m_000002_0,attempt_1373902166027_0622_m_000005_0,attempt_1373902166027_0622_m_000003_0 sent hash and receievd reply
2013-07-18 04:32:57,904 INFO [fetcher#3] org.apache.hadoop.mapreduce.task.reduce.Fetcher: fetcher#3 - MergerManager returned Status.WAIT ...
..."
MAPREDUCE-5381,Support graceful decommission of tasktracker,"When TTs are decommissioned for non-fault reasons (capacity change etc.), it's desirable to minimize the impact to running jobs.

Currently if a TT is decommissioned, all running tasks on the TT need to be rescheduled on other TTs. Further more, for finished map tasks, if their map output are not fetched by the reducers of the job, these map tasks will need to be rerun as well.

We propose to introduce a mechanism to optionally gracefully decommission a tasktracker."
MAPREDUCE-5368,"Save memory by  set capacity, load factor and concurrency level for ConcurrentHashMap in TaskInProgress","Below is histo from our JobTracker:

 num     #instances         #bytes  class name
----------------------------------------------
   1:     136048824    11347237456  [C
   2:     124156992     5959535616  java.util.concurrent.locks.ReentrantLock$NonfairSync
   3:     124156973     5959534704  java.util.concurrent.ConcurrentHashMap$Segment
   4:     135887753     5435510120  java.lang.String
   5:     124213692     3975044400  [Ljava.util.concurrent.ConcurrentHashMap$HashEntry;
   6:      63777311     3061310928  java.util.HashMap$Entry
   7:      35038252     2803060160  java.util.TreeMap
   8:      16921110     2712480072  [Ljava.util.HashMap$Entry;
   9:       4803617     2420449192  [Ljava.lang.Object;
  10:      50392816     2015712640  org.apache.hadoop.mapred.Counters$Counter
  11:       7775438     1181866576  [Ljava.util.concurrent.ConcurrentHashMap$Segment;
  12:       3882847     1118259936  org.apache.hadoop.mapred.TaskInProgress


ConcurrentHashMap takes more than 14G(5959535616 + 5959534704 + 3975044400).
The trouble maker are below codes in TaskInProgress.java:
  Map<TaskAttemptID, Locality> taskLocality = 
      new ConcurrentHashMap<TaskAttemptID, Locality>();
  Map<TaskAttemptID, Avataar> taskAvataar = 
      new ConcurrentHashMap<TaskAttemptID, Avataar>();
"
MAPREDUCE-5364,Deadlock between RenewalTimerTask methods cancel() and run(),"MAPREDUCE-4860 introduced a local variable {{cancelled}} in {{RenewalTimerTask}} to fix the race where {{DelegationTokenRenewal}} attempts to renew a token even after the job is removed. However, the patch also makes {{run()}} and {{cancel()}} synchronized methods leading to a potential deadlock against {{run()}}'s catch-block (error-path).

The deadlock stacks below:

{noformat}
 - org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal$RenewalTimerTask.cancel() @bci=0, line=240 (Interpreted frame)
 - org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal.removeDelegationTokenRenewalForJob(org.apache.hadoop.mapreduce.JobID) @bci=109, line=319 (Interpreted frame)
{noformat}

{noformat}
 - org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal.removeFailedDelegationToken(org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal$DelegationTokenToRenew) @bci=62, line=297 (Interpreted frame)
 - org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal.access$300(org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal$DelegationTokenToRenew) @bci=1, line=47 (Interpreted frame)
 - org.apache.hadoop.mapreduce.security.token.DelegationTokenRenewal$RenewalTimerTask.run() @bci=148, line=234 (Interpreted frame)
{noformat}"
MAPREDUCE-5357,Job staging directory owner checking could fail on Windows,"In {{JobSubmissionFiles.getStagingDir()}}, we have following code that will throw exception if the directory owner is not the current user.

{code:java}
      String owner = fsStatus.getOwner();
      if (!(owner.equals(currentUser) || owner.equals(realUser))) {
         throw new IOException(""The ownership on the staging directory "" +
                      stagingArea + "" is not as expected. "" +
                      ""It is owned by "" + owner + "". The directory must "" +
                      ""be owned by the submitter "" + currentUser + "" or "" +
                      ""by "" + realUser);
      }
{code}

This check will fail on Windows when the underlying file system is LocalFileSystem. Because on Windows, the default file or directory owner could be ""Administrators"" group if the user belongs to ""Administrators"" group.

Quite a few MR unit tests that runs MR mini cluster with localFs as underlying file system fail because of this."
MAPREDUCE-5351,JobTracker memory leak caused by CleanupQueue reopening FileSystem,"When a job is completed, closeAllForUGI is called to close all the cached FileSystems in the FileSystem cache.  However, the CleanupQueue may run after this occurs and call FileSystem.get() to delete the staging directory, adding a FileSystem to the cache that will never be closed.

People on the user-list have reported this causing their JobTrackers to OOME every two weeks."
MAPREDUCE-5308,Shuffling to memory can get out-of-sync when fetching multiple compressed map outputs,"When a reducer is fetching multiple compressed map outputs from a host, the fetcher can get out-of-sync with the IFileInputStream, causing several of the maps to fail to fetch.

This occurs because decompressors can return all the decompressed bytes before actually processing all the bytes in the compressed stream (due to checksums or other trailing data that we ignore). In the unfortunate case where these extra bytes cross an io.file.buffer.size boundary, some extra bytes will be left over and the next map_output will not fetch correctly (usually due to an invalid map_id).

This scenario is not typically fatal to a job because the failure is charged to the map_output immediately following the ""bad"" one and the subsequent retry will normally work. "
MAPREDUCE-5279,Jobs can deadlock if headroom is limited by cpu instead of memory,"YARN-2 imported cpu dimension scheduling, but MR RMContainerAllocator doesn't take into account virtual cores while scheduling reduce tasks.
This may cause more reduce tasks to be scheduled because memory is enough. And on a small cluster, this will end with deadlock, all running containers are reduce tasks but map phase is not finished. "
MAPREDUCE-5278,Distributed cache is broken when JT staging dir is not on the default FS,"Today, the JobTracker staging dir (""mapreduce.jobtracker.staging.root.dir) is set to point to HDFS, even though other file systems (e.g. Amazon S3 file system and Windows ASV file system) are the default file systems.

For ASV, this config was chosen and there are a few reasons why:

1. To prevent leak of the storage account credentials to the user's storage account; 
2. It uses HDFS for the transient job files what is good for two reasons – a) it does not flood the user's storage account with irrelevant data/files b) it leverages HDFS locality for small files

However, this approach conflicts with how distributed cache caching works, completely negating the feature's functionality.

When files are added to the distributed cache (thru files/achieves/libjars hadoop generic options), they are copied to the job tracker staging dir only if they reside on a file system different that the jobtracker's. Later on, this path is used as a ""key"" to cache the files locally on the tasktracker's machine, and avoid localization (download/unzip) of the distributed cache files if they are already localized.

In this configuration the caching is completely disabled and we always end up copying dist cache files to the job tracker's staging dir first and localizing them on the task tracker machine second.

This is especially not good for Oozie scenarios as Oozie uses dist cache to populate Hive/Pig jars throughout the cluster.

"
MAPREDUCE-5236,references to JobConf.DISABLE_MEMORY_LIMIT don't make sense in the context of MR2,"In MR1, a special value of -1 could be given for mapreduce.job.map|reduce.memory.mb when memory limits were disabled.  In MR2, this makes no sense, as with slots gone, this value is used for requesting resources and scheduling."
MAPREDUCE-5207,Add mapreduce.{map|reduce}.memory.mb defaults to mapred-default.xml,mapred-default.xml is missing defaults for mapredue.{map|reduce}.memory.mb
MAPREDUCE-5198,Race condition in cleanup during task tracker renint with LinuxTaskController,"This was noticed when job tracker would be restarted while jobs were running and would ask the task tracker to reinitialize. 

Tasktracker would fail with an error like

{code}
013-04-27 20:19:09,627 INFO org.apache.hadoop.mapred.TaskTracker: Good mapred local directories are: /grid/0/hdp/mapred/local,/grid/1/hdp/mapred/local,/grid/2/hdp/mapred/local,/grid/3/hdp/mapred/local,/grid/4/hdp/mapred/local,/grid/5/hdp/mapred/local
2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075 caught: java.nio.channels.ClosedChannelException
	at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)
	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)
	at org.apache.hadoop.ipc.Server.access$2000(Server.java:98)
	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)
	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)

2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075: exiting
2013-04-27 20:19:10,414 ERROR org.apache.hadoop.mapred.TaskTracker: Got fatal exception while reinitializing TaskTracker: org.apache.hadoop.util.Shell$ExitCodeException: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)
	at org.apache.hadoop.util.Shell.run(Shell.java:182)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)
	at org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)
	at org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)
	at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)
	at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)
	at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)
{code} "
MAPREDUCE-5155,Race condition in test case TestFetchFailure cause it to fail,"I run into this once: testFetchFailureWithRecovery(org.apache.hadoop.mapreduce.v2.app.TestFetchFailure): Num completion events not correct expected:<1> but was:<0>

There is a race condition between job.getTaskAttemptCompletionEvents and dealing with JOB_TASK_ATTEMPT_COMPLETED event.
If job.getTaskAttemptCompletionEvents invoked because of task in SUCCEEDED state ,but before JOB_TASK_ATTEMPT_COMPLETED event scheduled,the test case will fail.

"
MAPREDUCE-5154,staging directory deletion fails because delegation tokens have been cancelled,"In a secure setup, the jobtracker needs the job's delegation tokens to delete the staging directory.  MAPREDUCE-4850 made it so that job cleanup staging directory deletion occurs asynchronously, so that it could order it with system directory deletion.  This introduced the issue that a job's delegation tokens could be cancelled before the cleanup thread got around to deleting it, causing the deletion to fail."
MAPREDUCE-5133,TestSubmitJob.testSecureJobExecution is flaky due to job dir deletion race,"At the end of TestSubmitJob.testSecureJobExecution, the test waits for the job to be done and then asserts that the job submission directory has been deleted.  The directory is deleted by an asynchronous cleanup thread, so the test can hit the assert before the deletion is run.
"
MAPREDUCE-5086,MR app master deletes staging dir when sent a reboot command from the RM,"If the RM is restarted when the MR job is running, then it sends a reboot command to the job. The job ends up deleting the staging dir and that causes the next attempt to fail."
MAPREDUCE-5075,DistCp leaks input file handles,"DistCp wraps the {{InputStream}} for each input file it reads in an instance of {{ThrottledInputStream}}.  This class does not close the wrapped {{InputStream}}.  {{RetriableFileCopyCommand}} guarantees that the {{ThrottledInputStream}} gets closed, but without closing the underlying wrapped stream, it still leaks a file handle."
MAPREDUCE-5064,TestRumenJobTraces failing on 1.3.x and 1.2,"{{TestRumenJobTraces.testCurrentJHParser()}} is failing locally, both in a bulk test and standalone"
MAPREDUCE-5035,Update MR1 memory configuration docs,The pmem/vmem settings in the docs (http://hadoop.apache.org/docs/r1.1.1/cluster_setup.html#Memory+monitoring) have not been supported for a long time. The docs should be updated to reflect the new settings (mapred.cluster.map.memory.mb etc).
MAPREDUCE-5001,LocalJobRunner has race condition resulting in job failures ,"Hive is hitting a race condition with LocalJobRunner and the Cluster class. The JobClient uses the Cluster class to obtain Job objects. The Cluster class uses the job.xml file to populate the JobConf object (https://github.com/apache/hadoop-common/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java#L184). However, this file is deleted by the LocalJobRunner at the end of it's job (https://github.com/apache/hadoop-common/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapred/LocalJobRunner.java#L484).

This results in the following exception:
{noformat}
2013-02-11 14:45:17,755 (main) [FATAL - org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2001)] error parsing conf file:/tmp/hadoop-brock/mapred/staging/brock1916441210/.staging/job_local_0432/job.xml
java.io.FileNotFoundException: /tmp/hadoop-brock/mapred/staging/brock1916441210/.staging/job_local_0432/job.xml (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:120)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1917)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1870)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1777)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:712)
	at org.apache.hadoop.mapred.JobConf.checkAndWarnDeprecation(JobConf.java:1951)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:398)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:388)
	at org.apache.hadoop.mapred.JobClient$NetworkedJob.<init>(JobClient.java:174)
	at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:655)
	at org.apache.hadoop.mapred.JobClient.getJob(JobClient.java:668)
	at org.apache.hadoop.mapreduce.TestMR2LocalMode.test(TestMR2LocalMode.java:40)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
{noformat}

Here is code which exposes this race fairly quickly:

{noformat}
    Configuration conf = new Configuration();
    conf.set(""mapreduce.framework.name"", ""local"");
    conf.set(""mapreduce.jobtracker.address"", ""local"");
    File inputDir = new File(""/tmp"", ""input-"" + System.currentTimeMillis());
    File outputDir = new File(""/tmp"", ""output-"" + System.currentTimeMillis());
    while(true) {
      Assert.assertTrue(inputDir.mkdirs());
      File inputFile = new File(inputDir, ""file"");
      FileUtils.copyFile(new File(""/etc/passwd""), inputFile);
      Path input = new Path(inputDir.getAbsolutePath());
      Path output = new Path(outputDir.getAbsolutePath());
      JobConf jobConf = new JobConf(conf, TestMR2LocalMode.class);
      FileInputFormat.addInputPath(jobConf, input);
      FileOutputFormat.setOutputPath(jobConf, output);      
      JobClient jobClient = new JobClient(conf);
      RunningJob runningJob = jobClient.submitJob(jobConf);
      while(!runningJob.isComplete()) {
        runningJob = jobClient.getJob(runningJob.getJobID());
      }      
      FileUtils.deleteQuietly(inputDir);
      FileUtils.deleteQuietly(outputDir);
    }
{noformat}"
MAPREDUCE-4938,Job submission to unknown queue can leave staging directory behind,"There is a race where submitting a job to an unknown queue can appear to succeed to the client and then subsequently fail later.  Since there was no AM ever launched, there was nothing left to cleanup the staging directory.  At that point the client is the only thing that can cleanup the staging directory."
MAPREDUCE-4933,MR1 final merge asks for length of file it just wrote before flushing it,"createKVIterator in ReduceTask contains the following code:
{code}

          try {
            Merger.writeFile(rIter, writer, reporter, job);
            addToMapOutputFilesOnDisk(fs.getFileStatus(outputPath));
          } catch (Exception e) {
            if (null != outputPath) {
              fs.delete(outputPath, true);
            }
            throw new IOException(""Final merge failed"", e);
          } finally {
            if (null != writer) {
              writer.close();
            }
          }
{code}

Merger#writeFile() does not close the file after writing it, so when fs.getFileStatus() is called on it, it may not return the correct length.  This causes bad accounting further down the line, which can lead to map output data being lost."
MAPREDUCE-4927,Historyserver 500 error due to NPE when accessing specific counters page for failed job,"Went to the historyserver page for a job that failed and examined the counters page.  When I clicked on a specific counter, the historyserver returned a 500 error.  The historyserver logs showed it encountered an NPE error, full traceback to follow."
MAPREDUCE-4922,Request with multiple data local nodes can cause NPE in AppSchedulingInfo,"With the way that the schedulers work, each request for a container on a node must consist of 3 ResourceRequests - one on the node, one on the rack, and one with *.

AppSchedulingInfo tracks the outstanding requests.  When a node is assigned a node-local container, allocateNodeLocal decrements the outstanding requests at each level - node, rack, and *.  If the rack requests reach 0, it removes the mapping.

A mapreduce task with multiple data local nodes submits multiple container requests, one for each node.  It also submits one for each unique rack, and one for *.  If there are fewer unique racks than data local nodes, this means that fewer rack-local ResourceRequests will be submitted than node-local ResourceRequests, so the rack-local mapping will be deleted before all the node-local requests are allocated and an NPE will come up the next time a node-local request from that rack is allocated."
MAPREDUCE-4913,TestMRAppMaster#testMRAppMasterMissingStaging occasionally exits,"testMRAppMasterMissingStaging will sometimes cause the JVM to exit due to this error from AsyncDispatcher:

{noformat}
2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread
java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)
        at java.lang.Thread.run(Thread.java:662)
2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye..
{noformat}

This can cause a build to fail since the test process exits without unregistering from surefire which treats it as a build error rather than a test failure."
MAPREDUCE-4854,TestRumenJobTraces is broken in branch-1,"TestRumenJobTraces is broken in branch-1, need to fix the 'gold' events it's checking against which is broken."
MAPREDUCE-4850,Job recovery may fail if staging directory has been deleted,"The job staging directory is deleted in the job cleanup task, which happens before the job-info file is deleted from the system directory (by the JobInProgress garbageCollect() method). If the JT shuts down between these two operations, then when the JT restarts and tries to recover the job, it fails since the job.xml and splits are no longer available."
MAPREDUCE-4845,ClusterStatus.getMaxMemory() and getUsedMemory() exist in MR1 but not MR2 ,"For backwards compatibility, these methods should exist in both MR1 and MR2.

Confusingly, these methods return the max memory and used memory of the jobtracker, not the entire cluster.

I'd propose to add them to MR2 and return -1, and deprecate them in both MR1 and MR2.  Alternatively, I could add plumbing to get the resource manager memory stats."
MAPREDUCE-4842,Shuffle race can hang reducer,"Saw an instance where the shuffle caused multiple reducers in a job to hang.  It looked similar to the problem described in MAPREDUCE-3721, where the fetchers were all being told to WAIT by the MergeManager but no merge was taking place."
MAPREDUCE-4752,Reduce MR AM memory usage through String Interning,"There are a lot of strings that are duplicates of one another in the AM.  This comes from all of the PB events the come across the wire and also tasks heart-beating in through the umbilical.  There are even several duplicates from Configuration.  By ""interning"" all of these strings on the Heap I have been able to reduce the resting memory usage of the AM to be about 5KB per task attempt.  With about half of this coming from counters.  This results in a 5MB heap for a typical 1000 task job, or a 500MB heap for a 100,000 task attempt job.  I think I could cut the size of the counters in half by completely rewriting how counters work in the AM and History Server, but I don't think it is worth it at this point.

I am still investigating what the memory usage of the AM is like when running very large jobs, and I will probably have a follow-up JIRA for reducing that memory usage as well.
"
MAPREDUCE-4710,Add peak memory usage counter for each task,"Each task has counters PHYSICAL_MEMORY_BYTES and VIRTUAL_MEMORY_BYTES, which are snapshots of memory usage of that task. They are not sufficient for users to understand peak memory usage by that task, e.g. in order to diagnose task failures, tune job parameters or change application design. This new feature will add two more counters for each task: PHYSICAL_MEMORY_BYTES_MAX and VIRTUAL_MEMORY_BYTES_MAX. "
MAPREDUCE-4689,JobClient.getMapTaskReports on failed job results in NPE,"When calling JobClient.getMapTaskReports for a job that has failed results in an NPE.  For example:

{noformat}
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.counters.AbstractCounters.<init>(AbstractCounters.java:107)
	at org.apache.hadoop.mapred.Counters.<init>(Counters.java:71)
	at org.apache.hadoop.mapred.Counters.downgrade(Counters.java:80)
	at org.apache.hadoop.mapred.TaskReport.downgrade(TaskReport.java:81)
	at org.apache.hadoop.mapred.TaskReport.downgradeArray(TaskReport.java:88)
	at org.apache.hadoop.mapred.JobClient.getTaskReports(JobClient.java:691)
	at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:681)
...
{noformat}
"
MAPREDUCE-4682,TestKillSubProcess & TestTaskTrackerMemoryManager fail to compile on trunk due to MAPREDUCE-4253,"Fail with:

 /Users/acmurthy/dev/apache/hadoop/hadoop-trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestKillSubProcesses.java:411: cannot find symbol
    [javac] symbol  : variable TestProcfsBasedProcessTree
    [javac] location: class org.apache.hadoop.mapred.TestKillSubProcesses
    [javac]         childPid = TestProcfsBasedProcessTree.getPidFromPidFile(scriptDirName
    [javac]                    ^
    [javac] /Users/acmurthy/dev/apache/hadoop/hadoop-trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestTaskTrackerMemoryManager.java:449: cannot find symbol
    [javac] symbol  : variable TestProcfsBasedProcessTree
    [javac] location: class org.apache.hadoop.mapred.TestTaskTrackerMemoryManager
    [javac]       TestProcfsBasedProcessTree.setupProcfsRootDir(procfsRootDir);
"
MAPREDUCE-4657,WindowsResourceCalculatorPlugin has NPE,"When Shell command execution is interrupted then WindowsResourceCalculatorPlugin has NPE.
code}
2012-08-31 13:01:00,140 ERROR [Thread-771] util.WindowsResourceCalculatorPlugin(69): java.io.IOException: java.lang.InterruptedException^M
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:424)^M
        at org.apache.hadoop.util.Shell.run(Shell.java:336)^M
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:540)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getSystemInfoInfoFromShell(WindowsResourceCalculatorPlugin.java:66)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.refreshIfNeeded(WindowsResourceCalculatorPlugin.java:81)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getAvailableVirtualMemorySize(WindowsResourceCalculatorPlugin.java:126)^M
        at org.apache.hadoop.mapred.TaskTracker.getAvailableVirtualMemoryOnTT(TaskTracker.java:1933)^M
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1834)^M
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1664)^M
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2516)^M
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)^M
        at java.lang.Thread.run(Thread.java:662)^M
^M
2012-08-31 13:01:00,140 ERROR [Thread-771] mapred.TaskTracker(1766): Caught exception: java.lang.NullPointerException^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.refreshIfNeeded(WindowsResourceCalculatorPlugin.java:83)^M
        at org.apache.hadoop.util.WindowsResourceCalculatorPlugin.getAvailableVirtualMemorySize(WindowsResourceCalculatorPlugin.java:126)^M
        at org.apache.hadoop.mapred.TaskTracker.getAvailableVirtualMemoryOnTT(TaskTracker.java:1933)^M
        at org.apache.hadoop.mapred.TaskTracker.transmitHeartBeat(TaskTracker.java:1834)^M
        at org.apache.hadoop.mapred.TaskTracker.offerService(TaskTracker.java:1664)^M
        at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2516)^M
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.run(MiniMRCluster.java:217)^M
        at java.lang.Thread.run(Thread.java:662)^M
{code}"
MAPREDUCE-4655,MergeManager.reserve can OutOfMemoryError if more than 10% of max memory is used on non-MapOutputs,"The MergeManager does a memory check, using a limit that defaults to 90% of Runtime.getRuntime().maxMemory(). Allocations that would bring the total memory allocated by the MergeManager over this limit are asked to wait until memory frees up. Disk is used for single allocations that would be over 25% of the memory limit.

If some other part of the reducer were to be using more than 10% of the memory. the current check wouldn't stop an OutOfMemoryError.

Before creating an in-memory MapOutput, a check can be done using Runtime.getRuntime().freeMemory(), waiting until memory is freed up if it fails.

12/08/17 10:36:29 INFO mapreduce.Job: Task Id : attempt_1342723342632_0010_r_000005_0, Status : FAILED 
Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#6 
at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:123) 
at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:371) 
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:152) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:416) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232) 
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:147) 
Caused by: java.lang.OutOfMemoryError: Java heap space 
at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:58) 
at org.apache.hadoop.io.BoundedByteArrayOutputStream.<init>(BoundedByteArrayOutputStream.java:45) 
at org.apache.hadoop.mapreduce.task.reduce.MapOutput.<init>(MapOutput.java:97) 
at org.apache.hadoop.mapreduce.task.reduce.MergeManager.unconditionalReserve(MergeManager.java:286) 
at org.apache.hadoop.mapreduce.task.reduce.MergeManager.reserve(MergeManager.java:276) 
at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:327) 
at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:273) 
at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:153)
"
MAPREDUCE-4607,Race condition in ReduceTask completion can result in Task being incorrectly failed,"Problem reported by chackaravarthy in MAPREDUCE-4252

This problem has been handled when speculative task launched for map task and other attempt got failed (not killed)
Can the similar kind of scenario can happen in case of reduce task?
Consider the following scenario for reduce task in case of speculation (one attempt got killed):
1. A task attempt is started.
2. A speculative task attempt for the same task is started.
3. The first task attempt completes and causes the task to transition to SUCCEEDED.
4. Then speculative task attempt will be killed because of the completion of first attempt.
As a result, internal error will be thrown from this attempt (TaskImpl.MapRetroactiveKilledTransition) and hence task attempt failure leads to job failure.
TaskImpl.MapRetroactiveKilledTransition
if (!TaskType.MAP.equals(task.getType())) {
        LOG.error(""Unexpected event for REDUCE task "" + event.getType());
        task.internalError(event.getType());
      }
So, do we need to have following code in MapRetroactiveKilledTransition also just like in MapRetroactiveFailureTransition.
if (event instanceof TaskTAttemptEvent) {
        TaskTAttemptEvent castEvent = (TaskTAttemptEvent) event;
        if (task.getState() == TaskState.SUCCEEDED &&
            !castEvent.getTaskAttemptID().equals(task.successfulAttempt)) {
          // don't allow a different task attempt to override a previous
          // succeeded state
          return TaskState.SUCCEEDED;
        }
      }
please check whether this is a valid case and give your suggestion."
MAPREDUCE-4595,TestLostTracker failing - possibly due to a race in JobHistory.JobHistoryFilesManager#run(),"The source for occasional failure of TestLostTracker seems like the following:

On job completion, JobHistoryFilesManager#run() spawns another thread to move history files to done folder. TestLostTracker waits for job completion, before checking the file format of the history file. However, the history files move might be in the process or might not have started in the first place.

The attachment (force-TestLostTracker-failure.patch) helps reproducing the error locally, by increasing the chance of hitting this race."
MAPREDUCE-4560,Job can get stuck in a deadlock between mappers and reducers for low values of mapreduce.job.reduce.slowstart.completedmaps (<<1),"This issue has been seen with MapReduceV2, never with MapReduceV1 in our lab systems.

The parameter mapreduce.job.reduce.slowstart.completedmaps=0.05 (the default value).

We found Application master stuck in a deadlock between mappers and reducers with no progress in the job; the sequence appears to be:

1. Initial available map/reduce slots were allocated to mappers
2. Once mappers made progress and few of them completed, reducers started occupying few of the slots due to low values of above config param.
3. The scheduler appears to not give priority to mappers over reducers; after a while in our system we saw all slots occupied by reducers.
4. Since there were still mapper tasks not yet assigned any slot, the map phase never completed.
5. The system entered a deadlock state where reducers occupy all available slots, but are waiting for mappers to be complete; mappers cannot move forward because of no slot available.

The workaround in our system was to set 
mapreduce.job.reduce.slowstart.completedmaps=1 and the issue was no longer seen."
MAPREDUCE-4555,make user's mapred .staging area permissions configurable,"The directories are created in JobTracker and LocalRunner, but they are currently forced to be 0700. There is even a segment of the source code that will check the permissions are 0700, and if not it will change the permissions to match 0700. For monitoring purposes the permissions should be configurable.

Please note:
1. We can make the hard-coded 700 configurable at clients (its the client who creates it) but there's two issues here: 
1.1. It violates security principals (as its client sided and overridable) 
1.2. It can't be consistent, since some user may ignore configs provided to them and create it with 0700.

"
MAPREDUCE-4535,"Test failures with ""Container .. is running beyond virtual memory limits""","Tests org.apache.hadoop.tools.TestHadoopArchives.{testRelativePath,testPathWithSpaces} fail with the following message:

{code}
Container [pid=7785,containerID=container_1342495768864_0001_01_000001] is running beyond virtual memory limits. Current usage: 143.6mb of 1.5gb physical memory used; 3.4gb of 3.1gb virtual memory used. Killing container.
Dump of the process-tree for container_1342495768864_0001_01_000001 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 7797 7785 7785 7785 (java) 573 38 3517018112 36421 /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 
{code}

This is not a stably reproducible problem, but adding MALLOC_ARENA_MAX resolves the problem."
MAPREDUCE-4534,"Test failures with ""Container .. is running beyond virtual memory limits""","Tests org.apache.hadoop.tools.TestHadoopArchives.{testRelativePath,testPathWithSpaces} fail with the following message:

{code}
Container [pid=7785,containerID=container_1342495768864_0001_01_000001] is running beyond virtual memory limits. Current usage: 143.6mb of 1.5gb physical memory used; 3.4gb of 3.1gb virtual memory used. Killing container.
Dump of the process-tree for container_1342495768864_0001_01_000001 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 7797 7785 7785 7785 (java) 573 38 3517018112 36421 /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 
	|- 7785 7101 7785 7785 (bash) 1 1 108605440 332 /bin/bash -c /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1>/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001/stdout 2>/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001/stderr
{code}

This is not a stably reproducible problem, but adding MALLOC_ARENA_MAX resolves the problem."
MAPREDUCE-4533,"Test failures with ""Container .. is running beyond virtual memory limits""","Tests org.apache.hadoop.tools.TestHadoopArchives.{testRelativePath,testPathWithSpaces} fail with the following message:

{code}
Container [pid=7785,containerID=container_1342495768864_0001_01_000001] is running beyond virtual memory limits. Current usage: 143.6mb of 1.5gb physical memory used; 3.4gb of 3.1gb virtual memory used. Killing container.
Dump of the process-tree for container_1342495768864_0001_01_000001 :
	|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
	|- 7797 7785 7785 7785 (java) 573 38 3517018112 36421 /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 
	|- 7785 7101 7785 7785 (bash) 1 1 108605440 332 /bin/bash -c /usr/java/jdk1.6.0_33/jre/bin/java -Dlog4j.configuration=container-log4j.properties -Dyarn.app.mapreduce.container.log.dir=/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001 -Dyarn.app.mapreduce.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA -Xmx1024m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1>/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001/stdout 2>/var/lib/jenkins/workspace/Hadoop_gd-branch0.23_integration/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-logDir-nm-0_3/application_1342495768864_0001/container_1342495768864_0001_01_000001/stderr
{code}

This is not a stably reproducible problem, but adding MALLOC_ARENA_MAX resolves the problem."
MAPREDUCE-4520,Add experimental support for MR AM to schedule CPUs along-with memory,
MAPREDUCE-4437,Race in MR ApplicationMaster can cause reducers to never be scheduled,If the MR AM is notified of container completion by the RM before the AM receives notification of the container cleanup from the NM then it can fail to schedule reducers indefinitely.  Logs showing the issue to follow.
MAPREDUCE-4420,./mapred queue -info <queuename> -showJobs displays containers and memory as zero always,./mapred queue -info <queuename> -showJobs displays containers and memory as zero always.
MAPREDUCE-4395,Possible NPE at ClientDistributedCacheManager#determineTimestamps,"{code:title=ClientDistributedCacheManager#determineTimestamps|borderStyle=solid}
URI[] tfiles = DistributedCache.getCacheFiles(job);
{code}

It may be possible that tfiles array contains *null* as it's entry, and subsequently leads to NPE."
MAPREDUCE-4384,Race conditions in IndexCache,"TestIndexCache is intermittently failing due to a race condition. Up on inspection of IndexCache implementation, more potential issues have been discovered."
MAPREDUCE-4379,Node Manager throws java.lang.OutOfMemoryError: Java heap space due to org.apache.hadoop.fs.LocalDirAllocator.contexts,"{code:xml}
Exception in thread ""Container Monitor"" java.lang.OutOfMemoryError: Java heap space
	at java.io.BufferedReader.<init>(BufferedReader.java:80)
	at java.io.BufferedReader.<init>(BufferedReader.java:91)
	at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:410)
	at org.apache.hadoop.yarn.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:171)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:389)
	Exception in thread ""LocalizerRunner for container_1340690914008_10890_01_000003"" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOfRange(Arrays.java:3209)
	at java.lang.String.<init>(String.java:215)
	at com.sun.org.apache.xerces.internal.xni.XMLString.toString(XMLString.java:185)
	at com.sun.org.apache.xerces.internal.parsers.AbstractDOMParser.characters(AbstractDOMParser.java:1188)
	at com.sun.org.apache.xerces.internal.xinclude.XIncludeHandler.characters(XIncludeHandler.java:1084)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:464)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:808)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:119)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:235)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:284)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:180)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1738)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1689)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1635)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:722)
	at org.apache.hadoop.conf.Configuration.setStrings(Configuration.java:1300)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.initDirs(ContainerLocalizer.java:375)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.runLocalization(ContainerLocalizer.java:127)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.startLocalizer(DefaultContainerExecutor.java:103)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:862)
{code}"
MAPREDUCE-4372,Deadlock in Resource Manager between SchedulerEventDispatcher.EventProcessor and Shutdown hook manager,Please find the attached resource manager thread dump for the issue.
MAPREDUCE-4359,Potential deadlock in Counters,"jcarder identified this deadlock in branch-1 (though it may also be present in trunk):
- Counters.size() is synchronized and locks Counters before Group
- Counters.Group.getCounterForName() is synchronized and calls through to Counters.size()

This creates a potential cycle which could cause a deadlock (though probably quite rare in practice)"
MAPREDUCE-4340,Node Manager leaks socket connections connected to Data Node,"I am running simple wordcount example with default configurations, for every job run it increases one datanode socket connection and it will be there in CLOSE_WAIT state forever."
MAPREDUCE-4301,Dedupe some strings in MRAM for memory savings,"Recently an OutOfMemoryError caused one of our jobs to become a zombie (MAPREDUCE-4300).  It was a rather large job with 78000+ map tasks and only 750MB of heap configured.  I took a heap dump to see if there were any obvious memory leaks, and I could not find any, but yourkit and some digging found some potential memory optimizations that we could do.

In this particular case we could save about 20MB if SplitMetaInfoReader.readSplitMetaInfo only computed the JobSplitFile once instead of for each split. (a 2 line change)

I will look into some others and see if there are more savings I can come up with."
MAPREDUCE-4279,getClusterStatus() fails with null pointer exception when running jobs in local mode,"While migrating code from 0.20.2 hadoop codebase to 0.23.1 we encountered this issue for jobs run in local mode of execution:
{code}

java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobClient.arrayToStringList(JobClient.java:783)
	at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:138)
	at org.apache.hadoop.mapred.JobClient$4.run(JobClient.java:815)
	at org.apache.hadoop.mapred.JobClient$4.run(JobClient.java:812)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
	at org.apache.hadoop.mapred.JobClient.getClusterStatus(JobClient.java:812)
{code}

We are using cloudera distribution CDH4b2 for testing, however the underlying code is 0.23.1 and I could see no difference in this implementation."
MAPREDUCE-4255,Job History Server throws NPE if it fails to get keytab,"{code:xml}
2012-05-14 17:59:41,906 FATAL org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: Error starting JobHistoryServer
org.apache.hadoop.yarn.YarnException: History Server Failed to login
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.init(JobHistoryServer.java:69)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:132)
Caused by: java.io.IOException: Running in secure mode, but config doesn't have a keytab
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:258)
	at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:229)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.doSecureLogin(JobHistoryServer.java:98)
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.init(JobHistoryServer.java:67)
	... 1 more
2012-05-14 17:59:41,918 INFO org.apache.hadoop.yarn.service.CompositeService: Error stopping org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer
java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.stop(JobHistoryServer.java:115)
	at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
2012-05-14 17:59:41,918 INFO org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer: SHUTDOWN_MSG: 
{code}"
MAPREDUCE-4233,NPE can happen in RMNMNodeInfo.,"{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo.getLiveNodeManagers(RMNMInfo.java:96)
        at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:65)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:216)
        at javax.management.StandardMBean.getAttribute(StandardMBean.java:358)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:666)
{noformat}

Looks like rmcontext.getRMNodes() is not kept in sync with scheduler.getNodeReport(), so that the report can be null even though the context still knowns about the node.

The simple fix is to add in a null check."
MAPREDUCE-4229,Counter names' memory usage can be decreased by interning,"In our experience, most of the memory in production JTs goes to storing counter names (String objects and character arrays). Since most counter names are reused again and again, it would be a big memory savings to keep a hash set of already-used counter names within a job, and refer to the same object from all tasks."
MAPREDUCE-4200,packaging tar ball of trunk failed,"A command ""mvn clean package -Dtar -DskipTests"" executed on the root directory ""hadoop-common"" failed.
Its output logs are in an attached file.

A command ""mvn clean package -Pdist -DskipTests"" succeeded.
"
MAPREDUCE-4195,"With invalid queueName request param, jobqueue_details.jsp shows NPE","When you access /jobqueue_details.jsp manually, instead of via a link, it has queueName set to null internally and this goes for a lookup into the scheduling info maps as well.

As a result, if using FairScheduler, a Pool with String name = null gets created and this brings the scheduler down. I have not tested what happens to the CapacityScheduler, but ideally if no queueName is set in that jsp, it should fall back to 'default'. Otherwise, this brings down the JobTracker completely.

FairScheduler must also add a check to not create a pool with 'null' name.

The following is the strace that ensues:

{code}
ERROR org.mortbay.log: /jobqueue_details.jsp 
java.lang.NullPointerException 
at org.apache.hadoop.mapred.jobqueue_005fdetails_jsp._jspService(jobqueue_005fdetails_jsp.java:71) 
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97) 
at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) 
at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) 
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) 
at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:829) 
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) 
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399) 
at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216) 
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182) 
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766) 
at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450) 
at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230) 
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) 
at org.mortbay.jetty.Server.handle(Server.java:326) 
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) 
at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928) 
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549) 
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212) 
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) 
at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410) 
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) 
INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9001, call heartbeat from XYZ:MNOP: error: java.io.IOException: java.lang.NullPointerException 
java.io.IOException: java.lang.NullPointerException 
at org.apache.hadoop.mapred.SchedulingAlgorithms$FairShareComparator.compare(SchedulingAlgorithms.java:95) 
at org.apache.hadoop.mapred.SchedulingAlgorithms$FairShareComparator.compare(SchedulingAlgorithms.java:68) 
at java.util.Arrays.mergeSort(Unknown Source) 
at java.util.Arrays.sort(Unknown Source) 
at java.util.Collections.sort(Unknown Source) 
at org.apache.hadoop.mapred.FairScheduler.assignTasks(FairScheduler.java:435) 
at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:3226) 
at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) 
at java.lang.reflect.Method.invoke(Unknown Source) 
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1434) 
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1430) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Unknown Source) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127) 
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1428)
{code}"
MAPREDUCE-4144,ResourceManager NPE while handling NODE_UPDATE,"The RM on one of our clusters has exited twice in the past few days because of an NPE while trying to handle a NODE_UPDATE:

{noformat}
2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
 [ResourceManager Event Processor]java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)
        at java.lang.Thread.run(Thread.java:619)
{noformat}

This is very similar to the failure reported in MAPREDUCE-3005."
MAPREDUCE-4139,Potential ResourceManager deadlock when SchedulerEventDispatcher is stopped,"When the main thread calls ResourceManager$SchedulerEventDispatcher.stop() it grabs a lock on the object, kicks the event processor thread, and then waits for the thread to exit.  However the interrupted event processor thread can end up trying to call the synchronized getConfig() method which results in deadlock."
MAPREDUCE-4138,Reduce memory usage of counters due to non-static nested classes,"FrameworkCounter is a non-static nested class of FrameworkCounterGroup which means it retains a reference to the outer class, which isn't really needed."
MAPREDUCE-4121,dump the threads stack trace to stdout before killing a Task in timeout,"Typically when a job fails because of tasks timing out we investigate the issue by running the job again and triggering a dump of the thread stack traces of one of the tasks with jstack/""kill -3"" before it times out.
It would be convenient if the Task tracker could do the same right before killing tasks in time out. This usually points at the offending code.
"
MAPREDUCE-4099,ApplicationMaster may fail to remove staging directory,"When the ApplicationMaster shuts down it's supposed to remove the staging directory, assuming properties weren't set to override this behavior. During shutdown the AM tells the ResourceManager that it has finished before it cleans up the staging directory.  However upon hearing the AM has finished, the RM turns right around and kills the AM container.  If the AM is too slow, the AM will be killed before the staging directory is removed.

We're seeing the AM lose this race fairly consistently on our clusters, and the lack of staging directory cleanup quickly leads to filesystem quota issues for some users."
MAPREDUCE-4083,GridMix emulated job tasks.resource-usage emulator for CPU usage throws NPE when Trace contains cumulativeCpuUsage value of 0 at attempt level,GridMix emulated job tasks.resource-usage emulator for CPU usage throws NPE when Trace contains cumulativeCpuUsage value of 0 at attempt level
MAPREDUCE-4066,"To get ""yarn.app.mapreduce.am.staging-dir"" value, should set the default value","when submit the job use the windows eclipse, and the yarn.app.mapreduce.am.staging-dir value is null.
{code:title=MRApps.java|borderStyle=solid}

  public static Path getStagingAreaDir(Configuration conf, String user) {
    return new Path(
        conf.get(MRJobConfig.MR_AM_STAGING_DIR) + 
        Path.SEPARATOR + user + Path.SEPARATOR + STAGING_CONSTANT);
  }
{code}

should modify to:
{code:title=MRApps.java|borderStyle=solid}

  public static Path getStagingAreaDir(Configuration conf, String user) {
    return new Path(
        conf.get(MRJobConfig.MR_AM_STAGING_DIR,""/tmp/hadoop-yarn/staging"") + 
        Path.SEPARATOR + user + Path.SEPARATOR + STAGING_CONSTANT);
  }


{code}
"
MAPREDUCE-4038,null pointer exception and invocationtarget exception in jobtracker logs,"I have written the code for scheduling in hadoop in which i have written two function schedule() and call() the schedule function calls the resource calculator for knowing the CPU usage and reliability values(explicitly i gave) but i am getting null pointer exception in the end. even i have written the code for getting active tracker names it shows null pointer exception. help me!

ERROR IN JOB TRACKER LOG FILE

2012-03-12 14:37:01,198 INFO org.apache.hadoop.mapred.JobTracker: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting JobTracker
STARTUP_MSG:   host = slavenode2.yahoo/192.168.1.2
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 0.20.1
STARTUP_MSG:   build = http://svn.apache.org/repos/asf/hadoop/common/tags/release-0.20.1-rc1 -r 810220; compiled by 'oom' on Tue Sep  1 20:55:56 UTC 2009
************************************************************/
2012-03-12 14:37:01,451 INFO org.apache.hadoop.mapred.JobTracker: Scheduler configured with (memSizeForMapSlotOnJT, memSizeForReduceSlotOnJT, limitMaxMemForMapTasks, limitMaxMemForReduceTasks) (-1, -1, -1, -1)
2012-03-12 14:37:01,515 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.2 CPU usage: 0.0
2012-03-12 14:37:01,532 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.3 CPU usage: 0.0
2012-03-12 14:37:01,546 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.4 CPU usage: 0.0
2012-03-12 14:37:01,556 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.5 CPU usage: 0.0
2012-03-12 14:37:01,566 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.7 CPU usage: 0.0
2012-03-12 14:37:01,579 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.8 CPU usage: 0.0
2012-03-12 14:37:01,589 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.11 CPU usage: 0.0
2012-03-12 14:37:01,599 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.12 CPU usage: 0.0
2012-03-12 14:37:01,615 INFO org.apache.hadoop.mapred.resourcecalculator: 192.168.1.254 CPU usage: 0.0
2012-03-12 14:37:01,616 FATAL org.apache.hadoop.mapred.JobTracker: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
	at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:1573)
	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:180)
	at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:172)
	at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:3699)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:113)
	... 4 more
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobQueueTaskScheduler1.schedule(JobQueueTaskScheduler1.java:75)
	at org.apache.hadoop.mapred.JobQueueTaskScheduler1.call(JobQueueTaskScheduler1.java:139)
	at org.apache.hadoop.mapred.JobQueueTaskScheduler1.<init>(JobQueueTaskScheduler1.java:50)
	... 9 more

2012-03-12 14:37:01,617 INFO org.apache.hadoop.mapred.JobTracker: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down JobTracker at slavenode2.yahoo/192.168.1.2
************************************************************/


"
MAPREDUCE-3993,Graceful handling of codec errors during decompression,"When using a compression codec for intermediate compression, some cases of corrupt data can cause the codec to throw exceptions other than IOException (eg java.lang.InternalError). This will currently cause the whole reduce task to fail, instead of simply treating it like another case of a failed fetch."
MAPREDUCE-3977,LogAggregationService leaks log aggregator objects,LogAggregationService adds log aggregator objects to the {{appLogAggregators}} map but never removes them.
MAPREDUCE-3953,Gridmix throws NPE and does not simulate a job if the trace contains null taskStatus for a task,"In a trace file, if a succeeded job contains a failed task, then that task's taskStatus will be null. This is causing NPE in Gridmix and then Gridmix is ignoring/not-considering such jobs for simulation. The job could succeed even with failed tasks if the job submitter in original cluster configured that job to tolerate failures using mapreduce.map.failures.maxpercent and mapreduce.reduce.failures.maxpercent."
MAPREDUCE-3930,The AM page for a Reducer that has not been launched causes an NPE,"{noformat}
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)
        at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)
        at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)
        at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)
        at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)
        at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)
        at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)
        at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:120)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:940)
        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
        at org.mortbay.jetty.Server.handle(Server.java:322)
        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.yarn.util.ConverterUtils.toString(ConverterUtils.java:145)
        at org.apache.hadoop.mapreduce.v2.app.webapp.dao.TaskAttemptInfo.<init>(TaskAttemptInfo.java:69)
        at org.apache.hadoop.mapreduce.v2.app.webapp.dao.TaskAttemptInfo.<init>(TaskAttemptInfo.java:60)
        at org.apache.hadoop.mapreduce.v2.app.webapp.TaskPage$AttemptsBlock.render(TaskPage.java:76)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:64)
        at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:74)
        at org.apache.hadoop.yarn.webapp.View.render(View.java:233)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:47)
        at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
        at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:843)
        at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:54)
        at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:80)
        at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:210)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.task(AppController.java:250) 
        ... 37 more
{noformat}"
MAPREDUCE-3904,[NPE] Job history produced with mapreduce.cluster.acls.enabled false can not be viewed with mapreduce.cluster.acls.enabled true,"Job history page displays 'null'. It looks like job history files only populate job acls when mapreduce.cluster.acls.enabled is true. Upon reading job history files, getAcls can return null, throwing an exception on the HsJobBlock page."
MAPREDUCE-3872,event handling races in ContainerLauncherImpl and TestContainerLauncher,"TestContainerLauncher is failing intermittently for me.

{noformat}
junit.framework.AssertionFailedError: Expected: <null> but was: Expected 22 but found 21
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertNull(Assert.java:233)
	at junit.framework.Assert.assertNull(Assert.java:226)
	at org.apache.hadoop.mapreduce.v2.app.launcher.TestContainerLauncher.testPoolSize(TestContainerLauncher.java:117)
{noformat}

Patch momentarily."
MAPREDUCE-3859,CapacityScheduler incorrectly utilizes extra-resources of queue for high-memory jobs,"Imagine, we have a queue A with capacity 10 slots and 20 as extra-capacity, jobs which use 3 map slots will never consume more than 9 slots, regardless how many free slots on a cluster."
MAPREDUCE-3821,NPE while running Shuffle benchmark,"hadoop jar hadoop-mapreduce-test.jar loadgen -outKey org.apache.hadoop.io.Text -outValue org.apache.hadoop.io.Text
The tasks fail with the following exception:
{noformat}
Error: java.lang.NullPointerException
	at org.apache.hadoop.fs.Path.<init>(Path.java:69)
	at org.apache.hadoop.fs.Path.<init>(Path.java:58)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getPendingJobAttemptsPath(FileOutputCommitter.java:118)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(FileOutputCommitter.java:167)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(FileOutputCommitter.java:149)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getPendingTaskAttemptsPath(FileOutputCommitter.java:185)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209)
	at org.apache.hadoop.mapred.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:100)
	at org.apache.hadoop.mapred.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:94)
	at org.apache.hadoop.mapred.FileOutputCommitter.needsTaskCommit(FileOutputCommitter.java:176)
	at org.apache.hadoop.mapred.OutputCommitter.needsTaskCommit(OutputCommitter.java:248)
	at org.apache.hadoop.mapred.Task.isCommitRequired(Task.java:955)
	at org.apache.hadoop.mapred.Task.done(Task.java:912)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:331)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
{noformat}"
MAPREDUCE-3808,NPE in FileOutputCommitter when running a 0 reduce job,"This was while running LoadGen.

{noformat}
Error: java.lang.NullPointerException at org.apache.hadoop.fs.Path.<init>(Path.java:67) 
at org.apache.hadoop.fs.Path.<init>(Path.java:56) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getPendingJobAttemptsPath(FileOutputCommitter.java:118) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(FileOutputCommitter.java:167) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getJobAttemptPath(FileOutputCommitter.java:149) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getPendingTaskAttemptsPath(FileOutputCommitter.java:185) 
at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:209) 
at org.apache.hadoop.mapred.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:100) 
at org.apache.hadoop.mapred.FileOutputCommitter.getTaskAttemptPath(FileOutputCommitter.java:94) 
at org.apache.hadoop.mapred.FileOutputCommitter.needsTaskCommit(FileOutputCommitter.java:176) 
at org.apache.hadoop.mapred.OutputCommitter.needsTaskCommit(OutputCommitter.java:248) 
at org.apache.hadoop.mapred.Task.isCommitRequired(Task.java:955) 
at org.apache.hadoop.mapred.Task.done(Task.java:912) 
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:331) 
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147) 
at java.security.AccessController.doPrivileged(Native Method) 
at javax.security.auth.Subject.doAs(Subject.java:396) 
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157) 
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
{noformat}"
MAPREDUCE-3797,lGridMix fails to Run with NPE with latest branch-0.23 code,GridMix fails to start trowing NPE after gendata 
MAPREDUCE-3770,[Rumen] Zombie.getJobConf() results into NPE,"The error trace is as follows
{code}
java.lang.NullPointerException
        at java.util.Hashtable.put(Hashtable.java:394)
        at java.util.Properties.setProperty(Properties.java:143)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:623)
        at org.apache.hadoop.mapred.JobConf.setJobName(JobConf.java:1322)
        at org.apache.hadoop.tools.rumen.ZombieJob.getJobConf(ZombieJob.java:139)
        at org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator.updateHDFSDistCacheFilesList(DistributedCacheEmulator.java:315)
        at org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator.buildDistCacheFilesList(DistributedCacheEmulator.java:280)
        at org.apache.hadoop.mapred.gridmix.DistributedCacheEmulator.setupGenerateDistCacheData(DistributedCacheEmulator.java:253)
        at org.apache.hadoop.mapred.gridmix.Gridmix.setupDistCacheEmulation(Gridmix.java:528)
        at org.apache.hadoop.mapred.gridmix.Gridmix.setupEmulation(Gridmix.java:501)
        at org.apache.hadoop.mapred.gridmix.Gridmix.start(Gridmix.java:433)
        at org.apache.hadoop.mapred.gridmix.Gridmix.runJob(Gridmix.java:380)
        at org.apache.hadoop.mapred.gridmix.Gridmix.access$000(Gridmix.java:56)
        at org.apache.hadoop.mapred.gridmix.Gridmix$1.run(Gridmix.java:313)
        at org.apache.hadoop.mapred.gridmix.Gridmix$1.run(Gridmix.java:311)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)
        at org.apache.hadoop.mapred.gridmix.Gridmix.run(Gridmix.java:311)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.mapred.gridmix.Gridmix.main(Gridmix.java:606)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:200)
{code}

The bug seems to be in {{ZombieJob#getName()}} where a not-null check for jobName.getValue() is missing. "
MAPREDUCE-3758,NPE while submitting job through Oozie,"NPE while submitting job through oozie. 
Caused by: java.lang.NullPointerException
        at
org.apache.hadoop.mapreduce.v2.util.MRApps.setMRFrameworkClasspath(MRApps.java:212)
        at"
MAPREDUCE-3747,Memory Total is not refreshed until an app is launched,"Memory Total on the RM UI is not refreshed until an application is launched. This is a problem when the cluster is started for the first time or when there are any lost/decommissioned NMs.
When the cluster is started for the first time, Active Nodes is > 0 but the Memory Total=0. Also when there are any lost/decommissioned nodes, Memory Total has wrong value.
This is a useful tool for cluster admins and has to be updated correctly without having the need to submit an app each time."
MAPREDUCE-3721,Race in shuffle can cause it to hang,"If all current {{Fetcher}}s complete while an in-memory merge is in progress - shuffle could hang. 
Specifically - if the memory freed by an in-memory merge does not bring {{MergeManager.usedMemory}} below {{MergeManager.memoryLimit}} and all current Fetchers complete before the in-memory merge completes, another in-memory merge will not be triggered - and shuffle will hang. (All new fetchers are asked to WAIT).
"
MAPREDUCE-3632,Need better error message on the Web UI when NM can't find the container logs instead of NPEno,"If for some reason NM could not find container logs, then an NPE is seen while trying to access from web UI. Instead an error message should be displayed."
MAPREDUCE-3564,TestStagingCleanup and TestJobEndNotifier are failing on trunk.,"From recent jenkins test runs:


-1 core tests. The patch failed these unit tests:
org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup
org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier

https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1457//testReport/
"
MAPREDUCE-3556,Resource Leaks in key flows,"{code:title=MapTask.java|borderStyle=solid}

{code} 

{code:xml} 
 if (combinerRunner == null || numSpills < minSpillsForCombine) {
            Merger.writeFile(kvIter, writer, reporter, job);
          } else {
            combineCollector.setWriter(writer);
            combinerRunner.combine(kvIter, combineCollector);
          }

          //close
          writer.close();
{code} 



{code:title=InputSampler.java|borderStyle=solid}

{code} 

{code:xml} 
 for(int i = 1; i < numPartitions; ++i) {
      int k = Math.round(stepSize * i);
      while (last >= k && comparator.compare(samples[last], samples[k]) == 0) {
        ++k;
      }
      writer.append(samples[k], nullValue);
      last = k;
    }
    writer.close();{code} 


The key flows have potential resource leaks. 

{code:title=JobSplitWriter.java|borderStyle=solid}

{code} 

{code:xml} 

    SplitMetaInfo[] info = writeNewSplits(conf, splits, out);
    out.close();

    SplitMetaInfo[] info = writeOldSplits(splits, out);
    out.close();
{code} "
MAPREDUCE-3537,DefaultContainerExecutor has a race condn. with multiple concurrent containers,"DCE relies cwd before calling ContainerLocalizer.runLocalization. However, with multiple containers setting cwd on same localFS reference leads to race. "
MAPREDUCE-3530,Sometimes NODE_UPDATE to the scheduler throws an NPE causing the scheduling to stop,"Sometimes NODE_UPDATE to the scheduler throws NPE causes scheduling to stop but ResourceManager keeps on running.
I have been observing intermitently for last 3 weeks.
But with latest svn code. I tried to run sort twice and both times Job got stuck due to NPE.
{code}
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.containerLaunchedOnNode(SchedulerApp.java:181)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.containerLaunchedOnNode(CapacityScheduler.java:596)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:539)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:617)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)
        at java.lang.Thread.run(Thread.java:619)
{code}"
MAPREDUCE-3519,Deadlock in LocalDirsHandlerService and ShuffleHandler,"MAPREDUCE-3121 cloned Configuration object in LocalDirsHandlerService.init() to avoid others to access that configuration object. But since it is used in local FileSystem object creation in LocalDirAllocator.AllocatorPerContext and the same FileSystem object is used in ShuffleHandler.Shuffle.localDirAllocator, this is causing a deadlock when accessing this configuration object from LocalDirsHandlerService and ShuffleHandler along with AllocatorPerContext object."
MAPREDUCE-3518,mapred queue -info <queue> -showJobs throws NPE,"mapred queue -info default -showJobs

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.tools.CLI.displayJobList(CLI.java:572)
        at org.apache.hadoop.mapred.JobQueueClient.displayQueueInfo(JobQueueClient.java:190)
        at org.apache.hadoop.mapred.JobQueueClient.run(JobQueueClient.java:103)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.mapred.JobQueueClient.main(JobQueueClient.java:234)
"
MAPREDUCE-3513,Capacity Scheduler web UI has a spelling mistake for Memory.,"The web page for capacity scheduler has a column named ""Memopry Total"", a spelling mistake which needs to be fixed."
MAPREDUCE-3512,Batch jobHistory disk flushes,"The mr-am flushes each individual job history event to disk for AM recovery. The history even handler ends up with a significant backlog for tests like MAPREDUCE-3402. 
History events could be batched up based on num records / time / TaskFinishedEvents to reduce the number of DFS writes - with the potential drawback of having to rerun some tasks during AM recovery."
MAPREDUCE-3431,NPE in Resource Manager shutdown,bringing up a resource manager failed; shutdown triggered an NPE
MAPREDUCE-3392,Cluster.getDelegationToken() throws NPE if client.getDelegationToken() returns null.,"Caused by: java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.Cluster.getDelegationToken(Cluster.java:399)
        at org.apache.hadoop.mapred.JobClient.getDelegationToken(JobClient.java:1074)
        at
org.apache.oozie.service.KerberosHadoopAccessorService.createJobClient(KerberosHadoopAccessorService.java:139)
        at org.apache.oozie.action.hadoop.JavaActionExecutor.createJobClient(JavaActionExecutor.java:810)
        at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:551)"
MAPREDUCE-3390,NPE while submitting job,"Caused by: java.lang.NullPointerException
        at java.io.Reader.<init>(Reader.java:61)
        at java.io.InputStreamReader.<init>(InputStreamReader.java:55)
        at org.apache.hadoop.mapreduce.v2.util.MRApps.setMRFrameworkClasspath(MRApps.java:183)
        at org.apache.hadoop.mapreduce.v2.util.MRApps.setClasspath(MRApps.java:220)
        at org.apache.hadoop.mapred.YARNRunner.createApplicationSubmissionContext(YARNRunner.java:360)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:237)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:377)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1159)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1156)
        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1156)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:571)"
MAPREDUCE-3385,Add warning message for the overflow in reduce() of org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer,"When we call the function reduce() of IntSumReducer,the result may overflow.
We should send a warning message to users if overflow occurs."
MAPREDUCE-3381,[Rumen] TestRumenJobTraces fails after MAPREDUCE-3035,TestRumenJobTraces fails after MAPREDUCE-3035. The issue is due to the null value passed for rackName.
MAPREDUCE-3375,Memory Emulation system tests.,"1. Test the Gridmix memory emulation feature for gridmix jobs with default progress interval, different input data, submission policies and user resolver modes . Verify the maps phase of total heap usage of gridmix jobs with corresponding the original job in the trace.

2. Test the Gridmix memory emulation feature for gridmix jobs with custom progress interval, different input data, submission policies and user resolver modes . Verify the maps phase of total heap usage of gridmix jobs with corresponding the original job in the trace.

3. Test the Gridmix memory emulation feature for gridmix jobs with default progress interval, different input data, submission policies and user resolver modes. Verify the maps and reduces phase of total heap usage metric of gridmix jobs with corresponding the original job in the trace.

4. Disable Gridmix memory emulation option and verify the jobs whether it emulates the heap memory or not.
 
"
MAPREDUCE-3363,"The ""totalnodes""  and ""memorytotal"" fields show wrong information if the nodes are going down and coming up early(before 10min) ","The node details is not moved from Totalnodes to lostnodes for 600000 ms.So if the node is going down and coming up before the expiry interval, the cluster status in terms of the total nodes and Total cluster memory displays wrong values. 
Atleast, if the same node is coming up again...should not consider as new node.No point of time duplicate nodes should be displayed in Totalnodes list.
"
MAPREDUCE-3345,Race condition in ResourceManager causing TestContainerManagerSecurity to fail sometimes,See https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1247//testReport/org.apache.hadoop.yarn.server/TestContainerManagerSecurity/testUnauthorizedUser/
MAPREDUCE-3343,TaskTracker Out of Memory because of distributed cache,"This Out of Memory happens when you run large number of jobs (using the distributed cache) on a TaskTracker. 

Seems the basic issue is with the distributedCacheManager (instance of TrackerDistributedCacheManager in TaskTracker.java), this gets created during TaskTracker.initialize(), and it keeps references to TaskDistributedCacheManager for every submitted job via the jobArchives Map, also references to CacheStatus via cachedArchives map. I am not seeing these cleaned up between jobs, so this can out of memory problems after really large number of jobs are submitted. We have seen this issue in a number of cases."
MAPREDUCE-3333,MR AM for sort-job going out of memory,"[~Karams] just found this. The usual sort job on a 350 node cluster hung due to OutOfMemory and eventually failed after an hour instead of the usual odd 20 minutes.
{code}
2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002
_01_001434 : java.lang.reflect.UndeclaredThrowableException
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""gsbl91281.blue.ygrid.yahoo.com/98.137.101.189""; destination host is: """"gsbl91525.blue.ygrid.yahoo.com"":45450; 
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)
        at $Proxy20.startContainer(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)
        ... 4 more
Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""gsbl91281.blue.ygrid.yahoo.com/98.137.101.189""; destination host is: """"gsbl91525.blue.ygrid.yahoo.com"":45450; 
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)
        at org.apache.hadoop.ipc.Client.call(Client.java:1089)
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)
        ... 6 more
Caused by: java.io.IOException: Couldn't set up IO streams
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)
        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)
        at org.apache.hadoop.ipc.Client.call(Client.java:1065)
        ... 7 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:597)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)
        ... 10 more
{code}"
MAPREDUCE-3317,Rumen TraceBuilder is emiting null as hostname,Trace generated by Rumen TraceBuilder contains null as hostname even though hostName and rackName are seen in history file. This is after MAPREDUCE-3035.
MAPREDUCE-3290,list-active-trackers throws NPE,"bin/mapred -list-active-trackers throws NPE in mrV2. Trace in the next comment.

"
MAPREDUCE-3274,Race condition in MR App Master Preemtion can cause a dead lock,There appears to be a race condition in the MR App Master in relation to preempting reducers to let a mapper run.  In the particular case that I have been debugging a reducer was selected for preemption that did not have a container assigned to it yet. When the container became available that reduce started running and the previous TA_KILL event appears to have been ignored.
MAPREDUCE-3241,(Rumen)TraceBuilder throws IllegalArgumentException,"When we run the TraceBuilder, we get this exception. Output of the TraceBuilder doesn't contain the map and reduce task information.

{code}
2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist
java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type
        at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)
        at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)
        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)

{code}"
MAPREDUCE-3225,Killing an unkown job throws NPE.,"On a job -kill of an unkown job, the code currently throws a NPE. Stack trace on the next comment."
MAPREDUCE-3208,NPE while flushing TaskLogAppender,"NPE will be throwed out while calling flush() of TaskLogAppender,if the QuietWriter isn't initialized in advance."
MAPREDUCE-3205,"MR2 memory limits should be pmem, not vmem","Currently, the memory resources requested for a container limit the amount of virtual memory used by the container. On my test clusters, at least, Java processes take up nearly twice as much vmem as pmem - a Java process running with -Xmx500m uses 935m of vmem and only about 560m of pmem.

This will force admins to either under-utilize available physical memory, or oversubscribe it by configuring the available resources on a TT to be larger than the true amount of physical RAM.

Instead, I would propose that the resource limit apply to pmem, and allow the admin to configure a ""vmem overcommit ratio"" which sets the vmem limit as a function of pmem limit."
MAPREDUCE-3157,Rumen TraceBuilder is skipping analyzing 0.20 history files,Rumen TraceBuilder is assuming the Pre21 history file name format to be JTIdentifier_jobId_<something>. But it can be jobId_<something> also as it is now in latest 0.20.x version. This also needs to be understood by TraceBuilder.
MAPREDUCE-3109,NPE exception in the TestClass org.apache.hadoop.yarn.server.resourcemanager.Application,"Method *List<Container> org.apache.hadoop.yarn.server.resourcemanager.Application.getResources() throws IOException* throws NPE. 

This is causing TestCases to fail like:
*void org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager.testContainerStatusesWillBeStoredInResourceManager() throws Exception* with following trace:
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.resourcemanager.Application.getResources(Application.java:283)
	at org.apache.hadoop.yarn.server.resourcemanager.Application.schedule(Application.java:311)
	at org.apache.hadoop.yarn.server.resourcemanager.TestResourceManager.testContainerStatusesWillBeStoredInResourceManager(TestResourceManager.java:196)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:46)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)


{noformat}


Devraj,

Can you please take a look into it."
MAPREDUCE-3084,race when KILL_CONTAINER is received for a LOCALIZED container,"Depending on when ContainersLaunch starts a container, {{KILL_CONTAINER}} when container state is {{LOCALIZED}} ({{LAUNCH_CONTAINER}} event already sent) can end up generating a {{CONTAINER_LAUNCHED}} event - which isn't handled by ContainerState: {{KILLING}}. Also, the launched container won't be killed since {{CLEANUP_CONTAINER}} would have already been processed."
MAPREDUCE-3065,ApplicationMaster killed by NodeManager due to excessive virtual memory consumption,"> Hey Vinod,
> 
> OK, so I have a little more clarity into this.
> 
> When I bump my resource request for my AM to 4096, it runs. The important line in the NM logs is:
> 
> 2011-09-21 13:43:44,366 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 25656 for container-id container_1316637655278_0001_01_000001 : Virtual 2260938752 bytes, limit : 4294967296 bytes; Physical 120860672 bytes, limit -1 bytes
> 
> The thing to note is the virtual memory, which is off the charts, even though my physical memory is almost nothing (12 megs). I'm still poking around the code, but I am noticing that there are two checks in the NM, one for virtual mem, and one for physical mem. The virtual memory check appears to be toggle-able, but is presumably defaulted to on.
> 
> At this point I'm trying to figure out exactly what the VMEM check is for, why YARN thinks my app is taking 2 gigs, and how to fix this.
> 
> Cheers,
> Chris
> ________________________________________
> From: Chris Riccomini [criccomini@linkedin.com]
> Sent: Wednesday, September 21, 2011 1:42 PM
> To: mapreduce-dev@hadoop.apache.org
> Subject: Re: ApplicationMaster Memory Usage
> 
> For the record, I bumped to 4096 for memory resource request, and it works.
> :(
> 
> 
> On 9/21/11 1:32 PM, ""Chris Riccomini"" <criccomini@linkedin.com> wrote:
> 
>> Hey Vinod,
>> 
>> So, I ran my application master directly from the CLI. I commented out the
>> YARN-specific code. It runs fine without leaking memory.
>> 
>> I then ran it from YARN, with all YARN-specific code commented it. It again
>> ran fine.
>> 
>> I then uncommented JUST my registerWithResourceManager call. It then fails
>> with OOM after a few seconds. I call registerWithResourceManager, and then go
>> into a while(true) { println(""yeh"") sleep(1000) }. Doing this prints:
>> 
>> yeh
>> yeh
>> yeh
>> yeh
>> yeh
>> 
>> At which point, it dies, and, in the NodeManager,I see:
>> 
>> 2011-09-21 13:24:51,036 WARN  monitor.ContainersMonitorImpl
>> (ContainersMonitorImpl.java:isProcessTreeOverLimit(289)) - Process tree for
>> container: container_1316626117280_0005_01_000001 has processes older than 1
>> iteration running over the configured limit. Limit=2147483648, current usage =
>> 2192773120
>> 2011-09-21 13:24:51,037 WARN  monitor.ContainersMonitorImpl
>> (ContainersMonitorImpl.java:run(453)) - Container
>> [pid=23852,containerID=container_1316626117280_0005_01_000001] is running
>> beyond memory-limits. Current usage : 2192773120bytes. Limit :
>> 2147483648bytes. Killing container.
>> Dump of the process-tree for container_1316626117280_0005_01_000001 :
>> |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS)
>> VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
>> |- 23852 20570 23852 23852 (bash) 0 0 108638208 303 /bin/bash -c java -Xmx512M
>> -cp './package/*' kafka.yarn.ApplicationMaster
>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 5 1 1316626117280
>> com.linkedin.TODO 1
>> 1>/tmp/logs/application_1316626117280_0005/container_1316626117280_0005_01_000
>> 001/stdout
>> 2>/tmp/logs/application_1316626117280_0005/container_1316626117280_0005_01_000
>> 001/stderr
>> |- 23855 23852 23852 23852 (java) 81 4 2084134912 14772 java -Xmx512M -cp
>> ./package/* kafka.yarn.ApplicationMaster
>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 5 1 1316626117280
>> com.linkedin.TODO 1
>> 2011-09-21 13:24:51,037 INFO  monitor.ContainersMonitorImpl
>> (ContainersMonitorImpl.java:run(463)) - Removed ProcessTree with root 23852
>> 
>> Either something is leaking in YARN, or my registerWithResourceManager code
>> (see below) is doing something funky.
>> 
>> I'm trying to avoid going through all the pain of attaching a remote debugger.
>> Presumably things aren't leaking in YARN, which means it's likely that I'm
>> doing something wrong in my registration code.
>> 
>> Incidentally, my NodeManager is running with 1000 megs. My application master
>> memory is set to 2048, and my -Xmx setting is 512M
>> 
>> Cheers,
>> Chris
>> ________________________________________
>> From: Vinod Kumar Vavilapalli [vinodkv@hortonworks.com]
>> Sent: Wednesday, September 21, 2011 11:52 AM
>> To: mapreduce-dev@hadoop.apache.org
>> Subject: Re: ApplicationMaster Memory Usage
>> 
>> Actually MAPREDUCE-2998 is only related to MRV2, so that isn't related.
>> 
>> Somehow, your JVM itself is taking so much of virtual memory. Are you
>> loading some native libs?
>> 
>> And how many containers have already been allocated by the time the AM
>> crashes. May be you are accumulating some per-container data. You can try
>> dumping heap vai hprof.
>> 
>> +Vinod
>> 
>> 
>> On Wed, Sep 21, 2011 at 11:21 PM, Chris Riccomini
>> <criccomini@linkedin.com>wrote:
>> 
>>> Hey Vinod,
>>> 
>>> I svn up'd, and rebuilt. My application's task (container) now runs!
>>> 
>>> Unfortunately, my application master eventually gets killed by the
>>> NodeManager anyway, and I'm still not clear as to why. The AM is just
>>> running a loop, asking for a container, and executing a command in the
>>> container. It keeps doing this over and over again. After a few iterations,
>>> it gets killed with something like:
>>> 
>>> 2011-09-21 10:42:40,869 INFO  monitor.ContainersMonitorImpl
>>> (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 21666
>>> for container-id container_1316626117280_0002_01_000001 : Virtual 2260938752
>>> bytes, limit : 2147483648 bytes; Physical 77398016 bytes, limit -1 bytes
>>> 2011-09-21 10:42:40,869 WARN  monitor.ContainersMonitorImpl
>>> (ContainersMonitorImpl.java:isProcessTreeOverLimit(289)) - Process tree for
>>> container: container_1316626117280_0002_01_000001 has processes older than 1
>>> iteration running over the configured limit. Limit=2147483648, current usage
>>> = 2260938752
>>> 2011-09-21 10:42:40,870 WARN  monitor.ContainersMonitorImpl
>>> (ContainersMonitorImpl.java:run(453)) - Container
>>> [pid=21666,containerID=container_1316626117280_0002_01_000001] is running
>>> beyond memory-limits. Current usage : 2260938752bytes. Limit :
>>> 2147483648bytes. Killing container.
>>> Dump of the process-tree for container_1316626117280_0002_01_000001 :
>>>        |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS)
>>> SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
>>>        |- 21669 21666 21666 21666 (java) 105 4 2152300544 18593 java
>>> -Xmx512M -cp ./package/* kafka.yarn.ApplicationMaster
>>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280
>>> com.linkedin.TODO 1
>>>       |- 21666 20570 21666 21666 (bash) 0 0 108638208 303 /bin/bash -c
>>> java -Xmx512M -cp './package/*' kafka.yarn.ApplicationMaster
>>> /home/criccomi/git/kafka-yarn/dist/kafka-streamer.tgz 2 1 1316626117280
>>> com.linkedin.TODO 1
>>> 1>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_00
>>> 0001/stdout
>>> 2>/tmp/logs/application_1316626117280_0002/container_1316626117280_0002_01_00
>>> 0001/stderr
>>> 
>>> 2011-09-21 10:42:40,870 INFO  monitor.ContainersMonitorImpl
>>> (ContainersMonitorImpl.java:run(463)) - Removed ProcessTree with root 21666
>>> 
>>> I don't think that my AM is leaking memory. Full code paste after the break
>>> 
>>> 1. Do I need to release a container in my AM even if the AM receives it as
>>> a finished container in the resource request response?
>>> 2. Do I need to free any other resources after a resource request (e.g.
>>> ResourceRequest, AllocateRequest, etc)?
>>> 
>>> Cheers,
>>> Chris
>>> 
>>> 
>>> def main(args: Array[String]) {
>>>   // YARN will always give our ApplicationMaster
>>>   // the first four parameters as: <package> <app id> <attempt id>
>>> <timestamp>
>>>   val packagePath = args(0)
>>>   val appId = args(1).toInt
>>>   val attemptId = args(2).toInt
>>>   val timestamp = args(3).toLong
>>> 
>>>   // these are our application master's parameters
>>>   val streamerClass = args(4)
>>>   val tasks = args(5).toInt
>>> 
>>>   // TODO log params here
>>> 
>>>   // start the application master helper
>>>   val conf = new Configuration
>>>   val applicationMasterHelper = new ApplicationMasterHelper(appId,
>>> attemptId, timestamp, conf)
>>>     .registerWithResourceManager
>>> 
>>>   // start and manage the slaves
>>>   val noReleases = List[ContainerId]()
>>>   var runningContainers = 0
>>> 
>>>   // keep going forever
>>>   while (true) {
>>>     val nonRunningTasks = tasks - runningContainers
>>>     val response =
>>> applicationMasterHelper.sendResourceRequest(nonRunningTasks, noReleases)
>>> 
>>>     response.getAllocatedContainers.foreach(container => {
>>>       new ContainerExecutor(packagePath, container)
>>>         .addCommand(""java -Xmx256M -cp './package/*'
>>> kafka.yarn.StreamingTask "" + streamerClass + "" ""
>>>           + ""1>"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + ""/stdout ""
>>>           + ""2>"" + ApplicationConstants.LOG_DIR_EXPANSION_VAR +
>>> ""/stderr"").execute(conf)
>>>     })
>>> 
>>>     runningContainers += response.getAllocatedContainers.length
>>>     runningContainers -= response.getCompletedContainersStatuses.length
>>> 
>>>     Thread.sleep(1000)
>>>   }
>>> 
>>>   applicationMasterHelper.unregisterWithResourceManager(""SUCCESS"")
>>> }
>>> 
>>> 
>>> class ApplicationMasterHelper(iAppId: Int, iAppAttemptId: Int, lTimestamp:
>>> Long, conf: Configuration) {
>>> val rpc = YarnRPC.create(conf)
>>> val appId = Records.newRecord(classOf[ApplicationId])
>>> val appAttemptId = Records.newRecord(classOf[ApplicationAttemptId])
>>> val rmAddress =
>>> NetUtils.createSocketAddr(conf.get(YarnConfiguration.RM_SCHEDULER_ADDRESS,
>>> YarnConfiguration.DEFAULT_RM_SCHEDULER_ADDRESS))
>>> val resourceManager = rpc.getProxy(classOf[AMRMProtocol], rmAddress,
>>> conf).asInstanceOf[AMRMProtocol]
>>> var requestId = 0
>>> 
>>> appId.setClusterTimestamp(lTimestamp)
>>> appId.setId(iAppId)
>>> appAttemptId.setApplicationId(appId)
>>> appAttemptId.setAttemptId(iAppAttemptId)
>>> 
>>> def registerWithResourceManager(): ApplicationMasterHelper = {
>>>   val req = Records.newRecord(classOf[RegisterApplicationMasterRequest])
>>>   req.setApplicationAttemptId(appAttemptId)
>>>   // TODO not sure why these are blank- This is how spark does it
>>>   req.setHost("""")
>>>   req.setRpcPort(1)
>>>   req.setTrackingUrl("""")
>>>   resourceManager.registerApplicationMaster(req)
>>>   this
>>> }
>>> 
>>> def unregisterWithResourceManager(state: String): ApplicationMasterHelper
>>> = {
>>>   val finReq = Records.newRecord(classOf[FinishApplicationMasterRequest])
>>>   finReq.setAppAttemptId(appAttemptId)
>>>   finReq.setFinalState(state)
>>>   resourceManager.finishApplicationMaster(finReq)
>>>   this
>>> }
>>> 
>>> def sendResourceRequest(containers: Int, release: List[ContainerId]):
>>> AMResponse = {
>>>   // TODO will need to make this more flexible for hostname requests, etc
>>>   val request = Records.newRecord(classOf[ResourceRequest])
>>>   val pri = Records.newRecord(classOf[Priority])
>>>   val capability = Records.newRecord(classOf[Resource])
>>>   val req = Records.newRecord(classOf[AllocateRequest])
>>>   request.setHostName(""*"")
>>>   request.setNumContainers(containers)
>>>   pri.setPriority(1)
>>>   request.setPriority(pri)
>>>   capability.setMemory(128)
>>>   request.setCapability(capability)
>>>   req.setResponseId(requestId)
>>>   req.setApplicationAttemptId(appAttemptId)
>>>   req.addAllAsks(Lists.newArrayList(request))
>>>   req.addAllReleases(release)
>>>   requestId += 1
>>>   // TODO we might want to return a list of container executors here
>>> instead of AMResponses
>>>   resourceManager.allocate(req).getAMResponse
>>> }
>>> }
>>> 
>>> 
>>> ________________________________________
>>> From: Vinod Kumar Vavilapalli [vinodkv@hortonworks.com]
>>> Sent: Wednesday, September 21, 2011 10:08 AM
>>> To: mapreduce-dev@hadoop.apache.org
>>> Subject: Re: ApplicationMaster Memory Usage
>>> 
>>> Yes, the process-dump clearly tells that this is MAPREDUCE-2998.
>>> 
>>> +Vinod
>>> (With a smirk to see his container-memory-monitoring code in action)
>>> 
>>> 
>>> On Wed, Sep 21, 2011 at 10:26 PM, Arun C Murthy <acm@hortonworks.com>
>>> wrote:
>>> 
>>>> I'll bet you are hitting MR-2998.
>>>> 
>>>> From the changelog:
>>>> 
>>>>   MAPREDUCE-2998. Fixed a bug in TaskAttemptImpl which caused it to fork
>>>> bin/mapred too many times. Contributed by Vinod K V.
>>>> 
>>>> Arun
>>>> 
>>>> On Sep 21, 2011, at 9:52 AM, Chris Riccomini wrote:
>>>> 
>>>>> Hey Guys,
>>>>> 
>>>>> My ApplicationMaster is being killed by the NodeManager because of
>>> memory
>>>> consumption, and I don't understand why. I'm using -Xmx512M, and setting
>>> my
>>>> resource request to 2048.
>>>>> 
>>>>> 
>>>>>   .addCommand(""java -Xmx512M -cp './package/*'
>>>> kafka.yarn.ApplicationMaster "" ...
>>>>> 
>>>>>   ...
>>>>> 
>>>>>   private var memory = 2048
>>>>> 
>>>>>   resource.setMemory(memory)
>>>>>   containerCtx.setResource(resource)
>>>>>   containerCtx.setCommands(cmds.toList)
>>>>>   containerCtx.setLocalResources(Collections.singletonMap(""package"",
>>>> packageResource))
>>>>>   appCtx.setApplicationId(appId)
>>>>>   appCtx.setUser(user.getShortUserName)
>>>>>   appCtx.setAMContainerSpec(containerCtx)
>>>>>   request.setApplicationSubmissionContext(appCtx)
>>>>>   applicationsManager.submitApplication(request)
>>>>> 
>>>>> When this runs, I see (in my NodeManager's logs):
>>>>> 
>>>>> 
>>>>> 2011-09-21 09:35:19,112 INFO  monitor.ContainersMonitorImpl
>>>> (ContainersMonitorImpl.java:run(402)) - Memory usage of ProcessTree 28134
>>>> for container-id container_1316559026783_0003_01_000001 : Virtual
>>> 2260938752
>>>> bytes, limit : 2147483648 bytes; Physical 71540736 bytes, limit -1 bytes
>>>>> 2011-09-21 09:35:19,112 WARN  monitor.ContainersMonitorImpl
>>>> (ContainersMonitorImpl.java:isProcessTreeOverLimit(289)) - Process tree
>>> for
>>>> container: container_1316559026783_0003_01_000001 has processes older
>>> than 1
>>>> iteration running over the configured limit. Limit=2147483648, current
>>> usage
>>>> = 2260938752
>>>>> 2011-09-21 09:35:19,113 WARN  monitor.ContainersMonitorImpl
>>>> (ContainersMonitorImpl.java:run(453)) - Container
>>>> [pid=28134,containerID=container_1316559026783_0003_01_000001] is running
>>>> beyond memory-limits. Current usage : 2260938752bytes. Limit :
>>>> 2147483648bytes. Killing container.
>>>>> Dump of the process-tree for container_1316559026783_0003_01_000001 :
>>>>>      |- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS)
>>>> SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
>>>>>      |- 28134 25886 28134 28134 (bash) 0 0 108638208 303 /bin/bash -c
>>>> java -Xmx512M -cp './package/*' kafka.yarn.ApplicationMaster 3 1
>>>> 1316559026783 com.linkedin.TODO 1
>>>> 
>>> 1>/tmp/logs/application_1316559026783_0003/container_1316559026783_0003_01_00
>>> 0001/stdout
>>>> 
>>> 2>/tmp/logs/application_1316559026783_0003/container_1316559026783_0003_01_00
>>> 0001/stderr
>>>>>      |- 28137 28134 28134 28134 (java) 92 3 2152300544 17163 java
>>>> -Xmx512M -cp ./package/* kafka.yarn.ApplicationMaster 3 1 1316559026783
>>>> com.linkedin.TODO 1
>>>>> 
>>>>> 2011-09-21 09:35:19,113 INFO  monitor.ContainersMonitorImpl
>>>> (ContainersMonitorImpl.java:run(463)) - Removed ProcessTree with root
>>> 28134
>>>>> 
>>>>> It appears that YARN is honoring my 2048 command, yet my process is
>>>> somehow taking 2260938752 bytes. I don't think that I'm using nearly that
>>>> much in permgen, and my heap is limited to 512. I don't have any JNI
>>> stuff
>>>> running (that I know of), so it's unclear to me what's going on here. The
>>>> only thing that I can think of is that Java's Runtime exec is forking,
>>> and
>>>> copying its entire JVM memory footprint for the fork.
>>>>> 
>>>>> Has anyone seen this? Am I doing something dumb?
>>>>> 
>>>>> Thanks!
>>>>> Chris
>>>> 
>>>> 
>>> 
> 
"
MAPREDUCE-3057,Job History Server goes of OutOfMemory with 1200 Jobs and Heap Size set to 10 GB,"History server was started with -Xmx10000m
Ran GridMix V3 with 1200 Jobs trace in STRESS mode on 350 nodes with each node 4 NMS.
All jobs finished as reported by RM Web UI and HADOOP_MAPRED_HOME/bin/mapred job -list all
But found that GridMix job client was stuck while trying connect to HistoryServer
Then tried to do HADOOP_MAPRED_HOME/bin/mapred job -status jobid
JobClient also got stuck while looking for token to connect to History server
Then looked at History Server logs and found History is trowing ""java.lang.OutOfMemoryError: GC overhead limit exceeded"" error.

With 10GB of Heap space and 1200 Jobs, History Server should not go out of memory .
No matter what are the type of jobs.



"
MAPREDUCE-3036,Some of the Resource Manager memory metrics go negative.,"ReservedGB seems to always be decremented when a container is released, even though the container never reserved any memory.
AvailableGB also seems to be able to go negative in a few situations."
MAPREDUCE-3026,"When user adds hierarchical queues to the cluster, mapred queue -list returns NULL Pointer Exception","When User adds the hierarchical queues, and try to see them from the command line using 
mapred queue -list 
It returns Null Pointer Exception."
MAPREDUCE-3005,MR app hangs because of a NPE in ResourceManager,"The app hangs and it turns out to be a NPE in ResourceManager. This happened two of five times on [~karams]'s sort runs on a big cluster.
{code}
2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)
        at java.lang.Thread.run(Thread.java:619)
{code}"
MAPREDUCE-2997,MR task fails before launch itself with an NPE in ContainerLauncher,"Exception found on the AM web UI while the application is running:
{code}
Container launch failed for container_1315908079531_0002_01_000387 : java.lang.NullPointerException
  at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:162)
  at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:204)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:619) 
{code}"
MAPREDUCE-2970,"Null Pointer Exception while submitting a Job, If mapreduce.framework.name property is not set.","If mapreduce.framework.name property is not set in mapred-site.xml, Null pointer Exception is thrown.

java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.Cluster$1.run(Cluster.java:133)
	at org.apache.hadoop.mapreduce.Cluster$1.run(Cluster.java:1)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.mapreduce.Cluster.getFileSystem(Cluster.java:131)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1067)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1089)
	at org.apache.hadoop.examples.WordCount.main(WordCount.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
MAPREDUCE-2954,Deadlock in NM with threads racing for ApplicationAttemptId,"Found this:
{code}
Java stack information for the threads listed above:
===================================================
""Thread-45"":
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.getApplicationId(ApplicationAttemptIdPBImpl.java:101)
        - waiting to lock <0xb6a43ba0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:144)
        - locked <0xb6a443a0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:31)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:215)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:34)
        at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:797)
        at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1640)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:360)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:355)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:113)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:619)
""Thread-30"":
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.getApplicationId(ApplicationAttemptIdPBImpl.java:101)
        - waiting to lock <0xb6a443a0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:144)
        - locked <0xb6a43ba0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:31)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:215)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:34)
        at java.util.concurrent.ConcurrentSkipListMap.doRemove(ConcurrentSkipListMap.java:1078)
        at java.util.concurrent.ConcurrentSkipListMap.remove(ConcurrentSkipListMap.java:1673)
        at java.util.concurrent.ConcurrentSkipListMap$Iter.remove(ConcurrentSkipListMap.java:2256)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getNodeStatus(NodeStatusUpdaterImpl.java:223)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$300(NodeStatusUpdaterImpl.java:62)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:262)
Found 1 deadlock.
{code}"
MAPREDUCE-2953,"JobClient fails due to a race in RM, removes staged files and in turn crashes MR AM","[~Karams] ran into this multiple times. MR JobClient crashes immediately.

{code}
11/09/08 10:52:35 INFO mapreduce.JobSubmitter: number of splits:2094
11/09/08 10:52:36 INFO mapred.YARNRunner: AppMaster capability = memory: 2048,
11/09/08 10:52:36 INFO mapred.YARNRunner: Command to launch container for ApplicationMaster is : $JAVA_HOME/bin/java -Dhadoop.root.logger=INFO,console -Xmx1536m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1315478927026 1 <FAILCOUNT> 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr
11/09/08 10:52:36 INFO mapred.ResourceMgrDelegate: Submitted application application_1315478927026_1 to ResourceManager
11/09/08 10:52:36 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/gridperf/.staging/job_1315478927026_0001
RemoteTrace:
 at Local Trace:
        org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: failed to run job
        at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)
        at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:250)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:377)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1072)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1069)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1069)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1089)
        at org.apache.hadoop.examples.RandomWriter.run(RandomWriter.java:283)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.examples.RandomWriter.main(RandomWriter.java:294)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
}
{code}

The client crashes due to a race in RM.

Because the client fails, it immediately removes the staged files which in turn makes the MR AM itself to crash due to failed localization on the NM."
MAPREDUCE-2937,Errors in Application failures are not shown in the client trace.,"The client side does not show enough information on why the job failed. Here is step to reproduce it:

1) set the scheduler to be capacity scheduler with queues a, b
2) submit a job to a queue that is not a,b

The job just fails without saying why it failed. We should have enough trace log at the client side to let the user know why it failed."
MAPREDUCE-2906,FindBugs OutOfMemoryError,"When running the findbugs target from Jenkins, I get an OutOfMemory error.
The ""effort"" in FindBugs is set to Max which ends up using a lot of memory to go through all the classes. The jvmargs passed to FindBugs is hardcoded to 512 MB max.

We can leave the default to 512M, as long as we pass this as an ant parameter which can be overwritten in individual cases through -D, or in the build.properties file (either basedir, or user's home directory).
"
MAPREDUCE-2872,Optimize TaskTracker memory usage,"We observe high memory usage of framework level components on slave node, mainly TaskTracker & Child, especially for large clusters. To be clear at first, large jobs with 10000-100000 map and >10000 reduce tasks are very common in our offline cluster, and will very likely continue to grow. This is reasonable because the number of map & reduce slots are in the same range, and it's impractical for users to reduce their job's task number without execution time penalty. 

High memory consumption will:
* Limit the memory used by up level application; 
* Reduce page cache space, which plays a  important role in spill, merge, shuffle and even HDFS performance; 
* Increase the probability of slave node OOM, which may affect storage layer(HDFS) too. 

A stable TT with predictable memory behavior is desired, this also applies to Child JVM.

This issue focuses on TaskTracker memory optimization, on our cluster, TaskTracker use 600M+ memory & 300%+(3core+) CPU at peak, and 300M+ memory & much less CPU in average, so we need to set -Xmx to 1000M for TT to prevent OOM, then the TT memory is in 200M-1200M range, and 800M in average. 

Here are some ideas:  

Jetty http connection use a lot memory when these are many requests in queue, we need to limit the length of the queue, combine multiple requests into one request, or use netty just like MR2

TaskCompletionEvents use a lot memory too if a job have large number of map task, this won't be a problem in MR2, but can be optimized, A typical TaskCompletionEvent object use 296 bytes memory, a job with 100000 map will use about 30M memory, problem will appear if there are some big RunningJob in a TaskTracker. There are more memory efficient implementations for TaskCompletionEvent.

IndexCache: memory of indexcache varies directly as reduce number, on large cluster 10MB of indexcache is not enough, 
we set it to 100MB, again use primitive long[] instead of IndexRecord[] can save 50% of memory.

Although some of the above won't be a problem in MR-v2, since MR-v1 is still widely used, I think optimizations are needed.



"
MAPREDUCE-2799,[MR-279] NPE is throwing on job -status <Invalid Job ID/Job Id doesn't exist>,"
{code:xml} 
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.mapred.ClientServiceDelegate.refreshProxy(ClientServiceDelegate.java:113)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getProxy(ClientServiceDelegate.java:101)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getRefreshedProxy(ClientServiceDelegate.java:94)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:384)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:515)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:154)
        at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:223)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1074)
{code} "
MAPREDUCE-2788,Normalize requests in FifoScheduler.allocate to prevent NPEs later,The assignContainer() method in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue can cause the scheduler to crash if the ResourseRequest capability memory == 0 (divide by zero).
MAPREDUCE-2784,[Gridmix] TestGridmixSummary fails with NPE when run in DEBUG mode.,TestGridmixSummary fails with NPE when run in debug mode. JobFactory tries to access the _createReaderThread()_ API of JobStoryProducer which returns null in TestGridmixSummary's FakeJobStoryProducer.
MAPREDUCE-2762,[MR-279] - Cleanup staging dir after job completion,"The files created under the staging dir have to be deleted after job completion. Currently, all job.* files remain forever in the ${yarn.apps.stagingDir}"
MAPREDUCE-2693,NPE in AM causes it to lose containers which are never returned back to RM,"The following exception in AM of an application at the top of queue causes this. Once this happens, AM keeps obtaining
containers from RM and simply loses them. Eventually on a cluster with multiple jobs, no more scheduling happens
because of these lost containers.

It happens when there are blacklisted nodes at the app level in AM. A bug in AM
(RMContainerRequestor.containerFailedOnHost(hostName)) is causing this - nodes are simply getting removed from the
request-table. We should make sure RM also knows about this update.

========================================================================
11/06/17 06:11:18 INFO rm.RMContainerAllocator: Assigned based on host match 98.138.163.34
11/06/17 06:11:18 INFO rm.RMContainerRequestor: BEFORE decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=4978 #asks=5
11/06/17 06:11:18 INFO rm.RMContainerRequestor: AFTER decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=4977 #asks=5
11/06/17 06:11:18 INFO rm.RMContainerRequestor: BEFORE decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=1540 #asks=5
11/06/17 06:11:18 INFO rm.RMContainerRequestor: AFTER decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=1539 #asks=6
11/06/17 06:11:18 ERROR rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.decResourceRequest(RMContainerRequestor.java:246)
        at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.decContainerReq(RMContainerRequestor.java:198)
        at
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:523)
        at
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:433)
        at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:151)
        at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:220)
        at java.lang.Thread.run(Thread.java:619)"
MAPREDUCE-2686,NPE while requesting info for a non-existing job,"While performing job related operations such as job -kill, -status, -events etc for an unknown job, the following NPE is seen:

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.mapred.ClientServiceDelegate.refreshProxy(ClientServiceDelegate.java:112)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getProxy(ClientServiceDelegate.java:100)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getRefreshedProxy(ClientServiceDelegate.java:93)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:383)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:515)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:154)
        at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:254)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1074)
"
MAPREDUCE-2651,Race condition in Linux Task Controller for job log directory creation,There is a rare race condition in linux task controller when concurrent task processes tries to create job log directory at the same time. 
MAPREDUCE-2647,Memory sharing across all the Tasks in the Task Tracker to improve the job performance,"	If all the tasks (maps/reduces) are using (working with) the same additional data to execute the map/reduce task, each task should load the data into memory individually and read the data. It is the additional effort for all the tasks to do the same job. Instead of loading the data by each task, data can be loaded into main memory and it can be used to execute all the tasks.


h5.Proposed Solution:
1. Provide a mechanism to load the data into shared memory and to read that data from main memory.
2. We can provide a java API, which internally uses the native implementation to read the data from the memory. All the maps/reducers can this API for reading the data from the main memory. 


h5.Example: 
	Suppose in a map task, ip address is a key and it needs to get location of the ip address from a local file. In this case each map task should load the file into main memory and read from it and close it. It takes some time to open, read from the file and process every time. Instead of this, we can load the file in the task tracker memory and each task can read from the memory directly.
"
MAPREDUCE-2631,Potential resource leaks in BinaryProtocol$TeeOutputStream.java,"{code:title=BinaryProtocol$TeeOutputStream.java|borderStyle=solid}

public void close() throws IOException {
      flush();
      file.close();
      out.close();
    }
{code} 

In the above code, if the file.close() throws any exception out will not be closed.
 
"
MAPREDUCE-2630,MR-279: refreshQueues leads to NPEs when used w/FifoScheduler,"The RM's admin service exposes a method refreshQueues that is used to update the queue configuration when used with the CapacityScheduler, but if it is used with the FifoScheduler, it will set the containerTokenSecretManager/clusterTracker fields on the FifoScheduler to null, which eventually leads to NPE. Since the FifoScheduler only has one queue that cannot be refreshed, the correct behavior is for the refreshQueues call to be a no-op.

I will attach a patch that fixes this by splitting the ResourceScheduler's reinitialize method into separate initialize/updateQueues methods."
MAPREDUCE-2618,"MR-279: 0 map, 0 reduce job fails with Null Pointer Exception","A 0 map, 0 reduce job fails with an NPE. This case works fine on hadoop-0.20.x. The job should succeed and run setup/cleanup code - with no tasks.  Below is the stacktrace:

11/06/05 19:35:37 WARN mapred.ClientServiceDelegate:
 StackTrace: java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTaskAttemptCompletionEvents(JobImpl.java:498)
        at
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler.getTaskAttemptCompletionEvents(MRClientService.java:290)
        at
org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBServiceImpl.java:139)
        at
org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:195)
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$TunnelResponder.call(ProtoOverHadoopRpcEngine.java:168)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:420)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1406)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1402)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1094)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1400)"
MAPREDUCE-2603,Gridmix system tests are failing due to high ram emulation enable by default for normal mr jobs in the trace which exceeds the solt capacity.,"In Gridmix high ram emulation enable by default.Because of this feature, some of the gridmix system tests are hanging for some time and then failing after timeout. Actually the failure case was occurring whenever reserved slot capacity exceeds the cluster slot capacity.So for fixing the issue by disabling the high ram emulation in the tests which are using the normal mr jobs in the traces."
MAPREDUCE-2598,"MR 279: miscellaneous UI, NPE fixes for JobHistory, UI",
MAPREDUCE-2552,MR 279: NPE when requesting attemptids for completed jobs ,"While constructing a CompletedJob instance on the JobHistory server - successfuleAttempt is not populated. Causes an NPE when listing completed attempts for a job via the CLI.

CLI: hadoop job -list-attempt-ids <job_id> MAP completed"
MAPREDUCE-2549,"Potential resource leaks in HadoopServer.java, RunOnHadoopWizard.java and Environment.java",
MAPREDUCE-2541,"Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception","The race condition goes like this:
Thread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())
Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());
When SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).
When this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.

A quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.
"
MAPREDUCE-2539,NPE when calling JobClient.getMapTaskReports for retired job,"When calling JobClient.getMapTaskReports for a retired job this results in a NPE.  In the 0.20.* version an empty TaskReport array was returned instead.

Caused by: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:588)
        at org.apache.pig.tools.pigstats.JobStats.addMapReduceStatistics(JobStats.java:388)
......"
MAPREDUCE-2513,Improvements in Job Tracker UI for monitoring and managing the map reduce jobs,"It will be helpful to the user/administrator if we provide following features in the Job Tracker UI 

1. User wants to get the list of jobs submitted with given state
2. User wants to kill a scheduled/running job through UI
3. User wants to change the priority of a job 
4. User wants to get the scheduling information of jobs
5. User wants to delete the logs of Jobs and tasks
6. Only authorized users to be able to perform the above operations through task management UI
7. Pagination support for the jobs listing


"
MAPREDUCE-2510,TaskTracker throw OutOfMemoryError after upgrade to jetty6,"Our product cluster's TaskTracker sometimes throw OutOfMemoryError after upgrade to jetty6. The exception in TT's log is as follows:
2011-05-17 19:16:40,756 ERROR org.mortbay.log: Error for /mapOutput

java.lang.OutOfMemoryError: Java heap space

        at java.io.BufferedInputStream.<init>(BufferedInputStream.java:178)

        at org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:44)

        at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:176)

        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:359)

        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:3040)

        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)

        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)

        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)

        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)

        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)

        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)

        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)

        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)

        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)

        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)

        at org.mortbay.jetty.Server.handle(Server.java:324)

        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)

        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)

        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)

        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)

        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)

        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)

        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

Exceptions in .out file:
java.lang.OutOfMemoryError: Java heap space

Exception in thread ""process reaper"" java.lang.OutOfMemoryError: Java heap space

Exception in thread ""pool-1-thread-1"" java.lang.OutOfMemoryError: Java heap space

java.lang.OutOfMemoryError: Java heap space

java.lang.reflect.InvocationTargetException

Exception in thread ""IPC Server handler 6 on 50050""     at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.mortbay.log.Slf4jLog.warn(Slf4jLog.java:126)

        at org.mortbay.log.Log.warn(Log.java:181)

        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:449)

        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)

        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)

        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)

        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)

        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)

        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)

        at org.mortbay.jetty.Server.handle(Server.java:324)

        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)

        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)

        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)

        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)

        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)

        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)

        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)


"
MAPREDUCE-2509,MR-279: Fix NPE in UI for pending attempts,The task attempts page gets a 500 (and NPE in the AM logs) if the attempt is pending (not running yet).
MAPREDUCE-2504,MR 279: race in JobHistoryEventHandler stop ,"The condition to stop the eventHandling thread currently requires it to be 'stopped' AND interrupted. If an interrupt arrives after a take, but before handleEvent is called - the interrupt status ends up being handled by hadoop.util.Shell.runCommand() - which ignores it (and in the process resets the flag).
The eventHandling thread subsequently hangs on eventQueue.take()
This currently randomly fails unit tests - and can hang MR AMs."
MAPREDUCE-2470,Receiving NPE occasionally on RunningJob.getCounters() call,"This is running in a Java daemon that is used as an interface (Thrift) to get information and data from MR Jobs. Using JobClient.getJob(JobID) I successfully get a RunningJob object (I'm checking for NULL), and then rarely I get an NPE when I do RunningJob.getCounters(). This seems to occur after the daemon has been up and running for a while, and in the event of an Exception, I close the JobClient, set it to NULL, and a new one should then be created on the next request for data. Yet, I still seem to be unable to fetch the Counters. Below is the stack trace.


java.lang.NullPointerException
            at org.apache.hadoop.mapred.Counters.downgrade(Counters.java:77)
            at org.apache.hadoop.mapred.JobClient$NetworkedJob.getCounters(JobClient.java:381)
            at com.telescope.HadoopThrift.service.ServiceImpl.getReportResults(ServiceImpl.java:350)
            at com.telescope.HadoopThrift.gen.HadoopThrift$Processor$getReportResults.process(HadoopThrift.java:545)
            at com.telescope.HadoopThrift.gen.HadoopThrift$Processor.process(HadoopThrift.java:421)
            at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:697)
            at org.apache.thrift.server.THsHaServer$Invocation.run(THsHaServer.java:317)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:619)
"
MAPREDUCE-2461,Hudson jobs failing because mapred staging directory is full,"All of the tests that submit MR jobs are failing on the h7 build machine. This is because the staging directory is entirely full:

hudson@h7:/tmp/mr/mr$ ls -l /tmp/hadoop-hudson/mapred/staging/ | wc -l
31999

This makes me think that there's some bug where we're leaking things in the staging directory. I will manually clean this for now, but we should investigate."
MAPREDUCE-2417,"In Gridmix, in RoundRobinUserResolver mode, the testing/proxy users are not associated with unique users in a trace","As per the Gridmix documentation, the testing users should associate with unique user in the trace. However, currently the gridmix impersonate the users based on job irrespective of user."
MAPREDUCE-2412,"When you submit a job on a non-existent queue, you get an opaque NPE",
MAPREDUCE-2411,When you submit a job to a queue with no ACLs you get an inscrutible NPE,"With this patch we'll check for that, and print a message in the logs.  Then at submission time you find out about it."
MAPREDUCE-2387,Potential Resource leak in IOUtils.java,"

{code:title=IOUtils.java|borderStyle=solid}


    try {
      copyBytes(in, out, buffSize);
    } finally {
      if(close) {
        out.close();
        in.close();
      }
    }
 
{code} 

In the above code if any exception throws from the out.close() statement, in.close() statement will not execute and the input stream will not be closed.
"
MAPREDUCE-2362,Unit test failures: TestBadRecords and TestTaskTrackerMemoryManager,Fix unit-test failures: TestBadRecords (NPE due to rearranged MapTask code) and TestTaskTrackerMemoryManager (need hostname in output-string pattern).
MAPREDUCE-2346,JobClient's isSuccessful and isComplete API's can throw NPE in some cases,"Description:

* Submit a job to the job tracker and let the job complete its execution through one of the job client's submitJob APIs. 
* Jobclient returns a handle to the job, in the form of a RunningJob object. Client can use this object to check whether job is sucessful or whether job is completed.
* Reduce the following property *mapred.jobtracker.retirejob.interval*.By default this value is 1 day. I reduced it to 5 min.
* Set the property *mapred.job.tracker.persist.jobstatus.active* to {color:blue}*false*{color}.
* Call either isComplete or isSuccessful APIs, after *mapred.jobtracker.retirejob.interval* time period, previously mentioned APIs throw NPE.

Below I am attaching stack trace
{code:xml} 

java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobClient$NetworkedJob.isSuccessful(JobClient.java:330)
	at com.huawei.isap.hdp.mapreduce.test.TestJobClient.testjobClientForNULL(TestJobClient.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
{code} 
"
MAPREDUCE-2345,Optimize jobtracker's  memory usage  ,"Too many tasks will eat up a considerable amount of JobTracker's heap space. According to our observation, 50GB heap size can support to 5,000,000 tasks, so we should optimize jobtracker's memory usage for more jobs and tasks. Yourkit java profile show that counters, duplicate strings, task waste too much memory. Our optimization around these three points reduced jobtracker's memory to 1/3. "
MAPREDUCE-2289,Permissions race can make getStagingDir fail on local filesystem,"I've observed the following race condition in TestFairSchedulerSystem which uses a MiniMRCluster on top of RawLocalFileSystem:
- two threads call getStagingDir at the same time
- Thread A checks fs.exists(stagingArea) and sees false
-- Calls mkdirs(stagingArea, JOB_DIR_PERMISSIONS)
--- mkdirs calls the Java mkdir API which makes the file with umask-based permissions
- Thread B runs, checks fs.exists(stagingArea) and sees true
-- checks permissions, sees the default permissions, and throws IOE
- Thread A resumes and sets correct permissions"
MAPREDUCE-2243,Close all the file streams propely in a finally block to avoid their leakage.,"In the following classes streams should be closed in finally block to avoid their leakage in the exceptional cases.

CompletedJobStatusStore.java
------------------------------------------
       dataOut.writeInt(events.length);
        for (TaskCompletionEvent event : events) {
          event.write(dataOut);
        }
       dataOut.close() ;

EventWriter.java
----------------------
   encoder.flush();
   out.close();

MapTask.java
-------------------
    splitMetaInfo.write(out);
     out.close();

TaskLog
------------
 1) str = fis.readLine();
      fis.close();

2) dos.writeBytes(Long.toString(new File(logLocation, LogName.SYSLOG
      .toString()).length() - prevLogLength) + ""\n"");
    dos.close();

TotalOrderPartitioner.java
-----------------------------------
 while (reader.next(key, value)) {
	      parts.add(key);
	      key = ReflectionUtils.newInstance(keyClass, conf);
	    }
reader.close();


"
MAPREDUCE-2237,Lost heartbeat response containing MapTask throws NPE when it is resent,"When the JT sends a heartbeat response, it records it in trackerToHeartbeatResponseMap. But after MapTask writes its input split, it sets that split to null (assumedly to save memory?). So, if the heartbeat response is lost, and the JT needs to resend it, it will throw NPE since the split information has been lost."
MAPREDUCE-2236,No task may execute due to an Integer overflow possibility,"If the attempts is configured to use Integer.MAX_VALUE, an overflow occurs inside TaskInProgress, and thereby no task is attempted by the cluster and the map tasks stay in pending state forever.

For example, here's a job driver that causes this:
{code}
import java.io.IOException;

import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.mapred.FileInputFormat;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapred.TextInputFormat;
import org.apache.hadoop.mapred.lib.IdentityMapper;
import org.apache.hadoop.mapred.lib.NullOutputFormat;


@SuppressWarnings(""deprecation"")
public class IntegerOverflow {

	/**
	 * @param args
	 * @throws IOException 
	 */
	@SuppressWarnings(""deprecation"")
	public static void main(String[] args) throws IOException {
		JobConf conf = new JobConf();
		
		Path inputPath = new Path(""ignore"");
		FileSystem fs = FileSystem.get(conf);
		if (!fs.exists(inputPath)) {
			FSDataOutputStream out = fs.create(inputPath);
			out.writeChars(""Test"");
			out.close();
		}
		
		conf.setInputFormat(TextInputFormat.class);
		conf.setOutputFormat(NullOutputFormat.class);
		FileInputFormat.addInputPath(conf, inputPath);
		
		conf.setMapperClass(IdentityMapper.class);
		conf.setNumMapTasks(1);
		// Problem inducing line follows.
		conf.setMaxMapAttempts(Integer.MAX_VALUE);
		
		// No reducer in this test, although setMaxReduceAttempts leads to the same problem.
		conf.setNumReduceTasks(0);
		
		JobClient.runJob(conf);
	}

}
{code}

The above code will not let any map task run. Additionally, a log would be created inside JobTracker logs with the following information that clearly shows the overflow:
{code}
2010-12-30 00:59:07,836 WARN org.apache.hadoop.mapred.TaskInProgress: Exceeded limit of -2147483648 (plus 0 killed) attempts for the tip 'task_201012300058_0001_m_000000'
{code}

The issue lies inside the TaskInProgress class (/o/a/h/mapred/TaskInProgress.java), at line 1018 (trunk), part of the getTaskToRun(String taskTracker) method.
{code}
  public Task getTaskToRun(String taskTracker) throws IOException {   
    // Create the 'taskid'; do not count the 'killed' tasks against the job!
    TaskAttemptID taskid = null;
    /* ============ THIS LINE v ====================================== */
    if (nextTaskId < (MAX_TASK_EXECS + maxTaskAttempts + numKilledTasks)) {
    /* ============ THIS LINE ^====================================== */
      // Make sure that the attempts are unqiue across restarts
      int attemptId = job.getNumRestarts() * NUM_ATTEMPTS_PER_RESTART + nextTaskId;
      taskid = new TaskAttemptID( id, attemptId);
      ++nextTaskId;
    } else {
      LOG.warn(""Exceeded limit of "" + (MAX_TASK_EXECS + maxTaskAttempts) +
              "" (plus "" + numKilledTasks + "" killed)""  + 
              "" attempts for the tip '"" + getTIPId() + ""'"");
      return null;
    }
{code}

Since all three variables being added are integer in type, one of them being Integer.MAX_VALUE makes the condition fail with an overflow, thereby logging and returning a null as the result is negative.

One solution would be to make one of these variables into a long, so the addition does not overflow?"
MAPREDUCE-2233,LinuxTaskController throws NPE when task fails to start,"When the LinuxTaskController is incorrectly configured (eg wrong permissions or path) an NPE is thrown when it tries to kill the failed tasks, since there is no known pid."
MAPREDUCE-2204,Implement gridmix system tests with different time intervals of  high ram job traces.,"Implement gridmix system tests with different time intervals of High Ram map reduce jobs with below scenarios.

1) Generate input data based on cluster size and create the synthetic jobs by using the 2 min MR High RAM jobs trace and submit the jobs with below arguments.
GRIDMIX_JOB_TYPE = SleepJob
GRIDMIX_USER_RESOLVER = SubmitterUserResolver
GRIDMIX_SUBMISSION_POLICY = SERIAL
GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = false
Input Size = 200 MB * No. of nodes in cluster.
TRACE_FILE = 5 min folded trace.
GRIDMIX_SLEEP_MAP_MAX_TIME=5 sec.                                                                 
GRIDMIX_SLEEP_REDUCE_MAX_TIME=5 sec.
Verify JobStatus for each job and summary (QueueName, UserName, StatTime, FinishTime, MAPS, REDUCERS and COUNTERS etc) after completion of execution.

2) Generate input data based on cluster size and create the synthetic jobs by using the 3 min MR High RAM jobs trace and submit the jobs with below arguments.
GRIDMIX_JOB_TYPE = LoadJob
GRIDMIX_USER_RESOLVER = RoundRobinUserResolver
GRIDMIX_SUBMISSION_POLICY = STRESS
GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = false
BYTES_PER_FILE = 200 MB
Input Size = 400 MB * No. of nodes in cluster.
TRACE_FILE = 3 min folded trace.
Verify JobStatus for each job and summary (QueueName, UserName, StatTime, FinishTime, MAPS, REDUCERS and COUNTERS etc) after completion of execution.


3) Generate input data based on cluster size and create the synthetic jobs by using the 5 min MR High RAM jobs trace and submit the jobs with below arguments.
GRIDMIX_JOB_TYPE = LoadJob
GRIDMIX_USER_RESOLVER = EchoUserResolver
GRIDMIX_SUBMISSION_POLICY = Replay
GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = false
Input Size = 300 MB * No. of nodes in cluster.
TRACE_FILE = 5 min folded trace.
Verify JobStatus for each job and summary (QueueName, UserName, StatTime, FinishTime, MAPS, REDUCERS and COUNTERS etc) after completion of execution."
MAPREDUCE-2192,Implement gridmix system tests with different time intervals for MR streaming job traces.,"Develop gridmix system tests for below scenarios by using different time intervals of  MR streaming jobs.

1. Generate input data based on cluster size and create the synthetic jobs by using the 2 min folded MR streaming jobs trace and submit the jobs with below arguments.
GRIDMIX_JOB_TYPE = LOADJOB
GRIDMIX_USER_RESOLVER = SubmitterUserResolver
GRIDMIX_SUBMISSION_POLICY = STRESS
GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = True
Input Size = 250 MB * No. of nodes in cluster.
MINIMUM_FILE_SIZE=150MB
TRACE_FILE = 2 min folded trace.
Verify JobStatus for each job, input split size for each job and summary (QueueName, UserName, StatTime, FinishTime, maps, reducers and counters etc) after completion of execution.

2.  Generate input data based on cluster size and create the synthetic jobs by using the 3 min folded MR streaming jobs trace and submit the jobs with below arguments.
GRIDMIX_JOB_TYPE = LoadJob
GRIDMIX_USER_RESOLVER = RoundRobinUserResolver
GRIDMIX_BYTES_PER_FILE = 150 MB
GRIDMIX_SUBMISSION_POLICY = REPLAY
GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = True
Input Size = 200 MB * No. of nodes in cluster.
PROXY_USERS = proxy users file path
TRACE_FILE = 3 min folded trace.
Verify JobStatus for each job, input split size for each job and summary (QueueName, UserName, StatTime, FinishTime, maps, reducers and counters etc) after completion of execution.

3. Generate input data based on cluster size and create the synthetic jobs by using the 5 min MR streaming jobs trace and submit the jobs with below arguments.
GRIDMIX_JOB_TYPE = LoadJob
GRIDMIX_USER_RESOLVER = SubmitterUserResolver
GRIDMIX_SUBMISSION_POLICY = SERIAL
GRIDMIX_JOB_SUBMISSION_QUEUE_IN_TRACE = false
GRIDMIX_KEY_FRC = 0.5f
Input Size = 200MB * No. of nodes in cluster.
TRACE_FILE = 5 min folded trace.
Verify JobStatus for each job and summary (QueueName, UserName, StatTime, FinishTime, MAPS, REDUCERS and COUNTERS etc) after completion of execution."
MAPREDUCE-2178,Race condition in LinuxTaskController permissions handling,"The linux-task-controller executable currently traverses a directory heirarchy and calls chown/chmod on the files inside. There is a race condition here which can be exploited by an attacker, causing the task-controller to improprly chown an arbitrary target file (via a symlink) to the user running a MR job. This can be exploited to escalate to root.

[this issue was raised and discussed on the security@ list over the last couple of months]"
MAPREDUCE-2173,Race condition in TestBlockFixer causes intermittent failure,TestBlockFixer sometimes fails in reportCorruptBlocks because a corrupt block is deleted before in.readFully is called. This causes a BlockMissingException instead of the expected ChecksumException.
MAPREDUCE-2153,Bring in more job configuration properties in to the trace file,"To emulate distributed cache usage in gridmix jobs, there are 9 configuration properties needed to be available in trace file: 
(1) mapreduce.job.cache.files
(2) mapreduce.job.cache.files.visibilities
(3) mapreduce.job.cache.files.filesizes
(4) mapreduce.job.cache.files.timestamps

(5) mapreduce.job.cache.archives
(6) mapreduce.job.cache.archives.visibilities
(7) mapreduce.job.cache.archives.filesizes
(8) mapreduce.job.cache.archives.timestamps

(9) mapreduce.job.cache.symlink.create

To emulate data compression in gridmix jobs, trace file should contain the following configuration properties:
(1) mapreduce.map.output.compress
(2) mapreduce.map.output.compress.codec
(3) mapreduce.output.fileoutputformat.compress
(4) mapreduce.output.fileoutputformat.compress.codec
(5) mapreduce.output.fileoutputformat.compress.type

Ideally, gridmix should set many job specific configuration properties like io.sort.mb, io.sort.factor, etc when running simulated jobs to get the same effect of original/real job in terms of spilled records, number of merges, etc.

TraceBuilder should bring in all these properties into the generated trace file."
MAPREDUCE-2138,"Gridmix tests with different time interval mr traces (1min, 3min and 5min).","1. Generate input data based on cluster size and create the synthetic jobs by using the 1 min folded MR trace and
submit the jobs with below arguments.

GRIDMIX_JOB_TYPE = LoadJob
GRIDMIX_USER_RESOLVER = SubmitterUserResolver
GRIDMIX_SUBMISSION_POLICY = STRESS
Input Size = 400 MB * No. of nodes in cluster.
TRACE_FILE = 1 min folded trace.
Verify each job status and summary(QueueName, UserName, StatTime, FinishTime, maps, reducers and counters etc) after
completion of execution.

2. Generate input data based on cluster size and create the synthetic jobs by using the 3 min folded MR trace and
submit the jobs with below arguments.

GRIDMIX_JOB_TYPE = LoadJob
GRIDMIX_USER_RESOLVER = RoundRobinUserResolver
GRIDMIX_SUBMISSION_POLICY = Replay
Input Size = 200 MB * No. of nodes in cluster.
TRACE_FILE = 3 min folded trace.
PROXY_USERS = proxy users file path.
Verify each job status, submitted user and summary(QueueName, UserName, StatTime, FinishTime, maps, reducers and
counters etc) after completion of execution.

3. Generate input data based on cluster size and create the synthetic jobs by using the 5 min folded MR trace and
submit the jobs with below arguments.

GRIDMIX_JOB_TYPE = SleepJob
GRIDMIX_USER_RESOLVER = EchoUserResolver
GRIDMIX_MIN_FILE = 100 MB
GRIDMIX_SUBMISSION_POLICY = Serial
Input Size = 300 MB * No. of nodes in cluster.
TRACE_FILE = 5 min folded trace.
Verify each job status, file size and summary(QueueName, UserName, StatTime, FinishTime, maps, reducers and counters
etc) after completion of execution.

"
MAPREDUCE-2107,Emulate Memory Usage of Tasks in GridMix3,"MAPREDUCE-220 makes CPU/Memory usage of Tasks available in JobHistory files. Use this
to emulate the memory usage of Tasks (of course, once MAPREDUCE-2104 is done)."
MAPREDUCE-2104,Rumen TraceBuilder Does Not Emit CPU/Memory Usage Details in Traces,"Via MAPREDUCE-220, we now have CPU/Memory usage information in MapReduce JobHistory files. However, Rumen's TraceBuilder
does not emit this information in the JSON traces. Without this information, GridMix3 cannot emulate CPU/Memory usage correctly."
MAPREDUCE-2095,Gridmix unable to run for compressed traces(.gz format).,"I was trying to run gridmix with compressed trace file.However, it throws a JsonParseException and exit.

exception details:
==================
org.codehaus.jackson.JsonParseException: Illegal character ((CTRL-CHAR, code 31)): only regular white space (\r, \n,
\t) is allowed between tokens
 at [Source: org.apache.hadoop.fs.FSDataInputStream@17ba38f; line: 1, column: 2]
        at org.codehaus.jackson.impl.JsonParserBase._constructError(JsonParserBase.java:651)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:635)
        at org.codehaus.jackson.impl.JsonParserBase._throwInvalidSpace(JsonParserBase.java:596)
        at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:981)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:77)
        at org.codehaus.jackson.map.ObjectMapper._initForReading(ObjectMapper.java:688)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:624)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:275)
        at org.apache.hadoop.tools.rumen.JsonObjectMapperParser.getNext(JsonObjectMapperParser.java:84)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:117)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:29)
        at org.apache.hadoop.mapred.gridmix.JobFactory.getNextJobFiltered(JobFactory.java:174)
        at org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread.run(StressJobFactory.java:166)
10/09/23 09:43:17 ERROR gridmix.Gridmix: Error in trace
org.codehaus.jackson.JsonParseException: Illegal character ((CTRL-CHAR, code 31)): only regular white space (\r, \n,
\t) is allowed between tokens
 at [Source: org.apache.hadoop.fs.FSDataInputStream@17ba38f; line: 1, column: 2]
        at org.codehaus.jackson.impl.JsonParserBase._constructError(JsonParserBase.java:651)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:635)
        at org.codehaus.jackson.impl.JsonParserBase._throwInvalidSpace(JsonParserBase.java:596)
        at org.codehaus.jackson.impl.Utf8StreamParser._skipWSOrEnd(Utf8StreamParser.java:981)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:77)
        at org.codehaus.jackson.map.ObjectMapper._initForReading(ObjectMapper.java:688)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:624)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:275)
        at org.apache.hadoop.tools.rumen.JsonObjectMapperParser.getNext(JsonObjectMapperParser.java:84)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:117)
        at org.apache.hadoop.tools.rumen.ZombieJobProducer.getNextJob(ZombieJobProducer.java:29)
        at org.apache.hadoop.mapred.gridmix.JobFactory.getNextJobFiltered(JobFactory.java:174)
        at org.apache.hadoop.mapred.gridmix.StressJobFactory$StressReaderThread.run(StressJobFactory.java:166)
10/09/23 09:43:17 INFO gridmix.Gridmix: Exiting...
"
MAPREDUCE-2082,Race condition in writing the jobtoken password file when launching pipes jobs,"In Application.java, when jobtoken password file is written, there is a race condition because the file is written in job's work directory. The file should rather be written in the task's working directory."
MAPREDUCE-2081,[GridMix3] Implement functionality for get the list of job traces which has different intervals.,"Girdmix system tests should require different job traces with different time intervals for generate and submit the gridmix jobs. So, implement a functionaliy for getting the job traces and arrange them in hash table with time interval as key.Also getting the list of traces from resource location irrespective of time. The following methods needs to implement.

Method signature:
public static Map <String, String> getMRTraces(Configuration conf)  throws IOException; - it get the traces with time intervals from resources default location.

public static Map <String, String> getMRTraces(Configuration conf,Path path)  throws IOException; - it get the traces with time intervals from user specified resource location.


public static List<String> listMRTraces(Configuration conf) throws IOException  -it list all the traces from resource default location irrespective of time interval.

public static List<String> listMRTraces(Configuration conf, Path tracesPath) throws IOException - it list all the traces from user specified user location irrespective of  time interval.

public static List<String> listMRTracesByTime(Configuration conf, String timeInterval) throws IOException - it list all traces of a given time interval from default resource location.

public static List<String> listMRTracesByTime(Configuration conf, String timeInterval,Path path) throws IOException - it list all traces of a given time interval from a given resources location.
"
MAPREDUCE-2078,TraceBuilder unable to generate the traces while giving the job history path by globing.,"I was trying to generate the traces for MR job histories by using TraceBuilder. However, it's unable to generate the traces while giving the job history path by globing. It throws a file not found exception even though the job history path is exists.

I have provide the job history path in the below way.

hdfs://<<clustername>>/dir1/dir2/dir3/*/*/*/*/*/*/

Exception:

java.io.FileNotFoundException: File does not exist:
hdfs://<<clustername>>/dir1/dir2/dir3/*/*/*/*/*/*
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:525)
        at org.apache.hadoop.tools.rumen.TraceBuilder$MyOptions.<init>(TraceBuilder.java:88)
        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:183)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:121)

It's truncating the last  slash in the path.
"
MAPREDUCE-2077,Name clash in the deprecated o.a.h.util.MemoryCalculatorPlugin,"Name clash compile error in the deprecated org.apache.hadoop.util.MemoryCalculatorPlugin due to JLS3 8.4.8.3 (cf. http://bugs.sun.com/view_bug.do?bug_id=6182950)

The bug doesn't manifest in jdk 1.6 up to 20, but shows up in NetBeans 6.9+ due to its bundled (conforming) compiler. Fix is trivial: just remove the offending method in the deprecated subclass as its equivalent erasure is inherited from the parent class anyway."
MAPREDUCE-2044,[Gridmix] Document the usage of '-' as the input-trace in GridMix,GridMix users can pipeline the input trace to GridMix using the '-' option. This needs a documentation.
MAPREDUCE-2037,"Capturing interim progress times, CPU usage, and memory usage, when tasks reach certain progress thresholds","We would like to capture the following information at certain progress thresholds as a task runs:

   * Time taken so far
   * CPU load [either at the time the data are taken, or exponentially smoothed]
   * Memory load [also either at the time the data are taken, or exponentially smoothed]

This would be taken at intervals that depend on the task progress plateaus.  For example, reducers have three progress ranges -- [0-1/3], (1/3-2/3], and (2/3-3/3] -- where fundamentally different activities happen.  Mappers have different boundaries, I understand, that are not symmetrically placed.  Data capture boundaries should coincide with activity boundaries.  For the state information capture [CPU and memory] we should average over the covered interval.

This data would flow in with the heartbeats.  It would be placed in the job history as part of the task attempt completion event, so it could be processed by rumen or some similar tool and could drive a benchmark engine."
MAPREDUCE-2034,TestSubmitJob triggers NPE instead of permissions error,"TestSubmitJob.testSecureJobExecution catches _any_ IOException and assumes a permissions error has been caught. In fact, it was passing an invalid path name to the NameNode and triggering an NPE, not a Permission denied error, in one case, but the test was not specific enough to detect this."
MAPREDUCE-2031,TestTaskLauncher and TestTaskTrackerLocalization fail with NPE in trunk.,"TestTaskLauncher and TestTaskTrackerLocalization fail in trunk after the commit of MAPREDUCE-1881 with NPE:
{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.kill(TaskTracker.java:2978)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:2941)
        at org.apache.hadoop.mapred.TaskTracker.purgeJob(TaskTracker.java:1919)
        at org.apache.hadoop.mapred.TestTaskTrackerLocalization.verifyUserLogsRemoval(TestTaskTrackerLocalization.java:816)
        at org.apache.hadoop.mapred.TestTaskTrackerLocalization.testJobFilesRemoval(TestTaskTrackerLocalization.java:897)
{noformat}

{noformat}
java.lang.NullPointerException
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.kill(TaskTracker.java:2978)
        at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:2941)
        at org.apache.hadoop.mapred.TaskTracker.purgeTask(TaskTracker.java:1981)
        at org.apache.hadoop.mapred.TaskTracker.processKillTaskAction(TaskTracker.java:420)
        at org.apache.hadoop.mapred.TestTaskLauncher.testExternalKillForLaunchTask(TestTaskLauncher.java:95)
{noformat}

NPE happens because taskTracker.myInstrumentation is not initialized."
MAPREDUCE-1992,NPE in JobTracker's constructor,"On my local machine, JobTracker is *not* coming up with current trunk. Logs show the following NPE:

2010-08-03 14:01:41,449 FATAL org.apache.hadoop.mapred.JobTracker: java.lang.NullPointerException
  at org.apache.hadoop.security.UserGroupInformation.isLoginKeytabBased(UserGroupInformation.java:703)
  at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:1383)
  at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:275)
  at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:267)
  at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:262)
  at org.apache.hadoop.mapred.JobTracker.main(JobTracker.java:4236)

2010-08-03 14:01:41,449 INFO org.apache.hadoop.mapred.JobTracker: SHUTDOWN_MSG:"
MAPREDUCE-1982,[Rumen] TraceBuilder's output shows jobname as NULL for jobhistory files with valid jobnames,"{{TraceBuilder}} fails to extract configuration properties (like job-name) from the job-conf if the job-conf has the properties stored using the deprecated keys.
"
MAPREDUCE-1978,[Rumen] TraceBuilder should provide recursive input folder scanning,"Currently, {{TraceBuilder}} assumes that the input is either jobhistory files or a folders containing jobhistory files directly underneath the specified folder. There could be a use cases where the input folder could contain sub-folders containing jobhistory files. Rumen should support such input folders."
MAPREDUCE-1929,Allow artifacts to be published to the staging Apache Nexus Maven Repository,MapReduce companion issue to HADOOP-6847.
MAPREDUCE-1925,TestRumenJobTraces fails in trunk,"TestRumenJobTraces failed with following error:
Error Message

the gold file contains more text at line 1 expected:<56> but was:<0>

Stacktrace

	at org.apache.hadoop.tools.rumen.TestRumenJobTraces.testHadoop20JHParser(TestRumenJobTraces.java:294)

Full log of the failure is available at http://hudson.zones.apache.org/hudson/job/Mapreduce-Patch-h4.grid.sp2.yahoo.net/292/testReport/org.apache.hadoop.tools.rumen/TestRumenJobTraces/testHadoop20JHParser/"
MAPREDUCE-1863,[Rumen] Null failedMapAttemptCDFs in job traces generated by Rumen,All the traces generated by Rumen for jobs having failed task attempts has null value for failedMapAttemptCDFs.
MAPREDUCE-1813,NPE in PipeMapred.MRErrorThread,"Some reduce tasks fail with following NPE
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)
        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:540)
        at org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:137)
        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:474)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)
        at org.apache.hadoop.mapred.Child.main(Child.java:159)
Caused by: java.lang.NullPointerException
       at org.apache.hadoop.streaming.PipeMapRed$MRErrorThread.setStatus(PipeMapRed.java:517)
        at org.apache.hadoop.streaming.PipeMapRed$MRErrorThread.run(PipeMapRed.java:449)
"
MAPREDUCE-1779,Should we provide a way to know JobTracker's memory info from client?,"In HADOOP-4435, in branch 0.20, getClusterStatus() method returns JobTracker's used memory and total memory.
But these details are missed in new api (through MAPREDUCE-777).
If these details are needed only for web UI, I don't think they are needed for client.
So, should we provide a way to know JobTracker's memory info from client?
If yes, an api should be added in org.apache.hadoop.mapreduce.Cluster for the same."
MAPREDUCE-1740,NPE in getMatchingLevelForNodes when node locations are variable depth,"In getMatchingLevelForNodes, we assume that both nodes have the same ""depth"" (ie number of path components). If the user provides a topology script that assigns one node a path like /foo/bar/baz and another node a path like /foo/blah, this function will throw an NPE.

I'm not sure if there are other places where we assume that all node locations have a constant number of paths. If so we should check the output of the topology script aggressively to be sure this is the case. Otherwise I think we simply need to add && n2 != null to the while loop"
MAPREDUCE-1730,Automate test scenario for successful/killed jobs' memory is properly removed from jobtracker after these jobs retire.,"Automate using herriot framework,  test scenario for successful/killed jobs' memory is properly removed from jobtracker after these jobs retire.

This should test when successful and failed jobs are retired,  their jobInProgress object are removed properly.
"
MAPREDUCE-1711,Gridmix should provide an option to submit jobs to the same queues as specified in the trace.,Gridmix should provide an option to submit jobs to the same queues as specified in the trace.
MAPREDUCE-1710,Process tree clean up of exceeding memory limit tasks.,"1. Submit a job which would spawn child processes and each of the child processes exceeds the memory limits. Let the job complete . Check if all the child processes are killed, the overall job should fail.

2. Submit a job which would spawn child processes and each of the child processes exceeds the memory limits. Kill/fail the job while in progress. Check if all the child processes are killed."
MAPREDUCE-1707,TaskRunner can get NPE in getting ugi from TaskTracker,"The following code in TaskRunner can get NPE in the scenario described below.
{code}
      UserGroupInformation ugi = 
        tracker.getRunningJob(t.getJobID()).getUGI();
{code}

The scenario:
Tracker got a LaunchTaskAction; Task is localized and TaskRunner is started.
Then Tracker got a KillJobAction; This would issue a kill for the task. But, kill will be a no-op because the task did not actually start; The job is removed from runningJobs. 
Then if TaskRunner calls tracker.getRunningJob(t.getJobID()), it will be null.

Instead of TaskRunner doing a back call to tasktracker to get the ugi, tracker.getRunningJob(t.getJobID()).getUGI(), ugi should be passed a parameter in the constructor of TaskRunner. 
"
MAPREDUCE-1702,CPU/Memory emulation for GridMix3,"Currently GridMix3 can successfully recreate I/O workload of jobs from job traces. The goal of this feature is to emulate CPU and memory usage of jobs as well. For this we need to record cpu/memory usage of tasks on the cluster, save them to JobHistory so that they can be read by Rumen, and replay the cpu and memory usage in gridmix3 jobs."
MAPREDUCE-1621,Streaming's TextOutputReader.getLastOutput throws NPE if it has never read any output,"If TextOutputReader.readKeyValue() has never successfully read a line, then its bytes member will be left null. Thus when logging a task failure, PipeMapRed.getContext() can trigger an NPE when it calls outReader_.getLastOutput()."
MAPREDUCE-1587,NPE in JobClient querying an (empty) queue,"Getting a stack trace on a VM-hosted cluster with an empty queue. Thought maybe it was the -verbose option, but no, remove that and I still see it.
{code}
java org.apache.hadoop.mapred.JobQueueClient -list -showjobs -verbose
  [sshexec] 10/03/10 13:56:21 INFO security.Groups: Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping; cacheTimeout=300000
  [sshexec] 10/03/10 13:56:21 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
  [sshexec] Exception in thread ""main"" 
  [sshexec] java.lang.NullPointerException
  [sshexec] 	at org.apache.hadoop.mapred.JobClient.getJobQueueInfoArray(JobClient.java:921)
  [sshexec] 	at org.apache.hadoop.mapred.JobClient.getRootQueues(JobClient.java:937)
  [sshexec] 	at org.apache.hadoop.mapred.JobQueueClient.displayQueueList(JobQueueClient.java:142)
  [sshexec] 	at org.apache.hadoop.mapred.JobQueueClient.run(JobQueueClient.java:96)
  [sshexec] 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
  [sshexec] 	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
  [sshexec] 	at org.apache.hadoop.mapred.JobQueueClient.main(JobQueueClient.java:232)


{code}"
MAPREDUCE-1571,OutOfMemoryError during shuffle,"A OutOfMemoryError can occur when determining if the shuffle can be accomplished in memory

2010-03-06 07:54:49,621 INFO org.apache.hadoop.mapred.ReduceTask:
Shuffling 4191933 bytes (435311 raw bytes) into RAM from
attempt_201003060739_0002_m_000061_0
2010-03-06 07:54:50,222 INFO org.apache.hadoop.mapred.ReduceTask: Task
attempt_201003060739_0002_r_000000_0: Failed fetch #1 from
attempt_201003060739_0002_m_000202_0
2010-03-06 07:54:50,223 WARN org.apache.hadoop.mapred.ReduceTask:
attempt_201003060739_0002_r_000000_0 adding host
hd37.dfs.returnpath.net to penalty box, next contact in 4 seconds
2010-03-06 07:54:50,223 INFO org.apache.hadoop.mapred.ReduceTask:
attempt_201003060739_0002_r_000000_0: Got 1 map-outputs from previous
failures
2010-03-06 07:54:50,223 FATAL org.apache.hadoop.mapred.TaskRunner:
attempt_201003060739_0002_r_000000_0 : Map output copy failure :
java.lang.OutOfMemoryError: Java heap space
       at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.shuffleInMemory(ReduceTask.java:1508)
       at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getMapOutput(ReduceTask.java:1408)
       at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:1261)
       at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:1195)


Ted Yu identified the following potential solution:

I think there is mismatch (in ReduceTask.java) between:
     this.numCopiers = conf.getInt(""mapred.reduce.parallel.copies"", 5);
and:
       maxSingleShuffleLimit = (long)(maxSize *
MAX_SINGLE_SHUFFLE_SEGMENT_FRACTION);
where MAX_SINGLE_SHUFFLE_SEGMENT_FRACTION is 0.25f

because
     copiers = new ArrayList<MapOutputCopier>(numCopiers);
so the total memory allocated for in-mem shuffle is 1.25 * maxSize

A JIRA should be filed to correlate the constant 5 above and
MAX_SINGLE_SHUFFLE_SEGMENT_FRACTION."
MAPREDUCE-1561,"mapreduce patch tests hung with ""java.lang.OutOfMemoryError: Java heap space""","http://hudson.zones.apache.org/hudson/view/Mapreduce/job/Mapreduce-Patch-h9.grid.sp2.yahoo.net/4/console

Error form the console:

 [exec]     [junit] 10/03/05 04:08:29 INFO datanode.DataNode: PacketResponder 2 for block blk_-3280111748864197295_19758 terminating
     [exec]     [junit] 10/03/05 04:08:29 INFO hdfs.StateChange: BLOCK* NameSystem.addStoredBlock: blockMap updated: 127.0.0.1:46067 is added to blk_-3280111748864197295_19758{blockUCState=UNDER_CONSTRUCTION, primaryNodeIndex=-1, replicas=[ReplicaUnderConstruction[127.0.0.1:46067|RBW], ReplicaUnderConstruction[127.0.0.1:37626|RBW], ReplicaUnderConstruction[127.0.0.1:48886|RBW]]} size 0
     [exec]     [junit] 10/03/05 04:08:29 INFO hdfs.StateChange: DIR* NameSystem.completeFile: file /tmp/hadoop-hudson/mapred/system/job_20100304162726530_3751/job-info is closed by DFSClient_79157028
     [exec]     [junit] 10/03/05 04:08:29 INFO mapred.JobTracker: Job job_20100304162726530_3751 added successfully for user 'hudson' to queue 'default'
     [exec]     [junit] 10/03/05 04:08:29 INFO mapred.JobTracker: Initializing job_20100304162726530_3751
     [exec]     [junit] 10/03/05 04:08:29 INFO mapred.JobInProgress: Initializing job_20100304162726530_3751
     [exec]     [junit] 10/03/05 04:08:29 INFO mapreduce.Job: Running job: job_20100304162726530_3751
     [exec]     [junit] 10/03/05 04:08:29 INFO jobhistory.JobHistory: SetupWriter, creating file file:/grid/0/hudson/hudson-slave/workspace/Mapreduce-Patch-h9.grid.sp2.yahoo.net/trunk/build/contrib/raid/test/logs/history/job_20100304162726530_3751_hudson
     [exec]     [junit] 10/03/05 04:08:29 ERROR mapred.JobTracker: Job initialization failed:
     [exec]     [junit] org.apache.avro.AvroRuntimeException: java.lang.NoSuchFieldException: _SCHEMA
     [exec]     [junit] 	at org.apache.avro.specific.SpecificData.createSchema(SpecificData.java:50)
     [exec]     [junit] 	at org.apache.avro.reflect.ReflectData.getSchema(ReflectData.java:210)
     [exec]     [junit] 	at org.apache.avro.specific.SpecificDatumWriter.<init>(SpecificDatumWriter.java:28)
     [exec]     [junit] 	at org.apache.hadoop.mapreduce.jobhistory.EventWriter.<init>(EventWriter.java:47)
     [exec]     [junit] 	at org.apache.hadoop.mapreduce.jobhistory.JobHistory.setupEventWriter(JobHistory.java:252)
     [exec]     [junit] 	at org.apache.hadoop.mapred.JobInProgress.logSubmissionToJobHistory(JobInProgress.java:710)
     [exec]     [junit] 	at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:619)
     [exec]     [junit] 	at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3256)
     [exec]     [junit] 	at org.apache.hadoop.mapred.EagerTaskInitializationListener$InitJob.run(EagerTaskInitializationListener.java:79)
     [exec]     [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
     [exec]     [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
     [exec]     [junit] 	at java.lang.Thread.run(Thread.java:619)
     [exec]     [junit] Caused by: java.lang.NoSuchFieldException: _SCHEMA
     [exec]     [junit] 	at java.lang.Class.getDeclaredField(Class.java:1882)
     [exec]     [junit] 	at org.apache.avro.specific.SpecificData.createSchema(SpecificData.java:48)
     [exec]     [junit] 	... 11 more
     [exec]     [junit] 
     [exec]     [junit] Exception in thread ""pool-1-thread-3"" java.lang.OutOfMemoryError: Java heap space
     [exec]     [junit] 	at java.util.Arrays.copyOf(Arrays.java:2786)
     [exec]     [junit] 	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)
     [exec]     [junit] 	at java.io.PrintStream.write(PrintStream.java:430)
     [exec]     [junit] 	at org.apache.tools.ant.util.TeeOutputStream.write(TeeOutputStream.java:81)
     [exec]     [junit] 	at java.io.PrintStream.write(PrintStream.java:430)
     [exec]     [junit] 	at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:202)
     [exec]     [junit] 	at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:272)
     [exec]     [junit] 	at sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:276)
     [exec]     [junit] 	at sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:122)
     [exec]     [junit] 	at java.io.OutputStreamWriter.flush(OutputStreamWriter.java:212)
     [exec]     [junit] 	at org.apache.log4j.helpers.QuietWriter.flush(QuietWriter.java:58)
     [exec]     [junit] 	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:316)
     [exec]     [junit] 	at org.apache.log4j.WriterAppender.append(WriterAppender.java:160)
     [exec]     [junit] 	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
     [exec]     [junit] 	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)10/03/05 04:08:36 INFO raid.RaidNode: Triggering Policy Filter RaidTest1 hdfs://localhost:44624/user/test/raidtest
     [exec]     [junit] 10/03/05 04:08:39 INFO raid.RaidNode: Trigger thread continuing to run...
     [exec]     [junit] Exception in thread ""org.apache.hadoop.raid.RaidNode$TriggerMonitor@5ebac9"" 10/03/05 04:08:44 INFO security.Groups: Returning cached groups for 'hudso10/03/05 04:08:47 INFO ipc.Server: IPC Server handler 8 on 44624, call getException in thread ""IPC Server handler 8 on 44624"" java.lang.OutOfMemoryError: Java heap space10/03/05 04:08:53 INFO mapreduce.Job:  map 0% reduce 0%

"
MAPREDUCE-1546,Jobtracker JSP pages should automatically redirect to the corresponding history page if not in memory,"MAPREDUCE-1185 redirects jobdetails.jsp to it's corresponding history page.

For convenience, we should also redirect the following JSP pages to the corresponding history pages:
jobconf.jsp
jobtasks.jsp
taskdetails.jsp
taskstats.jsp
"
MAPREDUCE-1524,Support for CLOB and BLOB values larger than can fit in memory,"The patch in MAPREDUCE-1446 provides support for ""inline"" CLOB and BLOB values which can be fully materialized. Values which are too big for RAM should be written to separate files in HDFS and referenced in an indirect fashion; access should be provided through a stream."
MAPREDUCE-1523,Sometimes rumen trace generator fails to extract the job finish time.,We saw sometimes (not very often) that rumen may fail to extract the job finish time from Hadoop 0.20 history log.
MAPREDUCE-1508,NPE in TestMultipleLevelCaching on error cleanup path,TestMultipleLevelCaching dereferences objects in a finally block which may not have been initialized.
MAPREDUCE-1506,Assertion failure in TestTaskTrackerMemoryManager,"With asserts enabled, TestTaskTrackerMemoryManager sometimes fails. From what I've inspected, it's because some tasks are marked as FAILED/TIPFAILED while others are marked SUCCEEDED.

This can be reproduced by applying MAPREDUCE-1092 and then running {{ant clean test -Dtestcase=TestTaskTrackerMemoryManager}}"
MAPREDUCE-1504,SequenceFile.Reader constructor leaking resources,"When {{SequenceFile.Reader}} constructor throws an {{IOException}} (because the file does not conform to {{SequenceFile}} format), we will have such a problem.
The caller won't have a pointer to the reader because of the {{IOException}} thrown.

We should call {{in.close()}} inside the constructor to make sure that we don't leak resources (file descriptor and connection to the data node, etc).
"
MAPREDUCE-1499,JobTracker.finalizeJob inverts lock order and causes potential deadlock,"This issue was brought up by Matei in MAPREDUCE-1436 as a fairsched bug, but it turns out it's a JT bug even with the fifo scheduler in unpatched 0.20.2. JobTracker.finalizeJob locks JT.jobs, JT.taskScheduler, etc, having gotten the JIP log before the JT lock."
MAPREDUCE-1444,Sqoop ConnManager instances can leak Statement objects,"The ConnManager API returns ResultSets to users but does not provide a mechanism to clean up the underlying Statement that generated the ResultSet. Problematically, closing the Statement will invalidate the ResultSet, so these must be cleaned up in LIFO order, putting the onus on the receiver of the ResultSet."
MAPREDUCE-1443,DBInputFormat can leak connections,"The DBInputFormat creates a Connection to use when enumerating splits, but never closes it. This can leak connections to the database which are not cleaned up for a long time."
MAPREDUCE-1442,StackOverflowError when JobHistory parses a really long line,"JobHistory.parseLine() fails with StackOverflowError on a really big COUNTER value, triggered via the web interface. See attached file."
MAPREDUCE-1436,Deadlock in preemption code in fair scheduler,"In testing the fair scheduler with preemption, I found a deadlock between updatePreemptionVariables and some code in the JobTracker. This was found while testing a backport of the fair scheduler to Hadoop 0.20, but it looks like it could also happen in trunk and 0.21. Details are in a comment below."
MAPREDUCE-1425,archive throws OutOfMemoryError,"{noformat}
-bash-3.1$ hadoop  archive -archiveName t4.har -p . t4 .
Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space
        at java.util.regex.Pattern.compile(Pattern.java:1432)
        at java.util.regex.Pattern.<init>(Pattern.java:1133)
        at java.util.regex.Pattern.compile(Pattern.java:847)
        at java.lang.String.replace(String.java:2208)
        at org.apache.hadoop.fs.Path.normalizePath(Path.java:146)
        at org.apache.hadoop.fs.Path.initialize(Path.java:137)
        at org.apache.hadoop.fs.Path.<init>(Path.java:126)
        at org.apache.hadoop.fs.Path.makeQualified(Path.java:296)
        at org.apache.hadoop.hdfs.DistributedFileSystem.makeQualified(DistributedFileSystem.java:244)
        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:256)
        at org.apache.hadoop.tools.HadoopArchives.archive(HadoopArchives.java:393)
        at org.apache.hadoop.tools.HadoopArchives.run(HadoopArchives.java:736)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.HadoopArchives.main(HadoopArchives.java:751)
{noformat}"
MAPREDUCE-1396,Display more details about memory usage on jobtracker web UI,"HDFS-850 is introducing changes to the NameNode web UI to display additional details of memory information. I think it will be good to have similar information for the jobtracker as well, particularly for heavily used clusters that run the risk of the masters running out of memory."
MAPREDUCE-1374,Reduce memory footprint of FileSplit,"We can have many FileInput objects in the memory, depending on the number of mappers.

It will save tons of memory on JobTracker and JobClient if we intern those Strings for host names.

{code}
FileInputFormat.java:

      for (NodeInfo host: hostList) {
        // Strip out the port number from the host name
-        retVal[index++] = host.node.getName().split("":"")[0];
+        retVal[index++] = host.node.getName().split("":"")[0].intern();
        if (index == replicationFactor) {
          done = true;
          break;
        }
      }
{code}

More on String.intern(): http://www.javaworld.com/javaworld/javaqa/2003-12/01-qa-1212-intern.html


It will also save a lot of memory by changing the class of {{file}} from {{Path}} to {{String}}. {{Path}} contains a {{java.net.URI}} which internally contains ~10 String fields. This will also be a huge saving.

{code}
  private Path file;
{code}

"
MAPREDUCE-1342,Potential JT deadlock in faulty TT tracking,"JT$FaultyTrackersInfo.incrementFaults first locks potentiallyFaultyTrackers, and then calls blackListTracker, which calls removeHostCapacity, which locks JT.taskTrackers
On the other hand, JT.blacklistedTaskTrackers() locks taskTrackers, then calls faultyTrackers.isBlacklisted() which goes on to lock potentiallyFaultyTrackers.

I haven't produced such a deadlock, but the lock ordering here is inverted and therefore could deadlock.

Not sure if this goes back to 0.21 or just in trunk."
MAPREDUCE-1317,Reducing memory consumption of rumen objects,We have encountered OutOfMemoryErrors in mumak and gridmix when dealing with very large jobs. The purpose of this jira is to optimze memory consumption of rumen produced job objects.
MAPREDUCE-1313,NPE in FieldFormatter if escape character is set and field is null,Performing an import with the {{\-\-escaped-by}} character set on a table with a null field will cause a NullPointerException in FieldFormatter
MAPREDUCE-1309,"I want to change the rumen job trace generator to use a more modular internal structure, to allow for more input log formats ","There are two orthogonal questions to answer when processing a job tracker log: how will the logs and the xml configuration files be packaged, and in which release of hadoop map/reduce were the logs generated?  The existing rumen only has a couple of answers to this question.  The new engine will handle three answers to the version question: 0.18, 0.20 and current, and two answers to the packaging question: separate files with names derived from the job ID, and concatenated files with a header between sections [used for easier file interchange]."
MAPREDUCE-1295,We need a job trace manipulator to build gridmix runs.,"Rumen produces ""job traces"", which are JSON format files describing important aspects of all jobs that are run [successfully or not] on a hadoop map/reduce cluster.  There are two packages under development that will consume these trace files and produce actions in that cluster or another cluster: gridmix3 [see jira MAPREDUCE-1124 ] and Mumak [a simulator -- see MAPREDUCE-728 ].

It would be useful to be able to do two things with job traces, so we can run experiments using these two tools: change the duration, and change the density.  I would like to provide a ""folder"", a tool that can wrap a long-duration execution trace to redistribute its jobs over a shorter interval, and also change the density by duplicating or culling away jobs from the folded combined job trace."
MAPREDUCE-1248,Redundant memory copying in StreamKeyValUtil,"I found that when MROutputThread collecting the output of  Reducer, it calls StreamKeyValUtil.splitKeyVal() and two local byte-arrays are allocated there for each line of output. Later these two byte-arrays are passed to variable key and val. There are twice memory copying here, one is the System.arraycopy() method, the other is inside key.set() / val.set().

This causes double times of memory copying for the whole output (may lead to higher CPU consumption), and frequent temporay object allocation."
MAPREDUCE-1228,OutOfMemoryErrors in ReducerTask due to int overflow on >2G RAM tasks,"The ReduceTask RAMManager uses ints for tracking amounts of memory. For tasks with >2G RAM allocated, these can overflow and cause memory usage to become incorrectly tracked and run away."
MAPREDUCE-1221,Kill tasks on a node if the free physical memory on that machine falls below a configured threshold,"The TaskTracker currently supports killing tasks if the virtual memory of a task exceeds a set of configured thresholds. I would like to extend this feature to enable killing tasks if the physical memory used by that task exceeds a certain threshold.

On a certain operating system (guess?), if user space processes start using lots of memory, the machine hangs and dies quickly. This means that we would like to prevent map-reduce jobs from triggering this condition. From my understanding, the killing-based-on-virtual-memory-limits (HADOOP-5883) were designed to address this problem. This works well when most map-reduce jobs are Java jobs and have well-defined -Xmx parameters that specify the max virtual memory for each task. On the other hand, if each task forks off mappers/reducers written in other languages (python/php, etc), the total virtual memory usage of the process-subtree varies greatly. In these cases, it is better to use kill-tasks-using-physical-memory-limits."
MAPREDUCE-1218,Collecting cpu and memory usage for TaskTrackers,"The information can be used for resource aware scheduling.
Note that this is related to MAPREDUCE-220. There the per task resource information is collected.
This one collects the per machine information."
MAPREDUCE-1188,NPE in decommissioning blacklisted nodes,Decommissioning a blacklisted node results into a NPE.
MAPREDUCE-1182,Reducers fail with OutOfMemoryError while copying Map outputs,"Reducers fail while copying Map outputs with following exception

java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.shuffleInMemory(ReduceTask.java:1539) at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.getMapOutput(ReduceTask.java:1432) at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:1285) at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:1216) ,Error:

Reducer's memory usage keeps on increasing and ultimately exceeds -Xmx value  
I even tried with -Xmx6.5g to each reducer but it's still failing 

While looking into the reducer logs, I found that reducers were doing shuffleInMemory every time, rather than doing shuffleOnDisk"
MAPREDUCE-1181,Enforce RSS memory limit in TaskMemoryManagerThread,"TaskMemoryManagerThread will periodically check the rss memory usage of every task. If the memory usage exceeds the specified threshold, the task will be killed. Also if the total rss memory of all tasks exceeds (total amount of memory - specified reserved memory). The task with least progress will be killed to recover the reserved rss memory.

This is similar to the virtual memory limit provided by TaskMemoryManagerThread. But now the limit is for rss memory. This new feature allow us to avoid page swapping which is prone to error.

The following are the related configurations
mapreduce.reduce.memory.rss.mb   // RSS memory allowed for a reduce task
mapreduce.map.memory.rss.mb       // RSS memory allowed for a map task
mapreduce.tasktracker.reserved.memory.rss.mb     // RSS memory reserved (not for tasks) on a tasktracker"
MAPREDUCE-1177,TestTaskTrackerMemoryManager retries a task for more than 100 times.,"TestTaskTrackerMemoryManager retries a task for more than 100 times.
The logs showing the same:
{noformat}
2009-11-02 12:41:20,489 INFO  mapred.JobInProgress (JobInProgress.java:completedTask(2530)) - Task 'attempt_20091102123356106_0001_m_000002_145' has completed task_20091102123356106_0001_m_000002 successfully.
{noformat}

Sometimes the test timesout also.
"
MAPREDUCE-1167,Make ProcfsBasedProcessTree collect rss memory information,"Right now ProcfsBasedProcess collects only virtual memory. We can make it collect rss memory as well.
Later we can use rss in TaskMemoryManagerThread to obtain better memory management."
MAPREDUCE-1104,RecoveryManager not initialized in SimulatorJobTracker led to NPE in JT Jetty server,RecoveryManager initialization is not copied to the JobTracker constructor Mumak depends on. This leads to NPE in JT Jetty server.
MAPREDUCE-1090,Modify log statement in Tasktracker log related to memory monitoring to include attempt id.,"Currently the TaskMemoryManagerThread logs a line like:
org.apache.hadoop.mapred.TaskMemoryManagerThread: Memory usage of ProcessTree 14321 :372686848bytes. Limit : 2147483648bytes. 
It would be very useful to include the Task attempt id for the process tree mentioned in the log statement."
MAPREDUCE-1089,Fair Scheduler preemption triggers NPE when tasks are scheduled but not running,"We see exceptions like this when preemption runs when a task has been scheduled on a TT but has not yet started running.

2009-10-09 14:30:53,989 INFO org.apache.hadoop.mapred.FairScheduler: Should preempt 2 MAP tasks for job_200910091420_0006: tasksDueToMinShare = 2, tasksDueToFairShare = 0
2009-10-09 14:30:54,036 ERROR org.apache.hadoop.mapred.FairScheduler: Exception in fair scheduler UpdateThread
java.lang.NullPointerException
        at org.apache.hadoop.mapred.FairScheduler$2.compare(FairScheduler.java:1015)
        at org.apache.hadoop.mapred.FairScheduler$2.compare(FairScheduler.java:1013)
        at java.util.Arrays.mergeSort(Arrays.java:1270)
        at java.util.Arrays.sort(Arrays.java:1210)
        at java.util.Collections.sort(Collections.java:159)
        at org.apache.hadoop.mapred.FairScheduler.preemptTasks(FairScheduler.java:1013)
        at org.apache.hadoop.mapred.FairScheduler.preemptTasksIfNecessary(FairScheduler.java:911)
        at org.apache.hadoop.mapred.FairScheduler$UpdateThread.run(FairScheduler.java:286)
"
MAPREDUCE-1075,getQueue(String queue) in JobTracker would return NPE for invalid queue name,
MAPREDUCE-1070,Deadlock in FairSchedulerServlet,FairSchedulerServlet can cause a deadlock with the JobTracker
MAPREDUCE-1042,rumen should be able to output compressed trace files,"rumen is used primarily to create job trace files which are then processed by other tools.

These trace files can exceed 100 gigabytes.  However, gzip compression normally achieves 15:1 compression on these traces.

I would like to modify rumen so it can output compressed files directly, rather than outputting unwieldy uncompressed files and letting me compress it later."
MAPREDUCE-1028,"Cleanup tasks are scheduled using high memory configuration, leaving tasks in unassigned state.","A cleanup task is launched for a failed task of a job. This task is created based on the TIP of the failed task, and so is marked as requiring as many slots to run as the original task itself. For instance, if a high RAM job requires 2 slots per task, a cleanup task of the high RAM jobs requires 2 slots as well.

Further, a cleanup task is scheduled to a tasktracker by the jobtracker itself and not the scheduler. While doing so, the JT doesn't check if the TT has enough slots free to run a high RAM cleanup task - always assuming 1 slot is enough. Thus, a task is oversubscribed to the TT.

However, on the TT, before launch, we check that the task can actually run, and wait for so many slots to become available. If the slots don't get freed quickly, we will have tasks stuck in an unassigned state."
MAPREDUCE-1018,Document changes to the memory management and scheduling model,"There were changes done for the configuration, monitoring and scheduling of high ram jobs. This must be documented in the mapred-defaults.xml and also on forrest documentation"
MAPREDUCE-979,JobConf.getMemoryFor{Map|Reduce}Task doesn't fallback to newer config knobs when mapred.taskmaxvmem is set to DISABLED_MEMORY_LIMIT of -1,"JobConf.getMemoryFor{Map|Reduce}Task doesn't fallback to newer config knobs when mapred.taskmaxvmem is set to DISABLED_MEMORY_LIMIT of -1, this results in failed job-submissions when mapred-default.xml has the default value of -1."
MAPREDUCE-968,NPE in distcp encountered when placing _logs directory on S3FileSystem,"If distcp is pointed to an empty S3 bucket as the destination for an s3:// filesystem transfer, it will fail with the following exception

Copy failed: java.lang.NullPointerException
at org.apache.hadoop.fs.s3.S3FileSystem.makeAbsolute(S3FileSystem.java:121)
at org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:332)
at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:633)
at org.apache.hadoop.tools.DistCp.setup(DistCp.java:1005)
at org.apache.hadoop.tools.DistCp.copy(DistCp.java:650)
at org.apache.hadoop.tools.DistCp.run(DistCp.java:857)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
at org.apache.hadoop.tools.DistCp.main(DistCp.java:884) "
MAPREDUCE-962,NPE in ProcfsBasedProcessTree.destroy(),"This causes the following exception in TaskMemoryManagerThread. I observed this while running TestTaskTrackerMemoryManager.
{code}
2009-09-02 12:08:25,835 WARN  mapred.TaskMemoryManagerThread (TaskMemoryManagerThread.java:run(239)) - \
            Uncaught exception in TaskMemoryManager while managing memory of attempt_20090902120812252_0001_m_000003_0 : \
java.lang.NullPointerException
        at org.apache.hadoop.util.ProcfsBasedProcessTree.assertPidPgrpidForMatch(ProcfsBasedProcessTree.java:234)
        at org.apache.hadoop.util.ProcfsBasedProcessTree.assertAndDestroyProcessGroup(ProcfsBasedProcessTree.java:257)
        at org.apache.hadoop.util.ProcfsBasedProcessTree.destroy(ProcfsBasedProcessTree.java:286)
        at org.apache.hadoop.mapred.TaskMemoryManagerThread.run(TaskMemoryManagerThread.java:229)
{code}"
MAPREDUCE-944,Extend FairShare scheduler to fair-share memory usage in the cluster,"The FairShare Scheduler has an extensible LoadManager API to regulate allocating new tasks on a particular TaskTracker. In similar lines, it would be nice if the FairShare Scheduler can have a pluggable policy to regulate new tasks from a particular job. This will allow one to skip scheduling tasks of a job that  is eating a large percentage of memory in the cluster, i.e. fair-share of memory resources among jobs. 
"
MAPREDUCE-921,Map-Reduce framework should gracefully handle heterogenous clusters,"Currently several parts of the framework: components, configuration etc. implicitly assume uniformity of the cluster. 

This jira is meant to be a meta-issue to track various improvements necessary to handle heterogenous clusters."
MAPREDUCE-918,Test hsqldb server should be memory-only.,"Sqoop launches a standalone hsqldb server for unit tests, but it currently writes its database to disk and uses a connect string of {{//localhost}}. If multiple test instances are running concurrently, one test server may serve to the other instance of the unit tests, causing race conditions."
MAPREDUCE-913,"TaskRunner crashes with NPE resulting in held up slots, UNINITIALIZED tasks and hung TaskTracker",
MAPREDUCE-891,Streaming tests fail with NPE in MiniDFSCluster,"Streaming testcases' usage of MiniDFSCluster.startDatanodes causes NPE in GenericOptionsParser:

{noformat}
java.lang.NullPointerException
	at org.apache.commons.cli.GnuParser.flatten(GnuParser.java:110)
	at org.apache.commons.cli.Parser.parse(Parser.java:143)
	at org.apache.hadoop.util.GenericOptionsParser.parseGeneralOptions(GenericOptionsParser.java:374)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:153)
	at org.apache.hadoop.util.GenericOptionsParser.<init>(GenericOptionsParser.java:138)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1314)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:414)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:278)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:119)
	at org.apache.hadoop.streaming.TestDumpTypedBytes.testDumping(TestDumpTypedBytes.java:40)
{noformat}
"
MAPREDUCE-866,Move all memory related parameters and their initialization out of TaskTracker.java into TaskMemoryManagerThread,"Design-wise, they belong to TaskMemoryManager. TaskTracker can use method calls to initialize/set/get the parameters."
MAPREDUCE-864,Enhance JobClient API implementations to look at history files to get information about jobs that are not in memory,"MAPREDUCE-817 added an API to get the JobHistory URL from the JobTracker. This is useful in two ways:
1) Users can use this API to get the URL, copy the history files to their local disk, and, do processing on them
2) APIs like JobSubmissionProtocol.getJobCounters, can read a part of the history file, and then return the information to the caller (if the job is not there in JT memory). This would  mimic most of the CompletedJobsStatusStore functionality."
MAPREDUCE-858,"NPE in heartbeat if ""mapred.job.tracker.history.completed.location"" is not writable ","If ""mapred.job.tracker.history.completed.location"" has been configured to write to a location which is not writable by JT, NullPointerException is thrown in TT heartbeat. Below is the Exception obtained:
{noformat}
2009-08-13 07:56:02,815 INFO org.apache.hadoop.ipc.Server: IPC Server handler <handler> on <port>, call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@1e7a6ae, false, false, true, 1775) from <ip>:<port>: error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobHistory$JobHistoryFilesManager.moveToDone(JobHistory.java:215)
        at org.apache.hadoop.mapred.JobHistory$JobInfo.markCompleted(JobHistory.java:1071)
        at org.apache.hadoop.mapred.JobTracker.finalizeJob(JobTracker.java:2413)
        at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:2729)
        at org.apache.hadoop.mapred.JobInProgress.jobComplete(JobInProgress.java:2327)
        at org.apache.hadoop.mapred.JobInProgress.completedTask(JobInProgress.java:2259)
        at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:957)
        at org.apache.hadoop.mapred.JobTracker.updateTaskStatuses(JobTracker.java:3946)
        at org.apache.hadoop.mapred.JobTracker.processHeartbeat(JobTracker.java:3123)
        at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2861)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
{noformat}

Instead of an NPE, it would be helpful if an useful error message is logged."
MAPREDUCE-857,task fails with NPE  when GzipCodec is used for mapred.map.output.compression.codec and native libary is not present,"Ran a job with mapred.map.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec.
Whenmaps of job completes they with following NPE  -:
tasklog -:
2009-08-12 13:48:13,423 INFO org.apache.hadoop.mapred.MapTask: io.sort.mb = 256
2009-08-12 13:48:13,611 INFO org.apache.hadoop.mapred.MapTask: data buffer = 204010944/214748368
2009-08-12 13:48:13,611 INFO org.apache.hadoop.mapred.MapTask: record buffer = 3187670/3355443
2009-08-12 13:49:45,473 INFO org.apache.hadoop.mapred.MapTask: Starting flush of map output
2009-08-12 13:49:45,544 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2009-08-12 13:49:45,545 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor
2009-08-12 13:49:45,546 WARN org.apache.hadoop.mapred.Child: Error running child : java.lang.NullPointerException
        at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:105)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1248)
        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1146)
        at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:528)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:604)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:318)
        at org.apache.hadoop.mapred.Child.main(Child.java:162)

Line 105 of IFile.java contains followings line in trunk code on which error was seen -:
Line 104: this.compressor = CodecPool.getCompressor(codec);
Line: this.compressor.reset();


If native is available job runs successfully without any failures 
"
MAPREDUCE-841,Protect Job Tracker against memory exhaustion due to very large InputSplit or JobConf objects,JobTracker only needs to examine a subset of information contained by InputSplit or JobConf objects. But currently JobTracker loads the complete user-defined InputSplit and JobConf objects in memory. This design would leave JobTracker susceptible to memory exhaustion particularly in cases when some bugs in user code which could result in very large input splits or job conf objects (e.g. PIG-901).
MAPREDUCE-834,When TaskTracker config use old memory management values its memory monitoring is diabled.,"TaskTracker memory config values -:
mapred.tasktracker.vmem.reserved=8589934592
mapred.task.default.maxvmem=2147483648
mapred.task.limit.maxvmem=4294967296
mapred.tasktracker.pmem.reserved=2147483648
TaskTracker start as -:
               2009-08-05 12:39:03,308 WARN org.apache.hadoop.mapred.TaskTracker: The variable mapred.tasktracker.vmem.reserved is no longer used
		2009-08-05 12:39:03,308 WARN org.apache.hadoop.mapred.TaskTracker: The variable mapred.tasktracker.pmem.reserved is no longer used
		2009-08-05 12:39:03,308 WARN org.apache.hadoop.mapred.TaskTracker: The variable mapred.task.default.maxvmem is no longer used
		2009-08-05 12:39:03,308 WARN org.apache.hadoop.mapred.TaskTracker: The variable mapred.task.limit.maxvmem is no longer used
		2009-08-05 12:39:03,308 INFO org.apache.hadoop.mapred.TaskTracker: Starting thread: Map-events fetcher for all reduce tasks on <tracker_name>
		2009-08-05 12:39:03,309 INFO org.apache.hadoop.mapred.TaskTracker:  Using MemoryCalculatorPlugin : org.apache.hadoop.util.LinuxMemoryCalculatorPlugin@19be4777
		2009-08-05 12:39:03,311 WARN org.apache.hadoop.mapred.TaskTracker: TaskTracker's totalMemoryAllottedForTasks is -1. TaskMemoryManager is disabled.

"
MAPREDUCE-833,Jobclient does not print any warning message when old memory config variable used with -D option from command line,
MAPREDUCE-821,JobClient.runJob leaks file descriptors,"In a Java-based driver that runs multiple MapReduce jobs (e.g. Mahout's K-means implementation), numerous calls to JobClient.runJob will cause many RPC connections to be opened and then never closed. This results in the driver job leaking file descriptors and will eventually crash once the OS limit is reached for Too Many Open Files.

This has been verified in Hadoop 18.3 by running the driver and as new MapReduce jobs are run, lsof -p dhows an increasing number of open TCP connections to the cluster.

Looking at the current code in the trunk, it looks like this is caused by runJob not calling close() on the JobClient object it creates. Or alternatively, it's cause by the fact that JobClient does not have a destructor that calls close().

I am going to verify this hypothesis and post a patch."
MAPREDUCE-820,NPE in TT heartbeat when there is a problem resolving the network topology,"When there is a problem while resolving the network topology (such as a non existent topology.script.file.name), NPE is being thrown in the TT heartbeats. Below is the exception obtained:

{noformat}
error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobTracker.resolveAndAddToTopology(JobTracker.java:2663)
        at org.apache.hadoop.mapred.JobTracker.addNewTracker(JobTracker.java:2645)
        at org.apache.hadoop.mapred.JobTracker.processHeartbeat(JobTracker.java:3093)
        at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2836)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
{noformat}"
MAPREDUCE-805,Deadlock in Jobtracker,"We are running a hadoop cluster (version 0.20.0) and have detected the following deadlock on our jobtracker:
{code}
""IPC Server handler 51 on 9001"":
	at org.apache.hadoop.mapred.JobInProgress.getCounters(JobInProgress.java:943)
	- waiting to lock <0x00007f2b6fb46130> (a org.apache.hadoop.mapred.JobInProgress)
	at org.apache.hadoop.mapred.JobTracker.getJobCounters(JobTracker.java:3102)
	- locked <0x00007f2b5f026000> (a org.apache.hadoop.mapred.JobTracker)
	at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:959)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:955)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:953)
 ""pool-1-thread-2"":
	at org.apache.hadoop.mapred.JobTracker.finalizeJob(JobTracker.java:2017)
	- waiting to lock <0x00007f2b5f026000> (a org.apache.hadoop.mapred.JobTracker)
	at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:2483)
	- locked <0x00007f2b6fb46130> (a org.apache.hadoop.mapred.JobInProgress)
	at org.apache.hadoop.mapred.JobInProgress.terminateJob(JobInProgress.java:2152)
	- locked <0x00007f2b6fb46130> (a org.apache.hadoop.mapred.JobInProgress)
	at org.apache.hadoop.mapred.JobInProgress.terminate(JobInProgress.java:2169)
	- locked <0x00007f2b6fb46130> (a org.apache.hadoop.mapred.JobInProgress)
	at org.apache.hadoop.mapred.JobInProgress.fail(JobInProgress.java:2245)
	- locked <0x00007f2b6fb46130> (a org.apache.hadoop.mapred.JobInProgress)
	at org.apache.hadoop.mapred.EagerTaskInitializationListener$InitJob.run(EagerTaskInitializationListener.java:86)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{code}"
MAPREDUCE-776,Gridmix: Trace-based benchmark for Map/Reduce,"Previous benchmarks ( HADOOP-2369 , HADOOP-3770 ), while informed by production jobs, were principally load generating tools used to validate stability and performance under saturation. The important dimensions of that load- submission order/rate, I/O profile, CPU usage, etc- only accidentally match that of the real load on the cluster. Given related work that characterizes production load ( MAPREDUCE-751 ), it would be worthwhile to use mined data to impose a corresponding load for tuning and guiding development of the framework.

The first version will focus on modeling task I/O, submission, and memory usage."
MAPREDUCE-772,Chaging LineRecordReader algo so that it does not need to skip backwards in the stream,"The current algorithm of the LineRecordReader needs to move backwards in the stream (in its constructor) to correctly position itself in the stream.  So it moves back one byte from the start of its split and try to read a record (i.e. a line) and throws that away.  This is so because it is sure that, this line would be taken care of by some other mapper.  This algorithm is difficult and in-efficient if used for compressed stream where data is coming to the LineRecordReader via some codecs. (Although in the current implementation, Hadoop does not split a compressed file and only makes one split from the start to the end of the file and so only one mapper handles it.  We are currently working on BZip2 codecs where splitting is possible to work with Hadoop.  So this proposed change will make it possible to uniformly handle plain as well as compressed stream.)

In the new algorithm, each mapper always skips its first line because it is sure that, that line would have been read by some other mapper.  So now each mapper must finish its reading at a record boundary which is always beyond its upper split limit.  Due to this change, LineRecordReader does not need to move backwards in the stream.
"
MAPREDUCE-754,NPE in expiry thread when a TT is lost,"NullPointerException is obtained in Tracker Expiry Thread. Below is the exception obtained in the JT logs 
{noformat}
ERROR org.apache.hadoop.mapred.JobTracker: Tracker Expiry Thread got exception: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobTracker.updateTaskTrackerStatus(JobTracker.java:2971)
        at org.apache.hadoop.mapred.JobTracker.access$300(JobTracker.java:104)
        at org.apache.hadoop.mapred.JobTracker$ExpireTrackers.run(JobTracker.java:381)
        at java.lang.Thread.run(Thread.java:619)
{noformat}
The steps to reproduce this issue are:
* Blacklist a TT. 
* Restart it. 
* The above exception is obtained when the first instance of TT is marked as lost.

However the above exception does not break any functionality.


"
MAPREDUCE-746,"When a  task tracker is killed, there is a Null Pointer exception thrown.","When a task tracker is killed, the job completes. But tehre is a null pointer exception thrown:

java.io.IOException: java.lang.NullPointerException
	at org.apache.hadoop.mapred.JobTracker$FaultyTrackersInfo.removeHostCapacity(JobTracker.java:759)
	at org.apache.hadoop.mapred.JobTracker$FaultyTrackersInfo.blackListTracker(JobTracker.java:624)
	at org.apache.hadoop.mapred.JobTracker$FaultyTrackersInfo.incrementFaults(JobTracker.java:601)
	at org.apache.hadoop.mapred.JobTracker.finalizeJob(JobTracker.java:2337)
	at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:2998)
	at org.apache.hadoop.mapred.JobInProgress.jobComplete(JobInProgress.java:2584)
	at org.apache.hadoop.mapred.JobInProgress.completedTask(JobInProgress.java:2473)
	at org.apache.hadoop.mapred.JobInProgress.updateTaskStatus(JobInProgress.java:1047)
	at org.apache.hadoop.mapred.JobTracker.updateTaskStatuses(JobTracker.java:3867)
	at org.apache.hadoop.mapred.JobTracker.processHeartbeat(JobTracker.java:3079)
	at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2817)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:964)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:960)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:958)

Steps to reproduce the issue:
1) Bring up a 5 node cluster.
2) set mapred.max.tracker.failures to 1
3) Run a sleep command with 5 maps and 5 redcues.
4) Kill a task tracker, when map is 80% complete.
5) Kill the task tracker, by using ""kill -9 process_id"". 
6) At the time of killing, it was running 2 maps and 2 reduces.
7) Allow 12 minutes to elapse for that task tracker to go out of job nodes list
8) Then job completes successfully by giving the task attempts of the lost trackers to other nodes.
9) The job tracker logs has this exception.





"
MAPREDUCE-702,eclipse-plugin jar target fails during packaging,
MAPREDUCE-685,Sqoop will fail with OutOfMemory on large tables using mysql,"The default MySQL JDBC client behavior is to buffer the entire ResultSet in the client before allowing the user to use the ResultSet object. On large SELECTs, this can cause OutOfMemory exceptions, even when the client intends to close the ResultSet after reading only a few rows. The MySQL ConnManager should configure its connection to use row-at-a-time delivery of results to the client."
MAPREDUCE-660,MRBench throws NPE,"On running ""hadoop  org.apache.hadoop.mapred.MRBench"" the following exception is obtained:
{noformat}
Exception in thread ""main"" java.lang.NullPointerException
        at java.util.Hashtable.put(Hashtable.java:394)
        at java.util.Properties.setProperty(Properties.java:143)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:403)
        at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:208)
        at org.apache.hadoop.mapred.MRBench.runJobInSequence(MRBench.java:177)
        at org.apache.hadoop.mapred.MRBench.main(MRBench.java:280)
{noformat}
"
MAPREDUCE-658,NPE in distcp if source path does not exist,distcp throws NullPointerException if the source path does not exist. It should emit a proper exception with meaningful error message.
MAPREDUCE-593,org.apache.hadoop.streaming.TestUlimit fails on JRockit 64-bit; not enough memory,"the testUlimit test sets a memory limit that is too small for Java to start. So it fails with a -1 response instead, which breaks the test. "
MAPREDUCE-587,Stream test TestStreamingExitStatus fails with Out of Memory,contrib/streaming tests are failing a test with an Out of Memory error on an OS/X Mac -same problem does not surface on Linux.
MAPREDUCE-586,Streaming reducers throw OutOfMemory for not so large inputs,"I am seeing OutOfMemoryError for moderate size inputs (~70 text files, 20k each ) causing job to fail in streaming. For very small inputs it still succeeds. Looking into details. 
"
MAPREDUCE-583,get rid of excessive flushes from PipeMapper/Reducer,"there's a flush on the buffered output streams in mapper/reducer for every row of data.

      // 2/4 Hadoop to Tool                                                                                                                   
      if (numExceptions_ == 0) {
        if (!this.ignoreKey) {
          write(key);
          clientOut_.write('\t');
        }
        write(value);
        if(!this.skipNewline) {
            clientOut_.write('\n');
        }
        clientOut_.flush();
      } else {
        numRecSkipped_++;
      }

tried to measure impact of removing this. number of context switches reported by vmstat shows marked decline. 

with flush (10 second intervals):
 r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa
 4  2    784  23140  83352 3114648    0    0  4819 32397 1175 13220 59 11 13 17
 1  2    784 129724  80704 3075696    0    0  4614 27196 1156 14797 49 11 19 21
 4  0    784  24160  83440 3174880    0    0    96 36070 1337 10976 67 11  9 12
 5  0    784 155872  84400 3158840    0    0   125 44084 1280 11044 68 14 10  8
 2  1    784 365128  87048 2892032    0    0   119 38472 1317 11610 69 14 10  7

without flush:
 5  0    784  24652  56056 3217864    0    0   310 29499 1379  7603 76  9  7  8
 5  3    784 118456  54568 3209992    0    0  3249 33426 1173  6828 63 11 12 14
 0  2    784 227628  54820 3198560    0    0  7840 30063 1146  8899 60 10 15 15
 3  1    784  25608  55048 3313512    0    0  3251 36276 1194  7915 60 10 15 15
 1  2    784 197324  49968 3194572    0    0  4714 35479 1281  8204 62 13 12 13

cs goes down by about 20-30%. but having trouble measuring overall speed improvement (too many variables due to spec. execution etc. - need better benchmark).

can't hurt.
"
MAPREDUCE-528,NPE in jobqueue_details.jsp page if scheduler has not started,NullPointerException is observed in jobqueue_details.jsp page if the scheduler has not yet started
MAPREDUCE-502,Allow jobtracker to be configured with zero completed jobs in memory,There is no way to specify that the jobtracker should not keep any completed job in memory.
MAPREDUCE-488,JobTracker webui should report heap memory used,As of today JobTracker's webui reports _total-available-heap-memory_ and _max-heap-memory_. I think it will be useful to show the _actual_ heap memory used i.e {{total - free}}. 
MAPREDUCE-479,Add reduce ID to shuffle clienttrace,Current clienttrace messages from shuffles note only the destination map ID but not the source reduce ID. Having both source and destination ID of each shuffle enables full tracing of execution. 
MAPREDUCE-451,TaskTracker's Memory resource should be considered when tasktracker asks for new task,"Currently, taskTracker only considers enough free disk space left when it asks for new task, memory resource should be considered too, or it may works badly in SMP environment.
"
MAPREDUCE-449,There is little information provided when the TaskTracker kills a Task that has not reported within the timeout (600 sec) interval - this patch provides a stack trace of the task ,"When we have a task that is killed for not reporting, sometimes there is an obvious programming error, and sometimes the reason the job didn't report is unclear.
This patch will cause the TaskTracker to try to generate a stack trace of the offending task before the task is killed.
Given how opaque process control is in java, a program is run to generate the stack trace, using the PID extracted from the undocumented UNIXProcess class

The attached patch is against 0.16.0, as that is the release we use.
This will only work on Unix machines -- or JVM's what use the java.lang.UNIXProcess implementation for the java Process object.
The script that generates the stack trace is very linux specific.
The code changes will run on jvm's where the UNIXProcess class is not available, without failure, but no stack trace will be generated.
"
MAPREDUCE-436,NPE in TaskRunner.run if hadoop.log.dir not set,"I'm getting an NPE in TaskRunner.run, looks like it happens when the system property hadoop.log.dir is unset"
MAPREDUCE-430,Task stuck in cleanup with OutOfMemoryErrors,"Obesrved a task with OutOfMemory error, stuck in cleanup.

"
MAPREDUCE-426,Race condition in LaunchTaskAction and KillJobAction,"One task wasn't killed when its job was killed. 
On the TaskTracker log, it showed, 

2007-08-21 17:02:29,219 INFO org.apache.hadoop.mapred.TaskTracker: LaunchTaskAction: task_0133_r_000080_2    <**************
2007-08-21 17:02:29,232 INFO org.apache.hadoop.mapred.TaskTracker: Received 'KillJobAction' for job: job_0131         <**************
2007-08-21 17:02:29,233 INFO org.apache.hadoop.mapred.TaskRunner: task_0131_m_000077_0 done; removing files.
2007-08-21 17:02:29,376 INFO org.apache.hadoop.mapred.TaskTracker: Received 'KillJobAction' for job: job_0133
2007-08-21 17:02:29,376 INFO org.apache.hadoop.mapred.TaskRunner: task_0133_r_000060_0 done; removing files.
2007-08-21 17:02:29,378 INFO org.apache.hadoop.mapred.TaskRunner: task_0133_r_000071_2 done; removing files.
2007-08-21 17:02:29,381 INFO org.apache.hadoop.mapred.TaskRunner: task_0133_r_000066_1 done; removing files.
2007-08-21 17:02:31,272 INFO org.apache.hadoop.mapred.TaskTracker: task_0133_r_000080_2 0.0% reduce > copy >
2007-08-21 17:02:32,275 INFO org.apache.hadoop.mapred.TaskTracker: task_0133_r_000080_2 0.0% reduce > copy >
2007-08-21 17:02:33,277 INFO org.apache.hadoop.mapred.TaskTracker: task_0133_r_000080_2 0.0% reduce > copy >
...
[task_0133_r_000080_2 continue to run]



Of course the JobTracker kept on complaining
2007-08-22 19:06:37,880 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid task_0133_r_000080_2
2007-08-22 19:06:38,124 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid task_0133_r_000080_2
2007-08-22 19:06:47,885 INFO org.apache.hadoop.mapred.JobTracker: Serious problem.  While updating status, cannot find taskid task_0133_r_000080_2
"
MAPREDUCE-425,NPE in TaskInProgress.cleanup,"This may be something that only my code triggers; an NPE in TaskTracker$TaskInProgress.cleanup
{code}
[sf-startdaemon-debug] 09/01/28 11:41:06 [TaskLauncher for task] INFO mapred.TaskTracker : Error cleaning up task runner: java.lang.NullPointerException
[sf-startdaemon-debug] 	at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.cleanup(TaskTracker.java:2487)
[sf-startdaemon-debug] 	at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:1825)
[sf-startdaemon-debug] 	at org.apache.hadoop.mapred.TaskTracker.access$1100(TaskTracker.java:104)
[sf-startdaemon-debug] 	at org.apache.hadoop.mapred.TaskTracker$TaskLauncher.run(TaskTracker.java:1779)
{code}

Looking at the code, the only source of NPE's on that line is localJobConf
{code}
  if (localJobConf.getNumTasksToExecutePerJvm() == 1) {
{code}

It looks like if TaskInProgress.cleanup() ever gets called with no valid localJobConf, then an NPE is the result. The exception gets logged and discarded, but it does appear in the logs."
MAPREDUCE-404,NPE in text.encode when writing an invalid(?) JobProfile,"I see an NPE in one of my tests in Text.encode(String), further up the stack is  JobProfile.write(), which appears to write a null user"
MAPREDUCE-399,Duplicate destroy of process trees in TaskMemoryManager.,"TaskMemoryManager currently works only on Linux and terminates tasks that transgress memory-limits by first calling TaskTracker.purgeTask() and then explicitly destroying the process tree to be sure that the whole process tree is cleaned up. After HADOOP-2721, we don't need this explicit process-tree destroying as the usual code-path of killing tasks itself takes care of cleaning up the whole process-trees."
MAPREDUCE-390,Corner case exists in detecting Java process deaths that might lead to orphan pipes processes lying around in memory,"In HADOOP-2092, the child pipes process periodically pings the parent Java process to find out whether it is alive. The ping cycle is 5 seconds. Consider the following scenario:
1) The Java task dies at the beginning of the ping cycle
2) A new Java task starts and binds to the same port as the earlier Java task's port
3) The pipes process wakes up and does a ping - it will still be successful since the port number hasn't changed
This will lead to orphan processes lying around in memory. The detection of parent process deaths can be made more reliable at least on Unix'ish platforms by checking whether the parent process ID is 1, and if so exit. This will take care of the most common platform that hadoop is run on. For non-unix platforms, the existing ping mechanism can be retained. Thoughts?"
MAPREDUCE-388,pipes combiner has a large memory footprint,"Pipes combiner implementation can have a huge memory overhead compared to the spill size. How much, depends on the record size. E.g., an application asks for >2GB memory when io.sort.mb=500, key is 16 bytes, and value is 4 bytes."
MAPREDUCE-331,Make jobtracker resilient to memory issues,"JobTracker is vulnerable to memory errors/attacks. Few of them are as follows
- *JOB INIT :* lot of users submitting large jobs. As every jobs is expanded, the jobtracker's memory can be completely used up
- *JSP :* jsp (jobhistory.jsp etc) can also interfere with jobtracker's memory and hence the jobtracker should be protected against such attacks
- *OLD JOBS :* lot of completed jobs can garble up jobtracker's memory and hence should be periodically cleaned up. HADOOP-4766 addresses this.

The main intention of this issue is to track various jira's that help jobtracker battle memory attacks. Jobtracker should always be up and available. "
MAPREDUCE-289,JobTracker should not expand jobs if its running low on memory,When the JobTracker detects that its running low on memory it should not expand new jobs if the job has the potential to bring it down. Consider and example where the JobTracker runs on 60% of the max memory and a new job is submitted which can take upto 40% of the max memory.  Ideally the JobTracker should _queue_ the job for expansion and expand when sufficient memory is available.
MAPREDUCE-268,Implement memory-to-memory merge in the reduce,"HADOOP-3446 fixed the reduce to not flush the in-memory shuffled map-outputs before feeding to the reduce. However for latency-sensitive applications with lots of memory like the terasort this hurts performance since the fan-in for the final in-memory merge is too large (all 8000 map-outputs very in-memory) resulting in less than optimal performance.

When I put in an intermediate memory-to-memory merge for the terasort's reduce (there-by avoiding disk i/o) to cut the fan-in from 8000 to <100 the 'reduce' phase (including the local datanode-write) sped-up 250% (from 10s to 4s). "
MAPREDUCE-249,[mapred] Enable tasks' memory management on Windows.,HADOOP-4173 disabled this.
MAPREDUCE-220,Collecting cpu and memory usage for MapReduce tasks,It would be nice for TaskTracker to collect cpu and memory usage for individual Map or Reduce tasks over time.
MAPREDUCE-193,NPEs in JobClient when mapred.jobtracker.completeuserjobs.maximum is set to zero.,Throwing NPEs is not enough of information for the user. Proper exceptions should be thrown with relevant messages.
MAPREDUCE-162,[mapred] Change TaskMemoryManager to use JvmIDs instead of TaskIDs for memory-tracking.,"To monitor tasks, TaskMemoryManager uses taskIDs to find pidFiles of the tasks. HADOOP-249 introduced jvm re-use because of which multiple tasks can run in a single JVM, and so will share the same pid(pidFile). HADOOP-249 works on a new task by creating a symlink to the pid-file of the task that ran first in the same jvm. Also, the process(jvm) is repeatedly added and removed from monitoring when tasks(under same jvm) come and go. The symlinks and the repetitive addition/removal from monitoring can be avoided if TaskMemoryManager uses JvmIDs instead of TaskIDs."
MAPREDUCE-154,Mapper runs out of memory,"The hadoop job has the task of processing 4 directories in HDFS, each with 15 files.  This is sample data, a test run, before I go to the needed 5 directories of about 800 documents each.  The mapper takes in nearly 200 pages (not files) and throws an OutOfMemory exception.  The largest file is 17 MB.

If this problem is something on my end and not truly a bug, I apologize.  However, after Googling a bit, I did see many threads of people running out of memory with small data sets."
MAPREDUCE-144,TaskMemoryManager should log process-tree's status while killing tasks.,This helps a lot in debugging why a particular task has gone beyond memory limits.
MAPREDUCE-142,Race condition in DistributedCache,"When an older version of a file in DistributedCache exists locally and multiple tasks per node start, they can run into a race condition:

dir/mapred/local/taskTracker/archive/subdir/filename is in use and cannot be refreshed
	at org.apache.hadoop.filecache.DistributedCache.localizeCache(DistributedCache.java:313)
	at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:161)
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:134)

We ran a job with the wrong file, then around 50 minutes later we put the fixed version into DFS, and ran the same job again. The job had 11,000 maps ~ about 4-5 waves of map tasks and produced 3,500 failed tasks with above error. We eventually killed it and restarted the same job again, with no problems this time.
"
MAPREDUCE-140,TaskMemoryManager not enforcing memory limits in the presence of rogue tasks,
MAPREDUCE-111,JobTracker.getSystemDir throws NPE if it is called during intialization,"JobTracker.getSystemDir throws NPE if it is called during intialization.
It should check if fileSystem is null and throw IllegalStateException, as in getFilesystemName method."
MAPREDUCE-102,NPE in tracker expiry thread.,"
I see NullPointerExceptions in Task Expiry thread of the JobTracker. Exception log in JT:
{code}
2009-02-28 07:22:51,392 ERROR org.apache.hadoop.mapred.JobTracker: Tracker Expiry Thread got exception: java.lang.NullPointerException
{code}"
MAPREDUCE-100,Sporadic TestEmptyJobWithDFS failure due to NPE is JobTracker.submitJob(),"org.apache.hadoop.mapred.TestEmptyJobWithDFS has failed a couple of times (low reproducibility) with the following exception:

2006-10-17 21:48:24,875 INFO  ipc.Server (Server.java:run(516)) - Server handler 2 on 50050 call error: java.io.IOException: java.lang.NullPointerException
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:1020)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:385)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:514)

Complete test log attached."
MAPREDUCE-84,JobClient waitForCompletion() method sometimes throws an NPE,"java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobClient$NetworkedJob.isComplete(JobClient.java:113)
        at org.apache.hadoop.mapred.JobClient$NetworkedJob.waitForCompletion(JobClient.java:128)

Does someone have an idea why this happens ?
Thanks for any help."
MAPREDUCE-73,Deadlock in JobTracker initJobs,"Found one Java-level deadlock:
=============================
""SocketListener0-26"":
  waiting to lock monitor 0x08ed5ce4 (object 0x567924c0, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 1 on 9001""
""IPC Server handler 1 on 9001"":
  waiting to lock monitor 0x08f7da88 (object 0x5744f5b8, a org.apache.hadoop.mapred.JobInProgress),
  which is held by ""initJobs""
""initJobs"":
  waiting to lock monitor 0x08ed5ce4 (object 0x567924c0, a org.apache.hadoop.mapred.JobTracker),
  which is held by ""IPC Server handler 1 on 9001""

Java stack information for the threads listed above:
===================================================
""SocketListener0-26"":
        at org.apache.hadoop.mapred.JobTracker.getClusterStatus(JobTracker.java:2313)
        - waiting to lock <0x567924c0> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.jobtracker_jsp._jspService(jobtracker_jsp.java:104)
        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)
        at org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)
        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1565)
        at org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)
        at org.mortbay.http.HttpContext.handle(HttpContext.java:1517)
        at org.mortbay.http.HttpServer.service(HttpServer.java:954)
        at org.mortbay.http.HttpConnection.service(HttpConnection.java:814)
        at org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)
        at org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)
        at org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)
        at org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)
        at org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)
""IPC Server handler 1 on 9001"":
        at org.apache.hadoop.mapred.JobInProgress.obtainTaskCleanupTask(JobInProgress.java:935)
        - waiting to lock <0x5744f5b8> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.JobTracker.getSetupAndCleanupTasks(JobTracker.java:2167)
        - locked <0x56795708> (a java.util.TreeMap)
        - locked <0x567924c0> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:1902)
        - locked <0x567924c0> (a org.apache.hadoop.mapred.JobTracker)
        at sun.reflect.GeneratedMethodAccessor3278.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:481)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:894)
""initJobs"":
        at org.apache.hadoop.mapred.JobTracker.finalizeJob(JobTracker.java:1539)
        - waiting to lock <0x567924c0> (a org.apache.hadoop.mapred.JobTracker)
        at org.apache.hadoop.mapred.JobInProgress.garbageCollect(JobInProgress.java:2320)
        - locked <0x5744f5b8> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.JobInProgress.terminateJob(JobInProgress.java:2004)
        - locked <0x5744f5b8> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:472)
        - locked <0x575b7ec8> (a org.apache.hadoop.mapred.JobInProgress$JobInitKillStatus)
        - locked <0x5744f5b8> (a org.apache.hadoop.mapred.JobInProgress)
        at org.apache.hadoop.mapred.EagerTaskInitializationListener$JobInitThread.run(EagerTaskInitializationListener.java:55)

Found 1 deadlock."
MAPREDUCE-69,NPE in TaskTracker RenitTrackerAction ,"TaskTracker log shows =============

2007-03-28 02:32:18,076 INFO org.apache.hadoop.mapred.TaskTracker: Recieved RenitTrackerAction from JobTracker
2007-03-28 02:32:18,494 ERROR org.apache.hadoop.mapred.TaskTracker: Can not start task tracker because java.lang.NullPointerException
  at org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:1187)
  at org.apache.hadoop.mapred.TaskTracker.close(TaskTracker.java:430)
  at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:917)
  at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:1589)

JobTracker log shows ==============
2007-03-28 02:31:18,977 INFO org.apache.hadoop.mapred.JobTracker: Lost tracker '____.____.com'
...
2007-03-28 02:31:18,977 INFO org.apache.hadoop.mapred.JobInProgress: TaskTracker at 'tracker____.___.com' turned 'flaky'
...
2007-03-28 02:32:18,075 WARN org.apache.hadoop.mapred.JobTracker: Status from unknown Tracker : tracker____.___.com:#####
"
MAPREDUCE-55,Reduce task should stop shuffle-retrying in case of out-of-memory errors,"In ReduceTask, MapOutputCopier threads catch Throwble and retry happens for the shuffle. It should not retry incase of Errors suchas OutOfMemoryError etc.

May be it should retry only in case of Connect/Read failures and die in all other cases. Thoughts?"
MAPREDUCE-44,"Hang JobTracker, running out of memory","This may be expected.

Hang JobTracker with 1G heapsize, top showed 99% cpu. 

Ran about 80 jobs.  Each with 2500 mappers 200 reducers.  They finish quite fast.  3-4 mins avg per job.
(200k tasks)


How much memory does JobTracker use for 'completed'  (but not expired) jobs ?

jmap -heap showed 
{noformat} 
...
PS Old Generation
   capacity = 932118528 (888.9375MB)
   used     = 932118528 (888.9375MB)
...
{noformat} 

jmap -histo showed 
{noformat} 
num   #instances    #bytes  class name
--------------------------------------
  1:   3974182   355869992  [C
  2:   5216606   125198544  java.lang.String
  3:   2238560   107450880  java.util.TreeMap
  4:    463206   101673488  [B
  5:   1979995    63359840  java.util.TreeMap$Entry
  6:    248400    35769600  org.apache.hadoop.mapred.TaskInProgress
  7:    308803    30898112  [Ljava.lang.Object;
  8:    978240    23477760  org.apache.hadoop.mapred.Counters$CounterRec
  9:    249876    19990080  org.apache.hadoop.mapred.TaskStatus
 10:    248836    19906880  java.net.URI
 11:    230337    16584264  org.apache.hadoop.mapred.MapTask
...
{noformat} 

Log showing many heartbeat discarded messages
{noformat} 
2007-10-30 22:55:46,912 WARN org.apache.hadoop.ipc.Server: IPC Server handler 6 on 58567, call heartbeat(org.apache.hadoop.mapred.TaskTrackerStatus@1afb9c9, false, true, 3942) from 99.99.99.99:9999 discarded for being too old (2578616)
{noformat} 

Is the solution either to increase the jobtracker heapsize or set shorter 'mapred.userlog.retain.hours'  ?


"
MAPREDUCE-40,Memory management variables need a backwards compatibility option after HADOOP-5881,HADOOP-5881 modified variables related to memory management without looking at the backwards compatibility angle. This JIRA is to adress the gap. Marking it a blocker for 0.20.1
MAPREDUCE-22,Per task memory usage stats from TaskMemoryManager on mapred web ui,It would be good to have per-task memory usage statistics from the TaskMemoryManager displayed on the web ui  as the task progresses on. This would make it easy for users to (roughly) track their tasks' memory usage.
MAPREDUCE-19,TaskMemoryMonitorThread is not stopped in close,"The task memory monitor thread is created in initialize, but is not stopped in close. So, if there's a reinit, this can result in a thread becoming a zombie as the thread variable is replaced in initialize."
MAPREDUCE-16,"Reduce task failed at shuffling time, throwing null pointer exception","

This happened for 0.17.0 branch.

Here is the stack trace:

2008-04-11 13:45:54,171 ERROR org.apache.hadoop.mapred.ReduceTask: Map output copy failure: java.lang.NullPointerException
	at org.apache.hadoop.fs.InMemoryFileSystem$RawInMemoryFileSystem.getFileStatus(InMemoryFileSystem.java:302)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:242)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.copyOutput(ReduceTask.java:853)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$MapOutputCopier.run(ReduceTask.java:777)
"
MAPREDUCE-13,Mapper failed due to out of memory,"
When a map/reduce job takes block compressed sequence files as input, 
the input data may be expanded significantly in size (a few to tens X, depending on
the compression ratio of the particular data blocks in the files).
This may cause out of memory problem in mappers.

In my case, I set heap space to 1GB.
The mappers started to fail when the accumulated expanded input size reaches above 300MB
 
"
MAPREDUCE-10,NPE in SocketChannelOutputStream during large sort benchmark,"Running sort benchmark, I saw this NPE trace in the JobTracker log

...
2007-02-04 02:07:23,753 INFO org.apache.hadoop.mapred.JobInProgress: Already complete TIP tip_0002_m_032689 has completed task task_0002_m_032689_2
2007-02-04 02:07:23,753 INFO org.apache.hadoop.mapred.TaskInProgress: Task 'task_0002_m_032689_2' has completed.
2007-02-04 02:07:24,566 WARN org.apache.hadoop.ipc.Server: handler output errorjava.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcher.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:29)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:104)
        at sun.nio.ch.IOUtil.write(IOUtil.java:75)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:302)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:556)
2007-02-04 02:07:24,569 WARN org.apache.hadoop.ipc.Server: handler output error
java.lang.NullPointerException
        at org.apache.hadoop.ipc.SocketChannelOutputStream.flushBuffer(SocketChannelOutputStream.java:108)
        at org.apache.hadoop.ipc.SocketChannelOutputStream.write(SocketChannelOutputStream.java:89)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:556)
2007-02-04 02:07:32,510 INFO org.apache.hadoop.mapred.JobInProgress: Task 'task_0002_m_077715_2' has completed tip_0002_m_077715 successfully.
...
"
