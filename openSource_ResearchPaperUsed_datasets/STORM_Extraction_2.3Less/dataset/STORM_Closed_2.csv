Bug_ID,Bug_Summary,Bug_Description
STORM-4060,Netty client will wait up to 10 minutes to send messages to unreachable worker on close()," 

Since PENDING_MESSAGES_FLUSH_TIMEOUT_MS is hardcoded to 10 minutes, this means buffered messages will remain in the worker 10 minutes before being given up and reprocessed.
This leads to increased latencies in the topology.
It is proposed that PENDING_MESSAGES_FLUSH_TIMEOUT_MS is exposed as property to be configured by the user.

Code that leads to this situation:
{code:java}
public class Client extends ConnectionWithStatus implements ISaslClient {
    private static final long PENDING_MESSAGES_FLUSH_TIMEOUT_MS = 600000L; {code}
{code:java}
@Override
public void close() {
    if (!closing) {
        LOG.info(""closing Netty Client {}"", dstAddressPrefixedName);
        // Set closing to true to prevent any further reconnection attempts.
        closing = true;
        waitForPendingMessagesToBeSent();
        closeChannel();

        // stop tracking metrics for this client
        if (this.metricRegistry != null) {
            this.metricRegistry.deregister(this.metrics);
        }
    }
} {code}
{code:java}
private void waitForPendingMessagesToBeSent() {
    LOG.info(""waiting up to {} ms to send {} pending messages to {}"",
             PENDING_MESSAGES_FLUSH_TIMEOUT_MS, pendingMessages.get(), dstAddressPrefixedName);
    long totalPendingMsgs = pendingMessages.get();
    long startMs = System.currentTimeMillis();
    while (pendingMessages.get() != 0) {
        try {
            long deltaMs = System.currentTimeMillis() - startMs;
            if (deltaMs > PENDING_MESSAGES_FLUSH_TIMEOUT_MS) {
                LOG.error(""failed to send all pending messages to {} within timeout, {} of {} messages were not ""
                    + ""sent"", dstAddressPrefixedName, pendingMessages.get(), totalPendingMsgs);
                break;
            }
            Thread.sleep(PENDING_MESSAGES_FLUSH_INTERVAL_MS);
        } catch (InterruptedException e) {
            break;
        }
    }

} {code}
 "
STORM-4024,Bolt Input Stats are blank if topology.acker.executors is null or 0,"On StormUI (and via API) the bolt Input Stats do not work when topology.acker.executors is null or 0 (see attachements showing difference with and without ackers)

Also, some of the per-bolt instance Executed and latency fields are also not working"
STORM-4017,isAnyWindowsProcessAlive does not work with multiple pids,"The method isAnyWindowsProcessAlive in ServerUtils.java does not work with 2 or more pids in the supplied pid collection

The refactor done as part of STORM-3638 has made an incorrect assumption that the 'tasklist' Windows tool can accept multiple arguments of ""/fi pid eq <id>"" and treat this list as an 'or' list when, in fact, the /fi arguments should be considered an 'and' list.

e.g. tasklist /fi ""pid eq 123"" /fi ""pid eq 456"" means 'give me information on processes where each process has a pid of 123 and 456'

The effect of this means that isAnyWindowsProcessAlive will always return false when given 2 or more (different) pids."
STORM-4002,Security Vulnerability - Action Required: “Incorrect Permission Assignment for Critical Resource” vulnerability in some components of  org.apache.storm," I think the method org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache) may have an “Incorrect Permission Assignment for Critical Resource”vulnerability which is vulnerable in in some components of  org.apache.storm. It shares similarities to a recent CVE disclosure _CVE-2017-3166_ in the project _""apache/hadoop""_ project. The influencing components are listed below:
 # org.apache.storm:storm-kafka-examples in the versions between 1.1.0 and 1.2.4.
 # org.apache.storm:storm-starter in the versions of 1.1.2-1.1.3 and 1.2.0-1.2.2

The source vulnerability information is as follows: !https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.1&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ_bBS_0CMiL9kNUgnr95IJelNJAQJp906nnAonpFswrxMbSt1EVV1S2q6kq_ur-YE-1H49gOCjMGqFYtm5xBOS_EBOZci8ukIw2Hn8kM-9OIKVIxXrlhcRm6LA&disp=emb&realattid=ii_lmt56kbv0|width=1,height=1!!https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.2&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ-8wPNUdQ35WBKaadck2X1lP34blTQ_qiyhu5T7l0G8T4cboSCiFNgfxaCQZZsK-Pm3ebzj4JSWBs558OxWHJPM1uJqKlMvPMhpx9J0TiojhC85DNqeLu3dr2Q&disp=emb&realattid=ii_lmt6415i0|width=1,height=1!!https://mail.google.com/mail/u/0?ui=2&ik=35947afd70&attid=0.0.1&permmsgid=msg-f:1782522681557497681&th=18bccaef464fb751&view=fimg&fur=ip&sz=s0-l75-ft&attbid=ANGjdJ9XERxykP1zaB9Codaz3lisQ9gKwLHXnEIHP4p4oUcINmdFEWTJAWeDMfayncBsWIBj_kc2cAKHx4c7InMtKL98nDb2Dnt3TpfGLQCcJhdFsSBhemVA14CI0rA&disp=emb&realattid=ii_loxzzieb0|width=1,height=1!

*Vulnerability Detail:*

*CVE Identifier:* CVE-2017-3166

{*}Description{*}: In Apache Hadoop versions 2.6.1 to 2.6.5, 2.7.0 to 2.7.3, and 3.0.0-alpha1, if a file in an encryption zone with access permissions that make it world readable is localized via YARN's localization mechanism, that file will be stored in a world-readable location and can be shared freely with any application that requests to localize that file.

*Reference:*[ |http://goog_608275719/] [https://nvd.nist.gov/vuln/detail/CVE-2017-3166]

{*}Patch{*}: [https://github.com/apache/hadoop/commit/a47d8283b136aab5b9fa4c18e6f51fa799d91a29]
*Vulnerability Description:* The vulnerability is present in the class  org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager  of method  checkPermissionOfOther(FileSystem fs, Path path, FsAction action, Map<URI, FileStatus> statCache)  , which is responsible for checking the permissions of other files in the distributed cache.. {*}But t{*}{*}he check snippet is similar to the vulnerable snippet for CVE-2017-3166{*} and may have the same consequence as CVE-2017-3166: {*}a file in an encryption zone with access permissions  will be stored in a world-readable location and can be freely shared with any application that requests the file to be localized{*}. Therefore, maybe you need to fix the vulnerability with much the same fix code as the CVE-2017-3166 patch. 
    Considering the potential risks it may have, I am willing to cooperate with you to verify, address, and report the identified vulnerability promptly through responsible means. If you require any further information or assistance, please do not hesitate to reach out to me. Thank you and look forward to hearing from you soon.
 "
STORM-3956,Fix cli monitor component's argument type ,"From https://github.com/apache/storm/pull/3423

The `component` value is a string, not a number, see [Monitor.java](https://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/utils/Monitor.java#L156).

Attempting to use a number throws a stacktrace like such:

```
~/apache-storm-2.3.0/bin/storm monitor -m wordGenerator production-topology
topology	component	parallelism	stream	time-diff ms	emitted	throughput (Kt/s)
Available components for production-topology :
------------------
__acker
wordGenerator
intermediateRanker
counter
finalRanker
------------------
Exception in thread ""main"" java.lang.IllegalArgumentException: component: wordGeneratotor not found
	at org.apache.storm.utils.Monitor.metrics(Monitor.java:128)
	at org.apache.storm.utils.Monitor.metrics(Monitor.java:83)
	at org.apache.storm.command.Monitor$1.run(Monitor.java:53)
	at org.apache.storm.utils.NimbusClient.withConfiguredClient(NimbusClient.java:128)
	at org.apache.storm.utils.NimbusClient.withConfiguredClient(NimbusClient.java:117)
	at org.apache.storm.command.Monitor.main(Monitor.java:50)
```
"
STORM-3943,May I ask that you can configure a user password to log in for security verification when visiting storm ui. Check the official website security verification is not very clear. storm version 1.2.3,
STORM-3881,How to dynamically update variable information in memory after the cluster starts storm,"_I want to dynamically update some of the memory information in the Bolt processing logic, but the number and concurrency of Bolts are too large. Another way of thinking, can it be achieved by changing the memory of each worker process?_"
STORM-3871,Storm blobstore leak space,"We observe after 5 days, with 3 nimbus deployed in HA, that the blostore keeps growing.

 

Usually we submit topologies each 10min  and it ends in around 10 min. The expected result is that the blobstore cleans itself so that topologies older than days are removed. But this is not the case, as seen below (done the 2022-06-03, expected result: only blob from the current day should appear. current behavior: blobs from 2 days ago are still here)

 
{noformat}
/space/StormApp/current/bin/storm blobstore list 2>&1 | grep o.a.s.c.Blobstore | sort -k7 | perl -anE '$date=localtime($F[6]/1000);shift @F;say join "" "", $date, @F'
Wed Jun  1 14:12:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-8f132aef-b401-4fa6-b07e-3588073da824.jar 1654092763312 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 14:13:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-ae74967a-0def-44a1-8d1b-53a255900239.jar 1654092823330 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 14:33:00 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-8fa7edcb-5f8a-4a79-8f8d-8c8fd50dbc9e.jar 1654093980305 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 15:14:19 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-9168e429-2cd6-433c-90d9-6db19d564824.jar 1654096459069 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 16:13:49 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-9ed7a9fd-2819-4784-a953-c04a65981c59.jar 1654100029294 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 22:53:29 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-124b606a-0d2b-44a1-9885-4bb1152a98e7.jar 1654124009759 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 23:23:59 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-be7e85b7-3cf1-495a-83a7-6a28da33af57.jar 1654125839685 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Wed Jun  1 23:33:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-f799a103-d544-4a31-a680-1d0625fab2b9.jar 1654126423302 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 01:13:49 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-3edcef03-f7d6-400e-8076-30dc0e5c4bff.jar 1654132429251 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 02:03:29 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-807000a6-0416-4821-b67d-a1e5e433edce.jar 1654135409776 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 02:13:30 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-4f7e7439-7788-4b74-81a2-4d6433caed4b.jar 1654136010028 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 03:03:59 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-038e17f3-7acd-4c19-a212-a095e6c7adba.jar 1654139039870 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 07:43:30 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-b4fb1761-4f97-4ac7-8785-85e2aefbb66c.jar 1654155810868 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 08:33:49 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-ac2c4004-d561-44ad-beca-0883ad8980c5.jar 1654158829119 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 09:44:00 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-31f8b9b4-8f79-47ef-8728-46127baf04a2.jar 1654163040830 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 09:53:00 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-e373b50f-6282-4eaa-a702-57c8f4d0be55.jar 1654163580973 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 10:03:29 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-9b0ac8d3-b446-4ce5-89c1-2591cdeb80e6.jar 1654164209828 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 11:43:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-638cbabd-b3ba-46c3-821c-5eb45578c542.jar 1654170223374 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 12:13:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-d8fb8058-d3d6-42eb-9e15-5d3682fcc2c7.jar 1654172023366 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 13:02:59 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-46e1ff96-da4d-4936-913a-d477d743027a.jar 1654174979704 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 16:52:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-b1b010ee-48d3-4fa5-b6dc-b136a4d54c59.jar 1654188763365 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 17:02:59 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-d11859ec-7750-434b-a7d1-da5ddd2d68c3.jar 1654189379774 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 18:03:00 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-22b34179-5dbd-4bf0-aa5d-c21a978fb8c7.jar 1654192980944 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Thu Jun  2 23:43:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-2b564b9c-1727-404e-9e92-edff6b4799a6.jar 1654213423405 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Fri Jun  3 01:13:43 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-c32e9295-abd0-48de-921f-a0d524ca4db2.jar 1654218823466 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Fri Jun  3 05:33:31 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-4f88e716-a620-4897-bc80-89e3bfa2282c.jar 1654234411000 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Fri Jun  3 08:43:31 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-deb4795a-1983-46ea-a6a3-00457d44d523.jar 1654245811327 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa]
Fri Jun  3 10:23:00 2022 [main] INFO o.a.s.c.Blobstore - dep-apeframework-3.11.3-e887952c-2403-484c-ab53-22ff75a26b9a.jar 1654251780168 [o::r--, o::rwa, u:class org.apache.storm.security.auth.NimbusPrincipal:rwa] {noformat}"
STORM-3863,tirupathi trip from chennai,"Padmavathi Travels T.Nagar Provides Chennai to tirupati Car Packages and Services at best Price. *Our Tirupati Tour Package by car* includes all the customer requirments, We are operating Daily Tirupati Balaji Darshan from Chennai for more than 23+ years. Padmavathi Travels chennai is considered as one of the best travel agents in chennai.

https://padmavathitravels.com/index.amp.shtml"
STORM-3856,"Warning ""contains cycles in components"" from StormSubmitter","StormSubmitter throw exception when cycles detected, 
with no configuration option to disable this warning.

Although sometime the cycle create by design.

please add configuration for disable the warning for specific topology."
STORM-3852,Storm 1.2.4 Vulnerability in Grype Scan,"[ Grype|https://github.com/anchore/grype] scan done on Storm 1.2.4 distribution identifies several vulnerabilities due dependent jars of several modules. Please refer to attached xls workbook for a detailed listing.

Summary of all CVEs are as below. Mitigating critical and high vulnerabilities are much needed for production deployment of storm. Please investigate and advise how the critical and high defects can be addressed at minimum.


||Severity||Count||
|Critical|63|
|High|122|
|Medium|43|
|Low|7|

*NOTE* : Over 90% of reported issues are originating from Storm external folder artifacts. Without considering artifacts in external folder the reported summary is as below.
||Severity||Count||
|Critical|14|
|High|31|
|Medium|24|
|Low|4|

 "
STORM-3840,log4j vulnerability,"Hi Team,

 

When we ran our vulnerability scanner we found following components has log4j vulnerability

lib/jetty-servlets-9.4.14.v20181114.jar
lib/kafka-clients-0.11.0.3.jar
lib-tools/sql/core/protobuf-java-3.1.0.jar
lib-tools/sql/runtime/calcite-core-1.14.0.jar
lib-tools/sql/runtime/guava-16.0.1.jar
lib-tools/sql/runtime/guava-16.0.1.jar
lib-webapp/dropwizard-validation-1.3.5.jar
lib-webapp/dropwizard-validation-1.3.5.jar
lib-webapp/hibernate-validator-5.4.2.Final.jar
lib-webapp/hibernate-validator-6.0.17.Final.jar
lib-webapp/hibernate-validator-6.0.17.Final.jar
lib-webapp/jakarta.el-3.0.2.jar

 

Required versions to resolve vulnerabilities :

 

jetty-servlets > 9.4.41.v20210516
kafka-clients > 2.1.1
protobuf-java > 3.4.0
calcite-core > 1.26.0
guava > 30.0
dropwizard-validation > 1.3.21
hibernate-validator > 6.0.20
jakartha-el > 3.0.4

 

is there any procedure to follow to resolve this vulnerability issue while changing the required libraries in the given storm version? or Apache Storm team is planning to release a new version of Storm which handles the vulnerability issues?

 

Kindly let is know your feedback so that we can either upgrade the given packages under the current version of storm we have or we download the newer version of storm which implicitly handles this issue.

 

Thanks in advance

 

Regards,

Adarsh"
STORM-3829,Remove log4j version 1 (1.2.17) from storm-core,"storm-core/pom.xml has the following entry:

{code:java}
        <!--Hadoop Mini Cluster cannot use log4j2 bridge,
        Surefire has a way to exclude the conflicting log4j API jar
        from the classpath, classpathDependencyExcludes, but it didn't work in practice.
        This is here as a work around to place it at the beginning of the classpath
        even though maven does not officially support ordering of the classpath.-->
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.17</version>
            <scope>test</scope>
        </dependency>
{code}

This dependency can be removed. ""mvn test"" goes to success. And ""mvn dependency:tree | grep log4j"" shows no transitive dependency on log4j-1.2.17.
"
STORM-3823,Release Prep fails in storm-client/test/py/test_storm_cli.py,"When running the release preparation script via:
      *mvn release:prepare -P dist,rat,externals,examples*
the python test script *storm-client/test/py/test_storm_cli.py* fails because the command generated by storm.py is slightly different than what the test script expects.

In particular, directories are added to java classpath with a ""/*"". This is correct.
However, the test script does not expect this.

Fix the test script storm-client/test/py/test_storm_cli.py to append ""/*"" to directories that contain jar files."
STORM-3818,Joining more than 2 streams in Stream API,"Joining two streams in storms Stream API works fine as described in the docs with the scheme: {{streamA.window(...).join(streamB)}}

However, when joining the result with a third join, a `punctuation error` is thrown.

So there is a way missing of joining {_}three streams{_}? I think, that one window configuration is enough. So the pseudocode would look like: {{{}streamA.window(...).join(streamB).join(streamC){}}}. However, this raises a punctuation error.

Here is the error log:
{code:java}
Caused by: java.lang.IllegalStateException: Received punctuation from streams [s6] expected [s7]
    at org.apache.storm.streams.ProcessorBoltDelegate.shouldPunctuate(ProcessorBoltDelegate.java:287) ~[classes/:?]
    at org.apache.storm.streams.ProcessorBoltDelegate.punctuateInitialProcessors(ProcessorBoltDelegate.java:189) ~[classes/:?]
    at org.apache.storm.streams.ProcessorBoltDelegate.process(ProcessorBoltDelegate.java:179) ~[classes/:?]
    at org.apache.storm.streams.WindowedProcessorBolt.execute(WindowedProcessorBolt.java:68) ~[classes/:?]
    at org.apache.storm.topology.WindowedBoltExecutor.boltExecute(WindowedBoltExecutor.java:371) ~[classes/:?]
    at org.apache.storm.topology.WindowedBoltExecutor$1.onActivation(WindowedBoltExecutor.java:364) ~[classes/:?]
    at org.apache.storm.windowing.WindowManager.onTrigger(WindowManager.java:156) ~[classes/:?]
    at org.apache.storm.windowing.TimeTriggerPolicy$1.run(TimeTriggerPolicy.java:119) ~[classes/:?]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305) ~[?:?]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
    ... 1 more {code}
 "
STORM-3817,"Upgrading to Zookeeper 3.5.x, 3.6.x or 3.7.x","Is there any possibility to upgrade the [shaded zookeeper version |https://github.com/apache/storm/blob/master/storm-shaded-deps/pom.xml#L64] from 3.4.14 to a newer version? Or are there any reasons for not doing an upgrade right now?

I am doing some testing with Storm in a Java 17 environment and it looks like I am suffering from this Zookeeper specific issue present in 3.4.14: https://issues.apache.org/jira/browse/ZOOKEEPER-3779

If necessary I can also provide a PR for an upgrade to 3.5.x, 3.6.x or 3.7.x

UPDATE: Looks like curator depends on 3.5.x - so probably 3.5.x should be an option."
STORM-3816,Unrecognized VM option 'PrintGCDateStamps',"When starting storm using the official docker images [https://hub.docker.com/_/storm] following the listed example, then deploying a topology - the worker does not come up (logs inside of the supervisor):
{code:java}
2022-01-10 14:24:14.803 STDERR Thread-0 [INFO] Unrecognized VM option 'PrintGCDateStamps'
2022-01-10 14:24:14.803 STDERR Thread-1 [INFO] [0.001s][warning][gc] -Xloggc is deprecated. Will use -Xlog:gc:artifacts/gc.log instead.
2022-01-10 14:24:14.811 STDERR Thread-0 [INFO] Error: Could not create the Java Virtual Machine.
2022-01-10 14:24:14.811 STDERR Thread-0 [INFO] Error: A fatal exception has occurred. Program will exit. {code}"
STORM-3814,"storm-core: Remediate log4j critical vulnerabilities -> 2.16.0 or newer, prefer 2.17.1","* [https://logging.apache.org/log4j/2.x/security.html]

 

*In order to remediate these bugs with Log4j, please update Storm 2.3.0 and 1.2.3* 
 * Criticals
 ** Fixed in Log4j 2.16.0 (Java 8) and Log4j 2.12.2 (Java 7)
 *** [CVE-2021-45046|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45046]: Apache Log4j2 Thread Context Lookup Pattern vulnerable to remote code execution in certain non-default configurations
 ** Fixed in Log4j 2.15.0 (Java 8)
 *** [CVE-2021-44228|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228]: Apache Log4j2 JNDI features do not protect against attacker controlled LDAP and other JNDI related endpoints.
 * Moderates
 ** Fixed in Log4j 2.17.1 (Java 8), 2.12.4 (Java 7) and 2.3.2 (Java 6)
 *** [CVE-2021-44832|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44832]: Apache Log4j2 vulnerable to RCE via JDBC Appender when attacker controls configuration.
 ** Fixed in Log4j 2.17.0 (Java 8), 2.12.3 (Java 7) and 2.3.1 (Java 6)
 *** [CVE-2021-45105|https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45105]: Apache Log4j2 does not always protect from infinite recursion in lookup evaluation

 "
STORM-3810,CVE-2021-44228 Log4J vulnerability,"Recent critical CVE about Log4J ([https://www.cvedetails.com/cve/CVE-2021-44228/)] affects Storm.

Please upgrade to latest Log4j2 >= 2.16.0 (see [https://search.maven.org/artifact/org.apache.logging.log4j/log4j/2.16.0/pom)] in 1.2.X Storm branch and also in 2.X.X Storm branches.

Thank you!"
STORM-3809,CVE-2021-44228 Log4Shell: upgrade log4j2,"Recent critical CVE about log4shell ([https://www.cvedetails.com/cve/CVE-2021-44228/)] affects Storm. (Eg: in Storm 2.2.0, it uses log4j-api-2.11.2.jar) Any log4j2 between 2.0 and 2.14 is affected.

 

I did not found any issue or news related to Apache Storm and a fix. So I create this ticket to track it down.

Please upgrade to latest Log4j2 >= 2.16.0 (see [https://search.maven.org/artifact/org.apache.logging.log4j/log4j/2.16.0/pom)] in both 2.2.X and 2.3.X Storm branches. Thank you!

 "
STORM-3806,Enable the Debug action of the topology by bypassing the configur file through the webapp interface,"{{    If I want to turn on the Debug action of topology, I need to set _topology.eventlogger.executors > 0_ in the Nimbus configuration file.Refer to the figure below:}}

!image-2021-11-08-15-46-36-131.png|width=640,height=304!

 

{{   But I found that this restriction can be bypassed.}}{{   If I don't set topology.eventlogger.executors, I can directly send HTTP request package to enable topology Debug the action. Refer to the following figure for HTTP request package:}}

!image-2021-11-08-15-56-33-059.png|width=1703,height=270!"
STORM-3798,Nimbus keep on crash once restart nimbus machine,"I have setup the storm cluster in AWS environment with 6 node clusters. 1 Master and 5 slave and the zookeeper are installed in 3 node clusters.
 
When I shutdown zookeeper and storm nodes every day night and start in the morning, When the machine boots up I have started all the nimbus and supervisor and zookeeper service via supervisord. Once machine up all the service are running but in the nimbus logs I am getting ""java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?"" error.
 
I have restarted zookeeper and nimbus also not working still getting this error. Once I remove all the data from zookeeper data directly then restart zookeeper and storm service then working fine.
Why I have to do this everyday when the machine starts up? how can I fix this issue. When machine boot up all the service should work fine."
STORM-3797,Cannot generate storm.thrift using Golang generator,"Generating Go code from storm.thrift fails with the following message:

[FAILURE:generation:1] Error: Cannot produce a valid type for a Go map key: []int64 - aborting.

 

The offending lines:

[https://github.com/apache/storm/blob/b5252eda18e76c4f42af58d7481ea66cf3ec8471/storm-client/src/storm.thrift#L517-L518|https://github.com/apache/storm/blob/b5252eda18e76c4f42af58d7481ea66cf3ec8471/storm-client/src/storm.thrift#L518]

 

Golang does not allow slices to be used as a key for a map since they are not comparable using the equality operator. Otherwise the rest of it will generate just fine if those maps are commented out.

 

What is the significance of those maps?"
STORM-3796,How many topologies can create in a cluster?,"This is not a bug. I want to know the topologies behaviour. I read many sites related to Storm's topologies design setup. But, I didn't get clarity.

In my project, I am going to processing more than a million records. So, I planned to create topologies dynamically based on internal modules. The count might be reached more than a thousand. My doubt is what is the best way to manage topologies? How many topologies can be created in a single cluster? Are there any problems with maintaining multiple topologies?"
STORM-3785,Rate metrics are wrongly divided by 1000000,"customers complained about odd behavior of storm rate metrics (m1_rate, etc). It turns out to be a Storm bug:

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L416
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L437


"
STORM-3784,my supervisor will shut down on 2:00 am everyday,"The cluster has one nimbus and two supervisors.one of the supervisors is alone with nimbus.

I deployed two topology that PradarLinkTopology and PradarLogTopology.

PradarLogTopology run with 4 workers.PradarLinkTopology run with 1 workers.

on 2:00 am everyday, all supervisors will shut down,i havn't find out the reason.

I try to clean up the status directory,but the problem still exsit.

this is my supervisor.log
{code:java}
//代码占位符
2021-07-21 02:03:42.070 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:Error occurred during initialization of VM2021-07-21 02:03:42.070 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:Error occurred during initialization of VM2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:java.lang.Error: Properties init: Could not determine current working directory.2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28: at java.lang.System.initProperties(Native Method)2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28: at java.lang.System.initializeSystemClass(System.java:1166)2021-07-21 02:03:42.071 o.a.s.u.Utils Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28:2021-07-21 02:03:42.323 o.a.s.d.s.BasicContainer SLOT_6702 [INFO] Removed Worker ID dcae9231-4be4-4842-9ed0-988e1b8a2b282021-07-21 02:03:42.329 o.a.s.d.s.Slot SLOT_6702 [INFO] STATE kill msInState: 68588 topo:PradarLogTopology-3-1626751922 worker:null -> empty msInState: 32021-07-21 02:03:42.329 o.a.s.d.s.Slot SLOT_6702 [INFO] SLOT 6702: Changing current assignment from LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:256.0, mem_off_heap:0.0, cpu:20.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=256.0, cpu.pcore.percent=20.0}, shared_resources:{}), owner:root) to null2021-07-21 02:03:42.353 o.a.s.d.s.Supervisor pool-10-thread-1 [WARN] Topology config is not localized yet...2021-07-21 02:03:42.449 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700 all processes are dead...2021-07-21 02:03:42.449 o.a.s.d.s.Container SLOT_6700 [INFO] Cleaning up 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86:b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:42.450 o.a.s.d.s.Container SLOT_6700 [INFO] GET worker-user for b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:42.450 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/pids/163262021-07-21 02:03:43.322 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/26b5ffbd-08b6-46df-aa04-6b86f78b8ad8/pids2021-07-21 02:03:43.322 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/26b5ffbd-08b6-46df-aa04-6b86f78b8ad8/tmp2021-07-21 02:03:45.209 o.a.s.d.s.BasicContainer Thread-17 [INFO] Worker Process dcae9231-4be4-4842-9ed0-988e1b8a2b28 exited with code: 12021-07-21 02:03:45.224 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.224 o.a.s.d.s.Supervisor pool-10-thread-7 [WARN] Topology config is not localized yet...2021-07-21 02:03:45.224 o.a.s.d.s.Container SLOT_6701 [INFO] REMOVE worker-user 26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.224 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/heartbeats2021-07-21 02:03:45.224 o.a.s.d.s.AdvancedFSOps SLOT_6701 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers-users/26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.224 o.a.s.t.ProcessFunction pool-10-thread-7 [ERROR] Internal error processing sendSupervisorWorkerHeartbeatorg.apache.storm.utils.WrappedNotAliveException: PradarLinkTopology-2-1626337413 does not appear to be alive, you should probably exit at org.apache.storm.daemon.supervisor.Supervisor$1.sendSupervisorWorkerHeartbeat(Supervisor.java:442) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:374) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:353) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:174) [storm-client-2.1.0.jar:2.1.0] at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.1.0.jar:2.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_201] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_201] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_201]2021-07-21 02:03:45.225 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/pids2021-07-21 02:03:45.225 o.a.s.d.s.BasicContainer Thread-16 [INFO] Worker Process b7963273-452a-43af-bc00-d814e0629f96 exited with code: 2542021-07-21 02:03:45.225 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f96/tmp2021-07-21 02:03:45.226 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers/b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.226 o.a.s.d.s.Container SLOT_6700 [INFO] REMOVE worker-user b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.226 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /data/apache-storm-2.1.0/status/workers-users/b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.227 o.a.s.d.s.BasicContainer SLOT_6701 [INFO] Removed Worker ID 26b5ffbd-08b6-46df-aa04-6b86f78b8ad82021-07-21 02:03:45.228 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Removed Worker ID b7963273-452a-43af-bc00-d814e0629f962021-07-21 02:03:45.229 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE kill msInState: 81385 topo:PradarLogTopology-3-1626751922 worker:null -> empty msInState: 02021-07-21 02:03:45.229 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Changing current assignment from LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:3, task_end:3)], resources:WorkerResources(mem_on_heap:128.0, mem_off_heap:0.0, cpu:10.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=128.0, cpu.pcore.percent=10.0}, shared_resources:{}), owner:root) to null2021-07-21 02:03:45.230 o.a.s.d.s.Slot SLOT_6701 [INFO] STATE kill-and-relaunch msInState: 95356 topo:PradarLogTopology-3-1626751922 worker:null -> waiting-for-blob-localization msInState: 12021-07-21 02:03:45.231 o.a.s.d.s.Slot SLOT_6701 [INFO] SLOT 6701: Changing current assignment from LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:3, task_end:3)], resources:WorkerResources(mem_on_heap:128.0, mem_off_heap:0.0, cpu:10.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=128.0, cpu.pcore.percent=10.0}, shared_resources:{}), owner:root) to null2021-07-21 02:03:45.231 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE empty msInState: 2 -> waiting-for-blob-localization msInState: 02021-07-21 02:03:45.232 o.a.s.d.s.Slot SLOT_6701 [ERROR] Error when processing eventjava.io.FileNotFoundException: File '/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLinkTopology-4-1626751925/stormconf.ser' does not exist at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[storm-shaded-deps-2.1.0.jar:2.1.0] at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:303) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:464) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:298) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:351) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:452) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:440) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:228) ~[storm-server-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:931) [storm-server-2.1.0.jar:2.1.0]2021-07-21 02:03:45.234 o.a.s.u.Utils SLOT_6701 [ERROR] Halting process: Error when processing an eventjava.lang.RuntimeException: Halting process: Error when processing an event at org.apache.storm.utils.Utils.exitProcess(Utils.java:512) [storm-client-2.1.0.jar:2.1.0] at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:978) [storm-server-2.1.0.jar:2.1.0]2021-07-21 02:03:45.235 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Created Worker ID 68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.236 o.a.s.d.s.Container SLOT_6700 [INFO] Setting up 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86:68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.236 o.a.s.d.s.Container SLOT_6700 [INFO] GET worker-user for 68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.240 o.a.s.d.s.Container SLOT_6700 [INFO] SET worker-user 68102ac7-a341-4d84-b1aa-db0f72934f99 root2021-07-21 02:03:45.241 o.a.s.d.s.Container SLOT_6700 [INFO] Creating symlinks for worker-id: 68102ac7-a341-4d84-b1aa-db0f72934f99 storm-id: PradarLogTopology-3-1626751922 for files(1): [resources]2021-07-21 02:03:45.241 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Launching worker with assignment LocalAssignment(topology_id:PradarLogTopology-3-1626751922, executors:[ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:256.0, mem_off_heap:0.0, cpu:20.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=256.0, cpu.pcore.percent=20.0}, shared_resources:{}), owner:root) for this supervisor 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86 on port 6700 with id 68102ac7-a341-4d84-b1aa-db0f72934f992021-07-21 02:03:45.243 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Launching worker with command: '/usr/local/java/bin/java' '-cp' '/data/apache-storm-2.1.0/lib-worker/*:/data/apache-storm-2.1.0/extlib/*:/data/apache-storm-2.1.0/conf:/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/stormjar.jar' '-Xmx64m' '-Dlogging.sensitivity=S3' '-Dlogfile.name=worker.log' '-Dstorm.home=/data/apache-storm-2.1.0' '-Dworkers.artifacts=/data/apache-storm-2.1.0/logs/workers-artifacts' '-Dstorm.id=PradarLogTopology-3-1626751922' '-Dworker.id=68102ac7-a341-4d84-b1aa-db0f72934f99' '-Dworker.port=6700' '-Dstorm.log.dir=/data/apache-storm-2.1.0/logs' '-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector' '-Dstorm.local.dir=/data/apache-storm-2.1.0/status' '-Dworker.memory_limit_mb=256' '-Dlog4j.configurationFile=/data/apache-storm-2.1.0/log4j2/worker.xml' 'org.apache.storm.LogWriter' '/usr/local/java/bin/java' '-server' '-Dlogging.sensitivity=S3' '-Dlogfile.name=worker.log' '-Dstorm.home=/data/apache-storm-2.1.0' '-Dworkers.artifacts=/data/apache-storm-2.1.0/logs/workers-artifacts' '-Dstorm.id=PradarLogTopology-3-1626751922' '-Dworker.id=68102ac7-a341-4d84-b1aa-db0f72934f99' '-Dworker.port=6700' '-Dstorm.log.dir=/data/apache-storm-2.1.0/logs' '-DLog4jContextSelector=org.apache.logging.log4j.core.selector.BasicContextSelector' '-Dstorm.local.dir=/data/apache-storm-2.1.0/status' '-Dworker.memory_limit_mb=256' '-Dlog4j.configurationFile=/data/apache-storm-2.1.0/log4j2/worker.xml' '-Xmx256m' '-XX:+PrintGCDetails' '-Xloggc:artifacts/gc.log' '-XX:+PrintGCDateStamps' '-XX:+PrintGCTimeStamps' '-XX:+UseGCLogFileRotation' '-XX:NumberOfGCLogFiles=10' '-XX:GCLogFileSize=1M' '-XX:+HeapDumpOnOutOfMemoryError' '-XX:HeapDumpPath=artifacts/heapdump' '-Xms2g' '-Xmx2g' '-XX:MaxDirectMemorySize=512m' '-XX:+HeapDumpOnOutOfMemoryError' '-XX:HeapDumpPath=java.hprof' '-XX:MetaspaceSize=256m' '-XX:MaxMetaspaceSize=256m' '-XX:-OmitStackTraceInFastThrow' '-Djava.library.path=/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/resources/Linux-amd64:/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/resources:/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64' '-Dstorm.conf.file=' '-Dstorm.options=' '-Djava.io.tmpdir=/data/apache-storm-2.1.0/status/workers/68102ac7-a341-4d84-b1aa-db0f72934f99/tmp' '-cp' '/data/apache-storm-2.1.0/lib-worker/*:/data/apache-storm-2.1.0/extlib/*:/data/apache-storm-2.1.0/conf:/data/apache-storm-2.1.0/status/supervisor/stormdist/PradarLogTopology-3-1626751922/stormjar.jar' 'org.apache.storm.daemon.worker.Worker' 'PradarLogTopology-3-1626751922' '8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86' '6628' '6700' '68102ac7-a341-4d84-b1aa-db0f72934f99'. 2021-07-21 02:03:45.243 o.a.s.u.Utils Thread-5 [INFO] Halting after 1 seconds2021-07-21 02:03:45.244 o.a.s.d.s.Supervisor Thread-6 [INFO] Shutting down supervisor 8cbbfd6c-961b-482d-9175-cf9b79473808-172.26.137.86
{code}"
STORM-3783,500 Server Error - Internal error processing getComponentPageInfo,"I'm still facing this issue in 2.2.0 storm ui.

org.apache.storm.thrift.TApplicationException: Internal error processing getComponentPageInfo
 at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
 at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1359)
 at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1343)
 at org.apache.storm.daemon.ui.UIHelpers.getComponentPage(UIHelpers.java:2050)
 at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyComponent(StormApiResource.java:428)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52)
 at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124)
 at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167)
 at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
 at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79)
 at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469)
 at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391)
 at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80)
 at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253)
 at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
 at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
 at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
 at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
 at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:232)
 at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:680)
 at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:392)
 at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:365)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:318)
 at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
 at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:867)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623)
 at org.apache.storm.daemon.ui.filters.HeaderResponseServletFilter.doFilter(HeaderResponseServletFilter.java:62)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
 at org.apache.storm.daemon.drpc.webapp.ReqContextFilter.handle(ReqContextFilter.java:83)
 at org.apache.storm.daemon.drpc.webapp.ReqContextFilter.doFilter(ReqContextFilter.java:70)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
 at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:46)
 at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:38)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
 at org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:311)
 at org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:265)
 at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610)
 at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
 at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1588)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)
 at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)
 at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)
 at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1557)
 at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)
 at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)
 at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)
 at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)
 at org.eclipse.jetty.server.Server.handle(Server.java:502)
 at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:364)
 at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)
 at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)
 at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)
 at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)
 at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)
 at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)
 at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)
 at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)
 at java.lang.Thread.run(Thread.java:748)

 "
STORM-3779,"killed topology worker does not removed with warn and error that ""Topology config is not localized yet...""","Hi developers,

We met critical issue when kill storm topology.

 

We killed the topology as below.
{code:java}
Config conf = new Config();
conf.put(Config.NIMBUS_SEEDS, ""SOME_NIMBUS_SEED_STRING"");
 
KillOptions opt = new KillOptions();
opt.set_wait_secs_isSet(true);
opt.set_wait_secs(10);
 
Nimbus.Iface nimbusClient = NimbusClient.getConfiguredClient(conf).getClient();
nimbusClient.killTopologyWithOpts(""TOPOLOGY_NAME"", opt);
{code}
 

Topology workers were distributed across multiple supervisors.
 Some supervisor's workers died normally.

 

But the problem is that,
h3. *Some supervisor workers never died with error message like below!!*

 
{noformat}
2021-06-29 02:58:44.284 o.a.s.d.s.Container SLOT_6707 [INFO] SET worker-user baef41a4-b5f6-4ea3-8868-5537dfba82f8 root
2021-06-29 02:58:44.284 o.a.s.d.s.Container SLOT_6707 [INFO] Creating symlinks for worker-id: baef41a4-b5f6-4ea3-8868-5537dfba82f8 storm-id: TOPOLOGY_NAME for files(1): [resources]
2021-06-29 02:58:44.284 o.a.s.d.s.BasicContainer SLOT_6707 [INFO] Launching worker with assignment LocalAssignment(topology_id:TOPOLOGY_NAME, executors:[ExecutorInfo(task_start:17, task_end:17), ExecutorInfo(task_start:29, task_end:29), ExecutorInfo(task_start:5, task_end:5)], resources:WorkerResources(mem_on_heap:6272.0, mem_off_heap:0.0, cpu:30.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=6272.0, cpu.pcore.percent=30.0}, shared_resources:{}), owner:root) for this supervisor d2ee514a-e40e-40fb-b119-59763f3bb95d-10.233.112.14 on port 6707 with id baef41a4-b5f6-4ea3-8868-5537dfba82f8
2021-06-29 02:58:44.285 o.a.s.d.s.Slot SLOT_6708 [INFO] STATE kill-and-relaunch msInState: 6 topo:TOPOLOGY_NAME worker:d06bb5c5-25e2-4557-8996-4d40045022d1 -> waiting-for-worker-start msInState: 0 topo:TOPOLOGY_NAME worker:d06bb5c5-25e2-4557-8996-4d40045022d1
2021-06-29 02:58:44.286 o.a.s.d.s.Slot SLOT_6707 [INFO] STATE kill-and-relaunch msInState: 7 topo:TOPOLOGY_NAME worker:baef41a4-b5f6-4ea3-8868-5537dfba82f8 -> waiting-for-worker-start msInState: 0 topo:TOPOLOGY_NAME worker:baef41a4-b5f6-4ea3-8868-5537dfba82f8
2021-06-29 02:58:46.799 o.a.s.d.s.BasicContainer Thread-7269 [INFO] Worker Process d06bb5c5-25e2-4557-8996-4d40045022d1 exited with code: 254
2021-06-29 02:58:48.065 o.a.s.d.s.BasicContainer Thread-7270 [INFO] Worker Process baef41a4-b5f6-4ea3-8868-5537dfba82f8 exited with code: 254
2021-06-29 02:59:09.234 o.a.s.d.s.t.SupervisorHealthCheck timer [INFO] Running supervisor healthchecks...
2021-06-29 02:59:09.234 o.a.s.h.HealthChecker timer [INFO] The supervisor healthchecks succeeded.
2021-06-29 02:59:39.234 o.a.s.d.s.t.SupervisorHealthCheck timer [INFO] Running supervisor healthchecks...
2021-06-29 02:59:39.234 o.a.s.h.HealthChecker timer [INFO] The supervisor healthchecks succeeded.
2021-06-29 02:59:53.558 o.a.s.d.s.Supervisor pool-11-thread-9 [INFO] Got an assignments from master, will start to sync with assignments: SupervisorAssignments(...)
2021-06-29 02:59:53.936 o.a.s.d.s.Slot SLOT_6702 [INFO] SLOT 6702: Assignment Changed from LocalAssignment(topology_id:TOPOLOGY_NAME, executors:[ExecutorInfo(task_start:23, task_end:23), ExecutorInfo(task_start:11, task_end:11)], resources:WorkerResources(mem_on_heap:3200.0, mem_off_heap:0.0, cpu:20.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=3200.0, cpu.pcore.percent=20.0}, shared_resources:{}), owner:root) to null
2021-06-29 02:59:53.939 o.a.s.d.s.Container SLOT_6702 [INFO] Killing d2ee514a-e40e-40fb-b119-59763f3bb95d-10.233.112.14:25976cac-9170-44ec-b835-099377cda893
2021-06-29 02:59:54.293 o.a.s.d.s.Slot SLOT_6708 [INFO] SLOT 6708: Assignment Changed from LocalAssignment(topology_id:TOPOLOGY_NAME, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:22, task_end:22)], resources:WorkerResources(mem_on_heap:3200.0, mem_off_heap:0.0, cpu:20.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=3200.0, cpu.pcore.percent=20.0}, shared_resources:{}), owner:root) to null
2021-06-29 02:59:54.293 o.a.s.d.s.Slot SLOT_6707 [INFO] SLOT 6707: Assignment Changed from LocalAssignment(topology_id:TOPOLOGY_NAME, executors:[ExecutorInfo(task_start:17, task_end:17), ExecutorInfo(task_start:29, task_end:29), ExecutorInfo(task_start:5, task_end:5)], resources:WorkerResources(mem_on_heap:6272.0, mem_off_heap:0.0, cpu:30.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=6272.0, cpu.pcore.percent=30.0}, shared_resources:{}), owner:root) to null
2021-06-29 02:59:54.296 o.a.s.d.s.Slot SLOT_6708 [INFO] STATE waiting-for-worker-start msInState: 70011 topo:TOPOLOGY_NAME worker:d06bb5c5-25e2-4557-8996-4d40045022d1 -> kill msInState: 0 topo:TOPOLOGY_NAME worker:d06bb5c5-25e2-4557-8996-4d40045022d1
2021-06-29 02:59:54.296 o.a.s.d.s.Slot SLOT_6707 [INFO] STATE waiting-for-worker-start msInState: 70010 topo:TOPOLOGY_NAME worker:baef41a4-b5f6-4ea3-8868-5537dfba82f8 -> kill msInState: 0 topo:TOPOLOGY_NAME worker:baef41a4-b5f6-4ea3-8868-5537dfba82f8
2021-06-29 02:59:54.298 o.a.s.d.s.Slot SLOT_6708 [INFO] SLOT 6708 all processes are dead...
2021-06-29 02:59:54.298 o.a.s.d.s.Container SLOT_6708 [INFO] Cleaning up d2ee514a-e40e-40fb-b119-59763f3bb95d-10.233.112.14:d06bb5c5-25e2-4557-8996-4d40045022d1
2021-06-29 02:59:54.298 o.a.s.d.s.AdvancedFSOps SLOT_6708 [INFO] Deleting path /storm/workers/d06bb5c5-25e2-4557-8996-4d40045022d1/pids/141225
2021-06-29 02:59:54.298 o.a.s.d.s.AdvancedFSOps SLOT_6708 [INFO] Deleting path /storm/workers/d06bb5c5-25e2-4557-8996-4d40045022d1/heartbeats
2021-06-29 03:00:06.452 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/stormjar.jar
2021-06-29 03:00:06.472 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/stormjar.jar.version
2021-06-29 03:00:06.472 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/resources
2021-06-29 03:00:06.472 o.a.s.l.LocalizedResourceRetentionSet AsyncLocalizer Task Executor - 1 [INFO] Deleted blob: TOPOLOGY_NAME-stormjar.jar (REMOVED FROM CLUSTER).
2021-06-29 03:00:06.475 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/stormconf.ser
2021-06-29 03:00:06.475 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/stormconf.ser.version
2021-06-29 03:00:06.475 o.a.s.l.LocalizedResourceRetentionSet AsyncLocalizer Task Executor - 1 [INFO] Deleted blob: TOPOLOGY_NAME-stormconf.ser (REMOVED FROM CLUSTER).
2021-06-29 03:00:06.477 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/stormcode.ser
2021-06-29 03:00:06.477 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME/stormcode.ser.version
2021-06-29 03:00:06.478 o.a.s.l.LocalizedResourceRetentionSet AsyncLocalizer Task Executor - 1 [INFO] Deleted blob: TOPOLOGY_NAME-stormcode.ser (REMOVED FROM CLUSTER).
2021-06-29 03:00:06.478 o.a.s.d.s.AdvancedFSOps AsyncLocalizer Task Executor - 1 [INFO] Deleting path /storm/supervisor/stormdist/TOPOLOGY_NAME
2021-06-29 03:00:07.062 o.a.s.d.s.Supervisor pool-11-thread-10 [WARN] Topology config is not localized yet...
2021-06-29 03:00:07.063 o.a.s.t.ProcessFunction pool-11-thread-10 [ERROR] Internal error processing sendSupervisorWorkerHeartbeat
org.apache.storm.utils.WrappedNotAliveException: TOPOLOGY_NAME does not appear to be alive, you should probably exit
        at org.apache.storm.daemon.supervisor.Supervisor$1.sendSupervisorWorkerHeartbeat(Supervisor.java:448) ~[storm-server-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:374) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:353) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:172) [storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:524) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
        at java.lang.Thread.run(Unknown Source) [?:?]
2021-06-29 03:00:07.064 o.a.s.t.ProcessFunction pool-11-thread-3 [ERROR] Internal error processing sendSupervisorWorkerHeartbeat
org.apache.storm.utils.WrappedNotAliveException: TOPOLOGY_NAME does not appear to be alive, you should probably exit
        at org.apache.storm.daemon.supervisor.Supervisor$1.sendSupervisorWorkerHeartbeat(Supervisor.java:448) ~[storm-server-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:374) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:353) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:172) [storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:524) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
        at java.lang.Thread.run(Unknown Source) [?:?]
2021-06-29 03:00:08.106 o.a.s.d.s.Supervisor pool-11-thread-9 [WARN] Topology config is not localized yet...
2021-06-29 03:00:08.107 o.a.s.t.ProcessFunction pool-11-thread-9 [ERROR] Internal error processing sendSupervisorWorkerHeartbeat
org.apache.storm.utils.WrappedNotAliveException: TOPOLOGY_NAME does not appear to be alive, you should probably exit
        at org.apache.storm.daemon.supervisor.Supervisor$1.sendSupervisorWorkerHeartbeat(Supervisor.java:448) ~[storm-server-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:374) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:353) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:172) [storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:524) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:?]
        at java.lang.Thread.run(Unknown Source) [?:?]
2021-06-29 03:00:08.108 o.a.s.d.s.Supervisor pool-11-thread-16 [WARN] Topology config is not localized yet...
2021-06-29 03:00:08.108 o.a.s.t.ProcessFunction pool-11-thread-16 [ERROR] Internal error processing sendSupervisorWorkerHeartbeat
org.apache.storm.utils.WrappedNotAliveException: TOPOLOGY_NAME does not appear to be alive, you should probably exit
        at org.apache.storm.daemon.supervisor.Supervisor$1.sendSupervisorWorkerHeartbeat(Supervisor.java:448) ~[storm-server-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:374) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.generated.Supervisor$Processor$sendSupervisorWorkerHeartbeat.getResult(Supervisor.java:353) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:38) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:172) [storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:524) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:?]{noformat}
*This error message repeated forever until we killed that worker process.*

 

 

 "
STORM-3773,Worker Reassignment - Difference between Storm 2.x  and Storm 1.x,"We are currently on Storm 1.2.1 and was in the process of upgrading it to Storm 2.2.0
 Observed the below while upgrading it to 2.2.0:

1) In a storm cluster (4 nodes) with 8 topologies running  ( with a mapping of 1-1 between worker and topologies), when i bring down nimbus,supervisor in one of the node (let's say Node 1, which is not nimbus leader) the workers running on that node gets reassigned to other 3, even though it is running on that node (Node 1). So i have 2 worker process for the same topology running at the same time ( saw the behaviour with or without using pacemaker). The worker process does get killed when nimbus and supervisor is brought up in Node 1

2) Observed from worker logs that it sends heartbeat to local supervisor and nimbus leader , which with 1.2.1 used to happen using Zookeeper ( i saw this behaviour in 2.2.0 with or without using Pacemaker). 
 If i bring down nimbus and supervisor on node where nimbus is a leader, it reassigns worker processes and in some cases leads to zombie worker processess ( is not killed when storm kill is executed)

These above behaviour (reassignment of worker) doesn't happen with Storm 1.2.1

Since this is a fundamental design change between 1.x and 2.x , are there any documentation which describes it in detail? ( couldn't find from Release Notes)

(I am raising this as a bug because its preventing us from moving to 2.2.0 due to the issue mentioned in 2) )

 "
STORM-3772,"With pacemaker, zNode /strom/workerbeats entries are not removed once topology is killed","With Pacemaker storage strategy, 

When topology is submitted, there is entry created inside zNode /strom/workerbeats. But same is not deleted when topology is killed. Because of this we are ending up having all old topology entries in this zNode.

Since pacemaker is taking care of worker heartbeat, shouldn't we stop adding entries in zNode /strom/workerbeats when topology submitted in case of pacemaker storage strategy?"
STORM-3770,Workaround for Issue STORM-3677 (assignment was null) in version 2.2.0,"Please suggest workaround for the issue STORM-3677 in version 2.2.0.

https://issues.apache.org/jira/browse/STORM-3677

 

Is there any other way to kill stale/orphan worker?



We are getting below exception-
2021-05-10 10:42:36.369 o.a.s.d.w.WorkerState refresh-connections-timer [WARN] Failed to read assignment. This should only happen when topology is shutting down.
 java.lang.RuntimeException: Failed to read worker assignment. Supervisor client threw exception, and *assignment in Zookeeper was null*
 at org.apache.storm.daemon.worker.WorkerState.getLocalAssignment(WorkerState.java:665) ~[storm-client-2.2.0.jar:2.2.0]
 at org.apache.storm.daemon.worker.WorkerState.refreshConnections(WorkerState.java:389) ~[storm-client-2.2.0.jar:2.2.0]
 
Worker is not listed in Storm UI, hence we are doing manual intervention every time to kill the process. 
As process is not releasing port, there is topology imbalance happening and load is going to only few supervisors. Attached screenshot."
STORM-3768,Zero Values for Assigned Mem (MB) in Topology Summary and Used Mem (MB) in Supervisor Summary,"We have recently built storm 2.3.0-SNAPSHOT from git with commit hash 4a1eea700766da2f175ac7eaba6064f0d7f0ff03 because we needed the bugfix in STORM-3652.

It seems in this built information about the sum of the used memory is lost (it was working before in 2.2.0, release version). Individual topologies/components still get their memory assignment displayed, but the total is missing. This affects the root view (*index.html*) of the UI, where information is missing in the *supervisor section* (displayed value for used mem is zero), as well as the topology view (*topology.html*, displayed value for assigned mem in *topology summary* is zero)."
STORM-3767,NPE on getComponentPendingProfileActions ,"When a topology is newly submitted, if the scheduling loop takes too long, the component UI might have error 500.

This is due to the NPE in nimbus code. An example:

1. When a scheduling loop finishes, nimbus will eventually update the assignmentsBackend. if a topology is newly submitted, its entry will be added to the idToAssignment map, otherwise, the entry will be updated with new assignments. The key point is the new topology Id doesn't exist in idToAssignment before it reaching here.

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2548-L2549
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java#L696
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/assignments/InMemoryAssignmentBackend.java#L63-L64

2. However, this assignmentsBackend update only started to happen at 2021-04-23 15:30:14.299


{code:java}
2021-04-23 15:30:14.299 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology
{code}

while this topology topo1-52-1619191499 has been scheduled at 2021-04-23 15:25:13.887. The scheduling loop took longer than 5mins.


{code:java}
2021-04-23 15:25:13.887 o.a.s.s.Cluster timer [INFO] STATUS - topo1-52-1619191499 Running - Fully Scheduled by DefaultResourceAwareStrategy (1297 states traversed in 1275 ms, backtracked 0 times)
other topologies were taking long time

2021-04-23 15:25:14.378 o.a.s.s.Cluster timer [INFO] STATUS - topo2-76-1612842912 Running - Fully Scheduled by DefaultResourceAwareStrategy (111 states traversed in 34 ms, backtracked 0 times)
...
2021-04-23 15:30:14.192 o.a.s.s.Cluster timer [INFO] STATUS - TrendingNowLES-11-1611713968 Not enough resources to schedule after evicting lower priority topologies. Additional Memory Required: 20128.0 MB (Available: 5411178.0 MB). Additional CPU Required: 1010.0% CPU (Available: 3100.0 % CPU).Cannot schedule by DefaultResourceAwareStrategy (65644 states traversed in 299804 ms, backtracked 65555 times, 89 of 150 executors scheduled)
...
2021-04-23 15:30:14.216 o.a.s.s.Cluster timer [INFO] STATUS - evaluateplus-dev-47-1605825401 Running - Fully Scheduled by GenericResourceAwareStrategy (41 states traversed in 10 ms, backtracked 0 times)
{code}

3. During this period, the idToAssignment map in assignmentsBackend wouldn't have the entry for topo1-52-1619191499, so when a component UI was visited,

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L3613-L3614
https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L3100
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java#L194
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/assignments/InMemoryAssignmentBackend.java#L69

it got a null value as the assignment, and hence NPE.

This can be produced easily by adding some sleep anywhere between 

{code:title=Nimbus.java}
            Map<String, SchedulerAssignment> newSchedulerAssignments =
                    computeNewSchedulerAssignments(existingAssignments, topologies, bases, scratchTopoId);
{code}

and
{code:title=Nimbus.java}
 state.setAssignment(topoId, assignment, td.getConf());
{code}

and submit a new topology and visit its component UI 
"
STORM-3765,NPE in DRPCSimpleACLAuthorizer.readAclFromConfig when drpc.authorizer.acl has no values,"When drpc.authorizer.acl has no values, for example:

{code:java}
-bash-4.2$ cat  drpc-auth-acl.yaml
drpc.authorizer.acl:
{code}

DRPCSimpleACLAuthorizer will have NPE
{code:java}
2021-04-22 15:22:48.795 o.a.s.t.ProcessFunction pool-9-thread-1 [ERROR] Internal error processing fetchRequest
java.lang.NullPointerException: null
        at org.apache.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer.readAclFromConfig(DRPCSimpleACLAuthorizer.java:59) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer.permitClientOrInvocationRequest(DRPCSimpleACLAuthorizer.java:108) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer.permitInvocationRequest(DRPCSimpleACLAuthorizer.java:150) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.authorizer.DRPCAuthorizerBase.permit(DRPCAuthorizerBase.java:51) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPC.checkAuthorization(DRPC.java:130) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPC.checkAuthorizationNoLog(DRPC.java:143) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPC.fetchRequest(DRPC.java:192) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.drpc.DRPCThrift.fetchRequest(DRPCThrift.java:42) ~[storm-server-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.generated.DistributedRPCInvocations$Processor$fetchRequest.getResult(DistributedRPCInvocations.java:393) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.generated.DistributedRPCInvocations$Processor$fetchRequest.getResult(DistributedRPCInvocations.java:372) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:152) [storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_262]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Th
{code}
"
STORM-3763,Backpressure message ignored by the receiver caused the topology to not progress,"We have noticed a case where topology is stuck due to the mis-interpretation of backpressure messge:

At beginning, the topology ran fine but a downstream component had backpressure, so it sent backpressure signal to its upstream component, and the upstream component paused sending data to the downstream bolt.
Then the downstream component restarted (due to any reason, for example, killed by supervisor due to heartbeat timeout). When it came back up, it sends backpressure message to the upstream bolt. However, the upstream component didn't know how to interpret the backpressure message so it logs the below error and ignores the message.  
{code:java}
2021-01-28 19:41:37.175 o.a.s.m.n.SaslStormClientHandler client-worker-1 [ERROR] Unexpected message from server: {worker=4c38160a-3c66-4eff-8572-2d0c493bd6c1, bpStatusId=254, bpTasks=[], nonBpTasks=[546, 790, 863]}
{code}

Then the downstream component will not receive any data from the upstream component, so it won't have any backpressure (since no data is sent to it), hence it won't send any backpressure update message to the upstream component.  This leads to a dead situation that the upstream component thinks the downstream has backpressure so it paused sending data to it, while the downstream doesn't have backpressure but can't receive any data from upstream. The topology is stuck because of it.


Let's look at the code:

When the connection between the downstream (server) and upstream (client) is established,
server invokes

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/StormServerHandler.java#L39-L41

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java#L237

which sends backpressure messages to the client.

This is because in this pipeline, ""StormServerHandler"" is the only one with that implemented ""channelActive()"" method.

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/StormServerPipelineFactory.java#L56

However, the Client side expects authentication messages.

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/SaslStormClientHandler.java#L70-L75

so the client can't interpret the backpressure message at the beginning, hence ""unexpected message"".

This can be supported with an example. I have a wordcount topology running. At the startup, the client tries to connect to the server. Once connected, it sends a ""SASL_TOKEN_MESSAGE_REQUEST"".

client log

{code:java}
021-01-29 19:03:21.355 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] SASL credentials for storm topology wc is -8603731884381183101:-9091319821854384981
2021-01-29 19:03:21.359 o.a.s.m.n.Client client-worker-1 [DEBUG] successfully connected to openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702, [id: 0x29da2e9c, L:/10.215.73.209:45870 - R:openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702] [attempt 12]
2021-01-29 19:03:21.359 o.a.s.m.n.SaslStormClientHandler client-worker-1 [INFO] Connection established from /10.215.73.209:45870 to openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702
...
2021-01-29 19:03:21.362 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] Creating saslNettyClient now for channel: [id: 0x29da2e9c, L:/10.215.73.209:45870 - R:openstorm14blue-n3.blue.ygrid.yahoo.com/10.215.73.209:6702]
2021-01-29 19:03:21.363 o.a.s.m.n.SaslNettyClient client-worker-1 [DEBUG] SaslNettyClient: Creating SASL DIGEST-MD5 client to authenticate to server
2021-01-29 19:03:21.368 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] Sending SASL_TOKEN_MESSAGE_REQUEST

...

2021-01-29 19:03:21.632 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] send/recv time (ms): 277
2021-01-29 19:03:21.633 o.a.s.m.n.SaslStormClientHandler client-worker-1 [ERROR] Unexpected message from server: {worker=cdf6f963-678c-45a4-91d2-e1067a9a8516, bpStatusId=1, bpTasks=[], nonBpTasks=[17, 1, 18, 3, 4, 22, 7, 8, 9, 12,
13]}
{code}

But the server sends the backpressure message first, before it deals with the SASL_TOKEN_MESSAGE_REQUEST message

server log

{code:java}
2021-01-29 19:03:21.473 o.a.s.m.n.SaslStormServerHandler Netty-server-localhost-6702-worker-1 [DEBUG] SASL credentials for storm topology wc is -8603731884381183101:-9091319821854384981
2021-01-29 19:03:21.482 o.a.s.u.Utils main [DEBUG] Using storm.yaml from resources
2021-01-29 19:03:21.490 o.a.s.d.w.WorkerState Netty-server-localhost-6702-worker-1 [INFO] Sending BackPressure status to new client. BPStatus: {worker=cdf6f963-678c-45a4-91d2-e1067a9a8516, bpStatusId=1, bpTasks=[], nonBpTasks=[17,
1, 18, 3, 4, 22, 7, 8, 9, 12, 13]}
2021-01-29 19:03:21.510 o.a.s.m.n.SaslStormClientHandler client-worker-1 [DEBUG] SASL credentials for storm topology wc is -8603731884381183101:-9091319821854384981
2021-01-29 19:03:21.572 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.maxCapacityPerThread: 4096
2021-01-29 19:03:21.573 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.maxSharedCapacityFactor: 2
2021-01-29 19:03:21.573 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.linkCapacity: 16
2021-01-29 19:03:21.574 o.a.s.s.i.n.u.Recycler Netty-server-localhost-6702-worker-1 [DEBUG] -Dio.netty.recycler.ratio: 8
2021-01-29 19:03:21.575 o.a.s.v.ConfigValidation main [WARN] topology.backpressure.enable is a deprecated config please see class org.apache.storm.Config.TOPOLOGY_BACKPRESSURE_ENABLE for more information.
2021-01-29 19:03:21.593 o.a.s.s.i.n.b.AbstractByteBuf Netty-server-localhost-6702-worker-1 [DEBUG] -Dorg.apache.storm.shade.io.netty.buffer.checkAccessible: true
2021-01-29 19:03:21.594 o.a.s.s.i.n.b.AbstractByteBuf Netty-server-localhost-6702-worker-1 [DEBUG] -Dorg.apache.storm.shade.io.netty.buffer.checkBounds: true
2021-01-29 19:03:21.594 o.a.s.s.i.n.u.ResourceLeakDetectorFactory Netty-server-localhost-6702-worker-1 [DEBUG] Loaded default ResourceLeakDetector: org.apache.storm.shade.io.netty.util.ResourceLeakDetector@524a134b


....

2021-01-29 19:03:21.695 o.a.s.m.n.SaslStormServerHandler Netty-server-localhost-6702-worker-1 [DEBUG] No saslNettyServer for [id: 0x6fa65bc5, L:/10.215.73.209:6702 - R:/10.215.73.209:45870] yet; creating now, with topology token: wc
2021-01-29 19:03:21.697 o.a.s.m.n.SaslNettyServer Netty-server-localhost-6702-worker-1 [DEBUG] SaslNettyServer: Topology token is: wc with authmethod DIGEST-MD5
2021-01-29 19:03:21.698 o.a.s.m.n.SaslNettyServer Netty-server-localhost-6702-worker-1 [DEBUG] SaslDigestCallback: Creating SaslDigestCallback handler with topology token: wc
{code}

https://github.com/apache/storm/blob/2.2.x-branch/storm-client/src/jvm/org/apache/storm/messaging/netty/SaslStormServerHandler.java#L46-L52

This is a bug likely introduced in STORM-2306 (https://github.com/apache/storm/pull/2502). This willl happen on every topology when ""storm.messaging.netty.authentication"" is set true (It is false by default)"
STORM-3760,Storm 2.2.0 not reporting newWorkerEvents metric,"Hi everyone,
  
 We have recently migrated from Storm 0.10.0 to Storm 2.2.0, we have a custom _StatsdMetricConsumer_ which implements _IMetricsConsumer_ interface. 
  
 Storm is still reporting some metrics (__transfer-count,_ _ack-count,_ _metrics, etc)_ but it seems after the migration it stopped reporting _*newWorkerEvents*_ metric. I made sure this is not a problem in our implementation by logging all the metrics received _handleDataPoints_ method.
  
 Is this a known issue? any way to get that metric fixed?

Best regards, thanks."
STORM-3757,Update jackson version to 2.10.0,Update jackson version to 2.10.0 to avoid CVE-2019-14892 and CVE-2019-14893
STORM-3752,Nimbus can't be run in MacOSX 10.13.x,"When starting nimbus on MacOSX 10.13.6 (High Sierra), the following error appears:
  
{code:java}
dyld: lazy symbol binding failed: Symbol not found: ____chkstk_darwin
 Referenced from: /private/var/folders/14/tncwm2kx3rl28872rmzx_l9w0000gn/T/librocksdbjni4444014704764445285.jnilib (which was built for Mac OS X 10.15)
 Expected in: /usr/lib/libSystem.B.dylib
dyld: Symbol not found: ____chkstk_darwin
 Referenced from: /private/var/folders/14/tncwm2kx3rl28872rmzx_l9w0000gn/T/librocksdbjni4444014704764445285.jnilib (which was built for Mac OS X 10.15)
 Expected in: /usr/lib/libSystem.B.dylib
Abort trap: 6
{code}
Even when trying to build from the sources, and starting nimbus from the final binary distribution, the error still appears.

Is it possible to provide a binary for older versions of MacOSX? This is a blocking issue.
  "
STORM-3751,NPE in WorkerState.transferLocalBatch,"Hello,

 

I've recently upgraded to Storm 2.2.0 and have been getting this error:

 
{code:java}
2021-03-07 04:36:51.061 o.a.s.m.n.StormServerHandler Netty-server-localhost-6700-worker-1 [ERROR] server errors in handling the request
java.lang.NullPointerException: null
        at org.apache.storm.daemon.worker.WorkerState.transferLocalBatch(WorkerState.java:543) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:71) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.messaging.netty.Server.enqueue(Server.java:146) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.messaging.netty.Server.received(Server.java:264) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.messaging.netty.StormServerHandler.channelRead(StormServerHandler.java:51) ~[storm-client-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:297) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.2.0.jar:2.2.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_272]
2021-03-07 04:36:51.061 o.a.s.m.n.StormServerHandler Netty-server-localhost-6700-worker-1 [INFO] Received error in netty thread.. terminating server... {code}
 
This issue happens every 20-30 minutes and causes the workers to die/restart.

It seems related to https://issues.apache.org/jira/browse/STORM-3141 but seems to have been fixed in 2.0. 

I am happy to provide more information but at the moment am unsure of what is relevant.

I have a suspicion that this is related to load-aware localOrShuffleGrouping (""LoadAwareShuffleGrouping"") because this issue seems to have started when I switched the Grouping, but again, not sure if it's actually related."
STORM-3750,Deactivation throws java.nio.channels.ClosedSelectorException in KafkaSpout,"When deactivating a topology that uses a kafka spout, the following exception is thrown:
{code:java}
java.lang.RuntimeException: java.nio.channels.ClosedSelectorExceptionjava.lang.RuntimeException: java.nio.channels.ClosedSelectorException at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.daemon.executor$fn__10727$fn__10742$fn__10773.invoke(executor.clj:634) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [storm-core-1.2.2.jar:1.2.2] at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]Caused by: java.nio.channels.ClosedSelectorException at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:83) ~[?:1.8.0_181] at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97) ~[?:1.8.0_181] at org.apache.kafka.common.network.Selector.select(Selector.java:499) ~[stormjar.jar:?] at org.apache.kafka.common.network.Selector.poll(Selector.java:308) ~[stormjar.jar:?] at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:349) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:226) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:188) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.Fetcher.retrieveOffsetsByTimes(Fetcher.java:408) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.Fetcher.beginningOrEndOffset(Fetcher.java:451) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.internals.Fetcher.beginningOffsets(Fetcher.java:436) ~[stormjar.jar:?] at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1473) ~[stormjar.jar:?] at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[stormjar.jar:?] at org.apache.storm.daemon.executor$metrics_tick$fn__10651.invoke(executor.clj:345) ~[storm-core-1.2.2.jar:1.2.2] at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?] at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?] at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?] at clojure.core$filter$fn__4580.invoke(core.clj:2679) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?] at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?] at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.7.0.jar:?] at clojure.lang.RT.next(RT.java:674) ~[clojure-1.7.0.jar:?] at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?] at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?] at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?] at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.daemon.executor$fn__10727$tuple_action_fn__10733.invoke(executor.clj:522) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.daemon.executor$mk_task_receiver$fn__10716.invoke(executor.clj:471) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.disruptor$clojure_handler$reify__10135.onEvent(disruptor.clj:41) ~[storm-core-1.2.2.jar:1.2.2] at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.2.jar:1.2.2] ... 7 more{code}
Problem with that is that it leads to the worker dying. "
STORM-3743,Add new topologies for TestLargeCluster,Add new set of topologies and a more nuanced Supervisor creation for TestLargeCluster.
STORM-3742,Topology arguments starting with -c get removed," with the following command

{{{{storm local target/stormcrawler-1.0-SNAPSHOT.jar global.climateemergency.ESCrawlTopology -conf crawler-conf.yaml -conf es-conf.yaml . seeds.txt}}}}

the topology class is only getting [crawler-conf.yaml, es-conf.yaml, ., seeds.txt]and the -conf have been removed.

Anything after the class name should be left as-is. 
This does not happen with Storm 1.2.3

 

 

{{{{}}}}"
STORM-3737,Share Worker Metric Registry For Guice AOP Based Metrics Integeration,"Metric Registry has been made private which makes it harder to integrate with Guice based AOP metrics.

Proposed solve is to add metric registry created in the worker to SharedMetricRegistries so while intializing guice based AOP metrics it can be done in worker hook
 [https://github.com/palominolabs/metrics-guice]

 

PR:
https://github.com/apache/storm/pull/3373"
STORM-3736,remove topologyId and worker port from V2 metrics API,the topologyId and port are now available to the StormMetricsRegistry and should be removed from the existing metric API
STORM-3735,Kyro serialization fails on some metric tuples when topology.fall.back.on.java.serialization is false,"When a metric consumer is used, metrics will be sent from all executors to the consumer. In some of the metrics,  it includes NodeInfo object, and kryo serialization will fail if topology.fall.back.on.java.serialization is false.

{code:title=worker logs}
2021-01-13 20:16:37.017 o.a.s.e.ExecutorTransfer Thread-16-__system-executor[-1, -1] [INFO] TRANSFERRING tuple [dest: 5 tuple: source: __system:-1, stream: __metrics, id: {}, [TASK_INFO: { host: openstorm14blue-n4.blue.ygrid.yahoo.com:6703 comp: __system[-1]}, [
[CGroupCpuStat = {nr.throttled-percentage=46.544980443285525, nr.period-count=767, nr.throttled-count=357, throttled.time-ms=27208}], [CGroupMemoryLimit = 1342177280], [__recv-iconnection = {dequeuedMessages=0, enqueued={/10.215.73.210:47038=3169}}], [__send-ico
nnection = {NodeInfo(node:149a917b-bc75-49c8-b351-f74b8ae0fbed-10.215.73.210, port:[6701])={reconnects=1, src=/10.215.73.210:34938, pending=0, dest=openstorm14blue-n4.blue.ygrid.yahoo.com/10.215.73.210:6701, sent=1896, lostOnSend=0}, NodeInfo(node:149a917b-bc75-
49c8-b351-f74b8ae0fbed-10.215.73.210, port:[6702])={reconnects=8, src=/10.215.73.210:39476, pending=0, dest=openstorm14blue-n4.blue.ygrid.yahoo.com/10.215.73.210:6702, sent=2115, lostOnSend=0}, NodeInfo(node:b77b5ec6-15ee-4bd2-a9b8-12fcadde7744-10.215.73.211, po
rt:[6700])={reconnects=125, pending=0, dest=openstorm14blue-n5.blue.ygrid.yahoo.com/10.215.73.211:6700, sent=108, lostOnSend=1331}}], [CGroupMemory = 316485632], [CGroupCpu = {user-ms=36960, sys-ms=25860}], [memory.pools.Metaspace.usage = 0.9695890907929322], [m
emory.heap.max = 1073741824], [receive-queue-overflow = 0], [memory.pools.Compressed-Class-Space.used = 6237424], [memory.pools.Compressed-Class-Space.max = 1073741824], [memory.non-heap.init = 2555904], [worker-transfer-queue-overflow = 0], [memory.pools.Metasp
ace.committed = 42074112], [receive-queue-sojourn_time_ms = 0.0], [threads.waiting.count = 5], [memory.pools.G1-Eden-Space.usage = 0.2777777777777778], [memory.pools.Metaspace.used = 40798320], [memory.total.used = 101783888], [memory.pools.Code-Cache.init = 255
5904], [memory.non-heap.committed = 63832064], [GC.G1-Young-Generation.time = 677], [receive-queue-insert_failures = 0.0], [memory.total.init = 130482176], [GC.G1-Old-Generation.count = 0], [memory.pools.Metaspace.init = 0], [memory.pools.G1-Survivor-Space.commi
tted = 5242880], [worker-transfer-queue-population = 0], [memory.pools.Compressed-Class-Space.committed = 6684672], [threads.timed_waiting.count = 31], [memory.pools.G1-Eden-Space.init = 7340032], [memory.pools.Metaspace.max = -1], [memory.pools.G1-Survivor-Spac
e.used = 5242880], [memory.heap.init = 127926272], [memory.pools.G1-Old-Gen.used-after-gc = 0], [worker-transfer-queue-capacity = 1024], [memory.pools.G1-Survivor-Space.used-after-gc = 5242880], [memory.pools.G1-Old-Gen.committed = 47185920], [memory.pools.G1-Ed
en-Space.committed = 75497472], [receive-queue-arrival_rate_secs = 0.109421162052741], [memory.pools.Compressed-Class-Space.usage = 0.0058090537786483765], [TGT-TimeToExpiryMsecs = 71282993], [threads.runnable.count = 15], [worker-transfer-queue-insert_failures
= 0.0], [worker-transfer-queue-sojourn_time_ms = 0.0], [memory.heap.committed = 127926272], [memory.non-heap.max = -1], [threads.daemon.count = 29], [memory.pools.Code-Cache.max = 251658240], [worker-transfer-queue-arrival_rate_secs = 90.47776674390379], [memory
.heap.usage = 0.037109360098838806], [memory.pools.G1-Old-Gen.init = 120586240], [memory.pools.Code-Cache.committed = 15138816], [receive-queue-pct_full = 0.0], [worker-transfer-queue-pct_full = 0.0], [receive-queue-population = 0], [memory.pools.Compressed-Clas
s-Space.init = 0], [memory.pools.Code-Cache.usage = 0.059299468994140625], [worker-transfer-queue-dropped_messages = 0], [GC.G1-Young-Generation.count = 18], [memory.pools.Code-Cache.used = 14923200], [memory.pools.G1-Old-Gen.usage = 0.012695297598838806], [memo
ry.non-heap.usage = -6.196368E7], [memory.total.max = 1073741823], [threads.count = 51], [memory.heap.used = 39845872], [memory.pools.G1-Survivor-Space.init = 0], [memory.pools.G1-Old-Gen.used = 13631472], [receive-queue-dropped_messages = 0], [threads.terminate
d.count = 0], [memory.pools.G1-Eden-Space.max = -1], [uptimeSecs = 76], [threads.deadlock.count = 0], [threads.blocked.count = 0], [newWorkerEvent = 1], [receive-queue-capacity = 32768], [threads.new.count = 0], [startTimeSecs = 1610568920], [memory.pools.G1-Ede
n-Space.used-after-gc = 0], [memory.pools.G1-Eden-Space.used = 20971520], [GC.G1-Old-Generation.time = 0], [memory.non-heap.used = 61964384], [memory.pools.G1-Old-Gen.max = 1073741824], [memory.pools.G1-Survivor-Space.max = -1], [memory.pools.G1-Survivor-Space.u
sage = 1.0], [memory.total.committed = 191823872], [doHeartbeat-calls.count = 64], [doHeartbeat-calls.m1_rate = 1.0730202200365234E-6], [doHeartbeat-calls.m5_rate = 1.1636999000665182E-6], [doHeartbeat-calls.m15_rate = 1.1870955900857726E-6], [doHeartbeat-calls.
mean_rate = 1.0067076836696486E-6]]] PROC_START_TIME(sampled): null EXEC_START_TIME(sampled): null]

...

2021-01-13 20:16:37.030 o.a.s.u.Utils Thread-16-__system-executor[-1, -1] [ERROR] Async loop died!
java.lang.RuntimeException: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at org.apache.storm.executor.Executor.accept(Executor.java:294) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:113) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:89) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:145) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.Utils$1.run(Utils.java:401) [storm-client-2.3.0.y.jar:2.3.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at org.apache.storm.executor.Executor.accept(Executor.java:294) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:113) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:89) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:145) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.utils.Utils$1.run(Utils.java:401) [storm-client-2.3.0.y.jar:2.3.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
Serialization trace:
value (org.apache.storm.metric.api.IMetricsConsumer$DataPoint)
        at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[kryo-3.0.3.jar:?]
        at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:38) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:40) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:118) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:553) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:68) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.Task.sendUnanchored(Task.java:215) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.metricsTick(Executor.java:345) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:205) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.accept(Executor.java:290) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        ... 6 more
Caused by: java.lang.IllegalArgumentException: Class is not registered: org.apache.storm.generated.NodeInfo
Note: To register this class use: kryo.register(org.apache.storm.generated.NodeInfo.class);
        at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:488) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:622) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:106) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:628) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:100) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:534) ~[kryo-3.0.3.jar:?]
        at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:38) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:40) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:118) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:553) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:68) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.daemon.Task.sendUnanchored(Task.java:215) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.metricsTick(Executor.java:345) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:205) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        at org.apache.storm.executor.Executor.accept(Executor.java:290) ~[storm-client-2.3.0.y.jar:2.3.0.y]
        ... 6 more
{code}

The related metric is ""__send-iconnection"" from https://github.com/apache/storm/blob/7bef73a6faa14558ef254efe74cbe4bfef81c2e2/storm-client/src/jvm/org/apache/storm/daemon/metrics/BuiltinMetricsUtil.java#L40-L43

Note that this can only be reproduced when metrics are sent across workers (otherwise there is no serialization).

The work around is one of the following
1) add org.apache.storm.generated.NodeInfo to topology.kryo.register in topology conf
2) set topology.fall.back.on.java.serialization true or unset topology.fall.back.on.java.serialization since the default is true


The fix is to register NodeInfo class in kryo.
https://github.com/apache/storm/blob/7bef73a6faa14558ef254efe74cbe4bfef81c2e2/storm-client/src/jvm/org/apache/storm/serialization/SerializationFactory.java#L67-L77
"
STORM-3729,Assigning memory greater and equal than 2048m will make assgin memory for slot values to 1m,"Hi, everyone.

I set my topology over 2048m, the storm ui shows only 65m, so i found its error in [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java]  line 1089 that value cast to int instead of long, It goes wrong if I pass 2048m and results 1m.

Simply change this line to cast Long can solve this problem.:) "
STORM-3728,Workers are not able to connect to Pacemaker if pacemaker.auth.method is KERBEROS,"When pacemaker.auth.method is KERBEROS,  worker will fail to connect to KERBEROS because of exceptions like the following:
 
{code:java}
2020-12-21 20:07:00.786 o.a.s.c.PaceMakerStateStorage executor-heartbeat-timer [ERROR] Timed out waiting for channel ready. Failed to set_worker_hb. Will make 2 more attempts.
2020-12-21 20:07:00.902 o.a.s.m.n.KerberosSaslClientHandler openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [INFO] Connection established from /10.215.73.209:45548 to openstorm3blue-n10.blue.ygrid.yahoo.com/10.215.79.152:6699
2020-12-21 20:07:00.903 o.a.s.m.n.KerberosSaslNettyClient openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [INFO] Creating Kerberos Client.
2020-12-21 20:07:00.906 o.a.s.m.n.KerberosSaslNettyClient openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [INFO] Kerberos Client Callback Handler got callback: class javax.security.auth.callback.PasswordCallback
2020-12-21 20:07:00.906 o.a.s.m.n.Login openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [ERROR] Login using jaas conf /home/y/lib/storm/current/conf/storm_jaas.conf failed
2020-12-21 20:07:00.906 o.a.s.m.n.KerberosSaslNettyClient openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [ERROR] Client failed to login in principal:javax.security.auth.login.LoginException: No password provided
javax.security.auth.login.LoginException: No password provided
        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:923) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:764) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:618) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_262]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_262]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_262]
        at org.apache.storm.messaging.netty.Login.login(Login.java:301) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.Login.<init>(Login.java:83) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslNettyClient.<init>(KerberosSaslNettyClient.java:66) [storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:59) [storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
2020-12-21 20:07:00.907 o.a.s.m.n.KerberosSaslClientHandler openstorm3blue-n10.blue.ygrid.yahoo.com-pm-1 [ERROR] Failed to authenticate with server due to error:
java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
        at org.apache.storm.messaging.netty.KerberosSaslNettyClient.<init>(KerberosSaslNettyClient.java:71) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:59) [storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_262]
Caused by: javax.security.auth.login.LoginException: No password provided
        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:923) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:764) ~[?:1.8.0_262]
        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:618) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_262]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_262]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_262]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_262]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_262]
        at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_262]
        at org.apache.storm.messaging.netty.Login.login(Login.java:301) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.Login.<init>(Login.java:83) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        at org.apache.storm.messaging.netty.KerberosSaslNettyClient.<init>(KerberosSaslNettyClient.java:66) ~[storm-client-2.3.0.y.jar:2.3.0-SNAPSHOT]
        ... 20 more
2020-12-21 20:07:01.802 o.a.s.p.PacemakerClient executor-heartbeat-timer [ERROR] Error attempting to write to a channel to host openstorm3blue-n10.blue.ygrid.yahoo.com - Timed out waiting for channel ready.
2020-12-21 20:07:01.803 o.a.s.p.PacemakerClient executor-heartbeat-timer [WARN] Not getting response or getting null response. Making 9 more attempts for openstorm3blue-n10.blue.ygrid.yahoo.com.
{code}

Currently by design [https://github.com/apache/storm/blob/master/docs/Pacemaker.md#security] pacemaker allows writes by anyone (which should be improved in the future). 

So a quick work around is to make sure worker always has pacemaker.auth.method set to  NONE

 "
STORM-3727,SUPERVISOR_SLOTS_PORTS could be list of Longs," 

A user reported:

There's no guarantee that the {{supervisorConf.getOrDefault}} will be a List of Integers.
Additionally, in ReadClusterState.java, {{.intValue()}} conversion is removed. Overall result

 

{{java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at org.apache.storm.daemon.supervisor.ReadClusterState.<init>(ReadClusterState.java:101) ~[storm-server-2.2.0.jar:2.2.0]
	at org.apache.storm.daemon.supervisor.Supervisor.launch(Supervisor.java:310) ~[storm-server-2.2.0.jar:2.2.0]}}"
STORM-3725,DRPC spout will crash when any one of DRPC server is down,The root cause is DRPC Spout does not handle drpc connections really asynchrously. Spout worker will not work unless all DRPC servers are up and running which leads to the SPOF.
STORM-3716,Nodes underutilized,"Topologies employing anchored tuples do not distribute across multiple nodes, regardless of the computation demands of the bolts. It works fine on a single node, but when throwing multiple nodes into the mix, only one machine gets pegged. When we disable anchoring, it will distribute across all nodes just fine, pegging each machine appropriately.

This bug manifests from version 2.1 forward. I first encountered this issue with my own production cluster on an app that does significant NLP computation across hundreds of millions of documents. This topology is fairly complex, so I developed a very simple exemplar that demonstrates the issue with only one spout and bolt. I pushed this demonstration up to github to provide the developers with a mechanism to easily isolate the bug, and maybe provide some workaround. I used gradle to build this simple topology and software and package the results. This code is well documented, so it should be fairly simple to reproduce the issue. I first encountered this issue on 3 32 core nodes, but when I started experimenting, I set up a test cluster with 8 cores, and then I increased each node to 16 cores, and plenty of memory in every case.

The topology can be accessed from github at https://github.com/cowchipkid/storm-issue.git <https://github.com/cowchipkid/storm-issue.git>."
STORM-3710,Storm error/timeout handling when using trident topology with window,"My question is basically as described in an open stackoverflow question:

[https://stackoverflow.com/questions/64575976/storm-error-timeout-handling-when-using-trident-topology-with-window]

Nobody accrually answered this, though it would be really helpfull to understand what goes behind the scene when trident topology timeout/failure happends, and the questions found in the github issue.

Please help :)"
STORM-3705,Storm UI and nimbus CLI not working,"When deploying a topology on nimbus, there were errors in topology because of wrong configuration, due to which every request was failing in the topology. In our code, there is this logic that if a topology observes error more than a particular threshold, then it will issue storm deactivate topology command to nimbus.

The restart of topologies was being done via script.

The scenario was:
2 topologies were restarted successfully but facing errors due to wrong configuration. Because of the same, deactivate command was submitted to nimbus.
3rd topology was killed successfully and command for topology submission for the same was received successfully by nimbus.

At this point, storm UI stopped responding completely. When tried to run kill command via CLI on nimbus, it didn't work either and stayed stuck.

At this point, the 2 topologies with errors were still running as deactivation was not successful via nimbus. And the 3rd topology wasn't restarted via nimbus.

Any other commands ran on nimbus, stayed at stuck state until the nimbus was stopped on the leader machine and another nimbus was made leader.

There are no helpful logs in nimbus or supervisors or worker logs of affected topologies, apart from zookeeper info logs below:
{color:#FF0000}zookeeper [INFO] exceptionorg.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /errors/<AFFECTED-TOPOLOGY-ID>/<BOLTNAME-WITH_ERRORS>/e0000001494

{color}Storm version: 1.2.1"
STORM-3704,"Cosmetic: columns shifted in ""Topology summary"" table",In STORM-3534 generic resources were added to be visible in Storm UI. There was a typo and table headers are shifted if generic resources aren't displayed.
STORM-3702,Change thrift exception classes to contain getMessage() method,"Thrift classes are currently being generated without camel casing for method names. And the exception classes have a member variable called ""msg"". This generates, get_msg() and set_msg() method names. Since this class extends java Exception class, when an TException is thrown, the getMessage() method returns null and cannot be relied upon.

To get around this problem with getMessage() method, storm code has Wrapper classes, eg WrappedNotAliveException with a getMessage() method that return wraps get_msg() method in base class.

Proposed Change:
 * Change the TException classes to rename member variable and generate camel cased class so that getMessage() is generated
 * Limit the change to specific TException classes only to avoid large change
 * Deprecate Wrapped classes"
STORM-3701,Race Condition between cleanup thread and download tasks,"We captured a issue on our supervisor:

{code}

2020-06-09 23:30:08.723 o.a.s.l.LocalizedResource AsyncLocalizer Task Executor - 0 [INFO] completelyRemoveUnusedUser directu for directory /home/y/var/storm/supervisor/usercache/directu
 2020-06-09 23:30:08.724 o.a.s.l.AsyncLocalizer AsyncLocalizer Task Executor - 0 [WARN] Caught Exception While Downloading (rethrowing)...
 java.io.FileNotFoundException: File '/home/y/var/storm/supervisor/stormdist/dg_itp-605-1591745383/stormconf.ser' does not exist
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:316) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:477) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:311) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.get(AsyncLocalizer.java:698) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.get(AsyncLocalizer.java:683) [storm-server-2.3.0.y.jar:2.3.0.y]
         at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) [?:1.8.0_242]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_242]
         at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_242]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_242]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
         at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
 2020-06-09 23:30:08.725 o.a.s.d.s.Slot SLOT_6782 [ERROR] java.io.FileNotFoundException: File '/home/y/var/storm/supervisor/stormdist/dg_itp-605-1591745383/stormconf.ser' does not exist
 2020-06-09 23:30:08.725 o.a.s.l.AsyncLocalizer SLOT_6782 [INFO] Port and assignment info: PortAndAssignmentImpl\{dg_itp-605-1591745383 on 6782}
 2020-06-09 23:30:08.726 o.a.s.l.AsyncLocalizer SLOT_6782 [WARN] Local base blobs have not been downloaded yet.
 java.io.FileNotFoundException: File '/home/y/var/storm/supervisor/stormdist/dg_itp-605-1591745383/stormconf.ser' does not exist
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[storm-shaded-deps-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:316) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:477) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:311) ~[storm-client-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:362) ~[storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:472) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:549) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:298) [storm-server-2.3.0.y.jar:2.3.0.y]
         at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:1039) [storm-server-2.3.0.y.jar:2.3.0.y]

{code}
 The root issue is the delay at [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L641] which will cause the safeTopologyIds information out-of-date.

 

 

 "
STORM-3700,"Storm UI functionality ""Stop flight recording"" doesn't work properly","The code is implemented at
[https://github.com/apache/storm/blob/36204eda00bca7e03ac3979d9c0d3527d1f08330/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java#L841-L878]

 

JPROFILE_STOP is used for both start flight recording and stop flight recording. The logic in the code is that if there is already a JPROFILE_STOP with the same topoId and request content (host, port, timestamp), the JPROFILE_STOP becomes a stop command; otherwise, it is a start command.

But the problem is every time when we invoke stop on UI:

[https://github.com/apache/storm/blob/3fb289b87c7d72bfe01ee1c7028adbc69f012439/storm-webapp/src/main/java/org/apache/storm/daemon/ui/resources/StormApiResource.java#L621-L631]

 
the request is actually configured with timestamp=0:

[https://github.com/apache/storm/blob/bb199d574eae337d0512670dcc4957f3c7ef4922/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L2320]

so it will never equal to any request in the pending profile action. So stop will never happen
 "
STORM-3699,fix flight.bash to support flight recording on openJDK8u262 or newer,"[https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/comline.htm#JFRUH193]

[https://docs.oracle.com/javacomponents/jmc-5-5/jfr-command-reference/diagnostic-command-reference.htm#resourceid-15422-48C8362C]

 

flight recorder command changed between versions. so the current flight.bash doesn't work for flight recording on newer JFR version (e.g. on openJDK8u262)

[https://github.com/apache/storm/blob/eb27556f7669c7c966716e1aca4867da04ce6e08/bin/flight.bash#L86-L93]

 
{code:java}
bash-4.2$ java -version
openjdk version ""1.8.0_262""
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_262-b10)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.262-b10, mixed mode)
bash-4.2$ jcmd 20 JFR.stop recording=1
20:
java.lang.IllegalArgumentException: Unknown argument 'recording' in diagnostic command.
{code}
 

 

 "
STORM-3698,AbstractHdfsBolt does not sync Writers that are purged,"We just discovered when using a SequenceFileBolt (although it might happen with other implementations as well) that the writers it uses, held in the map AbstractHdfsBolt.writers are not closed/synced when they are removed from the map by the removeEldestEntry method.

This leads to data loss.

Can be reproduced by creating a SequenceFileBolt.withMaxOpenFiles(1) and writing to just two different files.

One will have a size of zero, the other has the data in it. "
STORM-3693,TimeOut ticks should be addressed to Executor instead of being addressed to a task or broadcasted. ,"For the purpose of message timeouts, a spout executor uses a rotating map, irrespective of the number of spout tasks it is dealing with. When a time out tick tuple is received, it is broadcasted to all the tasks which means we rotate the map as many times as the number of assigned tasks and expire tuples prematurely. We need the tuple to be neither a broadcast not addressed to any task. The executor should act on it only once. "
STORM-3690,UI has NullPointerException when the scheduler is not ResourceAwareScheduler,"{code:java}
java.lang.NullPointerException at org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest.addResourceMap(NormalizedResourceRequest.java:195) at org.apache.storm.daemon.ui.UIHelpers.getClusterSummary(UIHelpers.java:644) at org.apache.storm.daemon.ui.resources.StormApiResource.getClusterSummary(StormApiResource.java:136) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory.lambda$static$0(ResourceMethodInvocationHandlerFactory.java:52) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:124) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:167) at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:79) at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:469) at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:391) at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:80) at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:253) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)

{code}
[https://github.com/apache/storm/blob/master/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L644]
 because only 
{code:java}
if (resources != null && underlyingScheduler instanceof ResourceAwareScheduler)
{code}
[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L2941]
 supervisor used_generic_resources will not be null.

    "
STORM-3686,o.a.s.t.s.AbstractNonblockingServer$FrameBuffer pool-14-thread-10 [ERROR] Unexpected throwable while invoking! java.lang.NullPointerException: null,"Hi Team,

 

Very often I am getting below error in storm nimbus and after that the storm UI loads only with some content.

 

2020-08-05 02:45:01.802 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer pool-14-thread-10 [ERROR] Unexpected throwable while invoking!
 java.lang.NullPointerException: null

 

Attached the storm nimbus and ui error from their logs.

 

Please advise."
STORM-3684,"receive-queue V2 metrics shouldn't have ""_system"" as componentId if it is from a system task","When storm registers receive-queue for executors,
[https://github.com/apache/storm/blob/v2.2.0/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java#L689]

it set componentId as ""__system"" for any task. This is wrong for V2 metrics.

V1 metrics are fine since the component Id used for V1 metric is not from the same place.

[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L342-L343]
It is from the member variable of the executor object."
STORM-3680,Upgrade Jedis library in storm-jedis module to 3.2.x,"To support future improvements to the storm-redis package, lets upgrade its underlying library (Jedis) it uses to communicate with Redis.

 

Upgrading will clear the way for #STORM-3665 as Stream support in the Jedis library was not added until 3.x"
STORM-3679,misuse of nodeId as hostname in LoadAwareShuffleGrouping," 
LoadAwareShuffleGrouping misuses nodeId as the hostname and it causes consistent invalid DNS queries for ""hostnames"" like 2a1f2cf3-c701-4621-9e93-640b4e63be48-<ip>.

This causes excessive unnecessary loads on nscd and DNS. Also because of this bug, every target tasks will be treated as at least RACK_LOCAL because if an ip address can't be determined, YahooDNSToSwitchMapping treats it as DEFAULT_RACK. This doesn't impact WORKER_LOCAL and HOST_LOCAL though.
 "
STORM-3678,从StormTopology中获得TopologyBuilder,在Storm项目中，我们可以从TopologyBuilder获得StormTopology，但是能否从StormTopology得到TopologyBuilder呢？或者说，从用户的Storm程序Jar中获得TopologyBuilder呢？
STORM-3677,Fix Worker Suicide Function if assignment is null,When topology is killed worker receives null for assignment. It should cause  worker restart.
STORM-3666,Validate component name in rebalance command and fix --executor option,"It appears rebalance does not change parallelism of components. It is advertised in storm help but not honoring it.
{quote}_storm rebalance --executors component-id:number topology-id_
{quote}"
STORM-3664,Nimbus cannot recover from LocalFsBlobStore deletion,"When all Nimbus instances in a cluster loose access to previously stored Blobs while at least one topology is deployed, the cluster cannot recover as none of the nodes is ever elected as leader due to missing blobs. Recovery is only possible when manually removing blob and topology data from Zookeeper.

I understand that the LocalFs blob store implementation is not particularly suited for high availability deployments. However, this issue prevents sensible automated disaster recovery on small deployments where a full deployment of HDFS would not provide any benefits and simply introduce additional complexity.
h3. Reproduction Steps
 # Deploy one or multiple Nimbus instances
 # Deploy a Topology (such as the WordCount example)
 # Stop all Nimbus Instances
 # Remove all Blob directories
 # Start all Nimbus Instances

h3. Expected Behavior

When a topology's blobs are permanently lost, the topology itself should be marked as failed in favor of maintaining the cluster's availability as a single lost topology suffices to take down the entire system."
STORM-3663,Topology with Mockito 1.x fails to run unit tests ,"We are observing problems building topologies with latest snapshot storm version. It used to work fine. After debugging, we have identified that the problem comes from 

[https://github.com/apache/storm/blob/master/storm-client/src/resources/mockito-extensions/org.mockito.plugins.MockMaker]

This break topology if Mockito was used. This file should be a test level resource.

 
{code:java}
Configuring TestNG with: TestNG652ConfiguratorConfiguring TestNG with: TestNG652ConfiguratorSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/Users/rli01/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/Users/rli01/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.11.2/log4j-slf4j-impl-2.11.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]Tests run: 37, Failures: 12, Errors: 0, Skipped: 0, Time elapsed: 1.135 sec <<< FAILURE! - in TestSuitetestCycling(fetl.bullet.propane.PropaneSpoutTest)  Time elapsed: 0.147 sec  <<< FAILURE!java.lang.ExceptionInInitializerError: null at org.mockito.internal.configuration.plugins.Plugins.getStackTraceCleanerProvider(Plugins.java:17) at org.mockito.internal.exceptions.stacktrace.StackTraceFilter.<clinit>(StackTraceFilter.java:21) at org.mockito.internal.exceptions.stacktrace.ConditionalStackTraceFilter.<init>(ConditionalStackTraceFilter.java:17) at org.mockito.exceptions.base.MockitoException.filterStackTrace(MockitoException.java:41) at org.mockito.exceptions.base.MockitoException.<init>(MockitoException.java:30) at org.mockito.exceptions.misusing.MockitoConfigurationException.<init>(MockitoConfigurationException.java:18) at org.mockito.internal.configuration.plugins.PluginLoader.loadImpl(PluginLoader.java:66) at org.mockito.internal.configuration.plugins.PluginLoader.loadPlugin(PluginLoader.java:24) at org.mockito.internal.configuration.plugins.PluginRegistry.<init>(PluginRegistry.java:12) at org.mockito.internal.configuration.plugins.Plugins.<clinit>(Plugins.java:11) at org.mockito.internal.util.MockUtil.<clinit>(MockUtil.java:24) at org.mockito.internal.MockitoCore.<init>(MockitoCore.java:44) at org.mockito.Mockito.<clinit>(Mockito.java:975) at fetl.bullet.propane.PropaneSpoutTest.createKafkaConsumer(PropaneSpoutTest.java:71) at fetl.bullet.propane.PropaneSpoutTest.createKafkaConsumer(PropaneSpoutTest.java:87) at fetl.bullet.propane.PropaneSpoutTest.testCycling(PropaneSpoutTest.java:170)

 {code}"
STORM-3658,Problematic worker stays alive because of a deadlock and race condition caused by ShutdownHooks,"During worker startup, it starts many threads including executor threads, and then registers two shutdown hooks, first hook will invoke worker::shutdown, the second hook will sleep for 3 seconds before force halting the whole process.

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L141-L147

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L253-L255

We have seen a case where a deadlock happened. Imaging there is a bolt in a topology consistently fails at prepare stage and throws RuntimeException. This thread will eventually invoke Runtime.getRuntime().exit. The code path is:

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L406-L407

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/error/ReportErrorAndDie.java#L41

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L510-L514

There are three scenarios here.

*Scenario 1*
If two shutdown hooks are registered before this bolt's prepare method is invoked, when this bolt throws RuntimeException, it eventually invokes Runtime.getRuntime().exit, which triggers two shutdown hooks to start. And then this bolt executor thread waits for these two shutdown hooks to finish.

https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u242-b08/jdk/src/share/classes/java/lang/ApplicationShutdownHooks.java#L104-L111 (we use this openJDK8u242 version)

However, what the first shutdown hooks does is to invoke worker::shutdown method
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java#L467-L469
which will interrupt all executor threads and then wait for them to finish 

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java#L95-L100

However, this bolt executor thread ignores InterruptedException and continues to wait for the first hook to finish. Hence there is a dead lock between the first shutdown hook and the bolt executor thread. After 3 seconds, the second shutdown hook force halting the worker process. So in the log, you will see ""Forcing Halt...""

*Scenario 2*
If the bolt executor thread invokes prepare method earlier than the main thread registers these two shutdown hooks, because the bolt executor thread already triggers shutdown, the main thread will receive an IllegalStateException(""Shutdown in progress"")

{code:java}
2020-06-18 11:57:29.159 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.lang.IllegalStateException: Shutdown in progress
{code}

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L1011-L1018

https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u242-b08/jdk/src/share/classes/java/lang/ApplicationShutdownHooks.java#L63-L67

Since there is no shutdown hook registered, Runtime.getRuntime.exit invoked by the bolt executor continues; in the meantime, the main thread also invokes Runtime.getRuntime.exit because of IllegalStateException. Eventually the process dies.

*Scenario 3*
This is the worse case. In this scenario, after the first shutdown hook is registered, and before the second hook is registered, the bolt executor thread invokes prepare method and throws a RuntimeException. 
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L355-L356
 
In this case, we know have the deadlock between the first shutdown hook and the bolt executor thread (like in scenario 1). The main thread will also have IllegalStateException when it tries to register the second shutdown hook, so main thread also invokes Runtime.getRuntime.exit (like in scenario 2). But this time, since the bolt executor thread already obtained the shutdown lock (because it invokes Runtime.getRuntime.exit earlier), 
https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/jdk8u242-b08/jdk/src/share/classes/java/lang/Shutdown.java#L208-L214

the main thread has to wait for the bolt executor thread to release this Shutdown.class lock. However, there is a deadlock between bolt executor thread and the first shutdown hook(thread), the main thread, the bolt executor thread and the shutdown hook are all BLOCKED. 

And in this case, since the second shutdown hook (sleepKill) is not registered, and every other threads like heartbeat timers still work (not yet closed), no one (not worker itself, not nimbus or supervisor) will kill this worker. 

So this worker stays alive but it does not function. And since this executor bolt is blocked, it doesn't consume tuples from the receiveQ, so the receiveQ can be quickly fill up by upstreams, the credential refresh thread is also blocked because it won't give up until the credential tuple is published to the receiveQ.

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java#L68-L69

https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/utils/JCQueue.java#L300-L313

*How to produce the deadlock*
It can be produced by modifying the [WordCountTopology|https://github.com/apache/storm/blob/v2.1.0/examples/storm-starter/src/jvm/org/apache/storm/starter/WordCountTopology.java] to add 

{code:java}
@Override
    public void prepare(Map<String, Object> topoConf, TopologyContext context) {
        throw new RuntimeException(""Runtime exception at WordCountBolt prepare"");
    }
{code}

in the WordCount bolt so everytime the prepare method is called, this bolt throws a RuntimeException. 


Optionally, add a delay in between registering two shutdown hooks can help reproduce scenario 3.

{code:java}
Runtime.getRuntime().addShutdownHook(wrappedFunc);
        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            LOG.info(""Sleep for 100ms between hooks"", e);
        }
        LOG.info(""Wait for 100ms"");
        Runtime.getRuntime().addShutdownHook(sleepKill);
{code}


*Solution*
There are better ways to deal with this issue. But a simple way is to register shutdown hooks before any executor threads are started to avoid IllegalStateException; And in 
https://github.com/apache/storm/blob/v2.1.0/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java#L98-L101
change it be a timed join so it won't wait indefinitely. This can prevent the deadlock from happening. 


"
STORM-3656,Change handling of Hadoop TGT renewal exception,"STORM-3606 identified an issue where Hadoop's TGT auto renewal thread causes an exception and worker restart.  The fix involved a lot of reflection calls to emulate the Hadoop code while avoiding launching this thread.

 

It's possible Hadoop could change their code, causing this reflection to fail.   To handle this case, we could instead allow Hadoop to launch this autorenewal thread, and have the worker catch the specific NPE from Hadoop in the exception handler.  "
STORM-3655,Worker should commit suicide On Change in its assignment.,"This is not urgent but essentially, it can take care of avoiding a lot of scenarios leading to left over rogue processes across the cluster. Two scenarios are: 
 * Worker still exists in assignment, but assignment is different
 * Worker is absent from assignments.
 "
STORM-3652, Last error not displayed  in Topology summary storm ui,"Under storm ui ""Topology summary"" lastError is not getting populated, you need to go into ""Component summary""
  
 Seems some body raised a similar question here [https://github.com/apache/storm/pull/2828#issuecomment-420790179]

STORM-3217 fixed the component page, but not the topology page
 is it because it is missing here [https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L1225]

 

I see it is in this section added [https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java#L1480]
  
h2. Topology summary ({color:#de350b}Last error appears empty, failed metric = 40{color})

*API response*
 localhost:8080/api/v1/topology/topology-test-1-1591927011
{code:java}
 
{
   ""bolts"":[
      {
         ""requestedCpu"":10.0,
         ""encodedBoltId"":""TopLevel.test"",
         ""transferred"":0,
         ""lastError"":"""",  <-- *empty*
         ""processLatency"":""0.000"",
         ""executeLatency"":""5.000"",
         ""executed"":40,
         ""failed"":40, <-- *failed count not zero*
         ""requestedMemOnHeap"":128.0,
         ""acked"":0,
         ""capacity"":""0.001"",
         ""emitted"":0,
         ""requestedMemOffHeap"":0.0,
         ""executors"":1,
         ""requestedGenericResourcesComp"":"""",
         ""boltId"":""TopLevel.test"",
         ""tasks"":1
      }
   ]
}
{code}
 
h2. Component summary ({color:#00875a}Displays the errors{color})

*API response*
 localhost:8080/api/v1/topology/topology-test-1-1591927011/component/TopLevel.test?sys=false
{code:java}
{
   ""requestedGenericResources"":"""",
   ""componentErrors"":[
      {
         ""errorTime"":1591928686,
         ""errorWorkerLogLink"":""http:\/\/localhost:8000\/api\/v1\/log?file=topology-test-1-1591927011%2F6700%2Fworker.log"",
         ""errorLapsedSecs"":16,
         ""errorPort"":6700,
         ""error"":""java.lang.IllegalArgumentException: Error with IllegalArgumentException\n\tat com.test.TestBolt.execute(TestBolt.java:35)\n\tat org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:236)\n\tat org.apache.storm.executor.Executor.accept(Executor.java:283)\n\tat org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131)\n\tat org.apache.storm.utils.JCQueue.consume(JCQueue.java:111)\n\tat org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:172)\n\tat org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159)\n\tat org.apache.storm.utils.Utils$1.run(Utils.java:394)\n\tat java.lang.Thread.run(Thread.java:748)\n"",
         ""errorHost"":""localhost""
      }
   ]
}
{code}
 Errors reported using
{code:java}
// Where outputCollector is the instance passed during BaseRichBolt::prepare

// public void prepare(Map stormConf, TopologyContext context, OutputCollector outputCollector) {

outputCollector().reportError(e);
outputCollector().fail(input);

{code}"
STORM-3649,Logic error regarding storm.supervisor.medium.memory.grace.period.ms,"Inside this chunk of code

https://github.com/apache/storm/blob/2.2.x-branch/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L758

{code:java}
if (systemFreeMemoryMb < mediumMemoryThresholdMb) {
                    if (memoryLimitExceededStart < 0) {
                        memoryLimitExceededStart = Time.currentTimeMillis();
                    } else {
                        long timeInViolation = Time.currentTimeMillis() - memoryLimitExceededStart;
                        if (timeInViolation > mediumMemoryGracePeriodMs) {
                            LOG.warn(
                                ""{} is using {} MB > memory limit {} MB for {} seconds"",
                                typeOfCheck,
                                usageMb,
                                memoryLimitMb,
                                timeInViolation / 1000);
                            return true;
                        }
                    }
                } 
{code}

At very beginning, memoryLimitExceededStart in BasicContainer is initialized as 0. :
https://github.com/apache/storm/blob/2.2.x-branch/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L80
{code:java}
protected volatile long memoryLimitExceededStart;
{code}

So once it hits this scenario, the grace period doesn't really take any effect because the timeInViolation will be very large (equals to currentTime)

The logs from a test:

{code:java}
2020-06-08 20:39:18.277 o.a.s.d.s.BasicContainer SLOT_6707 [WARN] WORKER 9c16e81e-4936-4029-bcda-ceb5b74b8f42 is using 167 MB > memory limit 158 MB for 1591648758 seconds
{code}

"
STORM-3643,Bring queue metrics documentation up to date,[https://github.com/apache/storm/blob/master/docs/Metrics.md#queue-metrics] should be updated 
STORM-3639,Remove asserts from Storm daemon code,"By default jvm disables assertion for performance, and we allow JVM options to be configured at deployment time,  it is best to avoid use of _*{{assert}}*_ statement and use explicit clauses to validate critical checks withing storm-daemon code. Also, it would be helpful in many cases to explicitly log and proceed on failure to meet such assert expectation for debugging purpose."
STORM-3637,Looping topology structure can cause backpressure to deadlock,"When you have a topology structure with loops in it (BoltA and BoltB send tuples to each other), it can cause backpressure to deadlock.

The scenario is that BoltA suddenly takes a long time to process a tuple (in our situation, it's doing a database operation). This causes the task input queue to fill up, setting the backpressure flag.

BoltB, which is sending a tuple to BoltA, then cannot send, and the tuple is held in the emit queue. This blocks any tuples behind it, and also stops BoltB from executing. This means the input queue to BoltB will build up, until that backpressure flag is also set - and then when BoltA next wants to send a tuple to BoltB, it will irrevocably deadlock."
STORM-3631,Wrong format of logs.users/groups in topology conf can cause supervisor/logviewer to terminate,"If users submit a topology with logs.users set as a single string, it will cause ClassCastException and cause Supervisor to terminate
{code:java}
2020-04-28 19:33:59.901 o.a.s.d.s.Slot SLOT_6707 [ERROR] Error when processing event
java.lang.ClassCastException: java.lang.String cannot be cast to java.util.List
        at org.apache.storm.daemon.supervisor.Container.writeLogMetadata(Container.java:)
{code}

Can be easily reproduced by 

{code:java}
storm jar storm-starter.jar org.apache.storm.starter.WordCountTopology wc -c logs.users=[null, ""fake-groups""] 
{code}


If users submit with logs.users set with a list with null member, for example, logs.users='[null, ""fake-2-users""]', it will cause NullPointerException and cause logviewer to terminate

{code:java}
Caused by: java.lang.NullPointerException
        at org.apache.storm.utils.ObjectReader.getStrings(ObjectReader.java:)
        at org.apache.storm.daemon.logviewer.utils.ResourceAuthorizer.getLogUserGroupWhitelist(ResourceAuthorizer.java)
{code}

Can be easily reproduced by 

{code:java}
storm jar storm-starter.jar org.apache.storm.starter.WordCountTopology wc -c logs.users=""fake-users""
{code}


Ideally logs.users and logs.groups should be daemon config only and we should have something like topology.logs.users and topology.logs.groups for topology level config sit in Config.java (so it can be validated by ConfigValidation).   Because now there two configs are in DaemonConfig, it wont' be validated against ""@isStringList"" rule when it is from topo conf. 

But even with the rule, it doesn't validate when logs.users include a null member in the list. 

For backwards compatibility, we have to fix these configs instead of removing them from topo conf (by adding topology.logs.users).
"
STORM-3630,Remove unwanted check from LocalFsBlobStore,"The LocalFsBlobStore _getBlob_ and _getBlobMeta_, alls synchronized methods _checkForBlobOrDownload_ which do not make sense for local disk."
STORM-3629, Logviewer should always allow admins to access logs,"https://github.com/apache/storm/blob/v2.1.0/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java#L86-L89

Currently if there is any problems with reading worker.yaml, no one can access logs from logviewer, including admins. Admins should always be able to access."
STORM-3628,Nimbus tries Worker HB on non un-deployed topologies,"I am running Storm 2.1.0 version.

After I undeploy topologies and later deploy new topologies, Nimbus log repeats the exception:

Please note: *fetch-inbox-16-1587324922*  is artifact id that does not exist anymore.

The Nimbus log is full of this HBs Exceptions.

org.apache.storm.utils.WrappedNotAliveException: *fetch-inbox-16-1587324922*
 at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConf(Nimbus.java:987) ~[storm-server-2.1.0.jar:2.1.0]
 at org.apache.storm.daemon.nimbus.Nimbus.sendSupervisorWorkerHeartbeat(Nimbus.java:4727) [storm-server-2.1.0.jar:2.1.0]
 at org.apache.storm.generated.Nimbus$Processor$sendSupervisorWorkerHeartbeat.getResult(Nimbus.java:4817) [storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.generated.Nimbus$Processor$sendSupervisorWorkerHeartbeat.getResult(Nimbus.java:4796) [storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0]"
STORM-3627,Allow use of shortNames for Metrics for worker in Metrics-V2,Adding topology configuration _topology.metrics.use.shortname_ to allow for use of shortnames in case metrics tick is enabled using _topology.enable.v2.metrics.tick_.
STORM-3626,"storm-kafka-migration should pull in storm-client as ""provided"" dependency","https://github.com/apache/storm/blob/master/external/storm-kafka-migration/pom.xml#L34-L39


{code:java}
<dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${project.version}</version>
        </dependency>

{code}

it is ""compile"" dependency as of now."
STORM-3624,Race condition on ArtifactoryConfigLoader.load,"https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java#L100-L102

config() is called in multiple threads. But ArtifactoryConfigLoader.load is not thread-safe. For example, https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/utils/ArtifactoryConfigLoader.java#L181-L187
{code:java}
JSONObject returnValue;
        try {
            returnValue = (JSONObject) jsonParser.parse(metadataStr);
        } catch (ParseException e) {
            LOG.error(""Could not parse JSON string {}"", metadataStr, e);
            return null;
        }
{code}

Multiple threads use the same jsonParser and since JsonParser is not thread-safe, the return value will be corrupted. 


I propose to create a separate thread to load scheduler configs periodically. This also makes the config loading logic cleaner."
STORM-3623,v2 metrics tick reports all worker metrics within each executor,"see https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L335-L341


{code:java}
    private void addV2Metrics(List<IMetricsConsumer.DataPoint> dataPoints) {
        boolean enableV2MetricsDataPoints = ObjectReader.getBoolean(topoConf.get(Config.TOPOLOGY_ENABLE_V2_METRICS_TICK), false);
        if (!enableV2MetricsDataPoints) {
            return;
        }
        StormMetricRegistry stormMetricRegistry = workerData.getMetricRegistry();
{code}


This should be reporting just the metrics for the Executor."
STORM-3622,Race Condition in CachedThreadStatesGaugeSet registered at SystemBolt,"We noticed that with the change in https://github.com/apache/storm/pull/3242, there is a race condition causing NPE.
{code:java}
2020-04-14 18:22:12.997 o.a.s.u.Utils Thread-17-__acker-executor[16, 16] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
 at org.apache.storm.executor.Executor.accept(Executor.java:291) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.utils.JCQueue.consume(JCQueue.java:111) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:172) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:159) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.utils.Utils$1.run(Utils.java:434) [storm-client-2.2.0.y.jar:2.2.0.y]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.lang.NullPointerException
 at com.codahale.metrics.jvm.ThreadStatesGaugeSet.getThreadCount(ThreadStatesGaugeSet.java:95) ~[metrics-jvm-3.2.6.jar:3.2.6]
 at com.codahale.metrics.jvm.ThreadStatesGaugeSet.access$000(ThreadStatesGaugeSet.java:20) ~[metrics-jvm-3.2.6.jar:3.2.6]
 at com.codahale.metrics.jvm.ThreadStatesGaugeSet$1.getValue(ThreadStatesGaugeSet.java:56) ~[metrics-jvm-3.2.6.jar:3.2.6]
 at org.apache.storm.executor.Executor.addV2Metrics(Executor.java:344) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.Executor.metricsTick(Executor.java:320) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:218) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 at org.apache.storm.executor.Executor.accept(Executor.java:287) ~[storm-client-2.2.0.y.jar:2.2.0.y]
 ... 6 more
{code}


This is due to a race condition in CachedGauge https://github.com/dropwizard/metrics/blob/v3.2.6/metrics-core/src/main/java/com/codahale/metrics/CachedGauge.java#L49-L53

There are two issues here.
The first one is STORM-3623. 
This makes all the executors to get values for all the metrics. So multiple threads will access the same metric.

So the threads gauges are now accessed by multiple threads. But in CachedGauge,


{code:java}
 @Override
    public T getValue() {
        if (shouldLoad()) {
            this.value = loadValue();
        }
        return value;
    }
{code}

this method is not thread-safe. Two threads can reach to getValue at the same time.
The first thread reaching shouldLoad knows it needs to reload, so it calls the next line this.value=loadValue()
The second thread is a little bit late so shouldLoad returns false. Then it returns the value directly.

There is a race condition between first thread calling loadValue() and the second thread returning value.

If the first thread finishes loadValue() first, both values returned to the threads are the same value (and current value). But if the second thread returns earlier, the second thread gets the original value (which is null ), hence NPE.

To summarize, the second issue is CachedThreadStatesGaugeSet is not thread-safe

To fix this NPE, we should avoid using CachedThreadStatesGaugeSet. 

But we still need to fix STORM-3623  to avoid unnecessary computations and redundant metrics."
STORM-3620,Data corruption can happen when components are multi-threaded because of non thread-safe serializer,"OutputCollector is not thread-safe in 2.x. 

It can cause data corruption if multiple threads in the same executor calls OutputCollector to emit data at the same time:

1. Every executor has an instance of ExecutorTransfer
https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L146

2. Every ExecutorTransfer has its own serializer

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L44

3. Every executor has its own outputCollector

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java#L146-L147

4. When outputCollector is called to emit to remote workers, it uses ExecutorTransfer to transfer data

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/executor/ExecutorTransfer.java#L66

5. which will try to serialize data

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerTransfer.java#L116

6. But serializer is not thread-safe

https://github.com/apache/storm/blob/00f48d60e75b28e11a887baba02dc77876b2bb3d/storm-client/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java#L33-L43


----

But in the doc, http://storm.apache.org/releases/2.1.0/Concepts.html, it says outputCollector is thread-safe. 
{code:java}
Its perfectly fine to launch new threads in bolts that do processing asynchronously. OutputCollector is thread-safe and can be called at any time.
{code}


We should either fix it to make it thread-safe, or update the document to not mislead users"
STORM-3617,Remove deprecation from PlainSaslTransportPlugin,"Whenever PlainSaslTransportPlugin is used, it logs an error about it being insecure. The class itself is also marked as deprecated - which is fair, as it is doesn't provide any security at all.

However, we are running in situations where we don't _need _security, and we don't want the extra fragility introduced by adding in security. We're perfectly ok with using an insecure transport, and we're using PlainSaslTransport over the default because we don't want to configure the buffer size of the default transport.

So please could you remove the deprecation marker from PlainSaslTransport, and downgrade the ERROR message to an INFO. We don't need security in our situations, and so running a default basic transport is exactly what we want to do."
STORM-3613,storm.py should include lib-worker instead of lib directory in the classpath while submitting a topology,"Currently the classpath is:
{code:java}
-cp /<path>/storm/2.2.0/*:/<path>/storm/2.2.0/lib/*:/<path>/storm/2.2.0/extlib/*:/tmp/storm-examples-1.0-SNAPSHOT.jar:/<path>/storm/2.2.0/conf:/<path>/storm/2.2.0/bin: 
{code}

 for ""storm jar"" command.

It should include lib-worker/ instead of lib/.

This can cause problems because we don't shade deps in lib/ so topology jar could conflict with jars in lib/."
STORM-3611,IllegalArgumentException thrown on windowed bolt,"The following topology with a single `BasicWindowedBolt`. The following topology:
{code:java}
public class GroupMean extends ConfigurableTopology {
  
  public static void main(String[] args) throws Exception {
    ConfigurableTopology.start(new GroupMean(), args);
  }
  
  @Override
  protected int run(String[] args) throws Exception {
    BaseWindowedBolt bolt = new GroupMeanBolt()
        .withTimestampExtractor(x -> x.getLong(0))
        .withTumblingWindow(BaseWindowedBolt.Duration.of(100));
    
    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout(""groups"", new RandomGroupNumberSpout(), 1);
    builder.setBolt(""mean"", bolt, 1).shuffleGrouping(""groups"");
   
    Config config = new Config();
    config.put(""topology.name"", ""group-mean-topo"");
    return submit(""group-mean-topo"", config, builder);
  }
}
{code}
 with the GroupMeanBolt being like:
{code:java}
public class GroupMeanBolt extends BaseWindowedBolt {
  private OutputCollector collector;
  
  @Override
  public void prepare(Map<String, Object> conf, TopologyContext context,
                      OutputCollector collector) {
    this.collector = collector;
  }
  @Override
  public void execute(TupleWindow inputWindow) {
    if (inputWindow.get().size() > 0) {
      Map<Integer, Double> sum = new HashMap<>();
      Map<Integer, Integer> histogram = new HashMap<>();
      for (Tuple t : inputWindow.get()) {
        Integer key = t.getIntegerByField(""key"");
        Double value = t.getDoubleByField(""value"");
        sum.compute(key, (k, v) -> v == null ? value : v + value);
        histogram.compute(key, (k, v) -> v == null ? 1 : v + 1);
      }
      Map<Integer, Number> result = new HashMap<>(sum.size());
      for (Map.Entry<Integer, Double> e : sum.entrySet()) {
        result.put(e.getKey(), e.getValue() / histogram.get(e.getKey()));
      }
      ArrayList<Object> out = new ArrayList<>(3);
      out.add(inputWindow.getStartTimestamp());
      out.add(inputWindow.getEndTimestamp());
      ArrayList<String> tokens = new ArrayList<>(result.size());
      for (Map.Entry<Integer, Number> e : result.entrySet()) {
        String token = String.join("":"", e.getKey().toString(), e.getValue().toString());
        tokens.add(token);
      }
      out.add(String.join("","", tokens));
      collector.emit(out);
    }
  }
}
{code}
At runtime, an IllegalArgumentException is thrown because the ""default"" stream is not recognized. Stack trace:
{code:java}
2020-03-27 15:42:42.705 o.a.s.w.WaterMarkEventGenerator watermark-event-generator-0 [ERROR] Failed while processing watermark event
java.lang.IllegalArgumentException: Unknown stream ID: default
        at org.apache.storm.daemon.Task.getOutgoingTasks(Task.java:164) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltOutputCollectorImpl.boltEmit(BoltOutputCollectorImpl.java:88) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltOutputCollectorImpl.emit(BoltOutputCollectorImpl.java:65) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.task.OutputCollector.emit(OutputCollector.java:93) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.task.OutputCollector.emit(OutputCollector.java:93) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor$WindowedOutputCollector.emit(WindowedBoltExecutor.java:403) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.task.OutputCollector.emit(OutputCollector.java:88) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at gr.katsip.spear.GroupMeanBolt.execute(GroupMeanBolt.java:51) ~[stormjar.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor.boltExecute(WindowedBoltExecutor.java:370) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor$1.onActivation(WindowedBoltExecutor.java:363) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WindowManager.onTrigger(WindowManager.java:156) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WatermarkTimeTriggerPolicy.handleWaterMarkEvent(WatermarkTimeTriggerPolicy.java:73) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WatermarkTimeTriggerPolicy.track(WatermarkTimeTriggerPolicy.java:43) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WindowManager.track(WindowManager.java:185) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WindowManager.add(WindowManager.java:121) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WaterMarkEventGenerator.run(WaterMarkEventGenerator.java:88) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_242]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_242]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_242]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
2020-03-27 15:42:42.715 o.a.s.w.WaterMarkEventGenerator Thread-14-mean-executor[3, 3] [ERROR] Got exception
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Unknown stream ID: default
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_242]
        at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_242]
        at org.apache.storm.windowing.WaterMarkEventGenerator.checkFailures(WaterMarkEventGenerator.java:115) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WaterMarkEventGenerator.track(WaterMarkEventGenerator.java:79) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor.execute(WindowedBoltExecutor.java:308) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:234) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.Executor.accept(Executor.java:275) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.utils.JCQueue.consume(JCQueue.java:111) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:171) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:158) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.utils.Utils$1.run(Utils.java:392) [storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.lang.IllegalArgumentException: Unknown stream ID: default
        at org.apache.storm.daemon.Task.getOutgoingTasks(Task.java:164) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltOutputCollectorImpl.boltEmit(BoltOutputCollectorImpl.java:88) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltOutputCollectorImpl.emit(BoltOutputCollectorImpl.java:65) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.task.OutputCollector.emit(OutputCollector.java:93) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.task.OutputCollector.emit(OutputCollector.java:93) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor$WindowedOutputCollector.emit(WindowedBoltExecutor.java:403) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.task.OutputCollector.emit(OutputCollector.java:88) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at gr.katsip.spear.GroupMeanBolt.execute(GroupMeanBolt.java:51) ~[stormjar.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor.boltExecute(WindowedBoltExecutor.java:370) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.topology.WindowedBoltExecutor$1.onActivation(WindowedBoltExecutor.java:363) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WindowManager.onTrigger(WindowManager.java:156) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WatermarkTimeTriggerPolicy.handleWaterMarkEvent(WatermarkTimeTriggerPolicy.java:73) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WatermarkTimeTriggerPolicy.track(WatermarkTimeTriggerPolicy.java:43) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WindowManager.track(WindowManager.java:185) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WindowManager.add(WindowManager.java:121) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.windowing.WaterMarkEventGenerator.run(WaterMarkEventGenerator.java:88) ~[storm-client-2.1.1-SNAPSHOT.jar:2.1.1-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_242]
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) ~[?:1.8.0_242]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_242]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) ~[?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_242]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_242]
        ... 1 more
{code}"
STORM-3610,CLONE - Topology runtime exception - Error on initialization,"{code:java}
2018-03-14 13:28:41.399 o.a.s.d.worker main [INFO] Reading Assignments. 2018-03-14 13:28:41.511 o.a.s.m.TransportFactory main [INFO] Storm peer transport plugin:org.apache.storm.messaging.netty.Context 2018-03-14 13:28:41.935 o.a.s.m.n.Server main [INFO] Create Netty Server Netty-server-localhost-6712, buffer_size: 5242880, maxWorkers: 1 2018-03-14 13:28:41.980 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6712 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data$fn__5244.invoke(worker.clj:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:269) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544.run(worker.clj:613) ~[storm-core-1.1.0.jar:1.1.0] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_51] at javax.security.auth.Subject.doAs(Subject.java:415) ~[?:1.7.0_51] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto____5543.invoke(worker.clj:611) ~[storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?] at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0] Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:444) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:436) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_51] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.1.0.jar:1.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_51] at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_51] 2018-03-14 13:28:42.004 o.a.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"") at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0]
{code}"
STORM-3609,ClassCastException when credentials are updated for ICredentialsListener spout/bolt instances,"
{code:java}
2020-03-26 21:04:38.526 o.a.s.u.Utils Thread-14-spout-executor[2, 2] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.storm.generated.Credentials cannot be cast to java.util.Map
	at org.apache.storm.executor.Executor.accept(Executor.java:291) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:131) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:111) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:102) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:170) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:159) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:433) [storm-client-2.2.0.y.jar:2.2.0.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242]
Caused by: java.lang.ClassCastException: org.apache.storm.generated.Credentials cannot be cast to java.util.Map
	at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:303) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.executor.Executor.accept(Executor.java:287) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 7 more
{code}



note: ""2.2.0.y"" is our internal version, which is master branch."
STORM-3607,Document the exceptions topologies will see from TGT renewal thread,This is to document STORM-3606 in the code so users can be less confusing about the exceptions from TGT renewal thread.
STORM-3606,AutoTGT shouldn't invoke TGT renewal thread (from UserGroupInformation.loginUserFromSubject),"When hadoop security is enabled, 
https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/AutoTGT.java#L199-L209

AutoTGT will invoke ""loginUserFromSubject"", and it will spawn a TGT renewal thread (""TGT Renewer for <username>""). 
https://github.com/apache/hadoop/blob/branch-2.8.5/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java#L928-L957

which will eventually invoke system command ""kinit -R"", and then fail with the exception

{code:java}
org.apache.hadoop.util.Shell$ExitCodeException: kinit: Credentials cache file '/tmp/krb5cc_xxx' not found while renewing credentials

	at org.apache.hadoop.util.Shell.runCommand(Shell.java:1004) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.run(Shell.java:898) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307) ~[stormjar.jar:?]
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation$1.run(UserGroupInformation.java:1011) [stormjar.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}


""kinit"" will never work from worker process since Storm don't keep TGT in local cache. Instead, TGT is saved in zookeeper and in memory of Worker process. 

This exception is confusing but not harmful to topologies. And the TGT renewal thread will eventually abort. 

It's better to find a real solution for it. But for now we can document what might happen in AutoTGT code.

To be clear, we still need loginUserFromSubject or some sort but we don't want to spawn TGT renewal thread.  This is found with hadoop-2.8.5. Other versions are similar. But it can also change in the future release."
STORM-3602,loadaware shuffle can overload local worker,"We were seeing a worker overloaded and tuples timing out with loadaware shuffle enabled.  From investigating, we found that the code allows switching from Host local to Worker local if the load average is lower than the low water mark.  It really should be checking the load on the worker instead. 

 

What's happening is the worker is overloaded with tons of idle host local tasks, so it switches to HOST_LOCAL.  Then the calculation across all the host tasks is below the low water mark and it immediately switches back to the overloaded worker local task.

 

 "
STORM-3601,"when i run DRPCServerTest, it occur java.lang.NoClassDefFoundError: org/apache/storm/thrift/TBase","when i run DRPCServerTest, occur a error:
{code:java}
java.lang.NoClassDefFoundError: org/apache/storm/thrift/TBase{code}
the project seem no such class called TBase in package :
{code:java}
org/apache/storm/thrift/{code}
, it is missing in a history version?"
STORM-3598,Storm UI visualization throws NullPointerException,"We encountered an issue with visualization on UI. 

 
{code:java}
2020-03-09 19:59:01.756 o.a.s.d.u.r.StormApiResource qtp1919834117-167291 [ERROR] Failure getting topology visualization
java.lang.NullPointerException: null
        at org.apache.storm.stats.StatsUtil.mergeWithAddPair(StatsUtil.java:1855) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.expandAveragesSeq(StatsUtil.java:2308) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.aggregateAverages(StatsUtil.java:832) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.aggregateBoltStats(StatsUtil.java:731) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.stats.StatsUtil.boltStreamsStats(StatsUtil.java:900) ~[storm-server-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.ui.UIHelpers.getVisualizationData(UIHelpers.java:1939) ~[storm-webapp-2.2.0.y.jar:2.2.0.y]
        at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyVisualization(StormApiResource.java:423) ~[storm-webapp-2.2.0.y.jar:2.2.0.y]
{code}
This is a bug in the code. https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java#L1846-L1858
{code:java}
for (K kk : mm1.keySet()) {
                    List seq1 = mm1.get(kk);
                    List seq2 = mm2.get(kk);
                    List sums = new ArrayList();
                    for (int i = 0; i < seq1.size(); i++) {
                        if (seq1.get(i) instanceof Long) {
                            sums.add(((Number) seq1.get(i)).longValue() + ((Number) seq2.get(i)).longValue());
                        } else {
                            sums.add(((Number) seq1.get(i)).doubleValue() + ((Number) seq2.get(i)).doubleValue());
                        }
                    }
                    tmp.put(kk, sums);
                }
{code}
It assume mm1 and mm2 always have the same key, which is not true. 

And it can be reproduced by my example code: 

 {code:java}
public class  WordCountTopology extends ConfigurableTopology {
    private static final Logger LOG = LoggerFactory.getLogger(WordCountTopology.class);

    public static void main(String[] args) {
        ConfigurableTopology.start(new WordCountTopology(), args);
    }

    protected int run(String[] args) {
        TopologyBuilder builder = new TopologyBuilder();

        builder.setSpout(""spout1"", new RandomSpout(1), 1);
        builder.setSpout(""spout2"", new RandomSpout(2), 1);
        builder.setBolt(""bolt"", new RandomBolt(), 2).directGrouping(""spout1"", ""stream1"")
                .directGrouping(""spout2"", ""stream2"");

        String topologyName = ""word-count"";

        conf.setNumWorkers(3);

        if (args != null && args.length > 0) {
            topologyName = args[0];
        }
        return submit(topologyName, conf, builder);
    }

    static class RandomSpout extends BaseRichSpout {
        String stream;
        int id;

        public RandomSpout(int id) {
            this.id = id;
            stream = ""stream"" + id;
        }

        int taskId = 0;
        SpoutOutputCollector collector;
        public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
            taskId = context.getThisTaskId();
            this.collector = collector;
        }

        /**
         * Different spout send tuples to different bolt via different stream.
         */
        public void nextTuple() {
            LOG.info(""emitting {}"", id);
            if (id == 1) {
                Values val = new Values(""test a sentence"");
                collector.emitDirect(2, stream, val, val);
            } else {
                Values val = new Values(""test 2 sentence"");
                collector.emitDirect(3, stream, val, val);
            }
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {
            declarer.declareStream(stream, new Fields(""word""));
        }
    }

    static class RandomBolt extends BaseBasicBolt {

        public void execute(Tuple input, BasicOutputCollector collector) {
            LOG.info(""executing:"" + input.getSourceComponent());
        }

        public void declareOutputFields(OutputFieldsDeclarer declarer) {

        }
    }
}
{code}

 In this example, one of the bolt will only receive data from stream1 and another bolt will only receive data from stream2. So in the map, 

 {code:java}
                    List seq1 = mm1.get(kk);
                    List seq2 = mm2.get(kk);
{code}
seq1 is null if kk is stream1, seq2 is null if kk is stream2.

 

 We have other places aggregating executor stats without this problem because it's using different code https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/stats/StatsUtil.java#L502-L513 and this problem has been taken cared of."
STORM-3597,workers not getting killed after topology gets killed,"Hi Team,

I am using apache storm version 1.2.2.

Topology was not getting killed from the storm UI as it was throwing exception - "" Error while communicating with Nimbus"" , Then after killing it from the storm cli , it says KILLED both at UI and cli but doesn't goes off and the workers are still running on the supervisors. Please advise on how to kill and remove this topology.

 

!image-2020-03-11-13-47-41-005.png!

!image-2020-03-11-13-45-59-811.png!"
STORM-3591,Improve GRAS Strategy Log,"[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java#L123]
{code:java}
2020-02-24 14:53:59.652 o.a.s.s.r.s.s.GenericResourceAwareStrategy pool-21-thread-1 [WARN] Scheduling [[1, 1]] left over task (most likely sys tasks)
{code}
This message seems to be confusing on debugging.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java#L82]

Default Strategy actually uses debug level instead of warn."
STORM-3589,Iterator in BaseResourceStrategy is potentially buggy,"[https://github.com/apache/storm/blame/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java#L280]

We should probably only peek but not remove value from nodeIterator in hasNext() function.

 

[https://github.com/apache/storm/blame/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java#L296-L300]

 

And two consecutive next() call will cause problem."
STORM-3587,Allow Scheduler futureTask to gracefully exit and register message on timeout,"ResourceAwareScheduler creates a FutureTask with timeout specified in DaemonConfig.

ConstraintSolverStrategy uses the the another configuration variable to determine when to terminate its effort. Limit this value so that it terminates at most slightly before TimeoutException. This graceful exit allows result (and its error) to be available in ResourceAwareScheduler.

 "
STORM-3582,Kryo errors when using custom serialization,"(this has been reported on the user list in January and previously by a different user)
 
------------------------------------------------------------------------------------
 
My code works fine with Storm 1.x but the workers crash constantly with Storm 2.x 
 
Some exceptions look like 
 
{{_com.esotericsoftware.kryo.KryoException: Buffer underflow._}}
{{_at com.esotericsoftware.kryo.io.Input.require(Input.java:199) ~[kryo-3.0.3.jar:?]_}}
{{_at com.esotericsoftware.kryo.io.Input.readUtf8_slow(Input.java:575) ~[kryo-3.0.3.jar:?]_}}
{{_at com.esotericsoftware.kryo.io.Input.readUtf8(Input.java:553) ~[kryo-3.0.3.jar:?]_}}
{{_at com.esotericsoftware.kryo.io.Input.readString(Input.java:483) ~[kryo-3.0.3.jar:?]_}}

{{whereas others are}}
 
{{_2020-01-21 11:13:39.368 o.a.s.m.n.StormServerHandler Netty-server-localhost-6701-worker-1 [ERROR] server errors in handling the request_}}
{{_com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: *95*
at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:670) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:153) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:793) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:689) ~[kryo-3.0.3.jar:?]
at org.apache.storm.serialization.KryoValuesDeserializer.deserializeFrom(KryoValuesDeserializer.java:31) ~[storm-client-2.1.0.jar:2.1.0]_}}


{{the class ID values are random integers. }}

{{}}
The tuples in my topologies contain mostly standard classes like String; the only exception is the following class 
 
[https://github.com/DigitalPebble/storm-crawler/blob/2.x/core/src/main/java/com/digitalpebble/stormcrawler/Metadata.java#L41]
 
for which we specify a custom serialization for Kryo.
 
My configurations contain
 
| topology.kryo.register:|- com.digitalpebble.stormcrawler.Metadata|
to register the custom class.
 
The user who had reported the problem first also used custom serialization.
 
 
 "
STORM-3581,Change log level to info to show the config classes being used for validation,"[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java#L82]

This is trivial but since it's caused some confusion, it's better to have it in the log as INFO instead of DEBUG
{code:java}
LOG.debug(""Will use {} for validation"", ret);
{code}
 

Because the classes being used for validation depends on whether the following file is in the classpath or not

 

[https://github.com/apache/storm/blob/master/storm-server/src/main/resources/META-INF/services/org.apache.storm.validation.Validated]"
STORM-3580,Config overrides supplied using -c in storm.py not passed to all commands,"-c is used to supply configuration overide options. Storm.py in the client code converts these overrides into one -Dstorm.options system property. However, this jvm option is not handled properly when the actual command is executed. For example, Rebalance command completely ignores this setting.

Commands that currently process ""-c"" options:
 * Activate - Not Needed
 * *AdminCommands - Yes*
 * BasicDrpcClient - Not Needed
 * Blobstore - Not Needed
 * CLI - Not Needed
 * *ConfigValue - Yes*
 * Deactivate - Not Needed
 * *DevZookeeper - Yes*
 * *DRPCServer - Yes*
 * GetErrors - Not Needed
 * *HealthCheck - Yes*
 * *Heartbeats - Yes*
 * KillTopology - Not Needed
 * *KillWorkers - Yes*
 * ListTopologies - Not Needed
 * *LogViewerServer - Yes*
 * *LocalCluster - Yes*
 * Monitor - Not Needed
 * *Nimbus - Yes*
 * *Pacemaker - Yes*
 * Rebalance - {color:#ff0000}Added as part of this Jira{color}
 * SetLogLevel - Not Needed
 * *ShellSubmission - Yes*
 * *StormSqlRunner - Yes*
 * Supervisor -Not Needed
 * *UI - Yes*
 * *UploadCredentials - Yes, but specific options*
 * VersionsInfo - Not Needed

 "
STORM-3579,Fix Kerberos connection from Worker to Nimbus/Supervisor,BUG2 in the parent JIRA
STORM-3578,ClientAuthUtils.insertWorkerTokens removes exiting and new WorkerToken altogether if they are equal,BUG1 in the parent JIRA
STORM-3577,upload-credentials Breaks Topology in secure cluster,"*Background*

Worker uses WorkerToken to connect to Nimbus/Supervisor, (e.g. in Worker.doHeartBeat method). If WorkerToken is not in place, it will fall back to Kerberos.

 

*Issue:*

Users can submit topology and the topology is running fine.

But error shows up in worker log if ""storm upload-credentials"" is executed (with AutoTGT being used). (2.2.0.y is our internal version of apache-storm master branch)

 
{code:java}
2020-02-04 00:12:57.975 o.a.s.d.w.Worker heartbeat-timer [WARN] Exception when send heartbeat to local supervisor
2020-02-04 00:12:57.984 o.a.s.s.a.k.ClientCallbackHandler heartbeat-timer [WARN] Could not login: the client is being asked for a password, but the  client code does not currently support obtaining a password from the user. Make sure that the client is configured to use a ticket cache (using the JAAS configuration setting 'useTicketCache=true)' and restart the client. If you still get this message after that, the TGT in the ticket cache has expired and must be manually refreshed. To do so, first determine if you are using a password or a keytab. If the former, run kinit in a Unix shell in the environment of the user who is running this client using the command 'kinit <princ>' (where <princ> is the name of the client's Kerberos principal). If the latter, do 'kinit -k -t <keytab> <princ>' (where <princ> is the name of the Kerberos principal, and <keytab> is the location of the keytab file). After manually refreshing your cache, restart this client. If you continue to see this message after manually refreshing your cache, ensure that your KDC host's clock is in sync with this host's clock.
2020-02-04 00:12:57.984 o.a.s.s.a.k.KerberosSaslTransportPlugin heartbeat-timer [ERROR] Server failed to login in principal:javax.security.auth.login.LoginException: No password provided
javax.security.auth.login.LoginException: No password provided
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:919) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.storm.messaging.netty.Login.login(Login.java:300) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.messaging.netty.Login.<init>(Login.java:84) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:112) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:69) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:80) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:221) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:179) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.heartbeatToMasterIfLocalbeatFail(Worker.java:456) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.doHeartBeat(Worker.java:361) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.lambda$loadWorker$2(Worker.java:209) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.2.0.y.jar:2.2.0.y]
2020-02-04 00:12:57.985 o.a.s.u.NimbusClient heartbeat-timer [WARN] Ignoring exception while trying to get leader nimbus info from quadiumtan-ni.tan.ygrid.yahoo.com. will retry with a different seed host.
java.lang.RuntimeException: java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:108) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:69) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:80) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:221) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:179) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.utils.NimbusClient.getConfiguredClient(NimbusClient.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.heartbeatToMasterIfLocalbeatFail(Worker.java:456) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.doHeartBeat(Worker.java:361) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.daemon.worker.Worker.lambda$loadWorker$2(Worker.java:209) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.2.0.y.jar:2.2.0.y]
Caused by: java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:117) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 10 more
Caused by: javax.security.auth.login.LoginException: No password provided
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:919) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.storm.messaging.netty.Login.login(Login.java:300) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.messaging.netty.Login.<init>(Login.java:84) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.mkLogin(KerberosSaslTransportPlugin.java:112) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.kerberosConnect(KerberosSaslTransportPlugin.java:171) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:138) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:98) ~[storm-client-2.2.0.y.jar:2.2.0.y]
	... 10 more
{code}
It can be reproduced by
{code:java}
/storm jar /home/y/lib64/jars/storm-starter.jar  org.apache.storm.starter.WordCountTopology wc -c topology.debug=false

kinit -R # refresh TGT. This is must-have. So upload-credentials will do something and trigger the bug

storm upload-credentials wc

## Errors will show up in worker log in up to 30s (credential refresh period)
{code}
 

*BUGS*

 

*BUG1* When new credentials got uploaded, Worker will try to update credentials. But while it does it, it will also try to replace WorkerToken if it changes. But it has a bug in the code:

[https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/ClientAuthUtils.java#L411-L416]

 

Here in the code, ""token"" could equal to ""previous"" if tokens didn't change because WorkerToken.equals() method only cares about the content of WorkerToken. The result of this function is the tokens got removed completely.

So in this case, because tokens are not present, Worker will fall back to use kerberos to connect to Nimbus/Supervisor. [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java#L122-L139]

And here comes the second bug

*BUG2*. Kerberos connection from Worker to Nimbus/Supervisor is not working properly, hence the error logs above. "
STORM-3575,Fix Scheduler Status on failure after multiple attempts,"The RAS on multiple attempts when fails to schedule a topology, it is overriding status as to with {color:#FF0000}_Failed to schedule within 5 attempts_{color}
But I think, it should append this message to existing reason/status on the topology.

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java#L239]"
STORM-3574,Rebalance should re-computeExecutors so metrics consumers can be added,"Since metrics consumers are a system component and dynamically adding them should be possible with simple configuration change _topology.metrics.consumer.register_. Currently the _computeExecutors_ or more precisely - _StormCommon#startTaskInfo_ which evaluates this variable is only invoked during _startTopology_ , the rebalance functionality is not starting newer tasks.

We need to ensure rebalance recalculates the idToExecutors again - hopefully only adding new tasks for added executors without removing old ids."
STORM-3573,Performance issues with Thrift 0.13.0,"We pushed Storm 2.2 with Thrift 0.13.0 and started seeing performance issues.  Supervisors would fail to launch workers within timeouts and nimbus would fail to send new assignments to supervisors.  

 

When these alerts occurred, we saw supervisor or nimbus CPU spiked.  Restarting the services corrects the problem for a time.

 

Reverting to Thrift 0.11.0 resolved the issue for us.

 

I'll attach screenshots of profile callstacks."
STORM-3567,Topology UI page is showing total resources for each component if not scheduled,
STORM-3566,add serialVersionUID field to class which implement Serializable interface.,add serialVersionUID field to class which implement Serializable interface
STORM-3564,Deprecated task.heartbeat.frequency.secs still set in defaults,"task.heartbeat.frequency.secs has been deprecated but it is still being set in the default config:

[https://github.com/apache/storm/blob/master/conf/defaults.yaml#L217]

As such logs get full of:
{code:java}
o.a.s.v.ConfigValidation main [WARN] task.heartbeat.frequency.secs is a deprecated config please see class org.apache.storm.Config.TASK_HEARTBEAT_FREQUENCY_SECS for more information. {code}"
STORM-3563,Travis fails because of missing maven package from the mirror,"Travis fails because of the following error:
{code:java}
0.39s$ wget http://mirrors.rackhosting.com/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz -P $HOME
--2020-01-03 06:21:16--  http://mirrors.rackhosting.com/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz
Resolving mirrors.rackhosting.com (mirrors.rackhosting.com)... 77.247.64.34, 2a02:4de0:21::2
Connecting to mirrors.rackhosting.com (mirrors.rackhosting.com)|77.247.64.34|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2020-01-03 06:21:17 ERROR 404: Not Found.
{code}

The latest maven version is 3.6.3. 3.6.1 is removed from the mirror. 

We should use https://archive.apache.org/dist/maven/maven-3/3.6.1/ to prevent this from happening every time there is a new maven minor release.

Also we should cache maven to avoid re-download every time. 

"
STORM-3562,"Storm code can not built repeatedly. It means that the same code builds different packages at different times. After comparison, the class file with the same name has the same content, but the location of the method is different. Is there any solution?",
STORM-3561,java.lang.RuntimeException: java.io.UTFDataFormatException,"My application encountered the following problems online, which caused my worker to exit unexpectedly. Does anyone know why this is? I am considering whether to use the  java native serialization instead of Kryo

 

2019-12-29 21:35:41.669 o.a.s.m.n.StormServerHandler Netty-server-localhost-1800-worker-1 [ERROR] server errors in handling the request
java.lang.RuntimeException: java.io.eTFDataFormatException
 at org.apache.storm.serialization.SerializableSerializer.read(SerializableSerializer.java:53) ~[storm-client-2.1.0.jar:2.1.0]
 at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:793) ~[kryo-3.0.3.jar:?]
 at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134) ~[kryo-3.0.3.jar:?]
 at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40) ~[kryo-3.0.3.jar:?]
 at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:689) ~[kryo-3.0.3.jar:?]
 at org.apache.storm.serialization.KryoValuesDeserializer.deserializeFrom(KryoValuesDeserializer.java:31) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:45) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:66) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.netty.Server.enqueue(Server.java:146) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.netty.Server.received(Server.java:264) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.messaging.netty.StormServerHandler.channelRead(StormServerHandler.java:51) ~[storm-client-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:323) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:426) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1434) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:965) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [storm-shaded-deps-2.1.0.jar:2.1.0]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_231]
Caused by: java.io.UTFDataFormatException
 at java.io.ObjectInputStream$BlockDataInputStream.readUTFSpan(ObjectInputStream.java:3470) ~[?:1.8.0_231]
 at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3414) ~[?:1.8.0_231]
 at java.io.ObjectInputStream$BlockDataInputStream.readUTF(ObjectInputStream.java:3226) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readUTF(ObjectInputStream.java:1133) ~[?:1.8.0_231]
 at java.io.ObjectStreamClass.readNonProxy(ObjectStreamClass.java:768) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readClassDescriptor(ObjectInputStream.java:891) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1857) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1751) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2042) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2287) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2211) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2069) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1573) ~[?:1.8.0_231]
 at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431) ~[?:1.8.0_231]
 at org.apache.storm.serialization.SerializableSerializer.read(SerializableSerializer.java:51) ~[storm-client-2.1.0.jar:2.1.0]
 ... 31 more
2019-12-29 21:35:41.673 o.a.s.m.n.StormServerHandler Netty-server-localhost-1800-worker-1 [INFO] Received error in netty thread.. terminating server...
2019-12-29 21:35:41.674 o.a.s.d.w.Worker Thread-43 [INFO] Shutting down worker owl_analyze_1228_0-16-1577543204 f611fb0f-3a97-4043-bb42-8f74190926ff-10.16.20.6 1800
2019-12-29 21:35:41.675 o.a.s.u.Utils Thread-42 [INFO] Halting after 3 seconds"
STORM-3560,Worker Process 2d4d998d-4a8c-43ca-aafd-19808c63e4da exited with code: 139,"Our online storm application has the problem of worker exiting abnormally. There is no abnormal information in worker.log. You can see the restart log in worker.log. It is found in supervisor.log:


Worker Process 2d4d998d-4a8c-43ca-aafd-19808c63e4da exited with code: 139
  2019-12-30 12: 33: 26.248 o.a.s.d.s. Slot SLOT_1801 [WARN] SLOT 1801: main process has exited


Then the worker exits and is restarted by the supervisor. Is this a segment fault generated by the jni call to the C code?"
STORM-3559,storm2.0.0 can not support python2.6.6 ,"Due to the lack of the argparse module in Python2.6.6, the various roles of Storm fail to start
but storm2.0.0 claims to support Python2.6.6"
STORM-3558,com/codahale/metrics/JmxReporter is not found on LocalCluster,"Hi team,

I need add the following metrics-core dependency in the pom.xml, otherwise the exception will be thrown when I run storm locally. Is it a bug?

pom.xml:

    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-server</artifactId>
        </dependency>

        <dependency>
            <groupId>com.codahale.metrics</groupId>
            <artifactId>metrics-core</artifactId>
            <scope>compile</scope>
        </dependency>

    </dependencies>

Exception log:

18:10:10.810 [main] INFO  o.a.s.d.m.ClientMetricsUtils - Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
18:10:10.811 [main] INFO  o.a.s.d.m.r.JmxPreparableReporter - Preparing...
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/codahale/metrics/JmxReporter
	at org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter.prepare(JmxPreparableReporter.java:32)
	at org.apache.storm.metric.StormMetricsRegistry.startMetricsReporters(StormMetricsRegistry.java:74)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:287)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:159)
	at com.cc.trident.TridentWordCountTopologyLocal.main(TridentWordCountTopologyLocal.java:29)
Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.JmxReporter
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 5 more
"
STORM-3556,blob meta exception in Nimbus log,"Hi team,

When I kill topology on the UI, [WARN] get blob meta exception will be thrown in the Nimbus log. Is it a bug?


2019-12-18 02:14:16.782 o.a.s.d.n.Nimbus pool-29-thread-46 [INFO] Created download session ec557d66-96d9-4071-b99f-3122fa50f61c for word-count-9-1576653249-stormjar.jar
2019-12-18 02:14:16.794 o.a.s.d.n.Nimbus pool-29-thread-50 [INFO] Created download session 1bbae64e-89c6-4337-b87b-634dbb78f61c for word-count-9-1576653249-stormconf.ser
2019-12-18 02:14:16.807 o.a.s.d.n.Nimbus pool-29-thread-40 [INFO] Created download session a3f78bd3-673f-4780-a402-90708bd754f7 for word-count-9-1576653249-stormcode.ser
2019-12-18 02:14:17.472 o.a.s.d.n.Nimbus pool-29-thread-18 [INFO] Created download session a05848fa-e83e-4c13-b025-f14a468acf14 for word-count-9-1576653249-stormjar.jar
2019-12-18 02:14:17.483 o.a.s.d.n.Nimbus pool-29-thread-58 [INFO] Created download session 9119de95-cd34-48a4-b7ef-ae0f84de5689 for word-count-9-1576653249-stormcode.ser
2019-12-18 02:14:17.495 o.a.s.d.n.Nimbus pool-29-thread-29 [INFO] Created download session d8d06e2e-e82c-46a8-beee-ee8bf313e584 for word-count-9-1576653249-stormconf.ser
2019-12-18 02:14:17.538 o.a.s.d.n.Nimbus pool-29-thread-21 [INFO] Created download session f634e74b-a161-4295-bf17-bd631b3d6e6d for word-count-9-1576653249-stormjar.jar
2019-12-18 02:14:17.546 o.a.s.d.n.Nimbus pool-29-thread-18 [INFO] Created download session 29e1592a-136a-4ff6-a4de-9c4420d55849 for word-count-9-1576653249-stormcode.ser
2019-12-18 02:14:17.557 o.a.s.d.n.Nimbus pool-29-thread-48 [INFO] Created download session ca75e5b0-e855-4b5f-962f-f7accefdc260 for word-count-9-1576653249-stormconf.ser
2019-12-18 02:23:42.461 o.a.s.d.n.Nimbus pool-29-thread-36 [INFO] TRANSITION: word-count-9-1576653249 KILL 5 true
2019-12-18 02:23:42.463 o.a.s.d.n.Nimbus pool-29-thread-36 [INFO] Delaying event REMOVE for 5 secs for word-count-9-1576653249
2019-12-18 02:23:42.471 o.a.s.d.n.Nimbus pool-29-thread-36 [INFO] Adding topo to history log: word-count-9-1576653249
2019-12-18 02:23:47.464 o.a.s.d.n.Nimbus timer [INFO] TRANSITION: word-count-9-1576653249 REMOVE null false
2019-12-18 02:23:47.467 o.a.s.d.n.Nimbus timer [INFO] Killing topology: word-count-9-1576653249
2019-12-18 02:25:58.331 o.a.s.d.n.Nimbus timer [INFO] Cleaning up word-count-9-1576653249
2019-12-18 02:25:58.338 o.a.s.c.StormClusterStateImpl timer [INFO] Removing worker keys under /secretkeys/NIMBUS/word-count-9-1576653249
2019-12-18 02:25:58.339 o.a.s.c.StormClusterStateImpl timer [INFO] Removing worker keys under /secretkeys/DRPC/word-count-9-1576653249
2019-12-18 02:25:58.340 o.a.s.c.StormClusterStateImpl timer [INFO] Removing worker keys under /secretkeys/SUPERVISOR/word-count-9-1576653249
2019-12-18 02:25:58.341 o.a.s.d.n.Nimbus timer [INFO] Removing dependency jars from blobs - []
2019-12-18 02:26:04.774 o.a.s.d.n.Nimbus pool-29-thread-61 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: word-count-9-1576653249-stormcode.ser
	at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:257) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:287) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3651) [storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3951) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3930) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:174) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
2019-12-18 02:26:04.784 o.a.s.d.n.Nimbus pool-29-thread-5 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: word-count-9-1576653249-stormjar.jar
	at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:257) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:287) ~[storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3651) [storm-server-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3951) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3930) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:174) [storm-client-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) [storm-shaded-deps-2.1.0.jar:2.1.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_45]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]"
STORM-3554,LocalCluster can't be shutdown and JVM still run,"Hi team,

[Storm version: 2.1.0] 

According to the doc [https://storm.apache.org/releases/2.1.0/Local-mode.html], after I shutdown LocalCluster by calling shutdown or close method, JVM still run and can't exit on IDEA. I think it is a simple demo blow. I also try storm 1.2.3 and JVM can exit. Is it a bug for storm 2 ?

 

import org.apache.storm.Config;
 import org.apache.storm.LocalCluster;
 import org.apache.storm.topology.TopologyBuilder;

public class WordCountTopology {

public static void main(String[] args) throws Exception

{

TopologyBuilder builder = new TopologyBuilder();

// builder.setSpout(""spout"", new RandomSentenceSpout(), 5);

// builder.setBolt(""split"", new SplitSentenceBolt(), 8).shuffleGrouping(""spout"");

// builder.setBolt(""count"", new WordCountBolt(), 2).fieldsGrouping(""split"", new Fields(""word""));

Config conf = new Config();

conf.setDebug(true);

String topologyName = ""word-count"";

conf.setNumWorkers(2);

LocalCluster cluster = null;

cluster = new LocalCluster();

cluster.submitTopology(topologyName, conf, builder.createTopology());

Thread.sleep(10000);

// cluster.close();

cluster.shutdown();

}

}


POM.xml:

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <storm.version>2.1.0</storm.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-client</artifactId>
            <version>${storm.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.storm</groupId>
            <artifactId>storm-server</artifactId>
            <version>${storm.version}</version>
        </dependency>
    </dependencies>
"
STORM-3553,Upgrade JQuery to 3.5.1,"JQuery < 3.4.0 has some security issues ([https://snyk.io/vuln/npm:jquery)]

 JQuery 1.11.1 that currently being used is having this security issue:
 - Prototype Pollution"
STORM-3552,Storm CLI set_log_level no longer updates the log level,"Using the example StatefulWindowingTopology, when trying to update the log level via command line with the following command a NullPointer is thrown in the worker log and the log level is not updated.
{code:java}
storm set_log_level -l ROOT=DEBUG:0 test{code}
{code:java}
2019-12-09 17:16:02.600+0100 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl main-EventThread [ERROR] Event listener threw exception
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_131]
        at org.apache.logging.log4j.Level.getLevel(Level.java:261) ~[log4j-api-2.11.2.jar:2.11.2]
        at org.apache.storm.daemon.worker.LogConfigManager.setLoggerLevel(LogConfigManager.java:145) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.daemon.worker.LogConfigManager.processLogConfigChange(LogConfigManager.java:98) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.daemon.worker.Worker.checkLogConfigChanged(Worker.java:422) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl.issueMapCallback(StormClusterStateImpl.java:177) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl$1.changed(StormClusterStateImpl.java:122) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.cluster.ZKStateStorage$ZkWatcherCallBack.execute(ZKStateStorage.java:243) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.zookeeper.ClientZookeeper.lambda$mkClientImpl$0(ClientZookeeper.java:314) ~[storm-client-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$7.apply(CuratorFrameworkImpl.java:1048) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$7.apply(CuratorFrameworkImpl.java:1041) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:100) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:30) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:92) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.processEvent(CuratorFrameworkImpl.java:1040) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.access$000(CuratorFrameworkImpl.java:66) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$1.process(CuratorFrameworkImpl.java:126) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.ConnectionState.process(ConnectionState.java:185) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:533) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:508) [storm-shaded-deps-2.1.0.jar:2.1.1-SNAPSHOT]{code}
This appears to be a regression from the migration from clojure to java in STORM-1267"
STORM-3551,Fix LocalAssignment Equivalency in Slot for Generice Resource Aware Scheduler,"If supervisor defines generic resource then it needs to ignore it from comparison while assignment is not using such generic resource.

 

{code}

2019-12-03 21:02:10.635 o.a.s.d.s.Slot SLOT_6726 [INFO] SLOT 6726: Assignment Changed from LocalAssignment(topology_id:TEST-WordCount-281-1570127542, executors:[ExecutorInfo(task_start:261, task_end:261), ExecutorInfo(task_start:188, task_end:188)], resources:WorkerResources(mem_on_heap:10000.0, mem_off_heap:0.0, cpu:400.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:\{offheap.memory.mb=0.0, onheap.memory.mb=10000.0, cpu.pcore.percent=400.0}, shared_resources:{}), owner:bud_storm) to LocalAssignment(topology_id:TEST-WordCount-281-1570127542, executors:[ExecutorInfo(task_start:261, task_end:261), ExecutorInfo(task_start:188, task_end:188)], resources:WorkerResources(mem_on_heap:10000.0, mem_off_heap:0.0, cpu:400.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:\{offheap.memory.mb=0.0, network.resource.units=0.0, onheap.memory.mb=10000.0, cpu.pcore.percent=400.0}, shared_resources:{}), owner:bud_storm)

{code}"
STORM-3549,use of topology specific jaas conf doesn't work with kafka,"{code:java}
2019-09-17 19:22:23.006 o.a.s.u.Utils Thread-22-line-reader-spout-executor[4, 4] [ERROR] Async loop died!
org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:702) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) [storm-client-2.0.1.y.jar:2.0.1.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
Caused by: org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:86) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	... 10 more
Caused by: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:940) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.kafka.common.security.authenticator.AbstractLogin.login(AbstractLogin.java:69) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.kerberos.KerberosLogin.login(KerberosLogin.java:110) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.&lt;init&gt;(LoginManager.java:46) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:68) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:78) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	... 10 more
2019-09-17 19:22:23.196 o.a.s.e.e.ReportError Thread-22-line-reader-spout-executor[4, 4] [ERROR] Error
java.lang.RuntimeException: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.storm.utils.Utils$1.run(Utils.java:445) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:702) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	... 1 more
Caused by: org.apache.kafka.common.KafkaException: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:86) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	... 1 more
Caused by: javax.security.auth.login.LoginException: Could not login: the client is being asked for a password, but the Kafka client code does not currently support obtaining a password from the user. not available to garner  authentication information from the user
	at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:940) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:760) ~[?:1.8.0_181]
	at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_181]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_181]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_181]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680) ~[?:1.8.0_181]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680) ~[?:1.8.0_181]
	at javax.security.auth.login.LoginContext.login(LoginContext.java:587) ~[?:1.8.0_181]
	at org.apache.kafka.common.security.authenticator.AbstractLogin.login(AbstractLogin.java:69) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.kerberos.KerberosLogin.login(KerberosLogin.java:110) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.&lt;init&gt;(LoginManager.java:46) ~[stormjar.jar:?]
	at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:68) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:78) ~[stormjar.jar:?]
	at org.apache.kafka.common.network.ChannelBuilders.create(ChannelBuilders.java:70) ~[stormjar.jar:?]
	at org.apache.kafka.clients.ClientUtils.createChannelBuilder(ClientUtils.java:83) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:623) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:557) ~[stormjar.jar:?]
	at org.apache.kafka.clients.consumer.KafkaConsumer.&lt;init&gt;(KafkaConsumer.java:540) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:26) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.internal.ConsumerFactoryDefault.createConsumer(ConsumerFactoryDefault.java:22) ~[stormjar.jar:?]
	at org.apache.storm.kafka.spout.KafkaSpout.open(KafkaSpout.java:147) ~[stormjar.jar:?]
	at org.apache.storm.executor.spout.SpoutExecutor.init(SpoutExecutor.java:148) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:158) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.spout.SpoutExecutor.call(SpoutExecutor.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$1.run(Utils.java:425) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	... 1 more
2019-09-17 19:22:23.277 o.a.s.u.Utils Thread-22-line-reader-spout-executor[4, 4] [ERROR] Halting process: Worker died
java.lang.RuntimeException: Halting process: Worker died
	at org.apache.storm.utils.Utils.exitProcess(Utils.java:550) [storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.Utils$3.run(Utils.java:846) [storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException(ReportErrorAndDie.java:41) [storm-client-2.0.1.y.jar:2.0.1.y]
	at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_181]
2019-09-17 19:22:23.281 o.a.s.u.Utils Thread-26 [INFO] Halting after 1 seconds
{code}"
STORM-3548,Remove iterator from Task.sendUnanchored,"Storm 2.x aims to remove iterators from the critical path to reduce garbage.

 

This method is called to send acking tuples so should use a for loop instead of list iterator."
STORM-3547,KinesisSpout incorrectly handles closed shards,"The KinesisSpout throws an exception when consuming closed Kinesis shards, which return null from `GetShardIterator`, as it tries to call `GetRecords` with the null `shardIterator` value instead of abandoning the closed shard.

This means KinesisSpout fails after re-sharding a stream with an exception like the following:

 
{code:java}
java.lang.RuntimeException: com.amazonaws.services.kinesis.model.AmazonKinesisException: 1 validation error detected: Value null at 'shardIterator' failed to satisfy constraint: Member must not be null (S ervice: AmazonKinesis; Status Code: 400; Error Code: ValidationException; Request ID: d85076a4-3953-fb6e-8e08-331e7d91ef0f) at org.apache.storm.utils.Utils$1.run(Utils.java:407) ~[storm-client-2.1.0.jar:2.1.0] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101] Caused by: com.amazonaws.services.kinesis.model.AmazonKinesisException: 1 validation error detected: Value null at 'shardIterator' failed to satisfy constraint: Member must not be null (Service: AmazonKin esis; Status Code: 400; Error Code: ValidationException; Request ID: d85076a4-3953-fb6e-8e08-331e7d91ef0f) at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1695) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1350) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1101) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:758) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:732) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:714) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:674) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:656) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:520) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.doInvoke(AmazonKinesisClient.java:2803) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2772) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.invoke(AmazonKinesisClient.java:2761) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.executeGetRecords(AmazonKinesisClient.java:1288) ~[stormjar.jar:3.4.6-1569965] at com.amazonaws.services.kinesis.AmazonKinesisClient.getRecords(AmazonKinesisClient.java:1259) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisConnection.fetchRecords(KinesisConnection.java:113) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisRecordsManager.fetchNewRecords(KinesisRecordsManager.java:329) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisRecordsManager.next(KinesisRecordsManager.java:135) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.kinesis.spout.KinesisSpout.nextTuple(KinesisSpout.java:82) ~[stormjar.jar:3.4.6-1569965] at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:192) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:159) ~[storm-client-2.1.0.jar:2.1.0] at org.apache.storm.utils.Utils$1.run(Utils.java:392) ~[storm-client-2.1.0.jar:2.1.0]
{code}
 

Same Problem was resolved in https://issues.apache.org/jira/browse/BEAM-2582

 

 "
STORM-3543,Avoid iterators for task hook info objects,"A big drive was made in 2.0 to avoid iterators on the critical path to reduce garbage collection.

 

When task hooks are used, an iterator is used for each info object to pass it to all hooks.

 

As this is on the critical path when task hooks are used, the same reasoning should apply and the iterators be replaced with for loops."
STORM-3542,storm-kafka-monitor has wrong classpath,"When
{code:ruby}
ui.disable.spout.lag.monitoring: false
{code}
storm-kafka-monitor execution fails with   
{code:java}
Could not find or load main class org.apache.storm.kafka.monitor.KafkaOffsetLagUtil""
{code}
 

storm-kafka-monitor contains the following instruction:
{code:bash}
exec $JAVA $STORM_JAAS_CONF_PARAM $STORM_JAR_JVM_OPTS -cp $STORM_BASE_DIR/toollib/storm-kafka-monitor-*.jar org.apache.storm.kafka.monitor.KafkaOffsetLagUtil ""$@""{code}
When it should contain this one:
{code:bash}
exec $JAVA -Xms64m -Xmx64m $STORM_JAAS_CONF_PARAM -cp ""$STORM_BASE_DIR/lib-tools/storm-kafka-monitor/*"" org.apache.storm.kafka.monitor.KafkaOffsetLagUtil ""$@""
{code}"
STORM-3540,Pacemaker race condition can cause continual reconnection,Seeing issues with connections to pacemaker with some workers despite pacemaker being up.
STORM-3537,Ports configed in storm.yaml file can be used by other application.,"When submit a storm topology, it fail if the port is used by other application, for example a python Flask application.  
{code:java}
//代码占位符
// storm.yaml
supervisor.slots.ports:
 - 6720orc@bj2904:

// start storm nibums ans Supervisor

// start python flask 
~/program/simhash_doc_title$ ./start.sh 
 * Serving Flask app ""/home/orc/program/simhash_doc_title/simhash.py""
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://0.0.0.0:6720/ (Press CTRL+C to quit)


// submit storm topology
2019-11-15 18:16:12.310 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.security.PrivilegedActionException: java.net.BindException: Address already in use
    at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:653) ~[storm-client-2.1.0.jar:2.1.0]
    at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:632) ~[storm-client-2.1.0.jar:2.1.0]
    at org.apache.storm.utils.Utils.lambda$createDefaultUncaughtExceptionHandler$2(Utils.java:1014) ~[storm-client-2.1.0.jar:2.1.0]
    at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_191]
    at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_191]
    at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_191]
Caused by: java.security.PrivilegedActionException
    at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_191]
    at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_191]
    at org.apache.storm.daemon.worker.Worker.start(Worker.java:180) ~[storm-client-2.1.0.jar:2.1.0]
    at org.apache.storm.daemon.worker.Worker.main(Worker.java:144) ~[storm-client-2.1.0.jar:2.1.0]
Caused by: java.net.BindException: Address already in use
    at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_191]
    at sun.nio.ch.Net.bind(Net.java:433) ~[?:1.8.0_191]
    at sun.nio.ch.Net.bind(Net.java:425) ~[?:1.8.0_191]
    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:1.8.0_191]
    at org.apache.storm.shade.io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:130) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1358) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:1019) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:366) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:462) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) ~[storm-shaded-deps-2.1.0.jar:2.1.0]
    at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_191]
{code}"
STORM-3535,KafkaOffsetMetric is not properly synchronized,"KafkaOffsetMetric.getValueAndReset runs in a different thread from the rest of the spout. It uses the KafkaConsumer from the KafkaSpout. The consumer is accessed through an unsynchronized field, and the spout may replace the consumer at any time.

We should consider whether we can fix this, or if it would be better to give the offset metric it's own consumer.

Also the metric accesses a number of properties in OffsetManagers, which are also not synchronized."
STORM-3533,Make a distinct storm-ras artifact,"Storm 2.0.0 release page [here|http://storm.apache.org/2019/05/30/storm200-released.html] suggest us to put storm-server as test scope for testing. But RAS packages are in storm-server package too, and we need it when we launch our topology. Some of RAS packages should be either integrated in storm-core or in a distinct package storm-ras, so that we do not have to put storm-server as compile test, which we do for now."
STORM-3532,Support Kubernetes in Storm Supervisor,"Hello,

We are using Apache Storm 1.x and we want to migrate it to 2.x on Kubernetes. I found that we can't request resources from Kubernetes directly. I searched if there is any integration with Kubernetes: when Storm Supervisor can run workers' slots inside separate pods(I refer to the [https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java#L852]) instead of run multiple supervisors on the same K8s node.

I see the following tickets https://issues.apache.org/jira/browse/STORM-911 and https://issues.apache.org/jira/browse/STORM-1740 which could be related to my proposal.

 

I propose to add an ability to Storm Supervisor manage worker processes via Kubernetes /another API and not just on the same host.

 

 "
STORM-3531,Kafka Spouts Lag error when worker die,"I deploy topology on cluster (3 Supervisors) with 3 workers, 3 Spouts read topic kafka with 3 partitions.

Step 1: I kill worker1  on Supervisor1

Step 2: Worker1 automatic running again.

Step 3: Kafka spout of worker1 register partition 1 and read not lag. Worker2 and worker3 error lag kafka, not continue read and commit offset

Log worker: 

{color:#FF0000}Not polling on partition [TEST-0]. It has [250] uncommitted offsets, which exceeds the limit of [250]{color}"
STORM-3529,Catch and log RetriableException in KafkaOffsetMetric,"When the KafkaOffsetMetric.getValueAndReset method calls the KafkaClient methods, exceptions may be thrown. When these exceptions are retriable, we should not crash the worker by letting them escape the method. We should instead catch and log the exception.

An example of the desired behavior can be seen at https://github.com/apache/storm/blob/7b1a98fc10fad516ef9ed0b3afc53a1d7be8a169/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L295"
STORM-3525,Large Contraint Solver test fails on some VM,TestConstraintSolverStrategy.testScheduleLargeExecutorConstraintCount() method fails on some VMs when the parallelismMultiplier is set 20.
STORM-3524,worker fails to launch due to missing parent directory for localized resource,"{code:java}
2019-10-14 14:59:29.839 o.a.s.l.LocalizedResource AsyncLocalizer Executor - 2 [WARN] Nothing to cleanup with badeDir /home/y/var/storm/supervisor/usercache/xxx/filecache/files even though we expected there to be something there 2019-10-14 14:59:29.839 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [WARN] Failed to download blob xxx:xxx.topology.yaml will try again in 100 ms java.nio.file.NoSuchFileException: /home/y/var/storm/supervisor/usercache/xxx/filecache/files at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[?:1.8.0_181] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.8.0_181] at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.8.0_181] at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) ~[?:1.8.0_181] at java.nio.file.Files.createDirectory(Files.java:674) ~[?:1.8.0_181] at org.apache.storm.localizer.LocalizedResource.lambda$fetchUnzipToTemp$4(LocalizedResource.java:257) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.LocallyCachedBlob.fetch(LocallyCachedBlob.java:92) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.LocalizedResource.fetchUnzipToTemp(LocalizedResource.java:250) ~[storm-server-2.0.1.y.jar:2.0.1.y] at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$10(AsyncLocalizer.java:277) ~[storm-server-2.0.1.y.jar:2.0.1.y] at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) [?:1.8.0_181] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_181] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_181] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_181] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}
 

A worker on a supervisor was failing to come up with this error continually presenting."
STORM-3523,supervisor restarts when releasing slot with missing file,"{code:java}
2019-10-03 16:25:32.809 o.a.s.d.s.Slot SLOT_6719 [ERROR] Error when processing event
java.io.FileNotFoundException: File 'x/storm/supervisor/stormdist/xxx-190213-004131-001-209-1550018519/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:297) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1851) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:308) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:469) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:303) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.localizer.AsyncLocalizer.getLocalResources(AsyncLocalizer.java:359) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.localizer.AsyncLocalizer.releaseSlotFor(AsyncLocalizer.java:460) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:435) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:229) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [storm-server-2.0.1.y.jar:2.0.1.y]
2019-10-03 16:25:32.810 o.a.s.u.Utils SLOT_6719 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:550) [storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:947) [storm-server-2.0.1.y.jar:2.0.1.y]
{code}"
STORM-3522,Some code can not find,"When I download source code by github, end import the source code into intellij idea. but the code do not work, some code can not find.

How can i resolve this issue

For example, the org.apache.storm.daemon.nimbus.Nimbus depend on org.apache.storm.shade.com.google.common.base.Strings, but can not find the java file in storm-core

 "
STORM-3521,Storm CLI jar command doesn't handle topology arguments correctly,
STORM-3520,Storm CLI drpc-client incorrectly validating function args,
STORM-3519,Change ConstraintSolverStrategy::backtrackSearch to avoid StackOverflowException,"When ConstraintSolverStrategy::backtrackSearch recursively call itself - after approximately 20000 calls, there is a StackOverflowException. This can be replicated by running TestConstraintSolverStrategy::testScheduleLargeExecutorConstraintCount."
STORM-3517,NotifyBuilder.from does not normalize endpoint URI,"When using NotifyBuilder.from, it is possible to supply both a URI pattern and an actual URI for matching. NotifyBuilder matches actual endpoint URIs against the specified pattern using EndpointHelper.matchEndpoint(URI endpointUri, String pattern), which does either exact or regex matching. Before matching, matchEndpoint will normalize the URI in order to ensure that e.g. query parameter order doesn't matter.

It would be nice if NotifyBuilder.from would attempt to normalize the pattern/URI specified in the ""from"" method. If it isn't possible to normalize, the pattern can be passed to matchEndpoint without modification, but if the ""from"" method is used with a concrete URI, query parameter order there can be some surprising behavior during the matching.

For example, given a route like

from(""sjms:queue:my-queue?transacted=true&consumerCount=1"")

the following will not match

new NotifyBuilder()
.from(""sjms:queue:my-queue?transacted=true&consumerCount=1"")

The reason for this is that Camel will normalize the route URI given in the actual route before passing it to EndpointHelper, but the pattern given to NotifyBuilder is not normalized, so the order of query parameters end up not matching.

It would be good to either update NotifyBuilder so it tries normalizing the URI, or updating EndpointHelper.matchEndpoint, so it tries normalizing both the URI parameter and the pattern parameter."
STORM-3515,Storm CLI config options are passed directly to underlying JAVA cli,
STORM-3514,"""topology.enable.message.timeouts: false"" has no effect on ackers","""topology.enable.message.timeouts: false"" does prevent tuples from being failed if not acked in ""topology.message.timeout.secs"" seconds, but it still prevents __ackers from acking anchored tuples to the spout. When used with ""topology.max.spout.pending"" this effectively stalls the spout completely as the tuple is neither failed, nor acked."
STORM-3513,Add note to install instructions for Windows users that they need to install Visual C++ redistributable,"See the note at the bottom of this section https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#maven

And also this stackoverflow question https://stackoverflow.com/questions/58123218/i-have-a-trouble-running-nimbus-apache-storm-2-0-0-on-windows"
STORM-3512,Nimbus failing on startup with `GLIBC_2.12' not found,"Nimbus failing to start with and exception (see below).

 
{code:java}
2019-09-25 17:21:56.013 o.a.s.u.Utils main [ERROR] Received error in thread main.. terminating server...
java.lang.Error: java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni3787537456845796855.so: /lib64/libpthread.so.0: version `GLIBC_2.12' not found (required by /tmp/librocksdbjni3787537456845796855.so)
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:647) ~[storm-client-2.0.0.jar:2.0.0]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:626) ~[storm-client-2.0.0.jar:2.0.0]
        at org.apache.storm.utils.Utils.lambda$createDefaultUncaughtExceptionHandler$2(Utils.java:982) ~[storm-client-2.0.0.jar:2.0.0]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_211]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_211]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_211]
Caused by: java.lang.UnsatisfiedLinkError: /tmp/librocksdbjni3787537456845796855.so: /lib64/libpthread.so.0: version `GLIBC_2.12' not found (required by /tmp/librocksdbjni3787537456845796855.so)
        at java.lang.ClassLoader$NativeLibrary.load(Native Method) ~[?:1.8.0_211]
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941) ~[?:1.8.0_211]
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824) ~[?:1.8.0_211]
        at java.lang.Runtime.load0(Runtime.java:809) ~[?:1.8.0_211]
        at java.lang.System.load(System.java:1086) ~[?:1.8.0_211]
        at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64) ~[rocksdbjni-5.8.6.jar:?]
        at org.rocksdb.RocksDB.<clinit>(RocksDB.java:35) ~[rocksdbjni-5.8.6.jar:?]
        at org.apache.storm.metricstore.rocksdb.RocksDbStore.prepare(RocksDbStore.java:67) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.metricstore.MetricStoreConfig.configure(MetricStoreConfig.java:33) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:528) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:471) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:465) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.launchServer(Nimbus.java:1282) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1307) ~[storm-server-2.0.0.jar:2.0.0]
        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1312) ~[storm-server-2.0.0.jar:2.0.0]
 {code}
 

Environment:
{code:java}
>>> uname -a
Linux gctdwp03 3.0.101-108.98-default #1 SMP Mon Jul 15 13:58:06 UTC 2019 (262a94d) x86_64 x86_64 x86_64 GNU/Linux
 
>>> cat /etc/SuSE-release
SUSE Linux Enterprise Server 11 (x86_64)
VERSION = 11
PATCHLEVEL = 4{code}
 "
STORM-3511,Nimbus logs got flood with TTransportException Error messages (because of thrift 0.12.0),"Submitting a wordCountTopology works in secure cluster. But the following

{code:java}
2019-09-25 13:53:46.560 o.a.s.t.s.TThreadPoolServer pool-15-thread-1 [ERROR] Thrift error occurred during processing of message.
org.apache.storm.thrift.transport.TTransportException: null
        at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:374) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:451) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslTransport.read(TSaslTransport.java:433) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TSaslServerTransport.read(TSaslServerTransport.java:43) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:425) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:321) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:225) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:27) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310) [shaded-deps-2.0.1.y.jar:2.0.1.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_181]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}
flood the nimbus log. (2.0.1.y is our internal version. The code is basically community 2.0.0)

This is similar to the issue found in Thrift community and got fixed in 0.13.0 but it's not released yet https://issues.apache.org/jira/browse/THRIFT-4805

"
STORM-3510,WorkerState.transferLocalBatch backpressure resend logic fix,"WorkerState.transferLocalBatch uses an int lastOverflowCount to track the size of the overflow queue, and periodically resend the backpressure status to remote workers if the queue continues to grow.

 

The current implementation has two problems:
 * The single variable tracks the receive queue of every executor in the worker, meaning it will be overwritten as tuples are sent to different executors.
 * The variable is locally scoped, and so is not carried over between mini-batches.

 

This only comes in to effect when the overflow queue grows beyond 10000, which shouldn't happen unless a backpressure signal isn't received by an upstream worker, but if it does happen then a backpressure signal is going to be sent for every mini-batch processed.  I do not know if this is the intended behaviour, but the way the code is written seems to indicate that it isn't.

 

I have thought of two redesigns to fix these problems and make the behaviour align with how one would interpret the code:

 
 #  *Change the lastOverflowCount variable to a map of taskId to overflow count* - This will retain the behaviour of resending the backpressure update every mini-batch once over the threshold, if that behaviour is intended.  However, it will increase garbage by creating a new map every time WorkerState.transferLocalBatch is called by the NettyWorker thread.
 # *Change the lastOverflowCount variable to a map of taskId to overflow count* *and move it to the BackPressureTracker class* - This will retain the counter between batches, and so only resend backpressure status every 10000 received tuples per task.

 

My preference is for the second option, as if the intended behaviour is to resend every mini batch it should be rewritten so the intent is explicit from the code.

 

It is also possible that doing it the second way could run in to concurrency issues i didn't think of, but as far as i can tell the topology.worker.receiver.thread.count config option isn't used at all?  If that's the case and there is only one NettyWorker thread per worker then it should be fine.

 

I have implemented both methods and attempted to benchmark them with [https://github.com/yahoo/storm-perf-test] but as i am running all workers on one machine i couldn't get it to the point that the relevant code was ever called."
STORM-3506,prevent topology from overriding STORM_CGROUP_HIERARCHY_DIR and WORKER_METRICS,We had an issue where users were using older versions of storm the set these differing values than the cluster supported.  These parameters don't make sense for topologies to override.
STORM-3505,improve topology config creation,"We currently allow topologies to possibly override any field in a config.  This can cause problems requiring special case filtering when creating the conf at submission time.

Going forward it would be nice to have a strict naming convention for all topology conf keys (and code support) to prevent overriding anything that is not topology specific.

 

 "
STORM-3504,AsyncLocalizerTest is stubbing file system operations,"AsyncLocalizerTest mocks AdvancedFSOps in order to avoid interacting with the real file system. This is most likely unnecessary, and could be replaced with using temporary files/directories. If possible, we should rewrite the tests to use temporary files, and use the real AdvancedFSOps."
STORM-3502,metrics-core isn't shaded,"""Apache Storm version 1.2 introduced a new metrics system for reporting internal statistics (e.g. acked, failed, emitted, transferred, disruptor queue metrics, etc.) as well as a new API for user defined metrics.

The new metrics system is based on [Dropwizard Metrics|http://metrics.dropwizard.io/].""

Currently at my company we're still using a storm version that does not have any metric reporter (1.1.x), we're planning to move to 2.2 version, however since, we've been using codahale metrics reporter to track some events on storm and since the newer versions already include this, it means that we have to change every topology we already have in production.

It would be nice, that this dependency inside of storm could be shaded in order to ease any migration, or at least without a lot of refactoring on the topologies, I assume this problem must be similar in other companies, and storm already does this for other artifacts.

I would be happy to perform the necessary changes in order to accomplish this."
STORM-3501,Local Cluster worker restarts,"I was trying to launch a topology that I'm developing (in 2.0.0) and noticed that the worker was getting restarted each ~30 seconds. 
 I placed a breakpoint in the _kill_ method of _LocalContainer_ ([https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainer.java#L66]) to try and understand why the worker was getting restarted. 
  
 The call stack was:
_kill:66, LocalContainer (org.apache.storm.daemon.supervisor)
killContainerFor:269, Slot (org.apache.storm.daemon.supervisor)
handleRunning:724, Slot (org.apache.storm.daemon.supervisor) 
stateMachineStep:218, Slot (org.apache.storm.daemon.supervisor)
run:931, Slot (org.apache.storm.daemon.supervisor) _
  
 With this I can understand that the worker is killed because a blob has changed ([https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java#L724]). In fact, there's a changing blob in the _dynamicState_ at that point.
  
 I checked the _AsyncLocalizer_ which downloads, caches blobs locally, and notifies the Slot state machine of a changing blob.
  
 I noticed this:
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L339]
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L265]
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java#L142]
 * [https://github.com/apache/storm/blob/2ba95bbd1c911d4fc6363b1c4b9c4c6d86ac9aae/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java#L192]
  

Which tell me that (correct me if I'm wrong):
 * Supervisor tries to update blobs each 30 seconds.
 * The topology jar blob requires extraction of the resources directory (either from a jar or directly in a classpath URL). It does so in _fetchUnzipToTemp_ and it's existence is checked in _isFullyDownloaded_.
 * The Slot is notified of a changing blob if:
 * the remote version is different from the local version (the code has changed).
 * OR the blob is not fully downloaded (the jar exists, and the extracted resources directory exists).

 
 Well, I did not have a resources folder under the root of the classpath, and that's why the worker was being restarted each ~30 seconds, as the Slot was being notified of a changing blob everytime _updateBlobs_ ran. 
 I created a resources folder (with dummy files) under the root of the classpath and the problem is now solved.
  
 However, if I understand correctly, the resources folder is only required for _multilang_. Our topologies do not use _multilang_ and this do not happen in Storm 1.1.3 for instance.

 

Happy to submit MR."
STORM-3500,Spelling issue in storm.blobstore.dependency.jar.upload.chuck.size.bytes,"It should be ""chunk"". As the property hasn't been in a release yet, we can safely fix it without worrying about backward compatibility"
STORM-3494,Use UserGroupInformation to login to HDFS only once per process,"UserGroupInformation (UGI) loginUserFromKeytab should be used only once in a process to login to hdfs because it overrides static fields. Also loginUserFromKeytabAndReturnUGI function is also problematic according to hadoop team. So the correct way to connect to hdfs is to use UGI loginUserFromKeytab once and only in a process.

Currently we only use HDFS in hdfs-blobstore. It works correctly. But the code is implemented in the hdfs-blobstore plugin. It will be problematic if we want to add another plugin which also needs to connect to HDFS.

So the proposal here is to remove the login piece of code from hdfs-blobstore. And explicitly login to hdfs once and only once when the server (nimbus, supervisor, etc) or the client (storm cli command) launches. It can guarantee one login per process.

The plugins like hdfs-blobstore then simply assume the process has already logged in."
STORM-3493,Allow overriding python interpreter by environment variable,"$subj

https://github.com/apache/storm/pull/3111

 "
STORM-3491,BoltReaderRunnable shouldn't throw IllegalArgumentException for sync command,"So I was preparing for a new release candidate i.e. rc3. I can build it from source without any problem. Then I set up a standalone cluster and submitted a WordCountTopology. The workers kept restarting because of 

{code:java}
2019-08-19 15:09:49.635 o.a.s.t.ShellBolt Thread-30 [ERROR] Halting process: ShellBolt died. Command: [python, splitsentence.py], ProcessInfo pid:3756, name:split exitCode:-1, errorString:
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
2019-08-19 15:09:49.635 o.a.s.e.e.ReportError Thread-30 [ERROR] Error
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
2019-08-19 15:09:49.636 o.a.s.t.ShellBolt Thread-28 [ERROR] Halting process: ShellBolt died. Command: [python, splitsentence.py], ProcessInfo pid:3755, name:split exitCode:-1, errorString:
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
2019-08-19 15:09:49.637 o.a.s.e.e.ReportError Thread-28 [ERROR] Error
java.lang.IllegalArgumentException: command sync is not supported
        at org.apache.storm.task.ShellBolt$BoltReaderRunnable.run(ShellBolt.java:366) [storm-client-2.1.0.jar:2.1.0]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_181]
{code}


https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/task/ShellBolt.java#L365-L367 I believe we shouldn’t throw exceptions here. "
STORM-3486,Upgrade to Jersey 2.29,Jersey 2.29 supports Java 11 http://blog.supol.cz/?p=144
STORM-3485,VersionInfoMojo fails to run when the sources are built outside the Git repo,
STORM-3483,Backpressure turns on after topology is idle for 160 minutes,"Storm version used 1.2.2

While testing a topology we noticed that backpressure was being triggered on an idle topology, without any data going through the system for roughly 160 minutes. However once we starting putting data through the system, backpressure behaved as normal and is off.

We have storm metrics logger to log the disruptor queue sizes, and during this test we can see the receive queue population is 0 when backpressure kicks in. 

 

We confirmed this by turning debug logs on for storm.

!image-2019-08-08-17-32-10-241.png!

 

In an idle system where backpressure occurs... 

!image-2019-08-08-16-02-09-750.png!

 

 

What a normal system with data going through looks like...

!image-2019-08-08-15-59-25-655.png!

 "
STORM-3481,IllegalArgumentException in ConstraintSolverStrategy,"We found this scheduling error based on our internal mirror. 
{code:java}
2019-08-06 13:00:20.344 o.a.s.s.r.ResourceAwareScheduler timer [ERROR] propane-0-170-1564778552 Internal Error - Exception thrown when scheduling. Please check logs for details
java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: Don't know how to convert null to int
        at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_181]
        at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_181]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.scheduleTopology(ResourceAwareScheduler.java:164) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.schedule(ResourceAwareScheduler.java:117) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.schedule(BlacklistScheduler.java:118) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2092) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2256) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2242) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2187) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$29(Nimbus.java:2890) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.1.y.jar:2.0.1.y]
Caused by: java.lang.IllegalArgumentException: Don't know how to convert null to int
        at org.apache.storm.utils.ObjectReader.getInt(ObjectReader.java:55) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.strategies.scheduling.ConstraintSolverStrategy.schedule(ConstraintSolverStrategy.java:263) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.resource.ResourceAwareScheduler.lambda$scheduleTopology$3(ResourceAwareScheduler.java:161) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_181]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_181]
{code}
"
STORM-3478,Upgrade to JUnit 5.5.1,"https://github.com/apache/storm/pull/2990 upgraded to 5.5.0-M1, we should upgrade to a final version."
STORM-3477,HDFS blobstore isRemoteBlobExists performs unnecessary file opens,"isRemoteBlobExists eventually performs an HDFS file open and returns an input file stream, which is not closed.

 

We should just be calling file exists instead.  Lower HDFS overhead."
STORM-3476,LocalizedResourceRetentionSet cleanup causing excessive load on Hadoop namenode,"One of our local dev Hadoop devs noticed our storm user was by far creating the heaviest load on our production Hadoop cluster.  Looking at one of the heaviest supervisor nodes, and comparing debug logs to the Hadoop audit log, it looks like LocalizedResourceRetentionSet cleanup was constantly doing opens and never deleting any files.

 

The frequency can be addressed by supervisor.localizer.cleanup.interval.ms, but even so, it seems we will continually look for files to delete even when the target size is acceptable, resulting in unnecessary calls to Hadoop.

 

 "
STORM-3473,Hive can't read records written from HiveBolt,"I'm trying to stream items from storm into hive using the HiveBolt, but Hive does not seem to see the records at all.

Test program:
{code:java}
package com.datto.hivetest;

import org.apache.storm.Config;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.hive.bolt.HiveBolt;
import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.spout.SpoutOutputCollector;
import org.apache.storm.streams.StreamBuilder;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichSpout;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.Time;

import java.util.Map;
import java.util.Random;

public class MainStorm {
	public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException {
		HiveOptions hiveOptions = new HiveOptions(
			""<url>"",
			""default"",
			""test_table"",
			new JsonRecordHiveMapper()
				.withColumnFields(new Fields(""value""))
		)
			.withAutoCreatePartitions(true);

		StreamBuilder builder = new StreamBuilder();
		builder.newStream(new TestSpout())
			.map(tup -> tup.getStringByField(""word"").toLowerCase())
			.to(new HiveBolt(hiveOptions));

		Config config = new Config();
		config.setMessageTimeoutSecs(30);
		config.setMaxSpoutPending(1024);
		config.setClasspath(""/etc/hadoop/conf/"");

		StormSubmitter.submitTopology(""hive-test"", config, builder.build());
	}

	public static class TestSpout extends BaseRichSpout {
		private transient SpoutOutputCollector out;
		private transient Random random;

		@Override
		public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {
			out = collector;
			random = new Random();
		}

		@Override
		public void nextTuple() {
			try {
				Time.sleep(100);
			} catch (InterruptedException e) {
				Thread.currentThread().interrupt();
				throw new RuntimeException(e);
			}

			final String[] words = new String[]{ ""nathan"", ""mike"", ""jackson"", ""golda"", ""bertels"" };
			final String word = words[random.nextInt(words.length)];
			out.emit(new Values(word));
		}

		@Override
		public void declareOutputFields(OutputFieldsDeclarer declarer) {
			declarer.declare(new Fields(""word""));
		}
	}
}
{code}
Table creation:
{code:sql}
CREATE TABLE test_table (value string) CLUSTERED BY (value) INTO 4 BUCKETS STORED AS ORC TBLPROPERTIES('orc.compress' = 'ZLIB', 'transactional' = 'true');

GRANT ALL ON test_table TO USER storm;{code}

Setting the ACL:

{code}
sudo -u hdfs hdfs dfs -setfacl -m user:storm:rwx /warehouse/tablespace/managed/hive/test_table
sudo -u hdfs hdfs dfs -setfacl -m default:user:storm:rwx /warehouse/tablespace/managed/hive/test_table
{code}

Hive results after running for around 10 minutes:

{code:java}
> SELECT COUNT(*) FROM test_table;
INFO  : Compiling command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403): SELECT COUNT(*) FROM test_table
INFO  : Semantic Analysis Completed (retrial = false)
INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
INFO  : Completed compiling command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403); Time taken: 1.138 seconds
INFO  : Executing command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403): SELECT COUNT(*) FROM test_table
INFO  : Completed executing command(queryId=hive_20190722195152_2315b4c9-f527-4b6e-8652-151d9c4f6403); Time taken: 0.013 seconds
INFO  : OK
+------+
| _c0  |
+------+
| 0    |
+------+
{code}

So hive thinks there are no results, which isn't good. But if I look at hdfs, there are some files there:

{code}
# sudo -u hdfs hdfs dfs -ls -R -h /warehouse/tablespace/managed/hive/test_table
drwxrwx---+  - storm hadoop          0 2019-07-22 19:15 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100
-rw-rw----+  3 storm hadoop          1 2019-07-22 19:15 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/_orc_acid_version
-rw-rw----+  3 storm hadoop     74.4 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00001
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00001_flush_length
-rw-rw----+  3 storm hadoop     73.4 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00002
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00002_flush_length
-rw-rw----+  3 storm hadoop     84.9 K 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00003
-rw-rw----+  3 storm hadoop        376 2019-07-22 19:27 /warehouse/tablespace/managed/hive/test_table/delta_0000001_0000100/bucket_00003_flush_length
{code}

And they seem to have valid rows:

{code}
❯❯❯ ./orc-contents /tmp/bucket_00002  | head
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 0, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 1, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 2, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 3, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 4, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 5, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 6, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 7, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 8, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{""operation"": 0, ""originalTransaction"": 1, ""bucket"": 537001984, ""rowId"": 9, ""currentTransaction"": 1, ""row"": {""value"": ""bertels""}}
{code}

I can insert into the table manually, and I've also written a test java program that uses the hive streaming API to write one row, and hive sees those inserts. I don't see any errors in the storm logs; the tuples seem to be flushed and acked ok. I don't think I've seen any errors in the metastore logs either.

Anyone know what's up? I can get more info if needed."
STORM-3472,"STORM-3411 should have tests, and we shouldn't catch NPE for control flow","I think the code merged in STORM-3411 should have added tests that the new functionality works.

We should get rid of the new bit of code that try-catches an NPE to check whether the downloaded file is inside a worker dir. Instead, we should move the name generation up the call hierarchy to a place where we can tell whether we're inside a worker dir or not."
STORM-3470,Possible Null Dereference in SimpleSaslServer authentication function,"On line 183, nid could possible be null. By comparing using ""nid.equals()"" we will get a null pointer exception instead of comparing possibly null values."
STORM-3466,storm-kafka-monitor not found jar,"Hi, i'm a beginner to the storm. in order words, i'm a newbie.

 

I have tried to upgrade from Storm 1.2.2 to 2.0.0. and confirmed that kafka spout lag does not work. changed the value of ui.disable.spout.lag.monitoring to false, but it did not work.

So I checked the log of storm ui and got the following error message.

 

org.apache.storm.utils.ShellUtils$ExitCodeException: Error: Could not find or load main class .apache-storm-2.0.0.lib-tools.storm-kafka-monitor.audience-annotations-0.5.0.jar

at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:271) ~[storm-client-2.0.0.jar:2.0.0]
 at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:194) ~[storm-client-2.0.0.jar:2.0.0]...

 

In version 1.2.2, the storm-kafka-monitor works fine. However, version 2.0.0 throws an error."
STORM-3437,More license check automation,
STORM-3436,"TupleInfo.id is not set, making debugging more difficult than it should be","TupleInfo has an id field, which is supposed to contain the tuple root id. This is printed when we enable topology debug logging, and is supposed to make it easier to track down e.g. why a tuple timed out.

We currently don't set this field, so it's always null. The logs end up looking like

2019-06-29 11:26:31.990 o.a.s.e.s.SpoutExecutor Thread-14-kafka_spout-executor[4, 4] [INFO] SPOUT Acking message null {topic-partition=kafka-spout-test-0, offset=28, numFails=0, nullTuple=false}

The message doesn't contain the root id, and is therefore not useful for debugging."
STORM-3435,Use the Jetty BOM,"We should import the Jetty BOM, we're currently mixing different versions of Jetty jars together on the classpath because we only declare some of them, while others are pulled in transitively."
STORM-3422,TupleCaptureBolt is not thread-safe,"Marking this as Major because it's a crash. That said, the problem lies in testing code. This makes integration testing hard, but the issue does not affect any production code.

 

First, let me show you a stack trace for Storm 2.0.0:

{{java.lang.RuntimeException: java.lang.NullPointerException}}
{{at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:171) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.bolt.BoltExecutor$1.call(BoltExecutor.java:158) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.utils.Utils$1.run(Utils.java:388) [storm-client-2.0.0.jar:2.0.0]}}
{{at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]}}
{{Caused by: java.lang.NullPointerException}}
{{at org.apache.storm.testing.TupleCaptureBolt.execute(TupleCaptureBolt.java:45) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.bolt.BoltExecutor.tupleActionFn(BoltExecutor.java:234) ~[storm-client-2.0.0.jar:2.0.0]}}
{{at org.apache.storm.executor.Executor.accept(Executor.java:275) ~[storm-client-2.0.0.jar:2.0.0]}}
{{... 6 more}}

 

 Here's the same for Storm 1.2.2:

{{java.lang.RuntimeException: java.lang.NullPointerException}}
{{at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:74) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.daemon.executor$fn__10795$fn__10808$fn__10861.invoke(executor.clj:861) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [storm-core-1.2.2.jar:1.2.2]}}
{{at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]}}
{{at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]}}
{{Caused by: java.lang.NullPointerException}}
{{at org.apache.storm.testing.TupleCaptureBolt.execute(TupleCaptureBolt.java:50) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.daemon.executor$fn__10795$tuple_action_fn__10797.invoke(executor.clj:739) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.daemon.executor$mk_task_receiver$fn__10716.invoke(executor.clj:468) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.disruptor$clojure_handler$reify__10135.onEvent(disruptor.clj:41) ~[storm-core-1.2.2.jar:1.2.2]}}
{{at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.2.jar:1.2.2]}}
{{... 6 more}}

 

This is a topology running as our integration test using {{Testing.completeTopology()}}. Both the stack traces point to the same code in the {{TupleCaptureBolt}} - its {{name}} field is not safely published (it should be marked {{final}}), and the internal {{HashMap}} does not safely store the data put in it. Perhaps it should be a {{ConcurrentHashMap}}?

Would you accept a PR with a more detailed analysis, or are you going to investigate on your side?"
STORM-3408,Rocks version shipped with Storm2 doesn't work on Windows 10,"The version of rocks referenced by storm 2 cannot be used on windows 10 due to https://github.com/facebook/rocksdb/issues/2531 - this means `LocalClusterRunner` cannot be instantiated.

The stack trace is
{code}java.lang.UnsatisfiedLinkError: C:\Users\...\AppData\Local\Temp\librocksdbjni5428427063666929934.dll: A dynamic link library (DLL) initialization routine failed

	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)
	at java.lang.Runtime.load0(Runtime.java:809)
	at java.lang.System.load(System.java:1086)
	at org.rocksdb.NativeLibraryLoader.loadLibraryFromJar(NativeLibraryLoader.java:78)
	at org.rocksdb.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:56)
	at org.rocksdb.RocksDB.loadLibrary(RocksDB.java:64)
	at org.rocksdb.RocksDB.<clinit>(RocksDB.java:35)
	at org.apache.storm.metricstore.rocksdb.RocksDbStore.prepare(RocksDbStore.java:67)
	at org.apache.storm.metricstore.MetricStoreConfig.configure(MetricStoreConfig.java:33)
	at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:528)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:244)
	at org.apache.storm.LocalCluster.<init>(LocalCluster.java:159)
	...{code}

This issue is fixed by at least rocks version 5.17.2. I also note rocks v6 is out.

This is a blocker for us - we cannot upgrade to storm 2 as-is, as we cannot run our local unit tests using a local cluster runner on our Windows-based development machines."
STORM-3406,Allow customizing storm-hdfs FileReader ,Allow customizing storm-hdfs _FileReader_. The interface is currently package private preventing any possible custom implementation
STORM-3404,storm v1.2.2 KafkaOffsetLagUtil cant pull the offset correctly,"when use SASL_PLAIN kafka JAAS auth, missing sasl.mechanism will lead to KafkaOffsetLagUtil cant pull the offset correctly
 kafka version 2.2.0

storm version 1.2.2

The PR url is: https://github.com/apache/storm/pull/3377

 "
STORM-3403,Incorrect Assigned memory displayed on Storm UI,"Hi Team,

 

We are working on storm upgrade from 1.0.2 to 1.2.2. During upgrade, we realised that assigned memory displayed on Storm UI is incorrect. Attached screenshot for more details.

 

Looks like assigned memory is sum of memory provided in topology.worker.childopts and topology.worker.logwriter.childopts options.

 

Multiple scenarios we have observed where  assigned memory is not correct-
 # If topology.worker.childopts memory is more than 3 GB, in this case assigned memory is showing 65 MB.


 # If topology.worker.childopts memory+ topology.worker.logwriter.childopts memory is below 50 for last 2 digits.
For eg.,
topology.worker.childopts = 2048 MB
topology.worker.logwriter.childopts = 64 MB
Total = 2112 MB
Here, last 2 digits are below 50, in this case assigned memory is showing 65 MB."
STORM-3398,Closing socket for xxx because of error (kafka.network.Processor) kafka.network.InvalidRequestException,"deploy a topology on storm cluster, storm start a *org.apache.storm.kafka.monitor.KafkaOffsetLagUtil* process, this process error:
{code:java}
[2019-05-31 11:24:54,425] ERROR Closing socket for xxxx:9092-xxxxx:17547 because of error (kafka.network.Processor)
kafka.network.InvalidRequestException: Error getting request for apiKey: 3 and apiVersion: 2
at kafka.network.RequestChannel$Request.liftedTree2$1(RequestChannel.scala:95)
at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:87)
at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:488)
at kafka.network.Processor$$anonfun$processCompletedReceives$1.apply(SocketServer.scala:483)
at scala.collection.Iterator$class.foreach(Iterator.scala:893)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
at kafka.network.Processor.processCompletedReceives(SocketServer.scala:483)
at kafka.network.Processor.run(SocketServer.scala:413)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Invalid version for API key 3: 2
at org.apache.kafka.common.protocol.ProtoUtils.schemaFor(ProtoUtils.java:31)
at org.apache.kafka.common.protocol.ProtoUtils.requestSchema(ProtoUtils.java:44)
at org.apache.kafka.common.protocol.ProtoUtils.parseRequest(ProtoUtils.java:60)
at org.apache.kafka.common.requests.MetadataRequest.parse(MetadataRequest.java:96)
at org.apache.kafka.common.requests.AbstractRequest.getRequest(AbstractRequest.java:48)
at kafka.network.RequestChannel$Request.liftedTree2$1(RequestChannel.scala:92)
{code}
Too many kafka sockets lead to delayed consumption and production of kafka services"
STORM-3397,Upgrade to Zookeeper 3.5,Zookeeper 3.5 has a stable release now. We should upgrade I think.
STORM-3396,uploading dependency jars too slow when StormSubmitter and Nimbus located in different IDC,"     when storm client and server is locating in different IDC(one is in Beijing, while another in Shanghai), uploading dependency jars may take a very long long time(in my case, 31minutes!)...

    when I digged into this, I found that in DependencyUploader,  method ""uploadDependencyToBlobStore"" using JDK NIO's Files.copy to upload local jars to remote Blob server. In Files.copy(Path, OutputStream), the buffer size is 8k by default, given that latency between Beijing and Shanghai is about 20ms, a dependency fat jar of 360M finally cost me 'a lunch time' to finish uploading!!!

   "
STORM-3393,OffsetManager doesn't recover after missing offsets,"When missing offsets are encountered, but a committable offset exists after the missing offset, the condition is detected and logged but not properly processed.  You will see three log messages in this case:
{code:java}
Processed non-sequential offset.  The earliest uncommitted offset is no longer part of the topic.  Missing offset: [{}], Processed: [{}]
...
Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset
...
Topic-partition [{}] has no offsets ready to be committed{code}
However, this is not the proper handling.  While a committable offset has been found, the found flag is not set to true (resulting in the 3rd log message).

The fix is to add a found=true within this logic:

In OffsetManager.java
{code:java}
if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {
                        LOG.debug(""Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset"",
                            currOffset, nextCommitOffset);
                        nextCommitOffset = currOffset + 1;
                        found = true;       //  ADD THIS LINE TO FIX THIS BUG
                    }{code}
Because of this bug, offsets are not committed properly."
STORM-3392,Topology page should show components even if workers haven't started,"After starting a topology, the user can view the topology UI, but the components are not shown until aggregate stats are available which causes a delay. The components are known even before the stats are populated. Improvement is to populate placeholder components aggregate stats until actual aggregate stats become available, which immediately shows the components."
STORM-3391,"MongoMapState causes ""IllegalArgumentException: Invalid BSON field name _id"" while multiPut operation","MongoMapState causes ""IllegalArgumentException: Invalid BSON field name _id"" while multiPut operation. 

mongoDB server: 3.6.7
mongo-java-driver: 3.8.2"
STORM-3390,Lock python test dependencies so we don't get accidentally upgraded,"Tests are currently failing on Travis with

{code}
[INFO] --- exec-maven-plugin:1.6.0:exec (python2.7-test) @ storm-client ---
Traceback (most recent call last):
  File ""test_storm_cli.py"", line 20, in <module>
    import mock
  File ""/home/travis/.local/lib/python2.7/site-packages/mock/__init__.py"", line 2, in <module>
    import mock.mock as _mock
  File ""/home/travis/.local/lib/python2.7/site-packages/mock/mock.py"", line 69, in <module>
    from six import wraps
ImportError: cannot import name wraps
{code}

This is most likely because we're installing ""mock"" via pypi during the build, but we're not specifying a version. Since mock just released a new version, we're getting upgraded to that one on Travis.

I think we don't want this to happen automagically. "
STORM-3387,Test failure in integration test,"{code}
[ERROR] testTumbleTime(org.apache.storm.st.tests.window.TumblingWindowTest)  Time elapsed: 36.751 s  <<< FAILURE!
com.google.gson.JsonSyntaxException: com.google.gson.stream.MalformedJsonException: Unterminated string at line 1 column 32 path $.now
	at com.google.gson.internal.Streams.parse(Streams.java:60)
	at com.google.gson.internal.bind.TreeTypeAdapter.read(TreeTypeAdapter.java:65)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:129)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:220)
	at com.google.gson.Gson.fromJson(Gson.java:887)
	at com.google.gson.Gson.fromJson(Gson.java:852)
	at com.google.gson.Gson.fromJson(Gson.java:801)
	at com.google.gson.Gson.fromJson(Gson.java:773)
	at org.apache.storm.st.topology.window.data.TimeData.fromJson(TimeData.java:58)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:373)
	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1380)
	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
	at org.apache.storm.st.wrapper.TopoWrap.deserializeLogData(TopoWrap.java:303)
	at org.apache.storm.st.wrapper.TopoWrap.getDeserializedDecoratedLogLines(TopoWrap.java:295)
	at org.apache.storm.st.tests.window.WindowVerifier.runAndVerifyTime(WindowVerifier.java:100)
	at org.apache.storm.st.tests.window.TumblingWindowTest.testTumbleTime(TumblingWindowTest.java:91)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:84)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:714)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:901)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1231)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:127)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:111)
	at org.testng.TestRunner.privateRun(TestRunner.java:767)
	at org.testng.TestRunner.run(TestRunner.java:617)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:334)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:329)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:291)
	at org.testng.SuiteRunner.run(SuiteRunner.java:240)
	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52)
	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86)
	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1224)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:1149)
	at org.testng.TestNG.run(TestNG.java:1057)
	at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:135)
	at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.executeMulti(TestNGDirectoryTestSuite.java:193)
	at org.apache.maven.surefire.testng.TestNGDirectoryTestSuite.execute(TestNGDirectoryTestSuite.java:94)
	at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:146)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: com.google.gson.stream.MalformedJsonException: Unterminated string at line 1 column 32 path $.now
	at com.google.gson.stream.JsonReader.syntaxError(JsonReader.java:1559)
	at com.google.gson.stream.JsonReader.nextQuotedValue(JsonReader.java:1017)
	at com.google.gson.stream.JsonReader.nextString(JsonReader.java:815)
	at com.google.gson.internal.bind.TypeAdapters$29.read(TypeAdapters.java:718)
	at com.google.gson.internal.bind.TypeAdapters$29.read(TypeAdapters.java:714)
	at com.google.gson.internal.Streams.parse(Streams.java:48)
	... 50 more
{code}"
STORM-3386,Set minimum Maven version for build to 3.5.0,"Alexandre Vermeerbergen found that the build doesn't work on Maven 3.3.9. This is likely because some plugin requires Maven version 3.5.0, but they forgot to specify that requirement in their POM.

We might as well bump our Maven version check to 3.5.0."
STORM-3384,storm set-log-level command throws wrong exception when the topology is not running,"https://github.com/apache/storm/blob/1.1.x-branch/storm-core/src/clj/org/apache/storm/command/set_log_level.clj#L31
will throw an exception like the following if the topology is not running
{code:java}
3396 [main] INFO  b.s.c.set-log-level - Sent log config LogConfig(named_logger_level:{ROOT=LogLevel(action:UPDATE, target_log_level:DEBUG, reset_log_level_timeout_secs:30)}) for topology w
Exception in thread ""main"" java.lang.IllegalArgumentException: No matching field found: IllegalArgumentException for class java.lang.String
	at clojure.lang.Reflector.getInstanceField(Reflector.java:271)
	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:315)
	at backtype.storm.command.set_log_level$get_storm_id.invoke(set_log_level.clj:31)
	at backtype.storm.command.set_log_level$_main.doInvoke(set_log_level.clj:75)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at backtype.storm.command.set_log_level.main(Unknown Source)
{code}
"
STORM-3383,Apache Storm is not able to hit the python bolt and run it,"Hi,
We have a requirement to send the data to the Python model. We have added bolt for the same. We are testing on local and the topology does not seem to hit the python code.
PFA the topology and python sample bolt with the python job.

Can someone help us with the sample code to achieve it, or provide the solution. It will be a great help."
STORM-3382,"ERROR o.a.s.d.s.ReadClusterState - Failed to Sync Supervisor, while reading the file size of 30 GB","We are reading the contents of a file from S3 via spout. 
When we are reading a small file like 30KB or so, the spout works fine.
But when we read the file of size 40 GB approx the topology fails with the error, ERROR o.a.s.d.s.ReadClusterState - Failed to Sync Supervisor. And the thread is interrupted.
Please finds the logs attached."
STORM-3381,Upgrading to Zookeeper 3.4.14 added an LGPL dependency,https://issues.apache.org/jira/browse/ZOOKEEPER-3367
STORM-3379,Intermittent NPE during worker boot in local mode,"{quote}
java.io.IOException: java.lang.NullPointerException
	at org.apache.storm.daemon.supervisor.LocalContainer.launch(LocalContainer.java:57) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.LocalContainerLauncher.launchContainer(LocalContainerLauncher.java:49) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobUpdate(Slot.java:536) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:230) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:931) [storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
Caused by: java.lang.NullPointerException
	at org.apache.storm.daemon.worker.WorkerState.readWorkerExecutors(WorkerState.java:623) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerState.<init>(WorkerState.java:156) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.worker.Worker.loadWorker(Worker.java:174) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.worker.Worker.lambda$start$0(Worker.java:166) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_201]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_201]
	at org.apache.storm.daemon.worker.Worker.start(Worker.java:165) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	at org.apache.storm.daemon.supervisor.LocalContainer.launch(LocalContainer.java:55) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
	... 4 more
{quote}

The issue is that the WorkerState tries to read executor assignment from ZK, and gets null back. "
STORM-3378,"Clean up integration test a bit, and switch to JUnit 5",
STORM-3377,Scheduling may not fully utilize cluster with STORM_WORKER_MIN_CPU_PCORE_PERCENT set,I encountered a bug when testing STORM_WORKER_MIN_CPU_PCORE_PERCENT.  I need to add a unit test and fix for the case I encountered.
STORM-3376,Storm drops messages in the interval between server bind and callback registration,"This is one of the causes of unstable integration tests.

When a worker starts, WorkerState boots up a Server, which binds Netty to a port in order to receive messages from other workers. Slightly later, we register a callback with the server that defines where the Server should deliver received messages.

In the interim between binding the port and registering the callbacks, Storm quietly discards any received messages.

Other workers will happily send messages to the worker that is not ready, as the sending side considers an open connection to be sufficient to send messages.

We should change the setup so we set the receive callback before we start the Server."
STORM-3374,StormClientHandler exceptionCaught() log message does not warrant callstack,"Info level message spits out a large stack trace.  Since this is apparently not serious, let's not pollute the log."
STORM-3373,"Use Log4j BOM, ensure SLF4J dependencies use the same version, upgrade SLF4J to latest",
STORM-3372,HDFS bolt can throw NPE on shutdown if not using a TimedRotationPolicy,"{quote}42612 [SLOT_1024] ERROR o.a.s.d.s.Slot - Error when processing event
java.lang.NullPointerException: null
    at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.cleanup(AbstractHdfsBolt.java:261) ~[f083f1dc515311e9868bcf07babd3298.jar:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.executor$fn__9739.invoke(executor.clj:878) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at clojure.lang.MultiFn.invoke(MultiFn.java:233) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.executor$mk_executor$reify__9530.shutdown(executor.clj:437) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.worker$fn__10165$exec_fn__1369__auto__$reify__10167$shutdown_STAR___10187.invoke(worker.clj:684) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.worker$fn__10165$exec_fn__1369__auto__$reify$reify__10213.shutdown(worker.clj:724) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:67) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.LocalContainer.kill(LocalContainer.java:69) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.killContainerForChangedAssignment(Slot.java:311) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.handleRunning(Slot.java:527) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:265) ~[storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]
    at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:752) [storm-core-1.2.1.3.1.0.0-78.jar:1.2.1.3.1.0.0-78]{quote}
The error is due to a bug in storm-hdfs.

That variable in https://github.com/apache/storm/blob/v1.2.1/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L261 is only initialized if the rotation policy is a TimedRotationPolicy, which yours isn't."
STORM-3371,Metrics v2 is inaccessible in Trident StateFactory. ,"The injected IMetricsContext only grants access to metrics v1 methods. Metrics v2 is normally accessible via TopologyContext, but this is not supplied to StateFactory. We should add metrics v2 methods to IMetricsContext."
STORM-3370,Make StormMetricRegistry non-static,In STORM-3197 we made the StormMetricsRegistry for internal Storm components non-static. I think we should do the same for the worker-oriented StormMetricRegistry.
STORM-3369,Apache Storm 2.0.0 release artifacts are not avaible in maven central,"Hi All,

Couple of weeks ago, Apache storm 2.0.0 was released (based on github release branches). We have been eyeing the release for some time. With such great news, we wanted to start utilizing it.

However, the release artifacts have not been published yet into maven central: [https://search.maven.org/search?q=org.apache.storm]

We were wondering if it was a matter of time and the release process is not complete yet, or it was a missed step.

Thanks"
STORM-3367,Upgrade Dropwizard Metrics to 5.0.0-rc2,"Dropwizard Metrics 5 introduces metrics tagging. 

One of the goals of metrics v2 was to move worker metrics to dropwizard metrics instead of sending the metrics via Zookeeper. A good way to do this would be to register the metrics with Dropwizard and create a NimbusMetricsReporter that sends metrics to Nimbus. Since we don't want to send all metrics, tags would be useful to allow the reporter to send only metrics relevant for Storm UI.

In relation to https://issues.apache.org/jira/browse/STORM-3204, it will probably also be an easier upgrade, since Metrics 5 has a new package/artifact name, so there won't be conflicts with dependencies."
STORM-3363,Migrate Aether to maven-resolver as Aether was brought to ASF as a subproject of Apache Maven,"Looks like Aether is donated to ASF (in couple of years) and renamed as ""maven-resolver"" as a subproject of Apache Maven.

[https://github.com/apache/maven-resolver]
https://issues.apache.org/jira/browse/MNG-6007
[http://incubator.apache.org/ip-clearance/maven-aether.html]

As it is a kind of complete fork of Aether and further versions have been published here (the latest version of Aether is 1.1.0, whereas the latest version maven-resolver is 1.3.3), it should be easy to migrate to maven-resolver and reduce using EPLv1 license in Apache Storm."
STORM-3361,Add output of license plugin to git and distributions,"We should make it obvious to users which licenses our dependencies are under. In particular category B licensed dependencies need to be listed in a place users can find them.

We could use the license plugin to generate a list of dependencies along with their licenses, and include the generated file in our distributions."
STORM-3360,tuples are getting double from Spout to bolt,the next tuple of the spout sends 10 tuples and the execute of the bolt reads 20. It is like one tuple is received twice before the send tuple comes. the atomacity is 1.
STORM-3358,Upgrade Storm to Hadoop 3,"We should upgrade to Hadoop 3 at some point.

We are currently blocked by https://issues.apache.org/jira/browse/HBASE-22027, but I believe that is the only bit that prevents us from upgrading."
STORM-3357,Bump Clojure to 1.10,"Clojure 1.10 contains a few fixes for Java 9+, notably https://dev.clojure.org/jira/browse/CLJ-2284, and we're currently using 1.7.0. We should upgrade."
STORM-3356,Storm-hive should not pull in a compile-scope sfl4j binding,"Our Hive dependencies are leaking org.apache.logging.log4j:log4j-slf4j-impl into our compile-scope dependencies. This causes a multiple bindings warning when starting e.g. storm-starter, since the jar gets bundled into the topology jar, and there is also a log4j-slf4j-impl present in the Storm cluster. "
STORM-3355,Make force kill delay for workers follow the supervisor's SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS,"We currently have the supervisor.worker.shutdown.sleep.secs parameter allowing users to specify how long the supervisor should wait between starting the initial graceful shutdown of a worker, and sending the followup force kill. 

When workers are asked to shut down gracefully, they run a shutdown hook that allows 1 second of cleanup, before force halting the JVM. I think it would be good to make the delay between starting the shutdown hook and halting the JVM follow the same config as in the supervisor. 

I don't see why it is useful to specify the force kill delay in the supervisor, if the worker just suicides after one second anyway. Letting the user configure how long shutdown is allowed to take lets them make use of the bolt's cleanup method for cleaning up resources in non-crash scenarios.

Use case here https://stackoverflow.com/questions/55024919/resource-clean-up-after-killing-storm-topology"
STORM-3354,LeaderElector is not shut down properly,"Nimbus' LeaderElector is not shut down when Nimbus shuts down. This can cause test flakiness because the elector callback may be called after the Zookeeper client is closed.

Additionally the LeaderListenerCallback may in some cases quit the leadership election if Nimbus isn't ready to become master, but I don't see any code to re-enter the election.
 
{quote}java.lang.IllegalStateException: Client is not started
         at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:139) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:602) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1191) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$17.call(CreateBuilderImpl.java:1158) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:1155) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:605) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:595) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:360) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:308) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.zookeeper.ClientZookeeper.createNode(ClientZookeeper.java:98) ~[storm-client-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.nimbus.LeaderListenerCallback.setUpNimbusInfo(LeaderListenerCallback.java:154) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.nimbus.LeaderListenerCallback.leaderCallBack(LeaderListenerCallback.java:96) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.zookeeper.Zookeeper$1.isLeader(Zookeeper.java:123) ~[storm-server-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) ~[shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:660) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:539) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:590) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT]
         at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [shaded-deps-2.0.1-SNAPSHOT.jar:2.0.1-SNAPSHOT] 
{quote}"
STORM-3353,Upgrade to Curator 4.2.0,Curator 4.2.0 removes an outdated version of Jackson that has some security holes https://issues.apache.org/jira/browse/CURATOR-481.
STORM-3352,Use Netty BOM to lock all Netty artifacts to same version,"We are not properly ensuring that Netty artifacts are the same version. I got a test failure in storm-cassandra, because netty-all is version 4.1.30, but cassandra pulls in Netty in version 4.0.37.

{quote}

java.lang.NoSuchMethodError: io.netty.util.internal.PlatformDependent.normalizedArch()Ljava/lang/String;
    at io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:180) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]
    at io.netty.channel.epoll.Native.<clinit>(Native.java:61) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]
    at io.netty.channel.epoll.Epoll.<clinit>(Epoll.java:38) ~[netty-all-4.1.30.Final.jar:4.1.30.Final]
    at org.apache.cassandra.transport.Server.run(Server.java:147) ~[cassandra-all-2.1.7.jar:2.1.7]

{quote}"
STORM-3351,MqttSpout failed too many," 

  how could cause mqtt_spout failed ?

!image-2019-03-07-10-44-37-680.png!"
STORM-3350,Upgrade some old dependencies,"Jackson, ActiveMQ, commons-collections, commons-compress, Kafka and Maven plugins all have newer versions we should be able to upgrade to with little risk. We should do these upgrades."
STORM-3349,"Upgrade Hadoop, HBase and Hive to latest compatible","We should upgrade Hadoop, Hive, HDFS and HBase to the latest compatible versions. HBase in particular has fallen pretty far behind."
STORM-3348,Incorrect message when group id is not provided as kafka spout config on storm ui,"Steps to produce the issue - 
 # Use kafka as source for a spout.
 # Don't provide group id in spout configuration.
 # Start the topology and go to storm UI topology page.
 # Instead of showing kafka spout lags it shows the following message - ""Offset lags for kafka not supported for older versions. Please update kafka spout to latest version."", even though kafka spout is having correct version."
STORM-3347,Storm-starter should not suggest using maven-exec-plugin,"Storm-starter pom contains a maven-exec-plugin section for running examples via that plugin. This doesn't make sense, since the examples don't reference LocalCluster anymore."
STORM-3346,ClassNotFoundException: clojure.lang.persistentList whiile submitting topology to local cluster in storm,"Getting below exceptions while submitting storm topology to local cluster 

Exception in thread ""main"" java.lang.ExceptionInInitializerError
 at clojure.lang.Namespace.<init>(Namespace.java:34)
 at clojure.lang.Namespace.findOrCreate(Namespace.java:176)
 at clojure.lang.Var.internPrivate(Var.java:156)
 at org.apache.storm.LocalCluster.<clinit>(Unknown Source)
 at KafkaCEPTopology.main(KafkaCEPTopology.java:53)
Caused by: Syntax error compiling . at (clojure/core.clj:20:8).
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7114)
 at clojure.lang.Compiler.analyze(Compiler.java:6789)
 at clojure.lang.Compiler.access$300(Compiler.java:38)
 at clojure.lang.Compiler$DefExpr$Parser.parse(Compiler.java:596)
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7106)
 at clojure.lang.Compiler.analyze(Compiler.java:6789)
 at clojure.lang.Compiler.analyze(Compiler.java:6745)
 at clojure.lang.Compiler.eval(Compiler.java:7180)
 at clojure.lang.Compiler.load(Compiler.java:7635)
 at clojure.lang.RT.loadResourceScript(RT.java:381)
 at clojure.lang.RT.loadResourceScript(RT.java:372)
 at clojure.lang.RT.load(RT.java:463)
 at clojure.lang.RT.load(RT.java:428)
 at clojure.lang.RT.doInit(RT.java:471)
 at clojure.lang.RT.<clinit>(RT.java:338)
 ... 5 more
Caused by: java.lang.ClassNotFoundException: clojure.lang.PersistentList
 at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
 at clojure.lang.DynamicClassLoader.findClass(DynamicClassLoader.java:69)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
 at clojure.lang.DynamicClassLoader.loadClass(DynamicClassLoader.java:77)
 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
 at java.lang.Class.forName0(Native Method)
 at java.lang.Class.forName(Class.java:348)
 at clojure.lang.RT.classForName(RT.java:2207)
 at clojure.lang.RT.classForNameNonLoading(RT.java:2220)
 at clojure.lang.Compiler$HostExpr.maybeClass(Compiler.java:1041)
 at clojure.lang.Compiler$HostExpr$Parser.parse(Compiler.java:982)
 at clojure.lang.Compiler.analyzeSeq(Compiler.java:7106)

 "
STORM-3344,blacklist scheduler causing nimbus restart,"{code:java}
2019-02-22 10:48:41.460 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.UnsupportedOperationException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$27(Nimbus.java:2872) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.1.y.jar:2.0.1.y]
Caused by: java.lang.UnsupportedOperationException
        at org.apache.storm.shade.com.google.common.collect.UnmodifiableIterator.remove(UnmodifiableIterator.java:43) ~[shaded-deps-2.0.1.y.jar:2.0.1.y]
        at java.util.AbstractCollection.remove(AbstractCollection.java:293) ~[?:1.8.0_102]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.removeLongTimeDisappearFromCache(BlacklistScheduler.java:216) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.scheduler.blacklist.BlacklistScheduler.schedule(BlacklistScheduler.java:110) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:2070) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lockingMkAssignments(Nimbus.java:2234) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2220) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2165) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$27(Nimbus.java:2868) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        ... 2 more
2019-02-22 10:48:41.461 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:520) ~[storm-client-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$9(Nimbus.java:564) ~[storm-server-2.0.1.y.jar:2.0.1.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:253) [storm-client-2.0.1.y.jar:2.0.1.y]
2019-02-22 10:48:41.462 o.a.s.u.Utils Thread-19 [INFO] Halting after 10 seconds
{code}"
STORM-3343,JCQueueTest can still be flaky,"Made a mistake in the fix for STORM-3310. The consumer in one of the tests check for interrupt in a place it shouldn't. 

{code}
[ERROR] Failures:
[ERROR]   JCQueueTest.lambda$testFirstMessageFirst$0:63 We expect to receive first published message first, but received null expected:<FIRST> but was:<null>

Exception in thread ""Thread-125"" java.lang.RuntimeException: java.lang.InterruptedException: ConsumerThd interrupted
	at org.apache.storm.utils.JCQueueTest$1.accept(JCQueueTest.java:48)
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133)
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110)
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101)
	at org.apache.storm.utils.JCQueueTest$ConsumerThd.run(JCQueueTest.java:207)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException: ConsumerThd interrupted
	... 6 more
{code}

The consumer accept method shouldn't check for interrupt, as that is handled by the ConsumerThd.run method. When the accept check for interrupt is hit, the consumer exits without draining the JCQueue, and the test may fail."
STORM-3342,Add plugin to generate list of dependency licenses to build,"I think it would be helpful if we could easily generate a list of the licenses used by our dependencies. When we do a release, we need to make sure we don't let dependencies with e.g. GPL license slip through, and I think it will be easier if we can list dependencies with their licenses."
STORM-3340,Nicer path handling in storm-webapp,Some of the path handling in storm-webapp is a little haphazard and differs between methods that do similar things. We should make the handling more uniform.
STORM-3339,Port all the AtomicReference to ConcurrentHashMap for Nimbus,"Now for many concurrent access resource in Nimbus.java, we use AtomicReference to make them multi thread safe. The resources summarized below:

1. heartbeatsCache
2. schedulingStartTimeNs
3. idToSchedStatus
4. nodeIdToResources
5. idToWorkerResources
6. idToExecutors

The 1, 4, 5 and 6 may grows huge if we have hundreds of topologies on cluster, when we update AtomicReference, actually we passed in a Function and use compareAndSet to update the whole val to the new returned by the Function, in that case, we must do a reference copy and merge the changes, which seems not necessary.

I think the reason to use AtomicReference is a legacy from old Clojure code, we can replace them totally with ConcurrentHashMap which supported better performance."
STORM-3337,KafkaTridentSpoutOpaque can lose offset data in Zookeeper,"I see this issue happening once a twice a week for a number of topologies I have running in production that are reading from Kafka. I was able to reproduce it in a more pared down environment using 1 worker with a parallelism hint of at least 2 reading from a topic that had 16 partitions. The issue is reproducible using less partitions but it occurs less frequently.

What happens is that while committing offsets for the first transaction after a worker crash the partition offset data in Zookeeper is wiped for a subset of that worker's partitions. The state is restored after the next batch or two is committed and the worker continues as normal, however if the worker happens to crash a 2nd time before the data restores itself then it gets lost and those partitions reset (in my case to their earliest offsets since I used a reset strategy of UNCOMMITTED_EARLIEST) after the worker restarts again.

I've attached a log file showing what's happening. In this example ZK had offset data committed for all partitions for txid=29 before the worker crashed. After the worker came back up partitions 1, 3, 5, 7, 9, 11, 13, 15 lose their child nodes in ZK, the remaining partitions get child nodes created for txid=30. The important steps are:

1. Thread-7 and Thread-19 both hit 'Emitted Batch' for txid=30 here:

[https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L146]
 Thread-7 is assigned even numbered partitions 0-14, has _index=0, _changedMeta=true, coordinatorMeta=[] 
 Thread-19 is assigned odd numbered partitions 1-15, has _index=1, _changedMeta=true, coordinatorMeta=[]
 Even though `coordinatorMeta` is empty the `KafkaTridentSpoutEmitter` ignores this parameter in `getPartitionsForTask` and returns partition assignments for each thread:
 [https://github.com/apache/storm/blob/v1.2.3/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutEmitter.java#L253]

2. Both threads hit 'Committing transaction' here: 
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L160]

3. Thread-19 begins create state for txid=30 for its partitions:
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L186]

4. Thread-7 enters this special condition since it has `_index==0` and `_changedMeta==true`
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L169]

5. Thread-7 calls ` _emitter.getOrderedPartitions(_savedCoordinatorMeta)`, which for the KakfaTridentSpoutEmitter implementation returns an empty list since coordinatorMeta was empty for this batch:
 [https://github.com/apache/storm/blob/v1.2.3/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutEmitter.java#L241]

6. Thread-7 does a list for all the partition nodes for this component in ZK and since `validIds` is empty they all pass the check and `removeState` is called on each of them for txid=30:
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L177]
 This is the key part that causes the bug. Thread-7 hasn't started committing state for txid=30 for any of its assigned partitions so the calls to `removeState` on those partitions won't do anything. However Thread-19 is running concurrently so any partitions it has already written state for will get deleted here.

7. Thread-7 and Thread-19 enter the success handler and call `cleanupBefore(txid=29)`:
 [https://github.com/apache/storm/blob/v1.2.3/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L153]
 For the partitions owned by Thread-19 that got their state removed in #6 that means that both the nodes for 29 and 30 are deleted and the partitions' sates are empty in ZK.

Couple things that are contributing to this bug. I'm not sure if it's expected that `coordinatorMeta` be passed into `emitBatch` as an empty list for the first batch after the worker restart. If this contained all the partitions assigned across all tasks (which it does on subsequent batches) then `validIds` wouldn't trigger for partitions that were owned by other tasks and their state wouldn't accidentally get removed. This is exacerbated by the fact that the `KafkaTridentSpoutEmitter` returns valid partitions for `getPartitionsForTask` even when `getOrderedPartitions` returns empty which seems to break expectations.

One additional safe-guard that might be worth investigation is having `cleanupBefore` make sure it wasn't going to leave a node in ZK without any children before running."
STORM-3335,timeout when scheduling topology runs too long,"We encountered an issue where a user submitted a topology and after a few minutes tried to kill it and failed.  After debugging we found that scheduling was running for topology for at least 30 minutes, causing the kill to be backlogged on the timer task. "
STORM-3334,Storm java processes using very high CPU with very low load,"We observer that Storm_Nimbus, storm_supervisor and NMStormTopology is consuming 100% CPU with all CPU cores. 

We found that Dispatcher Thread is taking quiet lot CPU. We observer that this issue was also reported earlier in Jira with title ""{color:#333333}Short disruptor queue wait time leads to high CPU usage when idle{color}"" 

https://issues.apache.org/jira/browse/STORM-503

This say the issue got fixed in the version 0.9.6 while in the version 1.0.0 seems the issue is still fixed.

Kindly help us to resolve this issue, or advice us with the stable version of Storm which has this issue resolved.

 

Regards,

Adarsh"
STORM-3333,storm-kafka-monitor - NoClassDefFoundError: org/apache/kafka/shaded/clients/consumer/KafkaConsumer," 

ui log contains errors trying to run storm-kafka-monitor.  I see a similar callstack when running from the command line:

 
{code:java}
org.apache.storm.utils.ShellUtils$ExitCodeException: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/kafka/shaded/clients/consumer/KafkaConsumer
	at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.getOffsetLags(KafkaOffsetLagUtil.java:145)
	at org.apache.storm.kafka.monitor.KafkaOffsetLagUtil.main(KafkaOffsetLagUtil.java:71)
Caused by: java.lang.ClassNotFoundException: org.apache.kafka.shaded.clients.consumer.KafkaConsumer
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 2 more

	at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:271) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:194) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellUtils$ShellCommandExecutor.execute(ShellUtils.java:428) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellCommandRunnerImpl.execCommand(ShellCommandRunnerImpl.java:33) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.ShellCommandRunnerImpl.execCommand(ShellCommandRunnerImpl.java:26) ~[storm-client-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.getLagResultForKafka(TopologySpoutLag.java:162) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.getLagResultForNewKafkaSpout(TopologySpoutLag.java:193) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.addLagResultForKafkaSpout(TopologySpoutLag.java:127) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.utils.TopologySpoutLag.lag(TopologySpoutLag.java:61) ~[storm-core-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.daemon.ui.UIHelpers.getTopologyLag(UIHelpers.java:1620) ~[storm-webapp-2.0.1.y.jar:2.0.1.y]
	at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyLag(StormApiResource.java:359) ~[storm-webapp-2.0.1.y.jar:2.0.1.y]
	at sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source) ~[?:?]
{code}"
STORM-3330,"Migrate parts of storm-webapp, and reduce use of mocks for files","Parts of storm-webapp are hard to modify due to overuse of mocks, e.g. for files. We should swap out the mock code with real temporary files."
STORM-3329,allow HttpForwardingMetricsConsumer to be used generally by topologies,
STORM-3328,Allow overriding function name in BasicDRPCTopology,
STORM-3327,Storm-webapp uses default charset for URL encoding in a bunch of places,"I think we should use UTF-8, the URLEncoder Javadoc recommends it, linking to https://www.w3.org/TR/html40/appendix/notes.html#non-ascii-chars."
STORM-3326,"FakeMetricsConsumer exposes internal lists, causing ConcurrentModificationException","{quote}
classname: org.apache.storm.metrics-test / testname: test-builtin-metrics-2
Uncaught exception, not in assertion.
expected: nil
  actual: java.util.ConcurrentModificationException: null
 at java.util.ArrayList$Itr.checkForComodification (ArrayList.java:907)
    java.util.ArrayList$Itr.next (ArrayList.java:857)
    com.google.common.collect.AbstractMapBasedMultimap$WrappedCollection$WrappedIterator.next (AbstractMapBasedMultimap.java:466)
    clojure.lang.PersistentVector.create (PersistentVector.java:105)
    clojure.lang.LazilyPersistentVector.create (LazilyPersistentVector.java:32)
    clojure.core$vec.invoke (core.clj:361)
    org.apache.storm.util$clojurify_structure$fn__219.invoke (util.clj:85)
    clojure.walk$prewalk.invoke (walk.clj:64)
    clojure.core$partial$fn__4527.invoke (core.clj:2493)
    clojure.core$map$fn__4553.invoke (core.clj:2622)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
    clojure.core.protocols/fn (protocols.clj:101)
    clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
    clojure.core$reduce.invoke (core.clj:6519)
    clojure.core$into.invoke (core.clj:6600)
    clojure.walk$walk.invoke (walk.clj:49)
    clojure.walk$prewalk.invoke (walk.clj:64)
    clojure.core$partial$fn__4527.invoke (core.clj:2493)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
    clojure.core.protocols/fn (protocols.clj:101)
    clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
    clojure.core$reduce.invoke (core.clj:6519)
    clojure.core$into.invoke (core.clj:6600)
    clojure.walk$walk.invoke (walk.clj:49)
    clojure.walk$prewalk.invoke (walk.clj:64)
    org.apache.storm.util$clojurify_structure.invoke (util.clj:83)
    org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_$reify__2950.exec (metrics_test.clj:79)
    org.apache.storm.Testing.whileTimeout (Testing.java:103)
    org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_.invoke (metrics_test.clj:77)
    org.apache.storm.metrics_test$assert_metric_running_sum_BANG_.invoke (metrics_test.clj:98)
    org.apache.storm.metrics_test/fn (metrics_test.clj:333)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval4721$iter__4722__4726$fn__4727$fn__4728$fn__4729.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval4721$iter__4722__4726$fn__4727$fn__4728.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval4721$iter__4722__4726$fn__4727.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval4721.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
{quote}"
STORM-3325,Storm-webapp should not be part of Externals on travis,"Storm-webapp is built as part of ""Externals"" in Travis. I think it should be part of Server instead."
STORM-3324,connection attempt 3 to Netty-client-compute06/192.168.28.110:6708 failed,connection attempt 3 to Netty-client-compute06/192.168.28.110:6708 failed
STORM-3323,Make storm.py work without wrapper scripts,"I think it would be good if we could get rid of the platform specific shell scripts wrapping storm.py.

I think we should be able to move all the functionality in the shell scripts into storm.py. The only change we'd likely have to make is to have a storm-env.py file people can modify in storm/conf, instead of having the storm-env.sh/storm-env.ps1 as we do now."
STORM-3321,Tests are flaky due to long timeouts in Nimbus and supervisor when using LocalCluster,"Tests will sometimes fail with timeout when using e.g. Testing.completeTopology.

The issue is that the timeout is 10 seconds, and Nimbus and the supervisor both have timers that monitor for new deployments that are also set to 10 seconds. This causes tests to time out because a lot of the test time is wasted waiting for Nimbus/the supervisors to catch that the test topology is deployed.

We should reduce these timeouts to their minimums.

There is also a race in Nimbus that can cause test failures 
{quote}
2019-01-21 02:00:19.587 [main] WARN  org.apache.storm.daemon.nimbus.Nimbus - Topology submission exception. (topology name='topologytest-45f5ad59-ec16-45a4-ba4a-eea992411cc1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='DESKTOP-AGC8TKM', port=6627, isLeader=true}
	at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1525) ~[classes/:?]
	at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2982) ~[classes/:?]
	at org.apache.storm.daemon.nimbus.Nimbus.submitTopology(Nimbus.java:2965) ~[classes/:?]
	at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:444) ~[classes/:?]
	at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:125) ~[classes/:?]
	at org.apache.storm.Testing.completeTopology(Testing.java:424) ~[classes/:?]
{quote}

The issue is that Nimbus has to acquire leadership in order to submit topologies, but LocalCluster doesn't wait for the Nimbus instance it creates to gain leadership.

We should make LocalCluster wait for Nimbus to gain leadership."
STORM-3320,Executors should start when all worker connections are ready,"We conflate ""being activated"" with ""all workers are ready"" in WorkerState, by making isWorkerActivated a part of isTopologyActivated.

The issue with this is that isTopologyActivated is used to communicate activation/deactivation to the executors, and is updated on a timer (default only every 10 seconds). isWorkerActivated is really meant to be a one-way switch, which lets us delay executor initialization until all other workers in the topology are also started.

Since we mix the two up, if a worker is started in the topology and all other connections aren't ready immediately (e.g. as happens every time you deploy a topology, some workers will boot faster than others), the worker may have to wait up to 10 seconds to start.

We should make sure the wait for isWorkerActivated happens via CountDownLatch instead, so the executor will start as soon as the connections are ready."
STORM-3319,Slot can fail assertions in some cases,"{quote}
2019-01-19 22:47:03.045 [SLOT_1024] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.lang.AssertionError: null
	at org.apache.storm.daemon.supervisor.Slot.handleEmpty(Slot.java:781) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:217) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [classes/:?]
2019-01-19 22:47:03.045 [SLOT_1025] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.lang.AssertionError: null
	at org.apache.storm.daemon.supervisor.Slot.handleEmpty(Slot.java:781) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:217) ~[classes/:?]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:900) [classes/:?]
{quote}

The issue is that Slot tries to go from WAITING_FOR_LOCALIZATION to EMPTY when there's an exception downloading a blob. It then fails one of the assertions in EMPTY because it doesn't clear its pendingChangingBlobsAssignment field.

There's no reason to go back to EMPTY. The Slot still wants to download some blobs, so it should just restart the downloads and go back to WAITING_FOR_LOCALIZATION."
STORM-3318,Complete information in Class NewKafkaSpoutOffsetQuery,"Just complete information in three methods(toString , equals,  hashCode)"
STORM-3317,upload credentials fails when using different java.security.auth.login.config file,Our launcher box has a differing version of java.security.auth.login.config from the system property.  Having this property set differently causes upload-credentials to fail with the current code.
STORM-3315,Upgrade to Kryo 4,"A user seems to be hitting https://github.com/EsotericSoftware/kryo/issues/462, which is fixed in Kryo 4.

We should upgrade. It seems like we can ensure compatibility with current Kryo by setting kryo.getFieldSerializerConfig().setOptimizedGenerics(true), going by the Kryo 4 release notes."
STORM-3312,Upgrade Guava to latest,"As part of STORM-3311, I want to use https://google.github.io/guava/releases/23.0/api/docs/com/google/common/io/MoreFiles.html to replace Guava's Files class. We're currently on Guava 16.0.1, which is too old. Since we're shading Guava, there shouldn't be an issue with upgrading it. Modules like storm-cassandra that require old Guava versions can depend directly on unshaded Guava in the version they like."
STORM-3311,Use Java 7 Files API for IO instead of the older API,"We should try to use the NIO Files API for file IO. The older file API causes issues on Windows, since it doesn't set the FILE_SHARE_DELETE flag when opening files. This causes Storm to be unable to delete files that have open handles, which is unlike the behavior on Linux. This can cause e.g. unnecessary supervisor crashes because one thread tries to delete a file that is open in another.

For the same reason, we should get rid of uses of common-io FileUtils, and get rid of uses of Guava's Files class that opens IO streams."
STORM-3310,JCQueueTest is flaky,"The JCQueueTest is flaky

{quote}
[ERROR]   JCQueueTest.testFirstMessageFirst:61 We expect to receive first published message first, but received null expected:<FIRST> but was:<null>
{quote}

The issue is that the test has a race condition. There is no check that the consumer thread has read all (or any) of the produced messages before the test terminates."
STORM-3309,TickTupleTest is still flaky,"{quote}
 testTickTupleWorksWithSystemBolt  Time elapsed: 6.802 s  <<< FAILURE!
java.lang.AssertionError: Iteration 1 expected:<52000> but was:<51000>
{quote}

The test runs a topology in a local cluster with time simulation. One of the bolts has tick tuples enabled, and the test tries to check that the ticks arrive with 1 second intervals.

As far as I can tell, the problem is that time simulation doesn't cover the bolt and spout executors. When the test increases simulated time by 1 second and waits for the cluster to idle, the test expects that to mean that the bolt will at that point have consumed the tick. In some cases this doesn't happen, and multiple tick tuples may end up queued before the bolt consumes them. Since the bolt is responsible for generating the timestamp, the test will fail."
STORM-3308,o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key,"Related to issue STORM-2736

We run a single instance of storm-nimbus (Non HA). Due to issues with connections issues with zookeeper, the ephemeral child nodes at  /storm.104/blobstore/key/  were lost that was created by the instance. Nimbus constantly logs the following line which was fixed in the issue mentioned above - 
{code:java}
o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<key>
{code}

But, Shouldn't nimbus automatically create the children with hostname:port-sequencenumber automatically if it is up and running ? In my case, nimbus did not crash but the ephemeral nodes( the children in the case) vanished  when the connection was reset between nimbus and zookeeper. I don't see any code path that creates the children if they are missing in the /blobstore zk path."
STORM-3307,0 timestamp in component errors in Storm UI,
STORM-3306,Some tests in storm-core/test/jvm/org/apache/storm/integration/TopologyIntegrationTest.java are using Thrift to build topologies. They should use TopologyBuilder instead. ,
STORM-3304,Storm-hdfs tests don't run on Java 11,
STORM-3301,The KafkaSpout can in some cases still replay tuples that were already committed,"In the fix for STORM-2666 and followups, we added logic to handle cases where the spout received the ack for an offset after the following offsets were already acked. The issue was that the spout might commit all the acked offsets, but not adjust the consumer position forward, or clear out waitingToEmit properly. If the acked offset was sufficiently far behind the log end offset, the spout might end up polling for offsets it had already committed.

The fix is slightly wrong. When the consumer position drops behind the committed offset, we make sure to adjust the position forward, and clear out any waitingToEmit messages that are behind the committed offset. We don't clear out waitingToEmit unless we adjust the consumer position, which turns out to be a problem.

For example, say offset 1 has failed, offsets 2-10 have been acked and maxPollRecords is 10. Say there are 11 records (1-11) in Kafka. If the spout seeks back to offset 1 to replay it, it will get offsets 1-10 back from the consumer in the poll. The consumer position is now 11. The spout emits offset 1. Say it gets acked immediately. On the next poll, the spout will commit offset 1-10 and check if it should adjust the consumer position and waitingToEmit. Since the position (11) is ahead of the committed offset (10), it doesn't clear out waitingToEmit. Since waitingToEmit still contains offsets 2-10 from the previous poll, the spout will end up emitting these tuples again."
STORM-3300,Potential NPE in Acker when using reset timeout,
STORM-3297,NimbusMetricProcessor.processWorkerMetrics() can cause supervisor restart,NimbusClient.getConfiguredClient() can throw a (runtime) NimbusLeaderNotFoundException.  This can cause a supervisor restart when there is no nimbus leader.
STORM-3296,Upgrade Curator-test to resolve CURATOR-409,"I'd like to preemptively upgrade curator-test, so we don't get affected by CURATOR-409 once we upgrade Zookeeper to the latest 3.4 version."
STORM-3295,Blacklist scheduling doesn't handle multiple supervisors on a host properly,"If two supervisors on a host are blacklisted, the RasBlacklistStrategy code can release one supervisor and keep the other blacklisted.  The BlacklistScheduler then sees the one blacklisted supervisor and considers that host still blacklisted, preventing scheduling from operating properly.

 

 

 "
STORM-3291,Worker can't run as the user who submitted the topology,"Without principal, worker can't be launched as the user who submitted the topology even we set ""supervisor.run.worker.as.user"" to true.Because the submitterUser will be overwrited by the user who launched nimbus."
STORM-3290,Split storm-kafka-client KafkaSpoutConfig into a config for the Storm spout and a config for the Trident spouts,"The KafkaSpoutConfig class is being used for configuration of both the Trident and non-Trident spouts. I think we should split it up, because about half the properties are only used by the non-Trident spout. It is confusing for users to have a lot of settings that don't do anything."
STORM-3289,Add note about KAFKA-7044 to storm-kafka-client compatibility docs,"We should add a note about KAFKA-7044 to the storm-kafka-client docs, since it can cause crashes in the spout. "
STORM-3288,Extracting jar dirs with resources in them are corrupted,"The current code when it tries to remove resources/ from the beginning of the path ends up removing it everywhere it exists in the path, which messes up anything with a directory named
{code:java}
*resources/{code}"
STORM-3286,MENIFEST.MF may loss by use of storm-rename-hack,"Function shadeJarStream in class DefaultShader may produce lost  of MENIFEST.MF,because of JarOutputStream created by the contructor without manifest.For some situation, such as mongodb connection, may lead a NullPointerException."
STORM-3283,Can not connect to kerberized hdfs with HdfsSpout,"In HdfsSpout hdfs FileSystem object is [first created|https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L407] and then [additional settings are read|https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L418].

As a result config parameters like:
{code:java}
conf.set(""hadoop.security.authentication"", ""kerberos"");
{code}
can not be set and HdfsSpout fails with the following exception:
{code:java}
org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS]
 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_171]
 at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_171]
 at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_171]
 at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_171]
 at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1656) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1583) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1580) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1595) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1734) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.storm.hdfs.spout.HdfsSpout.validateOrMakeDir(HdfsSpout.java:518) [stormjar.jar:?]
 at org.apache.storm.hdfs.spout.HdfsSpout.open(HdfsSpout.java:435) [stormjar.jar:?]
 at org.apache.storm.daemon.executor$fn__9593$fn__9608.invoke(executor.clj:615) [storm-core-1.2.1.3.3.0.0-157.jar:1.2.1.3.3.0.0-157]
 at org.apache.storm.util$async_loop$fn__555.invoke(util.clj:482) [storm-core-1.2.1.3.3.0.0-157.jar:1.2.1.3.3.0.0-157]
 at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_171]
Caused by: org.apache.hadoop.ipc.RemoteException: SIMPLE authentication is not enabled. Available:[TOKEN, KERBEROS]
 at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1497) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.Client.call(Client.java:1443) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.Client.call(Client.java:1353) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at com.sun.proxy.$Proxy47.getFileInfo(Unknown Source) ~[?:?]
 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_171]
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_171]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_171]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_171]
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.1.1.3.0.1.0-187.jar:?]
 at com.sun.proxy.$Proxy48.getFileInfo(Unknown Source) ~[?:?]
 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654) ~[hadoop-hdfs-client-3.1.1.3.0.1.0-187.jar:?]
{code}"
STORM-3281,ZK ACL checks breaks nimbus HA with LocalFsBlobStore,"A new check [1] is introduced to check the ZK ACLs before nimbus starts up. However this can fail with Nimbus HA enabled with LocalFsBlobStore causing nimbus to not start up.

1. Set up a cluster with 3 Nimbus (blobstore replication factor = 2). It might be reproducible with two nimbus but I havent checked.
2. Deploy topology
3. Once the topology is activated and running, Kill the leader nimbus.
4. The nimbus becomes offline and one of the other two is elected as a leader
5. Now try to bring up the killed nimbus.

It will fail with following exception


{noformat}
2018-11-06 10:22:05.195 o.a.s.t.t.TSaslTransport main [DEBUG] CLIENT: reading data length: 37
2018-11-06 10:22:05.204 o.a.s.b.BlobStoreUtils main [DEBUG] Updating state inside zookeeper for an update
2018-11-06 10:22:05.209 o.a.s.u.StormBoundedExponentialBackoffRetry main [DEBUG] The baseSleepTimeMs [2000] the maxSleepTimeMs [60000] the maxRetries [5]
2018-11-06 10:22:05.224 o.a.s.m.n.Login main [INFO] successfully logged in.
2018-11-06 10:22:05.224 o.a.s.s.a.k.KerberosSaslTransportPlugin main [DEBUG] SASL GSSAPI client transport is being established
2018-11-06 10:22:05.238 o.a.s.s.a.k.KerberosSaslTransportPlugin main [DEBUG] do as:storm_componentsrandYPTkmQBYmmTRvE@EXAMPLE.COM
2018-11-06 10:22:05.240 o.a.s.t.t.TSaslTransport main [DEBUG] opening transport org.apache.storm.thrift.transport.TSaslClientTransport@75dc1c1c
2018-11-06 10:22:05.241 o.a.s.s.a.k.KerberosSaslTransportPlugin main [ERROR] Client failed to open SaslClientTransport to interact with a server during session initiation: org.apache.storm.thrift.transport.TT
ransportException: java.net.ConnectException: Connection refused (Connection refused)
org.apache.storm.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)
        at org.apache.storm.thrift.transport.TSocket.open(TSocket.java:226) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.thrift.transport.TSaslTransport.open(TSaslTransport.java:266) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_151]
        at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_151]
        at org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:53) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:104) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.security.auth.ThriftClient.<init>(ThriftClient.java:73) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.utils.NimbusClient.<init>(NimbusClient.java:131) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStoreUtils.createStateInZookeeper(BlobStoreUtils.java:233) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:269) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.LocalFsBlobStore.checkForBlobUpdate(LocalFsBlobStore.java:344) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:274) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:271) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:300) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.zookeeper.AclEnforcement.verifyAcls(AclEnforcement.java:111) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2588) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2612) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
        at clojure.lang.AFn.applyToHelper(AFn.java:152) ~[clojure-1.7.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus.main(Unknown Source) ~[storm-core-1.2.1.3.3.0.0-154.jar:1.2.1.3.3.0.0-154]
Caused by: java.net.ConnectException: Connection refused (Connection refused)
        at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_151]
        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_151]
        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_151]
        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_151]
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_151]
        at java.net.Socket.connect(Socket.java:589) ~[?:1.8.0_151]
{noformat}


The ACL check tries to download the topology conf from blobstore [2] and in that process tries to update the nimbus state in ZK via itself [3] causing the connection failure.

To fix it I think we should wait until nimbus comes up before doing ACL checks.

cc [~revans2] to suggest the right fix since you may have more context.

[1] https://github.com/apache/storm/blob/1.x-branch/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L2588
[2] https://github.com/apache/storm/blob/1.x-branch/storm-core/src/jvm/org/apache/storm/zookeeper/AclEnforcement.java#L111
[3] https://github.com/apache/storm/blob/1.x-branch/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L269"
STORM-3280,Trident-based windowing does not appear to guarantee at-least-once,"[~shaikasifullah] mentioned that he was experiencing lost tuples when restarting a Trident topology that uses windowing alongside the opaque Kafka spout.

I think this is due to a bug in the Trident windowing implementation.

Trident doesn't use the regular acking mechanism to keep track of all tuples in a batch. Instead, the bolt executors in Trident send ""coordinator"" tuples downstream following each batch, indicating how many tuples were in the batch. These coordinator tuples are anchored to the initial ""emit batch"" tuple at the master batch coordinator (MBC). The next bolt executor in line checks if it received all the expected tuples, and fails the ""emit batch"" tree if not. Otherwise, the entire batch is considered acked when the coordinator tuple is acked, which happens as soon as it is received (purposefully ignoring the commit mechanism here).

The bolt executor notifies the wrapped bolt when a batch starts, and when it finishes. The expectation is that the bolt will emit any new tuples it wants anchored to the coordinator tuple before the bolt executor considers the batch finished. See https://github.com/apache/storm/blob/19fbfb9ac8f82719cf70fedf6a024acaeec4e804/storm-client/src/jvm/org/apache/storm/trident/topology/TridentBoltExecutor.java#L127.

The windowing mechanism in Trident is implemented via a processor https://github.com/apache/storm/blob/19fbfb9ac8f82719cf70fedf6a024acaeec4e804/storm-client/src/jvm/org/apache/storm/trident/windowing/WindowTridentProcessor.java#L147. The processor collects received tuples grouped by batch, and only passes them to the WindowManager when a batch is considered complete. At this point, it will also check if any triggers have fired (e.g. due to timeout), and will emit any resulting windows.

The issue here is that there is no correlation between the finished batch and which tuples the window processor chooses to emit during the finishBatch call. Unless it emits exactly the tuples from the received batch, there is a risk of losing the at-least-once property, since the bolt executor will ack the coordinator tuple immediately following finishBatch.

Just to give a concrete example:

MBC starts txid 1 by emitting an ""emit batch"" tuple
Spout executor receives the tuple, emits tuple 1-10, then emits coordinator tuple containing expected count of 10 tuples.
Bolt executor receives tuple 1-10
Bolt executor receives coordinator tuple from upstream spout, containing an expected count of 10 tuples
Bolt executor calls finishBatch
Window processor is configured with a window of 10 seconds, and decides not to emit the 10 tuples. Since nothing is emitted, no new tuples are anchored at the coordinator tuple.
Bolt executor acks the coordinator tuple at the MBC
The MBC sees that the ""emit batch"" tuple has been acked, and starts the commit process. At this point Trident is free to assume the 10 tuples have been correctly processed and e.g. write to Zookeeper that the Kafka spout should pick up at offset 10 next time it starts."
STORM-3279,Kafka trident spout could loose its position with EARLIEST or LATEST FirstPollOffsetStrategy,"In KafkaTridentSpoutEmitter emitPartitionBatch() function, when kafkaConsumer.poll(pollTimeoutMs) returns 0 records for the very first transaction where FirstPollOffsetStrategy is set to EARLIEST or LATEST, the spout fails to move to EARLIEST or LATEST, and continues from the last metadata position.

 

The flow of events which would cause this bug :

 

1. FirstPollOffsetStrategy set to EARLIEST or LATEST

2. For first transaction after restart txid1 Based on [link L164|https://github.com/apache/storm/blob/master/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/trident/KafkaTridentSpoutEmitter.java#L164] ,

The currentBatch is initialized to lastBatchMeta (which need not be null);

3. Later in L171, the consumer seeks to ""start"" OR ""end""

4. Then consumer.poll(pollTimeoutMs) is called.

5. If poll returns non 0 records , currentBatch is set to a new metadata . *If poll returns 0 records,*

*currentBatch is not reset ie, currentBatch is still lastBatchMeta (which need not be null)*

 

So now in transaction txid2 after txid1, isFirstPoll() returns false, and the spout continues from lastBatchMeta.

 

 "
STORM-3277,Flux ues in storm-kafka-client not work?,"I use yaml like this :
{code:java}
//代码占位符
# Topology定义
name: ""testTopology""
config:
  topology.workers: 1
components:
  - id: ""spoutConfigBuilder""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig$Builder""
    #contructorArgs 是一个列表，其元素是对象
    constructorArgs:
      # bootstrapServers
      - ""10.7.3.45:9092""
      # topics
      - [""test-topic""]
    #查找class中setter函数进行set此propertie name对应值
    properties:
      - name: ""firstPollOffsetStrategy""
        value: EARLIEST
      - name: ""offsetCommitPeriodMs""
        value: 200
    #配置方法是属性和构造函数的参数
    configMethods:
    - name: ""setProp""
      args:
        - {
          ""key.deserializer"": ""org.apache.kafka.common.serialization.StringDeserializer"",
          ""value.deserializer"": ""org.apache.kafka.common.serialization.StringDeserializer"",
          ""max.partition.fetch.bytes"": 200,
          ""group.id"": ""kafkaSpoutTestGroup_test"",
          }
  - id: ""spoutConfig""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig""
    constructorArgs:
      - ref: ""spoutConfigBuilder""
# spout定义
spouts:
  - id: ""spout-1""
    className: ""org.apache.storm.kafka.spout.KafkaSpout""
    parallelism: 1
    constructorArgs:
      - ref: ""spoutConfig""

# bolt定义
bolts:
  - id: ""bolt-1""
    className: ""com.crfchina.stream.test.OneBolt""
    parallelism: 1
  - id: ""bolt-2""
    className: ""com.crfchina.stream.test.TwoBolt""
    parallelism: 1

# stream定义
streams:
  - name: ""spout-1 --> bolt-1"" #name暂时未用上（可以在logging,UI等中作为placeholder）
    from: ""spout-1""
    to: ""bolt-1""
    grouping:
      type: SHUFFLE
      args: [""text""]

  - name: ""bolt-1 --> bolt2""
    from: ""bolt-1""
    to: ""bolt-2""
    grouping:
      type: SHUFFLE
{code}
the error is 
{code:java}
737 [main] WARN o.a.s.f.FluxBuilder - Found multiple invokable constructors for class class org.apache.storm.kafka.spout.KafkaSpoutConfig$Builder, given arguments [10.7.3.45:9092, [test-topic]]. Using the last one found.
Exception in thread ""main"" java.lang.IllegalArgumentException: argument type mismatch
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.storm.flux.FluxBuilder.applyProperties(FluxBuilder.java:282)
at org.apache.storm.flux.FluxBuilder.buildObject(FluxBuilder.java:383)
at org.apache.storm.flux.FluxBuilder.buildComponents(FluxBuilder.java:421)
at org.apache.storm.flux.FluxBuilder.buildTopology(FluxBuilder.java:101)
at org.apache.storm.flux.Flux.runCli(Flux.java:158)
at org.apache.storm.flux.Flux.main(Flux.java:103){code}
 "
STORM-3276,Can't run Flux with Storm 2.0.0,"I try to run a Flux-based topology with Storm 2.0.0

 

_apache-storm-2.0.0/bin/storm local target/2-1.0-SNAPSHOT.jar org.apache.storm.flux.Flux crawler.flux --local-ttl 9999999_

 

Am getting 

 

_17:41:22.191 [main] ERROR o.a.s.f.Flux - To run in local mode run with 'storm local' instead of 'storm jar'_
_17:41:22.191 [main] INFO o.a.s.LocalCluster -_

_RUNNING LOCAL CLUSTER for 9999999 seconds._

and nothing happens after that. 

 

The documentation for Flux [http://storm.apache.org/releases/2.0.0-SNAPSHOT/flux.html] still mentions using 'storm jar' as well as --local and --sleep.

My test topology can be found at [https://github.com/DigitalPebble/storm2] and requires the branch 2.x of StormCrawler [https://github.com/DigitalPebble/storm-crawler/tree/2.x] to by installed.

 

 

 

 "
STORM-3274,Ensure Python 3 and 2.7 compatibility for all scripts (disallow 2.6),* Also test with Travis
STORM-3273,Don't pass storm.local.hostname to topology conf,"We have found that if we set storm.local.hostname on nimbus it gets put into the topology conf which in turn causes the topology to report all metrics as coming from nimbus, which is not what we want."
STORM-3270,"Build Storm with JDK 11, excluding incompatible modules",
STORM-3269,storm-client and storm-server indirectly depend on storm-core,"When trying to get the version information for nimbus it looks for storm-core, which requires the storm-core class to be on the classpath.  We need to fix this, because VersionInfo is in storm-client so it is possible for someone who uses it from storm-client to load the wrong thing."
STORM-3268,Try to make the integration test more stable,"The integration test is still flaky, and most of the time it's not because of bugs in Storm. Try to make it more stable."
STORM-3256,"If all thread counts exceed 32767, the system will generate errors","If all thread counts exceed 32767, the system will generate errors, MessageBatch.java Line141.

 

Storm systems are used for low latency and a linear increase in the number of concurrencies as servers increase.
If you set up four workers per server, it's easy to achieve 4000 degrees of parallelism, that is, 4000 threads per server, then 10 servers will exceed 32767.
Supporting only 32767 threads is a disaster for large-scale computing.
It is hoped that this function can be improved and repaired as soon as possible."
STORM-3252,Blobstore sync bug fix,
STORM-3249,Nimbus Shutdown Faster,Nimbus takes for ever to shut down in 2.x.  It would really be nice to fix that. (and it is really our own fault why it takes so long)
STORM-3247,remove BLOBSTORE_SUPERUSER,BLOBSTORE_SUPERUSER doesn't appear to be used. 
STORM-3245,Log viewer cleanup failes if multiple empty worker-artifact dirs,"If a drive fills up for various reasons it is possible that we can create the directory for a worker in worker-artifacts, but not able to create the worker.yaml.  If we get more than one of these on a node we get an exception like.

 
{code:java}
o.a.s.d.l.u.LogCleaner logviewer-cleanup [ERROR] Exception while cleaning up old log.
java.lang.IllegalStateException: Duplicate key ...
        at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133) ~[?:1.8.0_102]
        at java.util.HashMap.merge(HashMap.java:1253) ~[?:1.8.0_102]
        at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320) ~[?:1.8.0_102]
        at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ~[?:1.8.0_102]
        at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ~[?:1.8.0_102]
        at java.util.TreeMap$KeySpliterator.forEachRemaining(TreeMap.java:2746) ~[?:1.8.0_102]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ~[?:1.8.0_102]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ~[?:1.8.0_102]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ~[?:1.8.0_102]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:1.8.0_102]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ~[?:1.8.0_102]
        at org.apache.storm.daemon.logviewer.utils.WorkerLogs.identifyWorkerLogDirs(WorkerLogs.java:214) ~[storm-webapp-...{code}

After that we cannot clean up any directories any longer..."
STORM-3240,healthchecks fail if scripts return 0 exit code,
STORM-3239,Worker profile actions don't work,"I just noticed that all worker profile actions don't work in 2.0.0-RC1. access-ui logged the message that the action happened, but there's no further log message as well as no action.

Assigning to [~govindmenon] since he said he is working on this in mailing list."
STORM-3238,The result of log search for topology doesn't show in UI page,Please refer the attachments to see the symptom. The result seems to be returned from Logviewer but UI doesn't reflect the output.
STORM-3237,create metric to track mkAssignments exceptions,We had an issue in the past where scheduling was throwing exceptions on nimbus.  I'd like to be able to monitor this issue going forward.
STORM-3236,DRPC meterShutdownCalls marked after metrics stopped,"If we want to track shutdown calls, we need to mark before the metricsRegistry.stopMetricsReporters() call occurs."
STORM-3234,Document Cluster Metrics,We need better docs on what cluster metrics we have in storm.
STORM-3233,Upgrade zookeeper client to newest version (3.4.13),"Hi,

I would like to see new zookeeper client (3.4.13) used in storm.

New release contains an important fix for cloud environments where zookeeper servers have dynamic ips ([https://jira.apache.org/jira/browse/ZOOKEEPER-2184]).

If possible, it would be nice to see updated zookeeper also on older storm versions (1.2.x, 1.1.x)"
STORM-3232,Display other versions of storm offered on the cluster in the UI,In Storm 2.0.0 we have the option to have older versions of storm installed on the cluster for backwards compatibility.  If this happens we should display it on the UI do others know everything that is installed on the cluster.
STORM-3231,TopologyBySubmissionTimeComparator does not consider priority,"TopologyBySubmissionTimeComparator indicates ""Comparator that sorts topologies by priority and then by submission time"", but the code only considers uptime.

 

I am not sure what the intent should be.  Either the code or comment should be fixed."
STORM-3230,Small race with worker tokens.,There is a small race in zookeeper I missed when trying to get the secret out of ZK for worker tokens.
STORM-3229,Better error reporting in WorkerTokenAuthorizer,"The error handling code in WorkerTokenAuthorizer logs some errors at a debug level, that should be logged at a much higher level.  Specifically ZK auth errors, where we can have the Zk client not authenticated because it is using an older jaas conf.  When this happens DRPC auth just does not work and there is no indication as to why."
STORM-3228,Supervisor blobstore ref counting not working properly,"When supervisors release slots, they can continue to indefinitely download the blob.
{code:java}
2018-09-17 16:18:00.955 o.a.s.l.AsyncLocalizer SLOT_6703 [INFO] Releasing slot for logviewer-ui-groups-test-11-1537201071 6703
2018-09-17 16:18:00.956 o.a.s.l.LocallyCachedBlob SLOT_6703 [WARN] {logviewer-ui-groups-test-11-1537201071 on 6703} had no reservation for logviewer-ui-groups-test-11-1537201071 stormjar.jar

2018-09-17 16:22:50.198 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_CONF logviewer-ui-groups-test-11-1537201071 will try again in 100 ms{code}"
STORM-3226,Improve Supervisor authorization handler error message,The error in the supervisor about the authorization handler is confusing and does not explain how to fix the issue.
STORM-3225,AuthorizedUserFilter should not convert the media type to a string,The AuthorizedUserFilter is converting the media type to a string so it can do a comparison check.  There is a better way.
STORM-3223,RAS can get an NPE if entire rack is blacklisted,If an entire rack is blacklisted the RAS scheduler can end up getting an NPE.  This is a more extreme version of a fix that was merged in previously for single nodes that are blacklisted causing NPEs.
STORM-3222,Fix KafkaSpout internals to use LinkedList instead of ArrayList,"KafkaSpout internally maintains a waitingToEmit list per topic partition and keeps removing the first item to emit during each nextTuple. The implementation uses an ArrayList which results in un-necessary traversal and copy for each tuple.

Also I am not sure why the nextTuple only emits a single tuple wheres ideally it should emit whatever it can emit in a single nextTuple call which is more efficient.  However the logic appears too complicated to refactor."
STORM-3221,Utilization in clusterSummary is inverted - shows free instead of used,
STORM-3219,Storm UI javascript needs better error reporting,"In many places on the UI we report errors by placing them in a section at the bottom of the page, but not all of the sections of the pages do this.  Which can make it hard to debug what is happening, and not very obvious to our end users that there is a problem."
STORM-3218,AuthorizedUserFilter should handle authorization exceptions better.,"Sorry I missed this before when I added back in impersonation.  The code that gets the topology conf to validate if the user is allowed to make the given REST call should not be doing impersonation because.  I tested the code as a single user, but the issue is that because the ReqContext is tied to a thread if we don't clear/clean up the impersonation code properly the old user is still in the ReqContext so when we try to get the conf we are doing it as the wrong user and get an error."
STORM-3217,Component errors missing in /api/v1/component API call,"componentErrors and other component stats were missed by me in the migration.

 "
STORM-3215,New UI is not impersonating user,The new UI switched APIs that it used to get the nimbus client.  Turns out that the APIs are inconsistent on using ReqContext or not.
STORM-3214,使用 kafka.topic.wildcard.match =true的时候，ZkCoordinator.refresh中deletedManagers会出现逻辑错误,"使用 kafka.topic.wildcard.match =true的时候，如果topic数目大于1，ZkCoordinator.refresh中deletedManagers会出现逻辑错误

只需要将ZkCoordinator@L91：    Map<Integer, PartitionManager> deletedManagers = new HashMap<>();

将key修改为topic+partition

 

在org.apache.storm.kafka.ZkCoordinatorTest中添加了如下测试
{code:java}
//代码占位符
public static GlobalPartitionInformation buildPartitionInfo(int numPartitions, int brokerPort, String topic) {
GlobalPartitionInformation globalPartitionInformation = new GlobalPartitionInformation(topic);
for (int i = 0; i < numPartitions; i++) {
globalPartitionInformation.addPartition(i, Broker.fromString(""broker-"" + i + "" :"" + brokerPort));
}
return globalPartitionInformation;
}

@Test
public void testTwoTopicPartitionsChange() throws Exception {
int numPartitions = 2;
int partitionsPerTask = 1;
final Set<Partition> unregisterList = new HashSet<>();
Mockito.doAnswer(new Answer() {
@Override
public Object answer(InvocationOnMock invocation) throws Throwable {
Object[] arguments = invocation.getArguments();
Partition partition = new Partition((Broker) arguments[0], (String) arguments[1], (int) arguments[2], false);
unregisterList.add(partition);
return null;
}
}).when(dynamicPartitionConnections).unregister(any(Broker.class), any(String.class), anyInt());

List<ZkCoordinator> coordinatorList = buildCoordinators(partitionsPerTask);
ArrayList<GlobalPartitionInformation> prePartitionInformations = Lists.newArrayList(buildPartitionInfo(numPartitions, 9092, ""TOPIC1""), 
buildPartitionInfo(numPartitions, 9092, ""TOPIC2""));
when(reader.getBrokerInfo()).thenReturn(prePartitionInformations);
List<List<PartitionManager>> partitionManagersBeforeRefresh = getPartitionManagers(coordinatorList);
waitForRefresh();
when(reader.getBrokerInfo()).thenReturn(Lists.newArrayList(buildPartitionInfo(numPartitions, 9093, ""TOPIC1""), buildPartitionInfo(numPartitions, 9093, ""TOPIC2"")));
List<List<PartitionManager>> partitionManagersAfterRefresh = getPartitionManagers(coordinatorList);
List<Partition> allPrePartition = KafkaUtils.calculatePartitionsForTask(prePartitionInformations, 1, 0, 0);
assertEquals(unregisterList.size(), allPrePartition.size());
for (Partition partition : allPrePartition) {
assertTrue(unregisterList.contains(partition));
}
}
{code}"
STORM-3212,Trident Kafka Spout throws NPE if Translator Returns Null,"The Javadoc for the RecordTranslator#apply(ConsumerRecord) method says to return null to discard a ConsumerRecord. But doing that when using a Trident Kafka Spout causes the spout to throw a NullPointerException.

 "
STORM-3211,WindowedBoltExecutor NPE if wrapped bolt returns null from getComponentConfiguration,"{code}
Exception in thread ""main"" java.lang.NullPointerException
    at org.apache.storm.topology.WindowedBoltExecutor.declareOutputFields(WindowedBoltExecutor.java:309)
    at org.apache.storm.topology.TopologyBuilder.getComponentCommon(TopologyBuilder.java:432)
    at org.apache.storm.topology.TopologyBuilder.createTopology(TopologyBuilder.java:120)
    at Main.main(Main.java:23)
{code}"
STORM-3209,org.apache.storm.jdbc.common.Util,"Util.getJavaType(int sqlType) not support NUMERIC,

when my sqlType is NUMERIC( 2 also),it throw ex;

 

 "
STORM-3208,supervisor NPE trying to kill workers,"{code:java}
2018-08-29 15:37:47.891 o.a.s.u.Utils main [INFO] UNNAMED:main : user is gstorm
2018-08-29 15:37:47.893 o.a.s.d.s.Supervisor main [ERROR] Error trying to kill 7f4dd1bb-ea77-4f13-a785-0299e81bf5a5
java.lang.NullPointerException: null
        at org.apache.storm.daemon.supervisor.BasicContainer.cleanUpForRestart(BasicContainer.java:216) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Container.cleanUp(Container.java:360) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.killWorkers(Supervisor.java:482) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.ReadClusterState.<init>(ReadClusterState.java:111) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.launch(Supervisor.java:282) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.launchDaemon(Supervisor.java:312) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.supervisor.Supervisor.main(Supervisor.java:185) [storm-server-2.0.0.y.jar:2.0.0.y]
2018-08-29 15:37:47.904 o.a.s.d.s.Supervisor main [INFO] Starting supervisor with id 03ee87f5-28ca-491b-95cb-15b841f249e1-10.215.76.240 at host openqe74blue-n1.blue.ygrid.yahoo.com.

{code}"
STORM-3205,Optimization in TuplImpl,"Wrapping {{TuplImpl.values}} with Collections.unmodifiableList() turns out be very expensive. Its intention is obviously to check and prevent accidental tweaking TuplImpl once created. Given the high cost, if needed, we can limit this extra checking mechanism in debug/dev mode. Being in the critical path it means several thousand/million additional allocations per second of the List wrapper object .... proportional to the number of bolt/spout instances.

*TVL :*
| |throughput (k/sec)|cores|mem (mb)|
|*master (#f5a410ba3)*|412 |4.26|103|
|*storm-3205*|547  (+33%)|4.09|132|

{{+cmd:+ bin/storm jar topos/storm-loadgen-2.0.0-SNAPSHOT.jar org.apache.storm.loadgen.ThroughputVsLatency *--rate 550000* --spouts 1 --splitters 3 --counters 2 -c topology.acker.executors=0}}

*+ConstSpoutIdentityBoltNullBolt :+*
| |throughput|
|*master*|4.25 mill/sec|
|*storm-3205*|5.4 mill/sec (+27%)|

+cmd+: {{bin/storm jar topos/storm-perf-2.0.0-SNAPSHOT.jar org.apache.storm.perf.ConstSpoutIdBoltNullBoltTopo -c topology.acker.executors=0 -c topology.producer.batch.size=1000 400}}

*Note:* The perf gains are more evident when operating at high thoughputs w/o backpressure occurring (i.e. some bolts have not yet become a bottleneck)"
STORM-3204,Upgrade Dropwizard Metrics to 4.0.3,"Since we've removed metrics-ganglia, we can upgrade to the latest Metrics version. It has some fixes for Java 9+ https://github.com/dropwizard/metrics/pull/1236."
STORM-3203,AsyncLocalizer is not updating permissions for storm.conf storm.ser and storm.jar,"Only in 2.0 the AsyncLocalizer is not updating the permissions at all.  It looks like it is some code I missed when refactoring things, but I have a fix for it."
STORM-3202,Include offset information to spout metrics and remove storm-kafka-monitor,"To provide offset information on Kafka spout (old and new), we have storm-kafka-monitor module which is being run by UI (shell). This approach requires UI doing too many things - basically UI process does most of things via interacting with Nimbus - and also running external shell process in UI process per opening topology page doesn't look right.

We could just let Spout include offset information into spout metric, and let UI leverage the information. I have been thinking about this approach but forgot about addressing it while thinking about generalizing the format. Now I think we don't have to put too much effort to generalize format, because Kafka spout is used mainly.

 "
STORM-3199,"Metrics-ganglia depends on an LGPL library, so we shouldn't depend on it","https://issues.apache.org/jira/browse/STORM-2153 introduced a dependency on metrics-ganglia, which depends on remotetea-oncrpc. This library appears to be licensed under LGPL.

The Dropwizard metrics project removed it for the same reason https://github.com/dropwizard/metrics/issues/1319. I think we also need to get rid of it."
STORM-3197,Make StormMetricsRegistry a regular instance class rather than a static utility,Having the registry be a static utility makes fixing some issues harder than it should be. Preventing daemons from accidentally registering metrics for other daemons (STORM-3101) and flushing reporters on shutdown without breaking the tests (STORM-3173) for example.
STORM-3194,Reduce logging level of FIFOSchedulingPriorityStrategy,I'm seeing FIFOSchedulingPriorityStrategy eating up a ton of logging lines on larger clusters.  I'd like to switch the logging to debug to allow a more readable log (as well as preserve a longer log history).
STORM-3192,ClusterSummaryMetrics is activating/deactivating by polling for whether Nimbus is leader. It should be notified instead.,See https://github.com/apache/storm/pull/2764#discussion_r209333494.
STORM-3190,Unnecessary null check of directory stream in LogCleaner,This should be using try-with-resources https://github.com/apache/storm/blob/a1b3e02aab57b4e458b8b5763a0d467852906bb7/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogCleaner.java#L263
STORM-3189,Remove unused data file LogViewer api,"Discovered in STORM-3133.

`findNMatches` in LogviewerLogSearchHandler returns a `Matched` object which contains a field `fileOffset`. However, in current implementation, `fileOffset` behaves a bit odd and is not being used anywhere in the app. I'm wondering if we should remove this field altogether

Specifically, the difference in behavior follows,
`fileOffset is passed in as the desired amount of file to skip in search (equiv to index of first file to search)

if desired amount of matches found, fileOffset will be the index of last scanned file (starting from 0).
if not enough matches found in all logs, fileOffset will be number of all logs (equiv to one past the index of last file)

See 
https://github.com/apache/storm/pull/2754#discussion_r208691016
https://github.com/apache/storm/pull/2754#discussion_r208726809"
STORM-3188,Removing try-catch block from getAndResetWorkerHeartbeats,"After refactoring, SupervisorUtils.readWorkerHeartbeats no longer throws checked Exceptions. I'm wondering if we still want to keep the try-catch block to wrap around its invocation in getAndResetWorkerHeartbeats in ReportWorkerHeartbeats.java."
STORM-3187,Nimbus code refactoring and cleanup,"Nimbus.java is bloated with many legacy code that are convoluted and inefficient. It would be nice if we can clean up the code a bit, especially now that we're moving away from Clojure.

Several suggestion are made in STORM-3133, including,

1. Remove logging that is of the same purpose of some metrics: https://github.com/apache/storm/pull/2764#discussion_r203727117

2. Refactor data type of return values/parameters to improve readability: https://github.com/apache/storm/pull/2764#discussion_r208699933
https://github.com/apache/storm/pull/2764#discussion_r208721202
https://github.com/apache/storm/pull/2764#discussion_r208707855

3. Other performance improvement
https://github.com/apache/storm/pull/2764#discussion_r208714561"
STORM-3186,Customizable configuration for metric reporting interval,"In current implementation, all subclass of ScheduledReporter are hard coded report interval of 10 seconds. However I think it would make sense to make this an item in configuration so user can change the reporting frequency to fit their needs.

See discussion https://github.com/apache/storm/pull/2764#discussion_r203726617"
STORM-3184,Storm supervisor log showing keystore and truststore password in plaintext,"When we enable SSL for Apache storm, the superviosr log shows the keystore and truststore password in the plaintext



log name : /var/log/storm/supervisor.log 
{code}

2018-05-28 16:21:12.594 o.a.s.d.s.Supervisor main [INFO] Starting supervisor for storm version '1.1.1.3.1.1.0-35'. 
2018-05-28 16:21:12.595 o.a.s.d.s.Supervisor main [INFO] Starting Supervisor with conf {storm.messaging.netty.min_wait_ms=100, storm.zookeeper.auth.user=null, storm.messaging.netty.buffer_s 
ize=5242880, client.jartransformer.class=org.apache.storm.hack.StormShadeTransformer, storm.exhibitor.port=8080, pacemaker.auth.method=NONE, ui.filter=null, worker.profiler.enabled=false 
ui.https.key.password=pass123
ui.https.keystore.password=pass123 

{code}


For the below properties created in custom-storm-site section in Ambari while enabling SSL. 
{code}

ui.https.key.password=pass123 
ui.https.keystore.password=pass123

{code}"
STORM-3183,"Topology Visualization is broken: shows components at most, but no more information","Regardless of enabling ack it doesn't show stream between components.
 
Please attach screenshot.

cc. [~govindmenon]"
STORM-3182,Owner summary page in non-secured cluster shows 500 server error because of NullPointerException,"When opening owner summary page in non-secured cluster, all contents become empty except html template, and shows 500 error with below stack trace:

{code}
java.lang.NullPointerException
	at org.apache.storm.daemon.ui.filters.AuthorizedUserFilter.makeResponse(AuthorizedUserFilter.java:83)
	at org.apache.storm.daemon.ui.filters.AuthorizedUserFilter.filter(AuthorizedUserFilter.java:121)
	at org.glassfish.jersey.server.ContainerFilteringStage.apply(ContainerFilteringStage.java:132)
	at org.glassfish.jersey.server.ContainerFilteringStage.apply(ContainerFilteringStage.java:68)
	at org.glassfish.jersey.process.internal.Stages.process(Stages.java:197)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:269)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:272)
{code}

I'll attach the screenshot.

I'm not sure it would work normally in secured cluster, since I don't have secured cluster.

cc. [~govindmenon]"
STORM-3181,Topology stats doesn't show complete latency even for topology which ack is enabled,"Please see attached screenshot.

cc. [~govindmenon]"
STORM-3180,Total executors in Cluster Summary in main UI page is not exposed even a topology is running,"After STORM-1311, Total executors in Cluster Summary is showing nothing (blank) even one topology is running.

Will attach screenshot which shows the behavior.

cc. [~govindmenon]"
STORM-3179,No data is available for Nimbus Summary in main UI page even Nimbus is running,"After STORM-1311, ""Nimbus Summary"" is showing ""No data available in table"", even it runs with Nimbus.

Will attach screenshot which shows the behavior.

cc. [~govindmenon]"
STORM-3178,Decouple `ClientSupervisorUtils` and refactor metrics registration,See conversation https://github.com/apache/storm/pull/2710#discussion_r207576736
STORM-3177,MockRemovableFile returns true on `#exists` even after `#delete` is called.,See conversation in https://github.com/apache/storm/pull/2788#pullrequestreview-142918985
STORM-3176,KafkaSpout commit offset occurs CommitFailedException which leads to worker dead,"KafkaSpout use the commitAsync api of Consumer, if the interval time between the call of consumer.poll() more than _max.poll.interval.ms_ or the heartbeat of consumer timeout, that will occur CommitFailedException,  and then the worker will die, the log like this: 
{code:java}
// 2018-07-31 19:19:03.341 o.a.s.util [ERROR] Async loop died!
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer th
an the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in
poll() with max.poll.records.
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:698) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:577) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1126) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:XXX) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:430) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:264) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$fn__10936$fn__10951$fn__10982.invoke(executor.clj:647) ~[XXX.jar:?]
at org.apache.storm.util$async_loop$fn__553.invoke(util.clj:484) [XXX.jar:?]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2018-07-31 19:19:03.342 o.a.s.d.executor [ERROR]
{code}
I find it will catch the Exception in auto-commit mode of consumer, the source code is:
{code:java}
// private void maybeAutoCommitOffsetsSync(long timeoutMs) {
    if (autoCommitEnabled) {
        Map<TopicPartition, OffsetAndMetadata> allConsumedOffsets = subscriptions.allConsumed();
        try {
            log.debug(""Sending synchronous auto-commit of offsets {} for group {}"", allConsumedOffsets, groupId);
            if (!commitOffsetsSync(allConsumedOffsets, timeoutMs))
                log.debug(""Auto-commit of offsets {} for group {} timed out before completion"",
                        allConsumedOffsets, groupId);
        } catch (WakeupException | InterruptException e) {
            log.debug(""Auto-commit of offsets {} for group {} was interrupted before completion"",
                    allConsumedOffsets, groupId);
            // rethrow wakeups since they are triggered by the user
            throw e;
        } catch (Exception e) {
            // consistent with async auto-commit failures, we do not propagate the exception
            log.warn(""Auto-commit of offsets {} failed for group {}: {}"", allConsumedOffsets, groupId,
                    e.getMessage());
        }
    }
}
{code}
I think KafkaSpout should do like this, catch the Exception avoid to worker die. And when the msg ack failed, Spout should judge the offset of the msgID is larger than the last commit offset(Spout can guarantee that these msgs which offset less than the last commit offset are all ack), if not, the msg should not retry.

 "
STORM-3174,Standardize exit codes,"Exit codes are hard-coded.  It would be better to centralize and tie them to a meaningful constant.

 

 "
STORM-3173,flush metrics to ScheduledReporter on shutdown,"We lose shutdown related metrics that we should alert on at shutdown. We should flush metrics on a shutdown.

https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L4497"
STORM-3172,NoSuchMethodError on init if using Clojure 1.8 or higher,"Attempting to run `storm jar` with something built on Clojure 1.8 or higher fails with this error:
 ""NoSuchMethodError: clojure.lang.Utils.loadWithClass"".

It turns out this method was added to Clojure's `gen-class` in version 1.8 (for reasons to do with serialization that honestly I don't fully understand: [https://dev.clojure.org/jira/browse/CLJ-1157]).

To reproduce the error try running the storm-starter project with this lein project definition at storm/examples/storm-starter/project.clj:
{code:java}
(defproject clojure-gen-class-error-example ""0.1.0-SNAPSHOT""
  :license {:name ""Eclipse Public License""
            :url ""http://www.eclipse.org/legal/epl-v10.html""}
  :dependencies [[org.clojure/clojure ""1.9.0""] 
                 [org.apache.storm/storm-core ""1.2.2""]]
  :source-paths [""src/clj""]
  :aot :all
  :main org.apache.storm.starter.clj.word-count)
{code}
And then run against Storm 1.2.2:
{noformat}
$ lein uberjar
$ storm jar target/clojure-gen-class-error-example-0.1.0-SNAPSHOT.jar

==>
Running: /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/apache-storm-1.2.2 -Dstorm.log.dir=/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /apache-storm-1.2.2/*:/apache-storm-1.2.2/lib/*:/apache-storm-1.2.2/extlib/*:/clojure-gen-class-error-example-0.1.0-SNAPSHOT.jar:/conf:/apache-storm-1.2.2/bin -Dstorm.jar=/data-processor.jar -Dstorm.dependency.jars= -Dstorm.dependency.artifacts={} org.apache.storm.starter.clj.word_count

Exception in thread ""main"" java.lang.NoSuchMethodError: clojure.lang.Util.loadWithClass(Ljava/lang/String;Ljava/lang/Class;)Ljava/lang/Object;
	at org.apache.storm.starter.clj.word_count.<clinit>(Unknown Source)

{noformat}
However switching down to Clojure 1.7.0 in the project.clj (matching storm-core) makes this example work correctly. I assume upgrading the dependency in storm-core to 1.9.0 would resolve the problem. It's also a good idea because Clojure 1.7.0 is quite old now (released Jun 2015, 1.8.0 was Jan 2016 and 1.9.0 was Dec 2017).

Apologies if this upgrade is already planned, I searched the tickets here and couldn't find anything."
STORM-3171,java.lang.NoSuchMethodError in org.apache.storm:storm-kafka-monitor:jar:1.1.2 caused by dependency conflict issue,"Hi, we found a dependency conflict issue in *org.apache.storm:storm-kafka-monitor:jar:1.1.2*, *caused by org.apache.zookeeper:zookeeper:jar*. As shown in the following dependency tree, due to Maven version management, *org.apache.zookeeper:zookeeper:jar:3.4.6* will be loaded, during the packaging process.

 

However, method *<org.apache.zookeeper.server.quorum.flexible.QuorumMaj: void <init>(java.util.Map)>* only defined in *org.apache.zookeeper:zookeeper:jar 3.5.3-beta*, so that there is a crash with the following stack trace when your project referencing the missing method.

 

*Stack trace:*

Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.zookeeper.server.quorum.flexible.QuorumMaj.<init>(Ljava/util/Map;)V

         at org.apache.curator.framework.imps.EnsembleTracker.<init>(EnsembleTracker.java:57)

         at org.apache.curator.framework.imps.CuratorFrameworkImpl.<init>(CuratorFrameworkImpl.java:159)

         at org.apache.curator.framework.CuratorFrameworkFactory$Builder.build(CuratorFrameworkFactory.java:158)

         at org.apache.curator.framework.CuratorFrameworkFactory.newClient(CuratorFrameworkFactory.java:109)

 

*Dependency tree:*

org.apache.storm:storm-kafka-monitor:jar:1.1.2

+- org.apache.kafka:kafka-clients:jar:0.10.1.0:compile
|  +- net.jpountz.lz4:lz4:jar:1.3.0:compile|
|  +- org.xerial.snappy:snappy-java:jar:1.1.2.6:compile|
|  - org.slf4j:slf4j-api:jar:1.7.21:compile|

+- org.apache.curator:curator-framework:jar:4.0.0:compile
|  - org.apache.curator:curator-client:jar:4.0.0:compile|
|     +- *org.apache.zookeeper:zookeeper:jar:3.4.6:compile (version managed from 3.5.3-beta)*|
|     +- jline:jline:jar:0.9.94:compile| |
|      - io.netty:netty:jar:3.9.9.Final:compile (version managed from 3.7.0.Final)| |
|     +- com.google.guava:guava:jar:16.0.1:compile (version managed from 20.0)|
|     - (org.slf4j:slf4j-api:jar:1.7.21:compile - version managed from 1.7.6; omitted for duplicate)|

+- com.googlecode.json-simple:json-simple:jar:1.1:compile

+- commons-cli:commons-cli:jar:1.3.1:compile
 - junit:junit:jar:4.11:test

   - org.hamcrest:hamcrest-core:jar:1.3:test

 

*Solution:*

One choice is to upgrade *org.apache.zookeeper:zookeeper:jar to 3.5.3-beta,* but it is not the best solution, as 3.5.3-beta is not a release version.**

 

Thanks a lot!

Regards,

Leo"
STORM-3170,DirectoryCleaner may not correctly report correct number of deleted files,"In DirectoryCleaner#deleteOldestWhileTooLarge, the original implementation calls file#delete without checking if it succeeds or not, and they're always reported as deleted. This prevents DirectoryCleaner from clean up other files and invalidates any metrics built on top of this."
STORM-3169,Misleading logviewer.cleanup.age.min,"Config specification logviewer.cleanup.age.min labels the duration in minutes passed since a log file is modified before we consider the log to be old. However in the actual use it's been subtracted by nowMills, which is the current time in milliseconds. We should convert it to milliseconds."
STORM-3168,AsyncLocalizer cleanup appears to crash,"I was investigating these blobstore download messages which keep repeating for hours in the supervisor (and nimbus logs).  I turned on debug logging, and was expecting a cleanup debug message every 30 seconds ([https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java#L606).]  It did not log.  I restarted the supervisor, and it started logging again.  It appears to have crashed with some error.  

We should make sure the cleanup runs continuously and logs any failures to investigate.

 
{code:java}
2018-07-30 23:25:35.691 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 2 [ERROR] Could not update blob, will retry again later

java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not download...

        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_131]

        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895) ~[?:1.8.0_131]

        at org.apache.storm.localizer.AsyncLocalizer.updateBlobs(AsyncLocalizer.java:303) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_131]

        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_131]

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]

        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

Caused by: java.lang.RuntimeException: Could not download...

        at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:268) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_131]

        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_131]

        ... 3 more

Caused by: org.apache.storm.generated.KeyNotFoundException

        at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25853) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25821) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25752) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:798) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:785) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:85) ~[storm-client-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:122) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:252) ~[storm-server-2.0.0.y.jar:2.0.0.y]

        at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) ~[?:1.8.0_131]

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_131]

        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_131]

        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_131]

        ... 3 more
{code}"
STORM-3167,Flaky test in metrics_test.clj,"{code}
classname: org.apache.storm.metrics-test / testname: test-builtin-metrics-2
Uncaught exception, not in assertion.
expected: nil
 actual: java.util.ConcurrentModificationException: null
 at java.util.ArrayList$Itr.checkForComodification (ArrayList.java:907)
 java.util.ArrayList$Itr.next (ArrayList.java:857)
 com.google.common.collect.AbstractMapBasedMultimap$WrappedCollection$WrappedIterator.next (AbstractMapBasedMultimap.java:486)
 clojure.lang.PersistentVector.create (PersistentVector.java:105)
 clojure.lang.LazilyPersistentVector.create (LazilyPersistentVector.java:32)
 clojure.core$vec.invoke (core.clj:361)
 org.apache.storm.util$clojurify_structure$fn__206.invoke (util.clj:85)
 clojure.walk$prewalk.invoke (walk.clj:64)
 clojure.core$partial$fn__4527.invoke (core.clj:2493)
 clojure.core$map$fn__4553.invoke (core.clj:2622)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.RT.seq (RT.java:507)
 clojure.core/seq (core.clj:137)
 clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
 clojure.core.protocols/fn (protocols.clj:101)
 clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
 clojure.core$reduce.invoke (core.clj:6519)
 clojure.core$into.invoke (core.clj:6600)
 clojure.walk$walk.invoke (walk.clj:49)
 clojure.walk$prewalk.invoke (walk.clj:64)
 clojure.core$partial$fn__4527.invoke (core.clj:2493)
 clojure.core$map$fn__4553.invoke (core.clj:2624)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.RT.seq (RT.java:507)
 clojure.core/seq (core.clj:137)
 clojure.core.protocols$seq_reduce.invoke (protocols.clj:30)
 clojure.core.protocols/fn (protocols.clj:101)
 clojure.core.protocols$fn__6452$G__6447__6465.invoke (protocols.clj:13)
 clojure.core$reduce.invoke (core.clj:6519)
 clojure.core$into.invoke (core.clj:6600)
 clojure.walk$walk.invoke (walk.clj:49)
 clojure.walk$prewalk.invoke (walk.clj:64)
 org.apache.storm.util$clojurify_structure.invoke (util.clj:83)
 org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_$reify__1258.exec (metrics_test.clj:79)
 org.apache.storm.Testing.whileTimeout (Testing.java:103)
 org.apache.storm.metrics_test$wait_for_atleast_N_buckets_BANG_.invoke (metrics_test.clj:77)
 org.apache.storm.metrics_test$assert_metric_running_sum_BANG_.invoke (metrics_test.clj:98)
 org.apache.storm.metrics_test/fn (metrics_test.clj:326)
 clojure.test$test_var$fn__7670.invoke (test.clj:704)
 clojure.test$test_var.invoke (test.clj:704)
 clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
 clojure.test$default_fixture.invoke (test.clj:674)
 clojure.test$test_vars$fn__7692.invoke (test.clj:722)
 clojure.test$default_fixture.invoke (test.clj:674)
 clojure.test$test_vars.invoke (test.clj:718)
 clojure.test$test_all_vars.invoke (test.clj:728)
 clojure.test$test_ns.invoke (test.clj:747)
 clojure.core$map$fn__4553.invoke (core.clj:2624)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.Cons.next (Cons.java:39)
 clojure.lang.RT.boundedLength (RT.java:1735)
 clojure.lang.RestFn.applyTo (RestFn.java:130)
 clojure.core$apply.invoke (core.clj:632)
 clojure.test$run_tests.doInvoke (test.clj:762)
 clojure.lang.RestFn.invoke (RestFn.java:408)
 org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132$fn__5133.invoke (test_runner.clj:107)
 org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132.invoke (test_runner.clj:53)
 org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131.invoke (test_runner.clj:52)
 clojure.lang.LazySeq.sval (LazySeq.java:40)
 clojure.lang.LazySeq.seq (LazySeq.java:49)
 clojure.lang.RT.seq (RT.java:507)
 clojure.core/seq (core.clj:137)
 clojure.core$dorun.invoke (core.clj:3009)
 org.apache.storm.testrunner$eval5125.invoke (test_runner.clj:52)
 clojure.lang.Compiler.eval (Compiler.java:6782)
 clojure.lang.Compiler.load (Compiler.java:7227)
 clojure.lang.Compiler.loadFile (Compiler.java:7165)
 clojure.main$load_script.invoke (main.clj:275)
 clojure.main$script_opt.invoke (main.clj:337)
 clojure.main$main.doInvoke (main.clj:421)
 clojure.lang.RestFn.invoke (RestFn.java:421)
 clojure.lang.Var.invoke (Var.java:383)
 clojure.lang.AFn.applyToHelper (AFn.java:156)
 clojure.lang.Var.applyTo (Var.java:700)
 clojure.main.main (main.java:37)
{code}
 

It looks to me like the issue is that the FakeMetricsConsumer.getTaskIdToBuckets returns a view of a map that may be modified at any time (the getTaskIdToBuckets is synchronized on the map). When the method returns, the lock is released, but the return value is a view of the map, rather than a copy. This makes iteration over the return value unsafe. The method should instead copy the map before returning it."
STORM-3166,Utils.threadDump does not account for dead threads,"Saw this test failure
{code}
classname: integration.org.apache.storm.integration-test / testname: test-validate-topology-structure
Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.NullPointerException: null
 at org.apache.storm.utils.Utils.threadDump (Utils.java:1191)
    org.apache.storm.Testing.whileTimeout (Testing.java:107)
    org.apache.storm.Testing.completeTopology (Testing.java:437)
    integration.org.apache.storm.integration_test$try_complete_wc_topology.invoke (integration_test.clj:247)
    integration.org.apache.storm.integration_test/fn (integration_test.clj:259)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
    clojure.test$test_ns.invoke (test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132$fn__5133.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131$fn__5132.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval5125$iter__5126__5130$fn__5131.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval5125.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
{code}

Utils.threadDump needs to check whether ThreadInfo objects are null before trying to dereference them, since the ThreadMxBean.getThreadInfo method will return null for threads that are dead."
STORM-3165,Better Unified Metrics API with Dimensions,"The current metrics system is really painful.  We have multiple different metrics APIs for both daemon and worker metrics.  We don't support dimensions consistently because we hacked them on top of only one of the metrics APIs in a way that is not extensible compatible.

 

We need a real final solution.  Internally at Oath we have a new metrics client API that fulfills a lot of these, and I am working with the author to open source it, but at the same time I don't think everyone wants to use this API, some will want to use dropwizard or some other reporter so I am going to put up a thin API that will allow us to have multiple different back ends, probably similar to how bookkeeper does it."
STORM-3164,Multilang storm.py uses traceback.format_exc incorrectly,"{code:title=storm.py}
                    except Exception as e:
                        reportError(traceback.format_exc(e))
                        fail(tup)
        except Exception as e:
reportError(traceback.format_exc(e))
{code}

The method signature for traceback.format_exc is (limit=None, chain=True). Where limit is an int and chain a bool. See documentation for python2.7 and 3:

https://docs.python.org/2.7/library/traceback.html
https://docs.python.org/3/library/traceback.html

Passing an Exception object results in the exception handling code throwing an exception itself and crashing out as a result:
{code}
During handling of the above exception, another exception occurred:                                                                                                                                                
                                                                                                                                                                                                                   
Traceback (most recent call last):                                                                                                                                                                                 
  File ""word_joiner.py"", line 20, in <module>                                                                                                                                                                             
    WordJoiner().run()                                                                                                                                                                                             
  File ""/tmp/be86d36d-d293-4694-a8f0-0f018e540936/supervisor/stormdist/test-1-1532824220/resources/storm.py"", line 200, in run                                                                                   
    reportError(traceback.format_exc(e))                                                                                                                                                                           
  File ""/usr/lib/python3.4/traceback.py"", line 256, in format_exc                                                                                                                                                  
    return """".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))                                                                                                                                    
  File ""/usr/lib/python3.4/traceback.py"", line 181, in format_exception                                                                                                                                            
    return list(_format_exception_iter(etype, value, tb, limit, chain))                                                                                                                                            
  File ""/usr/lib/python3.4/traceback.py"", line 153, in _format_exception_iter                                                                                                                                      
    yield from _format_list_iter(_extract_tb_iter(tb, limit=limit))                                                                                                                                                
  File ""/usr/lib/python3.4/traceback.py"", line 18, in _format_list_iter                                                                                                                                            
    for filename, lineno, name, line in extracted_list:                                                                                                                                                            
  File ""/usr/lib/python3.4/traceback.py"", line 58, in _extract_tb_or_stack_iter                                                                                                                                    
    while curr is not None and (limit is None or n < limit):                                                                                                                                                       
TypeError: unorderable types: int() < TypeError()
{code}

The solution in this case is to simply not pass any arguments to traceback.format_exc. It will automatically fetch the context of the catch block it resides in and gracefully return the traceback as a string, which is what storm.py is expecting."
STORM-3163,ShellLogHandler loses thread context between setup and use,"*UPDATE*

Turns out that this is much better solved by leveraging {{log4j2}}'s {{isThreadContextMapInheritable}} property, which hands child threads a point-in-time copy of the parent's {{MDC}} contents, which completely solves the issue this sought to address.

---

*ORIGINAL*

tl;dr: {{ShellLogHandler}} is handed context in one thread, before being used exclusively from another, this obstructs sane usage of {{slf4j}}'s {{MDC}} feature which is thread local.

---

{{ShellBolt}} instantiates the {{ShellLogHandler}} and calls its {{setUpContext}} as part of {{prepare}}, immediately before it spawns its {{BoltReaderRunnable}} and {{BoltWriterRunnable}} threads which are responsible for communication with the {{ShellProcess}} that's already been spawned.

The {{ShellLogHandler}} is used exclusively from {{BoltReaderRunnable}}. The upshot of this is that {{setUpContext}} is executed in the task thread (i.e. {{Thread-21-joiner-executor[2 2]}}) while the {{log}} method is executed in the anonymous thread (i.e. {{Thread-30}}) running the {{BoltReaderRunnable}}.

This creates a problem when trying to leverage {{slf4j}}'s {{MDC}} (or {{NDC}}) which are used for augmenting log messages with additional information which is localised and persisted at the thread level.

The current work around for this is to store all relevant context on the {{ShellLogHandler}} during {{setUpContext}}, and then write it into the {{MDC}} during the {{log}} call, and taking precautions around that state being thread safe. The thread safety requirement is the first drawback, the second is that each {{log}} call has the additional overhead of either unconditionally writing to the {{MDC}} or checking that the {{MDC}} is already populated. Neither is very appealing.

The suggested solution is to pass {{stormConf}}, {{_process}}, and {{_context}} into the constructor of {{BoltReaderRunnable}} (and perhaps also {{BoltWriterRunnable}}) and instantiate {{ShellLogHandler}} and call its {{setUpContext}} from that thread.
"
STORM-3162,Race condition at updateHeartbeatCache,"This is discovered during testing for STORM-3133. Travis-CI log can be found [here|https://travis-ci.org/apache/storm/jobs/408719153#L1897].

Specifically, updateHeartbeatCache can be invoked both by Nimbus (at `Nimbus#updateHeartBeats`) and by Supervisor (at `Nimbubs#updateCachedHeartbeatsFromWorker` at `Nimbus#updateCachedHeartbeatsFromSupervisor`), causing ConcurrentModificationException."
STORM-3161,Local mode should force setting min replication count to 1,"When topology.min.replication.count is set to more than 1, nimbus in local mode never achieve condition for replication, hence stuck on handling blobs. We should force set it to 1 in local mode."
STORM-3159,Fixed potential file resource leak,"`zipFileSize()` in ServerUtils is not correctly wrapped in try-with-resource block, which could lead to resource leak."
STORM-3158,improve login failure message,"We had an issue where the DRPC server (on an older version of storm) was misconfigured and constantly restarting without logging shutdown messages.  One of the warning messages that did log should have been an error and could provide more useful information about fixing the issue.

warning message: 

2018-07-20 15:09:30.191 clojure-agent-send-off-pool-0 b.s.s.a.k.ServerCallbackHandler [WARN] No password found for user: null

 

hopefully more helpful:

2018-07-20 15:44:39.835 clojure-agent-send-off-pool-0 b.s.s.a.k.ServerCallbackHandler [ERROR] No password found for user: null, validate klist matches jaas conf"
STORM-3157,General improvement to StormMetricsRegistry,The solution contains general improvement and clean up to StormMetricsRegistry. Therefore this may affect all current and future changes to server-side metrics
STORM-3156,Remove the transactional topology API,
STORM-3155,IOutputSerializer implementations always allocates a new ByteBuffer,"The IOutputSerializer javadoc specifies that the user may optionally provide a ByteBuffer to serialize into.

https://github.com/apache/storm/blob/af42f434f4a4c3d9087c6058b359033736d3b5e8/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/IOutputSerializer.java#L26

None of the IOutputSerializer implementations we ship with actually do this. They all ignore the ByteBuffer parameter.

If this is a useful feature, I think that we should update them to use the supplied ByteBuffer if not null. If it isn't a useful feature, we should instead remove the ByteBuffer parameter from the API."
STORM-3153,Restore storm sql provider tests affected via STORM-2406,"Current proposed patch for STORM-2406 gets rid of major parts of tests for provider, because we can't do same test newly changed code. To restore tests additional code change is needed.

This issue is to track the effort of making change of Storm SQL to be able to restore previous tests.

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-hdfs/src/test/org/apache/storm/sql/hdfs/TestHdfsDataSourcesProvider.java#L90]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-kafka/src/test/org/apache/storm/sql/kafka/TestKafkaDataSourcesProvider.java]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-mongodb/src/test/org/apache/storm/sql/mongodb/TestMongoDataSourcesProvider.java]

[https://github.com/apache/storm/blob/c9e9a7c294458c8bb1166e0646a5fa580661e21e/sql/storm-sql-external/storm-sql-redis/src/test/org/apache/storm/sql/redis/TestRedisDataSourcesProvider.java]

 "
STORM-3152,Storm has supported ipv6 but Troubleshooting.md didn't update,"As storm's socket get inetaddress by java's InetAddress class,I think there is no difficulty to run Topology on ipv6.And I have test it on ipv6,all socket can be established on ipv6's ip."
STORM-3151,Negative Scheduling Resource/Overscheduling issue,"Possible overscheduling captured when follow steps are performed (Logging is added in STORM-3147)

1) launch nimbus & zookeeper

2) launch supervisor 1

3) launch topology 1 (I used org.apache.storm.starter.WordCountTopology)

4) launch supervisor 2

5) launch topology 2 (I used org.apache.storm.starter.ExclamationTopology)

{noformat}
2018-07-13 12:58:43.196 o.a.s.d.n.Nimbus timer [WARN] Memory over-scheduled on 176ec6d4-2df3-40ca-95ca-c84a81dbcc22-172.130.97.212
{noformat}

Indicating there may be issues inside scheduler.
It is discovered when I ported ClusterSummay to StormMetrics"
STORM-3150,Improve Gauge Registration in StormMetricsRegistry,Make #registerGauge and #registerProvidedGauge generic and clean up other code.
STORM-3149,Why did an exception in the client read not bring down the entire worker,"[https://github.com/apache/storm/pull/2762]

 

ran into some issues where we got an array index out of bounds, but it didn't bring down the worker, just caused issues with the one message being sent.  We should understand what was happening and if there is anything we should fix."
STORM-3148,StormServerPipelineFactory can deserialize messages incorrectly,"We recently ran into an integration test failure. (TestingTest).

It looks like the only way for this error to happen would be if there was an internal bug in kryo, or if we were using Output from multiple threads.  It is the latter.

 

org.apache.storm.messaging.netty.Server creates a single KryoValuesSerializer and KryoValuesDeserializer instance.  These get passed through StormServerPipelineFactory and added to each channel (if there are multiple channels there are multiple threads) and we can mess up both encoding and decoding messages.

 
{code:java}
2018-07-12 17:41:49.408 [Netty-server-localhost-1030-worker-1] ERROR org.apache.storm.messaging.netty.StormServerHandler - server errors in handling the request
org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.ArrayIndexOutOfBoundsException
    at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.Server.channelActive(Server.java:261) [storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.StormServerHandler.channelActive(StormServerHandler.java:40) [storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:518) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:465) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.ArrayIndexOutOfBoundsException
    at java.lang.System.arraycopy(Native Method) ~[?:1.8.0_60]
    at com.esotericsoftware.kryo.io.Output.toBytes(Output.java:130) ~[kryo-3.0.3.jar:?]
    at org.apache.storm.serialization.KryoValuesSerializer.serializeObject(KryoValuesSerializer.java:50) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.BackPressureStatus.buffer(BackPressureStatus.java:68) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.BackPressureStatusEncoder.encode(BackPressureStatusEncoder.java:34) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.messaging.netty.BackPressureStatusEncoder.encode(BackPressureStatusEncoder.java:24) ~[storm-client-2.0.0.y.jar:2.0.0.y]
    at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
    ... 27 more
   {code}"
STORM-3147,Port ClusterSummary as Metrics to StormMetricsRegistry,
STORM-3146,dependencies conflict,"I have configure apache hadoop cluster (2.9.1) . storm hdfs by default it takes 2.6.1 hadoop dependencies  , I have excluded it from dependencies and add hadoop 2.9.1 dependencies. I have attached my pom along with it. 

I am finding following error:

java.lang.NoSuchMethodError: org.apache.hadoop.security.authentication.util.KerberosUtil.hasKerberosTicket(Ljavax/security/auth/Subject;)Z
 at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:666) ~[hadoop-common-2.9.1.jar:?]
 at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:861) ~[hadoop-common-2.9.1.jar:?]

 

When I change the hadoop version to 2.6.1 ,I am not finding that error.

 

 

 "
STORM-3144,Extend metrics on Nimbus,"Metrics include:
 # File upload time
 # Nimbus restart count
 # Nimbus loss of leadership: meter marking when a nimbus node gains or loses leadership
 # Excessive scheduling time (both duration distribution and current longest)"
STORM-3143,Unnecessary inclusion of empty match result in Json,"`FindNMatches()` didn't correctly filter out empty match result in `substringSearch()` and hence send back an empty map to user. I don't know if this the desired behavior but a fix to current behavior will make metrics for logviewer easier to implement. 

An example of current behavior:

{code:json}
{
    ""fileOffset"": 1,
    ""searchString"": ""sdf"",
    ""matches"": [
        {
            ""searchString"": ""sdf"",
            ""fileName"": ""word-count-1-1530815972/6701/worker.log"",
            ""matches"": [],
            ""port"": ""6701"",
            ""isDaemon"": ""no"",
            ""startByteOffset"": 0
        }
    ]
}
{code}

Desired behavior:

{code:json}
{
    ""fileOffset"": 1,
    ""searchString"": ""sdf"",
    ""matches"": []
}
{code}"
STORM-3142,Add support for JUnit 5 tests,"I think it would be nice if we could use the new JUnit 5 APIs for testing. Since JUnit 5 can run JUnit 4 tests, it shouldn't be too much work to add support."
STORM-3141,NPE in WorkerState.transferLocalBatch when receiving messages for a task that isn't the first task assigned to the executor,"{code}
2018-07-02 20:32:28.944 [Worker-Transfer] ERROR org.apache.storm.utils.Utils - Async loop died!
java.lang.NullPointerException: null
	at org.apache.storm.daemon.worker.WorkerState.transferLocalBatch(WorkerState.java:538) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:71) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.local.Context$LocalClient.send(Context.java:194) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.TransferDrainer.send(TransferDrainer.java:53) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.flush(WorkerTransfer.java:100) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:146) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.lambda$makeTransferThread$0(WorkerTransfer.java:82) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:353) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
2018-07-02 20:32:28.945 [Worker-Transfer] ERROR org.apache.storm.utils.Utils - Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.storm.utils.Utils$2.run(Utils.java:368) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: java.lang.NullPointerException
	at org.apache.storm.daemon.worker.WorkerState.transferLocalBatch(WorkerState.java:538) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.DeserializingConnectionCallback.recv(DeserializingConnectionCallback.java:71) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.local.Context$LocalClient.send(Context.java:194) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.TransferDrainer.send(TransferDrainer.java:53) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.flush(WorkerTransfer.java:100) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:146) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerTransfer.lambda$makeTransferThread$0(WorkerTransfer.java:82) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:353) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 1 more
{code}

When tuples are received, the method looks up which JCQueue to send to. It does this with the tuple destination task id. The map it looks in only has the queues by the starting task id of the executor's range, so if the destination is e.g. task 4 for an executor with assignment [3, 4], we hit an NPE."
STORM-3140,Duplicated method in Logviewer REST API?,"{code:java}
    /**
     * Handles '/searchLogs' request.
     */
    @GET
    @Path(""/searchLogs"")
    public Response searchLogs(@Context HttpServletRequest request) throws IOException {
        String user = httpCredsHandler.getUserName(request);
        String topologyId = request.getParameter(""topoId"");
        String portStr = request.getParameter(""port"");
        String callback = request.getParameter(""callback"");
        String origin = request.getHeader(""Origin"");

        return logviewer.listLogFiles(user, portStr != null ? Integer.parseInt(portStr) : null, topologyId, callback, origin);
    }

    /**
     * Handles '/listLogs' request.
     */
    @GET
    @Path(""/listLogs"")
    public Response listLogs(@Context HttpServletRequest request) throws IOException {
        meterListLogsHttpRequests.mark();

        String user = httpCredsHandler.getUserName(request);
        String topologyId = request.getParameter(""topoId"");
        String portStr = request.getParameter(""port"");
        String callback = request.getParameter(""callback"");
        String origin = request.getHeader(""Origin"");

        return logviewer.listLogFiles(user, portStr != null ? Integer.parseInt(portStr) : null, topologyId, callback, origin);
    }{code}

These two methods are identical although they seem to serve different functions."
STORM-3139,worker fails to start - KeeperErrorCode = NoAuth for /credentials/topologyname," 

Seeing a sporadic test failure internally for us with a worker that won't come up.  The test schedules a bunch of topologies, kills the supervisors, restarts nimbus, and then starts up the supervisors and validates the topologies are all fully running.

 

I've seen this test failure twice in the last two weeks.  The worker has migrated and cannot come up:

 
{code:java}
2018-06-30 10:15:24.102 b.s.util main [WARN] Expecting exception of class: class java.nio.channels.ClosedByInterruptException, but exception chain only contains: (#<RuntimeException java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966> #<NoAuthException org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966>) 2018-06-30 10:15:24.102 b.s.d.worker main [ERROR] Error on initialization of server mk-worker java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966 at backtype.storm.util$wrap_in_runtime.invoke(util.clj:53) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.zookeeper$get_data.invoke(zookeeper.clj:135) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4249.get_data(zookeeper_state_factory.clj:125) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__4296.get_data(pacemaker_state_factory.clj:175) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at backtype.storm.cluster$mk_storm_cluster_state$reify__3910.credentials(cluster.clj:563) ~[storm-core-0.10.2.y.jar:0.10.2.y] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_131] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_131] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131] at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?] at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$fn__7710$exec_fn__1599__auto____7711.invoke(worker.clj:623) ~[storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?] at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$fn__7710$mk_worker__7803.doInvoke(worker.clj:598) [storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?] at backtype.storm.daemon.worker$_main.invoke(worker.clj:810) [storm-core-0.10.2.y.jar:0.10.2.y] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?] at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.2.y.jar:0.10.2.y] Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /credentials/topology-testHardCoreFaultTolerance-7-21-1530352966 at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:113) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:327) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:316) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:313) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:304) ~[storm-core-0.10.2.y.jar:0.10.2.y] at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:35) ~[storm-core-0.10.2.y.jar:0.10.2.y] at backtype.storm.zookeeper$get_data.invoke(zookeeper.clj:131) ~[storm-core-0.10.2.y.jar:0.10.2.y] ... 31 more 2018-06-30 10:15:24.199 b.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"")
{code}"
STORM-3138,dev-zookeeper logging to stdout is annoying,
STORM-3137,Flaky test in nimbus_test,"Saw a test failure in storm-core

{code}
313081 [main] INFO  o.a.s.d.n.Nimbus - Cleaning up topo3
313081 [main] INFO  o.a.s.d.n.Nimbus - Exception {}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:54) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils.deserialize(Utils.java:717) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:684) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2424) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.rmDependencyJarsInTopology$accessor$ivuy1xAW(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824$auxiliary$0HJjHtWw.call(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.mockito.internal.invocation.RealMethod$FromCallable$1.call(RealMethod.java:40) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.RealMethod$FromBehavior.invoke(RealMethod.java:62) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.InterceptedInvocation.callRealMethod(InterceptedInvocation.java:127) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:43) [mockito-core-2.19.0.jar:?]
	at org.mockito.Answers.answer(Answers.java:100) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:104) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:63) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:110) [mockito-core-2.19.0.jar:?]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.rmDependencyJarsInTopology(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2478) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.doCleanup$accessor$ivuy1xAW(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824$auxiliary$g48aKOaZ.call(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.mockito.internal.invocation.RealMethod$FromCallable$1.call(RealMethod.java:40) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.RealMethod$FromBehavior.invoke(RealMethod.java:62) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.invocation.InterceptedInvocation.callRealMethod(InterceptedInvocation.java:127) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:43) [mockito-core-2.19.0.jar:?]
	at org.mockito.Answers.answer(Answers.java:100) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:104) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:63) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49) [mockito-core-2.19.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:110) [mockito-core-2.19.0.jar:?]
	at org.apache.storm.daemon.nimbus.Nimbus$MockitoMock$2109251824.doCleanup(Unknown Source) [storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) [clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) [clojure-1.7.0.jar:?]
	at org.apache.storm.nimbus_test$fn__4768$fn__4771.invoke(nimbus_test.clj:1899) [?:2.0.0-SNAPSHOT]
	at clojure.core$with_redefs_fn.invoke(core.clj:7209) [clojure-1.7.0.jar:?]
	at org.apache.storm.nimbus_test$fn__4768.invoke(nimbus_test.clj:1895) [?:2.0.0-SNAPSHOT]
	at clojure.test$test_var$fn__7670.invoke(test.clj:704) [clojure-1.7.0.jar:?]
	at clojure.test$test_var.invoke(test.clj:704) [clojure-1.7.0.jar:?]
	at clojure.test$test_vars$fn__7692$fn__7697.invoke(test.clj:722) [clojure-1.7.0.jar:?]
	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]
	at clojure.test$test_vars$fn__7692.invoke(test.clj:722) [clojure-1.7.0.jar:?]
	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]
	at clojure.test$test_vars.invoke(test.clj:718) [clojure-1.7.0.jar:?]
	at clojure.test$test_all_vars.invoke(test.clj:728) [clojure-1.7.0.jar:?]
	at clojure.test$test_ns.invoke(test.clj:747) [clojure-1.7.0.jar:?]
	at clojure.core$map$fn__4553.invoke(core.clj:2624) [clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]
	at clojure.lang.Cons.next(Cons.java:39) [clojure-1.7.0.jar:?]
	at clojure.lang.RT.boundedLength(RT.java:1735) [clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.applyTo(RestFn.java:130) [clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:632) [clojure-1.7.0.jar:?]
	at clojure.test$run_tests.doInvoke(test.clj:762) [clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.invoke(RestFn.java:408) [clojure-1.7.0.jar:?]
	at org.apache.storm.testrunner$eval5473$iter__5474__5478$fn__5479$fn__5480$fn__5481.invoke(test_runner.clj:107) [?:2.0.0-SNAPSHOT]
	at org.apache.storm.testrunner$eval5473$iter__5474__5478$fn__5479$fn__5480.invoke(test_runner.clj:53) [?:2.0.0-SNAPSHOT]
	at org.apache.storm.testrunner$eval5473$iter__5474__5478$fn__5479.invoke(test_runner.clj:52) [?:2.0.0-SNAPSHOT]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]
	at clojure.lang.RT.seq(RT.java:507) [clojure-1.7.0.jar:?]
	at clojure.core$seq__4128.invoke(core.clj:137) [clojure-1.7.0.jar:?]
	at clojure.core$dorun.invoke(core.clj:3009) [clojure-1.7.0.jar:?]
	at org.apache.storm.testrunner$eval5473.invoke(test_runner.clj:52) [?:2.0.0-SNAPSHOT]
	at clojure.lang.Compiler.eval(Compiler.java:6782) [clojure-1.7.0.jar:?]
	at clojure.lang.Compiler.load(Compiler.java:7227) [clojure-1.7.0.jar:?]
	at clojure.lang.Compiler.loadFile(Compiler.java:7165) [clojure-1.7.0.jar:?]
	at clojure.main$load_script.invoke(main.clj:275) [clojure-1.7.0.jar:?]
	at clojure.main$script_opt.invoke(main.clj:337) [clojure-1.7.0.jar:?]
	at clojure.main$main.doInvoke(main.clj:421) [clojure-1.7.0.jar:?]
	at clojure.lang.RestFn.invoke(RestFn.java:421) [clojure-1.7.0.jar:?]
	at clojure.lang.Var.invoke(Var.java:383) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyToHelper(AFn.java:156) [clojure-1.7.0.jar:?]
	at clojure.lang.Var.applyTo(Var.java:700) [clojure-1.7.0.jar:?]
	at clojure.main.main(main.java:37) [clojure-1.7.0.jar:?]
Caused by: java.lang.NullPointerException
	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106) ~[?:1.8.0_151]
	at org.apache.storm.utils.Utils.gunzip(Utils.java:826) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:51) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 80 more
{code}

Full run is here https://travis-ci.org/apache/storm/jobs/398415336."
STORM-3136,"Fix flaky integration test, and make it more readable","The integration test appears flaky, e.g. https://travis-ci.org/apache/storm/jobs/397471420.

We should try to fix this. I also find the integration test code hard to read and understand, so I'd like to do some cleanup."
STORM-3135,JCQueueTest is flaky,"Saw the JCQueueTest fail a couple of times on Travis.

{code}
testFirstMessageFirst(org.apache.storm.utils.JCQueueTest)  Time elapsed: 0.86 sec  <<< FAILURE!
java.lang.AssertionError: Unable to send halt interrupt
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.storm.utils.JCQueueTest.run(JCQueueTest.java:151)
	at org.apache.storm.utils.JCQueueTest.run(JCQueueTest.java:127)
	at org.apache.storm.utils.JCQueueTest.testFirstMessageFirst(JCQueueTest.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
{code}

I think we should just keep trying to send the interrupt until we succeed, or we hit a reasonable timeout."
STORM-3134,upload-credentials imporvements,"There are some things that would be nice to do with AutoCredentials to make debugging and usage simpler.

 

First it would be ideal if we could use the topology conf as a starting point for AutoCredentials instead of requiring the user to provide all configs from the get go.

 

It would also be nice to warn users if they are overriding the list of AutoCreds in a way that makes it so some of the creds might not be interpreted properly (meaning they are uploading creds that there is no plugin on the worker to interpret them)

 

At the same time it would also be really nice to have some help with debugging, both as an admin, but also as a developer to be able to see what creds currently exist (not necessarily the content of the credentials, but the keys)."
STORM-3133,Extend metrics on Nimbus and LogViewer,"Include but not limited to

Logviewer

1. Clean-up time
 2. Time to complete one clean up loop Time. 
 3. Disk usage by logs before cleanup and After cleanup loop. ( Just like GC.?)
 4. Failures/exceptions.
 5. Search request Cnt: By category - Archived/non-archived
 6. Search Request - Response time
 7. Search Request - 0 result Cnt
 8. Search Result - open files
 9. File partial read count
 10. File Download request Cnt/ And Size served
 11. Disk IO by logviewer
 12. CPU usage ( for unzipping files)

Nimbus Additional:
 * Topology stormjar.ser/stormconf.ser/stormser.ser file upload time.
 * Scheduler related metrics would be a long list generic and specific to different strategies.
 * Most if not all cluster summary can be pushed as Metrics.
 * Restart cnt
 * Nimbus loss of leadership !/jira/images/icons/emoticons/help_16.png|width=16,height=16,align=absmiddle!
 * UI not responding ([https://jira.ouroath.com/browse/YSTORM-4838])
 * Negative resource scheduling events ([https://jira.ouroath.com/browse/YSTORM-4940])
 * Excessive scheduling time  !/jira/images/icons/emoticons/help_16.png|width=16,height=16,align=absmiddle!"
STORM-3130,Add Timer registration and Timed object wrapper to Storm metrics util.,This allows us to time method running duration or variable/resource lifespan.
STORM-3129,Worker state machine does not use correct time util to get start time,Current implementation uses System.currentTimeMillis() instead of Time.currentTimeMillis() to get state start time. This may create problem in unit test as it uses simulated time controlled by Storm Time util.
STORM-3128,Connection refused error in AsyncLocalizerTest,"In AsyncLocalizerTest testKeyNotFoundException, a localBlobStore is created and tries but failed to connect to zookeeper due to connection error. I'm not sure if this compromises the test even though it is passed after connection retry timeout. But it's nice to keep in mind.

{noformat}
2018-06-27 13:05:28.005 [main-SendThread(localhost:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-06-27 13:05:28.032 [main] INFO  org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl - Default schema
2018-06-27 13:05:28.035 [main-SendThread(localhost:2181)] WARN  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_171]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:1.8.0_171]
	at org.apache.storm.shade.org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081) [shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
{noformat}

I managed to track down the source where the exception is thrown, but it's really strange that this is called by a StormTimer inside Supervisor, which is not declared anywhere in this test. I'm completely lost by now.


{noformat}
2018-08-08 11:45:30.217 [heartbeatTimer] ERROR org.apache.storm.zookeeper.ClientZookeeper - e: {}
org.apache.storm.shade.org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /supervisors
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:99) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:268) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl$3.call(ExistsBuilderImpl.java:257) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.connection.StandardConnectionHandlingPolicy.callWithRetry(StandardConnectionHandlingPolicy.java:64) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:100) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForegroundStandard(ExistsBuilderImpl.java:254) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:247) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:206) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:35) ~[shaded-deps-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:145) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:292) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.cluster.ZKStateStorage.set_ephemeral_node(ZKStateStorage.java:129) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.cluster.StormClusterStateImpl.supervisorHeartbeat(StormClusterStateImpl.java:522) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.daemon.supervisor.timer.SupervisorHeartbeat.run(SupervisorHeartbeat.java:96) [classes/:?]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) [storm-client-2.0.0-SNAPSHOT.jar:?]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0-SNAPSHOT.jar:?]
{noformat}
"
STORM-3127,Avoid potential race condition ,"PortAndAssignment and its call back is added after update to a blob is invoked asynchronously. It is not guaranteed that the new dependent worker will be registered before blob informs its update to listening workers. 

This can be fixed by moving addReference call up."
STORM-3126,Avoid unnecessary force kill when invoking storm kill_workers,"Supervisor tries to force kill a worker before checking if it has died, leading to unnecessary force kill call. This is minor but does help clean up logs a little bit. "
STORM-3125,Refactoring methods in components for Supervisor and DRPC,"This is a supplement issue page to STORM-3099, separating out refactoring work from metrics addition.

A few misc bug discovered during refactoring have been incorporate in this issue as well. See links for more information."
STORM-3124,Failures talking to Pacemaker,"{code:java}
2018-06-25 20:21:05.220 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 7 more attempts.
2018-06-25 20:21:06.220 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:06.220 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 6 more attempts.
2018-06-25 20:21:07.220 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:07.221 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 5 more attempts.
2018-06-25 20:21:08.221 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:08.221 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 4 more attempts.
2018-06-25 20:21:09.222 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:09.222 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 3 more attempts.
2018-06-25 20:21:10.222 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:10.222 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 2 more attempts.
2018-06-25 20:21:11.223 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:11.223 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 1 more attempts.
2018-06-25 20:21:12.223 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..
2018-06-25 20:21:12.223 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 0 more attempts.
2018-06-25 20:21:13.224 o.a.s.p.PacemakerClientPool timer [WARN] Failed to connect to the pacemaker server openqe74blue-n2.blue.ygrid.yahoo.com
2018-06-25 20:21:13.229 o.a.s.d.n.Nimbus pool-37-thread-481 [INFO] uploadedJar /home/y/var/storm/nimbus/inbox/stormjar-c5893ba3-21c6-4397-84e2-54aab8e091a9.jar
2018-06-25 20:21:13.225 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2773) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:110) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:226) [storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:214) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:482) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:897) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2469) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2771) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
Caused by: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.
        at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:71) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:199) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:482) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:897) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2469) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2771) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-06-25 20:21:13.231 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:470) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:490) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:253) [storm-client-2.0.0.y.jar:2.0.0.y]
2018-06-25 20:21:13.232 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master
2018-06-25 20:21:13.232 o.a.s.u.Utils Thread-13 [INFO] Halting after 10 seconds


2018-06-25 20:21:13.677 o.a.s.d.n.Nimbus pool-37-thread-481 [INFO] desired replication count 1 achieved, current-replication-count for conf key = 1, current-replication-count for code key = 1, current-replication-count for jar key = 1
2018-06-25 20:21:13.678 o.a.s.d.n.Nimbus pool-37-thread-481 [WARN] Topology submission exception. (topology name='run')
java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3024) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3511) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3490) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-06-25 20:21:13.680 o.a.s.t.ProcessFunction pool-37-thread-481 [ERROR] Internal error processing submitTopologyWithOpts
java.lang.RuntimeException: java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3049) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3511) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3490) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3024) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more

{code}
We're having sporadic failures talking to Pacemaker.  This callstack shows us unable to launch topologies."
STORM-3123,Storm Kafka Monitor does not work with Kafka over two-way SSL,"Storm Kafka Monitor has no option to read / parse SSL truststore/keystore properties which are required to connect to Kafka running over two-way SSL. As a fix, it needs to understand the following additional Kafka properties:
{code:java}
ssl.truststore.location=<truststore-file>
ssl.truststore.password=<password>
ssl.keystore.location=<keystore-file>
ssl.keystore.password=<password>
ssl.key.password=<password>
{code}
Since, JVM has a fallback mechanism for loading SSL truststore, Storm Kafka Monitor would always endup using some truststore and would eventually work with one-way SSL (which is also a default for Kafka setup).

Since there is no such fallback for SSL keystore, Storm Kafka Monitor would start without a keystore and would eventually throw this error (in SSL debug mode):
{code:java}
Warning: no suitable certificate found - continuing without client authentication
*** Certificate chain
<Empty>
***
{code}
At this time, Kafka broker would complain about above like this:
{code:java}
kafka-network-thread-1002-SSL-7, READ: TLSv1.2 Handshake, length = 141
*** Certificate chain
<Empty>
***
kafka-network-thread-1002-SSL-7, fatal error: 42: null cert chain
javax.net.ssl.SSLHandshakeException: null cert chain
{code}
Therefore, in the absence of this fix, the only available workaround is to stick to one-way SSL in Kafka (i.e. keep ssl.client.auth=none in Kafka)."
STORM-3122,"FNFE due to race condition between ""async localizer"" and ""update blob"" timer thread","There's race condition between ""async localizer"" and ""update blob"" timer thread.

When worker is shutting down, reference count for blob will be 0 and supervisor will remove actual blob file. There's also ""update blob"" timer thread which tries to keep blobs updated for downloaded topologies. While updating topology it should read some of blob files already downloaded assuming these files should be downloaded before, and the assumption is broken because of async localizer.

[~arunmahadevan] suggested an approach to fix this: ""updateBlobsForTopology"" can just catch the FIleNotFoundException and skip updating the blobs in case it can't find the stormconf, and the approach looks simplest fix so I'll provide a patch based on suggestion.

Btw, it doesn't apply to master branch, since in master branch all blobs are synced up separately (no need to read stormconf to enumerate topology related blobs), and update logic is already fault-tolerance (skip to next sync when it can't pull the blob)."
STORM-3121,Fix flaky metrics tests in storm-core,"The tests are flaky, but only rarely fail. I've only seen them fail on Travis when Travis is under load.

Example failures:
{code}
classname: org.apache.storm.metrics-test / testname: test-custom-metric-with-multi-tasks
expected: (clojure.core/= [1 0 0 0 0 0 2] (clojure.core/subvec (org.apache.storm.metrics-test/lookup-bucket-by-comp-id-&-metric-name! ""2"" ""my-custom-metric"") 0 N__3207__auto__))
  actual: (not (clojure.core/= [1 0 0 0 0 0 2] [1 0 0 0 0 0 0]))
      at: test_runner.clj:105
{code}
{code}
classname: org.apache.storm.metrics-test / testname: test-builtin-metrics-2
expected: (clojure.core/= [1 1] (clojure.core/subvec (org.apache.storm.metrics-test/lookup-bucket-by-comp-id-&-metric-name! ""myspout"" ""__emit-count/default"") 0 N__3207__auto__))
  actual: (not (clojure.core/= [1 1] [1 0]))
      at: test_runner.clj:105
{code}

The problem is that the tests increment metrics counters in the executor async loops, then expect the counters to end up in exact metrics buckets. The creation of a bucket is triggered by the metrics timer. The timer is included in time simulation and LocalCluster.waitForIdle, but the executor async loop isn't. There isn't any guarantee that the executor async loop gets to run when the test does a sequence like
{code}
Time.advanceClusterTime
cluster.waitForIdle
{code}
because the waitForIdle check doesn't know about the executor async loop."
STORM-3120,"Clean up leftover null checks in Time, ensure idle threads get to run when cluster time is advanced",
STORM-3119,Build Storm with JDK 10,"I think we should add Java 10 to the build matrix.

Storm-cassandra and storm-hive are expected not to work for now, because Cassandra and Hive aren't compatible with Java 9 yet, but the rest of Storm should work (Hadoop-based components may or may not work though, I believe they are still working on compatibility, see https://issues.apache.org/jira/browse/HADOOP-11123). We can exclude those two components from the Java 10 build for now."
STORM-3118,Netty incompatibilities with Pacemaker,"Nimbus has issues with Pacemaker:
{code:java}
2018-06-21 08:55:17.762 o.a.s.p.PacemakerClientHandler client-worker-2 [ERROR] Exception occurred in Pacemaker.
org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)
        at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:65) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.ChannelInboundHandlerAdapter.channelActive(ChannelInboundHandlerAdapter.java:64) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelActive(DefaultChannelPipeline.java:1422) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:199) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:635) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:582) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:461) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:884) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)
        at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.ensureWritable0(AbstractByteBuf.java:276) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.buffer.AbstractByteBuf.writeShort(AbstractByteBuf.java:966) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.messaging.netty.SaslMessageToken.write(SaslMessageToken.java:104) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.codec.ThriftEncoder.encodeNettySerializable(ThriftEncoder.java:44) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.pacemaker.codec.ThriftEncoder.encode(ThriftEncoder.java:77) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:88) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        ... 26 more
{code}
Prevents topology submission:

 
{code:java}
2018-06-21 09:10:46.343 o.a.s.d.n.Nimbus pool-37-thread-250 [WARN] Topology submission exception. (topology name='testStormKafkaNewApi')
java.lang.IllegalStateException: instance must be started before calling this method
        at org.apache.storm.shade.org.apache.curator.shaded.com.google.common.base.Preconditions.checkState(Preconditions.java:444) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.checkExists(CuratorFrameworkImpl.java:432) ~[shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.existsNode(ClientZookeeper.java:144) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirsImpl(ClientZookeeper.java:288) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.ClientZookeeper.mkdirs(ClientZookeeper.java:70) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.ZKStateStorage.mkdirs(ZKStateStorage.java:114) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.PaceMakerStateStorage.mkdirs(PaceMakerStateStorage.java:69) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.cluster.StormClusterStateImpl.setupHeatbeats(StormClusterStateImpl.java:435) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:3009) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3508) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3487) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
{code}"
STORM-3117,Deleting blobs for running topologies hoses Nimbus,"The following test pseudo-code causes issues:
{code:java}
cluster.submitTopology(cluster.getTopologiesJarFile(), topoName, config, topology);
cluster.waitTopologyUp(topoName);
cluster.deleteAllBlobs();
{code}
This causes nimbus to get stuck and restart:

 
{code:java}
2018-06-20 15:48:14.273 o.a.s.d.n.Nimbus pool-27-thread-694 [INFO] Received topology submission for wc-topology-test (storm-0.10.2.y.251 JDK-1.8.0_131) 
2018-06-20 15:48:14.629 o.a.s.d.n.Nimbus pool-27-thread-694 [INFO] Activating wc-topology-test: wc-topology-test-1-1529509694
2018-06-20 15:48:14.724 o.a.s.d.n.Nimbus pool-27-thread-703 [INFO] TRANSITION: wc-topology-test-1-1529509694 KILL null true
2018-06-20 15:48:14.812 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormconf.ser
2018-06-20 15:48:14.830 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormcode.ser
2018-06-20 15:48:14.863 o.a.s.d.n.Nimbus pool-27-thread-704 [INFO] Deleted blob for key wc-topology-test-1-1529509694-stormjar.jar
2018-06-20 15:48:18.449 o.a.s.s.r.s.p.DefaultSchedulingPriorityStrategy timer [INFO] SIM Scheduling wc-topology-test-1-1529509694 with score of 0.3125
2018-06-20 15:48:18.492 o.a.s.s.Cluster timer [INFO] STATUS - wc-topology-test-1-1529509694 Running - Fully Scheduled by DefaultResourceAwareStrategy
2018-06-20 15:48:18.527 o.a.s.d.n.Nimbus timer [INFO] Setting new assignment for topology id wc-topology-test-1-1529509694:

2018-06-20 15:48:18.979 o.a.s.d.n.Nimbus pool-27-thread-722 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormjar.jar
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

2018-06-20 15:48:22.884 o.a.s.d.n.Nimbus timer [INFO] Renewing Creds For wc-topology-test-1-1529509694 with org.apache.storm.security.auth.kerberos.AutoTGT@4482469c owned by hadoopqa@DEV.YGRID.YAHOO.COM


2018-06-20 15:48:37.947 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: KeyNotFoundException(msg:wc-topology-test-1-1529509694-stormcode.ser)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2822) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) [storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormcode.ser
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:420) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1517) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2675) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.sendClusterMetricsToExecutors(Nimbus.java:2686) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$48(Nimbus.java:2819) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-06-20 15:48:37.948 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:468) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:488) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) [storm-client-2.0.0.y.jar:2.0.0.y]
2018-06-20 15:48:37.950 o.a.s.d.n.Nimbus Thread-11 [INFO] Shutting down master
2018-06-20 15:48:37.950 o.a.s.u.Utils Thread-12 [INFO] Halting after 10 seconds

2018-06-20 15:48:46.672 o.a.s.d.n.Nimbus pool-27-thread-798 [WARN] get blob meta exception.
org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormconf.ser
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483) [storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:3990) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) [storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291) [shaded-deps-2.0.0.y.jar:2.0.0.y]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-06-20 15:48:47.950 o.a.s.u.Utils Thread-12 [WARN] Forcing Halt...

{code}
Nimbus then continually restarts:
{code:java}
2018-06-20 15:48:54.635 o.a.s.u.Utils main [ERROR] Received error in main thread.. terminating server...
java.lang.Error: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:603) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils.handleUncaughtException(Utils.java:582) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.Utils$5.uncaughtException(Utils.java:931) [storm-client-2.0.0.y.jar:2.0.0.y]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057) [?:1.8.0_131]
        at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052) [?:1.8.0_131]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1959) [?:1.8.0_131]
Caused by: java.lang.IllegalStateException: Could not find credentials for topology wc-topology-test-1-1529509694 at path /storms. Don't know how to fix this automatically. Please add needed ACLs, or delete the path.
        at org.apache.storm.zookeeper.AclEnforcement.getTopoAcl(AclEnforcement.java:194) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithTopoChildren(AclEnforcement.java:250) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyParentWithReadOnlyTopoChildren(AclEnforcement.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.AclEnforcement.verifyAcls(AclEnforcement.java:136) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1155) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1162) ~[storm-server-2.0.0.y.jar:2.0.0.y]
{code}"
STORM-3116,"storm.home is not set in Clojure tests, nor is it guaranteed to be set when using LocalCluster","Several of the Clojure tests are littering directories like storm-core/null/storm-local, because storm.home isn't set. The root pom sets storm.home through Surefire, but that isn't used when running Clojure tests.

Also LocalCluster doesn't set storm.home, so it has the same issue."
STORM-3115,Add admin command to get a zookeeper shell,"At times ZK might get messed up, or a user may just want to see what storm is doing with it and it would be nice to have a simple command line tool that pops us into ZK where we can start to look for issues."
STORM-3114,Ban jdk.tools,jdk.tools snuck back in after STORM-2799 got merged. We should use Enforcer to ban it.
STORM-3113,Upgrade Mockito to fix Java 10 incompatibility,"https://github.com/mockito/mockito/issues/1243

I see no harm in upgrading to the latest Mockito version, which is 2.19.0 at the time of writing."
STORM-3112,Incremental scheduling supports,"As https://issues.apache.org/jira/browse/STORM-3093 described, now the scheduling work for a round is a complete scan and computation for all the topologies on cluster, which is a very heavy work when topologies increment to hundreds.

So this JIRA is to refactor the scheduling logic that only care about topologies that need to.

Promotions list:
1. Cache the id to storm base mapping which reduce the pressure to ZooKeeper.
2. Only schedule the topologies that need to: with dead executors or not enough running workers.
3. For some schedulers we still need a full scheduling, i.e. IsolationScheduler.
4. Cache the scheduling resource bestride multi scheduling round, i.e. nodeId -> used slot, nodeId -> used resource, nodeId -> totalResource.

Cause in https://issues.apache.org/jira/browse/STORM-3093 i already cache the storm-id -> executors mapping, now for a scheduling round, thing we will do:
1. Scan all the active storm bases( cached ) and local storm-conf/storm-topology, then to refresh the heartbeats cache, and we will know which topologies need to schedule.
2. Compute scheduleAssignment only for need scheduling topologies.

About robustness when nimbus restarts:
1. The cached storm-bases are taken care of by ILocalAssignmentsBackend.
2. the scheduling cache will be refresh for the first time scheduling through a full topologies scheduling.
"
STORM-3111,StormSubmitter localNimbus field is unused,"The localNimbus field in StormSubmitter appears to always be null. It looks like this has been the case since at least 1.0.0. Local mode runs fine without it, so we should remove it."
STORM-3109,Wrong canonical path set to STORM_LOCAL_DIR in storm kill_workers,"When `STORM_LOCAL_DIR` is set as a relative path, the original implementation incorrectly append the `STORM_LOCAL_DIR` after the current working directory upon invocation of `storm kill_workers`. If the current working directory is not the home directory for storm, in this `STORM_LOCAL_DIR` points to the incorrect location, so `storm kill_workers` can't actually kill workers at all.

See pull request for implementation."
STORM-3108,strom-hdfs can support write tuple to orc file format,strom-hdfs can support write tuple to orc file format
STORM-3107,Nimbus confused about leadership after crash,"Nimbus crashed and restarted without shutting down zookeeper due to a deadlock in the timer shutdown code.  This could however also happen for various other issues.  

 

The problem is that once Nimbus restarted, it was really confused about who the leader was:

 
{code:java}
2018-05-24 09:27:21.762 o.a.s.z.LeaderElectorImp main [INFO] Queued up for leader lock.
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping assignments
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping cleanup
2018-05-24 09:27:22.633 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping credential renewal.

2018-05-24 09:27:40.771 o.a.s.d.n.Nimbus pool-37-thread-63 [WARN] Topology submission exception. (topology name='topology-testOverSubscribe-1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-05-24 09:27:40.771 o.a.s.b.BlobStoreUtils Timer-1 [ERROR] Could not download the blob with key: topology-testOverCapacityScheduling-2-1519992333-stormcode.ser
2018-05-24 09:27:40.771 o.a.t.s.TThreadPoolServer pool-37-thread-63 [ERROR] Error occurred during processing of message.
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2961) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more
{code}
The session timeout was set to 20 seconds, but we're exceeding this period, and Nimbus did not recover leadership.  It needed to be restarted manually to recover.

 "
STORM-3104,Delayed worker launch due to accidental transitioning in state machine,"In Slot.java, there is a comparison in 
{code:java}
handleWaitingForBlobUpdate()
{code}
 between dynamic state's current assignment and new assignment, which accidentally route back state machine just transitioned from WAIT_FOR_BLOB_LOCALIZATION back to WAIT_FOR_BLOB_LOCALIZATION, because the current assignment in this case is highly likely to be null and different from new assignment (I'm not sure if it's guaranteed). This causes delay for a worker to start/restart.

The symptom can be reproduced by launching an empty storm server and submit any topology. Here's the log sample (relevant transition starting from 2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [DEBUG]):

{code:sh}
2018-06-13 16:57:10.254 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 6024 -> EMPTY msInState: 6024
2018-06-13 16:57:10.255 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE EMPTY
2018-06-13 16:57:10.257 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from EMPTY to WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:10.257 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE EMPTY msInState: 6027 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2018-06-13 16:57:10.258 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:10.258 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingChangingBlobs are []
2018-06-13 16:57:11.259 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 1003 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 1003
2018-06-13 16:57:11.260 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:11.260 o.a.s.d.s.Slot SLOT_6700 [DEBUG] found changing blobs [BLOB CHANGING LOCAL TOPO BLOB TOPO_CONF test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02), BLOB CHANGING LOCAL TOPO BLOB TOPO_CODE test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)] moving them to pending...
2018-06-13 16:57:12.262 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 2005 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 2005
2018-06-13 16:57:12.263 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.263 o.a.s.d.s.Slot SLOT_6700 [DEBUG] found changing blobs [BLOB CHANGING LOCAL TOPO BLOB TOPO_JAR test-1-1528927024 LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)] moving them to pending...
2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02) == current null ? false
2018-06-13 16:57:12.274 o.a.s.d.s.Slot SLOT_6700 [INFO] There are pending changes, waiting for them to finish before launching container...
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from WAITING_FOR_BLOB_LOCALIZATION to WAITING_FOR_BLOB_UPDATE
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_LOCALIZATION msInState: 2018 -> WAITING_FOR_BLOB_UPDATE msInState: 1
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_UPDATE
2018-06-13 16:57:12.275 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization: null, new: LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02), current: null, pdchanging: LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)
2018-06-13 16:57:12.276 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Assignment Changed from null to LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02)
2018-06-13 16:57:12.278 o.a.s.d.s.Slot SLOT_6700 [DEBUG] Transition from WAITING_FOR_BLOB_UPDATE to WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.278 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE WAITING_FOR_BLOB_UPDATE msInState: 4 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] STATE WAITING_FOR_BLOB_LOCALIZATION
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingChangingBlobs are []
2018-06-13 16:57:12.279 o.a.s.d.s.Slot SLOT_6700 [DEBUG] pendingLocalization LocalAssignment(topology_id:test-1-1528927024, executors:[ExecutorInfo(task_start:10, task_end:10), ExecutorInfo(task_start:16, task_end:16), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:1, task_end:1), ExecutorInfo(task_start:13, task_end:13)], resources:WorkerResources(mem_on_heap:768.0, mem_off_heap:0.0, cpu:60.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=768.0, cpu.pcore.percent=60.0}, shared_resources:{}), owner:zhu02) == current null ? false
2018-06-13 16:57:12.280 o.a.s.d.s.Slot SLOT_6700 [DEBUG] launch container for the first time
2018-06-13 16:57:12.284 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Created Worker ID 4155b2bb-ebd1-431d-907f-d8a4ff1e1da4

{code}

I would like to know if this is actually the desired behavior of the state machine, or I can help fix the bug. The implementation would be to redesign the if statement."
STORM-3103,nimbus stuck shutting down causing leadership issues on startup,"When debugging an Nimbus NPE that caused restarts, I noticed that a forced halt occurred:

 
{code:java}
2018-05-24 09:27:05.569 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Opening socket connection to server openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-05-24 09:27:05.570 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Socket connection established to openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181, initiating session
2018-05-24 09:27:05.571 o.a.z.ClientCnxn main-SendThread(openqe82blue-gw.blue.ygrid.yahoo.com:2181) [INFO] Session establishment complete on server openqe82blue-gw.blue.ygrid.yahoo.com/10.215.77.115:2181, sessionid = 0x1624a86300f7f6b, negotiated timeout = 40000
2018-05-24 09:27:05.571 o.a.c.f.s.ConnectionStateManager main-EventThread [INFO] State change: CONNECTED
2018-05-24 09:27:05.636 o.a.s.d.n.Nimbus main [INFO] Starting nimbus server for storm version '2.0.0.y'
2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]
Caused by: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 2 more
2018-05-24 09:27:06.023 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y]
2018-05-24 09:27:06.032 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master
2018-05-24 09:27:06.032 o.a.s.u.Utils Thread-13 [INFO] Halting after 5 seconds
{code}
At times this would cause leadership confusion:

 
{code:java}
2018-05-24 09:27:21.762 o.a.s.z.LeaderElectorImp main [INFO] Queued up for leader lock.
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping assignments
2018-05-24 09:27:22.604 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping cleanup
2018-05-24 09:27:22.633 o.a.s.d.n.Nimbus timer [INFO] not a leader, skipping credential renewal.

2018-05-24 09:27:40.771 o.a.s.d.n.Nimbus pool-37-thread-63 [WARN] Topology submission exception. (topology name='topology-testOverSubscribe-1')
java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2018-05-24 09:27:40.771 o.a.s.b.BlobStoreUtils Timer-1 [ERROR] Could not download the blob with key: topology-testOverCapacityScheduling-2-1519992333-stormcode.ser
2018-05-24 09:27:40.771 o.a.t.s.TThreadPoolServer pool-37-thread-63 [ERROR] Error occurred during processing of message.
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2961) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3454) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:3438) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.3.jar:0.9.3]
        at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[libthrift-0.9.3.jar:0.9.3]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='openqe82blue-n1.blue.ygrid.yahoo.com', port=50560, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1311) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2807) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        ... 9 more
{code}
We should endeavor to shutdown cleanly.

 

 

 

 "
STORM-3102,Storm Kafka Client performance issues with Kafka Client v1.0.0,"Recently I upgraded our storm topology to use the storm-kafka-client instead of storm-kafka.  After the upgrade in our production environment we saw a significant (2x) reduction in our processing throughput.

We process ~20000 kafka messages per second, on a 10 machine kafka 1.0.0 server cluster.

After some investigation, it looks like the issue only occurs when using kafka clients 0.11 or newer.

In kafka 0.11, the kafka consumer method commited always blocks to make an external call o get the last commited offsets

[https://github.com/apache/kafka/blob/e18335dd953107a61d89451932de33d33c0fd207/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1326-L1351]

In kafka 0.10.2 the kafka consumer only made the blocking remote call if the partition is not assigned to the consumer

[https://github.com/apache/kafka/blob/695596977c7f293513f255e07f5a4b0240a7595c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java#L1274-L1311]

 

The impact of this is to require every tuple to make blocking remote calls before being emitted.  

[https://github.com/apache/storm/blob/2dc3d53a11aa3fea621666690d1e44fa8b621466/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L464-L473]

Removing this check returns performance to expected levels.

Looking through the storm-kafka-client code, it is not clear to me the impact of ignoring the check.  In our case we want at least once processing, but for other processing gurantees the call to kafkaConsumer.commited(tp) is not needed, as the value is only looked at if the processing mode is at least once."
STORM-3101,Fix unexpected metrics registration in StormMetricsRegistry,"Metrics that are registered using StormMetricRegistry all added through static method from the registry class, and attached to a singleton MetricRegistry object per process. Currently most metrics are bound to classes (static), so the issue occurs when metrics from irrelevant components are accidentally registered in class initialization phase. 

For example, a process running supervisor daemon will incorrectly register metrics from nimbus when BasicContainer class is initialized and statically imports ""org.apache.storm.daemon.nimbus.Nimbus.MIN_VERSION_SUPPORT_RPC_HEARTBEAT"", which triggers initialization of Nimbus class and all metrics registration, even though no functionalities of nimbus daemon will be used and no nimbus metrics will be updated. 

This creates many garbage metrics and makes metrics hard to read. Therefore we should filter metrics registration by the type of daemon that the process actually runs.

For implementation please see the pull request."
STORM-3099,"Extend metrics on supervisor, workers, and DRPC","This patch serves to extend metrics on supervisor and worker. Currently the following metrics are being implemented, including but not limited to:

Worker:
# Kill Count by Category - Assignment Change/HB too old/Heap Space
# Time spent in each state
# Time to Actually Kill worker (from identifying need by supervisor and actual change in the state of the worker) - per worker?
# Time to start worker for topology from reading assignment for the first time.
# Worker cleanup Time/Worker cleanup Retries
# Worker Suicide Count - category: internal error or Assignment Change

Supervisor:
# Supervisor restart Count 
# Blobstore (Request to download time) 
    - # Download time individual blob (inside localizer) localizer gettting requst to actually download hdfs request to finish
    - # Download rate individual blob (inside localizer)
    - # Supervisor localizer thread blob download - how long (outside localizer)
# Blobstore Update due to Version change Cnts
# Blobstore Storage by users

DRPC:
#  Avg/Max Time to respond to Http Request

There might be more metrics added later. 

This patch will also refactor code in relevant files. Bugs found during the process will be reported in other issues and handled separately."
STORM-3098,Fix bug in filterChangingBlobsFor() in Slot.java,"The following method is not implemented correctly

{code:java}
 private static DynamicState filterChangingBlobsFor(DynamicState dynamicState, final LocalAssignment assignment) {
        if (!dynamicState.changingBlobs.isEmpty()) {
            return dynamicState;
        }

        HashSet<BlobChanging> savedBlobs = new HashSet<>(dynamicState.changingBlobs.size());
        for (BlobChanging rc : dynamicState.changingBlobs) {
            if (forSameTopology(assignment, rc.assignment)) {
                savedBlobs.add(rc);
            } else {
                rc.latch.countDown();
            }
        }
        return dynamicState.withChangingBlobs(savedBlobs);
    }

{code}

It doesn't modify dynamicState in anyway.
The solution is to remove the negation in the first if statement."
STORM-3097,Remove storm-druid in 2.x and deprecate support for it in 1.x,"Trying again at this.

 

storm-druid depends on tranquility, which is not currently very well supported.  The druid community is moving in the direction of ingesting streaming data directly from Kafka, as such we are going to deprecate storm-druid in 1.x and remove it in the 2.x releases."
STORM-3094,Topology name needs to be validated at storm-client,"*Current Behavior :* Execute topology with invalid topology name is throwing exception after uploading the jar.

*Improvement :* Validating topology name at client side before uploading the jar.

 

 
{code:java}
2018-06-05 16:16:19.461 o.a.s.d.n.Nimbus pool-21-thread-53 [INFO] Uploading file from client to /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar
2018-06-05 16:16:20.596 o.a.s.d.n.Nimbus pool-21-thread-35 [INFO] Finished uploading file from client: /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar
2018-06-05 16:16:20.624 o.a.s.d.n.Nimbus pool-21-thread-29 [INFO] Received topology submission for test-[123] (storm-2.0.0-SNAPSHOT JDK-1.8.0_162) with conf {topology.users=[null], topology.acker.executors=null, storm.zookeeper.superACL=null, topology.workers=3, topology.submitter.principal=, topology.debug=true, topology.name=test-[123], topology.kryo.register={}, storm.id=test-[123]-7-1528195580, topology.kryo.decorators=[], topology.eventlogger.executors=0, topology.submitter.user=mvanam, topology.max.task.parallelism=null}
2018-06-05 16:16:20.624 o.a.s.d.n.Nimbus pool-21-thread-29 [INFO] uploadedJar /manu/Git/storm/storm-dist/binary/final-package/target/apache-storm-2.0.0-SNAPSHOT/storm-local/nimbus/inbox/stormjar-22979659-5176-46fd-9027-ffdde13f595a.jar
2018-06-05 16:16:20.624 o.a.s.b.BlobStore pool-21-thread-29 [ERROR] 'test-[123]-7-1528195580-stormjar.jar' does not appear to be valid. It must match ^[\w \t\._-]+$. And it can't be ""."", "".."", null or empty string.
2018-06-05 16:16:20.625 o.a.s.b.BlobStore pool-21-thread-29 [ERROR] 'test-[123]-7-1528195580-stormconf.ser' does not appear to be valid. It must match ^[\w \t\._-]+$. And it can't be ""."", "".."", null or empty string.
2018-06-05 16:16:20.626 o.a.s.d.n.Nimbus pool-21-thread-29 [WARN] Topology submission exception. (topology name='test-[123]')
java.lang.IllegalArgumentException: test-[123]-7-1528195580-stormconf.ser does not appear to be a valid blob key
 at org.apache.storm.blobstore.BlobStore.validateKey(BlobStore.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]{code}
 

 "
STORM-3093,Cache the storm id to executors mapping on master to avoid repeat computation,"Now nimbus will collect all the topologies's conf/topology-ser/storm-base to compute in a scheduling round, which is a very heavy work. The scheduling will still take to minutes even we now change to RPC heartbeats and assignment distribution.

So i decide to redesign the scheduler, so we can only schedule the topologies that need to: that have dead workers or not enough number workers.

Here i checkout out the code and found that the id->executors mapping is computed every time for every topology, which is really a heavy computation and totally not that necessary, because this mapping is fixed invariable for a topology unless we rebalance or kill it.

So i refactor the code a little here, and this is more powerful after the scheduler is resigned for delta-scheduling[  which is very lightweight even there are thousands of topologies on one cluster.]

For now this is enough for us."
STORM-3092,Metrics Reporter and Shutdown Hook on Supervisor not properly set up at launchDaemon,"The bug was introduced in commit 0dac58b0aa82133df242b3b2ebeb65bfea7d63cc, when launchSupervisorThriftServer method is invoked in the launchDaemon method in Supervisor class. launchSupervisorThriftServer() invokes a blocking call to thrift server under the hood, hence preventing Utils.addShutdownHookWithForceKillIn1Sec and StormMetricsRegistry.startMetricsReporters from correctly called. 

 

The bug can be solved by moving launchSupervisorThriftServer to the end of the code block."
STORM-3091,worker heartbeat directory recreated after killing,"We see occasional instances where we kill the workers, wait for them to die, force delete the worker directory successfully, and the heartbeat directory is then re-created.  

 

It looks like when new local states get created, these will create the directory.  There is probably a race condition between the worker calling LocalState VersionStore mkdirs() and being killed and the supervisor directory cleanup.

 

Containers already guarantee this directory exists, so a fix could be to add an option to allow the LocalStates to create the dir or not.

 "
STORM-3090,The same offset value is used by the same partition number of different topics.,"In the current implementation of `ZkCoordinator` deleted partition managers are used as state holders for newly created partition managers. This behaviour was introduced in the scope of [this|https://issues-test.apache.org/jira/browse/STORM-2296] ticket. However existing lookup is based on only on partition number.
{code:java}
Map<Integer, PartitionManager> deletedManagers = new HashMap<>();
for (Partition id : deletedPartitions) {
 deletedManagers.put(id.partition, _managers.remove(id));
}
for (PartitionManager manager : deletedManagers.values()) {
 if (manager != null) manager.close();
}
LOG.info(taskPrefix(_taskIndex, _totalTasks, _taskId) + "" New partition managers: "" + newPartitions.toString());

for (Partition id : newPartitions) {
 PartitionManager man = new PartitionManager(
 _connections,
 _topologyInstanceId,
 _state,
 _topoConf,
 _spoutConfig,
 id,
 deletedManagers.get(id.partition));
 _managers.put(id, man);
{code}
Which is definitely incorrect as the same task is able to manage multiple partitions with the same number but for different topics. In this case all new partition managers obtain the same offset value from a random deleted partition manager (as `HashMap` is used). And all fetch requests for the new partition managers fail with `TopicOffsetOutOfRangeException`. Some of them are recovered via this logic if assigned offset is smaller than the real one, but other continue to repetitively fail with offset out of range exception preventing fetching messages from Kafka.
{code:java}
if (offset > _emittedToOffset) {
 _lostMessageCount.incrBy(offset - _emittedToOffset);
 _emittedToOffset = offset;
 LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
}
{code}
I assume that state holder lookup should be based both on topic and partition number."
STORM-3089,Document worker hooks on the hooks page,The hooks page http://storm.apache.org/releases/2.0.0-SNAPSHOT/Hooks.html only mentions task hooks. We should also describe worker hooks.
STORM-3087,FluxBuilder.canInvokeWithArgs is too permissive when the method parameter type is a primitive,"One of the clauses in canInvokeWithArgs is too permissive. It returns true if the declared method parameter type is a primitive, regardless of what the type of the actual parameter value is. This causes Flux to attempt to invoke the wrong methods in certain cases, which will trigger an IllegalArgumentException."
STORM-3086,Update Flux documentation to demonstrate static factory methods (STORM-2796),"I think we should add examples for static factory methods to the Flux documentation, the STORM-2796 changes don't seem to be mentioned."
STORM-3085,Expose Ability to Disable CORS for Storm UI Rest API,"STORM-1960 enabled CORS by default in Storm UI Rest API. Some users may need to disable CORS to comply with security restrictions, so it should be possible to disable this."
STORM-3084,2.x NPE on Nimbus startup,"{code:java}
2018-05-24 09:27:05.636 o.a.s.d.n.Nimbus main [INFO] Starting nimbus server for storm version '2.0.0.y' 2018-05-24 09:27:06.012 o.a.s.d.n.Nimbus timer [ERROR] Error while processing event java.lang.RuntimeException: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y] Caused by: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681) ~[storm-server-2.0.0.y.jar:2.0.0.y] ... 2 more 2018-05-24 09:27:06.023 o.a.s.u.Utils timer [ERROR] Halting process: Error while processing event java.lang.RuntimeException: Halting process: Error while processing event at org.apache.storm.utils.Utils.exitProcess(Utils.java:469) ~[storm-client-2.0.0.y.jar:2.0.0.y] at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484) ~[storm-server-2.0.0.y.jar:2.0.0.y] at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252) ~[storm-client-2.0.0.y.jar:2.0.0.y] 2018-05-24 09:27:06.032 o.a.s.d.n.Nimbus Thread-12 [INFO] Shutting down master 2018-05-24 09:27:06.032 o.a.s.u.Utils Thread-13 [INFO] Halting after 5 seconds
{code}"
STORM-3083,Upgrade HikariCP version to 2.4.7,"I've encountered the issue which reason was version conflict regarding disruptor via Storm and Phoenix 5. I've filed STORM-3077 and resolved the root reason, but I also found that HikariCP doesn't give proper error message on this. It just complains about timeout while initializing pool, but the actual error was NoSuchMethodError.

The issue in HikariCP version we use is that it leverages default implementation of ThreadPoolExecutor even while it checks fail-fast in initialization. So fail-fast works anyway, but it says ""timed-out"", not exposing the error.

One thing to note is that HikariCP moved the main version line to support JDK 8 and change artifact name to ""HikariCP-java7"" on JDK7 compatible maintenance version. If we would like to provide same user experience, we should keep using ""HikariCP"" artifact, not ""HikariCP-java7"", since Maven might not recognize they're same.

Hopefully I found there's another HikariCP version (2.4.7) which all of below conditions are met:
 * artifact name is ""HikariCP"", not ""HikariCP-java7""
 * fail-fast works correctly
 * supports Java 7

 

So upgrading HikariCP to 2.4.7 would resolve the issue in simplest and safest way."
STORM-3082,NamedTopicFilter can't handle topics that don't exist yet,"[~aniket.alhat] reported on the mailing list that he got an NPE when trying to start the Trident spout.

{code}
2018-05-22 06:23:02.318 o.a.s.util [ERROR] Async loop died!
java.lang.NullPointerException: null
        at org.apache.storm.kafka.spout.NamedTopicFilter.getFilteredTopicPartitions(NamedTopicFilter.java:57) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.ManualPartitionSubscription.refreshAssignment(ManualPartitionSubscription.java:54) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.ManualPartitionSubscription.subscribe(ManualPartitionSubscription.java:49) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutManager.createAndSubscribeKafkaConsumer(KafkaTridentSpoutManager.java:59) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.<init>(KafkaTridentSpoutEmitter.java:84) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.<init>(KafkaTridentSpoutEmitter.java:100) ~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutOpaque.getEmitter(KafkaTridentSpoutOpaque.java:50) ~[stormjar.jar:?]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.<init>(OpaquePartitionedTridentSpoutExecutor.java:97) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor.getEmitter(OpaquePartitionedTridentSpoutExecutor.java:221) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor.getEmitter(OpaquePartitionedTridentSpoutExecutor.java:39) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.spout.TridentSpoutExecutor.prepare(TridentSpoutExecutor.java:60) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.trident.topology.TridentBoltExecutor.prepare(TridentBoltExecutor.java:245) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.daemon.executor$fn__5043$fn__5056.invoke(executor.clj:803) ~[storm-core-1.2.1.jar:1.2.1]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.2.1.jar:1.2.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]
{code}

It looks to me like the partitionsFor method on the consumer will return null if the specified topic doesn't exist. We didn't account for this in the filter, because the return type of the method is a List, and we assumed it wouldn't be null.

I think it's reasonable that people should be able to subscribe to topics that don't exist yet, and the spout should pick up the new topics eventually.

We should check for null here https://github.com/apache/storm/blob/93ed601425a79759c0189a945c6b46266e5c9ced/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/NamedTopicFilter.java#L55, and maybe log a warning if the returned value is null."
STORM-3081,Storm kafka client not consuming messages properly,"A single thread is pushing some messages to kafka serially and we consume it at the storm's end using storm-kafka-client.
After a few requests, the consumer is not able to consume from the queue blocking the thread which waits for the response from storm's end. 
We added one more consumer with a different consumer group and there the messages are getting read properly.
So we know there is some problem at the storm kafka client consumer's end.
The kafka spout config is written like this - 

KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(kafkaProperties.getProperty(""bootstrap.servers""), stormProperties.getProperty(""TOPIC""))
                .setFirstPollOffsetStrategy(KafkaSpoutConfig.FirstPollOffsetStrategy.LATEST)
                .setRecordTranslator(new MessageDeserializer(), arguments)
                .build();

I can't seem to figure out the issue.
Can someone please help me out?

Thanks."
STORM-3079,improve getMessage support for ThriftExceptions,"I've seen error callstacks similar to this and been confused as to the null message.  The generated thrift code does not support getMessage().  We should try and improve the log messages.

 
2018-05-16 21:15:04.596 o.a.s.d.n.Nimbus timer [INFO] Exception {}
org.apache.storm.generated.KeyNotFoundException: null        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:393) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:670) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2333) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2387) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2674) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$1.run(StormTimer.java:111) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227) ~[storm-client-2.0.0.y.jar:2.0.0.y]"
STORM-3078,HBAuthorizationException appears unused,Looks like this class is safe to remove.
STORM-3077,Upgrade Disruptor version to 3.3.11,"I encountered the version conflict from Disruptor.

Phoenix 5 depends on Disruptor 3.3.6 which added some new methods, and calls the method which is not available from Disruptor 3.3.2 (Storm 1.x), resulting NoSuchMethodError.

Instead of taking a look at all changes from https://github.com/LMAX-Exchange/disruptor/compare/3.3.2...3.3.11#diff-dd0cdb682e550bb371997155c2f2995d , I just simply checked with https://github.com/trohovsky/japi-checker

$ java -jar japi-checker-cli/target/japi-checker-cli-0.2.0-SNAPSHOT.jar disruptor-3.3.2.jar disruptor-3.3.11.jar
{code:java}
WARNING: com/lmax/disruptor/TimeoutBlockingWaitStrategy.java: The non-abstract and non-static method toString() has been added
ERROR: com/lmax/disruptor/SleepingWaitStrategy.java(66): The method waitFor(long, com.lmax.disruptor.Sequence, com.lmax.disruptor.Sequence, com.lmax.disruptor.SequenceBarrier) is not throwing java.lang.InterruptedException anymore
ERROR: com/lmax/disruptor/util/PaddedLong.java: The class com.lmax.disruptor.util.PaddedLong has been removed
WARNING: com/lmax/disruptor/dsl/Disruptor.java: The non-abstract and non-static method setDefaultExceptionHandler(com.lmax.disruptor.ExceptionHandler) has been added
WARNING: com/lmax/disruptor/dsl/Disruptor.java: The non-abstract and non-static method <A extends java.lang.Object, B extends java.lang.Object> publishEvent(com.lmax.disruptor.EventTranslatorTwoArg, java.lang.Object, java.lang.Object) has been added
WARNING: com/lmax/disruptor/dsl/Disruptor.java: The non-abstract and non-static method <A extends java.lang.Object, B extends java.lang.Object, C extends java.lang.Object> publishEvent(com.lmax.disruptor.EventTranslatorThreeArg, java.lang.Object, java.lang.Object, java.lang.Object) has been added
WARNING: com/lmax/disruptor/dsl/Disruptor.java: The non-abstract and non-static method getSequenceValueFor(com.lmax.disruptor.EventHandler) has been added
WARNING: com/lmax/disruptor/dsl/Disruptor.java: The non-abstract and non-static method toString() has been added
WARNING: com/lmax/disruptor/AbstractSequencer.java: The non-abstract and non-static method toString() has been added
ERROR: com/lmax/disruptor/util/MutableLong.java: The class com.lmax.disruptor.util.MutableLong has been removed{code}
Removed files were for testing, and they're moved to test package.

The utility missed new constructor of Disruptor which raises error, but looks like there are no backward incompatibility, so it looks safe to upgrade in point of API compatibility view."
STORM-3076,"In Strom0.9.5,there is no LocalCluster implements ILocalCluster, which results in examples in external can not be compiled","In the  external package,the modules include storm-hbase,storm-hdfs and storm-kafka , can not be compiled successfully, due to the lack of specific implements of ILocalCluster in the storm-core-0.9.5 。"
STORM-3075,NPE starting nimbus,"{code:java}
2018-05-15 14:14:59.873 o.a.c.f.l.ListenerContainer main-EventThread [ERROR] Listener (org.apache.storm.zookeeper.Zookeeper$1@26d820eb) threw an exception
java.lang.NullPointerException: null
        at org.apache.storm.nimbus.LeaderListenerCallback.leaderCallBack(LeaderListenerCallback.java:118) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.zookeeper.Zookeeper$1.isLeader(Zookeeper.java:124) ~[storm-server-2.0.0.y.jar:2.0.0.y]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:665) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$9.apply(LeaderLatch.java:661) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:93) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.shaded.com.google.common.util.concurrent.MoreExecutors$DirectExecutor.execute(MoreExecutors.java:435) ~[curator-client-4.0.1.jar:?]
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:85) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:660) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:539) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch.access$700(LeaderLatch.java:65) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:590) ~[curator-recipes-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:865) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:635) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187) ~[curator-framework-4.0.1.jar:4.0.1]
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) ~[zookeeper-3.4.6.jar:3.4.6-1569965]

{code}"
STORM-3074,Inconsistent null checking in SaslMessageToken,"The SaslMessageToken class will throw an NPE if buffer() is called and the payload is null. While the buffer method checks whether the token is null in a few places before dereferencing, the encodedLength method is called right off the bat, and it doesn't check for null.

The payload is always generated by either https://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslServer.html#evaluateResponse(byte[]) or https://docs.oracle.com/javase/7/docs/api/javax/security/sasl/SaslClient.html#evaluateChallenge(byte[]). The javadoc indicates that if these return null, authentication has succeeded and it is unnecessary to send any more messages to the other party.

I think if null SaslMessageToken payloads are never sent over the wire, we should remove all the null checking in SaslMessageToken and MessageDecoder, and ensure that the SASL handlers check for null before deciding to write tokens."
STORM-3073,In some cases workers may crash because pendingEmits is full,"Saw this while running the https://github.com/apache/storm/blob/master/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/ThroughputVsLatency.java topology.

{code}
2018-05-15 11:35:28.365 o.a.s.u.Utils Thread-16-spout-executor[8, 8] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: Queue full
	at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:168) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:157) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$2.run(Utils.java:349) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
Caused by: java.lang.IllegalStateException: Queue full
	at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_144]
	at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:516) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:140) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:70) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:42) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:360) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:120) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:63) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:295) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.Executor.accept(Executor.java:278) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 7 more
{code}

The executor's pendingEmits queue is full, and the executor then tries to add another tuple. It looks to me like we're preventing the queue from filling by emptying it between calls to nextTuple at https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java#L184.

The TVL topology reemits failed tuples directly from the fail method, which can be triggered by tick tuples. If the pendingEmits queue is already close to full when this happens, we might hit the error above. I think it can also happen if nextTuple emits too many tuples in a call, or if too many metrics ticks happen between pendingEmit flushes, since metrics ticks also trigger emits."
STORM-3072,Frequent test failures in storm-sql-core,"Seeing test failures in storm-sql-core, sometimes regular test failures, other times JVM crashes.
{code}
testExternalUdf(org.apache.storm.sql.TestStormSql)  Time elapsed: 8.177 sec  <<< ERROR!
java.lang.RuntimeException: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='DESKTOP-AGC8TKM.localdomain', port=6627, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2952)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopology(Nimbus.java:2761)
        at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:378)
        at org.apache.storm.sql.StormSqlLocalClusterImpl.runLocal(StormSqlLocalClusterImpl.java:68)
        at org.apache.storm.sql.TestStormSql.testExternalUdf(TestStormSql.java:214)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: not a leader, current leader is NimbusInfo{host='DESKTOP-AGC8TKM.localdomain', port=6627, isLeader=true}
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1302)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2798)
        ... 41 more

testExternalNestedArrayOutOfBoundAccess(org.apache.storm.sql.TestStormSql)  Time elapsed: 0.598 sec  <<< ERROR!
KeyNotFoundException(msg:storm-sql-1-1526308521-stormcode.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:422)
        at org.apache.storm.daemon.nimbus.Nimbus.getBlobReplicationCount(Nimbus.java:1443)
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl(Nimbus.java:2593)
        at org.apache.storm.daemon.nimbus.Nimbus.getClusterInfo(Nimbus.java:4183)
        at org.apache.storm.LocalCluster.getClusterInfo(LocalCluster.java:470)
        at org.apache.storm.sql.StormSqlLocalClusterImpl.runLocal(StormSqlLocalClusterImpl.java:71)
        at org.apache.storm.sql.TestStormSql.testExternalNestedArrayOutOfBoundAccess(TestStormSql.java:170)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
{code}

The cause seems to be similar to https://issues.apache.org/jira/browse/STORM-3065, some of the tests are interfering with each other. Reducing to fork count 1 fixes it."
STORM-3070,"MessageDecoder forgets to rewind buffer position if BackpressureStatus code is received, but the rest of the message is pending","It looks to me like https://github.com/apache/storm/blob/5deba40fca0f88f61d2086cb902318ad9bb044f1/storm-client/src/jvm/org/apache/storm/messaging/netty/MessageDecoder.java#L108 should reset the buffer position before returning, since it has read the message code at this point."
STORM-3068,STORM_JAR_JVM_OPTS are not passed to storm-kafka-monitor  properly,STORM_JAR_JVM_OPTS are not being passed to storm-kafka-monitor properly which can limit the user to pass java configuration such as ssl truststore etc.
STORM-3067,Kafka Spout has no active members in consumer group,"Using Kafka 10.2.1, two different behaviours were observed:

Storm 1.1.0 =>
{code}
/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test.topic.consumer-group --describe
Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).


TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID
test.topic 0 85186604 85186607 3 consumer-2-27a77d1b-e851-47a4-954e-4953ea612b72 /X.X.X.X consumer-2

{code}
 

Storm 1.1.2 =>
{code:java}
/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test.topic.consumer-group --describe
Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers).



Consumer group 'test.topic.consumer-group' has no active members.

TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID
test.topic 0 85202473 85202475 2 -{code}

Despite this behaviour, the topology continues to consume messages and commit offsets to Kafka. It's unclear if this lack of active clients on the consumer group affects the normal functioning of the Spout."
STORM-3065,Very frequent test failures in storm-server,"I'm seeing the following intermittent test failures in storm-server when I run locally

{code}
2018-05-09 16:32:23.377 [main] ERROR org.apache.storm.blobstore.KeySequenceNumber - Exception {}
java.lang.NullPointerException: null
	at java.lang.String.contains(String.java:2133) ~[?:1.8.0_152]
	at org.apache.storm.blobstore.KeySequenceNumber.checkIfStateContainsCurrentNimbusHost(KeySequenceNumber.java:206) ~[classes/:?]
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:159) [classes/:?]
	at org.apache.storm.daemon.nimbus.Nimbus.getVersionForKey(Nimbus.java:655) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:223) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStore$MockitoMock$1067706995.createBlob$accessor$Ub7aO1Cr(Unknown Source) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStore$MockitoMock$1067706995$auxiliary$x0GVZISq.call(Unknown Source) [classes/:?]
	at org.mockito.internal.invocation.RealMethod$FromCallable.invoke(RealMethod.java:48) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.InterceptedInvocation.callRealMethod(InterceptedInvocation.java:129) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:43) [mockito-core-2.10.0.jar:?]
	at org.mockito.Answers.answer(Answers.java:100) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:97) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:65) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:51) [mockito-core-2.10.0.jar:?]
	at org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:135) [mockito-core-2.10.0.jar:?]
	at org.apache.storm.blobstore.LocalFsBlobStore$MockitoMock$1067706995.createBlob(Unknown Source) [classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasic(LocalFsBlobStoreTest.java:325) [test-classes/:?]
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasicLocalFs(LocalFsBlobStoreTest.java:114) [test-classes/:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_152]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_152]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_152]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_152]
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) [junit-4.11.jar:?]
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.11.jar:?]
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) [junit-4.11.jar:?]
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.11.jar:?]
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.11.jar:?]
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) [junit-4.11.jar:?]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) [junit-4.11.jar:?]
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) [junit-4.11.jar:?]
	at org.junit.runners.Suite.runChild(Suite.java:127) [junit-4.11.jar:?]
	at org.junit.runners.Suite.runChild(Suite.java:26) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) [junit-4.11.jar:?]
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309) [junit-4.11.jar:?]
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161) [surefire-junit47-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290) [surefire-booter-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242) [surefire-booter-2.19.1.jar:2.19.1]
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121) [surefire-booter-2.19.1.jar:2.19.1]
{code}
and 
{code}
java.lang.NullPointerException
	at java.lang.String.startsWith(String.java:1405)
	at java.lang.String.startsWith(String.java:1434)
	at org.apache.storm.zookeeper.ClientZookeeper.deleteNodeBlobstore(ClientZookeeper.java:86)
	at org.apache.storm.cluster.ZKStateStorage.delete_node_blobstore(ZKStateStorage.java:93)
	at org.apache.storm.cluster.StormClusterStateImpl.setupBlob(StormClusterStateImpl.java:704)
	at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:223)
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasic(LocalFsBlobStoreTest.java:325)
	at org.apache.storm.blobstore.LocalFsBlobStoreTest.testBasicLocalFs(LocalFsBlobStoreTest.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
{code}

This seems to be because the LocalFsBlobStoreTest is mocking NimbusInfo, and the properties on that object aren't stubbed. Occasionally the test will try to e.g. do a string contains with the null value from the stub, which throws an NPE. Since NimbusInfo is just a POJO, I don't see why we would need to mock it, we could just supply dummy values.

I'm also seeing test failures on nearly every run locally, seemingly due to interference between the tests. Example:
{code}
testLocalTransport(org.apache.storm.MessagingTest)  Time elapsed: 8.197 sec  <<< ERROR!
java.lang.RuntimeException: java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2952)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopology(Nimbus.java:2761)
        at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:378)
        at org.apache.storm.LocalCluster.submitTopology(LocalCluster.java:121)
        at org.apache.storm.Testing.completeTopology(Testing.java:424)
        at org.apache.storm.MessagingTest.testLocalTransport(MessagingTest.java:57)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: No nimbus leader participant host found, have you started your nimbus hosts?
        at org.apache.storm.zookeeper.Zookeeper.toNimbusInfo(Zookeeper.java:109)
        at org.apache.storm.zookeeper.LeaderElectorImp.getLeader(LeaderElectorImp.java:108)
        at org.apache.storm.daemon.nimbus.Nimbus.assertIsLeader(Nimbus.java:1301)
        at org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts(Nimbus.java:2798)
        ... 39 more
{code}
as well as occasional connection losses to Zookeeper. Reducing storm-server to run one test at a time, rather than with a forkCount of 1 per core eliminates these issues."
STORM-3064,PartitionedTridentSpoutExecutor should use getPartitionsForTask,"https://issues.apache.org/jira/browse/STORM-2407 added a method to the IOpaquePartitionedTridentSpout.Emitter interface called getPartitionsForTask, which is now used to delegate partitioning between tasks (previously, the partitioning was hard coded to round robin).

If we want to be able to delegate partitioning, I don't see a reason not to make the same change on the IPartitionedTridentSpout.Emitter interface, where partitioning is still hard coded to use round robin. E.g. compare https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/storm-client/src/jvm/org/apache/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L131 to https://github.com/apache/storm/blob/4137328b75c06771f84414c3c2113e2d1c757c08/storm-client/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L131"
STORM-3063,Minor POM issues,"storm-core has a duplicate Zookeeper declaration.

The parent pom uses the prerequisites tag to set minimum Maven version. Maven 3.5.3 gives the following warning:
{quote}
The project org.apache.storm:storm:pom:2.0.0-SNAPSHOT uses prerequisites which is only intended for maven-plugin projects but not for non maven-plugin projects. For such purposes you should use the maven-enforcer-plugin. See https://maven.apache.org/enforcer/enforcer-rules/requireMavenVersion.html
{quote}"
STORM-3062,Document JMXStormReporter configuration,"discussion on the PR for STORM-2988(https://issues.apache.org/jira/browse/STORM-2988) to document JMXStormReporter.

 "
STORM-3061,Upgrade Dependencies before 2.x release,Storm has a lot of dependencies.  It would be great to upgrade many of them to newer versions ahead of a 2.x release.
STORM-3059,KafkaSpout throws NPE when hitting a null tuple if the processing guarantee is not AT_LEAST_ONCE,"Introduced with STORM-2994

{quote}
java.lang.NullPointerException: null
        at org.apache.storm.kafka.spout.KafkaSpout.emitOrRetryTuple(KafkaSpout.java:507)
~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.KafkaSpout.emitIfWaitingNotEmitted(KafkaSpout.java:440)
~[stormjar.jar:?]
        at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:308)
~[stormjar.jar:?]
{quote}"
STORM-3058,set TOPOLOGY_MAX_SPOUT_PENDING to one，but can get more than one tuple in bolt,"Spout in the nextTuple may pull more than one kafka message, will remain in the _waitingToEmit, and then launch one by one, even if the TOPOLOGY_MAX_SPOUT_PENDING set to 1, when the first tuple is defeated, the bolt will still receive follow-up Message until _waitingToEmit is completely transmitted.

This and I think TOPOLOGY_MAX_SPOUT_PENDING is not the same meaning, I hope that the message can be strictly processed one by one, my topology worker number, spout and bolt parallelism are set to 1"
STORM-3057,ERROR An exception occurred processing Appender syslog,"*ERROR An exception occurred processing Appender syslog*

2018-05-04 14:44:43,721 ERROR An exception occurred processing Appender syslog org.apache.logging.log4j.core.appender.AppenderLoggingException: Error flushing stream UDP:localhost:514
 at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:159)
 at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:112)
 at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:99)
 at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:430)
 at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:409)
 at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:367)
 at org.apache.logging.log4j.core.Logger.logMessage(Logger.java:112)
 at org.apache.logging.slf4j.Log4jLogger.log(Log4jLogger.java:375)
 at org.apache.log4j.Category.differentiatedLog(Category.java:186)
 at org.apache.log4j.Category.error(Category.java:272)
 at com.surfilter.analyse.es.common.service.impl.EsIndexServiceImpl.insertEs(EsIndexServiceImpl.java:141)
 at com.surfilter.analyse.es.common.service.impl.EsIndexServiceImpl.bulkIndex(EsIndexServiceImpl.java:97)
 at com.surfilter.analyse.es.service.RecordESService.saveDnsObjectToES(RecordESService.java:122)
 at com.surfilter.analyse.dns.dao.illegalWeb.IllegalWebESDAO.batchSave(IllegalWebESDAO.java:128)
 at com.surfilter.analyse.dns.service.illegalWeb.IllegalWebService.dowork(IllegalWebService.java:71)
 at com.surfilter.analyse.dns.storm.bolt.DataStoreHandler.storeData(DataStoreHandler.java:28)
 at com.surfilter.analyse.dns.storm.bolt.DnsCheckBolt.execute(DnsCheckBolt.java:168)
 at backtype.storm.daemon.executor$fn__5694$tuple_action_fn__5696.invoke(executor.clj:690)
 at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:435)
 at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58)
 at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132)
 at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106)
 at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)
 at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:818)
 at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479)
 at clojure.lang.AFn.run(AFn.java:22)
 at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.IOException: 消息过长
 at java.net.PlainDatagramSocketImpl.send(Native Method)
 at java.net.DatagramSocket.send(DatagramSocket.java:676)
 at org.apache.logging.log4j.core.net.DatagramOutputStream.flush(DatagramOutputStream.java:103)
 at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:156)
 ... 26 more

2018-05-04 14:56:27,877 ERROR Unable to write to stream UDP:localhost:514 for appender syslog
2018-05-04 15:27:45,512 ERROR Logger contains an invalid element or attribute ""appender""
2018-05-04 16:18:08,456 ERROR Logger contains an invalid element or attribute ""appender""

 

*but storm  worker  often auto shutdown 。*"
STORM-3056,Add a test for quickly rebinding to a port,We need to add a test for the bug fix of STORM-3039. We try to rebind to port 6700 a few times and expect it to be usable quickly.
STORM-3055,never refresh connection,"in our enviroment some worker's connection to other worker being closed and never reconnect,

the log show's that 

2018-05-02 10:28:49.302 o.a.s.m.n.Client Thread-90-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.31.1:6800 is being closed

......
2018-05-02 11:00:29.540 o.a.s.m.n.Client Thread-90-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.31.1:6800 is being closed

the log shows that it never can reconnect again. i can only fix it after restart the topo, "
STORM-3053,blobstores deleted before topologies can be submitted,"We have integration tests failing that create a blobstore and then fail to submit a topology.  Digging into the logs, it looks like there is a nimbus thread that runs every 10 seconds and deletes blobstores if there is no associated topology.  We're hitting a race between the two calls periodically.  

 

 

A possible fix could be to save the timestamp when the blobstore AtomicOutputStream closes and give some grace period before cleaning up based on this timestamp.

 

 

 "
STORM-3052,Let blobs un archive,"I noticed that in 1.x trying to unzip/untar etc an archive does not work because '.' is not allowed in the names of blobs.  To work around this we will match either '_' or '.' in the name of a blob when deciding how to decompress it.

 

At the same time I cleaned up the unzip/unjar code to make it more common, and added in support to the java untar to support symlinks and permissions.  At some point in the future we could then remove the shell version of untar."
STORM-3047,Ensure Trident emitter refreshPartitions is only called with partitions assigned to the emitter,"This is a backport of the changes made to OpaquePartitionedTridentSpoutExecutor in https://github.com/apache/storm/pull/2300/files.

The description of the issue is copied here for convenience:

The changes in https://github.com/apache/storm/pull/2009 released in 1.1.0 made some changes to the OpaquePartitionedTridentSpoutExecutor that likely broke IOpaquePartitionedTridentSpout implementations other than storm-kafka-client. The changed code used to request sorted partitions from the spout via getOrderedPartitions, do a round-robin partitioning, and assign partitions via refreshPartitions https://github.com/apache/storm/blob/v1.0.4/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L100. The new code just passes the output of getOrderedPartitions into refreshPartitions https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/trident/spout/OpaquePartitionedTridentSpoutExecutor.java#L120. It looks to me like refreshPartitions is passed the list of all partitions assigned to any spout task, rather than just the partitions assigned to the current task.

The proposed fix will use getOrderedPartitions to get the sorted partitions list, pass the list into getPartitionsForTask, and pass the resulting list of assigned partitions back into refreshPartitions.
"
STORM-3046,Getting a NPE leading worker to die when starting a topology.,"I am using storm-core and storm-kafka-client version 1.2.1 and kafka clients version 1.1.0.

We have an external kafka from where we get the messages.
 Whenever I try to run the topology, I get a NPE, which leads to the worker getting died.
If I set poll strategy to earliest and the topic already contains some messages, it works fine.
 I have used a custom record translator which is working fine.

 Can someone please help me fix the issue?

Thanks.

 

Error - 

10665 [Thread-58-spout-handle-rule-local-kafka-spout-executor[26 26]] ERROR o.a.s.util - Async loop died!
 java.lang.RuntimeException: java.lang.NullPointerException
 at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:74) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.executor$fn__5043$fn__5056$fn__5109.invoke(executor.clj:861) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
 Caused by: java.lang.NullPointerException
 at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.seek(KafkaTridentSpoutEmitter.java:193) ~[storm-kafka-client-1.2.1.jar:1.2.1]
 at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.emitPartitionBatch(KafkaTridentSpoutEmitter.java:127) ~[storm-kafka-client-1.2.1.jar:1.2.1]
 at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.emitPartitionBatch(KafkaTridentSpoutEmitter.java:51) ~[storm-kafka-client-1.2.1.jar:1.2.1]
 at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.emitBatch(OpaquePartitionedTridentSpoutExecutor.java:141) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.trident.spout.TridentSpoutExecutor.execute(TridentSpoutExecutor.java:82) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:383) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.executor$fn__5043$tuple_action_fn__5045.invoke(executor.clj:739) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:468) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]
 ... 6 more

 

 

Topology class - 

 

 

 

 

import org.apache.storm.Config;
import org.apache.storm.LocalCluster;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.*;
import org.apache.storm.kafka.spout.KafkaSpoutConfig;
import org.apache.storm.kafka.spout.trident.KafkaTridentSpoutOpaque;
import org.apache.storm.trident.Stream;
import org.apache.storm.trident.TridentState;
import org.apache.storm.trident.TridentTopology;
import org.apache.storm.tuple.Fields;

import java.util.Properties;

 


public class TestTopology {

 

private static StormTopology buildTopology(Properties stormProperties) {
 

Properties kafkaProperties = getProperties(""/kafka.properties"");
 TridentTopology topology = new TridentTopology();



Fields stageArguments = new Fields(""test"", ""issue"");




KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(kafkaProperties.getProperty(""bootstrap.servers""), ""test"")
 .setFirstPollOffsetStrategy(KafkaSpoutConfig.FirstPollOffsetStrategy.LATEST)
 .setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE)
 .setRecordTranslator(new RecordTranslator(), stageArguments)
 .build();






KafkaTridentSpoutOpaque kafkaTridentSpoutOpaque = new KafkaTridentSpoutOpaque(kafkaSpoutConfig);


Grouping partitionGroup = getPartitionGroup(""test"");

log.info(""Creating Opaque-Trident-Kafka-Spout"");



final Stream kafkaSpout = topology.newStream(stormProperties.getProperty(""SPOUT_NAME""), kafkaTridentSpoutOpaque).name(""kafkaSpout"").parallelismHint(1);
 
TridentState testUpdate = kafkaSpout.partition(partitionGroup).name(""testUpdate"").partitionPersist(new MainMemoryStateFactory(), stageArguments, new 
MainMemoryStateUpdater(), stageArguments).parallelismHint(1);
 

Stream viewUpdate = ruleUpdate.newValuesStream().name(""viewUpdate"").partition(partitionGroup).each(stageArguments, new UpdateView(), new Fields()).parallelismHint(2);

return topology.build();
 }

public static void main(String[] args) {
 Config conf = new Config();
 log.info(""Topology config: "" + conf);
 Properties properties = getProperties(""/storm-cluster.properties"");

conf.setMessageTimeoutSecs(600);

log.info(""Building Topology"");
 StormTopology topology = buildTopology(properties);
 log.info(topology.toString());

log.info(""Submitting handle-rule Topology"");
 try {
 LocalCluster cluster = new LocalCluster();
 cluster.submitTopology(""handle-rule"", conf, topology);
 } catch (Exception e) {
 e.printStackTrace();
 }
 }


}"
STORM-3045,Microsoft Azure EventHubs: Storm Spout and Bolt improvements,
STORM-3043,NullPointerException thrown in SimpleRecordTranslator.apply(),"When using a SimpleRecordTranslator with a user-defined translator Func, a NullPointerException will be thrown if Func.apply() returns null. A null List object is a valid return value from apply() if the ConsumerRecord is invalid.

SimpleRecordTranslator does not check for a null result before attempting to call the addAll method of the List."
STORM-3041,worker-launcher setup is confusing,"I had a misconfigured worker-launcher and multiple things confused me:

 

1) I wasn't immediately finding the config file location

2) config failures result in info messages in the logs.  My search for ERROR failed to find anything.

3) worker-launcher messages indicate node managers.

 

 "
STORM-3040,RAS scheduling performance improvements,Even after fixing a lot of the loops in RAS scheduling there is still more we can do to make the performance even better.  This is especially true for the generic resource aware strategy.
STORM-3039,Ports of killed topologies remain in TIME_WAIT state preventing to start new topology,"When topology is killed the slot ports (supervisor.slots.ports) remain in TIME_WAIT state. In that case new topology can not be started, because workers throw the following error:
{code:java}
2018-04-20 08:37:08.742 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker
org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6700
 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.worker$worker_data$fn__10395.invoke(worker.clj:285) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:282) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.daemon.worker$fn__10693$exec_fn__3301__auto__$reify__10695.run(worker.clj:626) ~[storm-core-1.2.1.jar:1.2.1]
 at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_161]
 at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_161]
 at org.apache.storm.daemon.worker$fn__10693$exec_fn__3301__auto____10694.invoke(worker.clj:624) ~[storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
 at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
 at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
 at org.apache.storm.daemon.worker$fn__10693$mk_worker__10784.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
 at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
 at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
 at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
 at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
Caused by: java.net.BindException: Address already in use
 at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_161]
 at sun.nio.ch.Net.bind(Net.java:433) ~[?:1.8.0_161]
 at sun.nio.ch.Net.bind(Net.java:425) ~[?:1.8.0_161]
 at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:1.8.0_161]
 at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.8.0_161]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.2.1.jar:1.2.1]
 at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.2.1.jar:1.2.1]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161]
 at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_161]

{code}
 

This exception occurs often when topologies stopped and started automatically."
STORM-3038,'not a leader' exception submitting topologies to LocalCluster,"In our local tests running on our openstack build servers, we get intermittent failures due to the following exception being thrown when submitting topologies to LocalCluster:

{code}275354 [main] WARN  o.a.s.d.nimbus - Topology submission exception. (topology name='AnalyseUpdate') #error {
 :cause not a leader, current leader is NimbusInfo{host='<hostname>', port=6627, isLeader=true}
 :via
 [{:type java.lang.RuntimeException
   :message not a leader, current leader is NimbusInfo{host='<hostname>', port=6627, isLeader=true}
   :at [org.apache.storm.daemon.nimbus$is_leader doInvoke nimbus.clj 150]}]
 :trace
 [[org.apache.storm.daemon.nimbus$is_leader doInvoke nimbus.clj 150]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10799 submitTopologyWithOpts nimbus.clj 1681]
  [org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10799 submitTopology nimbus.clj 1774]
  [sun.reflect.GeneratedMethodAccessor299 invoke nil -1]
  [sun.reflect.DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl.java 43]
  [java.lang.reflect.Method invoke Method.java 498]
  [clojure.lang.Reflector invokeMatchingMethod Reflector.java 93]
  [clojure.lang.Reflector invokeInstanceMethod Reflector.java 28]
  [org.apache.storm.testing$submit_local_topology invoke testing.clj 310]
  [org.apache.storm.LocalCluster$_submitTopology invoke LocalCluster.clj 49]
  [org.apache.storm.LocalCluster submitTopology nil -1]{code}

(note that {{isLeader=true}} in the exception message)

This is despite a retry mechanism we've implemented; this seems to be a continual failure, once its hit. This tends to be surrounded by lots of zookeeper connection lost/reconnection log message."
STORM-3036,Add isRemoteBlobExists RPC interface for deciding if remote blob exists,"As https://github.com/apache/storm/pull/2618 has described, now we try catch a KeyNotFoundException to decide remote blob does not exist.

It is confusing because user had removed it/them on their own initiative but we still got a KeyNotFoundException at both server and client point.

So i add a new api for this case."
STORM-3033,NullPointerException in consumeBatchToCursor method,"Error in consumeBatchToCursor method after few days or running. Error occurred on bolt's ack method call.

Linked Issue: https://issues.apache.org/jira/browse/STORM-770

 
{code:java}
java.lang.RuntimeException: java.lang.NullPointerException
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:74) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__5043$fn__5056$fn__5109.invoke(executor.clj:861) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:785) [?:?]
Caused by: java.lang.NullPointerException
at org.apache.storm.stats$emitted_tuple_BANG_.invoke(stats.clj:123) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$mk_tasks_fn$fn__4656.invoke(task.clj:166) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$send_unanchored.invoke(task.clj:119) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__5043$fn$reify__5068.ack(executor.clj:816) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.task.OutputCollector.ack(OutputCollector.java:213) ~[storm-core-1.2.1.jar:1.2.1]
at bolt execute method
at org.apache.storm.daemon.executor$fn__5043$tuple_action_fn__5045.invoke(executor.clj:739) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:468) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]
... 6 more
{code}
 "
STORM-3032,NullPointerException in KafkaSpout emitOrRetryTuple,"{code:java}
java.lang.NullPointerException: null
at clojure.lang.Numbers.ops(Numbers.java:1013) ~[clojure-1.7.0.jar:?]
at clojure.lang.Numbers.multiply(Numbers.java:148) ~[clojure-1.7.0.jar:?]
at org.apache.storm.stats$transferred_tuples_BANG_.invoke(stats.clj:131) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$mk_tasks_fn$fn__4656.invoke(task.clj:167) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.task$send_unanchored.invoke(task.clj:119) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$send_spout_msg__4996.invoke(executor.clj:594) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn$reify__5006.emit(executor.clj:618) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:50) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.kafka.spout.KafkaSpout.emitOrRetryTuple(KafkaSpout.java:496) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.emitIfWaitingNotEmitted(KafkaSpout.java:440) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:308) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:654) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:785) [?:?]
{code}
Error in KafkaSpout emitOrRetry method"
STORM-3031,size of particules in StormUI visualization,"In StormUI visualization, the size of particules is driven by the length of the component name in my opinion it's counter-intuitive.
I think that a fixed size or may be a configurable size would be a better choice

May be a good choice would be that the size be linked with some kind of performance or latency.
Currently the color is the capacity, we could set the size as a reflect of the component latency (with an lower and upper limit)?"
STORM-3028,HdfsSpout does not handle empty files in case of ack enabled,"When ackers are present and an empty file is read, HdfsSpout waits for acknowledgement to close the file. As nothing was emitted no ack arrives so HdfsSpout stays in ""fileReadCompletely"" state forever."
STORM-3026,Upgrade ZK instance for security,"It would be great to have the ability to move an existing cluster with it's ZK from insecure to secure without wiping everything clean.  This does not allow for a rolling upgrade because the running topologies will not have the credentials that they need, but you don't need to do the manual step of deleting the root ZK node."
STORM-3024,Allow scheduling for RAS to happen in the background,"We have run into some issues recently where occasionally a strategy on a very large cluster will take an extra long amount of time finish scheduling.  This slowness cascades into other issues, like topologies not being able to be killed because the timer thread is still in use trying to run scheduling.

The plan is to make scheduling happen in a thread pool.  The main thread will wait for up to a configurable amount of time for the topology to be scheduled, but if it does not complete in that time it will be left to keep running in the background thread in hopes that later on it will be scheduled.

If for some reason the state of the cluster changes while scheduling is happening in the background we will cancel the scheduling, as any scheduling it produced may not be able to fit on the cluster.  The next time the scheduler runs it will restart the scheduling and hopefully allow the cluster to reach a steady state even if it takes a while, but without blocking kills and other critical operations from happening.

Note that we are also working on optimizing scheduling as well so that these issues don't happen in the first place."
STORM-3020,Fix race condition is async localizer,"I think this impacts all of the code that uses asynclocalizer, but I need to check to be sure.

As part of a review of a different pull request against AsyncLocalizer I noticed that requestDownloadTopologyBlobs is synchronized, but everything it does is async, but there is a race in one of the async pieces where we read from a map, and then try to update the map later, all outside of a lock."
STORM-3019,StormReporter doesn't have information on where it's running,"Metrics2 StormReporter implementations don't have a lot of information on where they're running. In particular, they are missing:
 * Whether they are running for nimbus, supervisor, or worker, and what the worker is.
 * The full deployed config - it's just provided with the basic topology configuration, not the full effective configuration as specified at topology deployment
 * A TopologyContext object"
STORM-3018,Fix integration test DemoTest#testExclamationTopology fail problem,"For https://github.com/apache/storm/pull/2433 i changed the task metrics reporting interval default to 60 seconds, which is same with the  DemoTest#testExclamationTopology check time.

So fix it here."
STORM-3016,Nimbus gets down when job has large amount of parallelism components,"When a job having large amount of parallelism components( total parallelism rises to 5000 for example) been submmited to storm cluster, Nimubs might get crashed, the work flow is as below:

1)  Nimbus computting assignment

2) Nimbus sending assignment to zk

{color:#ff0000}3) When assignment mapping info string is too long due to  total parallelism of job being too large, sending this info to zk will fail (zNode datalength set default is 1M ){color}

{color:#333333}4) Nimbus keeps trying sending this assignment info, after some times, it gives up and crashed, with that happend, the stablity of the cluster will be greatly impacted{color}"
STORM-3015,storm-kafka-client-examples should not depend on the Kafka server jar,"The kafka_2.10 dependency is unnecessary, and adds a little more hassle than necessary to switching the Kafka client version. E.g. To switch to Kafka 1.1.0, you have to also replace kafka_2.10 with kafka_2.11"
STORM-3014,TickTupleTest.testTickTupleWorksWithSystemBolt fails intermittently in Travis CI,"[https://travis-ci.org/apache/storm/jobs/359417643]

 
{code:java}
classname: org.apache.storm.TickTupleTest / testname: testTickTupleWorksWithSystemBolt
java.lang.AssertionError: took over 110000 ms of simulated time to get a message back...
	at org.apache.storm.TickTupleTest.testTickTupleWorksWithSystemBolt(TickTupleTest.java:59)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeLazy(JUnitCoreWrapper.java:119)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:87)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121){code}
 

Note that it is not failing consistently. Here's the build which the test passed.

[https://travis-ci.org/HeartSaVioR/storm/builds/359567639]

 "
STORM-3013,Deactivated topology restarts if data flows into Kafka,"Hi, I have deactivated the storm topology & then if I produce any records into Kafka, Storm throws an exception. Exception follows,
{code:java}
2018-03-28 09:50:23.804 o.a.s.d.executor Thread-83-kafkaLogs-executor[130 130] [INFO] Deactivating spout kafkaLogs:(130)
2018-03-28 09:51:01.289 o.a.s.util Thread-17-kafkaLogs-executor[139 139] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.IllegalStateException: This consumer has already been closed.
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:70) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$fn__4990$fn__5021.invoke(executor.clj:634) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.2.1.jar:1.2.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
Caused by: java.lang.IllegalStateException: This consumer has already been closed.
at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:1787) ~[stormjar.jar:?]
at org.apache.kafka.clients.consumer.KafkaConsumer.beginningOffsets(KafkaConsumer.java:1622) ~[stormjar.jar:?]
at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[stormjar.jar:?]
at org.apache.storm.daemon.executor$metrics_tick$fn__4899.invoke(executor.clj:345) ~[storm-core-1.2.1.jar:1.2.1]
at clojure.core$map$fn__4553.invoke(core.clj:2622) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
at clojure.core$filter$fn__4580.invoke(core.clj:2679) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.7.0.jar:?]
at clojure.lang.RT.next(RT.java:674) ~[clojure-1.7.0.jar:?]
at clojure.core$next__4112.invoke(core.clj:64) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6523.invoke(protocols.clj:170) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6478$G__6473__6487.invoke(protocols.clj:19) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?]
at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?]
at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]
at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]
at org.apache.storm.daemon.executor$metrics_tick.invoke(executor.clj:349) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$fn__4975$tuple_action_fn__4981.invoke(executor.clj:522) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__4964.invoke(executor.clj:471) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.disruptor$clojure_handler$reify__4475.onEvent(disruptor.clj:41) ~[storm-core-1.2.1.jar:1.2.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:509) ~[storm-core-1.2.1.jar:1.2.1]
... 7 more
{code}"
STORM-3011,Use default bin path in flight.bash if $JAVA_HOME is undefined,"I had to modify the _flight.bash_ script as $JAVA_HOME is undefined, this yields
 
_/usr/share/apache-storm-1.2.1/bin/flight.bash: line 62: /bin/jstack: No such file or directory_
 
however, the  files can be found in the default path _/usr/bin_
 
I will submit a PR so that the script checks that $JDKPATH is not empty and exist before using it for BINPATH. I can't see any downsides in doing so."
STORM-3010,.NET Core adapter for Storm multi-lang protocol,"I have been using Storm with all topology components implemented in C# (.NET Core 2.0). I used [https://www.nuget.org/packages/Storm.Net.Adapter/] adapter but it has some problems and gaps (e.g. Storm context and configuration are not available, performance issue while reading message from standard input etc.). So I have implemented new adapter and would love to contribute it to Storm code base. The adapter is already implemented and I would like push it and create PR.

 

I tried to push the changes but it is insufficient permission issue. So what is the right way to do it?"
STORM-3009,Storm-webapp has multiple SLF4j bindings on the classpath,"Storm-webapp has both logback and log4j2 bindings for SLF4j on the classpath. The logback binding comes from our Dropwizard dependency. As of Dropwizard 1.2.0 (https://github.com/dropwizard/dropwizard/pull/1900), Dropwizard isn't tightly coupled to logback, so we can exclude that dependency."
STORM-3008,Add Windows CI coverage,"I'd like to see us add test runs on Windows to our CI. Tests occasionally break on Windows, and we don't catch it during PR review because Travis only tests on Linux. 

Since Travis doesn't offer Windows support, we could look at using https://www.appveyor.com/, which is also free for open source projects. It's already used by a few other Apache projects, like Thrift and Spark."
STORM-3006,Distributed RPC documentation needs an update,"Currently in: http://storm.apache.org/releases/1.2.1/Distributed-RPC.html

examples of DRPCClient initialization are outdated as you need to provide config parameter:

Config conf = new Config();
        conf.setDebug(false);
        conf.put(""storm.thrift.transport"", ""org.apache.storm.security.auth.plain.PlainSaslTransportPlugin"");
        conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 3);
        conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL, 10);
        conf.put(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING, 20);
        this.drpcClient = new DRPCClient(conf, ""10.0.9.10"", 3772);

What it more it would be useful to state that org.apache.storm.security.auth.plain.PlainSaslTransportPlugin can be used in DRPC as SimpleTransportPlugin is deprecated.

So also storm.yaml example can be extended:
drpc.servers:
  - ""drpc1.foo.com""
  - ""drpc2.foo.com""

*storm.thrift.transport: ""org.apache.storm.security.auth.plain.PlainSaslTransportPlugin""*

 "
STORM-3005,[DRPC] LinearDRPCTopologyBuilder shouldn't be deprecated ,"Apache Storm provides DRPC functionality. However LinearDRPCTopologyBuilder is deprecated due to: ""Trident subsumes the functionality provided by this class, so it's deprecated"". I think it shouldn't be so, because you may still want to use DRPC without Trident.

LinearDRPCTopologyBuilder is also mentioned as a part of example in current documentation: [http://storm.apache.org/releases/1.2.1/Distributed-RPC.html]"
STORM-3002,Checkstyle plugin failure,"I followed the directions on this page: [https://github.com/apache/storm/tree/master/examples/storm-starter#build-and-install-storm-jars-locally] working on ref 8c8c0c31c70f3e73b85083d84f5ab9475c6f1e2b

 

When I run:
{code:java}
mvn clean install -DskipTests=true{code}
 

I get the following error:
{code:java}
...[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 01:49 min
[INFO] Finished at: 2018-03-20T21:54:28-05:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project storm-starter: Execution validate of goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check failed: Plugin org.apache.maven.plugins:maven-checkstyle-plugin:2.17 or one of its dependencies could not be resolved: Could not find artifact org.apache.storm:storm-checkstyle:jar:2.0.0-SNAPSHOT in apache.snapshots (http://repository.apache.org/snapshots) -> [Help 1]
[ERROR]{code}"
STORM-2998,Wrong className in LoggerFactory.getLogger method,"If we use LoggerFactory.getLogger method in a class,the most accurate approach is use this class as a parameter."
STORM-2997,Add logviewer ssl module in SECURITY.md ,Add logviewer ssl module in SECURITY.md like UI and DRPC
STORM-2996,Inaccurate description about offset and kafkaspout will make users confused,"As you can see in the picture,we don't know which partition the offset belongs in first line.And consumer.host is the same as host in Partition in second line,so I think we can remove it."
STORM-2995,Topology runtime exception - Error on initialization,"{code:java}
2018-03-14 13:28:41.399 o.a.s.d.worker main [INFO] Reading Assignments. 2018-03-14 13:28:41.511 o.a.s.m.TransportFactory main [INFO] Storm peer transport plugin:org.apache.storm.messaging.netty.Context 2018-03-14 13:28:41.935 o.a.s.m.n.Server main [INFO] Create Netty Server Netty-server-localhost-6712, buffer_size: 5242880, maxWorkers: 1 2018-03-14 13:28:41.980 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6712 at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data$fn__5244.invoke(worker.clj:272) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.util$assoc_apply_self.invoke(util.clj:931) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:269) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544.run(worker.clj:613) ~[storm-core-1.1.0.jar:1.1.0] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_51] at javax.security.auth.Subject.doAs(Subject.java:415) ~[?:1.7.0_51] at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto____5543.invoke(worker.clj:611) ~[storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?] at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0] Caused by: java.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:444) ~[?:1.7.0_51] at sun.nio.ch.Net.bind(Net.java:436) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214) ~[?:1.7.0_51] at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.7.0_51] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-1.1.0.jar:1.1.0] at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-1.1.0.jar:1.1.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[?:1.7.0_51] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[?:1.7.0_51] at java.lang.Thread.run(Thread.java:744) ~[?:1.7.0_51] 2018-03-14 13:28:42.004 o.a.s.util main [ERROR] Halting process: (""Error on initialization"") java.lang.RuntimeException: (""Error on initialization"") at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$fn__5542$mk_worker__5633.doInvoke(worker.clj:585) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker$_main.invoke(worker.clj:769) [storm-core-1.1.0.jar:1.1.0] at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?] at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?] at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.1.0.jar:1.1.0]
{code}"
STORM-2994,KafkaSpout consumes messages but doesn't commit offsets,"A topology that consumes from two different Kafka clusters: 0.10.1.1 and 0.10.2.1.

Spouts consuming from 0.10.2.1 have a low lag (and regularly commit offsets) 

The Spout that consumes from 0.10.1.1 exhibits either:

1- Unknown lag

2- Lag that increments as the Spout reads messages from Kafka

 

In DEBUG, Offset manager logs: ""topic-partition has NO offsets ready to be committed"", despite continuing to consume messages.

Several configuration tweaks were tried, including setting maxRetries to 1, in case messages with a lower offset were being retried (logs didn't show it, though)

offsetCommitPeriodMs was also  lowered to no avail.

The only configuration that works is to have ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG=true, but this is undesired   since we lose processing guarantees.

 "
STORM-2992,Add support for starting storm-kafka-client spout consumer at a specific timestamp ,"The 0.10.1.0 KafkaConsumer has support for getting the offset corresponding to a given timestamp (the offsetsForTimes method). We could provide a new FirstPollOffsetStrategy to allow topologies to start consumption at a specific timestamp, instead of earliest or latest offset."
STORM-2991,Use MockConsumer for tests where possible in storm-kafka-client instead of using a raw Mockito mock,"MockConsumer seems like it will be less brittle for testing than using a raw Mockito mock, e.g. we don't have to manually stub the Consumer.position call. We should try to replace the mocks in existing tests."
STORM-2990,Make the storm-kafka-client Trident spout FirstPollOffsetStrategy behavior consistent with the regular spout,The EARLIEST and LATEST semantics were changed in the regular spout so the spout only starts over when the topology is redeployed. The Trident spout should behave the same way.
STORM-2989,LogCleaner should preserve current worker.log.metrics,"LogCleaner cleans up logs based on LastModified timestamp. Like 

[https://github.com/apache/storm/blob/master/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/DirectoryCleaner.java#L45]

 

I think we should at least preserve the current worker.log.metrics"
STORM-2988,"""Error on initialization of server mk-worker"" when using org.apache.storm.metrics2.reporters.JmxStormReporter on worker","As per documentation, I configured metrics v2 in my storm.yaml using the following configuration:
 
{code:yaml}
storm.metrics.reporters:

  - class: ""org.apache.storm.metrics2.reporters.JmxStormReporter""
    daemons:
        - ""supervisor""
        - ""nimbus""
        - ""worker""
    report.period: 10
    report.period.units: ""SECONDS""
{code}

When I start nimbus and supervisors everything works properly, I can see metrics reported to JMX, and logs (for nimbus in this example) report:

{code}
2018-03-07 15:35:22.201 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
2018-03-07 15:35:22.203 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2018-03-07 15:35:22.221 o.a.s.d.common main [INFO] Started statistics report plugin...
{code}

When I submit a topology, workers cannot initialize and report this error

{code:java}
2018-03-07 15:39:19.136 o.a.s.d.worker main [INFO] Launching worker for stp_topology-1-1520433551 on [... cut ...]
2018-03-07 15:39:19.169 o.a.s.m.StormMetricRegistry main [INFO] Starting metrics reporters...
2018-03-07 15:39:19.172 o.a.s.m.StormMetricRegistry main [INFO] Attempting to instantiate reporter class: org.apache.storm.metrics2.reporters.JmxStormReporter
2018-03-07 15:39:19.175 o.a.s.m.r.JmxStormReporter main [INFO] Preparing...
2018-03-07 15:39:19.182 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker
java.lang.IllegalArgumentException: Don't know how to convert {""class"" ""org.apache.storm.metrics2.reporters.JmxStormReporter"", ""daemons"" [""supervisor"" ""nimbus"" ""worker""], ""report.period"" 10, ""report.period.units"" ""SECONDS""} + to String
	at org.apache.storm.utils.Utils.getString(Utils.java:848) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain(JmxStormReporter.java:70) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.reporters.JmxStormReporter.prepare(JmxStormReporter.java:51) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.StormMetricRegistry.startReporter(StormMetricRegistry.java:119) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.metrics2.StormMetricRegistry.start(StormMetricRegistry.java:102) ~[storm-core-1.2.1.jar:1.2.1]
	at org.apache.storm.daemon.worker$fn__5545$exec_fn__1369__auto____5546.invoke(worker.clj:611) ~[storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
2018-03-07 15:39:19.195 o.a.s.util main [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]
{code}

Looking at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain() I found that it passes ""reporterConf"" map to Utils.getString() instead of a string:
{code:java}
    public static String getMetricsJMXDomain(Map reporterConf) {
        return Utils.getString(reporterConf, JMX_DOMAIN);
}
{code}

The ""prepare"" method in org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter used by nimbus and supervisor correctly passes a string to Utils.getString():

{code:java}
public void prepare(MetricRegistry metricsRegistry, Map stormConf) {
        LOG.info(""Preparing..."");
        JmxReporter.Builder builder = JmxReporter.forRegistry(metricsRegistry);
        String domain = Utils.getString(stormConf.get(Config.STORM_DAEMON_METRICS_REPORTER_PLUGIN_DOMAIN), null);
        if (domain != null) {
            builder.inDomain(domain);
}
[...]
{code}

Is this a bug or am I missing something in configuration?

Regards,
Federico Chiacchiaretta"
STORM-2987,PaceMakerStateStorage should deal with InterruptedException correctly,"We found an issue that when nimbus restarted, it can only get leadership after a few seconds (15~20s). 

 
{code:java}
2018-02-27 08:18:43.420 main o.a.s.z.LeaderElectorImp [INFO] Queued up for leader lock.
2018-02-27 08:18:43.481 main o.a.s.d.m.MetricsUtils [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
2018-02-27 08:18:43.483 main o.a.s.d.m.r.JmxPreparableReporter [INFO] Preparing...
2018-02-27 08:18:43.499 main o.a.s.m.StormMetricsRegistry [INFO] Started statistics report plugin...
2018-02-27 08:18:43.543 main o.a.s.m.n.Login [INFO] successfully logged in.
2018-02-27 08:18:43.551 main o.a.s.z.ClientZookeeper [INFO] Staring ZK Curator
2018-02-27 08:18:43.551 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
2018-02-27 08:18:43.552 Refresh-TGT o.a.s.m.n.Login [INFO] TGT refresh thread started.
2018-02-27 08:18:43.553 Refresh-TGT o.a.s.m.n.Login [INFO] TGT valid starting at:        Tue Feb 27 08:18:43 UTC 2018
2018-02-27 08:18:43.553 Refresh-TGT o.a.s.m.n.Login [INFO] TGT expires:                  Wed Feb 28 08:18:43 UTC 2018
2018-02-27 08:18:43.553 Refresh-TGT o.a.s.m.n.Login [INFO] TGT refresh sleeping until: Wed Feb 28 04:35:55 UTC 2018
2018-02-27 08:18:43.553 main o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=openqe74blue-gw.blue.ygrid.yahoo.com:2181 sessionTimeout=60000 watcher=org.apache.
curator.ConnectionState@2e185cd7
2018-02-27 08:18:43.559 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Default schema
2018-02-27 08:18:43.560 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.c.ZooKeeperSaslClient [INFO] Client will use GSSAPI as SASL mechanism.
2018-02-27 08:18:43.561 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Opening socket connection to server openqe74blue-gw.blue.ygrid.yahoo.co
m/10.215.68.156:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-02-27 08:18:43.562 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Socket connection established to openqe74blue-gw.blue.ygrid.yahoo.com/1
0.215.68.156:2181, initiating session
2018-02-27 08:18:43.565 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Session establishment complete on server openqe74blue-gw.blue.ygrid.yah
oo.com/10.215.68.156:2181, sessionid = 0x161d5f1ae970099, negotiated timeout = 40000
2018-02-27 08:18:43.565 main-EventThread o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2018-02-27 08:18:43.605 Curator-Framework-0 o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
2018-02-27 08:18:43.625 main o.a.z.ZooKeeper [INFO] Session: 0x161d5f1ae970099 closed
2018-02-27 08:18:43.625 main-EventThread o.a.z.ClientCnxn [INFO] EventThread shut down
2018-02-27 08:18:43.626 main o.a.s.z.ClientZookeeper [INFO] Staring ZK Curator
2018-02-27 08:18:43.626 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
2018-02-27 08:18:43.635 main o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=openqe74blue-gw.blue.ygrid.yahoo.com:2181/storm_ystormQE_CI sessionTimeout=60000 w
atcher=org.apache.curator.ConnectionState@46cc127b
2018-02-27 08:18:43.654 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.c.ZooKeeperSaslClient [INFO] Client will use GSSAPI as SASL mechanism.
2018-02-27 08:18:43.660 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Opening socket connection to server openqe74blue-gw.blue.ygrid.yahoo.co
m/10.215.68.156:2181. Will attempt to SASL-authenticate using Login Context section 'Client'
2018-02-27 08:18:43.663 main o.a.c.f.i.CuratorFrameworkImpl [INFO] Default schema
2018-02-27 08:18:43.663 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Socket connection established to openqe74blue-gw.blue.ygrid.yahoo.com/1
0.215.68.156:2181, initiating session
2018-02-27 08:18:43.666 main-SendThread(openqe74blue-gw.blue.ygrid.yahoo.com:2181) o.a.z.ClientCnxn [INFO] Session establishment complete on server openqe74blue-gw.blue.ygrid.yah
oo.com/10.215.68.156:2181, sessionid = 0x161d5f1ae97009a, negotiated timeout = 40000
2018-02-27 08:18:43.669 main-EventThread o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2018-02-27 08:18:43.790 main o.a.s.d.n.Nimbus [INFO] Starting nimbus server for storm version '2.0.0.y'
2018-02-27 08:18:44.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping assignments
2018-02-27 08:18:44.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping cleanup
2018-02-27 08:18:44.300 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518767144-stormcode.ser
2018-02-27 08:18:44.301 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: TestZkErrorNodesHaveCorrectAcls-3-1519540302-stormcode.ser
2018-02-27 08:18:44.302 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: logviewer-ui-groups-test-1-1518940914-stormcode.ser
2018-02-27 08:18:44.303 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518800831-stormcode.ser
2018-02-27 08:18:44.304 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518767144-stormconf.ser
2018-02-27 08:18:44.306 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: logviewer-ui-groups-test-1-1518940914-stormconf.ser
2018-02-27 08:18:44.307 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: TestZkErrorNodesHaveCorrectAcls-3-1519540302-stormconf.ser
2018-02-27 08:18:44.308 timer o.a.s.b.BlobStoreUtils [ERROR] Could not download the blob with key: blob-5-1518800831-stormconf.ser
2018-02-27 08:18:44.367 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping credential renewal.
2018-02-27 08:18:54.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping assignments
2018-02-27 08:18:54.274 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping cleanup
2018-02-27 08:18:59.367 timer o.a.s.d.n.Nimbus [INFO] not a leader, skipping credential renewal.
2018-02-27 08:19:02.059 main-EventThread o.a.s.z.Zookeeper [INFO] active-topology-blobs [] local-topology-blobs [] diff-topology-blobs []
2018-02-27 08:19:02.059 main-EventThread o.a.s.z.Zookeeper [INFO] active-topology-dependencies [] local-blobs [] diff-topology-dependencies []
2018-02-27 08:19:02.059 main-EventThread o.a.s.z.Zookeeper [INFO] Accepting leadership, all active topologies and corresponding dependencies found locally.
2018-02-27 08:19:04.754 timer o.a.s.d.n.Nimbus [INFO] Scheduling took 442 ms for 0 topologies
{code}
 

This can be re-produced by the following steps:
{code:java}
1. restart pacemaker;
2. before pacemaker is up, restart nimbus{code}
When we restart nimbus process,  it runs ShutDownHooks and stuck on timer.close(). 

[https://github.com/apache/storm/blob/master/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java#L4227]

The timer is not able to close because it's waiting for  doCleanup() to stop. However, the interruptedException is caught and ate in : [https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java#L180-L192]

 

 

 

 "
STORM-2985,Add jackson-annotations to dependency management," 

We recently upgraded to jackson version 2.9.4. However different versions of jackson-annotation dependencies are inherited via transitive dependencies of other jars. Its best to keep it in sync."
STORM-2981,Upgrade Curator to lastest patch version,"Looks like the fix for https://issues.apache.org/jira/browse/STORM-2706 (the fix is https://issues.apache.org/jira/browse/CURATOR-436) is resolved in 4.0.1, not 4.0.0. We should upgrade."
STORM-2980,The storm-starter documentation incorrectly states that the examples can be run in a local cluster via a command line flag.,
STORM-2979,WorkerHooks EOFException during run_worker_shutdown_hooks,"Hi,

I'm trying to use the BaseWorkerHook but an exception is thrown after I killed the topology.

The issue is exactly the same as : [http://user.storm.apache.narkive.com/uchOrwlH/workerhook-deserialization-problem|http://user.storm.apache.narkive.com/uchOrwlH/workerhook-deserialization-problem]

An extract of my code :

 
{code:java}
// topology
final TridentTopology topology = new TridentTopology();
// ... I skip all the topology configuration part
final StormTopology topo = topology.build();

// hook
final BaseWorkerHook hook = new BaseWorkerHook();
final ByteBuffer serializedHook = ByteBuffer.wrap(Utils.javaSerialize(hook ));
topo.add_to_worker_hooks(hook);

// submit topology
LocalCluster cluster = new LocalCluster();
cluster.submitTopology(name,config,topo);
Utils.sleep(60000);

// kill topology
final KillOptions killOptions = new KillOptions();
killOptions.set_wait_secs(0);
cluster.killTopologyWithOpts(name, killOptions);
Utils.sleep(10000);
cluster.shutdown();
{code}
 

I have the following error :
{code:java}
java.lang.RuntimeException: java.io.EOFException
    at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:254)
    at org.apache.storm.daemon.worker$run_worker_shutdown_hooks$iter__5456__5460$fn__5461.invoke(worker.clj:578)
    at clojure.lang.LazySeq.sval(LazySeq.java:40)
    at clojure.lang.LazySeq.seq(LazySeq.java:49)
    at clojure.lang.RT.seq(RT.java:507)
    at clojure.core$seq__4128.invoke(core.clj:137)
    at clojure.core$dorun.invoke(core.clj:3009)
    at clojure.core$doall.invoke(core.clj:3025)
    at org.apache.storm.daemon.worker$run_worker_shutdown_hooks.invoke(worker.clj:576)
    at org.apache.storm.daemon.worker$fn__5471$exec_fn__1371__auto__$reify__5473$shutdown_STAR___5493.invoke(worker.clj:693)
    at org.apache.storm.daemon.worker$fn__5471$exec_fn__1371__auto__$reify$reify__5519.shutdown(worker.clj:706)
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:67)
    at org.apache.storm.daemon.supervisor.LocalContainer.kill(LocalContainer.java:59)
    at org.apache.storm.daemon.supervisor.Slot.killContainerForChangedAssignment(Slot.java:311)
    at org.apache.storm.daemon.supervisor.Slot.handleRunning(Slot.java:527)
    at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:265)
    at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:741)
Caused by: java.io.EOFException
    at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2680)
    at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3155)
    at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:864)
    at java.io.ObjectInputStream.<init>(ObjectInputStream.java:360)
    at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:245)
    ... 16 more
{code}
 

Maybe it is related to log4j shutdown hooks (https://issues.apache.org/jira/browse/STORM-2176) so I tried to disable the hook in my src/test/resources/log4j2.xml.

 
{code:java}
<Configuration monitorInterval=""60"" shutdownHook=""disable"">
    <Appenders>
        <Console name=""Console"" target=""SYSTEM_OUT"">
            <PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"" />
        </Console>
    </Appenders>

    <Loggers>
        <Root level=""debug"">
            <AppenderRef ref=""Console"" />
        </Root>
    </Loggers>
</Configuration>
{code}
But it does not change anything.

 

Of course the purpose of my work is to use my own worker hook extending the BaseWorkerHook.

 

 

 

 "
STORM-2978,"The fix for STORM-2706 is broken, and adds a transitive dependency on Zookeeper 3.5.3-beta for projects that depend on e.g. storm-kafka","Shinhyung Yang wrote on the mailing list:

{quote}I have been running the Yahoo streaming benchmarks on Storm 0.9.7 [...] With the introduction of Storm 1.2.0, I decided to upgrade from 0.9.7 to 1.2.0. Currently I'm testing Yahoo streaming benchmark's topology on the new setup and I end up getting the following exceptions:

[...]
Caused by: org.apache.zookeeper.KeeperException$UnimplementedException: KeeperErrorCode = Unimplemented for /ad-events/7183b5b2-4971-41a1-b86d-0788f646bc64/partition_0
[...]
{quote}

When fixing STORM-2706, I used the Storm parent's DependencyManagement section to force Zookeeper to version 3.4.6 everywhere in Storm. Sadly it turns out that this mechanism doesn't extend to external projects that depend on Storm components. While e.g. storm-kafka will use Zookeeper 3.4.6 when built as part of Storm, it will have a transitive dependency on Zookeeper 3.5.3-beta when an external project declares a dependency on storm-kafka.

A quick google indicates that the ""proper"" way to export the transitive dependency versions to downstream projects would be with a BOM pom, i.e. we'd create a separate BOM project for Storm that exports our DependencyManagement, and users would then import the BOM. I'm not sure if we want to do that on master, but since it is a breaking change I don't think we should do this on 1.x.

For 1.x (and maybe master?), we'll have to make sure that Curator dependencies always exclude Zookeeper, and all projects depending on Curator will have to explicitly declare the right Zookeeper dependency version."
STORM-2975,Worker died if KafkaSpout catched a kafka CommitFailedException,"org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:792) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:738) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:808) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:204) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:167) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:127) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:488) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:348) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:262) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:208) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:184) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:605) ~[stormjar.jar:?]
 at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1173) ~[stormjar.jar:?]
 at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:?]
 at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:220) ~[stormjar.jar:?]
 at org.apache.storm.daemon.executor$fn__4962$fn__4977$fn__5008.invoke(executor.clj:646) ~[storm-core-1.1.1.jar:1.1.1]
 at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.1.jar:1.1.1]
 at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
 at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]"
STORM-2974,Add a transactional non-opaque spout to storm-kafka-client,
STORM-2973,Replace storm-perf storm-kafka topologies with storm-kafka-client versions,
STORM-2972,Replace storm-kafka in storm-sql-kafka with storm-kafka-client,
STORM-2971,Replace Flux storm-kafka example with an equivalent example for storm-kafka-client,
STORM-2966,Nimbus fails to log errors when shutting down at startup,"While testing, I encountered a NoClassDefFoundError for Nimbus at startup.  This caused Nimbus to shutdown with 0 lines in the log file.

 

Adding test code to catch the exception, I was able to see the log messages and fix the issue (see callstack below).

 

We should be logging the error on shutdown for debugging.

 

2018-02-19 14:56:28.842 o.a.s.d.n.Nimbus main [ERROR] Failed to initialize metric store

org.apache.storm.metricstore.MetricException: Failed to create metric store

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:41) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1113) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1103) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.<init>(Nimbus.java:1098) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.launchServer(Nimbus.java:1008) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.launch(Nimbus.java:1026) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at org.apache.storm.daemon.nimbus.Nimbus.main(Nimbus.java:1031) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/io/RawComparator

        at org.apache.storm.hbasemetricstore.HBaseStore.<clinit>(HBaseStore.java:87) ~[storm-hbasemetricstore-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]

        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        ... 6 more

Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.RawComparator

        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_131]

        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_131]

        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) ~[?:1.8.0_131]

        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_131]

        at org.apache.storm.hbasemetricstore.HBaseStore.<clinit>(HBaseStore.java:87) ~[storm-hbasemetricstore-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]

        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]

        at org.apache.storm.metricstore.MetricStoreConfig.configureMetricStore(MetricStoreConfig.java:37) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]

        ... 6 more"
STORM-2965,Interpret wildcard in classpath correctly when reading config from classpath,"While reading configuration file from classpath at Utils.getConfigFromClasspath(), it doesn't interpret wildcard (\*) as Java classpath does. It should behave same as Java."
STORM-2963,Updates to Performance.md ,
STORM-2958,Use new wait strategies for Spout as well,"STORM-2306 introduced a new configurable wait strategy system for these situations
 * BackPressure Wait (used by spout & bolt)
 * No incoming data (used by bolt)

There is another wait situation in the spout when there are no emits generated in a nextTuple() or if max.spout.pending has been reached. This Jira is to transition the spout wait strategy from the old model to the new model. Thereby we have a uniform model for dealing with wait strategies."
STORM-2957,Offsets are not committed when no ackers set (ackers=0) and KafkaSpout stops polling messages once it reaches the max uncommitted offsets ,"A KafkaSpout stops polling messages from topic when topology has no ackers set (""topology.acker.executors""= 0). This happens after KafkaSpout polls (and emits) messages equal to the max uncommitted offsets (KafkaspoutConfig.maxUncommittedOffsets).

This happens because the KafkaSpout.ack() is called even before the emit method adds the message to ""emitted"" collection (Set). In such cases, the message is not added ""acked"" collection (Map), and hence never committed. This seem to be bug introduced in a version later than storm-kafka-client 1.0.1 and current code (1.0.5) has a check in ack() that checks if the acked message is available in ""emitted"" collection and if not, ignores it (never adds it to ""acked"" collection)

Steps to reproduce issue:
1. Need a topology with KafkaSpout from storm-kafka-client
2. Set number of ackers (""topology.acker.executors"") to o (org.apache.storm.Config.setNumAckers(0)
3. While creating KafkaSpout instance, set SpoutConfig.setMaxUncommittedOffsets to small number (50 or 100)
4. Start topology, see to it that KafkaSpout polls more messages from Kafka that setMaxUncommittedOffsets
5. KafkaSpout stops polling messages from topic"
STORM-2953,Remove storm-kafka in 2.0.0,Remove storm-kafka from master. See the thread at http://mail-archives.apache.org/mod_mbox/storm-dev/201802.mbox/%3CCAF5108hZBRf7Y%2BOFPif9kSw9%2BuZuPx5iBgSt1TOX5zMvF0-NfQ%40mail.gmail.com%3E
STORM-2952,Deprecate storm-kafka in 1.x,Add deprecation notice to storm-kafka for the 1.x branch. See the thread at http://mail-archives.apache.org/mod_mbox/storm-dev/201802.mbox/%3CCAF5108hZBRf7Y%2BOFPif9kSw9%2BuZuPx5iBgSt1TOX5zMvF0-NfQ%40mail.gmail.com%3E. 
STORM-2951,Storm binaries packages oncrpc jar which is LGPL ,"With the recent storm metrics changes storm packages oncrpc-1.0.7.jar which is LGPL licence.

 

[https://mvnrepository.com/artifact/org.acplt/oncrpc/1.0.7]

 

I am not sure if its ok to package libraries with LGPL license in storm distribution. 

 

Its coming from metrics-ganglia dependency in storm-core.

 [~ptgoetz], can you provide inputs ? If this needs to be excluded, I can craft a patch and push it."
STORM-2949,Download Storm release link in 1.0.x and 1.1.1 points to github(source code) rather than the storm.apache.org link. ,"I am following the documentation to [Setup a Storm cluster for 1.0.5|http://storm.apache.org/releases/1.0.4/Setting-up-a-Storm-cluster.html] and I have some comments/ errors to point out.
 
- The documentation only exists for 1.0.4. Replacing 1.0.4 in the url with 1.0.5 gives a 404 error.
 
- The section to [Download and extract a Storm release to Nimbus and worker machines|http://storm.apache.org/releases/1.0.4/Setting-up-a-Storm-cluster.html#download-and-extract-a-storm-release-to-nimbus-and-worker-machineshttp://storm.apache.org/releases/1.0.4/Setting-up-a-Storm-cluster.html%23install-dependencies-on-nimbus-and-worker-machines] provides a link to download Storm releases from GitHub. 
 * The tar.gz download from [github|https://github.com/apache/storm/releases/tag/v1.0.5] is 8.5MB and is labeled as ""*Source code* (tar.gz)"". This is confusing for a release download.
 * Trying to run nimbus from the extracted folder gives the following error, which is consistent with the above labeling. 
 * [vagrant@node1 storm-1.0.6]$ bin/storm nimbus

******************************************

The storm client can only be run from within a release. You appear to be trying to run the client from a checkout of Storm's source code.

 

You can download a Storm release at [http://storm.apache.org/downloads.html]

******************************************

This link is incorrect in documentation for v1.0.x and v1.1.x.
 My understanding is the documentation is pointing to the wrong link. This link should be updated to point to the [releases|http://storm.apache.org/downloads.html] from [http://storm.apache.org/] as mentioned in the error message. I was able to setup the storm cluster using this download path. "
STORM-2947,Review and fix/remove deprecated things in Storm 2.0.0,"We've been deprecating the things but haven't have time to replace/get rid of them. It should be better if we have time to review and address them.
"
STORM-2945,Nail down and document how to support background emits in Spouts and Bolts,
STORM-2944,Eliminate these deprecated methods in Storm 3,"In Storm 2 we deprecated these methods. These methods were retained to allow storm 2.x nimbus & supervisor to manage older Storm 1.x workers.  

These are the methods in IStormClusterState.java 

{code}
/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */

@Deprecated
 boolean topologyBackpressure(String stormId, long timeoutMs, Runnable callback);

/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */
 @Deprecated
 void setupBackpressure(String stormId);

/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */
 @Deprecated
 void removeBackpressure(String stormId);

/** @deprecated: In Storm 2.0. Retained for enabling transition from 1.x. Will be removed soon. */
 @Deprecated
 void removeWorkerBackpressure(String stormId, String node, Long port);
{code}"
STORM-2943,Binary distribution includes storm-kafka-monitor source/javadoc in toollib directory,"Quoting Alexandre's RC3 vote:

 
{quote}I hate to be the one who always give bad news, but as a matter of
 facts, Storm 1.2.0 RC3 installation from binary artifacts (both
 apache-storm-1.2.0-src.tar.gz and apache-storm-1.2.0.zip) leads to ""by
 default KO Kafka monitor"" in Nimbus UI (which dirty exceptions in
 ui.log)

Here's for example what I get from apache-storm-1.2.0-src.tar.gz
 downloaded from
[https://dist.apache.org/repos/dist/dev/storm/apache-storm-1.2.0-rc3/apache-storm-1.2.0-src.tar.gz]:

$ tar ztvf apache-storm-1.2.0.tar.gz apache-storm-1.2.0/toollib
 -rwxrwxrwx ptgoetz/staff 16999 2018-02-06 21:22
 apache-storm-1.2.0/toollib/storm-kafka-monitor-1.2.0-sources.jar
 -rwxrwxrwx ptgoetz/staff 93461 2018-02-06 21:22
 apache-storm-1.2.0/toollib/storm-kafka-monitor-1.2.0-javadoc.jar
 -rwxrwxrwx ptgoetz/staff 21591320 2018-02-06 21:22
 apache-storm-1.2.0/toollib/storm-kafka-monitor-1.2.0.jar

And here's what I see in ui.log:

 org.apache.storm.kafka.spout.KafkaSpout
 2018-02-07 16:49:57.153 o.a.s.u.TopologySpoutLag qtp1997623038-18
[WARN] Exception message:Error: Could not find or load main class
 .usr.local.Storm.storm-stable.toollib.storm-kafka-monitor-1.2.0-javadoc.jar

org.apache.storm.utils.ShellUtils$ExitCodeException: Error: Could not
 find or load main class
 .usr.local.Storm.storm-stable.toollib.storm-kafka-monitor-1.2.0-javadoc.jar

        at org.apache.storm.utils.ShellUtils.runCommand(ShellUtils.java:231)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils.run(ShellUtils.java:161)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils$ShellCommandExecutor.execute(ShellUtils.java:371)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils.execCommand(ShellUtils.java:461)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.ShellUtils.execCommand(ShellUtils.java:444)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.TopologySpoutLag.getLagResultForKafka(TopologySpoutLag.java:163)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.TopologySpoutLag.getLagResultForNewKafkaSpout(TopologySpoutLag.java:189)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.utils.TopologySpoutLag.lag(TopologySpoutLag.java:57)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.ui.core$topology_lag.invoke(core.clj:805)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.ui.core$fn__9586.invoke(core.clj:1165)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$make_route$fn__5979.invoke(core.clj:100)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$if_route$fn__5967.invoke(core.clj:46)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$if_method$fn__5960.invoke(core.clj:31)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.compojure.core$routing$fn__5985.invoke(core.clj:113)
 ~[storm-core-1.2.0.jar:1.2.0]
         at clojure.core$some.invoke(core.clj:2570) ~[clojure-1.7.0.jar:?]
         at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
 ~[storm-core-1.2.0.jar:1.2.0]
         at clojure.lang.RestFn.applyTo(RestFn.java:139) ~[clojure-1.7.0.jar:?]
         at clojure.core$apply.invoke(core.clj:632) ~[clojure-1.7.0.jar:?]
         at org.apache.storm.shade.compojure.core$routes$fn__5989.invoke(core.clj:118)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.cors$wrap_cors$fn__8894.invoke(cors.clj:149)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__8841.invoke(json.clj:56)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6621.invoke(multipart_params.clj:118)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__7904.invoke(reload.clj:22)
 ~[storm-core-1.2.0.jar:1.2.0]
         at org.apache.storm.ui.helpers$requests_middleware$fn__6874.invoke(helpers.clj:52)
 ~[storm-core-1.2.0.jar:1.2.0]

Deleting the extraneous storm-kafka-monitor-1.2.0-sources.jar and
 storm-kafka-monitor-1.2.0-javadoc.jar file, then restarting Nimbus
 solves the issue.

However, binaries artifacts should be as clean as possible, isn't it ?
{quote}
 "
STORM-2942,Remove javadoc and source jars from toollib directory in binary distribution,Need to update the assembly to only include the classes jar.
STORM-2941,checkstyle failing on master,Got some checkstyle failures and build will not pass
STORM-2939,Create interface for processing worker metrics,"In Container.java, we send worker metrics to Nimbus to store to RocksDB.  Other implementations (HBase, etc) may want to process in different fashions.  "
STORM-2937,Overwrite storm-kafka-client 1.x-branch into 1.0.x-branch,"This is to track the effort of syncing up divergence between storm-kafka-client 1.x-branch and 1.0.x-branch so that critical fixes can be go in 1.0.x-branch as well.

Note that it can modify storm-core as well (unlikely in a backwards-incompatible way but not 100% sure), so we should make a decision whether we allow the change in bugfix version line.

Linking discussion thread:

[https://lists.apache.org/thread.html/0451fed132bb982b618d9e0780282a87554f1bc5747827599f276944@%3Cdev.storm.apache.org%3E]"
STORM-2936,Overwrite storm-kafka-client 1.x-branch into 1.1.x-branch,"This is to track the effort of syncing up divergence between storm-kafka-client 1.x-branch and 1.1.x-branch so that critical fixes can be go in 1.1.x-branch as well.

Linking discussion thread:

[https://lists.apache.org/thread.html/0451fed132bb982b618d9e0780282a87554f1bc5747827599f276944@%3Cdev.storm.apache.org%3E]

 "
STORM-2934,Logviewer ClassNotFoundException validating configs,"Caused by: java.lang.ClassNotFoundException: org.rocksdb.RocksDBException
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_131]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_131]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) ~[?:1.8.0_131]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_131]
        at java.lang.Class.forName0(Native Method) ~[?:1.8.0_131]
        at java.lang.Class.forName(Class.java:264) ~[?:1.8.0_131]
        at org.apache.storm.validation.ConfigValidation$ImplementsClassValidator.validateField(ConfigValidation.java:587) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.validation.ConfigValidation.validateField(ConfigValidation.java:758) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.validation.ConfigValidation.validateFields(ConfigValidation.java:811) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.validation.ConfigValidation.validateFields(ConfigValidation.java:772) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.ConfigUtils.readStormConfigImpl(ConfigUtils.java:397) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.utils.ConfigUtils.readStormConfig(ConfigUtils.java:136) ~[storm-client-2.0.0.y.jar:2.0.0.y]
        at org.apache.storm.daemon.logviewer.LogviewerServer.main(LogviewerServer.java:158) ~[storm-webapp-2.0.0.y.jar:2.0.0.y]"
STORM-2933,"Add a storm-perf topology that uses storm-kafka-client, so we can benchmark that module",
STORM-2928,(For 2.0.0) In a secure cluster with storm-autocreds enabled storm-druid can fail with NoSuchMethodError,"storm-autocreds brings in the curator 4.0 via transitive dependency of storm-core, even though storm-core is listed as provided scope, the app assembler plugin puts the dependency (curator 4.0) into external/storm-autocreds directory. This conflicts with the storm-druid tranquility library which depends on curator 2.6.0
2018-01-22 08:43:54.047 o.a.s.d.executor Thread-15-54-Dashboard-Violation-Predicted-executor[20 20] [ERROR]
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:125)"
STORM-2927,"Storm supervisor displaying message ""kill: sending signal to 23543 failed: No such process""","I started storm cluster and all one nimbus and two supervisors started fine and I could view them fine on ""Storm UI""  then I deployed example topology ""storm-opentsdb-2.0.0-SNAPSHOT.jar"", it is successfully deployed on the cluster but after few minutes supervisor console displays message and no data is inserted in Opentsdb.
kill: sending signal to 23543 failed: No such process
kill: sending signal to 23615 failed: No such process
kill: sending signal to 23612 failed: No such process
kill: sending signal to 23706 failed: No such process
kill: sending signal to 23776 failed: No such process.

The same topology run perfectly fine in Local Cluster mode and inserts data in Opentsdb.

To resolve this issue I killed the toplogy, killed the nimbus and supervisor nodes. I manuall deleted files under ""storm.local.dir: ""/tmp/storm-data"""" and also deleted the files in zookeeper znode /storm. Again started the cluster and deployed the same topology but got the same error.


"
STORM-2926,org.apache.storm.st.tests.window.SlidingWindowTest.testWindowCount in integration test fails intermittently,"Lost a build link... From what I've seen is, one of 13th trials in test had failed. So not often, but really intermittent."
STORM-2925,org.apache.storm.sql.TestStormSql consistently fails,"1. java.lang.RuntimeException: java.lang.IllegalStateException: It took over 60000ms to shut down slot Thread[SLOT_1024,5,main]

[https://travis-ci.org/apache/storm/jobs/335344937]

 

2. [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project storm-sql-core: ExecutionException The forked VM terminated without properly saying goodbye. VM crash or System.exit called?

[https://travis-ci.org/apache/storm/jobs/334308676]

 

I've skimmed a bit and looks like twos are same issue."
STORM-2924,testTickTupleWorksWithSystemBolt in org.apache.storm.TickTupleTest fails intermittently,"java.lang.AssertionError: took over 110000 ms of simulated time to get a message back...

[https://travis-ci.org/apache/storm/jobs/335075657]"
STORM-2923,org.apache.storm.st.tests.window.SlidingWindowTest in integration test fails intermittently with 1.x version lines,"Unfortunately lost the build link... The following message was presented:

java.lang.RuntimeException: Failed to kill topology SlidingWindowTest-window200-slide100. Subsequent tests may fail because worker slots are occupied"
STORM-2922,org.apache.storm.hdfs.spout.TestDirLock is consistently stuck with JDK 7 in Travis CI,"[https://travis-ci.org/apache/storm/jobs/333477869]

 "
STORM-2921,integration.org.apache.storm.testing4j-test fails intermittently on 1.x version line,[https://travis-ci.org/apache/storm/jobs/332445614]
STORM-2920,org.apache.storm.trident.tuple-test fails intermittently on 1.x version line,[https://travis-ci.org/apache/storm/jobs/332147829]
STORM-2919,integration.org.apache.storm.integration-test fails intermittently on 1.x version line,"testname ""test-submit-inactive-topology"" failed occasionally: I've seen various kinds of failures, refer the links to see detail:

[https://travis-ci.org/apache/storm/jobs/333477868]

[https://travis-ci.org/apache/storm/jobs/333477867]"
STORM-2918,Upgrade Netty version,"netty 3.9.0 has been out since June 2014, netty 3.9.9 has been released in July 2015. On top of it, there are two known CVEs for netty below 3.9.2
CVE-20140193 [https://www.us-cert.gov/ncas/bulletins/SB14-132]
CVE-20143488 [https://www.cvedetails.com/cve/CVE-2014-3488/]"
STORM-2915,How could I to get the fail Number   in Bolt When I use  Kafka Spout,"I want to get fail num in bolt , how could  I  to get it? 

if  fail it  retry, I see This 

if (!isScheduled || retryService.isReady(msgId)) {
 final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;

 if (!isAtLeastOnceProcessing()) {
 if (kafkaSpoutConfig.isTupleTrackingEnforced()) {
 collector.emit(stream, tuple, msgId);
 LOG.trace(""Emitted tuple [{}] for record [{}] with msgId [{}]"", tuple, record, msgId);
 } else {
 collector.emit(stream, tuple);
 LOG.trace(""Emitted tuple [{}] for record [{}]"", tuple, record);
 }
 } else {
 emitted.add(msgId);
 offsetManagers.get(tp).addToEmitMsgs(msgId.offset());
 if (isScheduled) { // Was scheduled for retry and re-emitted, so remove from schedule.
 retryService.remove(msgId);
 }
 collector.emit(stream, tuple, msgId);
 tupleListener.onEmit(tuple, msgId);
 LOG.trace(""Emitted tuple [{}] for record [{}] with msgId [{}]"", tuple, record, msgId);
 }
 return true;
}"
STORM-2914,Remove enable.auto.commit support from storm-kafka-client,"The enable.auto.commit option causes the KafkaConsumer to periodically commit the latest offsets it has returned from poll(). It is convenient for use cases where messages are polled from Kafka and processed synchronously, in a loop. 

Due to https://issues.apache.org/jira/browse/STORM-2913 we'd really like to store some metadata in Kafka when the spout commits. This is not possible with enable.auto.commit. I took at look at what that setting actually does, and it just causes the KafkaConsumer to call commitAsync during poll (and during a few other operations, e.g. close and assign) with some interval. 

Ideally I'd like to get rid of ProcessingGuarantee.NONE, since I think ProcessingGuarantee.AT_MOST_ONCE covers the same use cases, and is likely almost as fast. The primary difference between them is that AT_MOST_ONCE commits synchronously.

If we really want to keep ProcessingGuarantee.NONE, I think we should make our ProcessingGuarantee.NONE setting cause the spout to call commitAsync after poll, and never use the enable.auto.commit option. This allows us to include metadata in the commit."
STORM-2913,"STORM-2844 made autocommit and at-most-once storm-kafka-client spouts log warnings on every emit, because those modes don't commit the right metadata to Kafka","The mechanism added in https://issues.apache.org/jira/browse/STORM-2844 to allow us to check whether a committed offset was committed by the currently running topology requires that we commit some metadata along with the offset.

We are using this metadata for two things: Only applying the FirstPollOffsetStrategy when the topology is deployed, rather than when the worker is restarted, and an (IMO fairly unimportant) runtime check that the spout offset tracking is not in a bad state.

Autocommit spouts don't include this metadata, and we also don't include it when committing offsets in at-most-once mode. We can fix at-most-once by switching to committing a custom OffsetAndMetadata, rather than using the no-arg commitSync variant. 

I'm not sure what we should do to fix the autocommit case. There doesn't seem to be a way to include metadata in autocommits, so I don't think we can support this mechanism for autocommits. 

If we can't fix the autocommit case, I see two options for fixing this:
* Make doSeek have the old behavior for autocommits only (i.e. apply the FirstPollOffsetStrategy on every worker restart), and keep the new behavior for at-least-once/at-most-once. I think this behavior could be a little confusing.
* Revert doSeek to the old behavior in all cases, and throw out the runtime check that uses the metadata. This also isn't a great option, because the new seek behavior is more useful than restarting on every worker reboot.

What do you think [~hmclouro]? I'm leaning toward the first option."
STORM-2912,Tick tuple is being shared without resetting start time and incur side-effect to break metrics,"In STORM-2786 we have applied small optimization: create tick tuple only once and reuse. The optimization completely makes sense, but when measuring built-in metrics, when reused tick tuple is selected for sampling, we never reset start time unless it is selected for sampling again, hence further tick tuple is always considered as sampled with start time unchanged.

What I've observed is that delta for tick tuple is gradually increasing for some time-period, and reset to 0, which messes up latencies. It also messes up executed as well because it is always considered as selected tuple (hence recorded to 20x for tick tuple), but unless interval of tick tuple is super small, the effect is much smaller then latency.

 

Here's part of log denoting this issue. Please take a look at DELTA values, which shouldn't be such huge.
{code:java}
2018-01-25 13:34:41.464 o.a.s.d.executor Thread-14-__acker-executor[1 1] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [30] TASK: 1 DELTA: 0
2018-01-25 13:34:41.658 o.a.s.d.executor Thread-12-counter-executor[3 3] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 3 DELTA: 87083
2018-01-25 13:34:41.658 o.a.s.d.executor Thread-8-counter-executor[2 2] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 2 DELTA: 6003
2018-01-25 13:34:41.728 o.a.s.d.executor Thread-26-counter-executor[5 5] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 5 DELTA: 30036
2018-01-25 13:34:41.728 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 4001
2018-01-25 13:34:41.729 o.a.s.d.executor Thread-32-counter-executor[4 4] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 4 DELTA: 156155
2018-01-25 13:34:41.813 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 24043
2018-01-25 13:34:41.813 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 14025
2018-01-25 13:34:41.813 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 52091
2018-01-25 13:34:41.886 o.a.s.d.executor Thread-28-intermediateRanker-executor[10 10] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 10 DELTA: 18025
2018-01-25 13:34:43.731 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 6004
2018-01-25 13:34:43.817 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 26047
2018-01-25 13:34:43.817 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 16029
2018-01-25 13:34:43.817 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 1
2018-01-25 13:34:43.890 o.a.s.d.executor Thread-28-intermediateRanker-executor[10 10] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 10 DELTA: 20029
2018-01-25 13:34:44.661 o.a.s.d.executor Thread-12-counter-executor[3 3] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 3 DELTA: 90086
2018-01-25 13:34:44.662 o.a.s.d.executor Thread-8-counter-executor[2 2] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 2 DELTA: 9007
2018-01-25 13:34:44.734 o.a.s.d.executor Thread-26-counter-executor[5 5] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 5 DELTA: 33042
2018-01-25 13:34:44.734 o.a.s.d.executor Thread-32-counter-executor[4 4] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 4 DELTA: 159160
2018-01-25 13:34:45.735 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 8008
2018-01-25 13:34:45.820 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 2004
2018-01-25 13:34:45.821 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 28051
2018-01-25 13:34:45.821 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 18033
2018-01-25 13:34:45.892 o.a.s.d.executor Thread-28-intermediateRanker-executor[10 10] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 10 DELTA: 22031
2018-01-25 13:34:47.663 o.a.s.d.executor Thread-12-counter-executor[3 3] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 3 DELTA: 93088
2018-01-25 13:34:47.664 o.a.s.d.executor Thread-8-counter-executor[2 2] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 2 DELTA: 12009
2018-01-25 13:34:47.734 o.a.s.d.executor Thread-32-counter-executor[4 4] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 4 DELTA: 162160
2018-01-25 13:34:47.736 o.a.s.d.executor Thread-4-intermediateRanker-executor[8 8] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 8 DELTA: 10009
2018-01-25 13:34:47.736 o.a.s.d.executor Thread-26-counter-executor[5 5] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [3] TASK: 5 DELTA: 36044
2018-01-25 13:34:47.825 o.a.s.d.executor Thread-10-intermediateRanker-executor[7 7] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 7 DELTA: 20037
2018-01-25 13:34:47.826 o.a.s.d.executor Thread-16-finalRanker-executor[6 6] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 6 DELTA: 30056
2018-01-25 13:34:47.826 o.a.s.d.executor Thread-18-intermediateRanker-executor[9 9] [INFO] Execute done TUPLE source: __system:-1, stream: __tick, id: {}, [2] TASK: 9 DELTA: 4010{code}"
STORM-2911,SpoutConfig is serializable but does not declare a serialVersionUID field,See https://stackoverflow.com/questions/48420085/kafkaspout-is-not-emmiting-message
STORM-2910,Tests taking a long time to run,"The test for me went form 17 mins to run everything up to 30 mins to run everything.

I did some debugging and found that the slot is not very responsive if it cannot connect to nimbus (to send metrics back).  In those cases the connection has to time out before the slot can respond.

I think we want to do 2 things here.  First we don't want slot blocking for long periods of time if it cannot talk to nimbus, so I am going to background the sync, with a queue that will throw out old metrics if the new ones want to be sent.

Next I am going to figure out why the local override is not happening in the nimbus client in this situation."
STORM-2909,New Metrics Reporting API - for 2.0.0,"This is a proposal to provide a new metrics reporting API based on [Coda Hale's metrics library | http://metrics.dropwizard.io/3.1.0/] (AKA Dropwizard/Yammer metrics).

h2. Background
In a [discussion on the dev@ mailing list | http://mail-archives.apache.org/mod_mbox/storm-dev/201610.mbox/%3cCAGX0URh85NfH0Pbph11PMc1oof6HTycjCXSxgwP2nnofuKq0pQ@mail.gmail.com%3e]  a number of community and PMC members recommended replacing Storm’s metrics system with a new API as opposed to enhancing the existing metrics system. Some of the objections to the existing metrics API include:

# Metrics are reported as an untyped Java object, making it very difficult to reason about how to report it (e.g. is it a gauge, a counter, etc.?)
# It is difficult to determine if metrics coming into the consumer are pre-aggregated or not.
# Storm’s metrics collection occurs through a specialized bolt, which in addition to potentially affecting system performance, complicates certain types of aggregation when the parallelism of that bolt is greater than one.


In the discussion on the developer mailing list, there is growing consensus for replacing Storm’s metrics API with a new API based on Coda Hale’s metrics library. This approach has the following benefits:

# Coda Hale’s metrics library is very stable, performant, well thought out, and widely adopted among open source projects (e.g. Kafka).
# The metrics library provides many existing metric types: Meters, Gauges, Counters, Histograms, and more.
# The library has a pluggable “reporter” API for publishing metrics to various systems, with existing implementations for: JMX, console, CSV, SLF4J, Graphite, Ganglia.
# Reporters are straightforward to implement, and can be reused by any project that uses the metrics library (i.e. would have broader application outside of Storm)

As noted earlier, the metrics library supports pluggable reporters for sending metrics data to other systems, and implementing a reporter is fairly straightforward (an example reporter implementation can be found here). For example if someone develops a reporter based on Coda Hale’s metrics, it could not only be used for pushing Storm metrics, but also for any system that used the metrics library, such as Kafka.

h2. Scope of Effort
The effort to implement a new metrics API for Storm can be broken down into the following development areas:

# Implement API for Storms internal worker metrics: latencies, queue sizes, capacity, etc.
# Implement API for user defined, topology-specific metrics (exposed via the {{org.apache.storm.task.TopologyContext}} class)
# Implement API for storm daemons: nimbus, supervisor, etc.

h2. Relationship to Existing Metrics
This would be a new API that would not affect the existing metrics API. Upon completion, the old metrics API would presumably be deprecated, but kept in place for backward compatibility.

Internally the current metrics API uses Storm bolts for the reporting mechanism. The proposed metrics API would not depend on any of Storm's messaging capabilities and instead use the [metrics library's built-in reporter mechanism | http://metrics.dropwizard.io/3.1.0/manual/core/#man-core-reporters]. This would allow users to use existing {{Reporter}} implementations which are not Storm-specific, and would simplify the process of collecting metrics. Compared to Storm's {{IMetricCollector}} interface, implementing a reporter for the metrics library is much more straightforward (an example can be found [here | https://github.com/dropwizard/metrics/blob/3.2-development/metrics-core/src/main/java/com/codahale/metrics/ConsoleReporter.java].

The new metrics capability would not use or affect the ZooKeeper-based metrics used by Storm UI.

h2. Relationship to JStorm Metrics
[TBD]

h2. Target Branches
[TBD]

h2. Performance Implications
[TBD]

h2. Metrics Namespaces
[TBD]

h2. Metrics Collected
*Worker*
|| Namespace || Metric Type || Description ||

*Nimbus*
|| Namespace || Metric Type || Description ||

*Supervisor*
|| Namespace || Metric Type || Description ||

h2. User-Defined Metrics
[TBD]
"
STORM-2908,Document Metrics V2 for 2.0.0,"This issue is to track follow-up task on Metrics V2: documentation.

The purpose for 'blocker' is to ensure this to be finished before releasing Storm 2.0.0."
STORM-2907,In a secure cluster with storm-autocreds enabled storm-druid can fail with NoSuchMethodError,"storm-autocreds brings in the curator 4.0 via transitive dependency of storm-core, even though storm-core is listed as provided scope, the app assembler plugin puts the dependency (curator 4.0) into external/storm-autocreds directory. This conflicts with the storm-druid tranquility library which depends on curator 2.6.0
2018-01-22 08:43:54.047 o.a.s.d.executor Thread-15-54-Dashboard-Violation-Predicted-executor[20 20] [ERROR]
java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at com.metamx.tranquility.beam.ClusteredBeam$$anonfun$com$metamx$tranquility$beam$ClusteredBeam$$zpathWithDefault$1.apply(ClusteredBeam.scala:125)"
STORM-2905,Supervisor still downloads storm blob files when the topology was killed.,"When we kill a topology, at the moment of topology blob-files be removed, Supervisor executor still request blob-files and get an KeyNotFoundException.

I stepped in and found the reason:
1. We do not add a guarded lock on `topologyBlobs` of AsyncLocalizer which is singleton to a supervisor node.
2. And we remove jar/code/conf blob keys in `topologyBlobs` of killed storm only in a timer task: cleanUp() method of AsyncLocalizer, the remove condition is :[no one reference the blobs] AND [ blobs removed by master OR exceeds the max configured size ], the default scheduling interval is 30 seconds.
3. When we kill a storm on a node[ which means that the slot container are empty], the AsyncLocalizer will do: releaseSlotFor, which only remove reference on the blobs [topologyBlobs keys are still there.]
4. Then the container is empty, and Slot.java will do: cleanupCurrentContainer, which will invoke AsyncLocalizer #releaseSlotFor to release the slot.
5. AsyncLocalizer have a timer task: updateBlobs to update base/user blobs every 30 seconds, which based on the AsyncLocalizer#`topologyBlobs`
6. We know that AsyncLocalizer#`topologyBlobs` overdue keys are only removed by its AsyncLocalizer#cleanUp which is also a timer task.
7. So when we kill a storm, AsyncLocalizer#updateBlobs will update based on a removed jar/code/conf blob-key and fire a exception, then retried until the configured max times to end.

Here is how i fixed it:
1. just remove the base blob keys eagerly when we do AsyncLocalizer #releaseSlotFor when there is no reference [no one used] on the blobs, and remove the overdue keys in AsyncLocalizer#`topologyBlobs`
2. Guard the AsyncLocalizer#updateBlobs and AsyncLocalizer #releaseSlotFor on the same lock.
3. When container is empty, we do not need to exec AsyncLocalizer #releaseSlotFor[because we have already deleted them].
4. I also add a new RPC api for decide if there exists a remote blob, we can use it to decide it the blob could be removed instead of use getBlobMeta and catch an confusing KeyNotFoundException [both on supervisors and master log for every base blobs].

This is the partial of Supervisor log:

2018-03-31 13:41:17.089 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/pids/1115
 2018-03-31 13:41:17.090 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/heartbeats
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/pids
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0/tmp
 2018-03-31 13:41:17.102 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers/b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.103 o.a.s.d.s.Container SLOT_6700 [INFO] REMOVE worker-user b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.103 o.a.s.d.s.AdvancedFSOps SLOT_6700 [INFO] Deleting path /Users/danny0405/workspace/storm-2.x-test/supervisor1/apache-storm-2.0.0-SNAPSHOT/storm-local/workers-users/b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.104 o.a.s.d.s.BasicContainer SLOT_6700 [INFO] Removed Worker ID b50aa089-6584-498e-a5cc-85cba13e4cb0
 2018-03-31 13:41:17.105 o.a.s.d.s.Slot SLOT_6700 [INFO] STATE KILL msInState: 25 topo:word_count_fk_11-2-1522472558 worker:null -&gt; EMPTY msInState: 0
 2018-03-31 13:41:17.105 o.a.s.d.s.Slot SLOT_6700 [INFO] SLOT 6700: Changing current assignment from LocalAssignment(topology_id:word_count_fk_11-2-1522472558, executors:[ExecutorInfo(task_start:7, task_end:7), ExecutorInfo(task_start:6, task_end:6), ExecutorInfo(task_start:5, task_end:5), ExecutorInfo(task_start:4, task_end:4), ExecutorInfo(task_start:3, task_end:3), ExecutorInfo(task_start:2, task_end:2), ExecutorInfo(task_start:1, task_end:1)], resources:WorkerResources(mem_on_heap:896.0, mem_off_heap:0.0, cpu:70.0, shared_mem_on_heap:0.0, shared_mem_off_heap:0.0, resources:{offheap.memory.mb=0.0, onheap.memory.mb=896.0, cpu.pcore.percent=70.0}, shared_resources:{}), owner:danny0405) to null
 2018-03-31 13:41:18.001 o.a.s.d.s.t.SupervisorHealthCheck timer [INFO] Running supervisor healthchecks...
 2018-03-31 13:41:18.002 o.a.s.h.HealthChecker timer [INFO] The supervisor healthchecks succeeded.
 2018-03-31 13:41:36.837 o.a.s.l.AsyncLocalizer AsyncLocalizer Executor - 1 [WARN] Failed to download blob LOCAL TOPO BLOB TOPO_JAR word_count_fk_11-2-1522472558 will try again in 100 ms
 org.apache.storm.generated.KeyNotFoundException: null
         at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25225) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25193) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25124) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[libthrift-0.9.3.jar:0.9.3]
         at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:825) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:812) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:318) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:176) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$5(AsyncLocalizer.java:249) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
         at java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1626) [?:1.8.0_151]
         at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_151]
         at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_151]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_151]
         at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_151]
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_151]
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_151]
         at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]"
STORM-2904,Document Metrics V2,"This issue is to track follow-up task on Metrics V2: documentation.

The purpose for 'blocker' is to ensure this to be finished before releasing Storm 1.2.0."
STORM-2903,Fix possible NullPointerException in AbstractAutoCreds,"Observed below exception while testing Hive token mechanism.

    ```
    Caused by: java.lang.NullPointerException
            at org.apache.storm.common.AbstractAutoCreds.addTokensToUGI(AbstractAutoCreds.java:219) ~[storm-autocreds-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]
            at org.apache.storm.common.AbstractAutoCreds.populateSubject(AbstractAutoCreds.java:118) ~[storm-autocreds-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]
            at org.apache.storm.security.auth.AuthUtils.populateSubject(AuthUtils.java:228) ~[storm-core-1.2.0.3.1.0.0-526.jar:1.2.0.3.1.0.0-526]
            ... 10 more
    2018-01-19 16:23:26.157 o.a.s.util main [ERROR] Halting process: (""Error on initialization"")
    ```"
STORM-2902,Some improvements for storm-rocketmq module,"* Upgraded RocketMQ version to 4.2.0 which brings improvements and new features like batch sending
* Imporved retry policy for RocketMQ consumer push mode to avoid data loss in some scenes
* Batch sending supported for bolt and trident state
* Allow running several consumer instances in one process, that is to say, different topics in one worker is possible

 "
STORM-2901,Reuse ZK connection for getKeySequenceNumber,"Now when our nimbus restarts, many zookeeper connections will be made in minutes, and it's really a pressure for our zookeeper server.

I checkout the log and code to find that when nimbus restart, in order to sync local storm keys[ actually valid storms ], it will:
 # check storm keys diff of local storm and zk remote.
 # set up path for all the valid storm keys with a keySequenceNumber.
 # In order to get the keySequenceNumber, now blobstore will make a new zk-client and connect to zk-server.

This is the reason why thousands of connections are made. For our cluster, there are about 800+ topologies running, which means that at least 800 connections will be made which totally can be reused.

 

This is part of nimbus re-starting log:

2018-01-18 12:51:57.031 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
 2018-01-18 12:51:57.032 o.a.s.s.o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=dx-data-rt-zk01:2181,dx-data-rt-zk02:2181,dx-data-rt-zk04:2181/mtstorm_101_dx_storm01 sessionTimeout=30000 watcher=org.apache.storm.shade.org.apache.curator.ConnectionState@76513a57
 2018-01-18 12:51:57.032 o.a.s.s.o.a.z.ClientCnxn [INFO] Opening socket connection to server dx-data-rt-zk04.dx.sankuai.com/10.32.157.254:2181. Will not attempt to authenticate using SASL (unknown error)
 2018-01-18 12:51:57.033 o.a.s.s.o.a.z.ClientCnxn [INFO] Socket connection established to dx-data-rt-zk04.dx.sankuai.com/10.32.157.254:2181, initiating session
 2018-01-18 12:51:57.034 o.a.s.s.o.a.z.ClientCnxn [INFO] Session establishment complete on server dx-data-rt-zk04.dx.sankuai.com/10.32.157.254:2181, sessionid = 0x45cd92f0cc7e938, negotiated timeout = 30000
 2018-01-18 12:51:57.034 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
 2018-01-18 12:51:57.037 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
 2018-01-18 12:51:57.039 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x45cd92f0cc7e938 closed
 2018-01-18 12:51:57.039 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
 2018-01-18 12:51:57.040 o.a.s.cluster [INFO] setup-path/blobstore/app_waimairank_wm_recsys_user_block-4-1504509174-stormconf.ser/dx-data-rt-nimbus05.dx.sankuai.com:9827-1
 2018-01-18 12:51:57.051 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] Starting
 2018-01-18 12:51:57.051 o.a.s.s.o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=dx-data-rt-zk01:2181,dx-data-rt-zk02:2181,dx-data-rt-zk04:2181/mtstorm_101_dx_storm01 sessionTimeout=30000 watcher=org.apache.storm.shade.org.apache.curator.ConnectionState@69c222d6
 2018-01-18 12:51:57.052 o.a.s.s.o.a.z.ClientCnxn [INFO] Opening socket connection to server dx-data-rt-zk02.dx.sankuai.com/10.32.108.46:2181. Will not attempt to authenticate using SASL (unknown error)
 2018-01-18 12:51:57.053 o.a.s.s.o.a.z.ClientCnxn [INFO] Socket connection established to dx-data-rt-zk02.dx.sankuai.com/10.32.108.46:2181, initiating session
 2018-01-18 12:51:57.055 o.a.s.s.o.a.z.ClientCnxn [INFO] Session establishment complete on server dx-data-rt-zk02.dx.sankuai.com/10.32.108.46:2181, sessionid = 0x25cd386f245eb72, negotiated timeout = 30000
 2018-01-18 12:51:57.055 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
 2018-01-18 12:51:57.058 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
 2018-01-18 12:51:57.061 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x25cd386f245eb72 closed
 2018-01-18 12:51:57.061 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
 2018-01-18 12:51:57.061 o.a.s.cluster [INFO] setup-path/blobstore/app_waimairank_waimai_rank_rt_pipeline_user_feature-12-1507516853-stormconf.ser/dx-data-rt-nimbus05.dx.sankuai.com:9827-1"
STORM-2900,Subject is not populated and NPE is thrown while populating credentials in nimbus.,"Nimbus Auto Creds[1.x, 2.0] may get into NPE while populating credentials when there is no config for the given key.

1.x - [https://github.com/apache/storm/blob/1.x-branch/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java#L75]
 2.0 - [https://github.com/apache/storm/blob/master/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java#L55]"
STORM-2899,Remove/replace the contributors lists in the README and on https://storm.apache.org/contribute/People.html,"We've talked a few times on the mailing list about removing or replacing the contributors lists, because they are not really being maintained because keeping them up to date is a manual process.

The last time this was discussed http://mail-archives.apache.org/mod_mbox/storm-dev/201712.mbox/browser, there seemed to be some consensus for using Spark's scripted solution to generate the contributors list. It was also suggested that having the contributors list at all was maybe unnecessary.

In order to avoid having this discussion again in a few months, I'll submit a PR that removes the contributors list from the README and site, because it's the simplest solution in terms of release process (no extra step to generate the contributors list).

If there are strong feelings for using Spark's solution to generate a contributors list instead, people will hopefully chime in on the PR review, and we can make changes as necessary."
STORM-2898,Storm should support auth through delegation tokens for workers,"There are a lot of cases where it would be great for a worker to be able to communicate directly to nimbus, supervisors, or drpc servers in a secure way out of the box.

This is currently a pain to make work.  The user has to ship a TGT with their topology, and continually keep it up to date with credentials-push.  They also need a kind of hacked up jaas.conf to grab the TGT from AutoTGT and put it in the place that he client wants it.

We should just generate a signed data structure (aka delegation token from hadoop) that we can had off to the topologies to use when talking to nimbus, a supervisor, or drpc servers.

We may want to split up the different services from each other to make an attack against one not hit all of them, but that is something we can think about with the design of this.

I will try to come up with a design shortly.
"
STORM-2896,Support automatic migration of offsets from storm-kafka to storm-kafka-client KafkaSpout,"I think we can ease migration for people looking to move from storm-kafka to storm-kafka-client. We should be able to support migrating offsets from the old spout by setting some extra configuration in KafkaSpoutConfig, and by adding a new FirstPollOffsetStrategy (e.g. something like FirstPollOffsetStrategy.UNCOMMITTED_MIGRATE_FROM_STORM_KAFKA).

The old spout stores offsets in Storm's Zookeeper at one of two paths. The storm-kafka SpoutConfig has two parameters we'll also need, namely zkRoot and id. In addition we need to know if the storm-kafka subscription was a wildcard subscription or not.

The zookeeper path for commit info is 
{code}
zkRoot + ""/"" + id + ""/"" + topicName + ""partition_"" + partition
{code}
if the subscription was a wildcard. Otherwise it is 
{code}
zkRoot + ""/"" + id + ""/"" + ""partition_"" + partition
{code}

We can get topicName and partition numbers from the KafkaConsumer.assignment. When we run initialize, we should be able to read the old offset structure from Zookeeper when the strategy is UNCOMMITTED_MIGRATE_FROM_STORM_KAFKA, and seek the consumer to those offsets. We can just crash if the offsets are not present.

I'd be okay with this feature not being permanent, but I think this feature would make it a lot easier for people to move off the old spout."
STORM-2894,fix some random typos in tests,"* MultilangEnvirontmentTest
** There is no ""t"" between n and m in Environment.
* getOffsetFromConfigAndFroceFromStart
** The r and o are transposed in Force."
STORM-2893,maven-assembly-plugin upgrade is breaking distribution build,"{code: title=failure}
% mvn clean package && cd storm-dist/binary && mvn -Dgpg.skip=true install
...
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single (default) on project final-package: Execution default of goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single failed: group id '906175167' is too big ( > 2097151 ). Use STAR or POSIX extensions to overcome this limit -> [Help 1]
{code}

This is a [known issue with the maven-assembly-plugin version 2.5+|https://stackoverflow.com/a/30246880/318428].

Prior to [a change a few months ago|https://github.com/apache/storm/commit/84a4314d96b9e4e377a3d5d81d0a042d96a0625e], the maven-assembly-plugin was [version 2.2.2|https://github.com/apache/storm/blob/de1d468592b402fb862f98ce357cbd1268d13f8c/pom.xml#L1195-L1199], now it's 2.6, as inherited from the [apache pom version 18|https://github.com/apache/maven-pom/blob/apache-18/pom.xml#L119-L123]."
STORM-2892,Flux test fails to parse valid PATH environment variable,"The flux tests rely on substituting the {{PATH}} environment variable into the [{{substitution-test.yaml}}|https://github.com/apache/storm/blob/466a7ad74da27c1250eedf412a487db409e42c19/flux/flux-core/src/test/resources/configs/substitution-test.yaml#L44-L45] file.

I noticed that the tests were failing when my {{PATH}} had a trailing colon, despite that being a valid {{PATH}} \[1].

h2. Existing error

{code: title=mvn test output}
Running org.apache.storm.flux.TCKTest
Tests run: 18, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.852 sec <<< FAILURE! - in org.apache.storm.flux.TCKTest
testVariableSubstitution(org.apache.storm.flux.TCKTest)  Time elapsed: 0.012 sec  <<< ERROR!
org.yaml.snakeyaml.scanner.ScannerException: null; mapping values are not allowed here;  in 'string', line 45, column 890:
     ... /usr/local/bin:/usr/bin:
                                ^
        at org.yaml.snakeyaml.scanner.ScannerImpl.fetchValue(ScannerImpl.java:866)
        at org.yaml.snakeyaml.scanner.ScannerImpl.fetchMoreTokens(ScannerImpl.java:360)
        at org.yaml.snakeyaml.scanner.ScannerImpl.checkToken(ScannerImpl.java:226)
        at org.yaml.snakeyaml.parser.ParserImpl$ParseBlockMappingKey.produce(ParserImpl.java:558)
        at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)
        at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)
        at org.yaml.snakeyaml.composer.Composer.composeMappingNode(Composer.java:230)
        at org.yaml.snakeyaml.composer.Composer.composeNode(Composer.java:159)
        at org.yaml.snakeyaml.composer.Composer.composeMappingNode(Composer.java:237)
        at org.yaml.snakeyaml.composer.Composer.composeNode(Composer.java:159)
        at org.yaml.snakeyaml.composer.Composer.composeDocument(Composer.java:122)
        at org.yaml.snakeyaml.composer.Composer.getSingleNode(Composer.java:105)
        at org.yaml.snakeyaml.constructor.BaseConstructor.getSingleData(BaseConstructor.java:120)
        at org.yaml.snakeyaml.Yaml.loadFromReader(Yaml.java:481)
        at org.yaml.snakeyaml.Yaml.load(Yaml.java:400)
        at org.apache.storm.flux.parser.FluxParser.loadYaml(FluxParser.java:121)
        at org.apache.storm.flux.parser.FluxParser.parseInputStream(FluxParser.java:75)
        at org.apache.storm.flux.parser.FluxParser.parseResource(FluxParser.java:59)
        at org.apache.storm.flux.TCKTest.testVariableSubstitution(TCKTest.java:224)


Results :

Tests in error: 
  TCKTest.testVariableSubstitution:224 » Scanner null; mapping values are not al...
{code}

h2. Proposed solution

Just wrap the {{PATH}} variable's contents in the yaml file.

h3. \[1] PATH validity

{code: title=man bash}
...

       PATH   The  search  path  for  commands.  It is a colon-separated
              list of directories in which the shell looks for  commands
              (see  COMMAND  EXECUTION  below).   A  zero-length  (null)
              directory name in the value of PATH indicates the  current
              directory.   A null directory name may appear as two adja-
              cent colons, or as an  initial  or  trailing  colon.   The
              default path is system-dependent, and is set by the admin-
              istrator  who  installs   bash.    A   common   value   is
              ``/usr/gnu/bin:/usr/local/bin:/usr/ucb:/bin:/usr/bin''.
...
{code}"
STORM-2891,Upgrade Checkstyle plugin to version 3.0.0,It would be good to upgrade the maven-checkstyle-plugin to version 3.0.0. Some pretty nice quality of life improvements there https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317223&version=12333072.
STORM-2890,Update the UI to use the new metrics query API,"Once we have a query API and at least the same metric that go into ZK are going into the new metrics engine we should update the UI to display the metrics from the new query API where possible, and fall back on ZK when they are not available.

We want to get rid of the metrics in the heartbeats at some point soon, but to be able to support older topologies running under the newer versions of storm we will keep this code around until a 3.x build."
STORM-2889,Add Metrics Query API (Thrift and REST),"Once we have a storage engine and even if it is just a few metrics we want to add in an API to query the metrics.

It should be able to do simple aggregations (Min, Max, Mean, and Sum) across different time ranges, with different roll up windows.  It should also be able to filter based off of things like topology id, component name, component id, stream name, and host name.  It is fine to have some of these required, such as topology id, at the beginning.

It should be able to return results either all together or split up by the different topology ids, component names, component ids, stream names, or host names.

For Example the UI shows things like

SUM of {tuples_emitted, tuples_transferred, acked, failed}  in the last {10 mins, 3 hours, 1 day, all time} for all spouts a.k.a a set of component names that we got from the topology itself.
AVERAGE of {complete_latency} in the last {10 mins, 3 hours, 1 day, all time} for a set of component names.

SUM of {tuples_emitted, tuples_transferred, acked, failed}  in the last {10 mins, 3 hours, 1 day, all time} for all components separated by each component name.
AVERAGE of {complete_latency} in the last {10 mins, 3 hours, 1 day, all time} for all components separated by each component name.

And there are others too.



The API should be open to expansion for other types of filters, aggregates, and even data types.  The following are not things that should be implemented, but are things that we should think about how we can leave the API open for expansion alter on so we could support them in a clean way.

Specifically some filters I would like to see in the future include topology name or even better a basic pattern we can use to match topology ids and the owner of a topology.

I would also like to eventually support percentile sketches as a data type so instead of just having an average of the 99th%ile collected by each component, we get an much more accurate approximation of what the 99th%ile really is in aggregate.
"
STORM-2888,Integrate storage with metrics V2.,Once STORM-2887 and STORM-2153 are done and in this is to add in the glue code that allows the supervisor to forward all of the metrics to the storage engine.
STORM-2887,Add RocksDB storage and Basic Metrics forwarding from the supervisor,"This portion is just to put in the rocksdb storage engine, the APIs to be able to store the metrics, and to update the supervisor to send the few metrics that it knows about.

Memory usage when using cgroups.

This is by no means complete but should be able to unblock some work on elasticity in the scheduler."
STORM-2886,Look into better phemeral port support in LocalCluster.withNimbusDaemon,"We have a few tests that use a LocalCluster and then launch an actual Nimbus Thrift Server instance.  This is mostly to test that the thrift code works properly.

In some cases this can cause issues because by default it is going to get a single hard coded port.

In most cases we work around this, like in STORM-2885 by finding a free port and then using that for both client and server configs.

This does not eliminate the race, but it makes it very unlikely.  It would be great if we could eliminate the race by somehow bringing up the thrift server on an ephemeral port, and then providing a way to get that port through an API in LocalCluster.

One of the issues we would have to overcome is that we explicitly check for positive port numbers, and we would only want to turn that off for this particular use case."
STORM-2885,TickTupleTest can fail because it uses NimbusDaemon,"When STORM-2876 went it it made some of the tests run in parallel.  I have seen testTickTupleWorksWithSystemBolt fail, but rarely, when other tests are running that also want to launch a LocalCluster withNimbusDaemon enabled.

This test has no reason to have withNimbusDaemon enabled so I will just turn it off.

There are a few other tests that want to use this same functionality.  I will also look to see if I can reduce it's usage or find a good way to have it use an ephemeral port.

"
STORM-2882,Relocate dependencies on storm-client (and more),"STORM-2441 broke down storm-core to several parts which reduced actual dependencies for storm-client (worker), but it also got rid of relocations which may be considered as regression.

We should handle relocation before releasing Storm 2.0.0.

Earlier discussion link: https://mail-archives.apache.org/mod_mbox/incubator-storm-dev/201703.mbox/%3CCAF5108gjjQzSyYWCP99bgPGEc7TUfE-tbt9Fi0M78ah8RkMQCQ@mail.gmail.com%3E"
STORM-2880,Minor optimisation about kafka spout,"Based on the single responsibility principle, method isAtLeastOnceProcessing() should reside in KafkaSpoutConfig rather than KafkaSpout. This patch removes the dependency of KafkaSpoutConfig.ProcessingGuarantee from KafkaSpout."
STORM-2879,Supervisor collapse continuously when there is a expired assignment for overdue storm,"For now, when a topology is reassigned or killed for a cluster, supervisor will delete 4 files for an overdue storm:
- storm-code
- storm-ser
- storm-jar
- LocalAssignment

Slot.java
static DynamicState cleanupCurrentContainer(DynamicState dynamicState, StaticState staticState, MachineState nextState) throws Exception {
        assert(dynamicState.container != null);
        assert(dynamicState.currentAssignment != null);
        assert(dynamicState.container.areAllProcessesDead());
        
        dynamicState.container.cleanUp();
        staticState.localizer.releaseSlotFor(dynamicState.currentAssignment, staticState.port);
        DynamicState ret = dynamicState.withCurrentAssignment(null, null);
        if (nextState != null) {
            ret = ret.withState(nextState);
        }
        return ret;
    }

But we do not make a transaction to do this, if an exception occurred during deleting storm-code/ser/jar, an overdue local assignment will be left on disk.

Then when supervisor restart from the exception above, the slots will be initial and container will be recovered from LocalAssignments, the blob store will fetch the files from Nimbus/Master, but will get a KeyNotFoundException, and supervisor collapses again.

This will happens continuously and supervisor will never recover until we clean up all the local assignments manually.

This is the stack:
2017-12-27 14:15:04.434 o.a.s.l.AsyncLocalizer [INFO] Cleaning up unused topologies in /opt/meituan/storm/data/supervisor/stormdist
2017-12-27 14:15:04.434 o.a.s.d.s.AdvancedFSOps [INFO] Deleting path /opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785
2017-12-27 14:15:04.445 o.a.s.d.s.Slot [INFO] STATE EMPTY msInState: 109 -> WAITING_FOR_BASIC_LOCALIZATION msInState: 1
2017-12-27 14:15:04.471 o.a.s.d.s.Supervisor [INFO] Starting supervisor with id 255d3fed-f3ee-4c7e-8a08-b693c9a6a072 at host gq-data-rt48.gq.sankuai.com.
2017-12-27 14:15:04.502 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.611 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.718 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.825 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.932 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.039 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.140 o.a.s.u.Utils [INFO] Could not extract resources from /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar
2017-12-27 14:15:05.142 o.a.s.d.s.Slot [INFO] STATE WAITING_FOR_BASIC_LOCALIZATION msInState: 697 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-12-27 14:15:05.142 o.a.s.l.AsyncLocalizer [WARN] Caught Exception While Downloading (rethrowing)... 
java.io.FileNotFoundException: File '/opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785/stormconf.ser' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:264) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:376) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:370) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:226) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:213) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]"
STORM-2878,Supervisor collapse continuously when there is a expired assignment for overdue storm,"For now, when a topology is reassigned or killed for a cluster, supervisor will delete 4 files for an overdue storm:
- storm-code
- storm-ser
- storm-jar
- LocalAssignment

Slot.java
static DynamicState cleanupCurrentContainer(DynamicState dynamicState, StaticState staticState, MachineState nextState) throws Exception {
        assert(dynamicState.container != null);
        assert(dynamicState.currentAssignment != null);
        assert(dynamicState.container.areAllProcessesDead());
        
        dynamicState.container.cleanUp();
        staticState.localizer.releaseSlotFor(dynamicState.currentAssignment, staticState.port);
        DynamicState ret = dynamicState.withCurrentAssignment(null, null);
        if (nextState != null) {
            ret = ret.withState(nextState);
        }
        return ret;
    }

But we do not make a transaction to do this, if an exception occurred during deleting storm-code/ser/jar, an overdue local assignment will be left on disk.

Then when supervisor restart from the exception above, the slots will be initial and container will be recovered from LocalAssignments, the blob store will fetch the files from Nimbus/Master, but will get a KeyNotFoundException, and supervisor collapses again.

This will happens continuously and supervisor will never recover until we clean up all the local assignments manually.

This is the stack:
2017-12-27 14:15:04.434 o.a.s.l.AsyncLocalizer [INFO] Cleaning up unused topologies in /opt/meituan/storm/data/supervisor/stormdist
2017-12-27 14:15:04.434 o.a.s.d.s.AdvancedFSOps [INFO] Deleting path /opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785
2017-12-27 14:15:04.445 o.a.s.d.s.Slot [INFO] STATE EMPTY msInState: 109 -> WAITING_FOR_BASIC_LOCALIZATION msInState: 1
2017-12-27 14:15:04.471 o.a.s.d.s.Supervisor [INFO] Starting supervisor with id 255d3fed-f3ee-4c7e-8a08-b693c9a6a072 at host gq-data-rt48.gq.sankuai.com.
2017-12-27 14:15:04.502 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.611 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:123) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.718 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.825 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormcode.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:124) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:04.932 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.039 o.a.s.u.Utils [ERROR] An exception happened while downloading /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormconf.ser from blob store.
org.apache.storm.generated.KeyNotFoundException: null
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.downloadBaseBlobs(AsyncLocalizer.java:125) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:148) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBaseBlobsDistributed.call(AsyncLocalizer.java:101) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]
2017-12-27 14:15:05.140 o.a.s.u.Utils [INFO] Could not extract resources from /opt/meituan/storm/data/supervisor/tmp/ca4f8174-59be-40a4-b431-dbc8b697f063/stormjar.jar
2017-12-27 14:15:05.142 o.a.s.d.s.Slot [INFO] STATE WAITING_FOR_BASIC_LOCALIZATION msInState: 697 -> WAITING_FOR_BLOB_LOCALIZATION msInState: 0
2017-12-27 14:15:05.142 o.a.s.l.AsyncLocalizer [WARN] Caught Exception While Downloading (rethrowing)... 
java.io.FileNotFoundException: File '/opt/meituan/storm/data/supervisor/stormdist/app_dpsr_realtime_shop_vane_allcates-14-1513685785/stormconf.ser' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfGivenPath(ConfigUtils.java:264) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConfImpl(ConfigUtils.java:376) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.utils.ConfigUtils.readSupervisorStormConf(ConfigUtils.java:370) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:226) ~[storm-core-1.1.2-mt001.jar:?]
	at org.apache.storm.localizer.AsyncLocalizer$DownloadBlobs.call(AsyncLocalizer.java:213) ~[storm-core-1.1.2-mt001.jar:?]
	at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_76]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_76]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_76]"
STORM-2877,Introduce an option to configure pagination in Storm UI ,"The current pagination default value for Storm UI is hard-coded to be 20. Pagination has been introduced in Storm 1.x. Having 20 items in the list restricts searching through the page. It will be beneficial to have a configuration option, say {{ui.pagination}}, which will set the default pagination value when Storm UI loads. This option can be added to {{storm.yaml}} along with other configurations."
STORM-2876,Some storm-hdfs tests fail with out of memory periodically,"In our 2.x automated testing we have noticed that every so often we will see TestFileLock fail with out of memory errors.

{code}
java.lang.OutOfMemoryError: Java heap space
...
{code}

Which then appears to trigger other failures.  Not sure if there is a memory leak involved or if the tests really need 1.5GB of memory periodically."
STORM-2875,Hadoop-auth dependency in storm-core results in exception,"Storm-core 1.1.1 ships with hadoop-auth dependency for version 2.6.1 which results in exceptions while working with hadoop version 2.8.x and above due to changes in KerberosUtil class.


_java.lang.NoSuchMethodError: org.apache.hadoop.security.authentication.util.KerberosUtil.hasKerberosKeyTab(Ljavax/security/auth/Subject;)Z
	at org.apache.hadoop.security.UserGroupInformation.<init>(UserGroupInformation.java:715) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:925) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:873) ~[stormjar.jar:?]
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:740) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3472) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3462) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3304) ~[stormjar.jar:?]
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:476) ~[stormjar.jar:?]
	at org.apache.storm.hdfs.bolt.HdfsBolt.doPrepare(HdfsBolt.java:106) ~[stormjar.jar:?]
	at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.prepare(AbstractHdfsBolt.java:124) ~[stormjar.jar:?]
	at org.apache.storm.daemon.executor$fn__5030$fn__5043.invoke(executor.clj:793) ~[storm-core-1.1.1.jar:1.1.1]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.1.jar:1.1.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_151]_"
STORM-2874,Minor style improvements to backpressure code,"The WorkerBackpressureThreadTest uses looped sleep-and-check on an AtomicLong to verify that the callback is called after notifying the trigger. We should use a countdown latch for this instead.

The WorkerBackpressureThreadTest extends TestCase, which is a bad idea because it makes the JUnit 4 annotations non-functional."
STORM-2871,Performance optimizations for getOutgoingTasks ,"Task.getOutgoingTasks() is in critical messaging path. Two observed bottlenecks in it :

- Looking up HashMap 'streamToGroupers'. Need to look into converting HashMap into Array lookup ?
- [outTasks.addAll(compTasks)|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/daemon/Task.java#L139]  seems to be impacting throughput as well. Identified by .. running ConstSpoutNullBoltTopo with 1 spout & bolt paralllelism (no Ack) and replacing this line with hard coded logic to add the single known bolt's taskID. "
STORM-2870,FileBasedEventLogger leaks non-daemon ExecutorService which prevents process to be finished,"{code}
    private void setUpFlushTask() {
        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor();
        Runnable task = new Runnable() {
            @Override
            public void run() {
                try {
                    if(dirty) {
                        eventLogWriter.flush();
                        dirty = false;
                    }
                } catch (IOException ex) {
                    LOG.error(""Error flushing "" + eventLogPath, ex);
                    throw new RuntimeException(ex);
                }
            }
        };

        scheduler.scheduleAtFixedRate(task, FLUSH_INTERVAL_MILLIS, FLUSH_INTERVAL_MILLIS, TimeUnit.MILLISECONDS);
}
{code}

The code block initializes ExecutorService locally, which served threads are not daemons so it can prevent JVM to be exit successfully.

Moreover it should be considered as bad case: not labeling thread name. I observed the process hung and got jstack, but hard to know where is the root, because leaked thread has default thread name."
STORM-2869,KafkaSpout discards all pending records when adjusting the consumer position after a commit,"As part of the STORM-2666 fix the spout clears out waitingToEmit when the consumer position falls behind the committed offset during a commit. We only need to do it for the affected partition, and then only for the records that are behind the committed offset.

Also the validation check in emitOrRetryTuple is slightly too permissive, it should check whether the current record is behind the committed offset, not whether the consumer position is behind the committed offset."
STORM-2868,Address handling activate/deactivate in multilang module files,"The multilang modules in Javascript, ruby and python are missing activate and deactivate handling from the last multilang protocol change."
STORM-2867,Add Consumer lag metrics to Kafka Spout,
STORM-2866,ImportError: No module named shlex,"Hello Team,

Am trying to get Stormcrawler working on my Debian VM. I get this error when I do the following command:

root@demo76:/opt# storm

Traceback (most recent call last):

 File ""/opt/storm/apache-storm-1.1.0/bin/storm.py"", line 23, in <module>

 import shlex

ImportError: No module named shlex

 I have python 2.7.9 version. I tried a simple python program with shlex and it works fine but am not sure why the storm is not recognizing shlex. Can you please help?

Kind Regards,
Sai"
STORM-2865,KafkaSpout constructor with KafkaConsumerFactory shoud be public,"When we use custom implement of interface ""Deserializer""，KafkaSpout constructor can only use class ""KafkaConsumerFactoryDefault""，the method “configure” of Interface “Deserializer” will not be called。

We need change the ""KafkaSpout"" constructor with ""KafkaConsumerFactory"" to be public, so we can create custom custom implement of interface ""KafkaConsumerFactory""."
STORM-2864,Minor optimisation about trident kafka state,"Make TridentKafkaState a template class to eliminate warning messages in eclipse, and a minor optimisation that use StringBuilder.append instead of string concat operation."
STORM-2863,Some ras tests fail because of static resource caching,"We noticed that depending on the ordering of tests, some of the RAS tests can fail because of metrics being cached.  We really should clear this static cache before each test."
STORM-2862,"More flexible logging in multilang (Python, Ruby, JS)","We're running a Storm topology written in Python, using storm.py from storm-multilang. As well as human-readable logs, the topology is also configured to write JSON logs which are sent to ELK.

At the moment, when storm-core receives a ""log"" command, it outputs the pid, component name, and the message it received, like so:

{{ShellLog pid:<pid>, name:<component name> <message>}}

The code that does this is (currently) in [ShellBolt line 254|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/task/ShellBolt.java#L254] and [ShellSpout line 227|https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/spout/ShellSpout.java#L227].

As well as the pid and component name, it would be great to have the task ID, tuple ID, and the ID of the originating tuple - but this would make parsing the string even more laborious than it is now, and would make the default log message too long. 

Would it be possible to put contextual information like this in the [ThreadContext|https://logging.apache.org/log4j/2.x/manual/thread-context.html] instead? Then our JSON layout could read from the context instead of parsing the string, and human-readable logs could use ""%mdc"" in the PatternLayout format string."
STORM-2861,Explicit reference kafka-schema-registry-client,"storm-hdfs compile failure due to io.confluent.kafka.schemaregistry.client.* not found.
Changing the dependence from kafka-avro-serializer to kafka-schema-registry-client fixed this issue."
STORM-2859,NormalizedResources has some bugs in special cases where 0 of a resource is available.,
STORM-2858,Fix worker-launcher build,I got an error when building with -Pnative because GCC has marked asprintf as a function where you shouldn't ignore the return value. I'm guessing it's the same issue that's preventing Travis from building storm-core.
STORM-2853,Deactivated topologies cause high cpu utilization,"The issue is there is high cpu usage for deactivated apache storm topologies.  I can reliably re-create the issue using the steps below but I haven't identified the exact cause or a solution yet.

The environment is a storm cluster on which 1 topology is running (The topology is extremely simple, I used the exclamation example).  It is INACTIVE.  Initially there is normal CPU usage.  However, when I kill all topology JVM processes on all supervisors and let Storm restart them again, I find that some time later (~9 hours) the CPU usage per JVM process rises to nearly 100%.  I have tested an ACTIVE topology and this does not happen with it.  I have also tested more than one topology and observe the same results when they're in the INACTIVE state.

***Steps to re-create:***

 1. Run 1 topology on an Apache Storm cluster
 2. Deactivate it
 3. Kill **all** topology JVM processes on all supervisors (Storm will restart them)
 4. Observe the CPU usage on Supervisors rise to nearly 100% for all **INACTIVE** topology JVM processes.

***Environment***

Apache Storm 1.1.0 running on 3 VMs (1 nimbus and 2 supervisors).

Cluster Summary:

 - Supervisors: 2 
 - Used Slots: 2 
 - Available Slots: 38 
 - Total Slots: 40
 - Executors: 50 
 - Tasks: 50

the topology has 2 workers and 50 executors/tasks (threads).


***Investigation so far:***

Apart from being able to reliably re-create the issue, I have identified, for the affected topology JVM process, the threads using the most CPU.  There are 102 threads total in the process, 97 blocked, 5 IN_NATIVE.  The threads using the most CPU are identical and there are 23 of them (all in BLOCKED state):

    Thread 28558: (state = BLOCKED)
     - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
     - java.util.concurrent.locks.LockSupport.parkNanos(long) @bci=11, line=338 (Compiled frame)
     - com.lmax.disruptor.MultiProducerSequencer.next(int) @bci=82, line=136 (Compiled frame)
     - com.lmax.disruptor.RingBuffer.next(int) @bci=5, line=260 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue.publishDirect(java.util.ArrayList, boolean) @bci=18, line=517 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue.access$1000(org.apache.storm.utils.DisruptorQueue, java.util.ArrayList, boolean) @bci=3, line=61 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue$ThreadLocalBatcher.flush(boolean) @bci=50, line=280 (Interpreted frame)
     - org.apache.storm.utils.DisruptorQueue$Flusher.run() @bci=55, line=303 (Interpreted frame)
     - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
     - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
     - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)
     - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)
     - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)


I identified this thread by using `jstack` to get a thread dump for the process:
 

    jstack -F <pid> > jstack<pid>.txt

and `top` to identify the threads within the process using the most CPU:

    top -H -p <pid> "
STORM-2852,Dup log4j jars in storm-local,"After upgrading to 1.0.2, every time there was ""Could not initialize class org.apache.log4j.Log4jLoggerFactory"" error when deploying a topology, but nothing suspicious found against the dependency tree.

Turned out there are dup log4j implementation jars under storm lib dir
log4j-over-slf4j-1.6.6.jar
log4j-slf4j-impl-2.1.jar

After removing log4j-over-slf4j-1.6.6.jar from lib, the whole world is running well.

Could you someone help fix? It's quite tricky to figure out the problem"
STORM-2851,org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions sometimes throws ConcurrentModificationException,"Hello,

We have been running Storm 1.2.0 preview on our pre-production supervision system.
We noticed that in the logs of our topology to logs persistency in Hbase, we got the following exceptions (about 4 times in a 48 hours period):

java.util.ConcurrentModificationException at java.util.HashMap$HashIterator.nextNode(HashMap.java:1442) 
at java.util.HashMap$KeyIterator.next(HashMap.java:1466) 
at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:347) 
at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:320) 
at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:245) 
at org.apache.storm.daemon.executor$fn__4963$fn__4978$fn__5009.invoke(executor.clj:647) 
at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) 
at clojure.lang.AFn.run(AFn.java:22) 
at java.lang.Thread.run(Thread.java:748) 

It looks like there's something to fix here, such as making the map thread-safe, or managing the exclusivity of modification of this map at a caller level.

Note: this topology is using Storm Kafka Client spout with default properties (unlike other topologies we have based on autocommit). However, it's the one which deals with highest rate of messages (line of logs coming from about 10000 VMs, a nice scale test for Storm :))

Could it be fixed in Storm 1.2.0 final version?

Best regards,
Alexandre Vermeerbergen
"
STORM-2850,ManualPartitionSubscription assigns new partitions before calling onPartitionsRevoked,"ManualPartitionSubscription does partition assignment updates in the wrong order. It calls KafkaConsumer.assign, then onPartitionsRevoked and last onPartitionsAssigned. The order should be onPartitionsRevoked, then assign, then onPartitionsAssigned.

onPartitionsRevoked has to be called before we reassign partitions, because we try to commit offsets for the revoked partitions. If we try to commit to a partition the consumer is not assigned, it will throw an exception. The onRevoke, assign, onAssign order is also more in line with the javadoc for ConsumerRebalanceListener, which specifies that onRevoke should be called before the partition rebalance begins.
"
STORM-2848,[storm-sql] Separate the concept of STREAM and TABLE,"Currently we only support STREAM type of table, and don't provide type selection for users.

We have future plan of supporting join on stream and table, which requires tables to be defined either stream or table, because left join which left side is table and right side is stream doesn't make sense and vice versa.

This issue tracks the effort of separating STREAM and TABLE. We may want to also apply the constraint that only STREAM supports SELECT statement."
STORM-2847,Exception thrown after rebalance IllegalArgumentException,"After rebalance the storm-kafka-client spout attempts to check the current position of partitions that are no longer assigned to the current spout. This occurs in a topology with multiple spout instances.

java.lang.IllegalArgumentException: You can only check the position for partitions assigned to this consumer. at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1262) at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:473)"
STORM-2845,Drop standalone mode of Storm SQL,"Quoting discussion again:

{quote}
We have been exposing ""standalone mode"" of Storm SQL which leverages Storm SQL in a JVM process rather than composing topology and run.
At a start we implemented both standalone and trident modes with same approach, but while we improved Storm SQL by leveraging more features on Calcite, we addressed only trident mode, and now twos are diverged.

I guess there is likely no actual user on standalone mode since its classes are exposed but we didn't document it. I know a case, but the source codes on standalone mode code are migrated to the project (and modified to conform to the project) and the project no longer depends on Storm SQL.

If we all don't have any other case, how about dropping it and only concentrate to trident mode?
(Btw, I'm trying to replace the backend on Storm SQL from Trident to Streams API, which may make the mode name obsolete, but after dropping standalone mode we don't even need the name for mode since there will be only one mode.)
{quote}

Discussion link:
https://mail-archives.apache.org/mod_mbox/storm-dev/201712.mbox/%3CCAF5108g3yEQPWO-UPQaJmoqkN4%2BoZgnH64pvKs2ARju4ySUB4Q%40mail.gmail.com%3E
"
STORM-2844,KafkaSpout Throws IllegalStateException After Committing to Kafka When First Poll Strategy Set to EARLIEST,"This [code|https://github.com/apache/storm/blob/1.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L407-L409], which was committed to fix [STORM-2666|https://issues.apache.org/jira/browse/STORM-2666] throws IllegalStateException when the KafkaSpout commits to Kafka and is restarted with the same consumer group id and first poll strategy is set to EARLIEST.

For example consider the following sequence:
# KafkaSpout with consumer_group_id=TEST polls and commits offsets 1-5 
# KafkaSpout with consumer_group_id=TEST is restarted with first poll strategy set to EARLIEST

==> IllegalStateException will be thrown

This bug could be a blocker. I am setting it to Critical because assigning a different consumer id serves as a workaround to the problem.

"
STORM-2843,Flux: properties file not found when loading resources from classpath,"Scenario: The CI auto-build a fat jar including filter properties file and topology yaml file. Filter properties file not found when load resources from classpath.

example:
{code}storm jar myTopology-0.1.0-SNAPSHOT.jar org.apache.storm.flux.Flux --remote --resource --filter dev.properties my_config.yaml{code}

The dev.properties file cannot be found in classpath.

After this patch, the FluxParser will load filter properties file as same as the way of topology yaml.
"
STORM-2842,Fixed links for YARN&Kubernetes Integration,
STORM-2837,RAS Constraint Solver Strategy,"We have a use case where a user has some old native code and they need it to work with storm, but sadly the code is not thread safe so they need to be sure that each instance of a specific bolt is in a worker without other instances of the same bolt.  It also cannot co-exist with other bolts for a similar reason.  I know that this is a fairly strange use case, but to help fix the issue we wrote a strategy for RAS that can do a simple search of the state space trying to honor these constrains and we thought it best to push it back then to keep it internal."
STORM-2835,storm-kafka-client KafkaSpout can fail to remove all tuples from waitingToEmit,
STORM-2833,Cached Netty Connections can have different keys for the same thing.,"It turns out that if you set {{storm.local.hostname}} on your supervisors that the netty caching code might not work.  The issue is that when we go to add a netty connection to the cache we use the host name provided by the scheduling.  Which ultimately comes from the {{storm.local.hostname}} setting on each of the nodes.  But when we go to remove it from the cache, we use the resolved INetSocket address for the destination.  If the two do not match exactly then we can close a connection, but not have it removed from the cache, so when we go to try and use it again, the connection is closed."
STORM-2830,Upgrade Jackson to 2.9.2,"We recently hit an issue (https://issues.apache.org/jira/browse/STORM-2829) because Jackson can't serialize some Java 7 classes. In order to avoid this kind of problem in the future, I think we should upgrade Jackson.

Jackson has had support for the JDK7 additions since 2.8.0, so upgrading past this point should prevent this kind of bug from popping up again."
STORM-2829,Logviewer deepSearch not working,"
{code:java}
2017-11-21 21:06:19.369 o.e.j.s.HttpChannel qtp1471948789-17 [WARN] /api/v1/deepSearch/wc-1-1511188542
javax.servlet.ServletException: java.lang.RuntimeException: com.fasterxml.jackson.databind.JsonMappingException: Direct self-reference leading to cycle (through reference chain: org.apache.storm.daemon.logviewer.handler.Matched[""matches""]->java.util.ArrayList[0]->java.util.HashMap[""port""]->sun.nio.fs.UnixPath[""fileSystem""]->sun.nio.fs.LinuxFileSystem[""rootDirectories""]->sun.nio.fs.UnixPath[""root""])
        at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:489) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228) ~[jersey-container-servlet-core-2.24.1.jar:?]
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841) ~[jetty-servlet-9.4.7.v20170914.jar:9.4.7.v20170914]
{code}
"
STORM-2827,Logviewer search returns incorrect logviewerUrl,"Code in LogviewerLogSearchHandler
{code:java}
  @VisibleForTesting
    String urlToMatchCenteredInLogPage(byte[] needle, Path canonicalPath, int offset, Integer port) throws UnknownHostException {
        final String host = Utils.hostname();
        final Path truncatedFilePath = truncatePathToLastElements(canonicalPath, 3);

        Map<String, Object> parameters = new HashMap<>();
        parameters.put(""file"", truncatedFilePath.toString());
        parameters.put(""start"", Math.max(0, offset - (LogviewerConstant.DEFAULT_BYTES_PER_PAGE / 2) - (needle.length / -2)));
        parameters.put(""length"", LogviewerConstant.DEFAULT_BYTES_PER_PAGE);

        return UrlBuilder.build(String.format(""http://%s:%d/api/v1/log"", host, port), parameters);
    }

    @VisibleForTesting
    String urlToMatchCenteredInLogPageDaemonFile(byte[] needle, Path canonicalPath, int offset, Integer port) throws UnknownHostException {
        final String host = Utils.hostname();
        final Path truncatedFilePath = truncatePathToLastElements(canonicalPath, 1);

        Map<String, Object> parameters = new HashMap<>();
        parameters.put(""file"", truncatedFilePath.toString());
        parameters.put(""start"", Math.max(0, offset - (LogviewerConstant.DEFAULT_BYTES_PER_PAGE / 2) - (needle.length / -2)));
        parameters.put(""length"", LogviewerConstant.DEFAULT_BYTES_PER_PAGE);

        return UrlBuilder.build(String.format(""http://%s:%d/api/v1/daemonlog"", host, port), parameters);
    }
{code}
only returns http url. This url will be invalid if logviewer https port is configured, in which case the http url will be not found"
STORM-2826,KafkaSpoutConfig.builder doesn't set key/value deserializer properties in storm-kafka-client,"STORM-2548 replaced the KafkaSpoutConfig.builder() implementations with ones that don't set the key/value deserializer fields in KafkaSpoutConfig, but instead just sets the corresponding property in the kafkaProps map. This is a breaking change for applications that assume those properties are set after the builder is created.

Code like the following would break.
{quote}
this.keyDeserializer = config.getKeyDeserializer().getClass();
this.valueDeserializer = config.getValueDeserializer().getClass();
{quote}"
STORM-2825,"storm-kafka-client configuration fails with a ClassCastException if ""enable.auto.commit"" is present in the consumer config map, and the value is a string","{quote}
Exception in thread ""main"" java.lang.ClassCastException: java.lang.String
cannot be cast to java.lang.Boolean
        at
org.apache.storm.kafka.spout.KafkaSpoutConfig.setAutoCommitMode(KafkaSpoutConfig.java:721)
        at
org.apache.storm.kafka.spout.KafkaSpoutConfig.<init>(KafkaSpoutConfig.java:97)
        at
org.apache.storm.kafka.spout.KafkaSpoutConfig$Builder.build(KafkaSpoutConfig.java:671)
{quote}"
STORM-2824,Ability to configure topologies for exactly once processing,"The default implementation of a spout  (Kafka) is to wait for acknowledgement, if an acknowledgement is not provided the tuple is replayed leading to an at least once processing model.

Can an option be provided to always acknowledge even in the event of error in any spout or bolt and the user decide which mode the topology should be configured.

There are cases like multiple bolts (B) inserting to persistent stores (PS) like B1 - PS1, B2-PS2, B3-PS3, the fact that B2-PS2 bolt fail doesn't mean that the tuple needs to be replayed leading to complexity on the logic of bolts, it would be easier if this was configurable and the user of the topology decides which style to choose.
"
STORM-2823,Ability to have an option to combine topologies at run time in a single process space,"Unlike an API server which service multiple APIs within the same process space, the Topologies needs to run in separate processes.
Lets say we have Topology TP-1 which use 1 GB of memory.
Now we create the same for n Topologies of TP-1..... TP-n
As the topologies increase the memory allocation is now multiplied by the number of topologies.
This design though scalable is not similar to the API route we have before which was within the same process space.

So in a micros services world, each topology would be responsible for a similar set of objects, like employee, customer, product, order, order details etc.

As the number of topologies increase the worker allocation is not sufficient. Most topologies are not utilized fully but since these are in different process space the memory allocated can't be used.

If we have an ability to say that TP-1 --- TP 10 Can run within the same process space but behave like individual topologies we could conserve the resource usage.

Now user are forced to combine topologies to the hardware provided with ""if"" logics to route the correct object that needs to be processed.

This way one can still configure topologies as API in the same API server and reuse resources collectively for related group of topologies acting as micro services.




"
STORM-2822,Remove LinearDRPCTopologyBuilder,We should look into removing the deprecated LinearDRPCTopologyBuilder ibn 2.0.0.  But we need to make sure first that all of the use cases for it are clearly documented and covered by the other ways of using DRPC.
STORM-2821,Remove TransactionalTopologySupport.,"Transactional Topologies have been in storm for a long time, but Trident overrides it and the former is deprecated.  We should remove all of the code that supports it and from the code base."
STORM-2819,Ability to natively support JSON serialization in topologies,Now that the world is moving towards NoSQL and most of the data is in JSON. Can a native JSON Serializer be implemented similar to support for Kryo. 
STORM-2818,Storm UI doesn't show which version of the code was used to run the topology,"Lets say we create a Topology 1 with version 1 of the uber Jar namely Topololog1V1.jar
We submit this jar and the Topology1 is shown in Storm UI

We then do changes and now have another version of the Topology 1 which is Topology1V2.jar 
We submit this jar and the Topology1 is again shown in the Storm UI

In both case we can't find which version of the code are we running without actually logging in to the Storm Supervisor instance and find the process start parameters.

Can this start parameters be made available in the UI so that we can easily find which version of the code are we running for the topology?
"
STORM-2817,Topology Restart Counts are not maintained in Storm UI,"On the Storm UI, we need an ability to have a Topology Submission Time, Topology Uptime as well as how many times a Topology worker process has restarted since last Submission.

The reason been, lets say we have a Supervisor with 8 GB RAM.
We also have 4 Slots on this Supervisor.
We submit 4 Topologies each with worker memory of 3 GB leading to a total of 12GB / 8 GB utilization assuming not all topologies would use up all the memory at the same time.

Now, we find that topologies are dying behind the scenes due to out of memory and Storm Nimbus keeps restarting these topologies again.

The uptime requests as part of  [STORM-2816] (https://issues.apache.org/jira/browse/STORM-2816) we can address the uptime but it still won't say we have a deeper issue and the topologies are restarting behind the scene. Adding this counter would help to flag issues.

The counts should be at both per topology level like

Topology 1 
     Submission Time T1
     Uptime T2
     Restarts 4 (Possible log links to why restarted)

The other should be at the Storm UI level

Total Topologies : 20
Total Topologies Restart since Submission : 12 (Possible links to topologies that got restarted)

This way monitoring and alerting systems can hook into these counts and alert when things go wrong.
"
STORM-2816,Topology Summary Uptime is not reflecting worker restarts,"The Storm UI , Topology Summary Uptime is not reflecting the Worker Process restarts and always gives the initial topology submission uptime.

So if we submitted the Topology1 at time T1
The topology was running in 1 worker instance.
If the worker instance goes down at time T2 and a new worker instance was started at time T3.
The uptime always show time T1 and not starting with time T3

We might need to split the Topology Submission Time and Topology Uptime

"
STORM-2813,Clean up RAS resource Map.,"Under the new Generic RAS code we use a Map<String, Number> or Map<String, Double> for the resources.  This is really inefficient and we should look at normalizing the Maps into an array, which will be faster, and hopefully will make the code cleaner."
STORM-2811,"Nimbus may throw NPE if the same topology is killed multiple times, and the integration test kills the same topology multiple times","{quote}
2017-11-12 08:45:50.353 o.a.s.d.n.Nimbus pool-14-thread-47 [WARN] Kill topology exception. (topology name='SlidingWindowTest-window20-slide10')
java.lang.NullPointerException: null
	at org.apache.storm.cluster.IStormClusterState.getTopoId(IStormClusterState.java:171) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConfFromName(Nimbus.java:1970) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.nimbus.Nimbus.killTopologyWithOpts(Nimbus.java:2760) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3226) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3210) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.10.0.jar:0.10.0]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.10.0.jar:0.10.0]
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:167) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[libthrift-0.10.0.jar:0.10.0]
	at org.apache.thrift.server.Invocation.run(Invocation.java:18) ~[libthrift-0.10.0.jar:0.10.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
{quote}"
STORM-2810,Storm-hdfs tests are leaking resources,"The Storm-hdfs tests are leaking resources, and it seems to be making the tests fail. "
STORM-2809,Integration test is failing consistently and topologies sometimes fail to start workers,"The integration test has been failing fairly consistently since https://github.com/apache/storm/pull/2363. I tried running the test outside a VM with a locally installed Storm setup, and it has failed every time for me.

Most runs seem to fail in ways that make it look like the integration test is just flaky (e.g. tuple windows not matching the calculated window), but in at least a few tests I saw the topology get submitted to Nimbus followed by about 3 minutes of nothing happening. The workers never started and the supervisor didn't seem aware of the scheduling. The only evidence that the topology was submitted was in the Nimbus log. This still happens even if the test topologies are killed with a timeout of 0, so there should be slots free for the next test immediately.

I tried reverting https://github.com/apache/storm/pull/2363 and it seems to make the integration test pass much more often. Over 5 runs there was still an instance of a supervisor failing to start the workers, but the other 4 passed.

We should try to fix whatever is causing the supervisor to fail to start workers, and get the integration test more stable."
STORM-2807,Integration test should shut down topologies immediately after the test,"The integration test kills topologies with the default 30 second timeout. This is unnecessary and delays the following tests, because the killed topology is still occupying worker slots.

When the integration test kills topologies, it tries sending the kill message to Nimbus once, and may fail quietly. This breaks following tests, because the default Storm install has only 4 worker slots, and the test topologies each take up 3. When a topology is not shut down, it prevents the following topologies from being assigned."
STORM-2803,SlotTest failing on travis frequently,"I have seen SlotTest fail way too frequently on travis, but it never fails off of travis.

My guess is that there is some kind of a race condition happening and on slower hardware (aka VMs or Containers on overloaded build machines) that the tests tend to fail.

I'll try to find some time to look at this, but if someone else wants to steal it from me feel free to.  I don't know exactly when I will find time to do it."
STORM-2802,Storm-cassandra tests don't run on JDK 9,"The storm-cassandra tests don't run on JDK 9. 

{quote}
opaqueStateTest(org.apache.storm.cassandra.trident.MapStateTest)  Time elapsed: 0.627 sec  <<< ERROR!
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [localhost/127.0.0.1:9042] Error writing))
        at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:233)
        at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:79)
        at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1473)
        at com.datastax.driver.core.Cluster.init(Cluster.java:159)
        at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:330)
        at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:305)
        at com.datastax.driver.core.Cluster.connect(Cluster.java:247)
        at org.apache.storm.cassandra.trident.MapStateTest.setUp(MapStateTest.java:163)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:564)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
        at org.junit.rules.RunRules.evaluate(RunRules.java:20)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.junit.runners.Suite.runChild(Suite.java:127)
        at org.junit.runners.Suite.runChild(Suite.java:26)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
        at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
        at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
        at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
        at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
        at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)

{quote}

Java 9 support for Cassandra is tracked at https://issues.apache.org/jira/browse/CASSANDRA-9608"
STORM-2801,Storm-Hive tests don't run on JDK 9,"The Storm-Hive tests error out when running on JDK 9. 

{quote}
java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444)
	at org.apache.storm.hive.bolt.TestHiveBolt.<init>(TestHiveBolt.java:110)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:195)
	at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:244)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:241)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1449)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:63)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2661)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2680)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425)
	... 34 more
Caused by: java.lang.reflect.InvocationTargetException
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:488)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447)
	... 39 more
Caused by: javax.jdo.JDOFatalInternalException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
NestedThrowables:
org.datanucleus.exceptions.NucleusException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:591)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664)
	at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574)
	at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98)
	at com.sun.proxy.$Proxy27.verifySchema(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:178)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:73)
	... 44 more
Caused by: org.datanucleus.exceptions.NucleusException: The java type java.lang.Long (jdbc-type="""", sql-type="""") cant be mapped for this datastore. No mapping is available.
	at org.datanucleus.store.rdbms.mapping.RDBMSMappingManager.getDatastoreMappingClass(RDBMSMappingManager.java:1215)
	at org.datanucleus.store.rdbms.mapping.RDBMSMappingManager.createDatastoreMapping(RDBMSMappingManager.java:1378)
	at org.datanucleus.store.rdbms.table.AbstractClassTable.addDatastoreId(AbstractClassTable.java:392)
	at org.datanucleus.store.rdbms.table.ClassTable.initializePK(ClassTable.java:1087)
	at org.datanucleus.store.rdbms.table.ClassTable.preInitialize(ClassTable.java:247)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTable(RDBMSStoreManager.java:3118)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTables(RDBMSStoreManager.java:2909)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3182)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	... 62 more
{quote}

Someone asked about this error on Stack Overflow (https://stackoverflow.com/questions/43086904/error-with-nucleusexception-and-jdofatalexception-when-starting-hive), and it sounds like we'll need to wait for Hive to release a JDK 9 compatible version (https://issues.apache.org/jira/browse/HIVE-17632)"
STORM-2800,Use JAXB api dependency from Maven instead of relying on that API being available in the standard JDK,JDK 9 doesn't expose the javax.xml.bind package by default anymore. We should use the Maven package to get the APIs instead.
STORM-2799,Ensure jdk.tools is not being included transitively since tools.jar doesn't exist in JDK 9 and we don't need it.,"A few of our dependencies are leaking a jdk.tools dependency to us from hbase-annotations and hadoop-annotations. It seems like those projects use jdk.tools to run a custom doclet for generating their own Javadoc. We shouldn't need jdk.tools since we don't run custom doclets, and tools.jar doesn't exist in JDK 9."
STORM-2798,Build Storm with JDK 11,"Track what we need to do to make Storm build on Java 11 (i.e. fix issues introduced in Java 9, 10 and 11)."
STORM-2797,LogViewer worker logs broken on Windows,"LogViewer worker logs are broken on Windows. Attempting to access the log (e.g. http://localhost:8000/log?file=word-topo-5-1509750559%5C6701%5Cworker.log) leads to a 500 Server Error.

I've attached the LogViewer logs which show the stack trace. The issue is pretty clear from the log: on line 123 of logviewer.clj, the path is split using the path separator as a regex. This is fine on Posix systems as / is a normal character in regex; however, on Windows, backslash is the path separator. As this is also the regex escape character, it is not a valid regular expression."
STORM-2796,Flux: Provide means for invoking static factory methods and improve non-primitive number handling,"Provide a means to invoke static factory methods for flux components. E.g:

Java signature:
{code}
public static MyComponent newInstance(String... params)
{code}

Yaml:

{code}
    className: ""org.apache.storm.flux.test.MyComponent""
    factory: ""newInstance""
    factoryArgs: [""a"", ""b"", ""c""]
{code}

Also include a fix for non-primitive numbers, so constructs like the following work:

Java constructor:
{code}
public TestBolt(Long l){}
{code}

Yaml:
{code}
  - id: ""bolt-4""
    className: ""org.apache.storm.flux.test.TestBolt""
    constructorArgs:
      - 10
    parallelism: 1
{code}

(Before fix the above would fail because snakeyaml would convert `10` to an Integer.)
"
STORM-2795,Race in downloading resources can cause failure,Recently had a failure/hang in the async localizer test.  Turns out that there is a race when downloading dependencies and there is a race in trying to create the parent directory.
STORM-2788,supervisor.worker.version.classpath.map should support regex,"To enable 0.10 (or other version) topology to run on 2.x cluster, we need to set supervisor.worker.version.classpath.map and something else. But now the classpath.map doesn't support regex yet. We have to list all the jar file path.

{code:java}
supervisor.worker.version.classpath.map:
    0.10.2.y: ""/home/y/lib64/ystorm_compatibility/current/lib/asm-4.0.jar:/home/y/lib64/ystorm_compatibility/current/lib/auth_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/bcpkix.jar:/home/y/lib64/ystorm_compatibility/current/lib/bcprov.jar:/home/y/lib64/ystorm_compatibility/current/lib/bouncer_auth_java.jar:/home/y/lib64/ystorm_compatibility/current/lib/clojure-1.6.0.jar:/home/y/lib64/ystorm_compatibility/current/lib/data_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/disruptor-3.3.2.jar:/home/y/lib64/ystorm_compatibility/current/lib/junixsocket.jar:/home/y/lib64/ystorm_compatibility/current/lib/kryo-2.21.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-1.2-api-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-api-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-core-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/log4j-slf4j-impl-2.1.jar:/home/y/lib64/ystorm_compatibility/current/lib/servlet-api-2.5.jar:/home/y/lib64/ystorm_compatibility/current/lib/sia_java_client.jar:/home/y/lib64/ystorm_compatibility/current/lib/slf4j-api-1.7.7.jar:/home/y/lib64/ystorm_compatibility/current/lib/storm-core-0.10.2.y.jar:/home/y/lib64/ystorm_compatibility/current/lib/storm_yahoo-0.10.2.y.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_byauth.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_filter_logic.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_servlet.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_servlet_filters.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_yca.jar:/home/y/lib64/ystorm_compatibility/current/lib/yjava_ysecure.jar:/home/y/lib64/ystorm_compatibility/current/lib/zts_core.jar:/home/y/lib64/ystorm_compatibility/current/lib/zts_java_client.jar:""
{code}

We want to have the following configs working.

{code:java}
supervisor.worker.version.classpath.map:
    0.10.2.y: ""/home/y/lib64/ystorm_compatibility/current/lib/*""
{code}
"
STORM-2787,storm-kafka-client KafkaSpout should set 'initialized' flag independently of processing guarantees,"Currently the method 


{code:java}
public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
{code}

has the following condition

{code:java}
if (isAtLeastOnceProcessing() && initialized) {
                initialized = false;
                ...
}
{code}

initialized should be set to false independently of the processing guarantee"
STORM-2786,Ackers leak tracking info on failure and lots of other cases.,"Over the weekend we had an incident where ackers were running out of memory at a really scary rate.  It turns out that they were having a lot of failures, for an unrelated reason, but each of the failures were resulting in tuple tracking being lost because... 

We don't send ticks to any system components ever...

https://github.com/apache/storm/blob/124acb92dff04a57b530ab4d95a698abc8ff46d9/storm-client/src/jvm/org/apache/storm/executor/Executor.java#L384

and ackers are system components.

So the tracking map was never rotated and all failed tuples

https://github.com/apache/storm/blob/124acb92dff04a57b530ab4d95a698abc8ff46d9/storm-client/src/jvm/org/apache/storm/daemon/Acker.java#L97-L103

Were never deleted from the map.

This leak eventually made the ackers crash, and when they came back up the other components kept blasting them with messages that would never be fully acked which also leaked because of the tick problem.

Looking back this has been in every release since 0.9.1-incubating.  It appears to have been introduced by https://github.com/apache/storm/commit/483ce454a3b2cd31b5d1c34e9365346459b358a8

So every apache release has this problem (which is the only reason I have not marked this as a blocker, because apparently it is not so bad that anyone has noticed in the past 4 years)."
STORM-2784, storm-kafka-client KafkaTupleListener method onPartitionsReassigned() should be called after initialization is complete,
STORM-2783,"De-couple ""Spout Lag"" metrics on Storm UI from Kafka Spout and StormKafkaMonitor","As a developer of a spout, I'd love to be able to publish lag metrics to the storm UI.  After digging into the source code for how the UI interacts with storm-kafka-monitor to get these metrics, it appears to be strongly coupled.  I believe that the concept of ""Spout Lag"" extends beyond the scope of just consuming from Kafka. 

I'd like to propose the idea of restructuring how these metrics are queried by StormUI to a way that allows developers of other spouts to be able to ""plug into"" the UI.  The easiest way that springs to mind is to provide an interface that allows developers to code against.

"
STORM-2781,Refactor storm-kafka-client KafkaSpout  Processing Guarantees,
STORM-2779,NPE on shutting down WindowedBoltExecutor,"STORM-2724 introduced a bug on WindowedBoltExecutor which throws NPE when shutting down WindowedBoltExecutor which has waterMarkEventGenerator field as null.

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java#L330

"
STORM-2775,Improve KafkaPartition Metric Names,"The _storm-kafka_ `KafkaSpout` emits a metric group called _kafkaPartition_

These metric names are prefixed with 
{noformat}
Partition{host=some.broker.host.mycompany.com:9092,-topic=some/topic/name,-partition=40}
{noformat}

Which makes for ugly, difficult to discover metrics on systems like Graphite.

The metric prefix should match the metrics emitted by the _kafkaOffset_ metric group that look like:

_topicName/partition_<partitionNum>_"
STORM-2774,Workers get killed with FileNotFoundException on stormjar.jar,"Hi,

 Worker processes sometimes get killed unable to find stormjar.jar in tmp directory. The stacktrace looks as below. 

10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO] Caused by: java.lang.RuntimeException: Provider for class javax.xml.parsers.DocumentBuilderFactory cannot be c
reated
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:308)
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO]       ... 85 more
10.0.0.113 2017-10-12 10:28:33.657 STDERR Thread-1 [INFO] Caused by: java.util.ServiceConfigurationError: javax.xml.parsers.DocumentBuilderFactory: Error reading config
uration file
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.fail(ServiceLoader.java:232)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.parse(ServiceLoader.java:309)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.access$200(ServiceLoader.java:185)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:293)
10.0.0.113 2017-10-12 10:28:33.658 STDERR Thread-1 [INFO]       at java.security.AccessController.doPrivileged(Native Method)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       ... 85 more
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO] Caused by: java.io.FileNotFoundException: /var/log/storm/tmp/supervisor/stormdist/R2Topology-80-1507783551/stormjar.jar (No such file or directory)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.open(Native Method)
10.0.0.113 2017-10-12 10:28:33.659 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.<init>(ZipFile.java:219)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.zip.ZipFile.<init>(ZipFile.java:149)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.jar.JarFile.<init>(JarFile.java:166)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at java.util.jar.JarFile.<init>(JarFile.java:103)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.URLJarFile.<init>(URLJarFile.java:93)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)
10.0.0.113 2017-10-12 10:28:33.660 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at java.net.URL.openStream(URL.java:1045)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       at java.util.ServiceLoader.parse(ServiceLoader.java:304)
10.0.0.113 2017-10-12 10:28:33.661 STDERR Thread-1 [INFO]       ... 92 more"
STORM-2773,"If a drpcserver node in cluster is down,drpc cluster won't work if we don't modify the drpc.server configuration and restart the cluster","There is a cluster which includes three nodes named storm1,storm2,storm3.And there is a drpcserver in every node,a worker which has been started on strom1.When strom1 was down with hardware failure,my drpc topology won't work,when I send request from drpcclient.
As storm1 was down,so the worker will be restarted on another node,but it can't Initialize successfully because the call method of Adder will throw a RuntimeException,when drpcspout try to connect to storm1,so the worker will restart again. 

In conclusion,If a drpcserver node in cluster is down,drpc cluster won't work until we modify the drpc.server configuration and restart the cluster,but in production,it's difficult to restart whole cluster.

So I think we should catch the RuntimeException and log it,and the drpc topology will work normally."
STORM-2772,"In the DRPCSpout class, when the fetch from the DRPC server fails, the log should return to get the DRPC request failed instead of getting the DRPC result failed","In the DRPCSpout class, when the fetch from the DRPC server fails, the log error should return to get the DRPC request failed instead of getting the DRPC result failed.
for example, in line 216 of DRPCSpout class,
  LOG.error(""Not authorized to fetch DRPC result from DRPC server"", aze);
this should be modified to 
 LOG.error(""Not authorized to fetch DRPC request from DRPC server"", aze);"
STORM-2771,Some tests are being run twice,"I noticed that at least for storm-servier, and possibly others, that we have both the surefire plugin and the failsafe plugin.  Both of these run the unit tests.  They run them in slightly different ways, but we have not configured them to be exclusive, so most of the time we are running the storm-server tests twice."
STORM-2767,Surefire now truncates too much of the stack trace,"Surefire is truncating so much of the stack trace when tests fail that we often can't easily spot the error. As an example I manually threw an NPE from storm-kafka-client's KafkaSpout.commit() method, and here are the stack traces with trimStackTrace enabled and disabled:

trimmed
{code}
testCommitSuccessWithOffsetVoids(org.apache.storm.kafka.spout.KafkaSpoutCommitTest)  Time elapsed: 0.714 sec  <<< ERROR!
java.lang.NullPointerException: This is an NPE from inside nextTuple
	at org.apache.storm.kafka.spout.KafkaSpoutCommitTest.testCommitSuccessWithOffsetVoids(KafkaSpoutCommitTest.java:87)
{code}

not trimmed
{code}
testCommitSuccessWithOffsetVoids(org.apache.storm.kafka.spout.KafkaSpoutCommitTest)  Time elapsed: 0.78 sec  <<< ERROR!
java.lang.NullPointerException: This is an NPE from inside nextTuple
	at org.apache.storm.kafka.spout.KafkaSpout.commit(KafkaSpout.java:266)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:235)
	at org.apache.storm.kafka.spout.KafkaSpoutCommitTest.testCommitSuccessWithOffsetVoids(KafkaSpoutCommitTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.junit.runners.Suite.runChild(Suite.java:127)
	at org.junit.runners.Suite.runChild(Suite.java:26)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junitcore.JUnitCore.run(JUnitCore.java:55)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:137)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:107)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:83)
	at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:75)
	at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:161)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:290)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:242)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:121)
{code}

Note how the trimmed stack trace is also removing the trace lines from inside KafkaSpout.

As part of fixing https://issues.apache.org/jira/browse/STORM-2734 we upgraded to Surefire 2.19.1. It seems like 2.19 switched to a different interpretation of trimStackTrace, which trims all lines outside the test. It's my impression that it used to only trim lines before the trace reached a line inside the test. Going by https://issues.apache.org/jira/browse/SUREFIRE-1226?focusedCommentId=15140710&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15140710, this change seems intentional.

We should either downgrade Surefire, or disable stack trace trimming."
STORM-2764,HDFSBlobStore leaks file system objects,"This impacts all of the releases.  Each time we create a new HDFSBlobStore instance we call 

https://github.com/apache/storm/blob/v1.0.0/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java#L140

loginUserFromKeytab.

This results in a new subject being created each time when ends up causing a FileSystem object to leak each time."
STORM-2762,Sort out multiple places of codes of manipulation of collections,"Storm has codes of manipulation of collections at multiple places. One of example would be blacklist/Sets.java which will be merged in via STORM-2083.
https://github.com/apache/storm/pull/2343

It would be better to sort out and make them common utils, or if possible, replace them with specific library Storm already depends on."
STORM-2756,STORM-2548 on 1.x-branch broke setting key/value deserializers with the now deprecated setKey/setValue methods,"When STORM-2548 was backported, the setKey/setValue methods on KafkaSpoutConfig.Builder were deprecated, and users were directed to use setProp along with the relevant ConsumerConfig constants for setting deserializers instead.

As part of this change, the KafkaConsumerFactoryDefault switched from using the KafkaConsumer(props, keyDes, valDes) constructor to using the KafkaConsumer(props) constructor. Unfortunately I forgot to update the KafkaSpoutConfig.Builder constructor properly, so if the user configures the deserializer via either the Builder constructor parameters or setKey/setValue, the setting is not put in the kafkaProps map and the deserializer is not used."
STORM-2752,Nimbus crashes silently if scheduler is not found,"When nimbus is started and the custom scheduler specified in storm.yaml is not in the classpath, nimbus hangs and exits with status 13 about 10s later. No errors are logged.

Affected versions 1.0.3-5, I did not test any other. OpenJDK 8."
STORM-2750,fix double_checked locking,"update HBaseSecurityUtil singleton to fix double_checked locking

Double-Checked Locking is widely cited and used as an efficient method for implementing lazy initialization in a multithreaded environment.
Unfortunately, it will not work reliably in a platform independent way when implemented in Java, without additional synchronization. When implemented in other languages, such as C++, it depends on the memory model of the processor, the reorderings performed by the compiler and the interaction between the compiler and the synchronization library. Since none of these are specified in a language such as C++, little can be said about the situations in which it will work. Explicit memory barriers can be used to make it work in C++, but these barriers are not available in Java.
See url link for details: http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
"
STORM-2748,TickTupleTest is useless,"The test starts up a small topology on a simulated time cluster with TOPOLOGY_TICK_TUPLE_FREQ_SECS set to 1.  Then it simulates 2 seconds of cluster time.  This is not enough time to even launch the topology.  How do I know this?  Because the Bolt and Spout in the topology override `writeObject` so the resulting serialized bolt and spout are empty and trying to deserialize them results in an exception.

Just running a topology that does nothing and never verifies that the ticks showed up is a really horrible test.  We should either delete it entirely or actually verify that ticks are showing up once a second.  I am leaning towards just removing it totally."
STORM-2747,"Make the windowing classes use long instead of int for time parameters, and rename millisecond based time parameters so it's clear which unit they are.","Some parameters in the windowing classes are using int instead of long for describing time, which requires a bunch of casting to/from long in the code. Some variables are always in milliseconds but are named e.g. ""value"", which makes it easy to accidentally use the wrong timeunit. Such variables should be named e.g. ""valueMs"" to disambiguate the unit."
STORM-2746,Max Open Files does not close files for the oldest entry,"Description:

AbstractHDFSBolt has WritersMap. This evicts least recently used AbstractHDFSWriter out of the writers map, however, does not close the file in open state by the oldest entry.

Steps to reproduce  error: 

1) Use new Max open files feature and set the value to 1.
2) Write data to two or three different files in hdfs using AvroBolt.
3) Check output directory using fsck in hdfs.
   
Expected: only one file open in output directory.
Actual: > 1 files are in open state."
STORM-2745,Hdfs Open Files problem,"Issue:

Problem exists when there are multiple HDFS writers in writersMap. Each writer keeps an open hdfs handle to the file. Incase of Inactive writer(i.e. one which is not consuming any data from long period), the files are not closed and always remain in open state.

Ideally, these files should get closed and Hdfs writers removed from the WritersMap.

Solution:
Implement a ClosingFilesPolicy that is based on Tick tuple intervals. At each tick tuple all Writers are checked and closed if they exist for a long time."
STORM-2739,Storm UI fails to bind to ui.host when using https,"When using https with the Storm UI, it ignores the value of ui.host, and binds to 0.0.0.0.

Starting with this config:


{code}
storm.local.dir: ""/opt/storm""
storm.zookeeper.servers:
    - ""bigstorm.porcupineracing.com""
nimbus.seeds: [""bigstorm.porcupineracing.com""]
nimbus.childopts: ""-Xmx1024m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
ui.childopts: ""-Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
supervisor.childopts: ""-Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf""
storm.thrift.transport: ""org.apache.storm.security.auth.kerberos.KerberosSaslTransportPlugin""
java.security.auth.login.config: ""/keytabs/jaas.conf""
storm.zookeeper.superACL: ""sasl:storm@PORCUPINERACING.COM""

ui.host: 127.0.0.1

nimbus.authorizer: ""org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer""
nimbus.admins:
  - ""storm/bigstorm.porcupineracing.com@PORCUPINERACING.COM""
  - ""storm@PORCUPINERACING.COM""
  - ""storm""
nimbus.supervisor.users:
  - ""storm/bigstorm.porcupineracing.com@PORCUPINERACING.COM""
  - ""storm@PORCUPINERACING.COM""
  - ""storm""
nimbus.users:
   - ""steven.miller""
   - ""steven.miller@PORCUPINERACING.COM""
{code}

I can start the UI and verify using lsof that it's only listening on localhost:


{code}
[root@bigstorm bin]# ps axuww | grep ui.core
root      5080  0.1  5.6 2850232 217688 pts/1  Sl   Sep14   1:31 java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/opt/apache-storm-1.1.1 -Dstorm.log.dir=/opt/apache-storm-1.1.1/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /opt/apache-storm-1.1.1/lib/asm-5.0.3.jar:/opt/apache-storm-1.1.1/lib/clojure-1.7.0.jar:/opt/apache-storm-1.1.1/lib/disruptor-3.3.2.jar:/opt/apache-storm-1.1.1/lib/kryo-3.0.3.jar:/opt/apache-storm-1.1.1/lib/log4j-api-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-core-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-over-slf4j-1.6.6.jar:/opt/apache-storm-1.1.1/lib/log4j-slf4j-impl-2.8.2.jar:/opt/apache-storm-1.1.1/lib/minlog-1.3.0.jar:/opt/apache-storm-1.1.1/lib/objenesis-2.1.jar:/opt/apache-storm-1.1.1/lib/reflectasm-1.10.1.jar:/opt/apache-storm-1.1.1/lib/ring-cors-0.1.5.jar:/opt/apache-storm-1.1.1/lib/servlet-api-2.5.jar:/opt/apache-storm-1.1.1/lib/slf4j-api-1.7.21.jar:/opt/apache-storm-1.1.1/lib/storm-core-1.1.1.jar:/opt/apache-storm-1.1.1/lib/storm-rename-hack-1.1.1.jar:/opt/apache-storm-1.1.1:/opt/apache-storm-default/conf -Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf -Dlogfile.name=ui.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/opt/apache-storm-1.1.1/log4j2/cluster.xml org.apache.storm.ui.core
root     19913  0.0  0.0 112648   972 pts/1    R+   09:26   0:00 grep --color=auto ui.core

[root@bigstorm bin]# lsof -p 5080 -P | grep LISTEN
java    5080 root   27u     IPv6             597116       0t0      TCP localhost:8080 (LISTEN)
{code}


Now if I add the https config:

{code}
ui.https.host: ""localhost""
ui.https.port: 8443
ui.https.keystore.type: ""jks""
ui.https.keystore.path: ""/keytabs/keystore.jks""
ui.https.keystore.password: ""sooper-sekrit""
ui.https.key.password: ""sooper-sekrit""
{code}

and I restart the UI, I can see that it's listening on *:8443:

{code}
[root@bigstorm bin]# ps axuww | grep ui.core
root     19921 17.2  5.4 2849188 210896 pts/1  Sl   09:26   0:04 java -server -Ddaemon.name=ui -Dstorm.options= -Dstorm.home=/opt/apache-storm-1.1.1 -Dstorm.log.dir=/opt/apache-storm-1.1.1/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /opt/apache-storm-1.1.1/lib/asm-5.0.3.jar:/opt/apache-storm-1.1.1/lib/clojure-1.7.0.jar:/opt/apache-storm-1.1.1/lib/disruptor-3.3.2.jar:/opt/apache-storm-1.1.1/lib/kryo-3.0.3.jar:/opt/apache-storm-1.1.1/lib/log4j-api-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-core-2.8.2.jar:/opt/apache-storm-1.1.1/lib/log4j-over-slf4j-1.6.6.jar:/opt/apache-storm-1.1.1/lib/log4j-slf4j-impl-2.8.2.jar:/opt/apache-storm-1.1.1/lib/minlog-1.3.0.jar:/opt/apache-storm-1.1.1/lib/objenesis-2.1.jar:/opt/apache-storm-1.1.1/lib/reflectasm-1.10.1.jar:/opt/apache-storm-1.1.1/lib/ring-cors-0.1.5.jar:/opt/apache-storm-1.1.1/lib/servlet-api-2.5.jar:/opt/apache-storm-1.1.1/lib/slf4j-api-1.7.21.jar:/opt/apache-storm-1.1.1/lib/storm-core-1.1.1.jar:/opt/apache-storm-1.1.1/lib/storm-rename-hack-1.1.1.jar:/opt/apache-storm-1.1.1:/opt/apache-storm-default/conf -Xmx768m -Djava.security.auth.login.config=/keytabs/jaas.conf -Djava.security.krb5.conf=/etc/krb5.conf -Dlogfile.name=ui.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/opt/apache-storm-1.1.1/log4j2/cluster.xml org.apache.storm.ui.core
root     20018  0.0  0.0 112648   968 pts/1    R+   09:27   0:00 grep --color=auto ui.core
[root@bigstorm bin]# lsof -p 19921 -P | grep LISTEN
java    19921 root   38u  IPv6             677914       0t0      TCP *:8443 (LISTEN)
{code}

I have a situation in which I'm trying to limit access to the UI on a per-user basis.  The UI seems, as far as I can tell, only to support limiting access to users with valid Kerberos tickets (which is everyone here :) ), so I was trying to put a proxy in front of the UI and run it just on localhost, and rely on the proxy to do the authentication.

This bug means that if I was to do that, I'd have to run the UI without https, which means that people's credentials would be bouncing around in the clear (again, as far as I can tell; I tcpdumped that and I could see, say, storm@PORCUPINERACING.COM in the base64 decode of the Authorization: HTTP header, at least, which I figure was a bad sign).

I looked at the code and didn't see anything obvious but since I don't know Clojure or Netty it was probably staring me in the face. :) . But if you could fix this that'd be awesome, and it'd let me secure this in a way that I'd find much more reassuring.  Thanks!"
STORM-2736,o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key,"Sometimes, after our topologies have been running for a while, Zookeeper does not respond within an appropriate time and we see
{code}
2017-08-16 10:18:38.859 o.a.s.zookeeper [INFO] ip-10-181-20-70.ec2.internal lost leadership.
2017-08-16 10:21:31.144 o.a.s.zookeeper [INFO] ip-10-181-20-70.ec2.internal gained leadership, checking if it has all the topology code locally.
2017-08-16 10:21:46.201 o.a.s.zookeeper [INFO] Accepting leadership, all active topology found localy.
{code}

That's fine, and we probably need to allocate more resources. But after a new leader is chosen, we then see:
{code}
o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<key>
{code}
over and over.

I can't figure out yet how to cause the conditions that lead to Zookeeper becoming unresponsive, but it is possible to reproduce the {{BlobStoreUtils}} error by restarting Zookeeper.

The problem, I think, is that the loop [here|https://github.com/apache/storm/blob/v1.1.1/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L175] never executes because the {{nimbusInfos}} list is empty. If I add a check similar to [this|https://github.com/apache/storm/blob/v1.1.1/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java#L244] for a node which exists but has no children, the error goes away."
STORM-2735,LocalCluster and other testing classes are not documented in Javadoc because they are part of storm-server,"We don't publish any Javadoc about LocalCluster or the other testing tools because they are part of the storm-server module. Since users are expected to interact with these classes directly, we should figure out a way to publish Javadoc for them. 
"
STORM-2734,The master branch cannot release due to crash in Checkstyle,"The current master branch produces a stack overflow when mvn release:prepare is run. Checkstyle is executed multiple times per module, and in storm-sql-core it ends up checking generated files.

There are also several other minor issues I'd like to resolve:

* Checkstyle could be upgraded to latest version
* storm-integration-test is ""dangling"" in the sense that it's not attached to the rest of the project. This causes its version to get out of sync regularly (e.g. currently on 1.0.x-branch) because we have to update it manually.
* storm-hive declares calcite-core twice in different versions."
STORM-2733,Make Load Aware Shuffle much better at really bad situations,"We recently had an issue where some bolts got really backed up and started to die from OOMs.  The issue ended up being 2 fold.

First the GC really slowed down the worker so much that it could not keep up even with < 1% of the traffic that was still being sent to it.  Which made it almost impossible to recover.

The second issue was that the serialization of the tuples took a lot longer than the processing, which resulted in the send queue filling up much more quickly than the receive queue.

To help fix this issue I plan to address this in 2 ways.  First we need a better algorithm that can actually shut off the flow entirely to a very slow bolt and second we need to take the send queue into account when shuffling.

This is not a full set of changes needed by STORM-2686 but it is a step in that direction.  I am going to try and set it up so that the two algorithms would work nicely together."
STORM-2724,ExecutorService in WaterMarkEventGenerator never shutdown,"I have seen a topology with event time windowing never terminated on local mode. While looking into detail on thread dump I found only one non-daemon thread prevents process to be not finished: executorService in WaterMarkEventGenerator. 

Btw, I dumped thread via jstack but impossible to find from jstack result because it doesn't have thread factory hence thread name is pool-*."
STORM-2723,Nimbus crashes on joining cluster,"Cluster with N nodes and with running topologies. N new nodes join and the old machines start to be disconnected.
Some of the new nimbus fail with this message:

{code:java}
2017-09-06T16:30:53.551Z cluster [INFO] setup-path/blobstore/Topology-1-1504685635-stormconf.
ser/node02:6627-1
2017-09-06T16:30:53.608Z nimbus [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: or
g.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:98) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.nimbus$fn__10607.invoke(nimbus.clj:1458) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:233) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$fn__11005$exec_fn__1364__auto____11006$fn__11021.invoke(nimbus.clj:2460) ~[storm-core-1.1.0.j
ar:1.1.0]
	at org.apache.storm.timer$schedule_recurring$this__1737.invoke(timer.clj:105) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:50) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.storm.th
rift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:120) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.storm.thrift.transport.TTransportExc
eption
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:266) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.lang.RuntimeException: java.io.IOException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:194) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: java.io.IOException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:156) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:182) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:186) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
Caused by: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.generated.Nimbus$Client.recv_downloadBlobChunk(Nimbus.java:866) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.generated.Nimbus$Client.downloadBlobChunk(Nimbus.java:853) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.readMore(NimbusBlobStore.java:168) ~[storm-core-1.1.0.j
ar:1.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:146) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.NimbusBlobStore$NimbusDownloadInputStream.read(NimbusBlobStore.java:182) ~[storm-core-1.1.0.jar:1
.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.downloadUpdatedBlob(BlobStoreUtils.java:186) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:258) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:117) ~[storm-core-1.1.0.jar:1.1.0
]
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:77) ~[storm-core-1.1.0.jar:1.1.0]
	... 8 more
2017-09-06T16:30:53.618Z util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__9808.invoke(nimbus.clj:212) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:71) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) ~[storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
2017-09-06T16:30:53.619Z nimbus [INFO] Shutting down master

{code}
"
STORM-2722,JMSSpout test fails way too often,"{code}
java.lang.AssertionError: null
	at org.junit.Assert.fail(Assert.java:92)
	at org.junit.Assert.assertTrue(Assert.java:43)
	at org.junit.Assert.assertTrue(Assert.java:54)
	at org.apache.storm.jms.spout.JmsSpoutTest.testFailure(JmsSpoutTest.java:62)
{code}

Which corresponds to 

https://github.com/apache/storm/blob/d6e5e6d4e0a20c4c9f0ce0e3000e730dcb4700da/external/storm-jms/src/test/java/org/apache/storm/jms/spout/JmsSpoutTest.java?utf8=%E2%9C%93#L62
"
STORM-2721,Add mapping for KafkaConsumer metrics to storm metrics in KafkaTridentSpoutOpaque,"The current KafkaTridentSpoutOpque, does not have any metrics. We can use the metrics() call of the KafkaConsumer https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#metrics(), to get various metrics and map that to the storm metrics.

Eg:
We can add a generic KafkaClientMetric which would implement IMetric and has a getValueAndReset(), where the consumer metric calls are made.
requiredMetric can be initialized to any metrics like records-lag-max.

{code}
        @Override
        public Object getValueAndReset() {

            for (Map.Entry<MetricName, ? extends Metric> metricKeyVal : ((Map<MetricName, ? extends Metric>) kafkaConsumer.metrics()).entrySet()) {

                // Sample structure of Metric
                // MetricName [name=records-lag-max, group=consumer-fetch-manager-metrics, description=The maximum lag in terms of number of records for any partition in this window, tags={client-id=consumer-1}] metric.name()=MetricName [name=records-lag-max, group=consumer-fetch-manager-metrics, description=The maximum lag in terms of number of records for any partition in this window, tags={client-id=consumer-1}] metric.value()=-Infinity

                Metric metric = metricKeyVal.getValue();
                if(metric.metricName().name().equals(requiredMetric)) {
                    return metric.value();
                }

            }

            return null;
        }
{code}
"
STORM-2720,Add timestamp based FirstPollOffsetStrategy in KafkaTridentSpoutOpaque,"Offsets for a given partition at a particular timestamp can now be found using offsetsForTimes API. https://kafka.apache.org/0110/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes(java.util.Map).

One way to make use of this api would be to :
Add a new option for FirstPollOffsetStrategy called TIMESTAMP 
Add a new startTimeStamp option to KafkaSpoutConfig, which would be used only when FirstPollOffsetStrategy is set to TIMESTAMP.

Later in the KafkaTridentSpoutEmitter, when we do the first seek, we can do something like :

{code}
            if(firstPollOffsetStrategy.equals(TIMESTAMP)) {
                try {
                    startTimeStampOffset =
                        kafkaConsumer.offsetsForTimes(Collections.singletonMap(tp, startTimeStamp)).get(tp).offset();
                } catch (IllegalArgumentException e) {
                    LOG.error(""Illegal timestamp {} provided for tp {} "",startTimeStamp,tp.toString());
                } catch (UnsupportedVersionException e) {
                    LOG.error(""Kafka Server do not support offsetsForTimes(), probably < 0.10.1"",e);
                }

                if(startTimeStampOffset!=null) {
                    LOG.info(""Kafka consumer offset reset for TopicPartition {}, TimeStamp {}, Offset {}"",tp,startTimeStamp,startTimeStampOffset);
                    kafkaConsumer.seek(tp, startTimeStampOffset);
                } else {
                    LOG.info(""Kafka consumer offset reset by timestamp failed for TopicPartition {}, TimeStamp {}, Offset {}. Restart with a different Strategy "",tp,startTimeStamp,startTimeStampOffset);
                }
            }
{code}

"
STORM-2719,Trident Kafka Spout Emitters do not get full partition information in getOrderedPartitions(),"The storm kakfa trident spout uses the KafkaTridentSpoutTopicPartitionRegistry, to get partition information. The coordinator calls the getTopicPartitions() method to get partition information and passes it to the emitters. But this partition information will not be accurate as all instances of KafkaTridentSpoutTopicPartitionRegistry will not be updated with full partition information.

The update to the registry is done when the consumer subscribes using KafkaSpoutConsumerRebalanceListener. This calls the KafkaTridentSpoutTopicPartitionRegistry.INSTANCE.addAll(partitions); These calls would only update the registry in that particular worker with partition information for consumers in that worker.

So when the coordinator calls the getOrderedPartitions() and passes it to each emitter by calling getOrderedPartitions(), the full partition information will not be present. The only probable case this would work is if the emitters and coordinators were on the same worker."
STORM-2716,Storm-webapp tests don't work on Windows,"Several storm-webapp tests don't work on Windows because file paths like ""/tmp"" are used, and paths are sometimes constructed by String concatenation instead of using Path.resolve. The logviewer also doesn't seem to work on Windows, probably for the same reason.

I think there might be a few similar issues in other parts of the code, which I'd like to also fix as part of this. 

I haven't checked whether this is a problem in 1.x, but it's likely."
STORM-2712,accept arbitrary number of rows per tuple in storm-cassandra,"Current implementation in `TridentResultSetValuesMapper::map` restricts a SELECT query to return one row. In `StateQueryProcessor::finishBatch`, it checks the equality between the result size of `batchRetrieve` and input tuple size. When the number of result rows is less than 1 or greater than 1, it breaks the condition and an exception is thrown.

We should accept arbitrary number of rows by adjusting List dimensions."
STORM-2711,use KafkaTridentSpoutOpaque poll msg slowly.,"At first I run producer examples to make msgs in kafka, which topic is 5 partition 1 replication, then the number of total message was about 4000, per partition almost 800. Then I {color:red}run the part of consumer example in TridentKafkaClientWordCountNamedTopics in storm-kafka-client-examples{color}, First pull messages at a certain speed, when each partition to more than 500, significantly {color:red}slower speed {color}. I wonder why"
STORM-2707,Nimbus loops forever with getClusterInfo error if it looses storm.local.dir contents,"Hello,

Short issue description:
* Remove storm.local.dir directory
* Storm UI isn't anymore able to query anything from Nimbus
* Nimbus process prints getClusterInfo exceptions in its log whenever it gets a query from Nimbus UI or from ""storm"" command line
* To fix this issue, we have to stop all Storm processes, cleanup the content of Zookeeper nodes, then restart & redeploy our topologies

Excepted behavior:
* In such case, Storm should cleanup the content of Zookeeper and recover in a mode allowing to kill & restart all topologies

More details:
===========
Sometimes we loose the content of storm.local.dir on our single-node Nimbus production cluster.

We haven't yet considered deploying Nimbus in HA because this is a relatively modest deployment with budget constrains on the number of the number of IaaS resources which can be used for this application. So far so good, because in our environment, Nimbus & Nimbus UI (hosted on same VM) are supervized, and we also have self-healing crons to automatically kill & restart topologies blocked in Kafka consumption or having too many failed tuples (because Storm back pressure has some fuzzy limits, so we use this by-pass, as approved by Roshan in a past discussion, but that's not the point here).

Our problem is that sometime, we loose the content of storm.local.dir.

When it happens, our supervision detects the issue because it cannot anymore query Nimbus REST services on Nimbus-UI process.

In such case it tries to restart Storm-UI but this doesn't help because queries to Storm-UI fails with the following stack trace when it tries to list all topologies:

org.apache.storm.thrift.TApplicationException: Internal error processing getClusterInfo
	at org.apache.storm.thrift.TApplicationException.read(TApplicationException.java:111)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.storm.generated.Nimbus$Client.recv_getClusterInfo(Nimbus.java:1168)
	at org.apache.storm.generated.Nimbus$Client.getClusterInfo(Nimbus.java:1156)
	at org.apache.storm.ui.core$cluster_summary.invoke(core.clj:356)
	at org.apache.storm.ui.core$fn__9556.invoke(core.clj:1113)
	at org.apache.storm.shade.compojure.core$make_route$fn__5976.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__5964.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__5957.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__5982.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__5986.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.cors$wrap_cors$fn__8891.invoke(cors.clj:149)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__8838.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6618.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__7901.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__6871.invoke(helpers.clj:50)
	at org.apache.storm.ui.core$catch_errors$fn__9758.invoke(core.clj:1428)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__6538.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__6581.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__6510.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__6618.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__6833.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__6819.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__6368.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__6372.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.ui.helpers$x_frame_options_filter_handler$fn__6964.invoke(helpers.clj:189)
	at org.apache.storm.ui.helpers.proxy$java.lang.Object$Filter$abec9a8f.doFilter(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:748)

In Nimbus.log, we also have this kind of exception each time Nimbus UI is queried:

org.apache.storm.generated.KeyNotFoundException: null
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:147) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:299) ~[storm-core-1.1.0.jar:1.1.0]
        at sun.reflect.GeneratedMethodAccessor80.invoke(Unknown Source) ~[?:?]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:489) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.daemon.nimbus$get_cluster_info$iter__10687__10691$fn__10692.invoke(nimbus.clj:1550) ~[storm-core-1.1.0.jar:1.1.0]
        at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
        at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
        at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
        at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
        at clojure.core$dorun.invoke(core.clj:3009) ~[clojure-1.7.0.jar:?]
        at clojure.core$doall.invoke(core.clj:3025) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1524) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10782.getClusterInfo(nimbus.clj:1971) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3920) ~[storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3904) ~[storm-core-1.1.0.jar:1.1.0]

Even if Storm was shutting down in such case, this wouldn't help because we have to cleanup all Zookeepers to put our Storm cluster back to life.

Ideally, we would like that when storm.local.dir is lost, Nimbus will re-create it ""blank"" and cleanups Zookeeper nodes (that's the tricky part); then we expect that Supervisors (which are unaffected by this issue when it occurs) will re-register themselves to make Nimbus aware that topologies are Running. 

Also, Topologies restart from UI should consistently fail until topologies JARs are re-submitted (please make the error message very clear and easy to ""grep"" when such case occurs);

This is my first JIRA, I hope I provided everything to let Storm developers dig this issue ; otherwise please let me know if more information is required: I will be glad to help as much as I can... Storm rocks!

Best regards,
Alexandre Vermeerbergen

"
STORM-2706,Nimbus stuck in exception and does not fail fast,"We experience a problem in nimbus which leads it to get stuck in a retry and fail loop. When I manually restart the nimbus it works again as expected. However, it would be great if nimbus would shut down so our monitoring can automatically restart the nimbus. 

The nimbus log. 

{noformat}
24.8.2017 15:39:1913:39:19.804 [pool-13-thread-51] ERROR org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer - Unexpected throwable while invoking!
24.8.2017 15:39:19org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:151) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:133) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:453) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:19	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:19	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at org.apache.storm.zookeeper$zk_leader_elector$reify__1043.getLeader(zookeeper.clj:296) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:19	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:19	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:19	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10780.getLeader(nimbus.clj:2412) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.generated.Nimbus$Processor$getLeader.getResult(Nimbus.java:3944) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.generated.Nimbus$Processor$getLeader.getResult(Nimbus.java:3928) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:162) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:19	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
24.8.2017 15:39:19	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
24.8.2017 15:39:19	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
24.8.2017 15:39:2713:39:27.205 [pool-13-thread-52] ERROR org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer - Unexpected throwable while invoking!
24.8.2017 15:39:27org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:230) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:219) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:216) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:207) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:40) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getSortedChildren(LockInternals.java:151) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.recipes.locks.LockInternals.getParticipantNodes(LockInternals.java:133) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:453) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:27	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:27	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at org.apache.storm.zookeeper$zk_leader_elector$reify__1043.getLeader(zookeeper.clj:296) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) ~[?:?]
24.8.2017 15:39:27	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_131]
24.8.2017 15:39:27	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_131]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313) ~[clojure-1.7.0.jar:?]
24.8.2017 15:39:27	at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1544) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__10780.getClusterInfo(nimbus.clj:2006) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3920) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3904) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:162) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-1.1.1.jar:1.1.1]
24.8.2017 15:39:27	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131]
24.8.2017 15:39:27	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131]
24.8.2017 15:39:27	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
24.8.2017 15:39:2913:39:29.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:2913:39:29.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:39:3913:39:39.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:3913:39:39.270 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:39:4913:39:49.271 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:4913:39:49.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:39:5913:39:59.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:39:5913:39:59.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:40:0913:40:09.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping assignments
24.8.2017 15:40:0913:40:09.272 [timer] INFO  org.apache.storm.daemon.nimbus - not a leader, skipping cleanup
24.8.2017 15:40:1313:40:13.806 [timer] INFO  org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl - Starting
24.8.2017 15:40:1313:40:13.807 [timer] INFO  org.apache.storm.shade.org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181/storm sessionTimeout=20000 watcher=org.apache.storm.shade.org.apache.curator.ConnectionState@f90354
24.8.2017 15:40:1313:40:13.808 [timer-SendThread(10.42.174.214:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Opening socket connection to server 10.42.174.214/10.42.174.214:2181. Will not attempt to authenticate using SASL (unknown error)
24.8.2017 15:40:1313:40:13.862 [timer-SendThread(10.42.174.214:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Socket connection established to 10.42.174.214/10.42.174.214:2181, initiating session
24.8.2017 15:40:1313:40:13.865 [timer-SendThread(10.42.174.214:2181)] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - Session establishment complete on server 10.42.174.214/10.42.174.214:2181, sessionid = 0x15e14456dc70045, negotiated timeout = 20000
24.8.2017 15:40:1313:40:13.910 [timer] INFO  org.apache.storm.shade.org.apache.zookeeper.ZooKeeper - Session: 0x15e14456dc70045 closed
24.8.2017 15:40:1313:40:13.910 [timer-EventThread] INFO  org.apache.storm.shade.org.apache.zookeeper.ClientCnxn - EventThread shut down
{noformat}
"
STORM-2699,Put all the version information of third party components into the main pom ,I think it's better to put all the version information of third party components into the main pom for more efficient version control
STORM-2698,Upgrade to newest Mockito and Hamcrest versions,"We are currently depending on Mockito 1.9.5, which is from 2012. I think we should upgrade to the latest version, since some APIs have become a little nicer to work with."
STORM-2697,Failed to cleanup worker when GET worker-user failed,"""2017-08-15 11:25:53,554"" | INFO  | [Thread-4] | Shutting down and clearing state for id f5906569-41db-4c7f-9048-b3c551603fb4. Current supervisor time: 1502767553. State: :not-started, Heartbeat: nil | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,554"" | INFO  | [Thread-4] | Shutting down 136d9652-7b8b-4e3d-8d45-33d72dfe1462:f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | INFO  | [Thread-4] | GET worker-user f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.config (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | WARN  | [Thread-4] | Failed to get worker user for f5906569-41db-4c7f-9048-b3c551603fb4. #<FileNotFoundException java.io.FileNotFoundException: /var/streaming_data/stormdir/workers-users/f5906569-41db-4c7f-9048-b3c551603fb4 (No such file or directory)> | backtype.storm.config (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | WARN  | [Thread-4] | Failed to cleanup worker f5906569-41db-4c7f-9048-b3c551603fb4. Will retry later #<IllegalArgumentException java.lang.IllegalArgumentException: User cannot be blank when calling worker-launcher.> | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) 
""2017-08-15 11:25:53,555"" | INFO  | [Thread-4] | Shut down 136d9652-7b8b-4e3d-8d45-33d72dfe1462:f5906569-41db-4c7f-9048-b3c551603fb4 | backtype.storm.daemon.supervisor (NO_SOURCE_FILE:0) "
STORM-2694,Create a listener to handle tuple state changes of the KafkaSpout,"We had a couple of use cases where we needed the KafkaSpout to put failed tuples into a dead letter queue.

The pull request proposes a listener which is called every time a tuple in the KafkaSpout is emitted/acked/failed/retried.  

"
STORM-2693,Topology submission or kill takes too much time when topologies grow to a few hundred,"Now for a storm cluster with 40 hosts [with 32 cores/128G memory] and hundreds of topologies, nimbus submission and killing will take about minutes to finish. For example, for a cluster with 300 hundred of topologies，it will take about 8 minutes to submit a topology, this affect our efficiency seriously.

So, i check out the nimbus code and find two factor that will effect nimbus submission/killing time for a scheduling round:
* read existing-assignments from zookeeper for every topology [will take about 4 seconds for a 300 topologies cluster]
* read all the workers heartbeats and update the state to nimbus cache [will take about 30 seconds for a 300 topologies cluster]
the key here is that Storm now use zookeeper to collect heartbeats [not RPC], and also keep physical plan [assignments] using zookeeper which can be totally local in nimbus.

So, i think we should make some changes to storm's heartbeats and assignments management.

For assignment promotion:
1. nimbus will put the assignments in local disk
2. when restart or HA leader trigger nimbus will recover assignments from zk to local disk
3. nimbus will tell supervisor its assignment every time through RPC every scheduling round
4. supervisor will sync assignments at fixed time


For heartbeats promotion:
1. workers will report executors ok or wrong to supervisor at fixed time
2. supervisor will report workers heartbeats to nimbus at fixed time
3. if supervisor die, it will tell nimbus through runtime hook
    or let nimbus find it through aware supervisor if is survive 
4. let supervisor decide if worker is running ok or invalid , supervisor will tell nimbus which executors of every topology are ok
"
STORM-2692,Load only configs specific to the topology in populateCredentials,"Theres a single instance of AutoCredentials plugin in Nimbus and right now we load all the config keys in ""populateCredentials"". This can cause issues when multiple topologies are submitted. The second one tries to load the keys of the first topology."
STORM-2691,storm-kafka-client Trident spout implements the Trident interface incorrectly,"The Trident Kafka spout uses the KafkaTridentSpoutTopicPartitionRegistry enum to pass existing topic partitions from the spout to the coordinator. This only works when those components happen to be in the same JVM, because the coordinator gets the topic information from the KafkaConsumer started by KafkaTridentSpoutEmitter. 

The coordinator runs in the TridentSpoutCoordinator bolt here https://github.com/apache/storm/blob/4c8a986f519cdf3e63bed47e9c4f723e4867267a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java#L162, while the spout instances (emitters) run in TridentSpoutExecutors here https://github.com/apache/storm/blob/4c8a986f519cdf3e63bed47e9c4f723e4867267a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java#L176.

We should replace the registry enum with writes to Zookeeper or something similar.

Edit: The fix for this is likely to be a broader change where we split the Subscription API into a few parts so the assignment process can be split across the coordinator and emitter instead of the emitter doing everything."
STORM-2690,resurrect invocation of ISupervisor.assigned() & make Supervisor.launchDaemon() accessible,"As [discussed in STORM-2018|https://issues.apache.org/jira/browse/STORM-2018?focusedCommentId=16108307&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16108307], that change subtly broke the storm-mesos integration framework because of the removal of the invocation of [{{ISupervisor.assigned()}}|https://github.com/apache/storm/blob/v1.0.4/storm-core/src/jvm/org/apache/storm/scheduler/ISupervisor.java#L44].

So this ticket is tracking the reinstatement of that invocation from the supervisor core code.

Also, the [{{launchDaemon()}}|https://github.com/apache/storm/blob/v1.0.4/storm-core/src/jvm/org/apache/storm/daemon/supervisor/Supervisor.java#L248] method of the {{Supervisor}} is not public, so we had to use reflection to allow calling it from the storm-mesos integration.  That should be changed too."
STORM-2689,storm-kafka-examples and storm-kafka-client-examples are difficult for new users to run,"The storm-kafka-examples and storm-kafka-client-examples projects configure their dependencies in a way that makes them difficult to run for a new user. The other example projects set up a provided dependency on storm-client, and otherwise include all their dependencies in a shaded jar. 

storm-kafka(-client) by default produce jars without several necessary dependencies, e.g. the Kafka client libraries. The provided.scope Maven parameter was intended to be used to allow users to produce a shaded jar with all dependencies, but if provided scope is set to compile, the resulting jar will also contain storm-client. This prevents the jar from running on a real cluster.

While users can work around this by producing the slim jar and using --artifacts when submitting the topology, this is unnecessarily tedious. We should just produce a fat jar by default, then mention in the example documentation that --artifacts is there for users that want to make slimmer jars.

Edit:
This issue now includes simplifying storm-kafka-examples and storm-kafka-client-examples in general. The examples demonstrate use of State and DRPC when the focus should be on how to use storm-kafka(-client). It also causes the modules to have some undesirable dependencies, e.g. they both depend on storm-starter. "
STORM-2683,Storm UI - Topology action button error response hardcoded,"Right now on confirming the topology ui action buttons (https://github.com/apache/storm/blob/4966d7a69318d2ca690c47dd43466b03574e5e9e/storm-core/src/ui/public/templates/topology-page-template.html#L607) the UI behaviour is to reload the page always (https://github.com/apache/storm/blob/10d381b303c9176ede0d1260428ad61c7757e396/storm-core/src/ui/public/js/script.js#L169) and in case of an error display a hardcoded error message _""Error while communicating with Nimbus.""_

While this behaviour is okay for workflows with no authorization it gets confusing when the action buttons are put behind some form of authorization. It will be much clearer to define an error message format and have the UI display the correct error message on authorization and other non-nimbus related failures. "
STORM-2682,Supervisor crashes with NullPointerException,"When supervisor is started, it dies after about 30s like so:

{code:java}
...
2017-08-07 17:12:04.606 o.a.s.d.s.Slot main [WARN] SLOT 192.168.10.21:6701 Starting in state EMPTY - assignment null
2017-08-07 17:12:04.607 o.a.s.d.s.Slot main [WARN] SLOT 192.168.10.21:6702 Starting in state EMPTY - assignment null
2017-08-07 17:12:04.607 o.a.s.l.AsyncLocalizer main [INFO] Cleaning up unused topologies in /home/storm/data/supervisor/stormdist
2017-08-07 17:12:04.617 o.a.s.d.s.Supervisor main [INFO] Starting supervisor with id 65a0f977-474c-4938-a4f5-bc99939e96ff at host 192.168.10.
21.
2017-08-07 17:12:04.619 o.a.s.d.m.MetricsUtils main [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPrep
arableReporter
2017-08-07 17:12:04.620 o.a.s.d.m.r.JmxPreparableReporter main [INFO] Preparing...
2017-08-07 17:12:04.624 o.a.s.m.StormMetricsRegistry main [INFO] Started statistics report plugin...
2017-08-07 17:12:34.620 o.a.s.e.EventManagerImp Thread-4 [ERROR] {} Error when processing event
java.lang.NullPointerException: null
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_121]
        at org.apache.storm.localizer.Localizer.updateBlobs(Localizer.java:332) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.updateBlobsForTopology(UpdateBlobs.java:99) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run(UpdateBlobs.java:72) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:54) ~[storm-core-1.0.4.jar:1.0.4]
2017-08-07 17:12:34.620 o.a.s.u.Utils Thread-4 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1750) ~[storm-core-1.0.4.jar:1.0.4]
        at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:63) ~[storm-core-1.0.4.jar:1.0.4]
2017-08-07 17:12:34.631 o.a.s.d.s.Supervisor Thread-5 [INFO] Shutting down supervisor 65a0f977-474c-4938-a4f5-bc99939e96ff
{code}"
STORM-2680,The switch to turn on-off the cgroup in the doc should be “storm.resource.isolation.plugin.enable”,
STORM-2677,consider all sampled tuples which took greater than 0 ms processing time,"In Storm 1.x , tuples that aren't sampled shouldn't be considered for execute  latency calculations. Need to consider all sampled tuples which took greater than  0 ms  processing time"
STORM-2675,KafkaTridentSpoutOpaque not committing offsets to Kafka,"Every time I restart the topology the spout was picking the earliest message even though poll strategy is set UNCOMMITTED_EARLIEST.  I looked at Kafka's  __consumer_offsets topic to see if spout (consumer) is committing the offsets but did not find any commits. I am not even able to locate the code in the KafkaTridentSpoutEmitter class where we are updating the commits?

    conf.put(Config.TOPOLOGY_DEBUG, true);
    conf.put(Config.TOPOLOGY_WORKERS, 1);
    conf.put(Config.TOPOLOGY_MAX_SPOUT_PENDING, 4); //tried with1 as well
    conf.put(Config.TRANSACTIONAL_ZOOKEEPER_ROOT, ""/aggregate"");
    conf.put(Config.TRANSACTIONAL_ZOOKEEPER_SERVERS, Arrays.asList(new String[]{""localhost""}));
    conf.put(Config.TRANSACTIONAL_ZOOKEEPER_PORT, 2181);

 protected static KafkaSpoutConfig<String, String> getPMStatKafkaSpoutConfig() {
    ByTopicRecordTranslator<String, String> byTopic =
        new ByTopicRecordTranslator<>((r) -> new Values(r.topic(), r.key(), r.value()),
            new Fields(TOPIC, PARTITION_KEY, PAYLOAD), SENSOR_STREAM);

    return new KafkaSpoutConfig.Builder<String, String>(Utils.getBrokerHosts(),
        StringDeserializer.class, null, Utils.getKafkaEnrichedPMSTopicName())
            .setMaxPartitionFectchBytes(10 * 1024) // 10 KB
            .setRetry(getRetryService())
            .setOffsetCommitPeriodMs(10_000)
            .setFirstPollOffsetStrategy(FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST)
            .setMaxUncommittedOffsets(250)
            .setProp(""value.deserializer"", ""io.confluent.kafka.serializers.KafkaAvroDeserializer"")
            .setProp(""schema.registry.url"",""http://localhost:8081"")
            .setProp(""specific.avro.reader"",true)
            .setGroupId(AGGREGATION_CONSUMER_GROUP)
            .setRecordTranslator(byTopic).build();
  }

Stream pmStatStream =
        topology.newStream(""statStream"", new KafkaTridentSpoutOpaque<>(getPMStatKafkaSpoutConfig())).parallelismHint(1)

storm-version - 1.1.0"
STORM-2673,For debugging allow users to tell the scheduler which nodes they would prefer,"In some cases with debugging it would be nice to let the user tell the scheduler that it wants to run on host X and not run on host Y.

This is mostly for the case where we saw an odd issue with a topology and it was running on a specific host.  So to unblock the user giving them the ability to avoid a specific host is helpful, at the same time we may want to reproduce the issue and causing the topology to be scheduled on that bad node is helpful. "
STORM-2670,move storm-client-misc to external/http-forwarding-metrics-consumer,"storm-client-misc is not named very well, we should fix it."
STORM-2669,Extend the BinaryEventDataScheme in storm-eventhubs to include MessageId in addition to system properties,"Currently there are two types of EventDataScheme included with the storm-eventhubs spout.

The default is the StringEventDataScheme that emits a single output field, the message itself as a string.

There is an additional BinaryEventDataScheme that passes the message as is, but also has two additional fields: metadata and system_metadata that is passed by eventhubs-client.

The system_metadata only contains the sequence number, offset and enqeued time of an event.

As part of recent requirements by certain applications for tracking an event, they also need the partition id. The partition id is NOT sent by the eventhubs-client, instead the partition manager in the spout already has this information.

The goal of this JIRA is to introduce another output field in BinaryEventDataScheme that contains the MessageId for an event. The messageId will contain: partitionId, sequence number and the offset information for any downstream bolt to be able to locate where the message arrived from.

I will also be fixing any maven checkstyle warnings/errors in the files that I will be committing changes in."
STORM-2668,org.apache.storm.kafka.FailedFetchException,"2017-08-01 11:34:42.446 o.a.s.k.KafkaUtils [ERROR] Error fetching data from [Partition{host=xxx, topic=ABC, partition=5}] for topic [ABC]: [UNKNOWN]
2017-08-01 11:34:42.446 o.a.s.k.KafkaSpout [WARN] Fetch failed
org.apache.storm.kafka.FailedFetchException: Error fetching data from [Partition{host=xxx, topic=ABC, partition=5}] for topic [ABC]: [UNKNOWN]"
STORM-2666,Storm-kafka-client spout can sometimes emit messages that were already committed. ,"Under a certain heavy load, for failed/timeout tuples, the retry service will ack tuple for failed max times. Kafka Client Spout will commit after reached the commit interval. However seems some 'on the way' tuples will be failed again, the retry service will cause Spout to emit again, and acked eventually to OffsetManager.

In some cases such offsets are too many, exceeding the max-uncommit, causing org.apache.storm.kafka.spout.internal.OffsetManager#findNextCommitOffset unable to find next commit point, and Spout for this partition will not poll any more.

By the way I've applied STORM-2549 PR#2156 from Stig Døssing to fix STORM-2625, and I'm using Python Shell Bolt as processing bolt, if this information helps.

resulting logs like below. I'm not sure if the issue has already been raised/fixed, glad if anyone could help to point out existing JIRA. Thank you.


2017-07-27 22:23:48.398 o.a.s.k.s.KafkaSpout Thread-23-spout-executor[248 248] [INFO] Successful ack for tuple message [{topic-partition=kafka_bd_trigger_action-20, offset=18204, numFails=0}].
2017-07-27 22:23:49.203 o.a.s.k.s.i.OffsetManager Thread-23-spout-executor[248 248] [WARN] topic-partition [kafka_bd_trigger_action-18] has unexpected offset [16002]. Current committed Offset [16003]

Edit:
See https://issues.apache.org/jira/browse/STORM-2666?focusedCommentId=16125893&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16125893 for the current best guess at the root cause of this issue.
"
STORM-2664,Fix for the distribution packaged on Windows OS can't run on Linux,"I made a distribution from source code on windows OS, but when I want to launch this on linux. I found there are some mistake for line break in the scripts. And then I found the settings for line ending in the binary.xml was not set obviously, so it is decided by the OS where run the maven command.
I think its better to set the line ending obviously for unix. because we mostly use linux system to deploy storm and, at the same time, this distribution will run well in the windows OS."
STORM-2663,Backport STORM-2558 and deprecate storm.cmd on 1.x-branch,"The storm.cmd script has been removed from master, but it's still very difficult to work with on 1.x-branch, due to several deficiencies compared to storm.py. It is missing certain commands (pacemaker at least) and not setting some Java properties correctly (-DdaemonName and the Java classpath are set incorrectly). Rather than spending time fixing these issues, I'd like to deprecate storm.cmd in future 1.x releases and add the Powershell script as an alternative for Windows users."
STORM-2662,"Fix for STORM-2659 is incomplete, some commands still don't have daemonName set correctly","daemonName must be set for all commands that execute a Java class, it should be an empty string for commands other than those fixed in STORM-2659. "
STORM-2661,JmsSpout should support additional ack mode or should have option for child classes,"Currently , JmsSpout supports 3 acknowledge modes. But some of the queue Providers (like Solace) supports some more ack modes. Like :
Sol_client_ack which acknowledges each message individually to support Guaranteed Delivery.
As JmsSpout's 'jmsAcknowledgeMode' is private and setter has validations, so we can't simply extend JmsSpout(and override required APIs only) to support this additional features. We need to take the complete code.

JmsSpout reference Url :https://github.com/apache/storm/blob/v1.1.0/external/storm-jms/src/main/java/org/apache/storm/jms/spout/JmsSpout.java

So, either there should be support for enhancements by child classes or 
this additional ack mode(which seems generic requirement) should be included in JmsSpout itself - we've some reference code which is working fine. can share the same if required.  
"
STORM-2660,"The Nimbus storm-local directory is relative to the working directory of the shell executing ""storm nimbus""","When the storm.local.dir property is set to a relative directory, it should be interpreted as relative to STORM_HOME. This is how it works for ""storm supervisor"". For ""storm nimbus"" it is instead relative to the working directory of the shell, so running ""storm nimbus"" from STORM_HOME/bin will put a storm-local directory in STORM_HOME/bin/storm-local."
STORM-2659,"storm.cmd does not set -Ddaemon.name, which prevents Log4j2 from logging","When trying to start any daemon from storm.cmd the following error occurs:
{code}
main ERROR Unable to create file E:\apache-storm-1.1.1\logs/access-web-${sys:daemon.name}.log java.io.IOException: The filename, directory name, or volume label syntax is incorrect
{code}

It looks like the daemon-name variable was introduced at some point, but only the storm.py script was updated. This prevents Storm from logging when started from this script."
STORM-2658,Provide storm-kafka-client spout examples,"There are a few example topologies in storm-kafka-client, but trying them out as a new user requires you to modify the storm-kafka-client pom to add shading, then rebuild storm-kafka-client and copy the jar-with-dependencies into Storm's extlib. After that you can take the test jar and run the topology. I think this is needlessly complicated.

We should move the example topologies to examples/storm-kafka-client, and make the storm-kafka-client pom produce a jar with all dependencies. Since we are only including the example source with Storm distributions, I don't see a reason to try to minimize jar size at the cost of adding more steps for the user to try out the examples. storm-starter is a good example of a user friendly example test topology, since it contains all its dependencies. If we want to make the user aware that extlib can be used to reduce jar size, we can add notes and commented out provided scopes to the example pom."
STORM-2652,Exception thrown in JmsSpout open method,"Due to a bug, the message timeout configuration property is read as an integer even though it is of type long.

```
7317 [Thread-18-a-executor[2 2]] ERROR o.a.s.util - Async loop died!
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at org.apache.storm.jms.spout.JmsSpout.open(JmsSpout.java:175) ~[storm-jms-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.executor$fn__4976$fn__4991.invoke(executor.clj:600) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
7320 [Thread-18-a-executor[2 2]] ERROR o.a.s.d.executor - 
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at org.apache.storm.jms.spout.JmsSpout.open(JmsSpout.java:175) ~[storm-jms-1.1.0.jar:1.1.0]
	at org.apache.storm.daemon.executor$fn__4976$fn__4991.invoke(executor.clj:600) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:482) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
```"
STORM-2651,Executor stopped working after connectivity issues with ZK. Executor is not restarted  by nimbus scheduler. ,"After connectivity issues, nimbus scheduler assigned the appropriate executor to the slots(custom scheduler). 


{code:java}

{panel:title=My title}
o.a.s.d.nimbus [INFO] Setting new assignment for topology id <<topology_name>>-1499356635: #org.apache.storm.daemon.common.Assignment{:master-code-dir ""/opt/storm_datadir"", :node->host {""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" ""test1"", ""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" ""test2"", ""c13b0fc8-d5c1-4335-8339-17b3c048b160"" ""test3"", ""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" ""test4""}, :executor->node+port {[8 8] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [12 12] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [2 2] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [7 7] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [22 22] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [3 3] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [24 24] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [1 1] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [18 18] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [6 6] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [20 20] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [9 9] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [23 23] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [11 11] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [16 16] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [13 13] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [19 19] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [21 21] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [5 5] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [10 10] [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702], [14 14] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702], [4 4] [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703], [15 15] [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703], [17 17] [""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702]}, :executor->start-time-secs {[8 8] 1499356646, [12 12] 1499356646, [2 2] 1499356646, [7 7] 1499356646, [22 22] 1499356646, [3 3] 1499356646, [24 24] 1499356646, [1 1] 1499356646, [18 18] 1499356646, [6 6] 1499356646, [20 20] 1499356646, [9 9] 1499356646, [23 23] 1499356646, [11 11] 1499356646, [16 16] 1499356646, [13 13] 1499356646, [19 19] 1499356646, [21 21] 1499356646, [5 5] 1499356646, [10 10] 1499356646, [14 14] 1499356646, [4 4] 1499356646, [15 15] 1499356646, [17 17] 1499356646}, :worker->resources {[""c13b0fc8-d5c1-4335-8339-17b3c048b160"" 6702] [0.0 0.0 0.0], [""4ec038e7-281b-4dcc-9e70-57afa1fd84c4"" 6702] [0.0 0.0 0.0], [""c97a7a58-ec31-41a6-8585-43ef7b62ea83"" 6703] [0.0 0.0 0.0], [""0b8b056a-dda1-4d32-8c74-003a1fefad7e"" 6703] [0.0 0.0 0.0]}}
{panel}

{code}

Then all the executor are started working properly.  

When I checked in-depth I found that,  one of the spout executor has not started and also found  that nimbus stopped logging after this issue.

o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with key<<topology-name >>-1499356635-stormconf.ser


"
STORM-2650,Add test for non-string property substitution in Flux tests,"When discussing https://issues.apache.org/jira/browse/STORM-2646, it was unclear to me whether Flux was changing the types of properties substituted in via the --filter option. It would be good to add a check for this to the unit tests."
STORM-2649,Update config validation check to give better information,"As part of submitting a topology we need to serialize the config as JSON.  The check right now is rather bad. it just calls Utils.isValidConf and throws some generic Exception about problems.

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L978-L980

It would be much better to have a new function/method that would check if they are equal and if not it would walk through them looking for which configs are different when it finds one it includes which configs and how they are different in the exception."
STORM-2648,Kafka spout can't show acks/fails and complete latency when auto commit is enabled,"The storm-kafka-client spout currently emits tuples with no message ids if auto commit is enabled. This causes the ack/fail/complete latency counters in Storm UI to be 0. In some cases this is desirable because the user may not care, and doesn't want the overhead of Storm tracking tuples. [~avermeerbergen] expressed a desire to be able to use auto commit without these counters being disabled, presumably to monitor topology performance.

We should add a toggle that allows users to enable/disable tuple anchoring in the auto commit case. "
STORM-2646,NimbusClient Class cast exception when nimbus seeds is not an array of hosts,
STORM-2645,update storm.py to be python3 compatible ,"Function emitBolt() emits a map, which is not json serializeble in python3. It should be changed to return a list in order to be json serializeble.
"
STORM-2642,Storm-kafka-client spout cannot be serialized when using manual partition assignment,"The ManualPartitioner interface isn't serializable, which prevents topology submission."
STORM-2641,storm.py and storm.ps1 don't handle errors correctly on Windows,"The powershell script doesn't return the exit code it gets when running the python script. This causes it to always return 0.

When the python script executes a java class on Windows, it throws away the subprocess output if there's an error. This makes it difficult to figure out why the process failed (e.g. ""storm jar"" with an incorrect class name)"
STORM-2640,"Deprecate KafkaConsumer.subscribe APIs on 1.x, and make KafkaConsumer.assign the default","I thought it made sense to have a separate issue for this, so we can list the deprecation in the 1.2.0 changelog, and the removal separately in 2.0.0"
STORM-2639,Kafka Spout incorrectly computes numCommittedOffsets due to voids in the topic (topic compaction),"This is a followup to STORM-2505 to fix a minor issue with the computation of numUncommittedOffsets in OffsetManager. 
"
STORM-2635,Deep log search doesn’t work when there’s no topology in topology history,"When doing deep search with cluster which doesn't ever killed topology (hence no history on topology histories), Nimbus throws error on getting topology histories."
STORM-2625,KafkaSpout is not calculating uncommitted correctly,"This happens when:
1. KafkaSpout has already committed offsets to a topic before, and is not running/activated now;
2. There're messages in topic after the committed offsets;
3. The same consumer group topology with multi works is started/activated again;

The same issue may happen when running topology gets consumer group partition re-assignment with offsets not being able to be committed in time.

The underlying issue is:

a. Because workers are registering kafka consumers one by one, when the first consumer A registers itself with kafka broker with the consumer group, it's assigned all the partitions, say partition 0 & 1. Consumer A then retrieves messages from all the assigned partitions if possible, and started processing. With every tuple KafkaSpout A emits, UNCOMMITTED count numUncommittedOffsets++ (KafkaSpout#emitTupleIfNotEmitted());

b. At this point a second consumer B registers with the broker for the same consumer group. the broker then re-assigns the partitions among existing consumers, say consumer A is assigned partition 0, and consumer B assigned partition 1. 

b.1 At this point KafkaSpout A will try committing acked offsets, and remove the partition 1 offsets it's tracking (KafkaSpout.KafkaSpoutConsumerRebalanceListener#onPartitionsRevoked()); However because the tuples are not all acked, KafkaSpout is not able to commit full list of offsets to kafka broker.

b.2 Then KafkaSpout A will remove tracked partition 1 offsets in offsetManagers as well as emitted (
org.apache.storm.kafka.spout.KafkaSpout.KafkaSpoutConsumerRebalanceListener#onPartitionsAssigned()
org.apache.storm.kafka.spout.KafkaSpout.KafkaSpoutConsumerRebalanceListener#initialize()), resulting the not acked tuples won't be acked for ever (org.apache.storm.kafka.spout.KafkaSpout#ack()), also the UNCOMMITTED count numUncommittedOffsets will never be reduced back to a correct result.

"
STORM-2624,Kafka Storm Spout: Got fetch request with offset out of range,"If partition offset is out of range then kafka spout stops emitting new messages and keeps logging following warning:
2016-10-26 11:11:31.070 o.a.s.k.KafkaUtils [WARN] Partition{host=somehost.org:9092, topic=my-topic, partition=0} Got fetch request with offset out of range: [3]
2016-10-26 11:11:31.078 o.a.s.k.KafkaUtils [WARN] Partition{host=somehost.org:9092, topic=my-topic, partition=0} Got fetch request with offset out of range: [3]
...

I believe the trivial fix is in PartitonManager.java in fill method 
line 237:
{code:java}
            long partitionLatestOffset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.LatestTime());
            if (partitionLatestOffset < offset) {
                offset = partitionLatestOffset;
            } else {
                offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.EarliestTime());
            }
{code}
change to:
{code:java}
            offset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, _spoutConfig.startOffsetTime);
{code}

line 259:
{code:java}
            if (offset > _emittedToOffset) {
                _lostMessageCount.incrBy(offset - _emittedToOffset);
                _emittedToOffset = offset;
                LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
            }
{code}
change to:
{code:java}
            if (offset > _emittedToOffset) {
                _lostMessageCount.incrBy(offset - _emittedToOffset);
            }
            _emittedToOffset = offset;
            LOG.warn(""{} Using new offset: {}"", _partition, _emittedToOffset);
{code}
"
STORM-2621,STORM-2557 broke sojourn time estimation,"STORM-2557 updated the arrival_rate disruptor queue metric to go off of individual tuples, and not batches of tuples like it did before.  But the sojourn time is also calculated from the arrival rate and the population.  But the population is computed based off of slots, not tuples.  So it is now off proportionally to how many tuples are in a given slot.

We either need a way to compute the tuple population and use that (which would be a new metric), or we need to keep the old arrival rate metric around to, just for the sojourn time.

I think the first one is more likely to work out."
STORM-2617,log4j2 RollingFile rotation failing,"I noticed that the default log rotation configuration isn't working as expected.  A specific example is worker.log.  Here is the default log4j2 configuration for worker.log (log4j2/worker.xml):
{code:xml}
    <RollingFile name=""A1""
                fileName=""${sys:workers.artifacts}/${sys:storm.id}/${sys:worker.port}/${sys:logfile.name}""
                filePattern=""${sys:workers.artifacts}/${sys:storm.id}/${sys:worker.port}/${sys:logfile.name}.%i.gz"">
        <PatternLayout>
            <pattern>${pattern}</pattern>
        </PatternLayout>
        <Policies>
            <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
        </Policies>
        <DefaultRolloverStrategy max=""9""/>
    </RollingFile>
{code}

Even thought the DefaultRolloverStrategy is set to 9, I only ever see worker.log and worker.log.1.gz.  It seems like rotation is continually overwriting worker.log.1.gz .  I expect this is either an issue with log4j2 or an issue related to the RollingFileAppender and the associated filePattern."
STORM-2612,Supervisor crashes when use the StormSubmitter.submitTopologyAs api to submit another user's topology.,"when use the StormSubmitter.submitTopologyAs api to impersonate another user , the supervisor crashes.
I used the user ""nimbus"" to start the supervisor, then I submit a topology as ""Micheal"", then I got this:
{color:red}2017-07-03 17:47:11.340 o.a.s.u.Utils [ERROR] An exception happened while downloading /home/Micheal/apache-storm-1.0.0/data/supervisor/tmp/5331e7d8-eb6f-403d-a2c5-45cecde9ca25/stormjar.jar from blob store.
AuthorizationException(msg:[nimbus] does not have [READ ] access to topo3-8-1499073314-stormjar.jar)
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:25754)
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:25731)
	at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:25662)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
	at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:825)
	at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:812)
	at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357)
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:516)
	at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:497)
	at org.apache.storm.daemon.supervisor$fn__9590.invoke(supervisor.clj:948)
	at clojure.lang.MultiFn.invoke(MultiFn.java:243)
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9351$fn__9369.invoke(supervisor.clj:582)
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9351.invoke(supervisor.clj:581)
	at org.apache.storm.event$event_manager$fn__8903.invoke(event.clj:40)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
{color}
I wonder if there is some parameter I can set to make this work.
Thanks in advance."
STORM-2611,a batched kafkaspout with offsets in zookeeper,"There are some issues with org.apache.storm.kafka.spout.KafkaSpout.
1. When the topology is running in multi workers in different supervisors, it is very often to trigger kafkaspout rebalance. And so the streaming is not stable. And it will cause massive retransmission of lost packets.
2. When max.uncommitted.offsets is less than 200000 (for limited flow), sometimes there is deadlock. The phenomenon is the heartbeat between spout and kafka can not be performed.
3. When the data is from storm to hbase,  batch is used to improve writing productivity. So using batch from spout to bolt is better for special scene.
4. So a batched kafkaspout and bolt with offsets in zookeeper will be valuable."
STORM-2610,Spout throtteling metrics are unusable,"When helping someone debug an issue with backpressure I realized that the metrics we are collecting in the spout are mistakenly being multiplied by the rate, even though we are not sub-sampling them.  This results in the values being, by default, 20 times higher then they should be.  Thinking about how I would use the metrics to debug an issue also showed that some of them.  skipped-max-spout and skipped-throttle correspond to about 1 ms of sleep, but skipped-inactive corresponds to about 100 ms of sleep.  And the 1 ms sleep is configurable so it could be different from one topology to another, and even the code around it is pluggable, so it could be doing anything from not sleeping to sleeping a random amount of time.

I think we just need to scrap what we have been doing and record how long we sleep for and use that as the metric instead.

These metrics also don't appear to be documented anywhere so I am going to change what they mean and document them to actually be useful, and correct."
STORM-2609,We need a drpc-client command line.,"We have no simple way to send a DRPC request from the command line.  We really should have one for debugging/testing at a minimum, and also as an example of what works to use DRPC."
STORM-2608,Out Of Range Offsets Should Be Removed From Pending Queue,"There is a bug that occurs when failed tuples are invalidated due to Kafka throwing a {{TopicOffsetOutOfRangeException}}. 

Below is what happens:

- Spout emits tuples
- Offsets are added to the _pending_ tree
- Some tuples fail and are added to the _failedMsgRetryManager_
- On the next fetch request, a {{TopicOffsetOutOfRangeException}} is thrown and the new offset is _after_ the offset that are currently sitting in both the _pending_ tree and the _failedMsgRetryManager_
- All offsets smaller than the the new offset are removed from the _failedMsgRetryManager_ but *not* the _pending_ tree.
- Since those offsets were removed from the _failedMsgRetryManager_ they will never be retried and thus never get removed from _pending_
- {{lastCommittedOffset()}} will always return the same value which means that offset in zookeeper for that partition will never get updated.
"
STORM-2607,[kafka-client] Consumer group every time with lag 1,"When i put a message a partition, the storm-kafka-client consume this message.
But storm-kafka-client commit the offset -1.


storm-kafka-client: 1.1.0
storm-core : 1.1.0
kafka: 0.10.2.0

Steps to bug

#1 - Insert message in kafka
#2 - Read with storm Spout this topic
#3 - Get the offset for the consumer group and the offset is always offset -1

The KafkaSpoutConfig

protected static KafkaSpoutConfig<String, String> newKafkaSpoutConfig() {
		return KafkaSpoutConfig.builder(""192.168.57.11:9092"", ""topic"").
                          setGroupId(""storm"").setOffsetCommitPeriodMs(10_000).
                          setMaxUncommittedOffsets(100_0000).setRetry(newRetryService())
                          .build();
	}

"
STORM-2606,Bolt execute() called many times ( 2 ~ 4 times ),"Hello~

I am getting some problem.
The problem is that My Develop Bolt execute method is called over twice..
The Bolt Function is logging to HBASE.. So.. If execute method is called twice over, The Data is logging to HBASE twice over..

It is disaster in my project..

Please Recommand this problem to me.."
STORM-2603,url encoding issue when submit topology name with space,"How to reproduce: 
Use 1.0.3
LocalCluster lc = new LocalCluster();
lc.submitTopology(""Hello Storm"", conf, tb.createTopology());

I got following error:
Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/var/folders/1t/3jbxpldd5dgd34fvtd74mc6r0000gp/T/f1e5f859-895c-4767-b586-cd84ba2d0fc7/supervisor/stormdist/Hello%20Storm-1-1498499584/stormconf.ser' does not exist

I looked at my directory: I have the directory Hello+Storm-1-1498499584 instead of Hello%20Storm-1-1498499584

full log file and full code: https://github.com/richardxin/hello_storm/blob/master/err.log

"
STORM-2600,Improve or replace storm-kafka-monitor,"The storm-kafka-monitor module, which is used by Storm UI to show offset lag for topologies with Kafka spouts, has some shortcomings:

* The Storm UI integration code doesn't seem to be able to support topic subscriptions that change after topology submission. The UI code (https://github.com/apache/storm/blob/64e29f365c9b5d3e15b33f33ab64e200345333e4/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java#L91) gets the topic list it should request offset lag for via the spout's getComponentConfiguration method, as far as I can tell through this call https://github.com/apache/storm/blob/9e31509d47c4e91c1009f55c7ccf321d7d7e63aa/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java#L541. It seems like the component configuration is intended to be static once the topology has started running. This prevents us from showing the right topic list for subscriptions that are not known at submission time, which is currently the case for Pattern subscriptions. The topic list for that type of subscription isn't known until the spout has started the KafkaConsumer in {{ISpout.open()}}. I don't see a way to fix this, unless there is some way to update the component configuration when the subscription changes.
* The jar is installed along with the cluster, and depends on the Kafka version specified in Storm's root POM. Kafka guarantees backwards compatible client-server communication for one release only, so there's a potential coupling between Storm cluster version and Kafka version. If users want to update the Kafka version in storm-kafka-monitor, they have to rebuild that module and replace the jar in their Storm install.
* The UI integration uses the storm-kafka-monitor Bash script to start the monitoring code, in order to avoid a dependency between storm-core and storm-kafka-monitor. This prevents the UI integration from working on Windows. We could supply a Windows script as well, but then we'd need to keep the two in sync.

I am wondering if these problems could be solved by implementing offset lag monitoring via the metrics system instead. The spout could periodically seek to the log end offset and submit a metric for how far behind the committed offset is, then seek back to where it left off.
"
STORM-2599,"BasicContainer.getWildcardDir tries to resolve the wildcard character with Paths.get, which prevents workers from booting on Windows","STORM-2191 shortens the worker classpath by substituting in wildcards for the full list of jars. The path is constructed using Paths.get(dir, ""*""), but this doesn't work on Windows. It seems like Windows checks that the path is valid. 

{code}
Paths.get(new File(""."").toString(), ""*"");

Exception in thread ""main"" java.nio.file.InvalidPathException: Illegal char <*> at index 2: .\*
	at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182)
	at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153)
	at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77)
	at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94)
	at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255)
	at java.nio.file.Paths.get(Paths.java:84)
{code}

Paths doesn't guarantee support for globs, and we don't want the OS to examine the path in any case, since the wildcard isn't a ""real"" wildcard (including all files) but a special syntax for including jars in the Java classpath. The path should be constructed with String concatenation instead."
STORM-2596,Storm Worker not reconnect the Netty Client,"I have report the simliar bugs at [STORM-2561|https://issues.apache.org/jira/browse/STORM-2561] on the version of 0.10.1.

And these days I upgrade the storm to 1.1.0, but today the bug is appeared agagin.

The worker.log shows
{code:java}
$ cat worker.log|grep '10.24.40.254:6812'|more
2017-06-22 15:14:25.295 o.a.s.m.n.Client main [INFO] creating Netty Client, connecting to 10.24.40.254:6812, bufferSize: 5242880
2017-06-23 11:23:32.570 o.a.s.m.n.StormClientHandler client-worker-1 [INFO] Connection to /10.24.40.254:6812 failed:
2017-06-23 11:23:35.654 o.a.s.m.n.Client refresh-connections-timer [INFO] closing Netty Client Netty-Client-/10.24.40.254:6812
2017-06-23 11:23:35.655 o.a.s.m.n.Client refresh-connections-timer [INFO] waiting up to 600000 ms to send 0 pending messages to Netty-Client-/10.24.40.254
:6812
2017-06-23 14:57:03.352 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 14:57:59.777 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 14:59:16.038 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:01:27.092 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:04:08.654 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
2017-06-23 15:06:59.777 o.a.s.m.n.Client Thread-10-disruptor-worker-transfer-queue [ERROR] discarding 1 messages because the Netty client to Netty-Client-
/10.24.40.254:6812 is being closed
{code}
The worker close the netty client on 2017-06-23 11:23:35.654, and never start the netty client. So the messages later on that worker are been discarded.
 
On that time Storm Node(10.24.40.254:6812) is OOM.

{code:java}
2017-06-23 11:22:59.623 g.a.s.s.t.SolrPersistApi pool-10-thread-8 [INFO] write 200 doc at:invoketrace success cost 228060
2017-06-23 11:22:59.625 g.a.s.s.t.SolrPersistApi pool-10-thread-5 [INFO] write 66 doc at:invoketrace success cost 226739
2017-06-23 11:22:59.626 g.a.s.s.t.SolrPersistApi pool-10-thread-7 [INFO] write 200 doc at:invoketrace success cost 167869
2017-06-23 11:23:32.242 STDERR Thread-2 [INFO] java.lang.OutOfMemoryError: Java heap space
2017-06-23 11:23:32.253 STDERR Thread-2 [INFO] Dumping heap to artifacts/heapdump ...
@
{code}

"
STORM-2595,Apply new code style to storm-solr,
STORM-2594,Apply new code style to storm-rocketmq,
STORM-2593,Apply new code style to storm-redis,
STORM-2592,Apply new code style to storm-pmml,
STORM-2591,Apply new code style to storm-opentsdb,
STORM-2590,Apply new code style to storm-mqtt,
STORM-2589,Apply new code style to storm-mongodb,
STORM-2588,Apply new code style to storm-metrics,
STORM-2587,Apply new code style to storm-kinesis,
STORM-2586,Apply new code style to storm-kafka-monitor,
STORM-2585,Apply new code style to storm-kafka,
STORM-2584,Apply new code style to storm-jms,
STORM-2583,Apply new code style to storm-jdbc,
STORM-2582,Apply new code style to storm-hive,
STORM-2581,Apply new code style to storm-hdfs,
STORM-2580,Apply new code style to storm-hbase,
STORM-2579,Apply new code style to storm-eventhubs,
STORM-2578,Apply new code style to storm-elasticsearch,
STORM-2577,Apply new code style to storm-druid,
STORM-2576,Apply new code style to storm-caasandra,
STORM-2575,Apply new code style to storm-autocreds,
STORM-2574,Apply new code style to storm-sql,
STORM-2573,Apply new code style to flux,
STORM-2572,Apply new code style to storm-submit-tools,
STORM-2571,Apply new code style to storm-webapp,
STORM-2570,Apply new code style to storm-core,
STORM-2569,Apply new code style to storm-client-misc,
STORM-2568,'api/vi/topology/:id/lag' returns empty json {},"Hello

I've tried to use storm-kafka-monitor, and it works fine on command line If I changed 'toollib/storm-kafka-monitor-*.jar' to 'toollib/storm-kafka-monitor-1.1.0.jar'.

{code}
{""my-kafka-topic-name"":{""0"":{""consumerCommittedOffset"": 74804998, ""logHeadOffset"": 74805483, ""lag"": 485},""1"":{""consumerCommittedOffset"": 74804998, ""logHeadOffset"": 74805485, ""lag"": 487},""2"":{""consumerCommittedOffset"": 74804995, ""logHeadOffset"": 74805485, ""lag"": 490},""3"":{""consumerCommittedOffset"": 74805001, ""logHeadOffset"": 74805488, ""lag"": 487},""4"":{""consumerCommittedOffset"": 74805011, ""logHeadOffset"": 74805484, ""lag"": 473},""5"":{""consumerCommittedOffset"": 74805009, ""logHeadOffset"": 74805485, ""lag"": 476},""6"":{""consumerCommittedOffset"": 74805008, ""logHeadOffset"": 74805483, ""lag"": 475},""7"":{""consumerCommittedOffset"": 74805010, ""logHeadOffset"": 74805484, ""lag"": 474},""8"":{""consumerCommittedOffset"": 73641446, ""logHeadOffset"": 74805488, ""lag"": 1164042},""9"":{""consumerCommittedOffset"": 73641448, ""logHeadOffset"": 74805489, ""lag"": 1164041},""10"":{""consumerCommittedOffset"": 73641443, ""logHeadOffset"": 74805483, ""lag"": 1164040},""11"":{""consumerCommittedOffset"": 73641445, ""logHeadOffset"": 74805487, ""lag"": 1164042},""12"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805486, ""lag"": 483},""13"":{""consumerCommittedOffset"": 74804999, ""logHeadOffset"": 74805482, ""lag"": 483},""14"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805483, ""lag"": 481},""15"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805484, ""lag"": 482},""16"":{""consumerCommittedOffset"": 74804994, ""logHeadOffset"": 74805482, ""lag"": 488},""17"":{""consumerCommittedOffset"": 74805002, ""logHeadOffset"": 74805489, ""lag"": 487},""18"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805488, ""lag"": 485},""19"":{""consumerCommittedOffset"": 74805003, ""logHeadOffset"": 74805489, ""lag"": 486}}}
{code}

but it gives empty result when I call below api.

{code}
/api/v1/topology/:id/lag
...

{
""MySpoutName"": {
""spoutLagResult"": {},
""spoutId"": ""MySpoutName"",
""spoutType"": ""KAFKA""
}
}
{code}

-I think that needs to fix ""groupid"" to ""group.id"" in TopologySpoutLag.java I debug it, but groupid is right.-

the reason was topics has square brackets in command. 



{code}
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] json configuration: {config.security.protocol=null, config.bootstrap.servers=kafka.xxx.com:9092, config.topics=[my-kafka-topic], config.groupid=my-storm-kafka-spout-groupid, topology.tasks=5}

2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] /my/program/storm/bin/storm-kafka-monitor
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -t
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] [my-kafka-topic]
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -g
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] my-storm-kafka-spout-groupid
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] -b
2017-06-23 19:55:56.725 o.a.s.u.TopologySpoutLag qtp426435961-51 [INFO] kafka.xxx.com:9092
{code}

the square brackets automatically added because of this
{code}
package org.apache.storm.kafka.spout;
public class NamedSubscription extends Subscription {
...
    @Override
    public String getTopicsString() {
        return String.valueOf(topics);
    }
{code}
topics is Collections. so String.valueOf returns value with square brackets.

I fixed the code that remove square brackets in TopologySpoutLag.java for my case. 
but I think that fixing 'getTopicsString of NamedSubscription.java in org.apache.storm.kafka.spout' is might be better. 
"
STORM-2567,Apply new code style to storm-server,"Put effort to reduce max allowed violation count greatly, ideally 0, but even can't get rid of all, do as many as possible."
STORM-2566,Apply new code style to storm-client,"Put effort to reduce max allowed violation count greatly, ideally 0, but even can't get rid of all, do as many as possible."
STORM-2565,Apply new code style to current codebase,"We've introduced code style and also introduced checkstyle, but also set max allowed violation count for each modules to let build pass.

We should put effort to reduce max allowed violation count greatly, ideally 0, but even we can't get rid of all, we should do as many as possible."
STORM-2561,Netty Client is closed but the Worker is already using that Client.,"The Worker 's Netty Client is been closed  and The field ""closing"" in backtype.storm.messaging.netty.Client has been updated  to ""true"".
{code:java}
@Override
    public void close() {
        if (!closing) {
            LOG.info(""closing Netty Client {}"", dstAddressPrefixedName);
            context.removeClient(dstAddress.getHostName(),dstAddress.getPort());
            // Set closing to true to prevent any further reconnection attempts.
            closing = true;
            waitForPendingMessagesToBeSent();
            closeChannel();
        }
    }
{code}

But the worker is already  using that Netty Client. Because of the field 'closing' is true,  Connect in the Client will never reconnecting the target.

{code:java}
public void run(Timeout timeout) throws Exception {
            if (reconnectingAllowed()) {
             .....
             } else {
                close();
                throw new RuntimeException(""Giving up to scheduleConnect to "" + dstAddressPrefixedName + "" after "" +
                        connectionAttempts + "" failed attempts. "" + messagesLost.get() + "" messages were lost"");

            }
{code}

{code:java}
    private boolean reconnectingAllowed() {
        return !closing;
    }
{code}

So, How can me find the cause why Worker close the NettyClient while the NettyClient is just working.

The logs is uploaded to Attachments by command ""cat stt-jstorm-spout-4-28-1497837924-worker-6709.log|grep '10.24.41.10:6710' > /tmp/ClientClosed.txt"".

Please let me know any other useful logs  i can support.
"
STORM-2560,Storm-Kafka on CDH 5.11 with kerberos security enabled.,"Hi,
 
I have installed Apache Storm 1.1.0 manually on CDH 5.11 cluster. This cluster is secured with kerberos. 
I have storm sample written which ingest data from kafka topic and inserts into HDFS directory in real time. So, this sample uses storm-kafka as well as storm-hdfs. 
When I run the storm topology it gives the following error in kafka-spout.

 {color:#d04437}2017-06-18 22:29:31.297 o.a.z.ClientCnxn Thread-14-kafka-spout-executor[5 5]-SendThread(localhost:2181) [INFO] Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error){color}
 
{color:#d04437}2017-06-18 22:29:31.571 k.c.SimpleConsumer Thread-14-kafka-spout-executor[5 5] [INFO] Reconnect due to error:
java.nio.channels.ClosedChannelException: null
        at kafka.network.BlockingChannel.send(BlockingChannel.scala:110) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:85) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) [stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) [stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:129) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) [storm-core-1.1.0.jar:1.1.0]
        at org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]{color}
 
Kafka version: 2.1.1-1.2.1.1.p0.18
 
There is no storm-kafka*.jat present in - ""/usr/local/storm""
But this sample was workin fine before kerberizing the cluster, even in this case.
 
 
I have tried the same example on Hortonworks and after adding the below code to set security protcol, the topology runs fine:
*spoutConfig.securityProtocol = ""SASL_PLAINTEXT"";*
After Adding above code in case of cloudera it gives error: ""Symbol not found""
 
Please let me know if you nedd any other information...
Thanks in advance.."
STORM-2554,Trident Kafka Spout Refactoring to Include Manual Partition Assignment,Incorporate changes done in STORM-2541 and do some refactoring to internal state partition management to make it cleaner and more properly handle partitions reassignment.
STORM-2553,JedisCluster does not support password,"Now jediscluster for storm-redis is based on jedis-2.8.1, which does not support password for cluster. We can update to jedis-2.9.0 and do some modify for JedisClusterConfig and JedisCommandsContainerBuilder to support password for cluster."
STORM-2550,Supervisor dies if a worker dies in Windows,"When both the Supervisor and Workers are running on a Windows Server 2012 R2, killing a worker using the task manager will cause the Supervisor to die a while after.

It's important to mention that the supervisor.log shows nothing to explain this before it dies. Here are the final logs:

{code:java}
2017-06-13 11:28:40.460 o.a.s.u.Utils SLOT_6705 [INFO] Error when trying to kill 16152. Process is probably already dead.
2017-06-13 11:28:40.460 o.a.s.u.Utils SLOT_6700 [INFO] Error when trying to kill 12048. Process is probably already dead.
2017-06-13 11:28:40.461 o.a.s.u.Utils SLOT_6704 [INFO] Error when trying to kill 6796. Process is probably already dead.
2017-06-13 11:28:40.476 o.a.s.u.Utils SLOT_6709 [INFO] Error when trying to kill 6264. Process is probably already dead.
2017-06-13 11:28:40.673 o.a.s.u.Utils SLOT_6706 [INFO] Error when trying to kill 3932. Process is probably already dead.
2017-06-13 11:28:40.674 o.a.s.u.Utils SLOT_6707 [INFO] Error when trying to kill 1324. Process is probably already dead.
2017-06-13 11:28:42.144 o.a.s.d.s.Container SLOT_6708 [INFO] Found 3868 running as SYSTEM, but expected it to be root
2017-06-13 11:28:42.144 o.a.s.d.s.Slot SLOT_6708 [WARN] SLOT 6708 all processes are dead...
{code}
"
STORM-2549,"The fix for STORM-2343 is incomplete, and the spout can still get stuck on failed tuples","Example:
Say maxUncommittedOffsets is 10, maxPollRecords is 5, and the committedOffset is 0.
The spout will initially emit up to offset 10, because it is allowed to poll until numNonRetriableTuples is >= maxUncommittedOffsets
The spout will be allowed to emit another 5 tuples if offset 10 fails, so if that happens, offsets 10-14 will get emitted. If offset 1 fails and 2-14 get acked, the spout gets stuck because it will count the ""extra tuples"" 11-14 in numNonRetriableTuples.

An similar case is the one where maxPollRecords doesn't divide maxUncommittedOffsets evenly. If it were 3 in the example above, the spout might just immediately emit offsets 1-12. If 2-12 get acked, offset 1 cannot be reemitted.

The proposed solution is the following:
* Enforce maxUncommittedOffsets on a per partition basis (i.e. actual limit will be multiplied by the number of partitions) by always allowing poll for retriable tuples that are within maxUncommittedOffsets tuples of the committed offset. Pause any non-retriable partitions if the partition has passed the maxUncommittedOffsets limit, and some other partition is polling for retries while also at the maxUncommittedOffsets limit. 

Example of this functionality:
MaxUncommittedOffsets is 100
MaxPollRecords is 10
Committed offset for partition 0 and 1 is 0.
Partition 0 has emitted 0
Partition 1 has emitted 0...95, 97, 99, 101, 103 (some offsets compacted away)
Partition 1, message 99 is retriable
We check that message 99 is within 100 emitted tuples of offset 0 (it is the 97th tuple after offset 0, so it is)
We do not pause partition 0 because that partition isn't at the maxUncommittedOffsets limit.
Seek to offset 99 on partition 1 and poll
We get back offset 99, 101, 103 and potentially 7 new tuples. Say the lowest of these is at offset 104.
The spout emits offset 99, filters out 101 and 103 because they were already emitted, and emits the 7 new tuples.
If offset 104 (or later) become retriable, they are not retried until the committed offset moves. This is because offset 104 is the 101st tuple emitted after offset 0, so it isn't allowed to retry until the committed offset moves."
STORM-2548,Simplify KafkaSpoutConfig,"Some suggestions for simplifying KafkaSpoutConfig off the mailing list:

* We should not duplicate properties that users would normally set in the KafkaConsumer properties map. We should just have a setter (setProp) for setting properties in that map. For instance, setGroupId is just duplicating a setting that the user should be able to set directly in the consumer properties.

* We should get rid of the key/value deserializer setters. Setting the deserializers as classes is something the user can just as well do by using setProp. The SerializableDeserializer class should be removed. It is only offering extra type safety in the case where the user is defining their own deserializer type, and has the opportunity to subclass SerializableDeserializer. The setters don't work with the built in Kafka deserializers."
STORM-2546,Kafka spout can stall / get stuck due to edge case with failing tuples,"The mechanism for replaying a failed tuple involves seeking the kafka consumer to the failing offset and then re-emitting it into the topology. A tuple, when emitted the first time, will have an entry created in OffsetManager. This entry will be removed only after the tuple is successfully acknowledged and its offset successfully committed. Till then, commits for offsets beyond the failing offset for that TopicPartition will be blocked.

It is possible that when the spout seeks the consumer to the failing offset, the corresponding kafka message is not returned in the poll response. This can happen due to that offset being deleted or compacted away. In this scenario that partition will be blocked from committing and progressing."
STORM-2544,Bugs in the Kafka Spout retry logic when using manual commit,"Situation: Spout configured to use manual commit with a finite number of retries.

In the above scenario if and when a tuple fails repeatedly and hits the retry limit, it will neither be scheduled for an attempt again nor properly accounted for in the ack() method and in OffsetManager. This will block commits for the partition that the tuple belongs to."
STORM-2542,Deprecate storm-kafka-client KafkaConsumer.subscribe API subscriptions on 1.x and remove them as options in 2.x,"Most of this is copied off the mailing list post:

We've recently seen some issues raised by users using the default subscription API in the new KafkaSpout (https://issues.apache.org/jira/browse/STORM-2514, https://issues.apache.org/jira/browse/STORM-2538).

A while ago an alternative subscription implementation was added (https://github.com/apache/storm/pull/1835), which uses the KafkaConsumer.assign API instead.

The {{subscribe}} API used by default causes Kafka to assign partitions to available consumers automatically. It allows a consumer group to keep processing even in the presence of crashes because partitions are reassigned when a consumer becomes unavailable.

The {{assign}} API used in the alternative subscription implementation leaves it up to the consuming code to figure out a reasonable partition distribution among a consumer group. The {{assign}} API is essentially equivalent to how the old storm-kafka spout distributes partitions across spout instances, and as far as I know it has worked well there.

Storm already ensures that all spout instances are running, and restarts them if they crash, so we're not really gaining much by using the subscribe API.

The disadvantages to using the subscribe API are:

* Whenever an executor crashes, the Kafka cluster reassigns all partitions. This causes all KafkaSpout instances in that consumer group to pause until reassignment is complete.

* The partition assignment is random, so it is difficult for users to predict which partitions are assigned to which spout task.

* The subscribe API is extremely likely to cause hangs and other weird behavior if the KafkaSpout is configured to run multiple tasks in an executor. When KafkaConsumer.poll is called during partition reassignment, it will block until the reassignment is complete. If there are multiple consumers in a thread, the first consumer to get called will block, and the other consumer will get ejected from the list of active consumers after a timeout, because it didn't manage to call poll during the rebalance. See the example code in https://issues.apache.org/jira/browse/STORM-2514, which runs two KafkaConsumers in one thread. The result is that they flip flop between being active, and most polls take ~30 seconds (the Kafka session timeout)

* The random assignment of partitions causes more message duplication than is necessary. When an executor crashes, all the other executors have their partitions reassigned. This makes it likely that some of them will lose a partition they had in-flight tuples on, which they will then be unable to commit to Kafka. The message is then reemitted by whichever KafkaSpout instance was assigned the partition. See https://issues.apache.org/jira/browse/STORM-2538

I'd like to drop support for the subscribe API, and switch to using the assign API by default.

The KafkaConsumer Javadoc even mentions applications like Storm as a case where the {{subscribe}} API doesn't really add value.

{quote}
If the process itself is highly available and will be restarted if it fails (perhaps using a cluster management framework like YARN, Mesos, or AWS facilities, or as part of a stream processing framework). In this case there is no need for Kafka to detect the failure and reassign the partition since the consuming process will be restarted on another machine. 
{quote}"
STORM-2541,Manual partition assignment doesn't work,"The manual partition assignment logic in ManualPartitionNamed/PatternSubscription is broken. The spout is unable to start. The subscription needs to call onPartitionsAssigned even if the current assignment is null, otherwise it becomes impossible to initialize the spout. The order of KafkaConsumer.assign and the calls to onPartitionsAssigned/Revoked is also wrong. The assignment must happen first, otherwise onPartitionsAssigned will get an IllegalStateException when it tries to call KafkaConsumer.seek on a partition the consumer is not yet assigned."
STORM-2540,Get rid of window compaction in WindowManager,"Storm's windowing support uses trigger and eviction policies to control the size of the windows passed to WindowingBolts. The WindowManager has a hard coded limit of 100 tuples before tuples will start getting evicted from the window, probably as an attempt to avoid overly huge windows when using time based eviction policies. Whenever a tuple is added to the window, the hard cap is checked, and if the number of tuples in the window exceeds the cap the WindowManager evaluates the EvictionPolicy for the tuples to figure out if some can be removed.

This hard cap is ineffective in most configurations, and has a surprising interaction with the count based policy.

If the windowing bolt is configured to use timestamp fields in the tuples to determine the current time, the WatermarkingXPolicy classes are used. In this configuration, the compaction isn't doing anything because tuples cannot be evicted until the WatermarkGenerator sends a new watermark, and when it does the TriggerPolicy causes the WindowManager to evict any expired tuples anyway.

If the windowing bolt is using the count based policy, compaction has the unexpected effect of hard capping the user's configured max count to 100. If the configured count is less than 100, the compaction again has no effect.

When the bolt is configured to use the tuple arrival time based policy, the compaction only has an effect if there are tuples older than the configured window duration, which only happens if the window happens to trigger slightly late. This can cause tuples to be evicted from the window before the user's bolt sees them. Even when tuples are evicted with the compaction mechanism they are kept in memory until the next time a window is presented to the user's bolt.

I think the compaction mechanism should be removed. The only policy that benefits is the time based policy, and in that case it would be better to just add a configurable max tuple count to that policy. "
STORM-2538,New kafka spout emits duplicate tuples,"Currently, KafkaSpout in storm-kafka-client can cause duplicate tuples to be emitted. Reason is the implementation of ConsumerRebalanceListener interface is called by kafka everytime a new executor comes up. However, on PartitionsRevoked we already have some in flight tuples and are emitting the same ones from the new executor on which the onPartitionsAssigned was called. We need to make sure that we emit only one tuple per kafka message."
STORM-2536,storm-autocreds adds jersey 1.x to worker classpath,"While storm-autocreds module excludes some critical (easy to make version conflict) dependencies but still contains jersey 1.9 which makes conflict against jersey 2.x.

We should exclude jersey and additional unneeded libraries if any."
STORM-2534,"Visualization API missing stats/instances for ""system"" components","The topology visualization api end point ( /api/v1/topology/TOPOLOGY-ID/visualization ) does not return correct ""stats"" values for ""system"" components __system and __acker.

See the following example *correct* response for a spout or bolt within a topology, shorten for brevity.  Under the stats key it lists all of the instances of that component that is deployed.

{code}
{
	""spout"": {
		...
		"":stats"": [{
			"":host"": ""e54bb273-2a8a-4320-b23f-7c7ace52c961-10.153.0.30"",
			"":port"": 6700,
			"":uptime_secs"": 0,
			"":transferred"": {
				...
			}
		}],
		...
	},
{code}

See the following response for the __system and __acker components.  They do *not* correctly list any entries under the stats key.

{code}
{
	""__system"": {
		"":type"": ""spout"",
		"":capacity"": 0,
		"":latency"": null,
		"":transferred"": null,
		"":stats"": [],
		"":link"": ""\/component.html?id=__system&topology_id=test-1-1495630798"",
		"":inputs"": []
	},
	""__acker"": {
		"":type"": ""spout"",
		"":capacity"": 0,
		"":latency"": null,
		"":transferred"": null,
		"":stats"": [],
		"":link"": ""\/component.html?id=__acker&topology_id=test-1-1495630798"",
		"":inputs"": [...]
	}
}
{code}"
STORM-2533,"Visualization API returns ""spout"" for system components","Hitting the visualization api end point ( /api/v1/topology/TOPOLOGY-ID/visualization ) returns system components labeled as spouts.  These should not be labeled as spouts, but instead 'system' or some other more appropriate label.

See the following example response:

{code}
{
	...
	""__system"": {
		"":type"": ""spout"",
		...
	},
	""__acker"": {
		"":type"": ""spout"",
		 ...
	}
}
{code}"
STORM-2532,Get rid of most uses of Utils.getAvailablePort,Utils.getAvailablePort is a racy way to acquire a free port. We should only use it in cases where there is no other option (e.g. tests that need a client to try connecting before the server is started)
STORM-2531,CheckPointState.Action should have a serializer registered,CheckPointStae.Action does is not registered in Kryo so users of stateful topologies must register it.  Like other core classes this should be registered by Storm at startup.
STORM-2529,KeyNotFoundException on topology undeploy,"On undeploying a topology, the following is logged in the nimbus log:

{code}2017-05-23T11:59:17,785 o.a.s.d.nimbus [INFO] Killing topology: <topology>
2017-05-23T11:59:18,284 o.a.s.d.nimbus [INFO] Cleaning up <topology>
2017-05-23T11:59:18,359 o.a.s.d.nimbus [INFO] Removing dependency jars from blobs - 
2017-05-23T11:59:28,498 o.a.s.d.nimbus [INFO] Cleaning up <topology>
2017-05-23T11:59:28,515 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:<topology>-stormcode.ser){code}

This causes the client making the thrift call to nimbus to undeploy the topology to block indefinitely"
STORM-2525,Fix flaky integration tests,"The integration tests fail fairly often, e.g. https://travis-ci.org/apache/storm/jobs/233690012. The tests should be fixed so they're more reliable."
STORM-2523,Trident : Cannot join two streams if only one is passed through a persistant aggregator.,"When i define a topology such :
Spout1 -> persistantAggregator -> Stream ->  join
Spout2 ->  Join (the same one)

Join never produces data.

To produce data i need to add a persistantAggregator in other stream.
This  persistantAggregator aggregate nothing and just repeat input tuple.


I join a java class testing each configuration

thx
                    "
STORM-2522,examples in package do not build with checkstyle issues,"{code}
$ mvn clean package
[INFO] Scanning for projects...
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 2.0.0-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ storm-starter ---
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (cleanup) @ storm-starter ---
[INFO]
[INFO] --- maven-checkstyle-plugin:2.17:check (validate) @ storm-starter ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1.468 s
[INFO] Finished at: 2017-05-18T15:58:20-05:00
[INFO] Final Memory: 26M/437M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-checkstyle-plugin:2.17:check (validate) on project storm-starter: Failed during checkstyle execution: Unable to find configuration file at location: storm-buildtools/storm_checkstyle.xml: Could not find resource 'storm-buildtools/storm_checkstyle.xml'. -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
{code}"
STORM-2521,"""storm sql"" fails since '--jars' can't handle wildcard","STORM-2191 changes the approach to include jars for classpath: before the patch, storm script lists jars in directory and add them to classpath one by one. After the patch, we just use wildcard which is great to shorten the classpath.

storm-sql-runtime jars use '--jars' to be uploaded to blobstore and '--jars' doesn't support wildcard. So unfortunately, STORM-2191 breaks ""storm sql"".

We should choose whether making --jars supporting wildcard, or have an exceptional case, lists jars in directory only for above case."
STORM-2520,AutoHDFS should prefer cluster-wise hdfs kerberos principal to global hdfs kerberos principal,"After STORM-2482, we can set cluster-wise principal and keytab (and configurations) instead of setting global principal and keytab for HDFS and HBase. 
(Hive will be supported via STORM-2501.)

In AutoHDFS there's a missed spot which always uses global principal, and it throws some errors when global principal is not set.

It should prefer cluster-wise principal to global principal."
STORM-2518,NPE during uploading dependency artifacts with secured cluster,"While adding ACL to USER from uploading artifacts, ""name"" field is actually optional for thrift specification, but Nimbus reads the value without checking null while fixing ACL.

{code}
2017-05-16 14:57:02.527 o.a.s.t.s.TThreadPoolServer pool-45-thread-136 [ERROR] Error occurred during processing of message.
java.lang.NullPointerException: null
        at org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser(BlobStoreAclHandler.java:382) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs(BlobStoreAclHandler.java:357) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableBlobMeta(BlobStoreAclHandler.java:306) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:103) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_112]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_112]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_112]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_112]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9064.beginCreateBlob(nimbus.clj:2047) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3430) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3414) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286) ~[storm-core-1.1.0.2.6.0.2-SNAPSHOT.jar:1.1.0.2.6.0.2-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{code}

Uploading artifacts fails and topology submission also fails."
STORM-2517,storm-hdfs writers can't be subclassed,"{{HdfsBolt.makeNewWriter()}} returns an AbstractHDFSWriter instead of an interface. AbstractHDFSWriter is public but its constructor is package-private so it can't actually be subclassed unless your class is in the {{org.apache.storm.hdfs.common}} package. I subclassed HDFSWriter but it required some workarounds.

Also {{AbstractHDFSWriter.offset}} is package-private and write() is final, so there is no way for a subclass to control or update the offset after {{doWrite()}}"
STORM-2516,WindowedBoltExecutorTest.testExecuteWithLateTupleStream is flaky,See https://travis-ci.org/apache/storm/jobs/232571820.
STORM-2515,Fix most checkstyle violations for storm-kafka-client,
STORM-2513,NPE possible in getLeader call,"The getLeader call actually reads data from two different locations

https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L2371-L2385

One is /leader-lock and the other is /nimbuses.  There is a really rare possibility that these two can get out of sync when the leader crashes and we read from leader election saying it is still the leader, but after that it's entry is removed from ZK for /nimbuses.  So we either need to make them not be separate entries, or we need to add in some kind of a retry when this happens.

Also NimbusClient has not retry built in.  Not all operations are idempotent, but we really should look at adding a retry with possibly switching to a new nimbus on idempotent operations."
STORM-2511,Submitting a topology with name containing unicode getting failed.,"
Below error occurs when a topology name contains  unicode characters.

{quote}
$ storm jar WordCountTopology-1.0-SNAPSHOT.jar examples.WordCountTopology ""wordcount-中文topology""

2624 [main] INFO  o.a.s.StormSubmitter - Submitting topology wordcount-中文topology in distributed mode with conf {""storm.zookeeper.topology.auth.scheme"":""digest"",""storm.zookeeper.topology.auth.payload"":""-8594815830934962206:-8598394253140221278"",""topology.workers"":2,""topology.debug"":true}
Exception in thread ""main"" java.lang.RuntimeException: AuthorizationException(msg:wordcount-中文topology-4-1483689231-stormconf.ser does not appear to be a valid blob key)
        at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:255)
        at org.apache.storm.StormSubmitter.submitTopology(StormSubmitter.java:310)
        at org.apache.storm.StormSubmitter.submitTopologyWithProgressBar(StormSubmitter.java:346)
        at org.apache.storm.StormSubmitter.submitTopologyWithProgressBar(StormSubmitter.java:327)
        at com.microsoft.example.KafkaReaderTop.main(KafkaReaderTop.java:39)
Caused by: AuthorizationException(msg:wordcount-中文topology-4-1483689231-stormconf.ser does not appear to be a valid blob key)
        at org.apache.storm.generated.Nimbus$submitTopology_result$submitTopology_resultStandardScheme.read(Nimbus.java:7628)
        at org.apache.storm.generated.Nimbus$submitTopology_result$submitTopology_resultStandardScheme.read(Nimbus.java:7596)
        at org.apache.storm.generated.Nimbus$submitTopology_result.read(Nimbus.java:7530)
        at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86)
        at org.apache.storm.generated.Nimbus$Client.recv_submitTopology(Nimbus.java:294)
        at org.apache.storm.generated.Nimbus$Client.submitTopology(Nimbus.java:278)
        at org.apache.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:243)
        ... 4 more
{quote}"
STORM-2510,adjust checkstyle configurations to decrease violations,"Adjust Storm's checkstyle configuration:
* 4-space indent instead of 2-space indent that is default with google_checks.xml
** More hand-written code in Storm is 4-space indented than 2-space indented.
* exclude the thrift generated code from checkstyle
** since we shouldn't be touching it anyways
* go with 120 character line-length limits instead of the default of 100 from google_checks.xml
** This cuts ~70% of the line-length violations.  We might wanna increase it even more.  140 would cut out ~90% of the line-length violations.

With those adjustments, the total number of violations will shrink by ~100,000 (~140,000 -> ~40,000).

We can decrease the existing violations even more if we upgrade the checkstyle version from 6.11.2 to 7.7.   I figured this out after noticing that IntelliJ had different (& fewer) violations when I ran the checkstyle plugin on the same module in IntelliJ as compared to with cmdline maven.  A further benefit of the newer checkstyle version is that it runs *way* faster.  As in ~6+ times faster."
STORM-2509,Writers in AbstractHDFSBolt are not closed/rotated when evicted from WritersMap,"When the eldest entry in the WritersMap in the AbstractHDFSBolt gets removed due to the number of writers exceeding maxWriters (see below), the writer is not closed and rotation actions are not executed (doRotationAndRemoveWriter is not called).

{code}
static class WritersMap extends LinkedHashMap<String, AbstractHDFSWriter> {
    final long maxWriters;

    public WritersMap(long maxWriters) {
        super((int)maxWriters, 0.75f, true);
        this.maxWriters = maxWriters;
    }

    @Override
    protected boolean removeEldestEntry(Map.Entry<String, AbstractHDFSWriter> eldest) {
        return this.size() > this.maxWriters;
    }
}
{code}"
STORM-2505,Kafka Spout doesn't support voids in the topic (topic compaction not supported),"Kafka maintains the spout progress (offsets for partitions) which can hold a value which no longer exists (or offset+1 doesn't exist) in the topic due to following reasons
* Topology stopped processing (or died) & topic got compacted (cleanup.policy=compact) leaving offset voids in the topic.
* Topology stopped processing (or died) & Topic got cleaned up (cleanup.policy=delete) and the offset.

When the topology starts processing again (or restarted), the spout logic suggests that the next offset has to be (committedOffset+1) for the spout to make progress, which will never be the case as (committedOffset+1) has been removed from the topic and will never be acked.

{code:title=OffsetManager.java|borderStyle=solid}
 if (currOffset == nextCommitOffset + 1) {            // found the next offset to commit
      found = true;
      nextCommitMsg = currAckedMsg;
      nextCommitOffset = currOffset;
} else if (currOffset > nextCommitOffset + 1) {
      LOG.debug(""topic-partition [{}] has non-continuous offset [{}]. It will be processed in a subsequent batch."", tp, currOffset);
}
{code}

A smart forwarding mechanism has to be built so as to forward the spout pivot to the next logical location, instead of a hardcoded single forward operation.
"
STORM-2504,"At the start of topology, the KafkaTridentSpoutOpaque will sometimes emit the first batch twice ","The unit test in the attachment can reproduce the problem, and there is my simple fix."
STORM-2502,Use PID in file name for heap dumps,"The default JVM options for the workers specify the path to use for the heap dumps, see 
[https://github.com/apache/storm/blob/1.x-branch/conf/defaults.yaml#L171], however when a memory error happens more than once for the same worker, only the first dump is kept as the file can't be overridden. Instead, would it make sense to use something like 
_""-XX:HeapDumpPath=artifacts/heapdump_<pid>.hprof""_ so that a different dump is generated for each JVM instance? Or is the current pattern used on purpose to avoid too much disk space being used?"
STORM-2500,waitUntilReady in PacemakerClient cannot be invoked,"Heartbeat fails due to waitUntilReady not being invoked correctly.

storm.yaml includes: storm.cluster.state.store: ""org.apache.storm.pacemaker.pacemaker_state_factory"" and pacemaker.servers: <someip>

Here's a strack trace:
{code}
[executor-heartbeat-timer] ERROR org.apache.storm.pacemaker.pacemaker-state-factory - Failed to set_worker_hb. Will make [6] more attempts.
"" java.lang.IllegalArgumentException: No matching field found: waitUntilReady for class org.apache.storm.pacemaker.PacemakerClient
    at clojure.lang.Reflector.getInstanceField(Reflector.java:271) ~[clojure-1.7.0.jar:?]
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:315) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.pacemaker.pacemaker_state_factory$get_pacemaker_write_client.invoke(pacemaker_state_factory.clj:110) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__12511$fn__12512.invoke(pacemaker_state_factory.clj:187) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$pacemaker_retry_on_exception$fn__12506.invoke(pacemaker_state_factory.clj:139) ~[storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$pacemaker_retry_on_exception.invoke(pacemaker_state_factory.clj:139) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__12511.set_worker_hb(pacemaker_state_factory.clj:183) [storm-core-1.1.0.jar:1.1.0]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) [clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) [clojure-1.7.0.jar:?]
	at org.apache.storm.cluster$mk_storm_cluster_state$reify__4395.worker_heartbeat_BANG_(cluster.clj:468) [storm-core-1.1.0.jar:1.1.0]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_121]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_121]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_121]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_121]
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) [clojure-1.7.0.jar:?]
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$do_executor_heartbeats.doInvoke(worker.clj:76) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.RestFn.invoke(RestFn.java:439) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__5542$exec_fn__1364__auto__$reify__5544$fn__5547.invoke(worker.clj:624) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$schedule_recurring$this__1737.invoke(timer.clj:105) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720$fn__1721.invoke(timer.clj:50) [storm-core-1.1.0.jar:1.1.0]
	at org.apache.storm.timer$mk_timer$fn__1720.invoke(timer.clj:42) [storm-core-1.1.0.jar:1.1.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
{code}"
STORM-2498,Download Full File link broken in 1.x branch,"The download link points to ""download?file=%5BLjava.lang.Object%3B%406d6db3b"" instead of something like ""download?file=wordcount-1-1493298799%2F6701%2Fworker.log""
"
STORM-2497,Support Shared Memory Scheduling in RAS,"In some cases bolt and or spouts can share memory, but the scheduler has not good way to express that.  We should be able to support this."
STORM-2496,Dependency artifacts should be uploaded to blobstore with READ permission for all,"When we submit topology via specific user with dependency artifacts, submitter uploads artifacts to the blobstore with user which runs the submission.

Since uploaded artifacts are uploaded once and shared globally, other user might need to use uploaded artifact. (This is completely fine for non-secured cluster.) In this case, Supervisor fails to get artifact and crashes in result.

{code}
2017-04-28 04:56:46.594 o.a.s.l.AsyncLocalizer Async Localizer [WARN] Caught Exception While Downloading (rethrowing)...
org.apache.storm.generated.AuthorizationException: null
	at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-04-28 04:56:46.597 o.a.s.d.s.Slot SLOT_6701 [ERROR] Error when processing event
java.util.concurrent.ExecutionException: AuthorizationException(msg:<user> does not have READ access to dep-org.apache.curator-curator-framework-jar-2.10.0.jar)
	at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[?:1.8.0_112]
	at java.util.concurrent.FutureTask.get(FutureTask.java:206) ~[?:1.8.0_112]
	at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:380) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:740) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
Caused by: org.apache.storm.generated.AuthorizationException
	at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
2017-04-28 04:56:46.597 o.a.s.u.Utils SLOT_6701 [ERROR] Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
	at org.apache.storm.utils.Utils.exitProcess(Utils.java:1774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
	at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) ~[storm-core-1.1.0.2.6.0.3-8.jar:1.1.0.2.6.0.3-8]
2017-04-28 04:56:46.599 o.a.s.d.s.Supervisor Thread-7 [INFO] Shutting down supervisor 775c158b-0a2d-40be-9e02-a9662d8bc5c4
{code}

So we need to upload artifacts with READ permission to all, or at least supervisor should be able to read them at all."
STORM-2494,KafkaSpout does not handle CommitFailedException,"In situations when tuple processing takes longer than session timeout, we get CommitFailedException and instead of recovering from it Storm worker dies.

{code}
2017-04-26 11:07:04.902 o.a.s.util [ERROR] Async loop died!
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:578) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:519) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:679) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:658) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:426) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:278) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:360) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:192) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:404) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1058) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:219) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) ~[storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-04-26 11:07:04.909 o.a.s.d.executor [ERROR] 
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured session.timeout.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:578) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:519) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:679) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:658) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:426) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:278) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:360) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:192) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1058) ~[stormjar.jar:3.0.2]
\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:404) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:384) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:219) ~[stormjar.jar:3.0.2]
\tat org.apache.storm.daemon.executor$fn__4976$fn__4991$fn__5022.invoke(executor.clj:644) ~[storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.util$async_loop$fn__557.invoke(util.clj:484) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]
2017-04-26 11:07:04.953 o.a.s.util [ERROR] Halting process: (\""Worker died\"")
java.lang.RuntimeException: (\""Worker died\"")
\tat org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
\tat org.apache.storm.daemon.worker$fn__5646$fn__5647.invoke(worker.clj:763) [storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.daemon.executor$mk_executor_data$fn__4863$fn__4864.invoke(executor.clj:274) [storm-core-1.1.0.jar:1.1.0]
\tat org.apache.storm.util$async_loop$fn__557.invoke(util.clj:494) [storm-core-1.1.0.jar:1.1.0]
\tat clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
\tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]

2017-04-26 11:07:44.507 o.a.s.k.s.KafkaSpoutRetryExponentialBackoff [DEBUG] Instantiated KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=2, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=10, timeUnit=SECONDS}}
2017-04-26 11:07:44.516 o.a.s.k.s.KafkaSpoutRetryExponentialBackoff [DEBUG] Instantiated KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=0, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=0, timeUnit=MILLISECONDS}}
2017-04-26 11:07:45.048 o.a.s.k.s.KafkaSpout [INFO] Kafka Spout opened with the following configuration: KafkaSpoutConfig{kafkaProps={enable.auto.commit=false, request.timeout.ms=30000, group.id=Group1, bootstrap.servers=192.168.1.143:9092, session.timeout.ms=20000}, key=org.apache.kafka.common.serialization.StringDeserializer@1b5080fd, value=org.apache.kafka.common.serialization.StringDeserializer@2720873b, pollTimeoutMs=200, offsetCommitPeriodMs=5000, maxUncommittedOffsets=1000, firstPollOffsetStrategy=UNCOMMITTED_EARLIEST, subscription=org.apache.storm.kafka.spout.NamedSubscription@7f068c1f, translator=org.apache.storm.kafka.spout.SimpleRecordTranslator@1f1ca6a2, retryService=KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=2, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=10, timeUnit=SECONDS}}}
2017-04-26 11:07:45.111 o.a.s.k.s.KafkaSpout [INFO] Kafka Spout opened with the following configuration: KafkaSpoutConfig{kafkaProps={enable.auto.commit=false, request.timeout.ms=30000, group.id=Group2, bootstrap.servers=192.168.1.143:9092, session.timeout.ms=20000}, key=org.apache.kafka.common.serialization.StringDeserializer@45ffa954, value=org.apache.kafka.common.serialization.StringDeserializer@4b384f9b, pollTimeoutMs=200, offsetCommitPeriodMs=5000, maxUncommittedOffsets=1000, firstPollOffsetStrategy=UNCOMMITTED_EARLIEST, subscription=org.apache.storm.kafka.spout.NamedSubscription@4f07c224, translator=org.apache.storm.kafka.spout.SimpleRecordTranslator@a0545a0, retryService=KafkaSpoutRetryExponentialBackoff{delay=TimeInterval{length=0, timeUnit=SECONDS}, ratio=TimeInterval{length=2, timeUnit=MILLISECONDS}, maxRetries=2147483647, maxRetryDelay=TimeInterval{length=10, timeUnit=SECONDS}}}
2017-04-26 11:07:45.297 o.a.s.k.s.NamedSubscription [INFO] Kafka consumer subscribed topics [topic-1]
2017-04-26 11:07:45.302 o.a.s.k.s.NamedSubscription [INFO] Kafka consumer subscribed topics [topic-2]
2017-04-26 11:07:45.456 o.a.s.k.s.KafkaSpout [INFO] Partitions revoked. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@32cbdbb0, topic-partitions=[]]
2017-04-26 11:07:45.463 o.a.s.k.s.KafkaSpout [INFO] Partitions revoked. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@275d5222, topic-partitions=[]]
2017-04-26 11:07:45.545 o.a.s.k.s.KafkaSpout [INFO] Partitions reassignment. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@275d5222, topic-partitions=[topic-1]]
2017-04-26 11:07:45.546 o.a.s.k.s.KafkaSpout [INFO] Partitions reassignment. [consumer-group=Group1, consumer=org.apache.kafka.clients.consumer.KafkaConsumer@32cbdbb0, topic-partitions=[topic-2]]
2017-04-26 11:07:45.551 o.a.s.k.s.i.OffsetManager [DEBUG] Instantiated OffsetManager{topic-partition=topic-1, fetchOffset=11803, committedOffset=11802, ackedMsgs=[]}
2017-04-26 11:07:45.551 o.a.s.k.s.i.OffsetManager [DEBUG] Instantiated OffsetManager{topic-partition=topic-2, fetchOffset=11801, committedOffset=11800, ackedMsgs=[]}
2017-04-26 11:07:45.552 o.a.s.k.s.KafkaSpout [INFO] Initialization complete
2017-04-26 11:07:45.552 o.a.s.k.s.KafkaSpout [INFO] Initialization complete
{code}

I think expected behaviour would be that KafkaSpout would recover from exception (client will reconnect and get partitions reassigned) without worker getting killed."
STORM-2489,Overlap and data loss on WindowedBolt based on Duration,"The attachment is my test script, one of my test results is:
```
expired=1...55
get=56...4024
new=56...4024
Recived=3969,RecivedTotal=3969
expired=56...4020
get=4021...8191
new=4025...8191
Recived=4171,RecivedTotal=8140
SendTotal=12175
expired=4021...8188
get=8189...12175
new=8192...12175
Recived=3987,RecivedTotal=12127
```
This test result shows that some tuples appear in the expired list directly, we lost these data if we just use get() to get tuples, this is the first bug.
The second: the tuples of get() has overlap, the getNew() seems alright.

The problem not happen definitely, may need to try several times.

Actually, I'm newbie about storm, so I'm not sure this is a bug indeed, or, I use it in wrong way?"
STORM-2488,The UI user Must be HTTP,"The UI user Must be HTTP. Otherwise, the UI page can not be authorized"
STORM-2487,getting com.mongodb.MongoBulkWriteException while trying to save bulk messages using apache storm mongo,"While trying to save bulk numbers of messages by using storm-mongo, we are getting below exception

com.mongodb.MongoBulkWriteException: Bulk write operation error on server mongoserver:27017. Write errors: [BulkWriteError{index=0, code=11000, message='E11000 duplicate key error collection: NextMDC.EMAIL index: _id_ dup key: { : ""22596
079-1260-44f1-b4df-a5857f48f22d"" }', details={ }}].
        at com.mongodb.connection.BulkWriteBatchCombiner.getError(BulkWriteBatchCombiner.java:176) ~[stormjar.jar:?]
        at com.mongodb.connection.BulkWriteBatchCombiner.throwOnError(BulkWriteBatchCombiner.java:205) ~[stormjar.jar:?]
        at com.mongodb.connection.BulkWriteBatchCombiner.getResult(BulkWriteBatchCombiner.java:146) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:190) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation$1.call(MixedBulkWriteOperation.java:168) ~[stormjar.jar:?]
        at com.mongodb.operation.OperationHelper.withConnectionSource(OperationHelper.java:230) ~[stormjar.jar:?]
        at com.mongodb.operation.OperationHelper.withConnection(OperationHelper.java:221) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:168) ~[stormjar.jar:?]
        at com.mongodb.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:74) ~[stormjar.jar:?]
        at com.mongodb.Mongo.execute(Mongo.java:781) ~[stormjar.jar:?]
        at com.mongodb.Mongo$2.execute(Mongo.java:764) ~[stormjar.jar:?]
        at com.mongodb.MongoCollectionImpl.insertMany(MongoCollectionImpl.java:323) ~[stormjar.jar:?]
        at org.apache.storm.mongodb.common.MongoDBClient.insert(MongoDBClient.java:61) ~[stormjar.jar:?]
        at org.apache.storm.mongodb.bolt.MongoInsertBolt.execute(MongoInsertBolt.java:85) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:464) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) [storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]

But for less number of messages it is working fine.
"
STORM-2485,re-include example jars in storm distribution,"Beginning with Apache 1.1.0, it appears the example jar ""examples/storm-starter/storm-starter-topologies-$STORMVERSION.jar"" is no longer included in the distribution.

I maintain a project that has a series of ""sanity tests"" when new versions of projects came out.  The WordCountTopology example in the storm-start topology was the ""sanity test"" we used.

It would certainly be nice to have the jar re-included as it would automatically allow the tests to be run.  I'm sure for others, it is a quick way to try out Storm rather than have to build the jars by hand."
STORM-2484,Flux: support bolt+spout memory configuration,"Storm has features to tune memory and CPU settings on a per-bolt or per-spout basis, with the setMemoryLoad and setCPULoad functions: https://storm.apache.org/releases/1.1.0/javadocs/index.html

Flux doesn't appear to support these features"
STORM-2483,wrong parameters order,"org.apache.storm.utils.Utils#getGlobalStreamId has wrong parameters order:
    
public static GlobalStreamId getGlobalStreamId(String streamId, String componentId) {
        if (componentId == null) {
            return new GlobalStreamId(streamId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(streamId, componentId);
    }

but GlobalStreamId constructor is:   public GlobalStreamId(
    String componentId,
    String streamId)

so i think the nice code is:
    public static GlobalStreamId getGlobalStreamId(String streamId, String componentId) {
        if (streamId == null) {
            return new GlobalStreamId(componentId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(componentId, streamId);
    }
"
STORM-2481,Upgrade Aether version to resolve Aether bug BUG-451566,"I received a report that storm-submit-tools throws NPE.

{code}
 /usr/hdf/current/storm-client/bin/storm: line 2: /usr/hdf/3.0.0.0-179/etc/default/hadoop: No such file or directory
Resolving dependencies on demand: artifacts (['org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12', 'org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12', 'org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12', 'org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12', 'org.apache.storm:storm-druid:1.0.2.3.0.0.0-179', 'org.scala-lang:scala-library:2.11.8']) with repositories (['hwx-public^http://repo.hortonworks.com/content/groups/public/', 'hwx-private^http://nexus-private.hortonworks.com/nexus/content/groups/public/'])
DependencyResolver input - artifacts: org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12,org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12,org.apache.storm:storm-kafka:1.0.2.3.0.0.0-179^org.slf4j:slf4j-log4j12,org.apache.kafka:kafka_2.11:0.10.0.2.5.3.0-37^org.apache.zookeeper:zookeeper^log4j:log4j^org.slf4j:slf4j-log4j12,org.apache.storm:storm-druid:1.0.2.3.0.0.0-179,org.scala-lang:scala-library:2.11.8
DependencyResolver input - repositories: hwx-public^http://repo.hortonworks.com/content/groups/public/,hwx-private^http://nexus-private.hortonworks.com/...
Exception in thread ""main"" java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.submit.command.DependencyResolverMain.main(DependencyResolverMain.java:82)
Caused by: java.lang.NullPointerException
        at org.sonatype.aether.impl.internal.DefaultRepositorySystem.resolveDependencies(DefaultRepositorySystem.java:352)
        at org.apache.storm.submit.dependency.DependencyResolver.resolve(DependencyResolver.java:95)
        at org.apache.storm.submit.command.DependencyResolverMain.main(DependencyResolverMain.java:71)
Traceback (most recent call last):
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 884, in <module>
    main()
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 881, in main
    (COMMANDS.get(COMMAND, unknown_command))(*ARGS)
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 295, in jar
    artifact_to_file_jars = resolve_dependencies(DEP_ARTIFACTS_OPTS, DEP_ARTIFACTS_REPOSITORIES_OPTS)
  File ""/usr/hdf/3.0.0.0-179/storm/bin/storm.py"", line 182, in resolve_dependencies
    raise RuntimeError(""dependency handler returns non-zero code: code<%s> syserr<%s>"" % (p.returncode, errors))
RuntimeError: dependency handler returns non-zero code: code<1> syserr<None>
{code}

There was also an issue filed to Eclipse bug tracker.
https://bugs.eclipse.org/bugs/show_bug.cgi?id=451566

The issue was fixed for newer version of Aether so we just need to upgrade Aether version to get over."
STORM-2477,Configs should have generics,"Config since the beginning has not really had generics it has just been a Map.  We should really have it be consistent everywhere a {{Map<String, Object>}}

This will reduce the number of warnings in the code base by a lot."
STORM-2475,NimbusInfo does not handle IPv6 addresses,"This is probably fairly minor, but NimbusInfo tries to parse {{host:port}} but I found myself in a situation where I was getting an IPv6 address and not a host name so the "":"" parsing became a problem."
STORM-2473,KafkaTridentSpoutOpaque's implementation is incorrect.,"The coordinator relies on emitter to start subscription, this is in correct since coordinator and emitter may run on different machines."
STORM-2468,Remove Clojure from storm-client,"It would be great to remove clojure as a dependency of storm-client.  We should start looking at moving as much out of storm-client as possible, as the initial separation left some things in there that should not be part of the client, but are not easy to separate just yet."
STORM-2467,Encoding issues in Kafka consumer,"The StringScheme of the storm-kafka consumer does not set an explicit charset in all cases, which leads to messages being decoded in the environment specific default charset. 

I have a PR for that to fix it
https://github.com/apache/storm/pull/2055"
STORM-2460,Test with Storm testings completeTopology and Maven surefire fail,"Running tests that use Storm testings completeTopology fail when running with Maven surefire in some environments.

Some tests are run successfully and it is not always the same phase of tests that fail.

It seems to be issue similar to STORM-130."
STORM-2459,Support SSL for Redis (Jedis 2.9.0),"Jedis 2.9.0 added SSL support.  This helps with connecting to hosted Redis environments, such as in Azure, which are SSL-only by default.

However, the Redis support in Storm doesn't currently expose an option to use this.  I would hope for something like:

JedisPoolConfig poolConfig = new JedisPoolConfig.Builder()
    .setHost(host)
    .setPort(port)
    .useSSL(true)
    .build();

Thanks."
STORM-2456,Error when running Flux on Windows in non-elevated command prompt,"Running:
{code}
mvn compile exec:java -Dexec.args=""--local -R /topology.yaml""
{code}

I get the following errors.

It works if I run in an elevated command prompt though.

{code}
17:22:48 [SLOT_1027] ERROR org.apache.storm.daemon.supervisor.Slot - Error when processing event
java.nio.file.FileSystemException: C:\Users\maurgi\AppData\Local\Temp\0924ce14-5d18-4da2-ab49-abfe37e59742\workers\c487e249-7225-4836-b7f0-b840f1a05732\artifacts: A required privilege is
 not held by the client.
        at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:102) ~[?:1.8.0_121]
        at sun.nio.fs.WindowsFileSystemProvider.createSymbolicLink(WindowsFileSystemProvider.java:585) ~[?:1.8.0_121]
        at java.nio.file.Files.createSymbolicLink(Files.java:1043) ~[?:1.8.0_121]
        at org.apache.storm.daemon.supervisor.AdvancedFSOps.createSymlink(AdvancedFSOps.java:354) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Container.createArtifactsLink(Container.java:383) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Container.setup(Container.java:321) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.LocalContainerLauncher.launchContainer(LocalContainerLauncher.java:44) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:387) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275) ~[storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:741) [storm-core-1.0.3.jar:1.0.3]
17:22:48 [SLOT_1027] ERROR org.apache.storm.utils.Utils - Halting process: Error when processing an event
java.lang.RuntimeException: Halting process: Error when processing an event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1749) [storm-core-1.0.3.jar:1.0.3]
        at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774) [storm-core-1.0.3.jar:1.0.3]
{code}"
STORM-2453,Move non-connectors into the top directory,"This issue is being discussed from dev@ mailing list.

http://mail-archives.apache.org/mod_mbox/storm-dev/201703.mbox/%3CCAF5108iYx7rxKWqzqncP_8un9OTv-a3wEgo90v0MZ1pfYzH25w%40mail.gmail.com%3E

We have a consensus to move non-connectors into out of ""external"", and most of participants are OK to move them to the top directory. Unless there's no further objection, we can just put them into the top directory."
STORM-2451,windows storm.cmd does not set log4j2 config file correctly by default,"When running e.g. nimbus with the default storm.yaml, the log config file is set to file:///log4j2\cluster.xml since the start script does not convert the relative dir into an absolute path. It works when setting the storm.log4j2.conf.dir explicitly.
"
STORM-2450,supervisor v2 broke ShellBolt/Spout in local mode from storm jar,"In local mode from the command line the localizer was placing the resources from the jar in the wrong directory.  resources/resource instead of resources.  It looks like this was broken when we ""fixed"" doing it for copying resources from outside the jar (unit tests)."
STORM-2449,"Iterator of Redis State may return same key multiple time, with different values","Redis state iterator iterates pending prepare -> pending commit -> external storage (state) sequentially. While iterating, more of them are subject to change, so it can provide inconsistent result.

While we can't provide consistent result (since states are changing continuously), at least iterator needs to provide same key only once."
STORM-2448,Support running workers using older JVMs/storm versions,"As a part of STORM-2441 we are separating out the classpaths for the client+worker process from everything else in storm.  This is great but it really will make some of our users upset, because it is not a rolling upgrade, and because they will need to recompile their topologies (again).

We have done a really good job in maintaining compatibility with older versions of storm because we use thrift for all communication and state storage.  This means that a new supervisor and or nimbus should be able to talk to just about any existing client/worker out there.  So we should explicitly support this.

We should add in config options to supervisors.

{{storm.supported.jvms}} which is a map of the version of the JVM to the JAVA_HOME path for it.

and

{{storm.legacy.worker.classpath}} which is a map of the version of storm to a CLASSPATH that can be used to launch a legacy worker process.

They should be set for all supervisors and nimbus.

Then we also add in some metadata that the client submits to nimbus along with a topology.  Namely the version of the storm client they are running on and the version of the JVM they are running on.

Nimbus can then decide (possibly through another config, but probably just through convention with some config overrides) if the version of the client + JVM is compatible with the version of storm + JVM currently on the cluster.  If so it should just let it through.  If not it should pick a version of the JVM + storm that is compatible.  If there are none available it will reject the request.  We should also allow end users to set these configs.

For this to work well we need some good version matching/sorting code that is lenient, even during rolling upgrades.

For example if a user submits a topology with a 1.0.3 client to a 2.0.0 cluster. Nimbus sees that it has 1.0.3 installed great it will start running with that, but then we do a rolling upgrade of the cluster and upgrade move to 1.0.4.  The supervisors should be able to launch the workers for that topology with a 1.0.4 classpath.

As such part of the worker heartbeat should also include the version of storm + JVM that the worker is running with.

We should display that on the UI and display the version it was submitted with on the UI too.

"
STORM-2447,Make local cluster transparent,"As part of the work for STORM-2441.  We would like to split the storm classpath down so the client jar only has what it needs.  Everything else is in a separate classpath(s) for daemon processes.  One of the issues with this is that local mode is built into almost all examples because it uses a separate API from the normal storm client API.

To work around this we really should add in a new option to {{storm jar}} that will include everything on the classpath, set a SystemProperty and call into a special Main Method.   The new main method will 

1) start up the LocalCluster
2) configure SotrmSubmitter, NimbusClinet and DRPCClient to talk to the LocalCluster instead.
3) run the regular main method
4) optionally sleep for a configurable amount of time
5) shut down the local cluster."
STORM-2446,Add more ml algorithm to storm-ml as a new external,"Create a new external named storm-ml that contained not only pmml but also other ml algorithm like recommend ,linear and so on."
STORM-2445,Topology log search refers supervisor ID as host of worker which contains UUID,"It seems to take supervisor ID as worker's host in topology log search, so API request to logviewer fails.

Please check attachment."
STORM-2444,Nimbus sometimes throws NPE when clicking show topology visualization button,"Here's error message from Nimbus (containing stack trace): 

{code}
{""error"":""Internal Server Error"",""errorMessage"":""java.lang.NullPointerException\n\tat org.apache.storm.stats.StatsUtil.mergeWithAddPair(StatsUtil.java:1997)\n\tat org.apache.storm.stats.StatsUtil.expandAveragesSeq(StatsUtil.java:2511)\n\tat org.apache.storm.stats.StatsUtil.aggregateAverages(StatsUtil.java:877)\n\tat org.apache.storm.stats.StatsUtil.aggregateBoltStats(StatsUtil.java:776)\n\tat org.apache.storm.stats.StatsUtil.boltStreamsStats(StatsUtil.java:942)\n\tat org.apache.storm.ui.core$visualization_data$iter__3002__3006$fn__3007.invoke(core.clj:239)\n\tat clojure.lang.LazySeq.sval(LazySeq.java:40)\n\tat clojure.lang.LazySeq.seq(LazySeq.java:49)\n\tat clojure.lang.Cons.next(Cons.java:39)\n\tat clojure.lang.RT.next(RT.java:674)\n\tat clojure.core$next__4112.invoke(core.clj:64)\n\tat clojure.core$dorun.invoke(core.clj:3010)\n\tat clojure.core$doall.invoke(core.clj:3025)\n\tat org.apache.storm.ui.core$visualization_data.invoke(core.clj:268)\n\tat org.apache.storm.ui.core$build_visualization.invoke(core.clj:591)\n\tat org.apache.storm.ui.core$fn__3641.invoke(core.clj:1204)\n\tat org.apache.storm.shade.compojure.core$make_route$fn__324.invoke(core.clj:100)\n\tat org.apache.storm.shade.compojure.core$if_route$fn__312.invoke(core.clj:46)\n\tat org.apache.storm.shade.compojure.core$if_method$fn__305.invoke(core.clj:31)\n\tat org.apache.storm.shade.compojure.core$routing$fn__330.invoke(core.clj:113)\n\tat clojure.core$some.invoke(core.clj:2570)\n\tat org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)\n\tat clojure.lang.RestFn.applyTo(RestFn.java:139)\n\tat clojure.core$apply.invoke(core.clj:632)\n\tat org.apache.storm.shade.compojure.core$routes$fn__334.invoke(core.clj:118)\n\tat org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1383.invoke(json.clj:56)\n\tat org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__918.invoke(multipart_params.clj:118)\n\tat org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__747.invoke(reload.clj:22)\n\tat org.apache.storm.ui.helpers$requests_middleware$fn__2903.invoke(helpers.clj:54)\n\tat org.apache.storm.ui.core$catch_errors$fn__3813.invoke(core.clj:1462)\n\tat org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2632.invoke(keyword_params.clj:35)\n\tat org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2675.invoke(nested_params.clj:84)\n\tat org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2604.invoke(params.clj:64)\n\tat org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__918.invoke(multipart_params.clj:118)\n\tat org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2890.invoke(flash.clj:35)\n\tat org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2876.invoke(session.clj:98)\n\tat org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2498.invoke(servlet.clj:127)\n\tat org.apache.storm.shade.ring.util.servlet$servlet$fn__2502.invoke(servlet.clj:136)\n\tat org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)\n\tat org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)\n\tat org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)\n\tat org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)\n\tat org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)\n\tat org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\n""}
{code}"
STORM-2443,Nimbus throws error when changing log level on UI topology page,"Here's stacktrace from Nimbus log:

{code}
2017-03-30 16:53:26.954 o.a.s.d.n.Nimbus pool-14-thread-56 [WARN] set log config topology exception. (topology id='rolling-1-1490860365')
java.lang.NullPointerException: null
        at org.apache.storm.daemon.nimbus.Nimbus.setLogConfig(Nimbus.java:2688) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3295) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3280) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]
{code}"
STORM-2442,modify the Usage Examples in storm-kafka-client's README.md,"The Usage Example in storm-kafka-client's README.md is Incorrect in some place.For example, This variables ,“kafkaSpoutConfig,kafkaSpoutStreams,kafkaConsumerProps,tuplesBuilder,retryService”, should be defined before it is used.And I think ""props"" should be named kafkaConsumerProps as there is a Map named kafkaConsumerProps ."
STORM-2440,Kafka outage can lead to lockup of topology,"During two somewhat extended outages of our Kafka cluster, we experienced a problem with our Storm topologies consuming data from that Kafka cluster.

Almost all our topologies just silently stopped processing data from some of the topics/partitions, an the only way to fix this situation was to restart those topologies.

I tracked down one occurrence of the failure to this worker, which was running one the KafkaSpouts:

{noformat}
2017-03-18 04:06:15.389 o.a.s.k.KafkaUtils [ERROR] Error fetching data from [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}] for topic [tagging_log]: [NOT_LEADER_FOR_PARTITION]
2017-03-18 04:06:15.389 o.a.s.k.KafkaSpout [WARN] Fetch failed
org.apache.storm.kafka.FailedFetchException: Error fetching data from [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}] for topic [tagging_log]: [NOT_LEADER_FOR_PARTITION]
        at org.apache.storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:213) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.fill(PartitionManager.java:189) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:138) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2017-03-18 04:06:15.390 o.a.s.k.ZkCoordinator [INFO] Task [1/1] Refreshing partition manager connections
2017-03-18 04:06:15.395 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=tagging_log, partitionMap={0=kafka-03:9092, 1=kafka-12:9092,
 2=kafka-08:9092, 3=kafka-05:9092}}
2017-03-18 04:06:15.395 o.a.s.k.KafkaUtils [INFO] Task [1/1] assigned [Partition{host=kafka-03:9092, topic=tagging_log, partition=0}, Partition{host=kafka-12:9092, topic=tagging_log, partit
ion=1}, Partition{host=kafka-08:9092, topic=tagging_log, partition=2}, Partition{host=kafka-05:9092, topic=tagging_log, partition=3}]
2017-03-18 04:06:15.395 o.a.s.k.ZkCoordinator [INFO] Task [1/1] Deleted partition managers: [Partition{host=kafka-08:9092, topic=tagging_log, partition=1}]
2017-03-18 04:06:15.396 o.a.s.k.ZkCoordinator [INFO] Task [1/1] New partition managers: [Partition{host=kafka-12:9092, topic=tagging_log, partition=1}]
2017-03-18 04:06:15.398 o.a.s.k.PartitionManager [INFO] Read partition information from: /log_processing/tagging/kafka-tagging-spout/partition_1  --> {""partition"":1,""off
set"":40567174332,""topology"":{""name"":""tagging-aerospike-1"",""id"":""tagging-aerospike-1-3-1489587827""},""topic"":""tagging_log"",""broker"":{""port"":9092,""host"":""kafka-08""}}
2017-03-18 04:06:25.408 k.c.SimpleConsumer [INFO] Reconnect due to error:
java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:86) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) [stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) [stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) [stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) [stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) [stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) [stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
2017-03-18 04:06:35.416 o.a.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.net.SocketTimeoutException
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) ~[storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:99) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) ~[stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) ~[stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[stormjar.jar:?]
        ... 5 more
2017-03-18 04:06:35.419 o.a.s.d.executor [ERROR] 
java.lang.RuntimeException: java.net.SocketTimeoutException
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:103) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:144) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648) ~[storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
Caused by: java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:211) ~[?:1.8.0_121]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[?:1.8.0_121]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_121]
        at org.apache.kafka.common.network.NetworkReceive.readFromReadableChannel(NetworkReceive.java:81) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.readCompletely(BlockingChannel.scala:129) ~[stormjar.jar:?]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:120) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:99) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:83) ~[stormjar.jar:?]
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:149) ~[stormjar.jar:?]
        at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:75) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:65) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.<init>(PartitionManager.java:94) ~[stormjar.jar:?]
        at org.apache.storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[stormjar.jar:?]
        ... 5 more
2017-03-18 04:06:35.442 o.a.s.d.executor [INFO] Got interrupted excpetion shutting thread down...
{noformat}

There were no more outputs in the log after that until the toplogy was manually killed.

As you can see the {{java.net.SocketTimeoutException}} escapes the storm-kafka code (probably a problem in and of itself), but the worker is not killed. The thread that calls the {{.nextTuple}} method of the spout is exited on the other hand.
This is the culprit line: https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L270

I see that this has been fixed in the Java port of the executor code by explicitly excluding {{java.net.SocketTimeoutException}} from the condition.
I will open a pull request with a backport tomorrow."
STORM-2439,HealthCheck feature does not work,"There are a few issues with this feature:

1. The default timeout value produces `java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long at org.apache.storm.command.HealthCheck.processScript(HealthCheck.java:79)` because the value, 5000, is automatically deserialized by Jackson as an Integer, but we attempt to cast it to a long. (I successfully worked around this by setting a timeout greater than the maximum int.)

2. The documentation says that a script should print ""ERROR"" if the node is unhealthy, but in fact the script must *also* exit with a non-zero exit code. This appears to be the opposite of what is intended, given a comment that says ""We treat non-zero exit codes as indicators that the scripts failed to execute properly, not that the system is unhealthy"". I believe the test in this line is inverted: https://github.com/apache/storm/blob/70102643e74d577728adf5f8719920d1bf60e98a/storm-core/src/jvm/org/apache/storm/command/HealthCheck.java#L97

3. Even with workarounds for the above two bugs, a failing health check does not cause workers to shut down in my testing with Storm 1.0.3. I have not determined the cause, but because the previous two issues suggest to me that this code is rarely if ever tested, I do not plan to investigate further at the moment.

If this feature is, as it appears, untested and non-functional, I would suggest that it be removed from the code and documentation."
STORM-2438,on-demand resource requirement scaling,"As a first step towards true elasticity in a storm topology we propose allowing rebalance to also modify the resource requirements for each bolt/spout in the topology.  It will not be automatic, but it will let users scale up and down the CPU/memory needed for a component."
STORM-2437,LocalCluster in Unit Test crash the VM,"When unit testing Storm, we use LocalCluster. There is nothing to say when the Unit Test is working, the Unit Test ends gracefully.

However, when there are RuntimeException, for instance in the prepare functions, Storm crash and calls in Utils.mkSuicideFn, which calls Runtime.getRuntime().exit. So the VM crash and this is contradictory to Maven Surefire design (http://maven.apache.org/surefire/maven-surefire-plugin/faq.html#vm-termination).

I searched many ways to either prevent Storm from exiting (using SecurityManager), or make Unit Test accept the crash of the forked process.

If the Unit Test s VM crash, surefire will be unable to continue. My suggestion is to allow a configuration of LocalCluster that avoids System.exit, but just kills the topology (and closes all ressources if possible, but in the short term, this is not really important in a forked process)."
STORM-2436,Custom Log4j2 appender not working with storm,"We have created below logger configuration in worker.xml of storm cluster

<!-- This is new appender we want to add -->
    <FDPRollingFile name=""RollingFileInfo"" filename=""${sys:storm.log.dir}/userlogs/info-${sys:logfile.name}""
        filepattern=""${sys:storm.log.dir}/userlogs/info-${sys:logfile.name}.%d{yyyy-MM-dd-HH-mm}"" append=""true"">
            <PatternLayout>
             <pattern>${patternFdpNew}</pattern>
           </PatternLayout>
            <Policies>
                <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
            </Policies> 
        </FDPRollingFile>
    <FDPRollingFile name=""RollingFileDebug"" filename=""${sys:storm.log.dir}/userlogs/debug-${sys:logfile.name}""
            filepattern=""${sys:storm.log.dir}/userlogs/debug-${sys:logfile.name}.%d{yyyy-MM-dd-HH-mm}"" append=""true"">
          <PatternLayout>
             <pattern>${patternFdpNew}</pattern>
           </PatternLayout>
            <Policies>
                <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
            </Policies> 
    </FDPRollingFile>
    <FDPRollingFile name=""RollingFileError"" filename=""${sys:storm.log.dir}/userlogs/error-${sys:logfile.name}""
            filepattern=""${sys:storm.log.dir}/userlogs/error-${sys:logfile.name}.%d{yyyy-MM-dd-HH-mm}"" append=""true"">
        <PatternLayout>
             <pattern>${patternFdpNew}</pattern>
        </PatternLayout>
        <Policies>
            <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
        </Policies>
    </FDPRollingFile>
<!-- This is new appender we want to add -->



We have created below logger configuration in worker.xml of storm cluster

       <!-- This is new appender we want to add -->
    <FDPRollingFile name=""RollingFileInfo"" filename=""${sys:storm.log.dir}/userlogs/info-${sys:logfile.name}""
        filepattern=""${sys:storm.log.dir}/userlogs/info-${sys:logfile.name}.%d{yyyy-MM-dd-HH-mm}"" append=""true"">
            <PatternLayout>
             <pattern>${patternFdpNew}</pattern>
           </PatternLayout>
            <Policies>
                <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
            </Policies> 
        </FDPRollingFile>
    <FDPRollingFile name=""RollingFileDebug"" filename=""${sys:storm.log.dir}/userlogs/debug-${sys:logfile.name}""
            filepattern=""${sys:storm.log.dir}/userlogs/debug-${sys:logfile.name}.%d{yyyy-MM-dd-HH-mm}"" append=""true"">
          <PatternLayout>
             <pattern>${patternFdpNew}</pattern>
           </PatternLayout>
            <Policies>
                <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
            </Policies> 
    </FDPRollingFile>
    <FDPRollingFile name=""RollingFileError"" filename=""${sys:storm.log.dir}/userlogs/error-${sys:logfile.name}""
            filepattern=""${sys:storm.log.dir}/userlogs/error-${sys:logfile.name}.%d{yyyy-MM-dd-HH-mm}"" append=""true"">
        <PatternLayout>
             <pattern>${patternFdpNew}</pattern>
        </PatternLayout>
        <Policies>
            <SizeBasedTriggeringPolicy size=""100 MB""/> <!-- Or every 100 MB -->
        </Policies>
    </FDPRollingFile>
<!-- This is new appender we want to add -->
and custom logger defined in below fashion

    <Logger name=""custom-logger"" additivity=""false"" level=""INFO"">
     <appender-ref ref=""RollingFileDebug"" level=""TRACE""/>
     <appender-ref ref=""RollingFileError"" level=""WARN""/>
     <appender-ref ref=""RollingFileInfo"" level=""INFO""/>
   </Logger>


In storm topology builder main class

   config.put(""topology.classpath"",""/usr/local/Cellar/storm/mylogger.jar"");
and

 In spout

private static org.slf4j.Logger _logger =LoggerFactory.getLogger(""custom-    logger"");

Now what happens it detect my jar and write one line of log in log file but post that doesn't log any line to the log file.

"
STORM-2434,Storm spout is not reading/emitting data in storm cluster mode (version 1.0.0),"I am using apache storm 1.0.0 both in local as well as cluster mode. For the spout, I am reading the data from kafka topic (I am using kafka 2.11-0.8.2.1). Spout is reading the data from kafka topic and also emitting the data when I am using storm in local mode but the storm spout is not emitting any data when I am running storm in cluster mode.

My topology implementation for reading kafka data is as follow:

brokerHosts = new ZkHosts(kafkaZookeeper);
SpoutConfig kafkaConfig = new SpoutConfig(brokerHosts, kafkaTopicIn, """", ""storm"");
kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout(""spout"", new KafkaSpout(kafkaConfig), 2);

My storm configuration file:
storm.zookeeper.servers:
- ""localhost""
storm.zookeeper.port: 2181
nimbus.seeds: [""localhost""]

storm.local.dir: ""/tmp/storm""

I am also not getting any error while submitting Storm topology in cluster mode.

Any idea why topology spout is not emitting any data in cluster mode ??
Any help would be greatly appreciated."
STORM-2432,Storm-Kafka-Client Trident Spout Seeks Incorrect Offset With UNCOMMITTED_LATEST Strategy,"With UNCOMMITED_LATEST offset, Storm-Kafka-Client Trident Spout doesn't read all the data.
See worker logs for of the same topology with UNCOMMITED_EARLIEST as well as UNCOMITTED_LATEST.
The source topic has 3 partition and I publish data k000000-k000499(k000000, v000000 etc.)
In particular for k000000 value the data is not picked by spout at all."
STORM-2430,Potential Race condition in Kafka Spout,"Kafka spout hangs when the number of uncommitted messages exceeds the max allowed uncommitted messages and some intermediate tuples have failed in down stream bolt.

Steps of reproduction.
Create a simple topology with one kafka spout and a slow bolt. 
In kafka spout set the maximum uncommitted messages to a small number like 100.
Bolt should process 10 tuples in second. And program it to fail on some random tuples. For eg: say tuple number 10 fails. Also assume  there is only 1 Kafka partition the spout reads from.

Spout on first execution of nextTuple() gets 110 records and emits them. At this point number of uncommitted message would be 110.
First 9 tuples are acked by the bolt. 10th tuple is failed by the bolt. KafkaSpout puts it on retry queue.
Tuple number 11 to 110 are acked by bolt . But spout only commits till offset 9.[link | https://github.com/apache/storm/blob/1.0.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L510]

Now, the number of uncommitted  messages = 110 - 9 = 101 > 100 (max allowed uncommitted messages)
No new records are polled from kafka.[link | https://github.com/apache/storm/blob/1.0.x-branch/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L239]. The spout is stuck as the nothing is polled. 

Solution is to explicitly go through retry queue explicitly and emit tuples that are ready on every nextTuple().
"
STORM-2429,non-string values in supervisor.scheduler.meta cause crash,"The type of values in supervisor.scheduler.meta is not validated, but if one is not a string, supervisors crash during Thrift serialization with:

java.lang.ClassCastException: java.lang.Boolean cannot be cast to java.lang.String
        at org.apache.storm.generated.SupervisorInfo$SupervisorInfoStandardScheme.write(SupervisorInfo.java:1241)
        at org.apache.storm.generated.SupervisorInfo$SupervisorInfoStandardScheme.write(SupervisorInfo.java:1049)
        at org.apache.storm.generated.SupervisorInfo.write(SupervisorInfo.java:923)
        at org.apache.storm.thrift.TSerializer.serialize(TSerializer.java:79)
        at org.apache.storm.serialization.GzipThriftSerializationDelegate.serialize(GzipThriftSerializationDelegate.java:40)
        at org.apache.storm.utils.Utils.serialize(Utils.java:210)
        at org.apache.storm.cluster.StormClusterStateImpl.supervisorHeartbeat(StormClusterStateImpl.java:419)
        at org.apache.storm.daemon.supervisor.timer.SupervisorHeartbeat.run(SupervisorHeartbeat.java:85)
        at org.apache.storm.daemon.supervisor.Supervisor.launch(Supervisor.java:202)
        at org.apache.storm.daemon.supervisor.Supervisor.launchDaemon(Supervisor.java:243)
        at org.apache.storm.daemon.supervisor.Supervisor.main(Supervisor.java:362)

I will attach a PR with a simple fix"
STORM-2428,Flux-core jar contains unpacked dependencies,"The jar file for flux-core contains classes from /org/apache/http/. This was not the case before and causes problems with projects which rely on a different version of http-client. 
I can't see any references to http-client in the pom though."
STORM-2427,Event logger enable/disable UI is not working as expected in master branch,Need to pull missing commits from 1.x branch
STORM-2426,First tuples fail after worker is respawn,"Topology with two Kafka spouts (org.apache.storm.kafka.spout.KafkaSpout) reading from two different topics with same consumer group ID. 

1. Kill the only worker process for topology
2. Storm creates new worker
3. Kafka starts rebalancing (log line 15-16)
4. Kafka rebalancing done (log line 18-19)
5. Kafka topics read and tuples emitted (log line 28-29)
6. Tuples immediately fail (log line 30-33)

The delay between tuples emitted and tuples failing is just some 10 ms. No bolts in topology received the tuples.

What could cause this? The assumption is that there are uncommitted messages in Spout when it is killed and those are retried.
"
STORM-2425,Storm Hive Bolt not closing open transactions,"Hive bolt will close connection only if parameter ""max_connections"" is exceeded or bolt dies. So if we open a connection to Hive via Hive bolt and some time later we stop producing messages to Hive bolt, connection will be maintained and corresponding transactions will be opened. This can be a problem if we launch two topologies and one of them will maintain open transactions doing nothing, and other will work writing messages to hive. At some point hive will launch compactions to collapse small delta files generated by Hive Bolt into one base file. But compaction wont launch if we have opened transactions."
STORM-2424,Supervisor fails silently if started with old supervisor/localstate content,"If the following method in LocalState encounters an Exception and throws a RuntimeException, the supervisor quits silently without generating an error. I had to debug this by connecting with a remote debugger. Instead the method should generate an error to the user as to the source of the error. In my case, because I was upgrading my installation, the problem was due to a missing parameter in the content under the supervisor/localstate directory.

    private Map<String, ThriftSerializedObject> partialDeserializeLatestVersion(TDeserializer td) {
        try {
            String latestPath = _vs.mostRecentVersionPath();
            Map<String, ThriftSerializedObject> result = new HashMap<>();
            if (latestPath != null) {
                byte[] serialized = FileUtils.readFileToByteArray(new File(latestPath));
                if (serialized.length == 0) {
                    LOG.warn(""LocalState file '{}' contained no data, resetting state"", latestPath);
                } else {
                    if (td == null) {
                        td = new TDeserializer();
                    }
                    LocalStateData data = new LocalStateData();
                    td.deserialize(data, serialized);
                    result = data.get_serialized_parts();
                }
            }
            return result;
        } catch(Exception e) {
            throw new RuntimeException(e);
        }
    }"
STORM-2423,Join Bolt : Use explicit instead of default window anchoring of emitted tuples,"Default anchoring will anchor each emitted tuple to every tuple in current window. This requires a very large numbers of ACKs from any downstream bolt.  If topology.debug is enabled, it also worsens the load on the system significantly. 

Letting the topo run in this mode (in particular with max.spout.pending disabled), could lead to the worker running out of memory and crashing.

Fix: Join Bolt should avoid using default window anchoring, and explicitly anchor each emitted tuple with the exact matching tuples form each inputs streams. This reduces the complexity of the tuple trees and consequently the reduces burden on the ACKing & messaging subsystems. "
STORM-2421,Support lists of childopts beyond just worker,"The following worker childopts configuration options all support both a string value and a list of strings value:
{code}
WORKER_CHILDOPTS
WORKER_PROFILER_CHILDOPTS
WORKER_GC_CHILDOPTS
TOPOLOGY_WORKER_CHILDOPTS
TOPOLOGY_WORKER_GC_CHILDOPTS
TOPOLOGY_WORKER_LOGWRITER_CHILDOPTS
{code}

Currently the following childopts configuration options only support strings:
{code}
NIMBUS_CHILDOPTS
LOGVIEWER_CHILDOPTS
UI_CHILDOPTS
PACEMAKER_CHILDOPTS
DRPC_CHILDOPTS
SUPERVISOR_CHILDOPTS
{code}

Please could lists be supported across all childopts options as it makes configuration management and building easier using automated tools such as Chef and Puppet."
STORM-2418,add proper log message on metrics server fails/absent,"my metrics consumer writes to opentsdb server.
when this server went down, i see the following error message in my storm jobs.
which is very misleading.
create a ticket, so we can address this in future versions better.

{code:language=java}
2017-03-16 01:22:43.648 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) [storm-core-0.10.0.jar:0.10.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_45]
Caused by: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
	at backtype.storm.util$get_iterator.invoke(util.clj:929) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:432) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.jar:0.10.0]
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:127) ~[storm-core-0.10.0.jar:0.10.0]
	... 6 more
{code}"
STORM-2415,Storm fails to properly handle Zookeeper hosts going down,"We run a storm cluster (v.1.0.3) on AWS and have 3 Zookeepers supporting it. Because AWS sometimes terminates VMs, we sometimes lose a Zookeeper instance. When this happens, the hostname cannot be resolved for that zookeeper instance as AWS has taken the VM away. We noticed that in this case storm fails to connect to zookeeper – even though there are still 2 Zookeeper instances running. It fails with an exception something like:
{noformat}
java.net.UnknownHostException: zookeeper3
  at java.net.InetAddress.getAllByName0(InetAddress.java:1280) 
  at java.net.InetAddress.getAllByName(InetAddress.java:1192) 
  at java.net.InetAddress.getAllByName(InetAddress.java:1126) 
  at org.apache.storm.shade.org.apache.zookeeper.client.StaticHostProvider.<init>(StaticHostProvider.java:61) 
  at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:445) 
  at org.apache.storm.shade.org.apache.curator.utils.DefaultZookeeperFactory.newZooKeeper(DefaultZookeeperFactory.java:29) 
  at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl$2.newZooKeeper(CuratorFrameworkImpl.java:150) 
  at org.apache.storm.shade.org.apache.curator.HandleHolder$1.getZooKeeper(HandleHolder.java:94) 
  at org.apache.storm.shade.org.apache.curator.HandleHolder.getZooKeeper(HandleHolder.java:55) 
  at org.apache.storm.shade.org.apache.curator.ConnectionState.reset(ConnectionState.java:218) 
  at org.apache.storm.shade.org.apache.curator.ConnectionState.start(ConnectionState.java:103) 
  at org.apache.storm.shade.org.apache.curator.CuratorZookeeperClient.start(CuratorZookeeperClient.java:190) 
  at org.apache.storm.shade.org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:259) 
  at org.apache.storm.zookeeper$mk_client.doInvoke(zookeeper.clj:86) 
  at clojure.lang.RestFn.invoke(RestFn.java:494)
  at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState.invoke(zookeeper_state_factory.clj:28) 
  at org.apache.storm.cluster_state.zookeeper_state_factory.mkState(Unknown Source) 
  <SNIP REST OF STACKTRACE>
{noformat}
Having done some research it looks like this error is caused by a bug in the Zookeeper client library. There is an issue for it here:
[https://issues.apache.org/jira/browse/ZOOKEEPER-1576]
This issue has been resolved in the version 3.5.x branch of Zookeeper. However, after 2.5 years and 3 releases the 3.5.x branch of Zookeeper is still in Alpha .
Despite the fact that it is in alpha, there is a branch of Curator (v.3.x.x) that uses it, but Storm uses Curator version 2.x.x – possibly because it doesn’t rely on alpha code. So the bug is still unpatched in Storm.
I realise that an upgrade to alpha code may be too much of a risk, but this problem is a serious issue for those running Storm in a containerised or cloud environment - so perhaps it may be worth considering?"
STORM-2414,Skip checking meta's ACL when subject has write privileges for any blobs,"When BlobStore.deleteBlob is called, it always tries to get blobs if not existed on local because the logic needs to check ACL with given subject. That is not necessary when syncing up blobs in follower Nimbuses.

More generically, some subjects have write privilege for any blobs (say, superuser or admin) and for them BlobStore doesn't need to check (even download) meta's ACL and just deletes them from storage."
STORM-2410,modify Trident-state.md,"The sentence, ""(One side note – once Kafka supports replication, it will be possible to have transactional spouts that are fault-tolerant to node failure, but that feature does not exist yet.)"" ,should be removed,because Kafka has supported replication."
STORM-2409,Storm-Kafka-Client KafkaSpout Support for Failed and Null Tuples,
STORM-2407,KafkaTridentSpoutOpaque Doesn't Poll Data From All Topic-Partitions When Parallelism Hint Not a Multiple Total Topic-Partitions,
STORM-2406,[Storm SQL] Change underlying API to Streams API (for 2.0.0),"Since we dropped features which conform to the Trident semantic, Storm SQL doesn't need to rely on Trident, which is micro-batch.

Both core API and Streams API are candidates, but we should implement some bolts when we decide to rely on core API, whereas we don't need to do that for Streams API. (If we need to, that's the point to improve Streams API.)

Streams API also provides windowing feature via tuple-to-tuple semantic, so it's ready for STORM-2405 too."
STORM-2403,Fix KafkaBolt test failure: tick tuple should not be acked,"From STORM-2387, I changed KafkaBolt to make sure it doesn't ack tick tuples. 
(Tick tuples are generated from each executor and don't trigger ACK_INIT so actually it should not be acked. Acker will keep them and remove some for message timeout so not a big deal though.) 

But I forgot to fix unit test for that, and also missed to check test result. This issue is for making quick fix for that. "
STORM-2402,KafkaSpout sub-classes should be able to customize tuple processing,"We need a {{KafkaSpout}} that writes unprocessable records to a ""dead-letter-topic"". For this to function we sub-classed {{KafkaSpout}} and added the corresponding code. Without the incoming patch sub-classses can not have access to the actual tuples/records but just the {{KafkaSpoutMessageId}} in {{ack()}} and {{fail()}}."
STORM-2401,org.apache.storm.deamon.supervisor can not be found,"org.apache.storm.deamon.supervisor can not be found.
:supervisor
  set CLASS=org.apache.storm.daemon.supervisor

 in storm-core-1.0.3.jar . I can not find org.apache.storm.deamon.supervisor class, but can find org.apache.storm.deamon.supervisor$_main.class."
STORM-2399,How to configure different logback.xml in storm-project for each topology in the cluster.?,"I hava a storm cluster that runs multiple Topologies ,  I changed the ${storm_dir}/logback/cluster.xml in supervisor machines, so my topologies are using the same log configurations, how to configure the logback.xml in different project instead of changing it in storm-cluser? "
STORM-2397,modify storm-kafka.md,"KafkaBolt doesn't have a static variable named ""KAFKA_BROKER_PROPERTIES"" in storm 1.0.2.So storm-kafka.md should be modified."
STORM-2395,storm.cmd supervisor calls the wrong class name,"When running storm supervisor in Windows, the script tries to call org.apache.storm.daemon.supervisor. The class was renamed since it was changed from Clojure to Java recently, storm.cmd has not been changed accordingly.

current behaviour:

{quote}
λ storm supervisor

Error: Could not find or load main class org.apache.storm.daemon.supervisor
{quote}

expected behaviour:
... starts the daemon

"
STORM-2394,KafkaSpout: Has no leader of partitions for a short time,"In our case, there is something wrong with network for a short time. So some partitions of Kafka have no leaders.
The nextTuple of KafkaSpout throw an exception of ""No leader found for partition 0"" at the position of ""_coordinator.refresh();"". The exception is from the function getLeaderFor in DynamicBrokersReader.java. So the spout is hanged.
The partitions of Kafka have recover for a short time. But the spout can not deal with this problem. This problem appears several times on our server. Such as:
Feb 25 06:31:19 CST 2017, KafkaSpout threw the exception.
Feb 25 06:31:21 CST 2017, Kafka partitions recoverd.
To be stronger, I think that the ""_coordinator.refresh();"" can try times. At the last time, throw the exception. Anyway, it will die, why not try one more time?"
STORM-2392,Thrift source code generated by Storm not found,"In Maven, we can find storm-core-sources.jar, and this file contains Storm sources files. But it does not contains the Thrift source code, that Storm renamed in order to have multiple Thrift in Storm. We need source code to be in this Jar, so something should be done by storm assembly that generated Thrift code, so that these considered as source code by Maven when it deploys to Nexus.

Use case: I had to dig inside of Thrift Storm github page, download the matching branch, etc., rename the package because of thrift7 instead of thrift package etc. There are many things to be done in order to investigate Storm code, while it is much much simpler if Storm includes Thrift source natively."
STORM-2391,HdfsSpoutTopology example needs to be moved into storm-hdfs-examples from storm-starter,
STORM-2390,The storm-*-examples jars are missing in the binary distro,
STORM-2389,Event Logger bolt is instantiated even if topology.eventlogger.executors=0,
STORM-2388,JoinBolt breaks compilation against JDK 7,"STORM-2334 introduces compilation error on JDK 7, and it's included to 1.1.0 RC2.
We should fix it shortly."
STORM-2386,Fail-back Blob deletion also fails in BlobSynchronizer.syncBlobs,"This is a bug introduced from STORM-2321.

{code}
            for (String key : keySetToDownload) {
                try {
                    Set<NimbusInfo> nimbusInfoSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);
                    if (BlobStoreUtils.downloadMissingBlob(conf, blobStore, key, nimbusInfoSet)) {
                        BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);
                    }
                } catch (KeyNotFoundException e) {
                    LOG.debug(""Detected deletion for the key {} - deleting the blob instead"", key);
                    // race condition with a delete, delete the blob in key instead
                    blobStore.deleteBlob(key, BlobStoreUtils.getNimbusSubject());
                }
            }
{code}

'keySetToDownload' are keys which exist in Zookeeper, and do not exist in local. So deleting blob in local doesn't make sense. (Seems like I was confused at that time.)

if downloading throws KeyNotFoundException, it means that the blob is not available neither Zookeeper nor local, so just skipping would be OK."
STORM-2385,pacemaker_state_factory.clj does not compile on branch-1.0.x,"Assigning to Kyle because it looks like a fix for another issue broke this.

{code}
commit a6b9668c98b46a7acfb9d4f39c6e90342b2f41f7
Author: Kyle Nusbaum <knusbaum at yahoo dash inc dot com>
Date:   Tue Feb 21 14:18:31 2017 -0600

    Fixing pacemaker delete-path bug.
{code}"
STORM-2382,log4j and slf4j conflicting libraries issue,"my project storm 1.0.1 job's dependencies (log4j & slf4j) conflict with apache&hdp storm 1.0.1 default libraries (STORM_HOME/lib) and is preventing submitting new storm job.

* 1. shadow my storm job jar *
I get the following error:

{code:language=java}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/hdp/2.5.0.0-1245/storm/lib/log4j-slf4j-impl-2.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/tmp/356865dafc1a11e69341ecb1d7ac1510.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread ""main"" java.lang.IllegalAccessError: tried to access method org.apache.logging.log4j.core.lookup.MapLookup.newMap(I)Ljava/util/HashMap; from class org.apache.logging.log4j.core.lookup.MainMapLookup
	at org.apache.logging.log4j.core.lookup.MainMapLookup.<clinit>(MainMapLookup.java:37)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.logging.log4j.core.util.ReflectionUtil.instantiate(ReflectionUtil.java:185)
	at org.apache.logging.log4j.core.lookup.Interpolator.<init>(Interpolator.java:65)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.doConfigure(AbstractConfiguration.java:346)
	at org.apache.logging.log4j.core.config.AbstractConfiguration.start(AbstractConfiguration.java:161)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:359)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:420)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:138)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:147)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:175)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:102)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:43)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:42)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:29)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:277)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)
	at org.apache.storm.utils.LocalState.<clinit>(LocalState.java:45)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at org.apache.storm.config__init.__init7(Unknown Source)
	at org.apache.storm.config__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at clojure.lang.RT.loadClassForName(RT.java:2182)
	at clojure.lang.RT.load(RT.java:436)
	at clojure.lang.RT.load(RT.java:412)
	at clojure.core$load$fn__5448.invoke(core.clj:5866)
	at clojure.core$load.doInvoke(core.clj:5865)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5671)
	at clojure.core$load_lib$fn__5397.invoke(core.clj:5711)
	at clojure.core$load_lib.doInvoke(core.clj:5710)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:632)
	at clojure.core$load_libs.doInvoke(core.clj:5753)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:634)
	at clojure.core$use.doInvoke(core.clj:5843)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at org.apache.storm.command.config_value$loading__5340__auto____12764.invoke(config_value.clj:16)
	at org.apache.storm.command.config_value__init.load(Unknown Source)
	at org.apache.storm.command.config_value__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at clojure.lang.RT.classForName(RT.java:2154)
	at clojure.lang.RT.classForName(RT.java:2163)
	at clojure.lang.RT.loadClassForName(RT.java:2182)
	at clojure.lang.RT.load(RT.java:436)
	at clojure.lang.RT.load(RT.java:412)
	at clojure.core$load$fn__5448.invoke(core.clj:5866)
	at clojure.core$load.doInvoke(core.clj:5865)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.lang.Var.invoke(Var.java:379)
	at org.apache.storm.command.config_value.<clinit>(Unknown Source)
{code}


* 2. upgrade STORM_HOME/lib *
I get this warning.  looks like a racing condition (?)

{code:language=java}
log4j:WARN No appenders could be found for logger (org.apache.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
log4j:WARN No appenders could be found for logger (org.apache.storm.utils.Utils).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{code}"
STORM-2381,Add logging of JDBC connection string in storm-jdbc integration for debugging failures,"If there is an issue with the jdbc connection string, the user has to go back to the code and determine what it could be. They won't be able to determine if there is a bug in user code that is setting the jdbc connection string.

In order to make debugging easier we should print the jdbc connection string."
STORM-2380,worker.childopts with whitespace inside one param will be split into pieces,"worker.childopts params with whitespace inside, like -XX:OnError=""pstack %p >~/pstack%p.log"", will be split into pieces for supervisor use string.split(""\\s+"") to split params."
STORM-2379,[storm-elasticsearch] switch ES client to Java REST API,"following documentation:
https://storm.apache.org/releases/1.0.1/storm-elasticsearch.html



https://github.com/apache/storm/blob/master/external/storm-elasticsearch/pom.xml#L40

this causes errors while writing to elastic 5.x

{code:language=java}
java.lang.NoClassDefFoundError: org/elasticsearch/common/base/Preconditions
	at org.apache.storm.elasticsearch.common.EsConfig.<init>(EsConfig.java:62) ~[storm-elasticsearch-1.0.2.jar:1.0.2]
	at org.apache.storm.elasticsearch.common.EsConfig.<init>(EsConfig.java:49) ~[storm-elasticsearch-1.0.2.jar:1.0.2]

Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.base.Preconditions
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_112]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_112]
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_112]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_112]


261538 [elasticsearch[Ringleader][generic][T#2]] INFO  o.e.c.transport - [Ringleader] failed to get node info for [#transport#-1][svaddi][inet[localhost/127.0.0.1:9200]], disconnecting...
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[localhost/127.0.0.1:9200]][cluster:monitor/nodes/info] request_id [26] timed out after [5005ms]
	at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529) ~[elasticsearch-1.6.0.jar:?]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{code}


elastic logs:
{code:language=java}
[2017-02-23T15:47:04,487][WARN ][o.e.t.n.Netty4Transport  ] [Qt9qlNV] exception caught on transport layer [[id: 0x8f15e875, L:/127.0.0.1:9300 - R:/127.0.0.1:52031]], closing connection
java.lang.IllegalStateException: Received message from unsupported version: [1.0.0] minimal compatible version is: [5.0.0]
	at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1199) ~[elasticsearch-5.0.0.jar:5.0.0]
	at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74) ~[transport-netty4-5.0.0.jar:5.0.0]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:610) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:513) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:467) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:437) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-common-4.1.5.Final.jar:4.1.5.Final]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
{code}"
STORM-2374,Storm Kafka Client Func Interface Must be Serializable,
STORM-2372,Pacemaker client doesn't clean up heartbeats properly.,"Paths are not deleted correctly. Pacemaker's delete-path operates by matching a prefix against all the keys in the map.

The issue here is that the prefix is given a '/' on the end, but keys don't have a trailing '/' if there is no 'subkey'.

i.e. delete path /foo/bar/baz/ doesn't match the key /foo/bar/baz
The path has to have the trailing '/' so that delete path /foo/bar/baz doesn't also delete /foo/bar/bazoo

The solution here is to tack on a '/' to every key when checking against the prefix.

We also want to send the delete command to *every* pacemaker server rather than just the normal write client.
"
STORM-2366,SpoutTracker does not delegate to all methods,"h2. Problem
{{SpoutTracker}} does not implement and delegate to the following methods:

- {{activate}}
- {{deactivate}}
- {{getComponentConfiguration}}

h2. Effect
This causes problems for spouts that require initialization in or to be operational.

h2. Solution
The recommended fix is adding the following:

{code}
    @Override
    public void activate() {
        _delegate.activate();
    }

    @Override
    public void deactivate() {
        _delegate.deactivate();
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return _delegate.getComponentConfiguration();
    }
{code}
"
STORM-2364,Ensure kafka-monitor jar is included in classpath for UI process,The kafka-monitor jar has been moved into  toollib/ and not featuring in UI process's classpath. Leading to ClassNotFoundException for KafkaOffsetLagUtil class ... seen in the ui.log
STORM-2362,Cassandra Spout,
STORM-2361,"Kafka spout - after topic leader change, it stops committing offsets to ZK","After STORM-2296 although Kafka spouts do not generate duplicates, the offsets committment to ZK may stop on recreated PartitionManagers.

This is because ack's for messages emitted by already destroyed PartitionManagers are not routed properly to the new PartitionManagers handling that partition.

E.g: 
{code:java} public void ack(Object msgId) {
        KafkaMessageId id = (KafkaMessageId) msgId;
        PartitionManager m = _coordinator.getManager(id.partition);
        if (m != null) {
            m.ack(id.offset);
        }
{code}
id.partition is Partition(host, partition, topic), which is different if Kafka broker changed."
STORM-2360,Storm-Hive: Thrift version mismatch with storm-core,"Storm-Hive's libthrift version (0.9.0) is not in sync with storm-core's libthrift version (0.9.3) on branch-1.0.x. 

This issue has been resolved for master and 1.x-branch but was not put in the 1.0.x-branch. 

Related commits:
* master: [5df06bf523754d2bae35c23c24a6c6ffb99e4d9f|https://github.com/apache/storm/commit/5df06bf523754d2bae35c23c24a6c6ffb99e4d9f]
* branch-1.0.x: [60506cf8432366d0e5799974f374b4de95da8abe| https://github.com/apache/storm/commit/60506cf8432366d0e5799974f374b4de95da8abe]

Credit goes to [~ptgoetz] for discovering this issue."
STORM-2359,Revising Message Timeouts,"A revised strategy for message timeouts is proposed here.

Design Doc:
 https://docs.google.com/document/d/1am1kO7Wmf17U_Vz5_uyBB2OuSsc4TZQWRvbRhX52n5w/edit?usp=sharing"
STORM-2358,Update storm hdfs spout to remove specific implementation handlings,"I was looking at storm hdfs spout code in 1.x branch, I found below
improvements can be made in below code.

  1.  Make org.apache.storm.hdfs.spout.AbstractFileReader as public so
that it can be used in generics.

  2.  org.apache.storm.hdfs.spout.HdfsSpout requires readerType as
String. It will be great to have class<? extends AbstractFileReader>
readerType; So we will not use Class.forName at multiple places also it
will help in below point.

  3.  HdfsSpout also needs to provide outFields which are declared as
constants in each reader(e.g.SequenceFileReader). We can have abstract
API AbstractFileReader in which return them to user to make it generic."
STORM-2356,Storm-HDFS: NPE on empty & stale lock file,"In HDFSSpout a NPE can occur if a stale lock file is empty.

{{LogEntry.deserialize}} tries to split the line by colons.
If the line is null, the split will cause a NPE:
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java#L179

Moreover the callee of {{getLastEntry}} is also mishandling empty log files.
The {{lastEntry.eventTime}} could also cause a NPE if the above scenario is passed and the log file is empty:
https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java#L149-L160"
STORM-2354,Upgrade to Driver 3.1,"Added a pull request at https://github.com/apache/storm/pull/1936

"
STORM-2352,New Kafka spout retries for ever even with retries of 5,"v1.0.0 and above

KafkaSpout is created with a KafkaSpoutConfig having maxRetries of 5. Still the KafkaSpout retries the failed Tuple forever. 

Reason:
The numFails are incremented in fail() method of KafkaSpout.
{code}
public void fail(Object messageId) {
        final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;
        emitted.remove(msgId);
        if (msgId.numFails() < maxRetries) {
            msgId.incrementNumFails();
            retryService.schedule(msgId);
        } else { // limit to max number of retries
            LOG.debug(""Reached maximum number of retries. Message [{}] being marked as acked."", msgId);
            ack(msgId);
        }
    }
{code}

However the emitTupleIfNotEmitted() creates a new KafkaSpoutMessageId  and checks if the msgId is ready to be emitted (in the case of failure) and if so emits the new msgId instance (thus losing the numFails from the previous time)

{code}
    private void emitTupleIfNotEmitted(ConsumerRecord<K, V> record) {
        final TopicPartition tp = new TopicPartition(record.topic(), record.partition());
        final KafkaSpoutMessageId msgId = new KafkaSpoutMessageId(record);

        if (acked.containsKey(tp) && acked.get(tp).contains(msgId)) {   // has been acked
            LOG.trace(""Tuple for record [{}] has already been acked. Skipping"", record);
        } else if (emitted.contains(msgId)) {   // has been emitted and it's pending ack or fail
            LOG.trace(""Tuple for record [{}] has already been emitted. Skipping"", record);
        } else if (!retryService.isScheduled(msgId) || retryService.isReady(msgId)) {   // not scheduled <=> never failed (i.e. never emitted) or ready to be retried
            final List<Object> tuple = tuplesBuilder.buildTuple(record);
            kafkaSpoutStreams.emit(collector, tuple, msgId);
            emitted.add(msgId);
            numUncommittedOffsets++;
            if (retryService.isReady(msgId)) { // has failed. Is it ready for retry ?
                retryService.remove(msgId);  // re-emitted hence remove from failed
            }
            LOG.trace(""Emitted tuple [{}] for record [{}]"", tuple, record);
        }
    }
{code}

isReady() is not a side-effect. It just looks up and returns true. Fix is to either modify the RetryService interface to convey back the msgId in the RetryService or make the isReady() a side-effect to attach the numFails from the previous time OR to add 'failed' to KafkaSpout to keep track of failed msgs (similar to acked) and use the msgId from the failed to emit if isReady() is true"
STORM-2351,Unable to build native code on OS X,"Compilation of Storm fails under OS X if native profile is enabled for multiple reasons:

1)
{code}
~/w/storm ❯❯❯ mvn clean install -Pnative -DskipTests
....
....
[INFO] --- exec-maven-plugin:1.2.1:exec (default) @ storm-core ---
cp: illegal option -- u
usage: cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file target_file
       cp [-R [-H | -L | -P]] [-fi | -n] [-apvX] source_file ... target_directory
{code}

The problem is caused by the lack of ""u"" (upgrade) flag of cp.

2)
{code}
[INFO] 	gcc -DPACKAGE_NAME=\""worker-launcher\"" -DPACKAGE_TARNAME=\""worker-launcher\"" -DPACKAGE_VERSION=\""1.0.0\"" -DPACKAGE_STRING=\""worker-launcher\ 1.0.0\"" -DPACKAGE_BUGREPORT=\""user@storm.apache.org\"" -DPACKAGE_URL=\""\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -D__EXTENSIONS__=1 -D_ALL_SOURCE=1 -D_GNU_SOURCE=1 -D_POSIX_PTHREAD_SEMANTICS=1 -D_TANDEM_SOURCE=1 -DPACKAGE=\""worker-launcher\"" -DVERSION=\""1.0.0\"" -DHAVE_UNISTD_H=1 -DHAVE__BOOL=1 -DHAVE_STDBOOL_H=1 -DHAVE_DECL_STRERROR_R=1 -DHAVE_STRERROR_R=1 -DHAVE_MKDIR=1 -DHAVE_UNAME=1 -I.    -I./impl -Wall -g -Werror -DEXEC_CONF_DIR=/etc/storm  -MT impl/worker-launcher.o -MD -MP -MF $depbase.Tpo -c -o impl/worker-launcher.o impl/worker-launcher.c &&\
[INFO] 	mv -f $depbase.Tpo $depbase.Po
[INFO] impl/worker-launcher.c:60:15: error: use of undeclared identifier 'PATH_MAX'
[INFO]   char buffer[PATH_MAX];
[INFO]               ^
[INFO] impl/worker-launcher.c:61:20: error: use of undeclared identifier 'PATH_MAX'
[INFO]   snprintf(buffer, PATH_MAX, ""/proc/%u/exe"", getpid());
[INFO]                    ^
[INFO] impl/worker-launcher.c:62:27: error: use of undeclared identifier 'PATH_MAX'
[INFO]   char *filename = malloc(PATH_MAX);
[INFO]                           ^
[INFO] impl/worker-launcher.c:67:44: error: use of undeclared identifier 'PATH_MAX'
[INFO]   ssize_t len = readlink(buffer, filename, PATH_MAX);
[INFO]                                            ^
[INFO] impl/worker-launcher.c:72:21: error: use of undeclared identifier 'PATH_MAX'
[INFO]   } else if (len >= PATH_MAX) {
[INFO]                     ^
[INFO] impl/worker-launcher.c:74:13: error: use of undeclared identifier 'PATH_MAX'
[INFO]             PATH_MAX, filename, PATH_MAX);
[INFO]             ^
[INFO] impl/worker-launcher.c:74:33: error: use of undeclared identifier 'PATH_MAX'
[INFO]             PATH_MAX, filename, PATH_MAX);
[INFO]                                 ^
[INFO] impl/worker-launcher.c:99:9: error: unused variable 'binary_euid' [-Werror,-Wunused-variable]
[INFO]   uid_t binary_euid = filestat.st_uid; // Binary's user owner
[INFO]         ^
[INFO] impl/worker-launcher.c:450:42: error: use of undeclared identifier 'PATH_MAX'
[INFO]     char *(paths[]) = {strndup(local_dir,PATH_MAX), 0};
[INFO]                                          ^
[INFO] impl/worker-launcher.c:597:42: error: use of undeclared identifier 'PATH_MAX'
[INFO]     char *(paths[]) = {strndup(full_path,PATH_MAX), 0};
[INFO]                                          ^
[INFO] impl/worker-launcher.c:725:3: error: implicit declaration of function 'fcloseall' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
[INFO]   fcloseall();
[INFO]   ^
[INFO] impl/worker-launcher.c:725:3: note: did you mean 'fclose'?
[INFO] /usr/include/stdio.h:232:6: note: 'fclose' declared here
[INFO] int      fclose(FILE *);
[INFO]          ^
[INFO] 11 errors generated.
[INFO] make: *** [impl/worker-launcher.o] Error 1
{code}"
STORM-2350,Storm-HDFS's listFilesByModificationTime is broken,"Storm-HDFS module tries to locate the oldest locks. To sort based on the modification time the {{ModifTimeComparator}} is used which is broken:
The comparator currently compares the first object ({{o1}}) to itself.

Fix is trivial: compare {{o1}} to {{o2}}."
STORM-2347,JS errors in Topology Visualization,"On a freshly downloaded 1.0.2 after starting ZK, UI, NM, and SP, submitting a topology the Topology Visualization isn't working and 300+ JS errors are appearing on the console.
No changes to config files were made.
Any help would be appreciated.



{code}
...
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at screenUpdate (arbor.js:33)
gather_stream_count @ visualization.js:314
(anonymous) @ visualization.js:268
(anonymous) @ arbor.js:35
each @ jquery-1.11.1.min.js:2
eachEdge @ arbor.js:35
calculate_total_transmitted @ visualization.js:257
redraw @ visualization.js:54
screenUpdate @ arbor.js:33
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at screenUpdate (arbor.js:33)
gather_stream_count @ visualization.js:314
(anonymous) @ visualization.js:268
(anonymous) @ arbor.js:35
each @ jquery-1.11.1.min.js:2
eachEdge @ arbor.js:35
calculate_total_transmitted @ visualization.js:257
redraw @ visualization.js:54
screenUpdate @ arbor.js:33
visualization.js:314 Uncaught TypeError: Cannot read property 'default722480637' of undefined
    at gather_stream_count (visualization.js:314)
    at Object.<anonymous> (visualization.js:268)
    at Edge.<anonymous> (arbor.js:35)
    at Function.each (jquery-1.11.1.min.js:2)
    at Object.eachEdge (arbor.js:35)
    at calculate_total_transmitted (visualization.js:257)
    at Object.redraw (visualization.js:54)
    at Object.success (visualization.js:420)
    at j (jquery-1.11.1.min.js:2)
    at Object.fireWith [as resolveWith] (jquery-1.11.1.min.js:2)
...
{code}"
STORM-2345,Type mismatch in ReadClusterState's ProfileAction processing Map,"Discovered during reading STORM-2018's review comments:
{{ReadClusterState.run()}} method loads all the profiling requests from Zk then filters the ones designated to that particular node it runs on.  

The filtered profiling requests are stored in a Map: {{Map<Integer, Set<TopoProfileAction>>}} on a per Slot basis.
For some reason the TCP Port is serialized as i64/Long in NodeInfo, which is later used as a key in the Map.

The Map.put is converted properly to Integer, but the Map.get does not, causing the lookups to report a miss in the {{filtered}} Map.

This could cause TopoProfileActions ignored if one would send multiple TopoProfileActions through Zk. 
I don't think that could normally happen (using the UI), but the fix is trivial."
STORM-2343,New Kafka spout can stop emitting tuples if more than maxUncommittedOffsets tuples fail at once,"It doesn't look like the spout is respecting maxUncommittedOffsets in all cases. If the underlying consumer returns more records in a call to poll() than maxUncommittedOffsets, they will all be added to waitingToEmit. Since poll may return up to 500 records by default (Kafka 0.10.1.1), this is pretty likely to happen with low maxUncommittedOffsets.

The spout only checks for tuples to retry if it decides to poll, and it only decides to poll if numUncommittedOffsets < maxUncommittedOffsets. Since maxUncommittedOffsets isn't being respected when retrieving or emitting records, numUncommittedOffsets can be much larger than maxUncommittedOffsets. If more than maxUncommittedOffsets messages fail, this can cause the spout to stop polling entirely."
STORM-2342,storm-kafka-client consumer group getting stuck consuming from kafka 0.10.1.1,"I've created a topology that will read from kafka using storm-kafka-client but when it reaches the last message on kafka log it stops consuming and get stuck, new messages are never consumed, here are the kafka logs:
{quote}
[2017-02-03 19:15:26,865] INFO [GroupCoordinator 1002]: Preparing to restabilize group kafka-spout with old generation 29 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:15:26,865] INFO [GroupCoordinator 1002]: Stabilized group kafka-spout generation 30 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:15:26,868] INFO [GroupCoordinator 1002]: Assignment received from leader for group kafka-spout for generation 30 (kafka.coordinator.GroupCoordinator)
{quote}
========= here storm starts consuming messages, then, when it hits the last message, I can see this log in kafka == >
{quote}
[2017-02-03 19:16:01,266] INFO [GroupCoordinator 1002]: Preparing to restabilize group kafka-spout with old generation 30 (kafka.coordinator.GroupCoordinator)
[2017-02-03 19:16:01,266] INFO [GroupCoordinator 1002]: Group kafka-spout with generation 31 is now empty (kafka.coordinator.GroupCoordinator)
{quote}
=====
and then storm consumer group is stuck, no new messages are read from kafka. my topology/ spout are configured that way:

*Topology:*

      c.put(SConfig.TOPOLOGY_MAX_SPOUT_PENDING, 1000)
      c.put(SConfig.NIMBUS_SEEDS, ""my nimbus seeds"")
      c.put(SConfig.NIMBUS_THRIFT_PORT, 6627)
      c.put(SConfig.TOPOLOGY_WORKERS, 2)      c.put(SConfig.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS, 100)

*Spout:*

    props.put(KafkaSpoutConfig.Consumer.ENABLE_AUTO_COMMIT, true)
    props.put(KafkaSpoutConfig.Consumer.BOOTSTRAP_SERVERS, ...)
    props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""kafka-spout"")
    props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, keyDeserializer)
    props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, valueDeserializer)
 
Also the offsets are not seeming to be committed, despite I've set *enable.auto.commit* and *auto.commit.interval.ms* properties, because if I kill the topology and send it again, the same messages are being reprocessed...

 any hints?"
STORM-2341,worker-launcher is not included in binary distribution,Even though the documentation refers to [worker-launcher|http://storm.apache.org/releases/1.0.2/SECURITY.html] and the Travis builds with -Pnative the {{worker-launcher}} binary is not included in apache-storm-xyz.tar.gz files.
STORM-2339,Python code format cleanup in storm.py,"{{bin/storm.py}} has multiple stylistic shortcomings:
 - PEP8 standard is not followed
 - the python interpreter is hard-wired to /usr/bin/python
 - unnecessary global statements are posted before reading globals

These issues shadows error reporting by modern IDEs (such as PyCharm).
"
STORM-2338,Subprocess exception handling is broken in storm.py on Windows environment,"There is typo in the exception handling branch in {{storm.py:: exec_storm_class()}}:
{code}
        try:
            ret = sub.check_output(all_args, stderr=sub.STDOUT)
            print(ret)
        except sub.CalledProcessor as e:
            sys.exit(e.returncode)
{code}

There is no ""CalledProcessor"" type of exception exists in subprocess module.
The correct exception name is CalledProcessError."
STORM-2337,Broken documentation generation for storm-metrics-profiling-internal-actions.md and windows-users-guide.md,"The generated documentation is broken for {{storm-metrics-profiling-internal-actions.md}} and {{windows-users-guide.md}}. 

The format of these documents do not conform with Jekyll's standard and they are silently ignored. This causes 404 errors when someone want's to access those documents through the webpage.

Additionally the exclusion filter in _config.yml has a typo (READ*E*ME.md)"
STORM-2334,Bolt for Joining streams,"Create a general purpose windowed bolt that performs Joins on multiple data streams.

Since, depending on the topo config,  the bolt could be receiving data either on 'default' streams or on named streams .... join bolt should be able to differentiate the incoming data based on names of upstream components as well as stream names.

*Example:*

The following SQL style join involving 4 tables :

{code}
select  userId, key4, key2, key3
from stream1 
join       stream2  on stream2.userId =  stream1.key1
join       stream3  on stream3.key3   =  stream2.userId
left join  stream4  on stream4.key4   =  stream3.key3
{code}

Could be expressed using the Join Bolt over 4 named streams as :

{code}
new JoinBolt(STREAM, ""stream1"", ""key1"") //'STREAM' arg indicates that stream1/2/3/4 are names of streams. 'key1' is the key on which 
     .join     (""stream2"", ""userId"",  ""stream1"") //join stream2 on stream2.userId=stream1.key1
     .join     (""stream3"", ""key3"",    ""stream2"") //join stream3 on stream3.key3=stream2.userId   
     .leftjoin (""stream4"", ""key4"",    ""stream3"") //left join stream4 on stream4.key4=stream3.key3
     .select(""userId, key4, key2, key3"")         // chose output fields
     .withWindowLength(..)
     .withSlidingInterval(..);
{code}

Or based on named source components :

{code}
new JoinBolt(SOURCE, ""kafkaSpout1"", ""key1"") //'SOURCE' arg indicates that kafkaSpout1, hdfsSpout3 etc are names of upstream components 
     .join     (""kafkaSpout2"", ""userId"",    ""kafkaSpout1"" )    
     .join     (""hdfsSpout3"",  ""key3"",      ""kafkaSpout2"")
     .leftjoin (""mqttSpout1"",  ""key4"",      ""hdfsSpout3"")
     .select (""userId, key4, key2, key3"")
     .withWindowLength(..)
     .withSlidingInterval(..);
{code}


In order for the tuples to  be joined correctly, 'fields grouping' should be employed on the incoming streams. Each stream should be grouped on the same key using which it will be joined against other streams.  This is a restriction compared to SQL which allows join a table with others on any key and any number of keys.

*For example:* If a 'Stream1' is Fields Grouped on 'key1', we cannot use a different 'key2' on 'Stream1' to join it with other streams. However, 'Stream1' can be joined using the same key with multiple other streams as show in this SQL.

{code}
select ....
from stream1 
join  stream2  on stream2.userId =  stream1.key1
join  stream3  on stream3.key3   =  stream1.key2  // not supportable in Join Bolt 
{code}

Consequently the join bolt's syntax is a bit simplified compared to SQL. The key name for any given stream only appears once, as soon the stream is introduced for the first time in the join. Thereafter that key is implicitly used for joining. See the case of 'stream3' being joined with both 'stream2' and 'stream4' in the first example.
"
STORM-2329,Topology halts when getting HDFS writer in a secure environment,"Simple topologies writing to Kerberized HDFS will sometimes stop while getting a new writer (storm-hdfs) in a Kerberized environment: 

java.io.IOException: Failed on local exception: java.io.IOException: Couldn't setup connection for principal@realm to nn1/nn1IP; Host Details : local host is: ""hostname/ip""; destination host is: ""nn hostname"":8020; at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772) at org.apache.hadoop.ipc.Client.call(Client.java:1473) at org.apache.hadoop.ipc.Client.call(Client.java:1400) at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) at com.sun.proxy.$Proxy26.create(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296) at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:497) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) at com.sun.proxy.$Proxy27.create(Unknown Source) at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1726) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1668) at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1593) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397) at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393) at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775) at org.apache.storm.hdfs.bolt.AvroGenericRecordBolt.makeNewWriter(AvroGenericRecordBolt.java:115) at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.getOrCreateWriter(AbstractHdfsBolt.java:222) at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.execute(AbstractHdfsBolt.java:154) at backtype.storm.daemon.executor$fn_3697$tuple_action_fn3699.invoke(executor.clj:670) at backtype.storm.daemon.executor$mk_task_receiver$fn3620.invoke(executor.clj:426) at backtype.storm.disruptor$clojure_handler$reify3196.onEvent(disruptor.clj:58) at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) at backtype.storm.daemon.executor$fn3697$fn3710$fn3761.invoke(executor.clj:808) at backtype.storm.util$async_loop$fn_544.invoke(util.clj:475) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745) Caused by: java.io.IOException: Couldn't setup connection for principal@realm to nn1/nn1IP:8020 at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:673) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:644) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:731) at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:369) at org.apache.hadoop.ipc.Client.getConnection(Client.java:1522) at org.apache.hadoop.ipc.Client.call(Client.java:1439) ... 35 more Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:413) at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:554) at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:369) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:723) at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:719) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:718) ... 38 more Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:122) at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187) at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:224) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ... 47 more

Typically seen on low throughput topologies but recently witnessed in a topology that rotates files within minutes.  

From the trace it happens here: https://github.com/apache/storm/blob/master/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java#L151

My suspicion is this happens only when opening a new file otherwise I don't see why there would be a Kerberos context to complain about until a flush/sync perhaps. 

My shoot from the hip reaction is to pull that out of the current try and simply let the bolt fail and restart to establish a security context.  Thoughts? "
STORM-2328,Batching And Vector Operations,"Sub-topic of Storm Worker redesign. 
Design doc:  https://docs.google.com/document/d/13n0omjkc04h6KObC9-h7l4h4OL8Cp9Qnqz0XXtNJtno/edit?usp=sharing"
STORM-2327,Abstract class ConfigurableTopology,"Classes which run topologies often repeat the same code and pattern to:
* populate the configuration from a file instead of ~/.storm
* determine whether to run locally or remotely
* set a TTL for a topology

Flux provides an elegant way of dealing with these but sometimes it is simpler to define a topology in Java code. 

In [StormCrawler|http://stormcrawler.net], we implemented an abstract class named ConfigurableTopology which can be extended and saves users the hassle of having to write code for the things above. I will open a PR containing this class so that we can discuss and comment whether it is of any use at all."
STORM-2326,Upgrade log4j and slf4j,"The dependencies to log4j could be upgraded from 2.1 to 2.7, same for slf4j to 1.7.21.

This would help fix [STORM-1386]

BTW any idea why we need log4j-over-slf4j?
"
STORM-2322,Could not find or load main class blobstore ,I got this error (Could not find or load main class blobstore) on Windows machine while I'm trying to run a command: storm blobstore create --file README.txt --acl o::rwa --replication-factor 4 key1. This error occurs for other commands that I'm trying to use and for different machines.
STORM-2321,Nimbus did not come up after restart,"The nimbus was restarted during HA testing. After the restart the nimbus failed to come up. 
{code}
2017-01-18 04:57:58.231 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2017-01-18 04:57:58.247 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyKillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar
2017-01-18 04:57:58.273 o.a.s.b.KeySequenceNumber [ERROR] Exception {}
org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:200)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.storm.shade.org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:149)
	at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)
	at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58.274 o.a.s.s.o.a.c.f.i.CuratorFrameworkImpl [INFO] backgroundOperationsLoop exiting
2017-01-18 04:57:58.296 o.a.s.m.n.Login [INFO] successfully logged in.
2017-01-18 04:57:58.309 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x359afc1eaa2009b closed
2017-01-18 04:57:58.309 o.a.s.s.o.a.z.ClientCnxn [INFO] EventThread shut down
2017-01-18 04:57:58.310 o.a.s.t.s.TThreadPoolServer [ERROR] Error occurred during processing of message.
java.util.NoSuchElementException
	at java.util.TreeMap.key(TreeMap.java:1327)
	at java.util.TreeMap.lastKey(TreeMap.java:297)
	at java.util.TreeSet.last(TreeSet.java:401)
	at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:206)
	at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3755)
	at org.apache.storm.generated.Nimbus$Processor$createStateInZookeeper.getResult(Nimbus.java:3740)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)
	at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58.311 o.a.s.d.nimbus [ERROR] Error when processing event
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:92)
	at org.apache.storm.daemon.nimbus$fn__9373.invoke(nimbus.clj:1452)
	at clojure.lang.MultiFn.invoke(MultiFn.java:233)
	at org.apache.storm.daemon.nimbus$fn__9770$exec_fn__3656__auto____9771$fn__9786.invoke(nimbus.clj:2452)
	at org.apache.storm.timer$schedule_recurring$this__2188.invoke(timer.clj:105)
	at org.apache.storm.timer$mk_timer$fn__2171$fn__2172.invoke(timer.clj:50)
	at org.apache.storm.timer$mk_timer$fn__2171.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:114)
	at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:76)
	... 8 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:252)
	at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:111)
	... 9 more
Caused by: java.lang.RuntimeException: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:349)
	at org.apache.storm.blobstore.BlobStoreUtils.createStateInZookeeper(BlobStoreUtils.java:217)
	at org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore(BlobStoreUtils.java:249)
	... 10 more
Caused by: org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TSaslTransport.readLength(TSaslTransport.java:376)
	at org.apache.storm.thrift.transport.TSaslTransport.readFrame(TSaslTransport.java:453)
	at org.apache.storm.thrift.transport.TSaslTransport.read(TSaslTransport.java:435)
	at org.apache.storm.thrift.transport.TSaslClientTransport.read(TSaslClientTransport.java:37)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_createStateInZookeeper(Nimbus.java:1000)
	at org.apache.storm.generated.Nimbus$Client.createStateInZookeeper(Nimbus.java:987)
	at org.apache.storm.blobstore.NimbusBlobStore.createStateInZookeeper(NimbusBlobStore.java:346)
	... 12 more
2017-01-18 04:57:58.314 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__8579.invoke(nimbus.clj:212)
	at org.apache.storm.timer$mk_timer$fn__2171$fn__2172.invoke(timer.clj:71)
	at org.apache.storm.timer$mk_timer$fn__2171.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,317 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,318 FATAL Ignoring log event after log4j was shut down
2017-01-18 04:57:58,318 FATAL Ignoring log event after log4j was shut down
{code}"
STORM-2320,DRPC client printer class reusable for local and remote DRPC,"The Trident Kafka examples in the modules storm-kafka-examples and storm-kafka-client-examples use DRPC to print the results of DRPC computation. In local mode the results output is printed as illustrated in the screenshot attached.

For the DRPC results to be printable when running in distributed mode,it is necessary to connect a DRPC client to  retrieve the results. There was no DRPC client running in remote mode prior to this change, hence nothing was getting printed in distributed mode. This fact mislead users into believing that the examples were not working properly in remote mode.

This JIRA addresses the issue by printing the DRPC results in the log files. Thus, it allows users to query the results and investigate their validity. Furthermore, it serves as a good example on how to print results of DRPC computation in local and distributed mode."
STORM-2319,Remove hadoop-auth depeendency and add spnego filter in storm-core,
STORM-2316,Enumeration support for properties configuration,It would be great if a Flux builder will resolve enumeration within properties configuration. This feature is only available for constructor arguments.
STORM-2315,New kafka spout can't commit offset when ack is disabled. ,"When ack is disabled, kafka spout failed to commit offsets."
STORM-2314,Workers is dead or locked when netty connection is timeout,"Storm is running ,but some workers can not emit data when throw the following exception,  the exception offen occurs in heigh pressure

2017-01-22 18:23:25.137 s.k.CollectorZkCoordinator [INFO] Task [3/3] Finished refreshing
2017-01-22 18:24:08.170 o.a.s.m.n.StormClientHandler [INFO] Connection to xxxxxxx/192.168.175.25:6703 failed:
java.io.IOException: Connection timed out
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.7.0_80]
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.7.0_80]
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.7.0_80]
	at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.7.0_80]
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384) ~[?:1.7.0_80]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.0.2.jar:1.0.2]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [?:1.7.0_80]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [?:1.7.0_80]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80]"
STORM-2313,CPU Pinning,"Design Document:
https://docs.google.com/document/d/1zI0ax-8dE9SmLkF8ZoJaLnrM11EL7NV1uO3VymWG3OA/edit?usp=sharing"
STORM-2312,Memory Management,"Refer to this Doc for details

https://docs.google.com/document/d/1a-RLv1KKBn2vVliztLcdfC-5vDStxpNlR0ZLu33lwxg/edit?usp=sharing"
STORM-2311,A browser which is outside the cluster cant's access  storm ui when storm cluster in a security mode,"when storm cluster in a security mode such as kerberos,We can not use a browser which is outside the cluster to access  storm ui,even if we remove the configuration item which named ui.filter.There is a mistake like ""server 500"",because those methods to get cluster's info for ui can't access the cluster which is in  a security mode"
STORM-2310,Handling Back Pressure,"Design Doc:
https://docs.google.com/document/d/1btmpBpFeEl-bh1uhQ_W4Ao-cQK7muNBYbXXMwuc4YaU/edit?usp=sharing"
STORM-2309,Elasticity - for Storm topologies,
STORM-2308,Support for Non-replayable Sources,"In order to recover from failures without data loss, Storm (and other streaming systems) places the responsibility of buffering events on the source system. In the event of a crash or other failure, in-flight events can be re-fetched from the source and their processing can be retried on recovery. A nice benefit of this approach is that it keeps Storm’s architecture simple. 

While it is desirable to avoid the complexities of creating an internal reliable buffering system, it is not necessary to restrict Spouts to accept data only from persistent sources such Kafka, Hdfs or databases. Some amount of data loss is acceptable in many uses cases. Storm already supports such use cases by allowing ACK-ing to be disabled. 

Users who can tolerate data loss, benefit from having spouts that can accept data directly from a wider variety of sources such as HTTP, TCP/UDP, Syslog, Flume etc. For such use cases, by not forcing all data to go through a system like Kafka, end-to-end latency improves in addition to simplifying management and reducing cost of the data pipeline. Users who care about not losing data can always funnel the incoming data via Kafka or another persistent store and enable ACKs.
"
STORM-2307,Revised Threading and Execution Model,"Design Doc:
https://docs.google.com/document/d/1PBGQomJQ67gsLR0CNZlYfVWjGyzAEJMsQjpKumuyHuQ/edit?usp=sharing"
STORM-2306,"Redesign Messaging Subsystem, switch to JCTools Queues and introduce new Backpressure model","Details in these documents:

1) *Redesign of the messaging subsystem*
https://docs.google.com/document/d/1NK1DJ3aAkta-Im0m-2FObQ4cSRp8xSa301y6zoqcBeE/edit?usp=sharing
This doc discusses the new design for the messaging system. Plus some of the optimizations being made.

2) *Choosing a high performance messaging queue:*
https://docs.google.com/document/d/1PpQaWVHg06-OqxTzYxQlzg1yEhzA4Y46_NC7HMO6tsI/edit?usp=sharing
This doc looks into how fast hardware can do inter-thread messaging and why we chose the JCTools queues.

3) *Backpressure Model*
https://docs.google.com/document/d/1Z9pRdI5wtnK-hVwE3Spe6VGCTsz9g8TkgxbTFcbL3jM/edit?usp=sharing
Describes the Backpressure model integrated into the new messaging subsystem."
STORM-2305,STORM-2279 calculates task index different from grouper code,"Arun reported this from https://github.com/apache/storm/pull/1866#discussion_r95326528

Quoting his comment:

{quote}
This should match the task selected by fields grouping https://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/daemon/GrouperFactory.java#L159.
The modulo technique will return a different value than Math.abs. Probably we should move it to some common utility function and use it in both places.

In 1.x branch the fields grouper doesn't seem to account for negative hashCode, so not sure why didn't it come up before. https://github.com/apache/storm/blob/1.x-branch/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L52
{quote}

So we should fix Nimbus code to use Math.abs on master, 1.x, 1.0.x branches, and also fix executor.clj to use Math.abs on 1.x, 1.0.x branches."
STORM-2303,[storm-opentsdb] Fix list invariant issue for JDK 7,"From STORM-2297 I also fixed an issue where storm-opentsdb refers TupleOpenTsdbDatapointMapper (implemented one) to List/Iterable, not ITupleOpenTsdbDatapointMapper (interface). 
I just replaced TupleOpenTsdbDatapointMapper to ITupleOpenTsdbDatapointMapper, and it works with JDK 8, but later I realized it doesn't work with JDK 7 because generic is invariant.

While I don't know why it worked with JDK 8 (I googled about generic covariance change on JDK 8 but no luck.) it should be fixed."
STORM-2302,New Kafka spout doesn't support seek to given offset,"I was looking at code of current KafkaTridentSpoutEmitter & KafkaSpout class. Can we add functionality based on user provided offset to start from particular offset? This would be useful incase user wants to reprocess particular data set. Another example user has changed the group id & aware where old offset committed & he wants to start processing from same position.

Please refer attachment for further discussion happened over mail."
STORM-2301,[storm-cassandra] upgrade cassandra driver to 3.1.2,"Currently, storm-cassandra refers cassandra driver 2.1.7.1 which is not compatible with Cassandra 3.x. 

Fortunately cassandra driver 3.1.2 is compatible with various Cassandra versions (Apache Cassandra 1.2, 2.0, 2.1, 2.2 and 3.0), so upgrade driver should be safe for users who uses prior version of Cassandra 3.0.
http://docs.datastax.com/en/developer/java-driver/3.1/#compatibility"
STORM-2300,[Flux] support list of references,"There're many methods which receive list of objects (also array of objects with varargs) which object is not basic type of yaml.
(STORM-2297 is one of the case, though STORM-2297 is just going to fix storm-opentsdb itself.)

It would be better to support reference list so that it can support method which argument is List<Type>, Type[], Type... (varargs).
(Flux can automatically convert List to Array while assigning.)"
STORM-2299,Stop user from killing topology before X (configured) amount of time,"Currently user can kill topology directly without waiting for some amount of time so that all inflight messages will get processed.  For example, storm is writing to file & user kills topology, file is not closed or moved to proper location. We need to educate operation guys to do the right things also there are some chances that it will be not followed causing system to go in inconsistent state.
 
Can we set mandatory timeout (configurable) when user kills storm topology? User should not be allowed kill topology with time less than mentioned time.

Some case: 
1) If topology is long running don't allow user to kill but time not less than mentioned one
2) If topology is just deployed allow him to kill instantly (as it might be some mistake)
3) Handle same cases from command-line.
"
STORM-2298,Don't kill Nimbus when ClusterMetricsConsumer is failed to initialize,"ClusterMetricsConsumerExecutor doesn't pass errors to Nimbus when sending metrics to ClusterMetricsConsumer, but passes errors to Nimbus when initializing ClusterMetricsConsumer and kills Nimbus.

Nimbus should have fault tolerance on ClusterMetricsConsumer so that it could drop cluster metrics but still be alive.

It might be also ideal to retry initialization if ClusterMetricsConsumerExecutor is about to send metrics but initialization of ClusterMetricsConsumer is failed."
STORM-2297,[storm-opentsdb] Support Flux for OpenTSDBBolt,"Due to some limitations of Flux, we can't use storm-opentsdb with Flux.

- Flux doesn't support static factory method
- Flux doesn't support List of references

While it would be great to support these via Flux, fixing storm-opentsdb to support Flux would be easier and no harm."
STORM-2296,Kafka spout - no duplicates on topic leader changes,"Current behavior of Kafka spout emits duplicate tuples whenever Kafka topic leader's change.
In case of exception caused by leader changes, PartitionManagers are simply recreated losing the state about which tuples were already emitted and new PartitionManager re-emits them again.

This is fine as at-least-once is fulfilled, but still it would be better to not emit duplicate data if possible.
Moreover this could be easily avoided by moving the state related to emitted tuples from old PartitionManager to new one.

Pull requests implementing this: 
1.0.x-branch - https://github.com/apache/storm/pull/1873
1.x-branch - https://github.com/apache/storm/pull/1888

Pull request for related bugfix: https://github.com/apache/storm/pull/1940"
STORM-2295,KafkaSpoutStreamsNamedTopics changing the sequence of fields name while emitting data,"If you look at below code *allFields* variable is HashSet. To which we have added the o/p of *kafkaSpoutStream.getOutputFields().toList()*. That sort data on hash basis rather than keeping same sequence.
{code:java}
	@Override
	public Fields getOutputFields() {
		final Set<String> allFields = new HashSet<>();
		for (KafkaSpoutStream kafkaSpoutStream : topicToStream.values()) {
			allFields.addAll(kafkaSpoutStream.getOutputFields().toList());
		}
		return new Fields(new ArrayList<>(allFields));
	}
{code}

Changes needed is below
{code:java}
final Set<String> allFields = new LinkedHashSet<>();
{code}"
STORM-2294,Send activate and deactivate command from ShellSpout,"When deactivate and activate are called on ShellSpout those calls should be send to the corresponding ShellProcess as multilang commands. 

*For Example:*
When a ShellSpout polls some data from any source those resouces can be gracefully allocated or deallocated on (de)activation. Otherwise there is no possibility to react on those events via multilang support."
STORM-2293,hostname should only refer node's 'storm.local.hostname',"This is reported bug from user mailing list.
https://lists.apache.org/thread.html/206dfc6aefda13f27bb8d41a86da98355767d522882841296e09070b@%3Cuser.storm.apache.org%3E

{code}
Hi Guys,

I'm running a topology on 3 supervisor. I've registred a custom
MetricsConsumer class that output metrics on Elasticsearch.

My bug is that the taskInfo.srcWorkerHost that always the same values as
the first supervisor (nimbus as well), Even when I put a parallelismHint to
3...

I'm using storm 1.0.2

Thanks for your feedback
{code}

The reason is that `hostname` refers topology configuration when finding storm.local.hostname. It should always refer node's configuration."
STORM-2292,"Kafka spout enhancement, for our of range edge cases","@hmcl and all, we have communicated via email for a while and going forward let's talk in this thread so everyone is in same page.
Base on the spout from the community(written by you), we have several fixes and it worked quite stable in our production for about 6 months.

We want to share the latest spout to you and could you please kindly help review and merge to the community version if any fix is reasonable? we want to avoid diverging too much from the community version.

Below are our major fixes:

For failed message, in next tuple method, originally the spout seek back to the non-continuous offset, so the failed message will be polled again for retry, say we seek back to message 10 for retry, now if kafka log file was purged, earliest offset is 1000, it means we will seek to 10 but reset to 1000 as per the reset policy, and we cannot poll the message 10, so spout not work.
Our fix is: we manually catch the out of range exception, commit the offset to earliest offset first, then seek to the earliest offset

Currently the way to find next committed offset is very complex, under some edge cases – a), if no message acked back because bolt has some issue or cannot catch up with the spout emit; b) seek back is happened frequently and it is much faster than the message be acked back
We give each message a status – None, emit, acked, failed(if failed number is bigger than the maximum retry, set to acked)

One of our use cases need ordering in partition level, so after seek back for retry, we re-emit all the follow messages again no matter they have emitted or not, if possible, maybe you can give an option here to configure it – either re-emit all the message from the failed one, or just emit the failed one, same as current version.

We record the message count for acked, failed, emitted, just for statistics.

Could you please kindly help review and let us know if you can merge it into the community version? Any comments/concern pls feel free to let us know. Btw, our code is attached in this Jira."
STORM-2291,A Hash Collision Problem of Fields Grouping in Windowing Method,"I‘d like to discuss the hash collision issue that occurs when applying the _Grouping_ method [http://storm.apache.org/releases/current/Concepts.html] to _Windowing_ method [http://storm.apache.org/releases/current/Windowing.html] in Storm.

I first assume the following situation. Spout constantly emits tuples to Bolt. At this time, Bolt tries to perform operations while moving tuples of a certain interval. e.g. moving average, etc. To solve this situation, Storm provides _Windowing_ method.

However, consider the following complex situation. Two problems are added in the above situation.
# As a first problem, the tuple emitted by Spout is multidimensional with multiple pieces of information. For example, Alice, Bob, and Clark are mapped to random real numbers. That is, the tuples emitted from Spout are {[Alice, 0.18322], [Clark, 0.57833], [Bob, 0.27902], [Clark, 0.24553], [Alice, 0.50164], [Alice, 0.06463], ...}. While those tuples are transmitted to the next windowed bolt, they must be necessarily separated by keys such as Alice, Bob, and Clark. In other words, each tuples in which Alice, Bob, and Clark are mapped must belong to different windows.
# The second problem is Storm's parallelism. Spout and Bolt can be operated as multiple objects on multiple servers. This problem is that the tuples with the same key must be emit in the same window even if they are created by a different Spout object.

Storm can specify the Bolt objects which the tuples is to be input as a _Grouping_ method. Storm provides various _Grouping_ methods, but a fields grouping is best suited as a way to solve the above problems. The fields grouping is a way of partitioning an input stream by a specified field. With the fields grouping, tuples of the same field can only be passed to the same Bolt object. However, the fields grouping has been implemented as a hash method. ([http://storm.apache.org/releases/current/Tutorial.html]) Therefore, it can be cause *a hash collision problem* that can include the tuples in the same window although they have different fields. So I am interested in solving the hash collision.

The source code of [https://github.com/dke-knu/i2am/tree/master/i2am-app/fields-window-grouping/src/main/java/org/fields/window/grouping/as_is] is a situation where the hash collision occurs. First, Spout randomly emits the tuples mapping Alice, Bob, and Clark on random real numbers. Next, Bolt which extends BaseWindowedBolt prints the TupleWindow objects received from Spout. At this time, Bolt uses the fields grouping. Spout emits three fields: Alice, Bob, and Clark. If the parallelism of Bolt is set less than 3, it surely cause the hash collision. Conversely, the greater the parallelism of Bolt than 3, the lower the probability of the hash collision. But, it can not be guaranteed that the hash collision does not occur.

As an alternative to this hash collision, I used two-step IRichBolt instead of BaseWindowedBolt. The source code for this is [https://github.com/dke-knu/i2am/tree/master/i2am-app/fields-window-grouping/src/main/java/org/fields/window/grouping/to_be]. The first Bolt is important. This Bolt takes tuples from Spout and manages them through a hash map of list according to the field. If the list is as filled as a predefined window size, the oldest tuple is removed and a new tuple is added. And then, Bolt emits this list to the next Bolt.

Using this method, the above problems can be solved. That is, the tuples of the same fields is always managed in the same window regardless of the number of fields and the number of parallelism. 

I'm concerned that this problem will frequently happen to many Storm users who use the Windowing method. If there is not a better way than the one I presented, I think there should be a new grouping method for Windowing method."
STORM-2290,Upgrading zookeeper to 3.4.9 for stability,"We should upgrade zookeeper to 3.4.9 as it brings in a lot of stability improvements (http://zookeeper.apache.org/releases.html) and storm is still using 3.4.6 (https://github.com/apache/storm/blob/master/pom.xml)

One serious issue affecting zookeeper 3.4.6 is https://issues.apache.org/jira/browse/ZOOKEEPER-1506 which prohibits zookeeper from getting a quorum and hence affects storm's stability as well."
STORM-2289,Intermittent failure on DRPCtest (high chance on Travis CI),"DRPCtest is failing with high chance on Travis CI. 
Here're the builds for master from recent pull requests:

- https://travis-ci.org/apache/storm/jobs/189442311 (failed)
- https://travis-ci.org/apache/storm/jobs/189324650 (succeed)
- https://travis-ci.org/apache/storm/jobs/188740322 (failed)
- https://travis-ci.org/apache/storm/jobs/191431175 (failed)

More than 50% are failing."
STORM-2288,Nimbus client can timeout in log running tests,
STORM-2287,DemoTest fails intermittently on 1.x-branch & master branch,"See the following runs:
https://travis-ci.org/apache/storm/builds/191507849"
STORM-2286,Storm Rebalance command should support arbitrary component parallelism,"For legacy reasons, config TOPOLOGY-TASKS is considered first when schedule a topology, for a component, if user don’t specify TOPOLOGY-TASKS, storm just override it to be equal to component parallelism hint, and schedule based on TOPOLOGY-TASKS later on.

This works for the most cases, but not Rebalance command. Now, when do Rebalance, the StormBase :component->executors attribute will be overridden in Zookeeper which is used to partition component tasks into executors, as we said above, the TOPOLOGY-TASKS is considered here as the real tasks number for components, something goes weird here:

If we override a bigger executor numbers for a component when do rebalance, it just don’t work because smaller TOPOLOGY-TASKS [ not changed since first submitted at all ]is partitioned into bigger number of executors which read from ZooKeeper overridden by Rebalance command, but for smaller task, it works fine.

I see that storm support a command like this now: [storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*] which indicate that user can override a component parallelism freely, i think it’s more sensible to support this and it's meaningless to have a restriction like before."
STORM-2284,Storm Worker Redesign,"Much has been learnt from evolving the 1.x line. We can now use the benefit of hindsight and apply these learnings into the future work on 2.x line. 

The goal is to rethink the Worker to improve performance, enhance its abilities and also retain compatibility.


*Overview Document*:
Also covers results from experiments that motivate this work.
https://docs.google.com/document/d/1EzeHL3d7EE-RyyBEpN7CwRmWz3oqjbbKiVVAlzFp2Nc/edit?usp=sharing    "
STORM-2283,Fix DefaultStateHandler kryo multithreading issues,
STORM-2281,Running Multiple Kafka Spouts (Trident) Throws Illegal State Exception,"For Kafka Spout New Consumer in Trident, if we increase the spout parallelism more than one then we can see that the below error happens

It is reproducible most of the times, it it does not then just kill and restart topology.  (if spout parallelism is 1 there is no problem, it only happens with multiple spouts)

Steps to Reproduce:
1. Create a Spout Only Trident Topology (or read write topology)
2. Create a topic with multiple partition (2 or more) 
3. Pump some data and try to read with parallelism of 2 or more


No current assignment for partition input-1 
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2] 
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2] 
at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.2.jar:1.0.2] 
at org.apache.storm.daemon.executor$fn__8058$fn__8071$fn__8124.invoke(executor.clj:850) ~[storm-core-1.0.2.jar:1.0.2] 
at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2] 
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?] 
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77] 
Caused by: java.lang.IllegalStateException: No current assignment for partition input-1 
at org.apache.kafka.clients.consumer.internals.SubscriptionState.assignedState(SubscriptionState.java:231) ~[kafka-clients-0.10.0.0.jar:?] 
at org.apache.kafka.clients.consumer.internals.SubscriptionState.seek(SubscriptionState.java:256) ~[kafka-clients-0.10.0.0.jar:?] 
at org.apache.kafka.clients.consumer.KafkaConsumer.seek(KafkaConsumer.java:1134) ~[kafka-clients-0.10.0.0.jar:?] 
at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.seek(KafkaTridentSpoutEmitter.java:139) ~[storm-kafka-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT] 
at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.emitPartitionBatch(KafkaTridentSpoutEmitter.java:88) ~[storm-kafka-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT] 
at org.apache.storm.kafka.spout.trident.KafkaTridentSpoutEmitter.emitPartitionBatch(KafkaTridentSpoutEmitter.java:47) ~[storm-kafka-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT] 
at org.apache.storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.emitBatch(OpaquePartitionedTridentSpoutExecutor.java:128) ~[storm-core-1.0.2.jar:1.0.2] 
at org.apache.storm.trident.spout.TridentSpoutExecutor.execute(TridentSpoutExecutor.java:82) ~[storm-core-1.0.2.jar:1.0.2]"
STORM-2279,Unable to open bolt page of storm ui,"With latest storm code, I am unable to open ui and see bolt information. I am using the vagrant setup. On the ui page that open, I see the following error.
{code}
Internal Server Error
org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1369)
	at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1353)
	at org.apache.storm.ui.core$component_page.invoke(core.clj:1026)
	at org.apache.storm.ui.core$fn__4308.invoke(core.clj:1214)
	at org.apache.storm.shade.compojure.core$make_route$fn__789.invoke(core.clj:100)
	at org.apache.storm.shade.compojure.core$if_route$fn__777.invoke(core.clj:46)
	at org.apache.storm.shade.compojure.core$if_method$fn__770.invoke(core.clj:31)
	at org.apache.storm.shade.compojure.core$routing$fn__795.invoke(core.clj:113)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__799.invoke(core.clj:118)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__3573.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__3102.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__2152.invoke(helpers.clj:54)
	at org.apache.storm.ui.core$catch_errors$fn__4474.invoke(core.clj:1460)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__1844.invoke(keyword_params.clj:35)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__1887.invoke(nested_params.clj:84)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__1816.invoke(params.clj:64)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2139.invoke(flash.clj:35)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2125.invoke(session.clj:98)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__1674.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__1678.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{code}
Url: http://node1:8080/component.html?id=SlidingTimeCorrectness-winSec1slideSec1VerificationBolt&topology_id=SlidingWindowTestw1s1-2-1483646178

There is a stacktrace corresponding to this in nimbus.log showing IndexOutOfBound error:
{code}
2017-01-05 19:57:26.934 pool-15-thread-41 o.a.s.d.n.Nimbus [WARN] getComponentPageInfo exception. (topo id='SlidingWindowTestw1s1-2-1483646178')
java.lang.ArrayIndexOutOfBoundsException: -2
        at java.util.ArrayList.elementData(ArrayList.java:418)
        at java.util.ArrayList.get(ArrayList.java:431)
        at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:3606)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4097)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4081)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{code}

The problem is that we expect the index to be positive, but since it is a mod of hashcode it can be negative.
{code}
                int taskIndex = TupleUtils.listHashCode(Arrays.asList(componentId)) %
                        tasks.size();
                int taskId = tasks.get(taskIndex);
{code}
https://github.com/apache/storm/blob/2b82fc8b5328fd4fbd680998c6051d9496c102d7/storm-core/src/jvm/org/apache/storm/daemon/nimbus/Nimbus.java#L3605
"
STORM-2276,Remove twitter4j usages due to license issue (JSON.org is catalog X),"The ASF recently made the determination that the json.org license is category x. Storm doesn't depend on it directly, but Storm depends on twitter4j for storm-starter, and twitter4j depends on json.org.

This is a blocker for any releases. Please refer mail thread for detail: https://www.mail-archive.com/dev@storm.apache.org/msg40060.html"
STORM-2275,Nimbus crashed during state transition of topology,"I am copying last few lines of the nimbus logs including stack trace.
{code}
2017-01-04 22:18:10.106 pool-15-thread-47 o.a.s.d.n.Nimbus [INFO] Activating DemoTest: DemoTest-21-1483568289
2017-01-04 22:18:11.646 timer o.a.s.s.EvenScheduler [INFO] Available slots: [f0ea57ab-86d6-401f-9429-52f479b1d69f:6704, f0ea57ab-86d6-401f-9429-52f479b1d69f:6705, f0ea57ab-86d6-401f-9429-52f479b1d69f:670\
6, f0ea57ab-86d6-401f-9429-52f479b1d69f:6707, f0ea57ab-86d6-401f-9429-52f479b1d69f:6708, f0ea57ab-86d6-401f-9429-52f479b1d69f:6709, f0ea57ab-86d6-401f-9429-52f479b1d69f:6700, f0ea57ab-86d6-401f-9429-52f4\
79b1d69f:6701, f0ea57ab-86d6-401f-9429-52f479b1d69f:6702, f0ea57ab-86d6-401f-9429-52f479b1d69f:6703]
2017-01-04 22:18:11.648 timer o.a.s.d.n.Nimbus [INFO] Setting new assignment for topology id DemoTest-21-1483568289: Assignment(master_code_dir:storm-local, node_host:{f0ea57ab-86d6-401f-9429-52f479b1d69\
f=node1}, executor_node_port:{[10, 10]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [14, 14]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [16, 16]=NodeInfo(node:\
f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [12, 12]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [8, 8]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [6,\
 6]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [20, 20]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [4, 4]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f,\
 port:[6700]), [2, 2]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [18, 18]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [11, 11]=NodeInfo(node:f0ea57ab-86d6-401\
f-9429-52f479b1d69f, port:[6701]), [15, 15]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [7, 7]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [9, 9]=NodeInfo(node\
:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [21, 21]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [5, 5]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6701]), [3\
, 3]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702]), [19, 19]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [17, 17]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d6\
9f, port:[6701]), [1, 1]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700]), [13, 13]=NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700])}, executor_start_time_secs:{[12, 12]=1\
483568291, [6, 6]=1483568291, [18, 18]=1483568291, [2, 2]=1483568291, [8, 8]=1483568291, [14, 14]=1483568291, [16, 16]=1483568291, [20, 20]=1483568291, [4, 4]=1483568291, [10, 10]=1483568291, [9, 9]=1483\
568291, [3, 3]=1483568291, [15, 15]=1483568291, [21, 21]=1483568291, [5, 5]=1483568291, [11, 11]=1483568291, [13, 13]=1483568291, [17, 17]=1483568291, [19, 19]=1483568291, [1, 1]=1483568291, [7, 7]=14835\
68291}, worker_resources:{NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6702])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0), NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f,\
 port:[6701])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0), NodeInfo(node:f0ea57ab-86d6-401f-9429-52f479b1d69f, port:[6700])=WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)})
2017-01-04 22:18:11.660 timer o.a.s.d.n.Nimbus [INFO] Cleaning up DemoTest-20-1483567429
2017-01-04 22:18:11.668 timer o.a.s.d.n.Nimbus [INFO] Removing dependency jars from blobs - []
2017-01-04 22:18:12.420 pool-15-thread-51 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormjar.jar
2017-01-04 22:18:12.990 pool-15-thread-38 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormcode.ser
2017-01-04 22:18:12.995 pool-15-thread-59 o.a.s.d.n.Nimbus [INFO] Created download session for DemoTest-21-1483568289-stormconf.ser
2017-01-04 22:18:20.303 timer o.a.s.d.n.Nimbus [INFO] TRANSITION: DemoTest-20-1483567429 REMOVE null false
2017-01-04 22:18:20.304 timer o.a.s.d.n.Nimbus [ERROR] Error while processing event
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1174)
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83)
Caused by: java.lang.NullPointerException
        at org.apache.storm.daemon.nimbus.Nimbus.transition(Nimbus.java:1215)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1172)
        ... 1 more
2017-01-04 22:18:20.304 timer o.a.s.u.Utils [ERROR] Halting process: Error while processing event
java.lang.RuntimeException: Halting process: Error while processing event
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792)
        at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$15(Nimbus.java:1107)
        at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:104)
2017-01-04 22:18:20.315 Thread-9 o.a.s.d.n.Nimbus [INFO] Shutting down master
{code}

The problem is that we are assuming that the base will be non-null which is incorrect leading to NPE."
STORM-2274,Support named output streams in Hdfs Spout,Currently it emits only to default output stream
STORM-2273,Starting nimbus from arbitrary dir fails,"Here is the output that I got:
{code}
storm@node1:/home/vagrant$ storm nimbus
Running: java -server -Ddaemon.name=nimbus -Dstorm.options= -Dstorm.home=/usr/share/apache-storm-2.0.0-SNAPSHOT -Dstorm.log.dir=/usr/share/apache-storm-2.0.0-SNAPSHOT/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib:/usr/lib64 -Dstorm.conf.file= -cp /usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-api-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-slf4j-impl-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/clojure-1.7.0.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/slf4j-api-1.7.7.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/asm-5.0.3.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/minlog-1.3.0.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/reflectasm-1.10.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-over-slf4j-1.6.6.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/log4j-core-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/kryo-3.0.3.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/storm-rename-hack-2.0.0-SNAPSHOT.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/disruptor-3.3.2.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/storm-core-2.0.0-SNAPSHOT.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/objenesis-2.1.jar:/usr/share/apache-storm-2.0.0-SNAPSHOT/lib/servlet-api-2.5.jar:/usr/share/storm/conf -Xmx1024m -Dlogfile.name=nimbus.log -DLog4jContextSelector=org.apache.logging.log4j.core.async.AsyncLoggerContextSelector -Dlog4j.configurationFile=/usr/share/apache-storm-2.0.0-SNAPSHOT/log4j2/cluster.xml org.apache.storm.daemon.nimbus.Nimbus
{code}
Log added to nimbus.log.
{code}
2017-01-04 21:44:33.089 main o.a.s.n.NimbusInfo [INFO] Nimbus figures out its name to node1
{code}
"
STORM-2272,LocalCluster can leak simulated time,If the constructor for LocalCluster throws an exception while configured for simulated time it can leak the simulated time and leave it on.
STORM-2270,Kafka spout should consume from latest when zk committed offset bigger than latest offset,"Kafka spout should consume from latest when ZK offset bigger than latest offset[ an TopicOffsetOutOfRangeException thrown out ], especially when Kafka topic change it's leader and some data lost, if we consume from earliest offset, much meaningless duplicate records will be re-consumed. So, we should consume from latest offset instead.
"
STORM-2269,Dynamic reconfiguration for the nodes in Nimbus/Pacemaker clusters,"Reference: https://zookeeper.apache.org/doc/trunk/zookeeperReconfig.html

It would be nice to have a similar functionality for Nimbus/Pacemaker clusters too.
As that would eliminate the need for restarting servers in the Nimbus/Pacemaker clusters whenever a node exits or joins these clusters.


----------------------------------------------------------
Reply from Bobby Evans on the dev group:
----------------------------------------------------------

There is nothing for that right now on pacemaker.
You can do it with nimbus so long as at least one of the original nodes is still up.

But in either case it would not be too difficult to make it all fully functional.
The two critical pieces would be in giving the workers and daemons a way to reload these specific configs dynamically.
Then it would be documenting the order of operations to be sure nothing goes wrong.

*Adding Pacemaker Node(s)*
# bring up the new node(s).
# update nimbus configs to start reading from the new nodes.
# update all of the worker nodes to let workers start writing to the new node.

*Removing Pacemaker Node(s)*
# Shut down pacemaker nodes/update configs on workers (order should not matter so long as there are enough pacemaker nodes up to handle the load)
# update the nimbus configs to not try and read from the old nodes


*Adding new Nimbus Node(s)*
# Bring up the new nimbus with the new config.
# update all of the other nodes (including any machines that clients come from) with new config (order does not matter)

*Removing Nimbus Node(s)*
# Shut down the old nodes and update the configs on all the boxes in any order you want.
# This should just work so long as you have at least one nimbus node still up.
"
STORM-2267,[storm-submit-tools] Use user's local maven repo. directory to local repo.,"We've found that dependency resolver fetches old SNAPSHOT artifacts from remote repository other than new SNAPSHOT artifacts from local. 
While this only affects SNAPSHOT artifacts, using SNAPSHOT artifacts is sometimes inevitable for developer. 
If possible, it would be better to resolve this so that developer can use their own SNAPSHOT artifacts for their works."
STORM-2265,Incorrectly Serialized JSON in TransactionalState causes Worker to Die,"TransactionalState uses JSONValue to serialize / deserialize objects. However, the object GlobalPartitionInformation is incorrectly serialized by default, causing the exception bellow. To get around this problem, GlobalPartitionInformation must implement JSONAware.

2016-12-23 14:37:26.980 o.a.s.e.e.ReportError Thread-21-$spoutcoord-spout-spout1-executor[2, 2] [ERROR] Error
java.lang.RuntimeException: java.lang.RuntimeException: Unexpected character (G) at position 1.
        at org.apache.storm.utils.Utils$6.run(Utils.java:2190) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
Caused by: java.lang.RuntimeException: Unexpected character (G) at position 1.
        at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:174) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$6.run(Utils.java:2179) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 1 more
Caused by: org.apache.storm.shade.org.json.simple.parser.ParseException
        at org.apache.storm.shade.org.json.simple.parser.Yylex.yylex(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.nextToken(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.shade.org.json.simple.JSONValue.parseWithException(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:167) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$6.run(Utils.java:2179) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        ... 1 more
2016-12-23 14:37:26.987 o.a.s.u.Utils Thread-21-$spoutcoord-spout-spout1-executor[2, 2] [ERROR] Halting process: Worker died
java.lang.RuntimeException: Halting process: Worker died
        at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.utils.Utils$4.run(Utils.java:1800) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException(ReportErrorAndDie.java:45) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1956) [?:1.8.0_112]
2016-12-23 14:37:26.987 o.a.s.d.w.Worker Thread-38 [INFO] Shutting down worker tkst-consumer-4-1482532570 556a1e7b-49f7-4dc2-a936-d17e5e4ba9de 6700
2016-12-23 14:37:26.988 o.a.s.d.w.Worker Thread-38 [INFO] Terminating messaging context
"
STORM-2264,OpaqueTridentKafkaSpout failing after STORM-2216,"I've seen OpaqueTridentKafkaSpout failing after STORM-2216.

{code}
java.lang.RuntimeException: Unexpected character (G) at position 1.
	at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:172) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.sync(RotatingTransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.<init>(RotatingTransactionalState.java:46) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.prepare(TridentSpoutCoordinator.java:57) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.topology.BasicBoltExecutor.prepare(BasicBoltExecutor.java:43) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.bolt.BoltExecutor.init(BoltExecutor.java:84) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:93) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.executor.bolt.BoltExecutor.call(BoltExecutor.java:45) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.utils.Utils$6.run(Utils.java:2179) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]
Caused by: org.apache.storm.shade.org.json.simple.parser.ParseException
	at org.apache.storm.shade.org.json.simple.parser.Yylex.yylex(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.nextToken(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.parser.JSONParser.parse(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.json.simple.JSONValue.parseWithException(Unknown Source) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.trident.topology.state.TransactionalState.getData(TransactionalState.java:165) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	... 9 more
{code}

There's no bug on STORM-2216, but there has been limitation on TransactionalState which doesn't have type information and uses simple-json to serde, in result it can't serde GlobalPartitionInformation properly.

Previously we just ignored parse error but STORM-2216 exposes the issue.

We could make TransactionalState ignoring parse error again, but anyway it has been not intended behavior so better to be fixed properly."
STORM-2253,Storm PMML Bolt - Unit Tests,"Currently the patch has integration tests through the form of a test topology that loads a PMML Model and raw input data from a CSV file. The {@link RawInputFromCSVSpout}
creates a stream of tuples with raw inputs, and the {@link PMMLPredictorBolt} computes the predicted scores.

The main focus of the initial patch was to design the classes in such a way that they can accommodate arbitrary runtime environments. The default implementation provided uses one such runtime execution library, which is more suited to be tested using integration tests.

Will add some unit tests around to assert for edge and some common cases"
STORM-2250,Kafka Spout Refactoring to Increase Modularity and Testability,"Per the discussion here https://github.com/apache/storm/pull/1826 the KafkaSpout class should be split up a bit, and the unit tests should be improved to use time simulation and not break encapsulation on the spout to test."
STORM-2249,Make Distribution Scripts Put Examples to the Correct Locations,Make binary.xml put all the examples that exist in source packages with name pattern COMPONENT-NAME-examples-x.y.x.jar to STORM_HOME/examples/storm-COMPONENT-NAME-examples/COMPONENT-NAME-examples-x.y.x.jar
STORM-2248,Storm UI in Apache storm 1.0.2 does not update Executors and Tasks after Rebalance .Is anyone else facing this issue ,
STORM-2245,integration-test constant compilation failure,"The travis-ci constant build failure due to the following error:

{code}
[ERROR] COMPILATION ERROR : 
[ERROR] /home/travis/build/apache/storm/integration-test/src/main/java/org/apache/storm/ExclamationTopology.java:[80,11] error: cannot find symbol
{code}
"
STORM-2242,Trident state persisting does not honor batch.size.rows configuration,"Persisting the Trident state in {{org.apache.storm.cassandra.trident.state.CassandraState}} with batching enabled does not honor the configuration for {{cassandra.batch.size.rows}}.

This results in a warning at least:
{code}
10:33:33.720 [SharedPool-Worker-16] WARN  o.a.c.cql3.statements.BatchStatement - Batch of prepared statements for [gin.ngram_count] is of size 5200, exceeding specified threshold of 5120 by 80.
{code}

An exception like this is also possible:
{code}
10:30:54.287 [SharedPool-Worker-1] ERROR o.a.c.cql3.statements.BatchStatement - Batch of prepared statements for [gin.df] is of size 103428, exceeding specified threshold of 51200 by 52228. (see batch_size_fail_threshold_in_kb)
10:30:54.295 [Thread-29-b-1-executor[7 7]] WARN  o.a.s.c.trident.state.CassandraState - Batch write operation is failed.
10:30:54.297 [Thread-29-b-1-executor[7 7]] ERROR org.apache.storm.daemon.executor -
com.datastax.driver.core.exceptions.InvalidQueryException: Batch too large
    at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:50) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245) ~[cassandra-driver-core-3.1.0.jar:na]
    at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:64) ~[cassandra-driver-core-3.1.0.jar:na]
    at org.apache.storm.cassandra.trident.state.CassandraState.updateState(CassandraState.java:159) ~[storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.cassandra.trident.state.CassandraStateUpdater.updateState(CassandraStateUpdater.java:34) [storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.cassandra.trident.state.CassandraStateUpdater.updateState(CassandraStateUpdater.java:30) [storm-cassandra-1.0.2.IQSER_20161212.jar:1.0.2.IQSER_20161212]
    at org.apache.storm.trident.planner.processor.PartitionPersistProcessor.finishBatch(PartitionPersistProcessor.java:98) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.trident.planner.SubtopologyBolt.finishBatch(SubtopologyBolt.java:151) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.trident.topology.TridentBoltExecutor.finishBatch(TridentBoltExecutor.java:266) [storm-core-1.0.2.jar:1.0.2]
{code}

This effectivly disables the usage of batching."
STORM-2241,KafkaSpout implementaion,"Storm ISpout documentaion say 'Storm executes ack, fail, and nextTuple all on the same thread. This means that an implementor of an ISpout does not need to worry about concurrency issues between those methods. However, it also means that an implementor must ensure that nextTuple is non-blocking: otherwise the method could block acks and fails that are pending to be processed.'

Where as KafkaSpout has below nextTuple() implementation
@Override
    public void nextTuple() {
        List<PartitionManager> managers = _coordinator.getMyManagedPartitions();
        for (int i = 0; i < managers.size(); i++) {

            try {
                // in case the number of managers decreased
                _currPartitionIndex = _currPartitionIndex % managers.size();
                EmitState state = managers.get(_currPartitionIndex).next(_collector);
                if (state != EmitState.EMITTED_MORE_LEFT) {
                    _currPartitionIndex = (_currPartitionIndex + 1) % managers.size();
                }
                if (state != EmitState.NO_EMITTED) {
                    break;
                }
            } catch (FailedFetchException e) {
                LOG.warn(""Fetch failed"", e);
                _coordinator.refresh();
            }
        }

        long now = System.currentTimeMillis();
        if ((now - _lastUpdateMs) > _spoutConfig.stateUpdateIntervalMs) {
            commit();
        }
    }

We are seeing events are getting replayed when there is slower bolt in the topology chain causing duplicate messages.

Is there any way this can be fixed.

"
STORM-2240,STORM PMML Bolt - Add Support to Load Models from Blob Store,This PR follows from a [suggestion/request|https://github.com/apache/storm/pull/1816#discussion_r91586225] made by [~ptgoetz] during the PR review.
STORM-2239,New Kafka spout does not properly handle interrupts,"The KafkaConsumer underlying the new Kafka spout had a bug that meant that it could enter an infinite loop if certain methods were called on it from an interrupted thread. This can cause local mode clusters to hang on shutdown. It is fixed in the next Kafka release (https://issues.apache.org/jira/browse/KAFKA-4387), but a side effect is that some blocking calls on the consumer can now throw an unchecked InterruptedException variant. This will cause the executor to crash. The spout should instead catch the exception if it occurs and set thread interrupted state before returning control to Storm."
STORM-2236,storm kafka client should support manual partition management.,"Currently storm kafka client relies on kafka to assign partition to each spout. This may cause unnecessary rebalance in cases where storm itself, e.g. worker restart, slow processing of tuples."
STORM-2235,Introduce new option: 'add remote repositories' for dependency resolver,"Sometimes we need to pull the artifacts from other than maven central. For now dependency resolver in storm-submit-tool doesn't support adding remote repositories, so it would be better to add the feature."
STORM-2234,heartBeatExecutorService in shellSpout don't work well with deactivate ,"When using the activate and deactivate of a shellSpout (using the client):
1 .First we deactivate -which calls :  {quote}
heartBeatExecutorService.shutdownNow();{quote}
2. Then we actiavate which calls: {quote}         heartBeatExecutorService.scheduleAtFixedRate(new SpoutHeartbeatTimerTask(this), 1, 1, TimeUnit.SECONDS);
{quote}
3.This results in an {quote} RejectedExecutionException {quote} as we *already* shutdown the heartBeatExecutorService.
4.Simple test to prove this:
 {quote}{noformat}    
private class TestTimerTask extends TimerTask {


        @Override
        public void run() {
            System.out.println(""Im running now"");
        }
    }

    @Test(expectedExceptions = RejectedExecutionException.class)
    public void heartBeatShutdownTest() {
        ScheduledExecutorService heartBeatExecutorService = MoreExecutors.getExitingScheduledExecutorService(new ScheduledThreadPoolExecutor(1));
        heartBeatExecutorService.scheduleAtFixedRate(new TestTimerTask(), 1, 1, TimeUnit.SECONDS);
        heartBeatExecutorService.shutdownNow();
        heartBeatExecutorService.scheduleAtFixedRate(new TestTimerTask(), 1, 1, TimeUnit.SECONDS);
    }{noformat}{quote}

5.Already created a fix and opening a PR with it: https://github.com/apache/storm/pull/1813"
STORM-2233,BlobStore Cleanup/Optimization,"KeySequenceNumber creates a new connection to zookeeper each time.  This really does not need to happen.  At a minimum we should cache the connection and/or reuse it.

Also everywhere that we check for LocalBlobStore so we can store some blob state should be refactored so it can be inside by LocalBlobStore itself. "
STORM-2231,NULL in DisruptorQueue while multi-threaded ack,"I use simple topology with one spout (9 workers) and one bolt (9 workers).
I have topology.backpressure.enable: false in storm.yaml.
Spouts send about 10 000 000 tuples in 10 minutes. Pending for spout is 80 000.
Bolts buffer theirs tuples for 60 seconds and flush to database and ack tuples in parallel (10 threads).
I read that OutputCollector can be used in many threads safely, so i use it.
I don't have any bottleneck in bolts(flushing to database) or spouts(kafka spout), but about 2% of tuples fail due to tuple processing timeout (fails are recordered in spout stats only).
I am sure that bolts ack all tuples. But some of acks don't come to spouts.

While multi-threaded acking i see many errors in worker logs like that:
2016-12-01 13:21:10.741 o.a.s.u.DisruptorQueue [ERROR] NULL found in disruptor-executor[3 3]-send-queue:853877

I tried to use synchronized wrapper around OutputCollector to fix the error. But it didn't help.

I found the workaround that helps me: i do all processing in bolt in multiple threads but call OutputCollector.ack methods in a one single separate thread.

I think Storm has an error in the multi-threaded use of OutputCollector.

If my topology has much less load, like 500 000 tuples per 10 minutes, then  i don't lose any acks."
STORM-2230,Unable to send same topic in different streams using KafkaSpoutStreamsNamedTopics,
STORM-2229,KafkaSpout does not resend failed tuples,"When the topology fails a tuple, it is never resent by the KafkaSpout. This can easily be shown by constructing a small topology failing every tuple.

Apparent reason:

{code}
public class KafkaSpout<K, V> extends BaseRichSpout {
//...
private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                kafkaConsumer.seekToEnd(toArrayList(rtp));    // Seek to last committed offset <== Does seek to end of partition
            }
        }
    }
{code}

The code seeks to the end of the partition instead of seeking to the first uncommited offset.

Preliminary fix (worked for me, but needs to be checked by an expert)

{code}
    private void doSeekRetriableTopicPartitions() {
        final Set<TopicPartition> retriableTopicPartitions = retryService.retriableTopicPartitions();

        for (TopicPartition rtp : retriableTopicPartitions) {
            final OffsetAndMetadata offsetAndMeta = acked.get(rtp).findNextCommitOffset();
            if (offsetAndMeta != null) {
                kafkaConsumer.seek(rtp, offsetAndMeta.offset() + 1);  // seek to the next offset that is ready to commit in next commit cycle
            } else {
                OffsetAndMetadata committed = kafkaConsumer.committed(rtp);
                if(committed == null) {
                    // No offsets commited yet for this partition - start from beginning 
                    kafkaConsumer.seekToBeginning(toArrayList(rtp));
                } else {
                   // Seek to first uncommitted offset
                    kafkaConsumer.seek(rtp, committed.offset() + 1);
                }
            }
        }
    }
{code}
"
STORM-2228,KafkaSpout does not replay properly when a topic maps to multiple streams,"In the example.

KafkaSpoutTopologyMainNamedTopics.java

The code creates a TuplesBuilder and a KafkaSpoutStreams

{code}
protected KafkaSpoutTuplesBuilder<String, String> getTuplesBuilder() {
    return new KafkaSpoutTuplesBuilderNamedTopics.Builder<>(
            new TopicsTest0Test1TupleBuilder<String, String>(TOPICS[0], TOPICS[1]),
            new TopicTest2TupleBuilder<String, String>(TOPICS[2]))
            .build();
}

protected KafkaSpoutStreams getKafkaSpoutStreams() {
    final Fields outputFields = new Fields(""topic"", ""partition"", ""offset"", ""key"", ""value"");
    final Fields outputFields1 = new Fields(""topic"", ""partition"", ""offset"");
    return new KafkaSpoutStreamsNamedTopics.Builder(outputFields, STREAMS[0], new String[]{TOPICS[0], TOPICS[1]})  // contents of topics test, test1, sent to test_stream
            .addStream(outputFields, STREAMS[0], new String[]{TOPICS[2]})  // contents of topic test2 sent to test_stream
            .addStream(outputFields1, STREAMS[2], new String[]{TOPICS[2]})  // contents of topic test2 sent to test2_stream
            .build();
}
{code}

Essentially the code is trying to take {{TOPICS\[0]}}, {{TOPICS\[1]}}, and {{TOPICS\[2]}} translate them to {{Fields(""topic"", ""partition"", ""offset"", ""key"", ""value"")}} and output them on {{STREAMS\[0]}}. Then just for {{TOPICS\[2]}} they want it to be output as {{Fields(""topic"", ""partition"", ""offset"")}} to {{STREAMS\[2]}}.  (Don't know what happened to {{STREAMS\[1]}})

There are two issues here.  First with how the TupleBuilder and the SpoutStreams are split up, but coupled {{STREAMS\[2]}} is actually getting the full ""topic"" ""partition"" ""offset"" ""key"" ""value"", but this minor.  The real issue is that the code uses the same KafkaSpoutMessageId for all the tuples emitted to both {{STREAMS\[1]}} and {{STREAMS\[2]}}.

https://git.corp.yahoo.com/storm/storm/blob/5bcbb8d6d700d0d238d23f8f6d3976667aaedab9/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java#L284-L304

The code, however, is written to assume that it will only ever get one ack/fail for a given KafkaSpoutMessageId.  This means that if one of the emitted tuple trees succeed and then the other fails, the failure will not result in anything being replayed!  This violates how storm is intended to work.

I discovered this as a part of STORM-2225, and I am fine with fixing it on STORM-2225 (I would just remove support for that functionality because there are other ways of doing this correctly).  But that would not maintain backwards compatibility and I am not sure it would be appropriate for 1.x releases.  I really would like to have feedback from others on this.

I can put something into 1.x where it will throw an exception if acking is enabled and this situation is present, but I don't want to spend the time tying to do reference counting on the number of tuples actually emitted.  If someone else wants to do that I would be happy to turn this JIRA over to them."
STORM-2227,Better User Control of GC Options,"As a user, I would like to override certain JVM garbage connection options instead of overriding them all, so that I can make simpler, safer changes.

Currently, if a user wants to add some gc option in a topology, the user must copy everything from {{worker.gc.childopts}} to {{topology.worker.gc.childopts}} and make needed edits/additions.  This is error prone, since the provided cluster-wide options can change, and because they are overwritten by default.

A user can easily override settings unwittingly by adding new options if they forget to also copy the cluster-wide settings."
STORM-2226,New kafka spout offset lag tool does not work for secured kafka setup,
STORM-2225,Kafka New API make simple things simple,"The Kafka spouts in storm-kafka-client use the new API and are very extendable, but doing very simple things take way too many lines of code.

For example to create a KafkaTridentSpoutOpaque you need the following code (from the example).

{code}
    private KafkaTridentSpoutOpaque<String, String> newKafkaTridentSpoutOpaque() {
        return new KafkaTridentSpoutOpaque<>(new KafkaTridentSpoutManager<>(
                        newKafkaSpoutConfig(
                        newKafkaSpoutStreams())));
    }

    private KafkaSpoutConfig<String,String> newKafkaSpoutConfig(KafkaSpoutStreams kafkaSpoutStreams) {
        return new KafkaSpoutConfig.Builder<>(newKafkaConsumerProps(),
                    kafkaSpoutStreams, newTuplesBuilder(), newRetryService())
                .setOffsetCommitPeriodMs(10_000)
                .setFirstPollOffsetStrategy(EARLIEST)
                .setMaxUncommittedOffsets(250)
                .build();
    }

    protected Map<String,Object> newKafkaConsumerProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(KafkaSpoutConfig.Consumer.BOOTSTRAP_SERVERS, ""127.0.0.1:9092"");
        props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""kafkaSpoutTestGroup"");
        props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, ""org.apache.kafka.common.serialization.StringDeserializer"");
        props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, ""org.apache.kafka.common.serialization.StringDeserializer"");
        props.put(""max.partition.fetch.bytes"", 200);
        return props;
    }

    protected KafkaSpoutTuplesBuilder<String, String> newTuplesBuilder() {
        return new KafkaSpoutTuplesBuilderNamedTopics.Builder<>(
                new TopicsTupleBuilder<String, String>(TOPIC_1, TOPIC_2))
                .build();
    }

    protected KafkaSpoutRetryService newRetryService() {
        return new KafkaSpoutRetryExponentialBackoff(new KafkaSpoutRetryExponentialBackoff.TimeInterval(500L, TimeUnit.MICROSECONDS),
                KafkaSpoutRetryExponentialBackoff.TimeInterval.milliSeconds(2),
                Integer.MAX_VALUE, KafkaSpoutRetryExponentialBackoff.TimeInterval.seconds(10));
    }

    protected KafkaSpoutStreams newKafkaSpoutStreams() {
        return new KafkaSpoutStreamsNamedTopics.Builder(new Fields(""str""), new String[]{""test-trident"",""test-trident-1""}).build();
    }

    protected static class TopicsTupleBuilder<K, V> extends KafkaSpoutTupleBuilder<K,V> {
        public TopicsTupleBuilder(String... topics) {
            super(topics);
        }
        @Override
        public List<Object> buildTuple(ConsumerRecord<K, V> consumerRecord) {
            return new Values(consumerRecord.value());
        }
    }
{code}

All of this so I can have a trident spout that reads <String, String> values from ""localhost:9092"" on the topics ""test-trident"" and ""test-trident-1"" and outputting the value as the field ""str"".

I shouldn't need 50 lines of code for something I can explain in 3 lines of test.  It feels like we need to have some better defaults, and less overhead on a lot of these things."
STORM-2224,Expose a method to override in computing the field from given tuple in FieldSelector,org.apache.storm.cassandra.query.selector.FieldSelector should give a way to customize computing field value from tuple.
STORM-2223,Storm PMML Bolt,This JIRA is to build a Storm PMML bolt which uses JPMML library to load PMML doc and evaluate the incoming tuples based on the user provided PMML doc.
STORM-2222,Repeated NPEs thrown in nimbus if rebalance fails,"If the nimbus daemon crashed during a rebalance (rebalance didn't finish yet) and the daemon is restarted, it will always throw NPEs afterwards due to the wait time secs being gone after the restart. "
STORM-2221,Update DRPC Example,"Provide an example of how to do DRPC properly, and not use the deprecated LinearDRPCTopologyBuilder."
STORM-2220,Adding config keys for CassandraBolts instead of taking at topology level configuration.,Currently Cassandra bolts takes cassandra cluster configuration fro storm topology configuration. This is restrictive once it has two different cassandra bolts talking to different cassandra endpoints. Give a way to pass cassandra conf to any cassandra bolt. 
STORM-2211,KafkaSpout error after recreating kafka topic,"I have a storm topology with a KakfaSpout which was processing messages successfully. I shut down the topology, deleted the kafka topic and recreated it. After restarting the topology, it appears storm saved the partition in zookeeper and was unable to identify that the partition no longer existed. I saw the following error message in the storm logs:

o.a.s.k.KafkaUtils [WARN] Partition{host=<removed>, topic=<removed>, partition=0} Got fetch request with offset out of range: [10650]"
STORM-2210,ShuffleGrouping does not produce even distribution,"When testing the ShuffleGrouping in a multithreaded environment, it produces an extremely uneven distribution.

This appears to be a result of the Collection.shuffle call here. https://github.com/apache/storm/blob/1.0.x-branch/storm-core/src/jvm/org/apache/storm/grouping/ShuffleGrouping.java#L58

Because current was set to zero before the shuffle, other threads are able to access the arrayList while it is being shuffled.

Stephen's gist here includes a test that results in a very uneven distribution of taskIds from the ShuffleGrouping: https://gist.github.com/Crim/61537958df65a5e13b3844b2d5e28cde

I would have expected the taskIds from the ShuffleGrouping to be almost uniformly distributed."
STORM-2208,HDFS State Throws FileNotFoundException in Azure Data Lake Store file system (adl://),This is caused by the fact that AFS does not keep a handle to the file when the file is deleted.
STORM-2207,Kafka Spout NullPointerException during ack,"This occurs on startup of the topology.  There should be some null check safeguards, but i'm not sure what's causing it to occur in the first place...my guess is  the topic partition is not found in the ack map.

2016-11-17 23:11:05.366 o.a.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:420) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:69) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:628) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]
Caused by: java.lang.NullPointerException
    at org.apache.storm.kafka.spout.KafkaSpout.ack(KafkaSpout.java:316) ~[stormjar.jar:?]
    at org.apache.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:448) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$tuple_action_fn__7996.invoke(executor.clj:536) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$mk_task_receiver$fn__7979.invoke(executor.clj:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$clojure_handler$reify__7492.onEvent(disruptor.clj:40) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:451) ~[storm-core-1.0.2.jar:1.0.2]
    ... 7 more
2016-11-17 23:11:05.379 o.a.s.d.executor [ERROR] 
java.lang.RuntimeException: java.lang.NullPointerException
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:420) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$consume_batch.invoke(disruptor.clj:69) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:628) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]
Caused by: java.lang.NullPointerException
    at org.apache.storm.kafka.spout.KafkaSpout.ack(KafkaSpout.java:316) ~[stormjar.jar:?]
    at org.apache.storm.daemon.executor$ack_spout_msg.invoke(executor.clj:448) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$fn__7990$tuple_action_fn__7996.invoke(executor.clj:536) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$mk_task_receiver$fn__7979.invoke(executor.clj:464) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.disruptor$clojure_handler$reify__7492.onEvent(disruptor.clj:40) ~[storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:451) ~[storm-core-1.0.2.jar:1.0.2]
    ... 7 more
2016-11-17 23:11:05.473 o.a.s.util [ERROR] Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
    at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.worker$fn__8663$fn__8664.invoke(worker.clj:765) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.daemon.executor$mk_executor_data$fn__7875$fn__7876.invoke(executor.clj:274) [storm-core-1.0.2.jar:1.0.2]
    at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:494) [storm-core-1.0.2.jar:1.0.2]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_91]

The method and line number in question below:

@Override
    public void ack(Object messageId) {
        final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;
        if (!consumerAutoCommitMode) {  // Only need to keep track of acked tuples if commits are not done automatically
            acked.get(msgId.getTopicPartition()).add(msgId);
        }
        emitted.remove(msgId);
}
"
STORM-2205,Racecondition in getting nimbus summaries while ZK connections are reconnected.,
STORM-2204,Add caching to HBaseLookupBolt,"Add capability to cache Results from HBase lookup in HBaseLookupBolt. 

- Caching is disabled by default.
- Enabled by configuration
   hbase.cache.enable = true
   hbase.cache.ttl.seconds = 300 
   hbase.cache.size = 1000 

Using Guava LoadingCache implementation to create an LRU cache.

Also marking OutputCollector as transient in AbstractHbaseBolt (it should be marked as transient)

https://github.com/apache/storm/blob/cd5c9e8f904205a6ca6eee9222ca954ca8b37ec3/external/storm-hbase/src/main/java/org/apache/storm/hbase/bolt/HBaseLookupBolt.java

https://github.com/apache/storm/pull/1783"
STORM-2200,[Storm SQL] Drop Aggregate & Join support on Trident mode,"This was already discussed on dev@ mailing list. [1] 

As aggregation and join don't make sense for streaming without windowing, we're not ready to handle them on SQL semantic. Unless we're ready, it would be better to drop them to not making any confusions. (Aggregation and join on Trident semantic could be different from what users expect.) 

[1] http://mail-archives.apache.org/mod_mbox/storm-dev/201610.mbox/%3CCAF5108hNLfSJq+pUvD_RRMtGKmrGV7yQSwqn3j8dA9SaN6HcMg@mail.gmail.com%3E"
STORM-2198,perform RotationAction when stopping HdfsBolt,"I have a _HdfsBolt_ with _TimedRotationPolicy_ and _MoveFileAction_. I found the bolt didn't move files when I stopped the HdfsBolt, then _RotationPolicy_ was not triggered, 
Look at the code, the _rotateOutputFile_ method just be called when _RotationPolicy_ is triggered or _writer.needsRotation_. I will add some logic in _cleanup_ method."
STORM-2197,NimbusClient connectins leak due to leakage in ThriftClient.,"Nimbus client connections are not closed when there are errors while connecting to nimbus. Created TSocket in ThriftClient should have been closed in case of errors.

2016-11-03 08:09:37.766 b.s.s.a.k.KerberosSaslTransportPlugin [ERROR] Client failed to open SaslClientTransport to interact with a server during session initiation: org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed
org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed
	at org.apache.thrift7.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:277) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) ~[storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.7.0_60]
	at javax.security.auth.Subject.doAs(Subject.java:415) [?:1.7.0_60]
	at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:103) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:72) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:106) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:82) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.ui.core$nimbus_summary.invoke(core.clj:584) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at backtype.storm.ui.core$fn__10334.invoke(core.clj:1009) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$make_route$fn__7476.invoke(core.clj:93) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$if_route$fn__7464.invoke(core.clj:39) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$if_method$fn__7457.invoke(core.clj:24) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at compojure.core$routing$fn__7482.invoke(core.clj:106) [storm-core-2.4.2-debug-patch.jar:0.10.0-SNAPSHOT]
	at clojure.core$some.invoke(core.clj:2515) [clojure-1.6.0.jar:?]"
STORM-2194,ReportErrorAndDie doesn't always die,"I've been trying to track down a cause of some of our issues with some exceptions leaving Storm workers in a zombified state for some time. I believe I've isolated the bug to the behaviour in :report-error-and-die/reportErrorAndDie in the executor. Essentially:

{code}
     :report-error-and-die (fn [error]
                             (try
                               ((:report-error <>) error)
                               (catch Exception e
                                 (log-message ""Error while reporting error to cluster, proceeding with shutdown"")))
                             (if (or
                                    (exception-cause? InterruptedException error)
                                    (exception-cause? java.io.InterruptedIOException error))
                               (log-message ""Got interrupted excpetion shutting thread down..."")
                               ((:suicide-fn <>))))
{code}

has the grouping for the if statement slightly wrong. It shouldn't log OR die from InterruptedException/InterruptedIOException, but it should log under that condition, and ALWAYS die. 

Basically:

{code}
     :report-error-and-die (fn [error]
                             (try
                               ((:report-error <>) error)
                               (catch Exception e
                                 (log-message ""Error while reporting error to cluster, proceeding with shutdown"")))
                             (if (or
                                    (exception-cause? InterruptedException error)
                                    (exception-cause? java.io.InterruptedIOException error))
                               (log-message ""Got interrupted excpetion shutting thread down...""))
                             ((:suicide-fn <>)))
{code}

After digging into the Java port of this code, it looks like a different bug was introduced while porting:

{code}
        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
            LOG.info(""Got interrupted exception shutting thread down..."");
            suicideFn.run();
        }
{code}

Was how this was initially ported, and STORM-2142 changed this to:

{code}
        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
            LOG.info(""Got interrupted exception shutting thread down..."");
        } else {
            suicideFn.run();
        }
{code}

However, I believe the correct port is as described above:

{code}
        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)
                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {
            LOG.info(""Got interrupted exception shutting thread down..."");
        }
        suicideFn.run();
{code}

I'll look into providing patches for the 1.x and 2.x branches shortly."
STORM-2193,Storm UI/Logviewer passing in params in wrong order to FilterConfiguration,"FilterConfiguration has a few constructors and the order of the params on one of them seems off, but both the ui and logviewer are passing the params in the wrong order to it."
STORM-2192,Add a new IAutoCredentials plugin to support SSL files,"It would be nice to have a new IAutoCredentials plugin to support shipping SSL files with a topology.  

One particular usecase for this is storm reading from a SSL secured kafka cluster.  

We can make this easy for the user by having a topology.auto-credentials that automatically does this when a config is specified.  It also allows the user to upload new credentials easily if they are going to expire."
STORM-2191,shorten classpaths in worker and LogWriter commands,"When launching the worker daemon and its wrapping LogWriter daemon, the commands can become so long that they eclipse the default Linux limit of 4096 bytes. That results in commands that are cut off in {{ps}} output, and prevents easily inspecting the system to see even what processes are running.

The specific scenario in which this problem can be easily triggered: *running Storm on Mesos*.

h5. Details on why it happens:
# using the default Mesos containerizer instead of Docker containers, which causes the storm-mesos package to be unpacked into the Mesos executor sandbox.
# The [""expand all jars on classpath""|https://github.com/apache/storm/blob/6dc6407a01d032483edebb1c1b4d8b69a304d81c/bin/storm.py#L114-L140] functionality in the {{bin/storm.py}} script causes every one of the jars that storm bundles into its lib directory to be explicitly listed in the command.
#* e.g., say the mesos work dir is {{/var/run/mesos/work_dir/}}
#* and say that the original classpath argument in the supervisor cmd includes the following for the {{lib/}} dir in the binary storm package:
#** {{/var/run/mesos/work_dir/slaves/2357b762-6653-4052-ab9e-f1354d78991b-S12/frameworks/20160509-084241-1086985738-5050-32231-0000/executors/STORM_TOPOLOGY_ID/runs/e6a1407e-73fd-4be4-8d00-e882117b3391/storm-mesos-0.1.7-storm0.9.6-mesos0.28.2/lib/*}}
#* That leads to a hugely expanded classpath argument for the LogWriter and Worker daemons that get launched:
#** {{/var/run/mesos/work_dir/slaves/2357b762-6653-4052-ab9e-f1354d78991b-S12/frameworks/20160509-084241-1086985738-5050-32231-0000/executors/STORM_TOPOLOGY_ID/runs/e6a1407e-73fd-4be4-8d00-e882117b3391/storm-mesos-0.1.7-storm0.9.6-mesos0.28.2/lib/asm-4.0.jar:/var/run/mesos/work_dir/slaves/2357b762-6653-4052-ab9e-f1354d78991b-S12/frameworks/20160509-084241-1086985738-5050-32231-0000/executors/STORM_TOPOLOGY_ID/runs/e6a1407e-73fd-4be4-8d00-e882117b3391/storm-mesos-0.1.7-storm0.9.6-mesos0.28.2/lib/carbonite-1.4.0.jar:...}}"
STORM-2190,Topology submission blocked behind scheduling,"The submit-lock in nimbus seems to protect some very large and slow sections of code.  As more and more topologies are submitted scheduling can take longer and longer to complete making submitting a topology take increasingly longer.  But most of scheduling does not need to be protected by this lock.  Only a small section of the scheduler pulls state from zookeeper that the lock protects elsewhere.

We should split this lock up and protect scheduling separate from protecting StormBase stored in zk."
STORM-2183,BaseStatefulBoltExecutor does not handle cyclic graphs,"BaseStatefulBoltExecutor::getCheckpointInputTaskCount() returns the number of sources that a state transaction must wait for to process a transaction.  In a graph where there is a loop (e.g. A->B->C->D->C) components 'C' and 'D' the required number of tuples can will never be received.  The function shouldProcessTransaction will never receive the correct number of tuples, because the set required to come back form 'D' to 'C' will never be forwarded from 'C' to  'D' to begin with.  

Bolt 'C' and 'D' never finish the state initialization step and as such will never pass tuples to their wrapped bolt."
STORM-2182,Refactor Storm Kafka Examples Into Own Modules,Refactor storm-kafka-client and storm-kafka examples similarly to what was done in STORM-1970
STORM-2179,Storm.py doesn't detect JAVA_HOME properly on Windows,"line 92 in storm.py, should be inclusing a check for OS and using java.exe (Windows) vs java (Linux).

JAVA_CMD = 'java' if not JAVA_HOME else os.path.join(JAVA_HOME, 'bin', 'java.exe')"
STORM-2178,version not being returned on Windows,"""storm.cmd version"" returns blank string."
STORM-2176,Workers do not shutdown cleanly and worker hooks don't run when a topology is killed,"This appears to have been introduced in the 1.0.0 release. The issues does not seem to affect 0.10.2.

When a topology is killed and workers receive the notification to shutdown, they do not shutdown cleanly, so worker hooks never get invoked.

When a worker shuts down cleanly, the worker logs should contain entries such as the following:

{code}
2016-10-28 18:52:06.273 b.s.d.worker [INFO] Shut down transfer thread
2016-10-28 18:52:06.279 b.s.d.worker [INFO] Shutting down default resources
2016-10-28 18:52:06.287 b.s.d.worker [INFO] Shut down default resources
2016-10-28 18:52:06.351 b.s.d.worker [INFO] Disconnecting from storm cluster state context
2016-10-28 18:52:06.359 b.s.d.worker [INFO] Shut down worker exclaim-1-1477680593 61bddd66-0fda-4556-b742-4b63f0df6fc1 6700
{code}

In the 1.0.x line of releases (and presumably 1.x, though I haven't checked) this does not happen -- the worker shutdown process appears to get stuck shutting down executors (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/daemon/worker.clj#L666), no further log messages are seen in the worker log, and worker hooks do not run.

There are two properties that affect how workers exit. The first is the configuration property {{supervisor.worker.shutdown.sleep.secs}}, which defaults to 1 second. This corresponds to how long the supervisor will wait for a worker to exit gracefully before forcibly killing it with {{kill -9}}. When this happens the supervisor will log that the worker terminated with exit code 137 (128 + 9).

The second property is a hard-coded 1 second delay (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/util.clj#L463) added as a shutdown hook that will call {{Runtime.halt()}} if the delay is exceeded. When this happens, the supervisor will log that the worker terminated with exit code 20 (hard-coded).

Side Note: The hardcoded halt delay in worker.clj and the default value for {{supervisor.worker.shutdown.sleep.secs}} both being 1 second should probably be changed since it creates a race to see whether the supervisor delay or the worker delay wins.


To test this, I set {{supervisor.worker.shutdown.sleep.secs}} to 15 to allow plenty of time for the worker to exit gracefully, and deployed and killed a topology. In this case the supervisor consistently reported exit code 20 for the worker, indicating the hard-coded shutdown hook caused the worker to exit.

I thought the hard-coded 1 second shutdown hook delay might not be long enough for the worker to shutdown cleanly. To test that hypothesis, I changed the hard-code delay to 10 seconds, leaving {{supervisor.worker.shutdown.sleep.secs}} at 15 seconds. Again supervisor reported an exit code of 20 for the worker, and there were no log messages indicating the worker had exited cleanly and that the worker hook had run.
"
STORM-2175,Supervisor V2 can possibly shut down workers twice in local mode,"See https://github.com/apache/storm/pull/1697#issuecomment-256456889

{code}
java.lang.NullPointerException
    at org.apache.storm.utils.DisruptorQueue$FlusherPool.stop(DisruptorQueue.java:110)
    at org.apache.storm.utils.DisruptorQueue$Flusher.close(DisruptorQueue.java:293)
    at org.apache.storm.utils.DisruptorQueue.haltWithInterrupt(DisruptorQueue.java:410)
    at org.apache.storm.disruptor$halt_with_interrupt_BANG_.invoke(disruptor.clj:77)
    at org.apache.storm.daemon.executor$mk_executor$reify__4923.shutdown(executor.clj:412)
    at sun.reflect.GeneratedMethodAccessor303.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify__5552$shutdown_STAR___5572.invoke(worker.clj:668)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify$reify__5598.shutdown(worker.clj:706)
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:66)
    at org.apache.storm.ProcessSimulator.killAllProcesses(ProcessSimulator.java:79)
    at org.apache.storm.testing$kill_local_storm_cluster.invoke(testing.clj:207)
    at org.apache.storm.testing4j$_withLocalCluster.invoke(testing4j.clj:93)
    at org.apache.storm.Testing.withLocalCluster(Unknown Source)
{code}

and

{code}
java.lang.IllegalStateException: Timer is not active
    at org.apache.storm.timer$check_active_BANG_.invoke(timer.clj:87)
    at org.apache.storm.timer$cancel_timer.invoke(timer.clj:120)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify__5552$shutdown_STAR___5572.invoke(worker.clj:682)
    at org.apache.storm.daemon.worker$fn__5550$exec_fn__1372__auto__$reify$reify__5598.shutdown(worker.clj:706)
    at org.apache.storm.ProcessSimulator.killProcess(ProcessSimulator.java:66)
    at org.apache.storm.ProcessSimulator.killAllProcesses(ProcessSimulator.java:79)
    at org.apache.storm.testing$kill_local_storm_cluster.invoke(testing.clj:207)
    at org.apache.storm.testing4j$_withLocalCluster.invoke(testing4j.clj:93)
    at org.apache.storm.Testing.withLocalCluster(Unknown Source)
{code}

[~Srdo] is still working on getting a reproducible use case for us. But I will try to reproduce/fix it myself in the mean time."
STORM-2171,blob recovery on a single host results in deadlock,"It might be more versions but I have only tested this on 2.x.

Essentially when trying to find replicas to copy blobs from LocalFSBlobStore does not exclude itself.  This results in a deadlock where it is holding a lock trying to download the blob, and at the same time has done a request back to itself trying to download the blob, but it will never finish because it is blocked on the same lock."
STORM-2158,OutOfMemoryError in Nimbus' SimpleTransportPlugin,"{{OutOfMemoryError}} is thrown by Nimbus' {{SimpleTransportPlugin}} if malformed Thrift request is posted:
{code}
echo ""Hello"" | nc localhost 6627
{code}

In nimbus.log:
{noformat}
2016-10-20 12:54:09.978 b.s.d.nimbus [INFO] Starting Nimbus server...
2016-10-20 12:54:42.926 o.a.t.s.THsHaServer [ERROR] run() exiting due to uncaught error
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57) ~[?:1.8.0_92-internal]
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335) ~[?:1.8.0_92-internal]
	at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:207) ~[storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
	at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:158) [storm-core-0.10.3-SNAPSHOT.jar:0.10.3-SNAPSHOT]
2016-10-20 12:54:42.942 b.s.d.nimbus [INFO] Shutting down master
2016-10-20 12:54:43.003 b.s.d.nimbus [INFO] Shut down master
{noformat}

The problem is caused by the lack of specification of the {{maxReadBufferBytes}} of {{THsHaServer}}'s arguments."
STORM-2157,Search text in the Storm UI should be remained unchanged,"Each time press the 'Search' button in the Storm UI, the text in the search box is escaped.

For example, if I type 'Prepared bolt' and press the button, it becomes 'Prepared+bolt'. And if press the button again, now it becomes 'Prepared%2Bbolt'. So only the first search succeeds, and the second search fails.

For continuous successes of the search, the search text should be remained unchanged.
"
STORM-2152,Upgrade Curator Framework to Latest Version (3.2.0),
STORM-2150,ShellBolt raise subprocess heartbeat timeout Exception,"I've got a simple topology running with Storm 1.0.1. The topology consists of a KafkaSpout and several python multilang ShellBolt. I frequently got the following exceptions. 

{code}
java.lang.RuntimeException: subprocess heartbeat timeout at org.apache.storm.task.ShellBolt$BoltHeartbeatTimerTask.run(ShellBolt.java:322) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)
{code}

More information here:
1. Topology run with ACK mode.
2. Topology had 40 workers.
3. Topology emitted about 10 milliom tuples every 10 minutes. 


Every time subprocess heartbeat timeout, workers would restart and python processes exited with exitCode:-1, which affected processing capacity and stability of the topology. 

I've checked some related issues from Storm Jira. I first found STORM-1946 reported a bug related to this problem and said bug had been fixed in Storm 1.0.2. However I got the same exception even after I upgraded Storm to 1.0.2.

I checked other related issues. Let's look at history of this problem.
DashengJu first reported this problem with Non-ACK mode in STORM-738. STORM-742 discussed the approach of this problem with ACK mode, and it seemed that bug had been fixed in 0.10.0. I don't know whether this patch is included in storm-1.x branch. In a word, this problem still exists in the latest stable version."
STORM-2144,Fix Storm-sql group-by behavior in standalone mode,
STORM-2142,ReportErrorAndDie runs suicide function only when InterruptedException or InterruptedIOException is thrown,"When EvaluationFilter / EvaluationFunction throws Exception, async loop for the executor is died but others will continue to work.

{code}
2016-10-08 14:12:29.597 o.a.s.u.Utils Thread-23-b-0-LOGICALFILTER_6-LOGICALPROJECT_7-executor[5 5] [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:468) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
...
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_66]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_66]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_66]
        at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_66]
        at org.codehaus.janino.ScriptEvaluator.evaluate(ScriptEvaluator.java:982) ~[dep-janino-2.7.6-dcb5bd18-a5dd-4976-a967-0108dcf46df0.jar.1475903522000:2.7.6]
...
Caused by: java.lang.RuntimeException: Cannot convert null to int
        at org.apache.calcite.runtime.SqlFunctions.cannotConvert(SqlFunctions.java:1023) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]
        at org.apache.calcite.runtime.SqlFunctions.toInt(SqlFunctions.java:1134) ~[dep-calcite-core-1.9.0-e7846de7-7024-4041-89e4-67dd5edf31e8.jar.1475903521000:1.9.0]
        at SC.eval0(Unknown Source) ~[?:?]
{code}

While looking into detail, I found that ReportErrorAndDie implementation seems odd - completely opposite behavior compared to 1.x :report-error-and-die.
When InterruptedException or InterruptedIOException is thrown, it should just leave a log and shouldn't run suicide function. For others it should run suicide function."
STORM-2139,Let ShellBolts and ShellSpouts run with scripts from blobs,"It would be nice to be able to use the scripts and executable files distributed through the blob store rather then through the resources directory in a jar.

This is nice because it allows you to use a tgz that preserves the execute bit on files like scripts. 

The biggest issue here is that ShellProcess switches the current working directory for the process over to the code dir (where the storm jar is extracted).  And there is not simple way for a child process to find its way back to the blobs.  I will add in a new option to not change the CWD."
STORM-2138,java.io.FileNotFoundException: stormconf.ser does not exist,"We are seeing problems in our storm topology whereby all our workers crash.

The errors we see are

2016-10-07 09:49:33.599 o.a.s.d.supervisor [ERROR] Error on initialization of server mk-supervisor
java.io.FileNotFoundException: File '/opt/storm_local/supervisor/stormdist/production_2016_09_13-1-1475831938/stormconf.ser' does not exist
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:292)
        at org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1815)
        at org.apache.storm.config$read_supervisor_storm_conf_given_path.invoke(config.clj:142)
        at org.apache.storm.config$read_supervisor_storm_conf.invoke(config.clj:221)
        at org.apache.storm.daemon.supervisor$add_blob_references.invoke(supervisor.clj:495)
        at org.apache.storm.daemon.supervisor$fn__9307$exec_fn__2466__auto____9308.invoke(supervisor.clj:795)
        at clojure.lang.AFn.applyToHelper(AFn.java:160)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at clojure.core$apply.invoke(core.clj:630)
        at org.apache.storm.daemon.supervisor$fn__9307$mk_supervisor__9352.doInvoke(supervisor.clj:763)
        at clojure.lang.RestFn.invoke(RestFn.java:436)
        at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1200)
        at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1233)
        at clojure.lang.AFn.applyToHelper(AFn.java:152)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at org.apache.storm.daemon.supervisor.main(Unknown Source)
2016-10-07 09:49:33.608 o.a.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
        at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
        at clojure.lang.RestFn.invoke(RestFn.java:423)
        at org.apache.storm.daemon.supervisor$fn__9307$mk_supervisor__9352.doInvoke(supervisor.clj:763)
        at clojure.lang.RestFn.invoke(RestFn.java:436)
        at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1200)
        at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1233)
        at clojure.lang.AFn.applyToHelper(AFn.java:152)
        at clojure.lang.AFn.applyTo(AFn.java:144)
        at org.apache.storm.daemon.supervisor.main(Unknown Source)
2016-10-07 09:49:34.668 o.a.s.d.supervisor [INFO] Removing code for storm id production_2016_09_13-1-1475831938


We have looked at https://github.com/apache/storm/pull/418 and https://issues.apache.org/jira/browse/STORM-130, which both show the first issue as being fixed - however we are still experiencing it in 1.0.2. The changes from the fixing commit (https://github.com/apache/storm/pull/418/commits/ccd28f8a356f468e66865fa9d9901b0a2628ec74) don't seem to be in the current version of the file (https://github.com/apache/storm/blob/v1.0.2/storm-core/src/clj/org/apache/storm/daemon/supervisor.clj).

We get this often when resubmitting a topology, and our only workaround is to stop the topology, delete the whole /opt/storm_local directory (which is our storm.local.dir) and resubmit the topology. Often, the workers seem to be looking for stormconf.ser in the local directory of an old topology that isn't even running at the time."
STORM-2134,improving the current scheduling strategy for RAS,
STORM-2132,NullPointerException in KafkaSpout,"Received a null pointer exception using storm-kafka-client.  Not sure what caused it, was just sitting idle in my IDE.  It did crash my topology.

Here's the consumer information:
topic 	0	1,546	2,802	2,798	4
topic 	1	1,663	2,856	2,856	0
topic 	2	1,671	3,031	3,022	9
topic 	3	1,648	2,760	2,760	0
topic 	4	1,618	2,828	2,824	4
topic 	5	1,537	2,599	2,595	4
topic 	6	1,469	2,522	2,522	0

java.lang.NullPointerException
	at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:249)
	at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:237)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:203)
	at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
[Thread-34-kafka-spout-executor[4 5]] ERROR org.apache.storm.daemon.executor - 
java.lang.NullPointerException
	at org.apache.storm.kafka.spout.KafkaSpout.doSeekRetriableTopicPartitions(KafkaSpout.java:249)
	at org.apache.storm.kafka.spout.KafkaSpout.pollKafkaBroker(KafkaSpout.java:237)
	at org.apache.storm.kafka.spout.KafkaSpout.nextTuple(KafkaSpout.java:203)
	at org.apache.storm.daemon.executor$fn__7990$fn__8005$fn__8036.invoke(executor.clj:648)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
[Thread-34-kafka-spout-executor[4 5]] ERROR org.apache.storm.util - Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.worker$fn__8659$fn__8660.invoke(worker.clj:761)
	at org.apache.storm.daemon.executor$mk_executor_data$fn__7875$fn__7876.invoke(executor.clj:274)
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:494)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)"
STORM-2130,1.0.x does not compile (BaseConfigurationDeclarer),Looks like we missed pulling something into 1.0.x-branch because if I use the version of storm-core/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java from 1.x-branch everything works.
STORM-2128,SimpleSqlTridentConsumer missing license headder,Looks like STORM-2089 added in this file and it needs a header
STORM-2127,Storm-eventhubs should use latest amqp and eventhubs-client versions,"Storm eventhub jar needs to use the latest eventhubs-client version available (1.0.1). The latest version introduces several bug fixes, and resilient sender/receivers.

New event data schemes (how message content is handled) that were added to HDInsight based storm-eventhubs jar (Binary, and String) would be good features to merge into this offering. 

"
STORM-2120,Emit to outputStreamId configured in SpoutConfig for KafkaSpout,"Even though KafkaSpout.declareOutputFields declaresStream using outputStreamId (if present), the message gets emitted to a stream matching the Kafka topic it was read from. Looks like it may have been a merge conflict between the fix for STORM-1210 and STORM-1379."
STORM-2119,bug in log message printing to stdout,"https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/ui/core.clj#L987

Can cause stdout buffers to fill up thus causing threads calling this function to hang"
STORM-2118,A few fixes for storm-sql standalone mode,
STORM-2110,in supervisor v2 filter out empty command line args,"Just found this in Staging as well.  In the old supervisor code we would filter out all empty command line args.  This was missed in the new supervisor, but only when we split a String opts on white space."
STORM-2109,Under supervisor V2 SUPERVISOR_MEMORY_CAPACITY_MB and SUPERVISOR_CPU_CAPACITY must be Doubles,"Just found this rolling out Supervisor V2 to staging env, but it is a simple fix."
STORM-2108,Spout unable to recover after worker crashes...continuously see discarding messages errors...,"Hello:

We have a new situation that occurred after we upgraded to storm 1.0.2 (from 0.9.2). We had a worker crash due to a bug in our code that caused a stack overflow exception. The supervisor detected the issue and restarted the worker as expected.

After the worker crashed, the many of the tuples the spout sends out continuously time out and our throughput slows to a crawl. The spout seems to send out tuples until it hits the max spout pending. Then some of the tuples time out and it sends the next batch.

We saw this exception in the spout log when the worker crashed:

2016-09-21T01:54:32,749 [refresh-connections-timer] [org.apache.storm.messaging.netty.Client] [INFO]> closing Netty Client Netty-Client-/10.103.16.14:31437
2016-09-21T01:54:32,750 [refresh-connections-timer] [org.apache.storm.messaging.netty.Client] [INFO]> waiting up to 600000 ms to send 0 pending messages to Netty-Client-/10.103.16.14:31437
2016-09-21T01:55:35,925 [Netty-server-localhost-31009-worker-1] [org.apache.storm.messaging.netty.StormServerHandler] [ERROR]> server errors in handling the request
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[?:1.8.0_77]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[?:1.8.0_77]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[?:1.8.0_77]
        at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[?:1.8.0_77]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) ~[?:1.8.0_77]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-1.0.2.jar:1.0.2]
        at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-1.0.2.jar:1.0.2]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_77]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_77]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
2

And now we just continuously see these messages in the spout logs:

2016-09-21T02:03:35,513 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,644 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,774 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,817 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:35,849 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,073 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,141 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,169 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,340 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,365 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,416 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,560 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,607 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,660 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,865 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:36,894 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,026 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,051 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,065 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed
2016-09-21T02:03:37,219 [Thread-10-disruptor-worker-transfer-queue] [org.apache.storm.messaging.netty.Client] [ERROR]> discarding 1 messages because the Netty client to Netty-Client-/10.103.16.14:31437 is being closed

The worker that died (10.103.16.14.31437) was restarted by the supervisor, but I don't see any log messages in the logs indicating that it is receiving any tuples. The ""is being closed"" messages in the spout logs make me think that storm has failed to close its connection.

This has happened to us repeatedly since the upgrade. Does anyone have any suggestions about how to fix this issue? Also, I originally thought it might be related to STORM-1560, but I don't see the exception that is mentioned in that ticket.

Thanks, and I would greatly appreciate any help
"
STORM-2104,New Kafka spout crashes if partitions are reassigned while tuples are in-flight,"The new KafkaSpout may throw NPEs if partitions are reassigned while tuples are in-flight. The ack function assumes that the spout instance is always responsible for tuples it emitted, which isn't true if partitions were reassigned since the tuple was emitted. The fail function also assumes that failed tuples should be replayed, which is useless if the tuple is for a partition the spout isn't assigned, since it then can't commit the tuple if it succeeds. Both functions should check that the spout instance is responsible for the incoming tuple before scheduling it for retry or adding it to the acked list."
STORM-2100,Few tests are getting failed in external/sql module with JDK 7 ,"

Tests run: 11, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 31.095 sec <<< FAILURE! - in org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler
testCompileEquiJoinWithRightOuterJoin(org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler)  Time elapsed: 2.434 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: arrays first differed at element [0]; expected:<[2, null]> but was:<[3, null]>
	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
	at org.junit.Assert.internalArrayEquals(Assert.java:473)
	at org.junit.Assert.assertArrayEquals(Assert.java:265)
	at org.junit.Assert.assertArrayEquals(Assert.java:280)
	at org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.testCompileEquiJoinWithRightOuterJoin(TestPlanCompiler.java:161)

testCompileEquiJoinWithFullOuterJoin(org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler)  Time elapsed: 2.447 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: arrays first differed at element [0]; expected:<[null, dept-2]> but was:<[null, dept-3]>
	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
	at org.junit.Assert.internalArrayEquals(Assert.java:473)
	at org.junit.Assert.assertArrayEquals(Assert.java:265)
	at org.junit.Assert.assertArrayEquals(Assert.java:280)
	at org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.testCompileEquiJoinWithFullOuterJoin(TestPlanCompiler.java:179)

testCompileEquiJoinWithLeftOuterJoin(org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler)  Time elapsed: 2.381 sec  <<< FAILURE!
org.junit.internal.ArrayComparisonFailure: arrays first differed at element [0]; expected:<[2, null]> but was:<[3, null]>
	at org.junit.internal.ComparisonCriteria.arrayEquals(ComparisonCriteria.java:50)
	at org.junit.Assert.internalArrayEquals(Assert.java:473)
	at org.junit.Assert.assertArrayEquals(Assert.java:265)
	at org.junit.Assert.assertArrayEquals(Assert.java:280)
	at org.apache.storm.sql.compiler.backends.trident.TestPlanCompiler.testCompileEquiJoinWithLeftOuterJoin(TestPlanCompiler.java:143)
"
STORM-2097,Improve logging in trident core and examples,Improve logging to make code easier to debug and extend.
STORM-2096,Error creating jar file with maven,"I tried to alter the Wordcount file inside storm in order to make it take a file path externally in the command line. Then, I tried to make the jar file for operation using maven. 

First i ran mvn compile. The output is:-

[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 1.0.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 52 source files to /opt/storm/examples/storm-starter/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-starter ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 9.178 s
[INFO] Finished at: 2016-09-15T11:18:13+00:00
[INFO] Final Memory: 52M/691M
[INFO] ------------------------------------------------------------------------

The i ran mvn test. The output is:-

[INFO] Scanning for projects...
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building storm-starter 1.0.2
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 5 resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 52 source files to /opt/storm/examples/storm-starter/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.7.1:compile (compile) @ storm-starter ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:testResources (default-testResources) @ storm-starter ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /opt/storm/examples/storm-starter/src/test/resources
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ storm-starter ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 8 source files to /opt/storm/examples/storm-starter/target/test-classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
  symbol:   class Rankings
  location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
  symbol:   class Rankings
  location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,52] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,44] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,41] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[52,9] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[65,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,25] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[89,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[93,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,25] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[97,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[122,9] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,56] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,94] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,22] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,87] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,13] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,51] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,89] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[153,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[174,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[188,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[203,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,29] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[228,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,5] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,34] cannot find symbol
  symbol:   class Rankings
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[232,10] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,5] cannot find symbol
  symbol:   class Rankable
  location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,23] cannot find symbol
  symbol:   class RankableObjectWithFields
  location: class org.apache.storm.starter.tools.RankingsTest
[INFO] 141 errors 
[INFO] -------------------------------------------------------------
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.608 s
[INFO] Finished at: 2016-09-15T10:33:18+00:00
[INFO] Final Memory: 51M/706M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:testCompile (default-testCompile) on project storm-starter: Compilation failure: Compilation failure:
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,24] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[62,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,71] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[86,97] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[143,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,64] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[169,89] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[183,76] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[184,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,73] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[200,98] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[224,79] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,12] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[225,53] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[253,62] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,43] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[277,68] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[331,51] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,60] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[347,86] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/bolt/TotalRankingsBoltTest.java:[29,38] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: package org.apache.storm.starter.tools
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,49] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[93,77] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[98,62] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[99,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,45] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[112,73] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[117,54] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[118,7] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,31] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[137,63] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankableObjectWithFieldsTest.java:[241,36] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankableObjectWithFieldsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[34,52] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[35,44] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[36,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[37,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[38,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[39,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[40,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[41,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[42,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[43,41] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[52,9] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[64,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[65,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[70,25] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[88,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[89,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[93,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[96,25] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[97,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[114,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[122,9] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[128,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,56] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[136,94] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,22] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[137,87] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,13] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,51] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[138,89] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[150,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[153,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[171,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[174,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[186,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[187,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[188,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[202,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[203,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[206,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[227,29] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[228,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,5] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[231,34] cannot find symbol
[ERROR] symbol:   class Rankings
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[232,10] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,5] cannot find symbol
[ERROR] symbol:   class Rankable
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] /opt/storm/examples/storm-starter/test/jvm/org/apache/storm/starter/tools/RankingsTest.java:[245,23] cannot find symbol
[ERROR] symbol:   class RankableObjectWithFields
[ERROR] location: class org.apache.storm.starter.tools.RankingsTest
[ERROR] -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException

I tried removing the rank files. Even that didn't rectify the problem. Please advice how I can make a jar file to run the modified wordcount file."
STORM-2095,Nimbus dies and never recovers due to java.nio.file.DirectoryNotEmptyException,"To Recreate:
--------------------------------------
1) Create a blobstore key for a large file (1 or 2 GB). Size of the file does not matter if nimbus can be killed while the blob is being created.
2) while the blob is being created, restart nimbus (this is easiest way to regenerate, there can be various reasons due to which a blob couldn't be successfully created in nimbus)
3) When nimbus tries to start on restart, it will keep dying due to DirectoryNotEmptyException and never come up.

Expected Behavior
--------------------------------------
Partial blobstore key is deleted cleanly and doesn't affect nimbus.

The actual, incorrect behavior.
--------------------------------------
2016-09-14 15:07:48.518 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-09-14 15:07:48.576 o.a.s.zookeeper [INFO] xxx gained leadership
2016-09-14 15:07:48.581 o.a.s.d.nimbus [ERROR] Error on initialization of server service-handler
java.lang.RuntimeException: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file
	at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:229)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.daemon.nimbus$setup_blobstore.invoke(nimbus.clj:1196)
	at org.apache.storm.daemon.nimbus$fn__7064$exec_fn__2461__auto____7065.invoke(nimbus.clj:1416)
	at clojure.lang.AFn.applyToHelper(AFn.java:156)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at clojure.core$apply.invoke(core.clj:630)
	at org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)
	at clojure.lang.RestFn.invoke(RestFn.java:421)
	at org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)
	at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)
	at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)
	at clojure.lang.AFn.applyToHelper(AFn.java:152)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at org.apache.storm.daemon.nimbus.main(Unknown Source)
Caused by: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file
	at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1165)
	at org.apache.storm.blobstore.FileBlobStoreImpl.delete(FileBlobStoreImpl.java:239)
	at org.apache.storm.blobstore.FileBlobStoreImpl.deleteKey(FileBlobStoreImpl.java:178)
	at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:226)
	... 19 more
2016-09-14 15:07:48.588 o.a.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)
	at clojure.lang.RestFn.invoke(RestFn.java:421)
	at org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)
	at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)
	at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)
	at clojure.lang.AFn.applyToHelper(AFn.java:152)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at org.apache.storm.daemon.nimbus.main(Unknown Source)


[root]# ls -l  /opt/storm/storm-local/blobs/955/some_big_file
total 591060
-rw-r--r-- 1 storm storm 605241344 Sep 14 15:07 1473865562841.tmp"
STORM-2090,Add integration test for storm windowing,"We want to add integration test for storm windowing feature.
It will be nice to have this running with existing travis-ci setup."
STORM-2088,"Typos in documentation ""Guaranteeing Message Processing""","Minor typos in ""Guaranteeing Message Processing"" page."
STORM-2087,Storm-kafka-client: Failed tuples are not always replayed ,"I am working with kafka 10 and the storm-kafka-client from master. It appears that tuples are not always being replayed when they are failed.

With a topology that randomly fails tuples a small percentage of the time I found that the committed kafka offset would get stuck and eventually processing would stop even though the committed offset was no where near the end of the topic. 

I have also replicated the issue in unit tests with this PR: 
https://github.com/apache/storm/pull/1679

It seems that increasing the number of times I call nextTuple for the in order case will make it work, but it doesn't seem to help the case where tuples are failed out of order from which they were emitted. "
STORM-2084,after supervisor v2 merge async localizer and localizer,"Once we mere in STORM-2018 
https://github.com/apache/storm/pull/1642 

we should look into merging the two localizers into a single class."
STORM-2080,storm-submit-tools license check failure,
STORM-2077,KafkaSpout doesn't retry failed tuples,"KafkaSpout does not retry all failed tuples.

We used following Configuration:
        Map<String, Object> props = new HashMap<>();
        props.put(KafkaSpoutConfig.Consumer.GROUP_ID, ""c1"");
        props.put(KafkaSpoutConfig.Consumer.KEY_DESERIALIZER, ByteArrayDeserializer.class.getName());
        props.put(KafkaSpoutConfig.Consumer.VALUE_DESERIALIZER, ByteArrayDeserializer.class.getName());
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker.bootstrapServer());

        KafkaSpoutStreams kafkaSpoutStreams = new KafkaSpoutStreams.Builder(FIELDS_KAFKA_EVENT, new String[]{""test-topic""}).build();

        KafkaSpoutTuplesBuilder<byte[], byte[]> kafkaSpoutTuplesBuilder = new KafkaSpoutTuplesBuilder.Builder<>(new KeyValueKafkaSpoutTupleBuilder(""test-topic"")).build();
        KafkaSpoutRetryService retryService = new KafkaSpoutLoggedRetryExponentialBackoff(KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.milliSeconds(1), KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.milliSeconds(1), 3, KafkaSpoutLoggedRetryExponentialBackoff.TimeInterval.seconds(1));

        KafkaSpoutConfig<byte[], byte[]> config = new KafkaSpoutConfig.Builder<>(props, kafkaSpoutStreams, kafkaSpoutTuplesBuilder, retryService)
                .setFirstPollOffsetStrategy(UNCOMMITTED_LATEST)
                .setMaxUncommittedOffsets(30)
                .setOffsetCommitPeriodMs(10)
                .setMaxRetries(3)
                .build();

kafkaSpout = new org.apache.storm.kafka.spout.KafkaSpout<>(config);


The downstream bolt fails every tuple and we expect, that those tuple will all be replayed. But that's not the case for every tuple."
STORM-2076,Supervisor sync-processes and sync-supervisor race when downloading new topology code.,"The fix for https://issues.apache.org/jira/browse/STORM-1934 moved the cleanup of topology code to sync-processes. The cleanup is based on ls-local-assignment, but this is not called from sync-supervisor until all the new topology code has been downloaded. As a result, sync-processes may delete new topology code before sync-supervisor has had a chance to update ls-local-assignment."
STORM-2071,nimbus-test test-leadership failing with Exception,"When running unit tests on my Mac, I get repeated failures in test-leadership.

~~~~
73752 [main] INFO  o.a.s.l.ThriftAccessLogger - Request ID: 1 access from: null principal: null operation: deactivate
]]>            </system-out>
            <error message=""Uncaught exception, not in assertion."">Uncaught exception, not in assertion.
expected: nil
  actual: java.lang.RuntimeException: No transition for event: :inactivate, status: {:type :rebalancing} storm-id: t1-1-1472598899
 at org.apache.storm.daemon.nimbus$transition_BANG_$get_event__4879.invoke (nimbus.clj:365)
    org.apache.storm.daemon.nimbus$transition_BANG_.invoke (nimbus.clj:373)
    clojure.lang.AFn.applyToHelper (AFn.java:165)
    clojure.lang.AFn.applyTo (AFn.java:144)
    clojure.core$apply.invoke (core.clj:636)
    org.apache.storm.daemon.nimbus$transition_name_BANG_.doInvoke (nimbus.clj:391)
    clojure.lang.RestFn.invoke (RestFn.java:467)
    org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__5850.deactivate (nimbus.clj:1773)
    sun.reflect.NativeMethodAccessorImpl.invoke0 (NativeMethodAccessorImpl.java:-2)
    sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    java.lang.reflect.Method.invoke (Method.java:497)
    clojure.lang.Reflector.invokeMatchingMethod (Reflector.java:93)
    clojure.lang.Reflector.invokeInstanceMethod (Reflector.java:28)
    org.apache.storm.nimbus_test$fn__1203$fn__1209.invoke (nimbus_test.clj:1222)
    org.apache.storm.nimbus_test/fn (nimbus_test.clj:1210)
    clojure.test$test_var$fn__7670.invoke (test.clj:704)
    clojure.test$test_var.invoke (test.clj:704)
    clojure.test$test_vars$fn__7692$fn__7697.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars$fn__7692.invoke (test.clj:722)
    clojure.test$default_fixture.invoke (test.clj:674)
    clojure.test$test_vars.invoke (test.clj:718)
    clojure.test$test_all_vars.invoke (test.clj:728)
(test.clj:747)
    clojure.core$map$fn__4553.invoke (core.clj:2624)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.Cons.next (Cons.java:39)
    clojure.lang.RT.boundedLength (RT.java:1735)
    clojure.lang.RestFn.applyTo (RestFn.java:130)
    clojure.core$apply.invoke (core.clj:632)
    clojure.test$run_tests.doInvoke (test.clj:762)
    clojure.lang.RestFn.invoke (RestFn.java:408)
    org.apache.storm.testrunner$eval8358$iter__8359__8363$fn__8364$fn__8365$fn__8366.invoke (test_runner.clj:107)
    org.apache.storm.testrunner$eval8358$iter__8359__8363$fn__8364$fn__8365.invoke (test_runner.clj:53)
    org.apache.storm.testrunner$eval8358$iter__8359__8363$fn__8364.invoke (test_runner.clj:52)
    clojure.lang.LazySeq.sval (LazySeq.java:40)
    clojure.lang.LazySeq.seq (LazySeq.java:49)
    clojure.lang.RT.seq (RT.java:507)
    clojure.core/seq (core.clj:137)
    clojure.core$dorun.invoke (core.clj:3009)
    org.apache.storm.testrunner$eval8358.invoke (test_runner.clj:52)
    clojure.lang.Compiler.eval (Compiler.java:6782)
    clojure.lang.Compiler.load (Compiler.java:7227)
    clojure.lang.Compiler.loadFile (Compiler.java:7165)
    clojure.main$load_script.invoke (main.clj:275)
    clojure.main$script_opt.invoke (main.clj:337)
    clojure.main$main.doInvoke (main.clj:421)
    clojure.lang.RestFn.invoke (RestFn.java:421)
    clojure.lang.Var.invoke (Var.java:383)
    clojure.lang.AFn.applyToHelper (AFn.java:156)
    clojure.lang.Var.applyTo (Var.java:700)
    clojure.main.main (main.java:37)
~~~~"
STORM-2070,Sigar native binary download link went 404,"{code}
  <properties>
    <!-- settings for downloading the sigar native binary complete archive, which is not available in Maven central-->
    <sigar.version>1.6.4</sigar.version>
    <sigar.download.url>https://magelan.googlecode.com/files/hyperic-sigar-${sigar.version}.zip</sigar.download.url>
    <sigar.SHA1>8f79d4039ca3ec6c88039d5897a80a268213e6b7</sigar.SHA1>
    <!-- this will download the sigar ZIP to the local maven repository next to the sigar dependencies,
         so we only download it once -->
    <sigar.download.path>${settings.localRepository}/org/fusesource/sigar/${sigar.version}</sigar.download.path>
  </properties>
{code}

Sigar download url is set to https://magelan.googlecode.com/files/hyperic-sigar-1.6.4.zip which is not working. 
Google Code seems changed their download link. Current link of sigar binary 1.6.4 is https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/magelan/hyperic-sigar-1.6.4.zip"
STORM-2067,"""array element type mismatch"" from compute-executors in nimbus.clj","In some scenarios, Nimbus throws ""java.lang.IllegalArgumentException: array element type mismatch"".

{noformat}
08:49:35.321 [timer] ERROR o.a.s.d.nimbus - Error when processing event
java.lang.IllegalArgumentException: array element type mismatch
	at java.lang.reflect.Array.set(Native Method) ~[?:1.8.0_66]
	at clojure.lang.RT.seqToTypedArray(RT.java:1719) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.seqToTypedArray(RT.java:1692) ~[clojure-1.7.0.jar:?]
	at clojure.core$into_array.invoke(core.clj:3319) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$compute_executors$fn__4307.doInvoke(nimbus.clj:645) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:408) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$compute_executors.invoke(nimbus.clj:645) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$compute_executor__GT_component.invoke(nimbus.clj:655) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:565) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$mk_assignments$iter__4668__4672$fn__4673.invoke(nimbus.clj:967) ~[classes/:?]
	at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.7.0.jar:?]
	at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.seq(RT.java:507) ~[clojure-1.7.0.jar:?]
	at clojure.core$seq__4128.invoke(core.clj:137) ~[clojure-1.7.0.jar:?]
	at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.7.0.jar:?]
	at clojure.core.protocols$fn__6506.invoke(protocols.clj:101) ~[clojure-1.7.0.jar:?]
	at clojure.core.protocols$fn__6452$G__6447__6465.invoke(protocols.clj:13) ~[clojure-1.7.0.jar:?]
	at clojure.core$reduce.invoke(core.clj:6519) ~[clojure-1.7.0.jar:?]
	at clojure.core$into.invoke(core.clj:6600) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:966) ~[classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:410) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.nimbus$fn__5354$exec_fn__579__auto____5355$fn__5366$fn__5367.invoke(nimbus.clj:2409) ~[classes/:?]
	at org.apache.storm.daemon.nimbus$fn__5354$exec_fn__579__auto____5355$fn__5366.invoke(nimbus.clj:2408) ~[classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.StormTimer$1.run(StormTimer.java:190) ~[classes/:?]
	at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83) [classes/:?]
{noformat}

The exception is thrown from into-array, which is called from below line:
{code}
((fn [ & maps ] (Utils/joinMaps (into-array (into [component->executors] maps)))))
{code}"
STORM-2065,Add tooltip descriptions for config keys to the UI,"As an admin/a user, I would like to know the purpose of various config settings I see on the UI, so that I can make the correct changes to my cluster/topology.



This could be accomplished with a simple annotation for each key in our config classes, similar to those we use for validation, but just a simple string.  The annotations could simply duplicate the text we already have in the javadoc comments.

The UI could send this text to the browser in as a tooltip pop-up when the mouse hovers over one of the config keys."
STORM-2062,Hive streaming doesn't support non string partition fields,"create hive table with an int partition column

CREATE TABLE CDRDWH.CDR_FACT (
geo_id int,
time_id smallint,
cust_id smallint,
vend_id smallint,
cust_rel_id smallint,
vend_rel_id smallint,
route tinyint,
connect boolean,
earlyEvent boolean,
Call_duration_cust double,
I_PDD double,
E_PDD double,
orig_number string,
term_number string
)
partitioned by (date_id int)
clustered by (geo_id, time_id) into 16 buckets
stored as ORC
tblproperties (""orc.compress""=""SNAPPY”);

When i try to stream my topolgy output to Hive I get the following exception:

11829 [Thread-31-hivewriter-executor[5 5]] ERROR o.a.s.d.executor - 
java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String
at org.apache.storm.tuple.TupleImpl.getStringByField(TupleImpl.java:153) ~[storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper.mapPartitions(DelimitedRecordHiveMapper.java:92) ~[storm-hive-1.0.1.jar:1.0.1]
at org.apache.storm.hive.bolt.HiveBolt.execute(HiveBolt.java:112) [storm-hive-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) [storm-core-1.0.1.jar:1.0.1]
at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]


Line 92 of DelimtedRecordHiveMapper is attempting to access my integer field as a String and the subsequent exception is thrown
 @Override
    public List<String> mapPartitions(Tuple tuple) {
        List<String> partitionList = new ArrayList<String>();
        if(this.partitionFields != null) {
            for(String field: this.partitionFields) {
                partitionList.add(tuple.getStringByField(field));
            }
        }
        if (this.timeFormat != null) {
            partitionList.add(getPartitionsByTimeFormat());
        }
        return partitionList;
    }"
STORM-2055,Exception when running topology from Maven exec with Flux,"When running a topology from Maven with Flux as a dependency, we get

{code}
11335 [Thread-8] ERROR o.a.s.event - Error when processing event
java.io.FileNotFoundException: Source 'file:/home/julien/.m2/repository/org/apache/storm/flux-core/1.0.1/flux-core-1.0.1.jar!/resources' does not exist
    at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.daemon.supervisor$fn__9359.invoke(supervisor.clj:1194) ~[storm-core-1.0.1.jar:1.0.1]
    at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
    at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078$fn__9096.invoke(supervisor.clj:582) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078.invoke(supervisor.clj:581) ~[storm-core-1.0.1.jar:1.0.1]
    at org.apache.storm.event$event_manager$fn__8630.invoke(event.clj:40) [storm-core-1.0.1.jar:1.0.1]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.8.0_101]
{code}

The same topology runs fine when executed with Eclipse or via the storm command.

See [https://github.com/DigitalPebble/storm-crawler/issues/324]"
STORM-2054,DependencyResolver should be aware of relative path and absolute path,"DependencyResolver always create directory based on storm.home or current working directory which is intended for relative path but not intended for absolute path. 

Furthermore, DependencyResolverTest doesn't remove temporary directory after testing. Test creates a new temporary absolute path but due to this bug, temporary directory is created in working directory which prevents cleaning up, and finally making RAT error on all builds."
STORM-2052,Kafka Spout New Client API - Log Improvements and Parameter Tuning for Better Performance,"Tune Kafka Spout parameters 
Improve Logging to show more meaningful messages, and print detail appropriate to logging level."
STORM-2051,Flux should support the builder pattern,"While trying to work with {{Flux}} and the {{storm-kafka-client}} package we noticed that they are incompatible, unfortunately, as the needed {{KafkaSpoutConfig}} is based on the builder pattern. Unless some hacky method is used it will not be possible to configure a {{KafkaSpout}} and instantiate/use it with a Flux-based topology.

Flux could be enhanced to support the builder pattern with the following yaml configuration as a proposal:

{code}
builder:
  - id: ""spoutConfigBuilder""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig.Builder""
    builderMethod: ""build""
    constructorArgs:
      - [...]
    properties:
      - [...]
    configMethods:
      - [...]
components:
  - id: ""spoutConfig""
    className: ""org.apache.storm.kafka.spout.KafkaSpoutConfig""
    builderRef: ""spoutConfigBuilder""
spouts:
  - id: ""kafkaSpout""
    className: ""org.apache.storm.kafka.spout.KafkaSpout""
    constructorArgs:
      - ref: ""spoutConfig""
{code}

Unfortunately, for now, we are busy with other tasks so we cannot work on a patch for Flux. But we thought it's better to report / suggest this enhancement nevertheless."
STORM-2048,Refactor code blocks which are ported to for-loop to Java Stream API,"We just changed minimum requirement for master branch to Java 1.8 from STORM-2041. 

Thanks for the change we can change ported code block which was functional style to similar style again.

We could even broaden the boundary of this issue for applying other benefits from Java 8, or file separate issues."
STORM-2046,Errors when using TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE in local mode.,"When using a LocalCluster during tests, if {{TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE}} is specified, {{assert-can-serialize}} attempts to destructure a Java model object and throws, killing the worker. A minimal-ish case and the full logs are here: https://gist.github.com/ckolbeck/557734429e62b097efa9382a714122b0"
STORM-2045,NPE in SpoutExecutor in 2.0 branch,"This issue was raised in [STORM-1949], but since the original issue mainly discusses about whether to disable ABP by default, I'd like to pick this NPE as another issue."
STORM-2044,Nimbus should not make assignments crazily when Pacemaker goes down and up,"        Now pacemaker is a stand-alone service and no HA is supported. When it goes down, all the workers's heartbeats will be lost. It will take a long time to recover even if pacemaker goes up immediately if there are dozens GB of heartbeats. During the time worker heartbeats are not restored completely, Nimbus will think these workers are dead because of heartbeats timeout and reassign these ""dead"" workers continuously until heartbeats restore to normal. So, during recovery time, many topologies will be reassigned continuously and the throughout will goes very down.  
        This is not acceptable. 
        So i think, pacemaker is not suitable for production if the problem above exists.
               i think several ways to solve this problem:
              1. pacemaker HA
              2. when pacemaker does down, notice nimbus not to reassign any more until it recover"
STORM-2043,Nimbus should not make assignments crazy when Pacemaker down,"When pacemaker goes down, all the heartbeats of workers are lost. These heartbeats will need a long time to recover even if pacemaker goes up immediately if it costs dozens of GB memory. During the time worker heartbeats are not complete，Nimbus will think the workers are died( heartbeat time out ),  and reassign these workers crazily. But actually the workers are healthy, the reassignment will move in cycles until pacemaker heartbeats recover. During this time, all the topologies's throughout will goes down. We should avoid this, because Pacemaker has no HA."
STORM-2040,Config.TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE=true causes j.l.UnsupportedOperationException: nth not supported on this type: AddressedTuple,"When Config.TOPOLOGY_TESTING_ALWAYS_TRY_SERIALIZE is enabled for a topology, the components fail with the following exception:

26168 [Thread-13-disruptor-executor[1 1]-send-queue] ERROR o.a.s.d.executor - 
java.lang.RuntimeException: java.lang.UnsupportedOperationException: nth not supported on this type: AddressedTuple
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:464) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:430) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__7509.invoke(disruptor.clj:83) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.util$async_loop$fn__624.invoke(util.clj:484) [storm-core-1.0.2.jar:1.0.2]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_102]
Caused by: java.lang.UnsupportedOperationException: nth not supported on this type: AddressedTuple
	at clojure.lang.RT.nthFrom(RT.java:933) ~[clojure-1.7.0.jar:?]
	at clojure.lang.RT.nth(RT.java:883) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$assert_can_serialize.invoke(worker.clj:130) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.worker$mk_transfer_fn$fn__8214.invoke(worker.clj:202) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__7898.invoke(executor.clj:312) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.disruptor$clojure_handler$reify__7492.onEvent(disruptor.clj:40) ~[storm-core-1.0.2.jar:1.0.2]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:451) ~[storm-core-1.0.2.jar:1.0.2]
	... 6 more
26171 [Thread-15-__acker-executor[163 163]] INFO  o.a.s.d.executor - Preparing bolt __acker:(163)
26171 [Thread-15-__acker-executor[163 163]] INFO  o.a.s.d.executor - Prepared bolt __acker:(163)"
STORM-2038,Provide an alternative to using symlinks,"As of Storm 1.0 and above, some functionality (such as the worker-artifacts directory) require the use of symlinks. On Windows platforms, this requires that Storm either be run as an administrator or that certain group policy settings are changed.

In locked-down environments, both of these solutions are not suitable.

Where possible, an alternative option should be provided to the use of symlinks. For example, it may be possible to create additional copies of the worker artifacts directory for each worker (possibly inefficient) or provide the workers with the canonical path to the real directory.

See the [brief discussion|http://mail-archives.apache.org/mod_mbox/storm-dev/201608.mbox/%3C1293850887.13165119.1471022901569.JavaMail.yahoo%40mail.yahoo.com%3E] on the mailing list."
STORM-2037,debug operation should be whitelisted in SimpleAclAuthorizer,"For topology event logging to work in secure mode, the ""debug"" operation should be whitelisted."
STORM-2032,"""not fast enough"" metrics WARN message in netty client can be misinterpreted","Example:

bq. WARN Messages are not being delivered fast enough, got 3 metrics messages at once

It appears to some users to be a good signal for monitoring/alerting, but really this is not part of the design.

We should remove it, change it, or lower the log level."
STORM-2031,Multilang: support distribute cache API a.k.a BlobStore,"Distribute Cache API (BlobStore) comes in Storm 1.0.0 but there's no support for multi-lang.

It would be better to let multilang users enjoy the benefits together.

I just marked as 'Wish' since BlobStore implementation is fit for Java and it would be not easy to cover with only improving multilang protocol. BlobStore API exposes InputStream and OutputStream for downloading / creating & updating the contents so we should deal with this for other languages in order to get the feature."
STORM-2030,Multilang: support stateful bolt,"Stateful bolt with checkpointing feature comes in Storm 1.0.0 but there's no support for multi-lang.

It would be better to let multilang users enjoy the benefits together.

Implementation details and difficulty would be up to where we define and manage the State, Java (ShellBolt) vs Multi-lang."
STORM-2029,Multilang: support windowing feature,"Windowing feature comes in Storm 1.0.0 but there's no support for multi-lang.

It would be better to let multilang users enjoy the benefits together. 

Implementation should be fairly simple since there's tiny change in point of Bolt interface view."
STORM-2028,Exceptions in JDBCClient are hidden by subsequent SQL-Exception in close(),"When an Exception is triggered in JdbcClient.executeInsertQuery there is the potential for a follow-up Exception in close() to take precedence over the previously thrown Exception, when triggered in the finally block. This makes debugging the actual Exception impossible.

As far as I can tell it would be better to catch the Exception form close() in the finally-block, and to combine it with the existing Exception, so that the key information for debugging purposes isn't lost.

For data consistency purposes we have to make sure that the Exception from closing the connection is thrown (or do we? can we be sure that a successful commit has persisted the data?) but ""overlapping"" Exceptions have to be dealt with.

Alternatively it might be a good idea to log the Exceptions before throwing them, so that the stack trace isn't lost. This is probably easier than tracking in the finally block whether a previous Exception has been thrown, and what to do with it.

If there's a workaround for this, that I might have missed, to get to the root of the Exception, I would also be interested in hearing, I'm currently looking at a situation where jdbc fails, and there being no indication of what's going on.

I labelled this newbie-level, since the implementation is pretty trivial; but the decision of which way to pursue isn't as clear to me."
STORM-2026,"Inconsistency between (SpoutExecutor, BoltExecutor) and (spout-transfer-fn, bolt-transfer-fn)","As I left the comment from https://github.com/apache/storm/pull/1445#discussion_r73255197 for pull request, there's some difference between SpoutExecutor / BoltExecutor and spout-transfer-fn / bolt-transfer-fn.

While it's not that big to fix, I just want to not block port work and just address from here."
STORM-2025,dropping messages in withTumblingWindow,"when i use {{withTumblingWindow}} and process the input messages, if the processing time is longer than input rate, we will not get all input messages.

{code}
int count=0;
	@Override
	public void execute(TupleWindow inputWindow) {
		try {
			List<Event> windowEvenets = new ArrayList<>();
			for(Tuple tuple: inputWindow.get()) {
				count++;
				/* some operation here */
			}
			logger.info(count + ""======= Process event "");
			Thread.sleep(4000);
		}
		catch (Exception ex) {
			ex.printStackTrace();
		}
	}
{code}

The topology is as follow:
{code}
 TopologyBuilder builder = new TopologyBuilder();
            builder.setSpout(""KafkaSpout"", new KafkaSpout(kafkaConfig), 1);
            builder.setBolt(""WindowInputTest"", new WindowInputTest(zookeeperHosts).withTumblingWindow(new BaseWindowedBolt.Duration(4,TimeUnit.SECONDS)), 1).shuffleGrouping(""KafkaSpout"");
{code}"
STORM-2023,Add calcite-core to dependency of storm-sql-runtime,"storm-sql provides both ""storm-sql-core"" and ""storm-sql-runtime"". Former is for compiling sql to trident topology, and latter is for running compiled trident topology.

While testing storm-sql feature I found compiled class codes refers calcite so calcite-core is needed at runtime. 
I'm not sure we can make calcite-core get out of storm-sql-runtime, so for now we can add calcite-core to storm-sql-runtime so that it can be available on transitive dependencies for storm-sql-runtime."
STORM-2022,FieldsTest.selectingUnknownFieldThrowsTest is failing,"{code}
<testcase name=""selectingUnknownFieldThrowsTest"" classname=""org.apache.storm.tuple.FieldsTest"" time=""0.007"">
    <error message=""Unexpected exception, expected&lt;java.lang.NullPointerException&gt; but was&lt;java.lang.IllegalArgumentException&gt;"" type=""java.lang.Exception""><![CDATA[java.lang.Exception: Unexpected exception, expected<java.lang.NullPointerException> but was<java.lang.IllegalArgumentException>
        at org.apache.storm.tuple.Fields.fieldIndex(Fields.java:104)
        at org.apache.storm.tuple.Fields.select(Fields.java:63)
        at org.apache.storm.tuple.FieldsTest.selectingUnknownFieldThrowsTest(FieldsTest.java:124)
]]></error>
  </testcase>
{code}"
STORM-2021,storm-kinesis missing licenses,"{code}
Unapproved licenses:

  external/storm-kinesis/src/test/java/org/apache/storm/kinesis/spout/test/KinesisBoltTest.java
  external/storm-kinesis/src/test/java/org/apache/storm/kinesis/spout/test/KinesisSpoutTopology.java
  external/storm-kinesis/src/test/java/org/apache/storm/kinesis/spout/test/TestRecordToTupleMapper.java
{code}"
STORM-2020,Stop using sun internal classes,"sun.reflect.generics.reflectiveObjects.NotImplementedException, sun.misc.BASE64Decoder, and sun.misc.BASE64Encoder are not public APIs we should not be using them."
STORM-2019,NullPointerException in Kafka-Spout,"KafkaSpout reports following error:
java.lang.NullPointerException
        at java.util.TreeMap.rotateLeft(TreeMap.java:2220) ~[?:1.8.0_77]
        at java.util.TreeMap.fixAfterInsertion(TreeMap.java:2287) ~[?:1.8.0_77]
        at java.util.TreeMap.put(TreeMap.java:582) ~[?:1.8.0_77]
        at org.apache.storm.kafka.PartitionManager.fill(PartitionManager.java:235) ~[stormjar.jar:?]
        at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:138) ~[stormjar.jar:?]
        at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[stormjar.jar:?]
        at applications.spout.KafkaSpoutWrapper.nextTuple(KafkaSpoutWrapper.java:64) ~[stormjar.jar:?]
        at org.apache.storm.daemon.executor$fn__7885$fn__7900$fn__7931.invoke(executor.clj:645) ~[storm-core-1.0.1.jar:1.0.1]
        at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_77]
"
STORM-2018,Simplify Threading Model of the Supervisor,"We have been trying to roll out CGROUP enforcement and right now are running into a number of race conditions in the supervisor.  When using CGROUPS the timing of some operations are different and are exposing issues that we would not see without this.

In order to make progress with testing/deploying CGROUP and RAS we are going to try and refactor the supervisor to have a simpler threading model, but likely with more threads.  We will base the code off of the java code currently in master, and may replace that in the 2.0 release, but plan on having it be a part of 1.x too, if it truly is more stable.

I will try to keep this JIRA up to date with what we are doing and the architecture to keep the community informed.  We need to move quickly to meet some of our company goals but will not just shove this in.  We welcome any feedback on the design and code before it goes into the community."
STORM-2017,ShellBolt stops reporting task ids,"After running enough flow throw ShellBolt in some cases after tens of minutes ShellBolt stopped reporting task ids. After this error condition no new task ids where reported back. When acking of the tuples processed by the bolt where set in callback related to arrival of the task ids all tuple trees going through the bolt would fail after reporting stopped. ShellBolt will continue to operate new tuples and respond to heartbeats.

After running some tests and making some changes to the code. I have following hypothesis for the reason:

org.apache.storm.utils.ShellBoltMessageQueue has two queues one being for taskIds and the other for bolt messages.
taskIds queue is implemented by LinkedList and bolt msg queue LinkedBlockingQueue. Both of the queues are operated similarly.
One major difference between the structures is that LinkedList is not synchronized.

In the code:

ShellBoltMessageQueue.java:58 add method is used without holding the lock. Where as ShellBoltMessageQueue.java:110 uses the poll method with the lock. 
As in ShellBolt BoltReaderRunnable and BoltWriterRunnable are run concurrently this can lead to race condition.

If I move the ShellBoltMessageQueue.java:58 inside the lock and run the test in similar fashion it seems to solve the issue."
STORM-2006,Storm metrics feature improvement: support per-worker level metrics aggregation,"Storm provides per-task level metrics which could be huge when topology has a number of tasks. 
Task level metric is useful for determining load balance between tasks, but it doesn't need to be time-series fashion.

Before introducing topology level component like TopologyMaster for JStorm, we can utilize SystemBolt to aggregate task level metrics to per-worker level metrics.

We should provide options and this feature should be turned off by default to keep backward compatibility. "
STORM-2003,Make sure config contains TOPIC before get it,"When topic selector is not specified, KafkaBolt will get topic name from storm config . We should make sure the topic name is not null ."
STORM-2001,New Kafka Spout Consume Max Records,"Kafka 0.10 have add a new parameter (max.poll.records) that use to control the number of messages returned in a single call to poll(). It's useful to control topology QPS . 

[Implement max.poll.records for new consumer (KIP-41)|https://issues.apache.org/jira/browse/KAFKA-3007]
"
STORM-2000,Add opentsdb libs to external dir in installation.,storm-opentsdb is missing in $storm-installation-dir/external dir. This module should be packaged in external directory of the installation.
STORM-1999,Update docs with OutputCollector's threadsafety,"From 1.x branch, Updated docs about OutputCollector;s threadsafety."
STORM-1997,Support Kafka Bolt in Storm Kafka Client,Support KafkaBolt in storm-kafka-client
STORM-1996,Supplement Kafka Test Case,Storm Kafka Client's test package seems not a test case . 
STORM-1995,downloadChunk in nimbus.clj should close the input stream,
STORM-1992,Deploy multilang-javascript code as node package,"Now that storm includes Flux, it is easier than ever to deploy a topology with javascript components. If the Bolt and Spout base classes defined in storm/multi-lang/javascript were available in a node.js package on https://www.npmjs.com/ it would allow node.js storm users to take advantage of node's built in package manager to develop their own bolts and spouts.

It would be relatively trivial to add some maven tasks to storm/multi-lang/javascript/pom.xml to take the storm.js resource and package it in a node module and submit it to npm.

This could be added to the pom as a separate profile so it wouldn't impact the normal storm build process.

This integration will also make it easier to add unit tests for storm.js

For additional background see this discussion: 

http://mail-archives.apache.org/mod_mbox/storm-dev/201607.mbox/%3CCAD8EKPHc6O1LCnoQUUoYoDuMQ3uSaNpD5gR4onK%2B0EL5_qcZ3Q%40mail.gmail.com%3E

If this sounds like a worthwhile addition to the project I would be happy to submit a PR.
"
STORM-1991,Support auto.commit.interval in Kafka Client,Support auto.commit.interval in Kafka Client
STORM-1990,Make some constant unvisible,"Some constant used as default config , make them private and unvisible ."
STORM-1988,Kafka Offset not showing due to bad classpath,"STORM-1950 breaks classpath of storm-kafka-monitor. 

Classpath doesn't work with wildcard and filename prefix/postfix. It was added for purposing to prevent other libs to also included as classpath, but it just doesn't work. My bad.

We should fix classpath to specify full filename path or directory/* pattern."
STORM-1986,Local BlobStore only lists blobs in local rather than all available blobs on nimbuses.,"This is follow up issue from [~revans2]'s comment on STORM-1977

https://github.com/apache/storm/pull/1574#issuecomment-233638403

Quote part of the comment describing bug:
{quote}
The one bug I saw while going through the code is that when we list keys, we are doing it only from the local storage, not from ZK.
{quote}

This is not same behavior for HDFS backed so it is definitely a bug which should be addressed."
STORM-1984,Race during rebalance,"We have been seeing an issue with a storm cluster getting into a restart loop because of bad topology state saved in ZK.

On startup, we are seeing a rebalance timer being set with a time value of nil.

This rebalance was called during a startup state transition here..

https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L330-L336

The problem is that topology-action-options is nil in storm-base.  

(I added a temporary debug print)

2016-07-19 14:41:56.604 b.s.d.nimbus [INFO] In state-transitions #backtype.storm.daemon.common.StormBase{:storm-name ""test1"", :launch-time-secs 1468879726, :status {:type :rebalancing}, :num-workers 3, :component->executors {""__system"" 0, ""__acker"" 3, ""exclaim2"" 2, ""exclaim1"" 3, ""word"" 10}, :owner ""hadoopqa"", :topology-action-options nil, :prev-status {:type :active}}

If nimbus happens to crash during the rebalancing state, before the scheduler can reschedule the topology and then return it back to active or inactive, but after storm-base was set to nil here....

https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L292-L299

Then we get into a state where nimbus will crash repeatedly if supervised on startup.

We should remove the set of topology options to nil in do-rebalance, and / or ignore the rebalance on startup if the delay can't be read."
STORM-1983,"Topology Page: Visualization form is generated for each pushing of the 'Show Visualization' button, and only first one shows graph properly","In topology page, visualization form is generated for each pushing of the 'Show Visualization' button, and only first one shows graph properly.

I'll attach screenshot on this.
"
STORM-1982,Topology running Local Cluster is not properly destroyed,"The process which run LocalCluster is not properly destroyed.

I'll attach the console log after kill is triggered, and jstack dump."
STORM-1977,Leader Nimbus crashes with getClusterInfo when it doesn't have one or more replicated topology codes,"While investigating STORM-1976, I found that there're cases for nimbus to not having topology codes. 
Before BlobStore, only nimbuses which is having all topology codes can gain leadership, otherwise they give up leadership immediately. While introducing BlobStore, this logic is removed.

I don't know it's intended or not, but it incurs one of nimbus to gain leadership which doesn't have replicated topology code, and the nimbus will be crashed when getClusterInfo is requested.

Easiest way to reproduce is:

1. comment cleanup-corrupt-topologies! from nimbus.clj (It's a quick workaround for resolving STORM-1976), and patch Storm cluster
2. Launch Nimbus 1 (leader)
3. Run topology
4. Kill Nimbus 1
5. Launch Nimbus 2 from different node
6. Nimbus 2 gains leadership 
7. getClusterInfo is requested to Nimbus 2, and Nimbus 2 gets crashed

Log:

{code}
2016-07-17 08:47:48.378 o.a.s.b.FileBlobStoreImpl [INFO] Creating new blob store based in /grid/0/hadoop/storm/blobs
...
2016-07-17 08:47:48.619 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-07-17 08:47:48.651 o.a.s.zookeeper [INFO] <node1> gained leadership
...
2016-07-17 08:47:48.833 o.a.s.d.nimbus [INFO] Starting nimbus server for storm version '1.1.1-SNAPSHOT'
2016-07-17 08:47:49.295 o.a.s.t.ProcessFunction [ERROR] Internal error processing getClusterInfo
KeyNotFoundException(msg:production-topology-2-1468745167-stormcode.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:268)
...
        at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:498)
        at org.apache.storm.daemon.nimbus$get_cluster_info$iter__9520__9524$fn__9525.invoke(nimbus.clj:1427)
...
        at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1401)
        at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9612.getClusterInfo(nimbus.clj:1838)
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3724)
        at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3708)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
...
2016-07-17 08:47:49.397 o.a.s.b.BlobStoreUtils [ERROR] Could not download blob with keyproduction-topology-2-1468745167-stormconf.ser
2016-07-17 08:47:49.400 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyproduction-topology-2-1468745167-stormconf.ser
2016-07-17 08:47:49.402 o.a.s.d.nimbus [ERROR] Error when processing event
KeyNotFoundException(msg:production-topology-2-1468745167-stormconf.ser)
        at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)
        at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:239)
        at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:271)
        at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:300)
...
       at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
        at org.apache.storm.daemon.nimbus$read_storm_conf_as_nimbus.invoke(nimbus.clj:548)
        at org.apache.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:555)
        at org.apache.storm.daemon.nimbus$mk_assignments$iter__9205__9209$fn__9210.invoke(nimbus.clj:912)
...
        at org.apache.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:911)
        at clojure.lang.RestFn.invoke(RestFn.java:410)
        at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781$fn__9782.invoke(nimbus.clj:2216)
        at org.apache.storm.daemon.nimbus$fn__9769$exec_fn__1363__auto____9770$fn__9781.invoke(nimbus.clj:2215)
        at org.apache.storm.timer$schedule_recurring$this__1732.invoke(timer.clj:105)
        at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:50)
        at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)
...
2016-07-17 08:47:49.408 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
        at clojure.lang.RestFn.invoke(RestFn.java:423)
        at org.apache.storm.daemon.nimbus$nimbus_data$fn__8727.invoke(nimbus.clj:205)
        at org.apache.storm.timer$mk_timer$fn__1715$fn__1716.invoke(timer.clj:71)
        at org.apache.storm.timer$mk_timer$fn__1715.invoke(timer.clj:42)
        at clojure.lang.AFn.run(AFn.java:22)
        at java.lang.Thread.run(Thread.java:745)
2016-07-17 08:47:49.410 o.a.s.d.nimbus [INFO] Shutting down master
{code}
"
STORM-1976,Storm Nimbus H/A has issue on cleaning corrupted topologies,"In the following scenario storm-ha runs into issues:
1. Kill a non-leader nimbus
2. Submit a topology
3. Bring up the non-leader nimbus

After step-3 expectation is that the non-leader nimbus will download topology jar. Instead it cleans up the topology.

{code}
2016-07-12 07:11:09.511 o.a.s.c.zookeeper-state-factory [WARN] Received event ::none: with disconnected Reader Zookeeper.
2016-07-12 07:11:09.587 o.a.s.zookeeper [INFO] Queued up for leader lock.
2016-07-12 07:11:09.608 o.a.s.d.nimbus [INFO] Corrupt topology JoinedNonLeaderNimbusTriesToDownloadTopologyCode-2-1468307239 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2016-07-12 07:11:09.932 o.a.h.m.s.s.StormTimelineMetricsReporter [INFO] Preparing Storm Metrics Reporter
2016-07-12 07:11:09.946 o.a.s.d.m.MetricsUtils [INFO] Using statistics reporter plugin:org.apache.storm.daemon.metrics.reporters.JmxPreparableReporter
{code}"
STORM-1975,Support default value for KafkaBolt,"Support acks , key and value serializer for kafka bolt"
STORM-1974,Using System.lineSeparator to replacement write a new line,Using System.lineSeparator to replacement write a new line . It will write message in once and reduce writer synchroniz .
STORM-1972,Storm throws java.lang.ClassNotFoundException on Bolt class,"I'm trying to debug very simple topology (1 spout 2 bolts)

public class JoinerTopologyTest {

public static void main(String[] args) throws IOException {
    Config conf = new Config();
    conf.setNumWorkers(5);
    conf.setDebug(true);

    TopologyBuilder builder = new TopologyBuilder();
    builder.setSpout(""SPOUT-1"",new MySpout(),1);
    builder.setBolt(""BOLT-1"",new Bolt1(), 3)
            .shuffleGrouping(""SPOUT-1"");
    builder.setBolt(""JOINER"", new JoinerBolt(),1)
            .shuffleGrouping(""BOLT-1"")
            .shuffleGrouping(""SPOUT-1"",""str1"");

    final LocalCluster cluster = new LocalCluster();
    cluster.submitTopology(""TOPO1"",conf,builder.createTopology());


    System.in.read();

    cluster.shutdown();
}
}

But when i run it from InteliJ IDEA i get:

java.lang.RuntimeException: java.lang.ClassNotFoundException: com.pixonic.zephyr.compaction.tests.Bolt1 at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:181) ~[storm-core-1.0.1.jar:1.0.1] at org.apache.storm.utils.Utils.getSetComponentObject(Utils.java:430) ~[storm-core-1.0.1.jar:1.0.1]
and

[Thread-15] ERROR o.a.s.d.worker - Error on initialization of server mk-worker java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.storm.daemon.acker at org.apache.storm.utils.Utils.javaDeserialize(Utils.java:181) ~[storm-core-1.0.1.jar:1.0.1] at org.apache.storm.utils.Utils.getSetComponentObject(Utils.java:430) ~[storm-core-1.0.1.jar:1.0.1]
but same topology runs well in Cluster mode. PS in my pom.xml in debug mode i have:

    <dependency>
        <groupId>org.apache.storm</groupId>
        <artifactId>storm-core</artifactId>
        <version>1.0.1</version>
    </dependency>

project to reproduce bug: https://github.com/holinov/storm-101-localcluster/tree/master
"
STORM-1971,HDFS Timed Synchronous Policy,When the data need to be wrote to HDFS is not very large in quantity . We need a timed synchronous policy to flush cached date into HDFS periodically.
STORM-1967,Kafka 0.8 Incompatible with Storm 1.0.1,"Had a Storm Cluster deployed and functioning with storm 0.9.5. Updated the cluster and the topology to 1.0.1, leaving the same kafka version (0.8). Kafka Spout would not function, threw Kafka Buffer Underflow exceptions.

Updating to Kafka 0.9 fixed this issue, but I'm told I shouldn't have had to."
STORM-1966,Expand metric having Map type as value into multiple metrics based on entries,"We're introducing ""metrics filter"" (STORM-1700) into Storm 1.1.0, which can give a control of volume and kinds of metrics to users.

After playing with metrics, I found that most of built-in metrics in Storm (core and storm-kafka) are having Map as value which have been expected to be populated from Metrics Consumer. Since filter resides on metrics consumer bolt (not injected to metrics consumer) filter cannot know how metrics are populated, thus can't filter out some of populated metrics.

For example, let's say we have metric which name is 'A' and value is \{""B"": 1, ""C"": 2\}. For now we can't filter out 'A.C' and keep only 'A.B' since filter even doesn't know 'A' will be changed to 'A.B' and 'A.C'.

Since well-known metrics consumer (like storm-graphite) already supports populating metrics from one level map of value, I'd like to support this from Storm side and apply filter to populated metrics."
STORM-1964,Unexpected behavior when using count window together with timestamp extraction,"I launched a topology applying a tumbling count window of size 2 (watermark interval 200ms, lag 1s) with the following input (timestamp,value):

{noformat}
(10,10)
(10,20)
(11,30)
(12,40)
(12,50)
(12,60)
(12,70)
(13,80)
(14,90)
(15,100)
{noformat}

And I got these windows as output:

{noformat}
[(10,10), (10,20)]
[(12,60), (12,70)]
[(12,60), (12,70)]    // why (60, 70) twice?
[(13,80), (14,90)]
{noformat}


I would expect something like:

{noformat}
[(10,10), (10,20)]
[(11,30), (12,40)]
[(12,50), (12,60)]
[(12,70), (13,80)]
[(14,90), (15,100)]
{noformat}


It seems like that timestamp extraction and count windows does not fit each other."
STORM-1963,Replace Put add with addColumn,"HBase Put add() have deprecated , replace add() with addColumn()"
STORM-1962,python storm integration does not run on python 3,"This impacts other versions too (all of them), but fixing it is probably not that critical.

It is printing and exception handling that needs to be updated and can still maintain compatibility with 2.6 python."
STORM-1959,KafkaPartitionOffsetLag.java does not have license,"RAT is failing  external/storm-kafka-monitor/src/main/java/org/apache/storm/kafka/monitor/KafkaPartitionOffsetLag.java

Looks like this was introduced as a part of STORM-1950"
STORM-1958,storm-config.cmd doesn't handle spaces in JAVA_HOME,"We are currently upgrading the version of Storm we use in our environment to 1.0.1 (from 0.10.0). We have discovered that Storm does not start properly as JAVA_HOME has a space in it.

Investigating this, I have found that the main problem (in this case) seems to be in storm-config.cmd. In 0.10.0, line 128 contained:

{code}
set STORM_OPTS=-Dstorm.options= -Dstorm.home=%STORM_HOME% -Djava.library.path=%JAVA_LIBRARY_PATH%
{code}

The equivalent line in 1.0.1 (line 136) has:

{code}
set STORM_OPTS=%STORM_OPTS% -Dstorm.home=%STORM_HOME% -Djava.library.path=%JAVA_LIBRARY_PATH%;%JAVA_HOME%\bin;%JAVA_HOME%\lib;%JAVA_HOME%\jre\bin;%JAVA_HOME%\jre\lib
{code}

If JAVA_HOME has a space in it (as is frequently the case on Windows due to Java by default being installed under Program Files) this breaks the subsequent JVM command line.

This is an out of the box blocker to running Storm on Windows in commons configurations. I have not raised this as a blocker issue however, as there is a simple fix/workaround. We have changed storm-config.cmd in our local copy to add quotes around the java.library.path option:

{code}
set STORM_OPTS=%STORM_OPTS% -Dstorm.home=%STORM_HOME% ""-Djava.library.path=%JAVA_LIBRARY_PATH%;%JAVA_HOME%\bin;%JAVA_HOME%\lib;%JAVA_HOME%\jre\bin;%JAVA_HOME%\jre\lib""
{code}"
STORM-1957,Support Storm JDBC batch insert,"Batch insert support execute grouped SQL a batch and submit into one call . It can reduce the amount of communication , improving performance."
STORM-1956,Disable Backpressure by default,"Some of the context on this is captured in STORM-1949 
In short.. wait for BP mechanism to mature some more and be production ready before we enable by default."
STORM-1952,Keeping topology code for supervisor until topology got killed,"It's based on review comment from [~sriharsha].
https://github.com/apache/storm/pull/1528/files#r69152524
Please feel free to change reporter if you would like to.

In supervisor we're removing topology code when assignments for that supervisor has gone.
But there's valid scenario to need to keep the topology code though assignments for that supervisor is none, for example, rebalancing.

So it would be better for supervisor to keep topology code until topology has been killed (and all topology workers assigned to that supervisor are also killed)."
STORM-1950,"Change response json of ""Topology Lag"" REST API to keyed by spoutId, topic, partition","From code review for STORM-1945, there's an idea to change JSON response of ""Topology Lag"" API to keyed by topic, partition number.

https://github.com/apache/storm/pull/1541#issuecomment-230983140

I think also make result keyed by spout id would be good.
Here's sample JSON of output after this issue is resolved.

{code}
{
   ""spout1"":{
      ""spoutId"":""spout1"",
      ""spoutType"":""KAFKA"",
      ""spoutLagResult"":{
         ""topic"":{
            ""partition0"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            },
            ""partition2"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            }
         },
         ""topic2"":{
            ""partition0"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            },
            ""partition2"":{
               ""consumerCommittedOffset"":1175610,
               ""logHeadOffset"":5634192,
               ""lag"":4458582
            }
         }
      }
   }
}
{code}"
STORM-1945,Internal Server Error shown on topology page for topology using KafkaSpout,"When opening topology page which uses old storm-kafka Spout, page shows Internal Server Error on bottom side.
And REST API /api/v1/topology/:topology/lag also shows Internal Server Error.
Its errorMessage is describing NPE, but stack track gives less help since it's already within catch statement so we can't trace why origin exception is thrown by only looking at errorMessage. And there's no error message on ui log file."
STORM-1943,Support loading properties from a file,Support load properties from a file include config's argument . 
STORM-1942,Extra closing div tag in topology.html,Extra </div> in topology.html causing styling to be strage. Appears to have been introduced in STORM-1136.
STORM-1941,Nimbus discovery can fail when zookeeper reconnect happens.,"When zookeeper reconnect happens, nimbus registry can be deleted though nimbus is alive.

Below is zookeeper node for nimbus registry.

{code}
get /storm/nimbuses/<host>:6627
?f`d``??????M?-?-.?/??5??/H?+.IL???ON??``b`?|???^^???????
?'h?g?g?g?g
t-?,[??Q
cZxid = 0x4000005ae
ctime = Fri Jul 01 11:43:51 UTC 2016
mZxid = 0x4000005ae
mtime = Fri Jul 01 11:43:51 UTC 2016
pZxid = 0x4000005ae
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x255a62e310c0005
dataLength = 98
numChildren = 0
{code}

{code}
get /storm/nimbuses/<host>:6627
?f`d``??????M?-?-.?/??5??/H?+.IL???ON??``b`?|???^^???????
?'h?g?g?g?g
t-?,[??Q
cZxid = 0x4000005ae
ctime = Fri Jul 01 11:43:51 UTC 2016
mZxid = 0x50000000e
mtime = Fri Jul 01 11:46:08 UTC 2016
pZxid = 0x4000005ae
cversion = 0
dataVersion = 1
aclVersion = 0
ephemeralOwner = 0x255a62e310c0005
dataLength = 98
numChildren = 0
{code}

Below is transaction log for that node.
{code}
7/1/16 11:43:51 AM UTC session 0x255a62e310c0005 cxid 0xd zxid 0x4000005ae create '/storm/nimbuses/<host>:6627,#1fffffff8b80000000ffffffe36660646060ffffff90ffffffcfffffffcaffffffc9ffffffccffffffd54dffffffcc2dffffffd62d2effffffc92fffffffcaffffffd535ffffffd2ffffffcb2f48ffffffcd2b2e494cffffffceffffffceffffffc94f4effffffccffffffe160606260ffffff907cffffffccffffffc1ffffffc01c5e165effffffceffffffc4ffffffc0ffffffc2ffffffc0ffffffcdffffffc0affffffd42768ffffffa867ffffffa067ffffffa867ffffffa467affffffa4d742dffffff8c2c1805b14ffffffc2ffffffaf51000,v{s{31,s{'world,'anyone}}},T,10

7/1/16 11:46:08 AM UTC session 0x355a647bd8c0000 cxid 0x3 zxid 0x50000000e setData '/storm/nimbuses/<host>:6627,#1fffffff8b80000000ffffffe36660646060ffffff90ffffffcfffffffcaffffffc9ffffffccffffffd54dffffffcc2dffffffd62d2effffffc92fffffffcaffffffd535ffffffd2ffffffcb2f48ffffffcd2b2e494cffffffceffffffceffffffc94f4effffffccffffffe160606260ffffff907cffffffccffffffc1ffffffc01c5e165effffffceffffffc4ffffffc0ffffffc2ffffffc0ffffffcdffffffc0affffffd42768ffffffa867ffffffa067ffffffa867ffffffa467affffffa4d742dffffff8c2c1805b14ffffffc2ffffffaf51000,1
{code}

Please take a look at ctime, mtime, and ephemeralOwner.
Ephemeral owner session was already closed from nimbus side but there's possible for node to be not deleted immediately, so new session doesn't create new node but set the value to ephemeral node for other session which is already closed.
*And eventually that node is deleted although session 0x355a647bd8c0000 is alive.*

{code}
2016-07-01 11:45:05.675 o.a.s.s.o.a.z.ClientCnxn [DEBUG] Disconnecting client for session: 0x255a62e310c0005
2016-07-01 11:45:05.675 o.a.s.s.o.a.z.ZooKeeper [INFO] Session: 0x255a62e310c0005 closed
{code}

We can delete the node first and set ephemeral node when reconnect event handler is called."
STORM-1940,Storm Topo is auto re-balance after ZK RECONNECTED,"I have a Topo with 2 workers at 2 Vm, while ZK RECONNECTED, Storm Topo will be auto-reblance. 
The log show NodeExists for /meta/712285. I guess it cause by: After reconnect successfully, TridentSpoutCoordinator create this node again, but this node is already created before the reconnect.
 Can we check if node exist first? Or not throw this exception to make whole Topo re-balance. 
{code}
06-29 05:54:37.515 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] shade.org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae1, negotiated timeout = 10000
06-29 05:54:37.515 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]-EventThread] apache.curator.framework.state.ConnectionStateManager [INFO] State change: RECONNECTED
06-29 05:54:37.519 [Thread-133-spout-DataKafkaSpout1466801942228-executor[154 154]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae5, negotiated timeout = 10000
06-29 05:54:37.519 [Thread-133-spout-DataKafkaSpout1466801942228-executor[154 154]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.524 [Thread-25-spout-DataKafkaSpout1466801942228-executor[156 156]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae4, negotiated timeout = 10000
06-29 05:54:37.524 [Thread-25-spout-DataKafkaSpout1466801942228-executor[156 156]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.528 [main-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] shade.org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7b556f0cc3a40896, negotiated timeout = 10000
06-29 05:54:37.528 [main-EventThread] apache.curator.framework.state.ConnectionStateManager [INFO] State change: RECONNECTED
06-29 05:54:37.528 [Thread-149-spout-DataKafkaSpout1466801942228-executor[160 160]-SendThread(ip-10-9-255-26.us-west-2.compute.internal:2181)] org.apache.zookeeper.ClientCnxn [INFO] Session establishment complete on server ip-10-9-255-26.us-west-2.compute.internal/10.9.255.26:2181, sessionid = 0x7a556eeee8c70ae3, negotiated timeout = 10000
06-29 05:54:37.528 [Thread-149-spout-DataKafkaSpout1466801942228-executor[160 160]-EventThread] org.I0Itec.zkclient.ZkClient [INFO] zookeeper state changed (SyncConnected)
06-29 05:54:37.536 [Thread-151-$spoutcoord-spout-DataKafkaSpout1466801942228-executor[4 4]] org.apache.storm.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80]
Caused by: java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:119) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.overrideState(RotatingTransactionalState.java:52) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:71) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more
Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /meta/712285
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:721) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:704) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:701) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:477) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:467) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.forPath(TransactionalState.java:83) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:95) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:115) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.state.RotatingTransactionalState.overrideState(RotatingTransactionalState.java:52) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:71) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more
{code}"
STORM-1939,Frequent InterruptedException raised by ShellBoltMessageQueue.poll,"We've recently started testing out Storm 1.0.1 on a beta cluster we have setup, and we've noticed that one of our topologies frequently crashes with the following stack trace:

{code:java}
java.lang.InterruptedException 
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2017) 
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2095) 
    at org.apache.storm.utils.ShellBoltMessageQueue.poll(ShellBoltMessageQueue.java:104) 
    at org.apache.storm.task.ShellBolt$BoltWriterRunnable.run(ShellBolt.java:383) 
    at java.lang.Thread.run(Thread.java:745)
{code}

We're using a lot of Python components with streamparse 3.0.0.dev3 and are using the [MessagePackSerializer that was originally from pyleus|https://github.com/YelpArchive/pyleus/blob/develop/topology_builder/src/main/java/com/yelp/pyleus/serializer/MessagePackSerializer.java] with all the instances of ""backtype"" replaced with ""org.apache"".

Aside from the frequent bolt deaths from these exceptions, things seem to work, so I'm not sure what's going on here."
STORM-1937,trident topologies WindowTridentProcessor cause NullPointerException when using windowing,"I'm working with trident and try to use windows support, under the local model is fine, but in distributed mode we got the following excepiton(I can reliably reproduce this issue):

java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:418) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$fn__7966$fn__8019.invoke(executor.clj:847) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.util$async_loop$fn__625.invoke(util.clj:484) [storm-core-1.0.1.jar:1.0.1]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:744) [?:1.7.0_51]
Caused by: java.lang.NullPointerException
	at org.apache.storm.trident.windowing.WindowTridentProcessor.finishBatch(WindowTridentProcessor.java:167) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.planner.SubtopologyBolt.finishBatch(SubtopologyBolt.java:151) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.TridentBoltExecutor.finishBatch(TridentBoltExecutor.java:266) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.TridentBoltExecutor.checkFinish(TridentBoltExecutor.java:299) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:378) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$fn__7953$tuple_action_fn__7955.invoke(executor.clj:728) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.daemon.executor$mk_task_receiver$fn__7874.invoke(executor.clj:461) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.disruptor$clojure_handler$reify__7390.onEvent(disruptor.clj:40) ~[storm-core-1.0.1.jar:1.0.1]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:439) ~[storm-core-1.0.1.jar:1.0.1]
	... 6 more"
STORM-1934,Race condition between sync-supervisor and sync-processes raises several strange issues,"There're some strange issues including STORM-1933 and others (which I will file an issue soon) which are related to race condition in supervisor.

As I mentioned to STORM-1933, basically sync-supervisor relies on zk assignment, and sync-processes relies on local assignment and local workers directory, but in fact sync-supervisor also access local state and take some actions which affects sync-processes. And also Satish left the comment to STORM-1933 describing other issue related to race condition and idea to fix this which is same page on me.

"
STORM-1933,Intermittent test failure on test-multiple-active-storms-multiple-supervisors for supervisor-test ,"test-multiple-active-storms-multiple-supervisors is failing with fairly high chance. I've run unit test of 1.x branch 3 times and met this issue, and users report FileNotFound issue on supervisor which seems to be related to this.

I have log file so I'll attach once issue is created."
STORM-1931,Share mapper and selector in Storm-Kafka,Storm Kafka's mapper and selector and Storm Kafka trident's mapper and selector are the same . I try to merge them into one .
STORM-1930,Kafka New Client API - Support for Topic Wildcards,
STORM-1929,Check when create topology,"Add some check when create topology .

1. Spout and Bolt id shouldn't conflict

2. createTopology's spout and bolt set shouldn't empty ."
STORM-1927,Upgrade Jetty and Ring,"Jetty 7 is EOL , upgrade to Jetty 9 & Ring could also support it."
STORM-1926,Upgrade Jetty and Ring,Jetty 7 is EOL so we should upgrade to Jetty 9
STORM-1925,Nimbus fails to start in secure mode ,"We are noticing a failure in secure cluster as nimbus failed to start
2016-06-23 06:43:48.874 o.a.s.d.nimbus [ERROR] Error when processing event
java.lang.NullPointerException
	at org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer.permit(SimpleACLAuthorizer.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:1047)
	at org.apache.storm.daemon.nimbus$check_authorization_BANG_.invoke(nimbus.clj:1051)
	at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__11183.getClusterInfo(nimbus.clj:1772)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
	at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:313)
	at org.apache.storm.daemon.nimbus$send_cluster_metrics_to_executors.invoke(nimbus.clj:1393)
	at org.apache.storm.daemon.nimbus$fn__11394$exec_fn__3529__auto____11395$fn__11421.invoke(nimbus.clj:2254)
	at org.apache.storm.timer$schedule_recurring$this__2156.invoke(timer.clj:105)
	at org.apache.storm.timer$mk_timer$fn__2139$fn__2140.invoke(timer.clj:50)
	at org.apache.storm.timer$mk_timer$fn__2139.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)
2016-06-23 06:43:48.877 o.a.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.nimbus$nimbus_data$fn__10332.invoke(nimbus.clj:205)
	at org.apache.storm.timer$mk_timer$fn__2139$fn__2140.invoke(timer.clj:71)
	at org.apache.storm.timer$mk_timer$fn__2139.invoke(timer.clj:42)
	at clojure.lang.AFn.run(AFn.java:22)
	at java.lang.Thread.run(Thread.java:745)"
STORM-1924,Add a config file parameter to HDFS test topologies ,We need to add an ability to specify config parameters via an YAML for the test cases.
STORM-1923,Storm site page not found ,[DaemonMetrics/Monitoring|http://storm.apache.org/releases/1.0.1/storm-metrics-profiling-internal-actions.html]  Not found 
STORM-1921,Update parallelism_hint date type to integer,update TopologyBuilder's parallelism_hint date type from Number to int
STORM-1920,version of parent pom for storm-kafka-monitor is set 1.0.2-SNAPSHOT in master branch,"Recent Travis CI builds for pull requests are all failed due to this.

https://travis-ci.org/apache/storm/jobs/139371381

Unfortunately it should be hard to find with dev. environment since many of us ran ""mvn install"" on Storm 1.0.x-branch which eventually installed Storm 1.0.2 jar. That's why we shouldn't ignore CI bad sign."
STORM-1915,Supervisor keeps restarting forever,"While submitting a topology to a 20 node 40 worker strong cluster, the supervisor keeps throwing errors and keeps restarting the workers it is supervising.

For this reason the topology never starts, instead it keeps dancing by reassigning the bolts and spouts forever.

I'd love to attach the logs here but I can't find any upload button in the JIRA form.

The error basically says:
{code}
2016-06-18 12:04:26.589 o.a.s.config [WARN] Failed to get worker user for . #error {
 :cause /home/fogetti/downloads/apache-storm-1.0.1/storm-local/workers-users (Is a directory)
 :via
 [{:type java.io.FileNotFoundException
   :message /home/fogetti/downloads/apache-storm-1.0.1/storm-local/workers-users (Is a directory)
   :at [java.io.FileInputStream open0 FileInputStream.java -2]}]
 :trace
 [[java.io.FileInputStream open0 FileInputStream.java -2]
  [java.io.FileInputStream open FileInputStream.java 195]
  [java.io.FileInputStream <init> FileInputStream.java 138]
  [clojure.java.io$fn__9189 invoke io.clj 229]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9201 invoke io.clj 258]
  [clojure.java.io$fn__9102$G__9095__9109 invoke io.clj 69]
  [clojure.java.io$fn__9163 invoke io.clj 165]
  [clojure.java.io$fn__9115$G__9091__9122 invoke io.clj 69]
  [clojure.java.io$reader doInvoke io.clj 102]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [clojure.lang.AFn applyToHelper AFn.java 154]
  [clojure.lang.RestFn applyTo RestFn.java 132]
  [clojure.core$apply invoke core.clj 632]
  [clojure.core$slurp doInvoke core.clj 6653]
  [clojure.lang.RestFn invoke RestFn.java 410]
  [org.apache.storm.config$get_worker_user invoke config.clj 239]
  [org.apache.storm.daemon.supervisor$shutdown_worker invoke supervisor.clj 281]
  [org.apache.storm.daemon.supervisor$kill_existing_workers_with_change_in_components invoke supervisor.clj 536]
  [org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9078 invoke supervisor.clj 595]
  [org.apache.storm.event$event_manager$fn__8630 invoke event.clj 40]
  [clojure.lang.AFn run AFn.java 22]
  [java.lang.Thread run Thread.java 745]]}
{code}"
STORM-1914,Storm Kafka Field Topic Selector,Support field name and field index to select which kafka topic will used as a downstream .
STORM-1911,Inconsistency of timestamp between IMetricsConsumer and IClusterMetricsConsumer,"There's inconsistency of timestamp between IMetricsConsumer and IClusterMetricsConsumer: former is seconds and latter is milliseconds.

Since both of two are representing it to timestamp, and IMetricsConsumer is already being used, we need to change timestamp of IClusterMetricsConsumer to seconds."
STORM-1910,One topology can't use hdfs spout to read from two locations,"The hdfs uri is passed using config:
{code}
    conf.put(Configs.HDFS_URI, hdfsUri);
{code}
I see two problems with this approach:
1. If someone wants to used two hdfsUri in same or different spouts - then that does not seem feasible.
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/examples/storm-starter/src/jvm/storm/starter/HdfsSpoutTopology.java#L117-L117
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L331-L331
{code}
    if ( !conf.containsKey(Configs.SOURCE_DIR) ) {
      LOG.error(Configs.SOURCE_DIR + "" setting is required"");
      throw new RuntimeException(Configs.SOURCE_DIR + "" setting is required"");
    }
    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );
{code}
2. It does not fail fast i.e. at the time of topology submissing. We can fail fast if the hdfs path is invalid or credentials/permissions are not ok. Such errors at this time can only be detected at runtime by looking at the worker logs.
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L297-L297"
STORM-1909,Fix errors in HDFS Spout documentation,There is mistake in the example provided for the HDFS spout. The source/archive/bad directory settings should not have the 'hdfs://' portion.
STORM-1908,Support Storm HBase ZooKeeper Config ,"Support Storm HBase ZooKeeper Config . About zk host , port and parent ."
STORM-1907,PartitionedTridentSpoutExecutor has incompatible types that cause ClassCastException,"This bug added during the refactor that occurred in this [pull request|https://github.com/apache/storm/pull/683]

This change causes a ClassCastException cannot cast ArrayList to Integer to occur when running org.apache.storm.starter.trident.TridentKafkaWordCount"
STORM-1906,Window count/length of zero should be disallowed,"This is related to: STORM-1841.
I see that we are still allowing zero for window size & window length. This should be disallowed."
STORM-1905,Javascript API does not support reportError,"The javascript API does not support reportError, the python one for example does:
https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py#L135"
STORM-1904,Storm shell display summary info,"display storm cluster summary info about cluster , nimbus ,supervisor and history . "
STORM-1900,Log configuration with logback,"I am trying to use logback in my project and am getting this message:
{quote}
2016-06-14 16:55:56.945 STDERR [INFO] SLF4J: Class path contains multiple SLF4J bindings.
2016-06-14 16:55:56.997 STDERR [INFO] SLF4J: Found binding in [jar:file:/opt/apache-storm-0.10.1/lib/log4j-slf4j-impl-2.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: Found binding in [jar:file:/srv/storm/supervisor/stormdist/MatchIdentifiers-675106f-1465923070-5-1465923118/stormjar.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2016-06-14 16:55:56.998 STDERR [INFO] SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
2016-06-14 16:56:03.647 STDERR [INFO] 2016-06-14 16:56:03,641 ERROR Logger contains an invalid element or attribute ""appender""{quote}

The solution slf4j suggests is to exclude slf4j from the dependency which in this case would be storm. But since storm includes slf4j in the classpath when starting the topology this does not work.


Is there a known way to fix this?
"
STORM-1899,Release HBase connection when topology shutdown,Storm HBase Client release connection when topology shutdown.
STORM-1898, MAX_BATCH_SIZE_CONF not working in Trident storm Spout,"Ideally Trident process should process tuples in Batch.

ex > https://github.com/apache/storm/blob/ab66003c18fe4f8c0926b3219408b735b2ce2adf/storm-core/src/jvm/org/apache/storm/trident/spout/RichSpoutBatchExecutor.java

there is a parameter called  MAX_BATCH_SIZE_CONF which limits the size of the batch.

This parameter is not present in TridentKafkaEmitter.

https://github.com/apache/storm/blob/1.x-branch/external/storm-kafka/src/jvm/org/apache/storm/kafka/trident/TridentKafkaEmitter.java


---------------
Problem is that now everytime the topology restarts it just fetches all messages from Kafka.

Could any one throw some idea on it, I certainly feel its a bug.
"
STORM-1897,"re-pattern file-path-separator problem with windows, breaks logviewer","re-pattern file-path-separator will cause errors in windows...

more specifically 
----
java.util.regex.PatternSyntaxException: Unexpected internal error near index 1
\

----

the ""\"" character (windows separator) is a reserved one in regex.  

hence the logviewer will not work on a windows machine...

A potential fix could be to define:

(defn file-path-separator-regex []
    (if on-windows?
        (re-pattern ""\\\\"")
        (re-pattern file-path-separator)))

and use this instead of ""re-pattern file-path-separator"" in logviewer.clj and config.clj

"
STORM-1895,blobstore replication-factor argument,storm command line argument --repl-fctr have update to  replication-factor and update the document .
STORM-1894,storm-redis does not support a Redis cluster for state saving,"Working with Storm and stateful bolts we noticed that it is not possible to work with a Redis cluster at the moment. The problem is, that storm-redis requires that the configuration is of type {{JedisPoolConfig}} which only allows defining one host. If the given Redis instance is configured as a Redis cluster exceptions of type {{JedisMovedDataException}} might occur.
The configuration via {{JedisClusterConfig}} seems to provide support for a Redis cluster, but {{RedisKeyValueStateProvider}} does not handle it."
STORM-1892,class org.apache.storm.hdfs.spout.TextFileReader should be public,
STORM-1890,"Employ cache-busting method to ensure newly deployed UI forces browsers to refetch scripts, templates, and CSS","Currently we don't employ cache busting techniques in the Storm UI while fetching script.js, CSS and templates. Ring is providing the Last-Modified header, but browsers implement a heuristic to when they deem a resource stale (https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.2.4). This means that as the Last-Modified for a resource is further away in the past, the longer the browsers are going to wait until they refetch. It looks like 10% padding is common, so if script.js was last modified 100 days ago, the browser will not fetch it until 10 days after the time it was cached.

An easy approach is to add a url parameter to allow for cache busting whenever storm is packaged (mvn package). A more complicated method is versioning the files (we'd need to specify them in the pom.xml individually using the assembly plugin, unless we use some other plugin). The first method is (was?) considered less effective, since some CDNs/browsers can decide not to cache the query parameter.

I'd like to go with the simpler method, unless there are strong opinions to changing file names (this means we need to specify files in the assembly pom.xml). Also, going this route we don't need any new plugins, and the assembly build can just be changed to export a variable. We would modify calls to include a value that changes on mvn package:

{code}
<script src=""/js/script.js?_ts=${timestamp}"" type=""text/javascript""></script> 
{code}

instead of:

{code}
<script src=""/js/script.js"" type=""text/javascript""></script>
{code}

Where $\{timestamp\} will be replaced at assembly time by maven. This would be the time when the assembly build started.

The templates will also have the extra parameter. I think providing this to ajaxSetup will do the trick. For example:

{code}
$.ajaxSetup({ data: {""_ts"" : ""${timestamp}""}});
{code}"
STORM-1889,Datatables error message displayed when viewing UI,"Updating to storm 1.0.1, running on Windows 7, I receive error messages from Datatables.
This occurs on the Topology Summary as well as the Component Summary for a spout/bolt

Example error: DataTables warning: table id=executor-stats-table - Requested unknown parameter '9' for row 0. For more information about this error, please see http://datatables.net/tn/4

If I edit index.html to remove the type: num targets, the errors go away.

For example.

  $.getJSON(""/api/v1/topology/summary"",function(response,status,jqXHR) {
      $.get(""/templates/index-page-template.html"", function(template) {
          topologySummary.append(Mustache.render($(template).filter(""#topology-summary-template"").html(),response));
          //name, owner, status, uptime, num workers, num executors, num tasks, replication count, assigned total mem, assigned total cpu, scheduler info
          dtAutoPage(""#topology-summary-table"", {
            columnDefs: [
              //{type: ""num"", targets: [4, 5, 6, 7, 8, 9]},
			  {type: ""num"", targets: []},
              {type: ""time-str"", targets: [3]}
            ]
          });
          $('#topology-summary [data-toggle=""tooltip""]').tooltip();
      });




"
STORM-1887,Help message for the set_log_level command does not have the topology name parameter,"The help message for the set_log_level command does not have the topology-name parameter:

$ storm help set_log_level

    Dynamically change topology log levels

    Syntax: [storm set_log_level -l [logger name]=[log level][:optional timeout] -r [logger name]
    where log level is one of:
        ALL, TRACE, DEBUG, INFO, WARN, ERROR, FATAL, OFF
    and timeout is integer seconds.
(...)

If you don't pass a `topology-name` as last parameter, you'll get the following error:

$ storm set_log_level -l com.myapp=WARN
Running: /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/usr/local/Cellar/storm/1.0.1/libexec -Dstorm.log.dir=/usr/local/Cellar/storm/1.0.1/libexec/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /usr/local/Cellar/storm/1.0.1/libexec/lib/asm-5.0.3.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/clojure-1.7.0.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/disruptor-3.3.2.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/kryo-3.0.3.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-api-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-core-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-over-slf4j-1.6.6.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/log4j-slf4j-impl-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/minlog-1.3.0.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/objenesis-2.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/reflectasm-1.10.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/servlet-api-2.5.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/slf4j-api-1.7.7.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/storm-core-1.0.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/lib/storm-rename-hack-1.0.1.jar:/usr/local/Cellar/storm/1.0.1/libexec/conf:/usr/local/Cellar/storm/1.0.1/libexec/bin org.apache.storm.command.set_log_level -l com.myapp=WARN
2670 [main] INFO  o.a.s.c.set-log-level - Sent log config LogConfig(named_logger_level:{com.myapp=LogLevel(action:UPDATE, target_log_level:WARN, reset_log_level_timeout_secs:0)}) for topology
Exception in thread ""main"" java.lang.IllegalArgumentException: No matching field found: IllegalArgumentException for class java.lang.String
    at clojure.lang.Reflector.getInstanceField(Reflector.java:271)
    at clojure.lang.Reflector.invokeNoArgInstanceMember(Reflector.java:315)
    at org.apache.storm.command.set_log_level$get_storm_id.invoke(set_log_level.clj:31)
    at org.apache.storm.command.set_log_level$_main.doInvoke(set_log_level.clj:75)
    at clojure.lang.RestFn.applyTo(RestFn.java:137)
    at org.apache.storm.command.set_log_level.main(Unknown Source)
(...)

I opened a PR to fix this: https://github.com/apache/storm/pull/1463"
STORM-1883,FileReader extends Closeable Interface,use Closeable Interface to decorate FileReader to support close()
STORM-1882,Expose TextFileReader public,"[Storm HDFS Using|https://github.com/apache/storm/tree/master/external/storm-hdfs#usage-1]

TextFileReader is package-private .

Should make TextFileReader a public class to expose it to user and hdfs spout. "
STORM-1881,storm-redis is missing dependant libraries in distribution,"Despite the documentation on http://storm.apache.org/releases/1.0.1/State-checkpointing.html it is not enough to simply copy {{storm-redis-*.jar}} to {{extlib}} to get the {{RedisKeyValueStateProvider}} working. Depending jedis and apache-commons-pool2 jars are missing and must be copied by hand to get it working. Else one is greeted with exception stack traces like:

{code}
Caused by: java.lang.ClassNotFoundException: org.apache.commons.pool2.impl.GenericObjectPoolConfig
{code}

or

{code}
Caused by: java.lang.ClassNotFoundException: redis.clients.jedis.JedisPoolConfig
{code}

Copying {{commons-pool2-2.4.2.jar}} and {{jedis-2.8.1.jar}} from hand to {{extlib}} solves the issue.
It might be better to create a ""fat"" jar of {{storm-redis-*.jar}} or provide documentation, which libraries have to be made available."
STORM-1880,Support  EXISTS Command Storm-Redis,add exists command in storm-redis LookupBolt
STORM-1879,Supervisor may not shut down workers cleanly,"We've run into a strange issue with a zombie worker process. It looks like the worker pid file somehow got deleted without the worker process shutting down. This causes the supervisor to try repeatedly to kill the worker unsuccessfully, and means multiple workers may be assigned to the same port. The worker root folder sticks around because the worker is still heartbeating to it.

It may or may not be related that we've seen Nimbus occasionally enter an infinite loop of printing logs similar to the below.

{code}
2016-05-19 14:55:14.196 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormconf.ser
2016-05-19 14:55:14.210 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormcode.ser
2016-05-19 14:55:14.218 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormconf.ser
2016-05-19 14:55:14.256 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormcode.ser
2016-05-19 14:55:14.273 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormcode.ser
2016-05-19 14:55:14.316 o.a.s.b.BlobStoreUtils [ERROR] Could not update the blob with keyZendeskTicketTopology-5-1463647641-stormconf.ser
{code}

Which continues until Nimbus is rebooted. We also see repeating blocks similar to the logs below.

{code}
2016-06-02 07:45:03.656 o.a.s.d.nimbus [INFO] Cleaning up ZendeskTicketTopology-127-1464780171
2016-06-02 07:45:04.132 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:ZendeskTicketTopology-127-1464780171-stormjar.jar)
2016-06-02 07:45:04.144 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:ZendeskTicketTopology-127-1464780171-stormconf.ser)
2016-06-02 07:45:04.155 o.a.s.d.nimbus [INFO] ExceptionKeyNotFoundException(msg:ZendeskTicketTopology-127-1464780171-stormcode.ser)
{code}"
STORM-1878,Flux does not handle stateful bolts,"We noticed that it is not possible at the moment to create a topology with Flux which contains stateful bolts (based on IStatefulBolt). Those bolts will not be instantiated.

Pull request upcoming."
STORM-1875,Separate Jedis/JedisCluster Config,Separate Jedis / JedisCluster to provide full operations for each environment to users . 
STORM-1874,Update Logger access permissions,Update Log access permissions from public to private . 
STORM-1872,Storm Redis connection release ,Strom Redis connect should be release when topology shutdown .
STORM-1871,Storm Alluxio integrate,[alluxio|http://alluxio.org/] is a memory speed virtual distributed storage system.Alluxio’s memory-centric architecture enables data access orders of magnitude faster than existing solutions.
STORM-1867,Storm Topology Freezes,"My storm topology freezes after few hours. I have a KafkaSpout, a parser bolt and an aggregator bolt.

After few hours I see KafkaSpout is not emitting anything. I checked the STORM UI, there were no errors. I also checked the kafka topic via kafka-console-consumer, I was able to consume but kafkaspout was emitting nothing. 

I checked worker.log and there were no issues.

Following are the last few lines of worker.log. 


2016-05-26 03:45:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:45:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:45:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:45:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:47:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:47:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:47:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:47:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:49:16.252 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:49:16.259 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:49:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:49:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:51:16.254 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:51:16.259 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:51:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:51:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:53:16.254 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:53:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:53:16.260 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:53:16.260 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:55:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:55:16.265 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:55:16.265 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:55:16.266 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:57:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:57:16.261 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:57:16.261 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:57:16.262 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
2016-05-26 03:59:16.255 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Refreshing partition manager connections
2016-05-26 03:59:16.260 o.a.s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{topic=l7v_flows, partitionMap={0=qagg2-storm:6667, 1=qagg3-storm:6667, 2=qagg1-storm:6667}}
2016-05-26 03:59:16.261 o.a.s.k.KafkaUtils [INFO] Task [3/3] assigned [Partition{host=qagg1-storm:6667, topic=l7v_flows, partition=2}]
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Deleted partition managers: []
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] New partition managers: []
2016-05-26 03:59:16.261 o.a.s.k.ZkCoordinator [INFO] Task [3/3] Finished refreshing
"
STORM-1862,Flux ShellSpout and ShellBolt can't emit to named streams,See pull request: https://github.com/apache/storm/pull/1426
STORM-1861,Storm submit command returns exit code of 0 even when it fails,"When we run the command twice:
{code}
/usr/hdp/current/storm-client/bin/storm jar /usr/hdp/current/storm-client/contrib/storm-starter/storm-starter-topologies-1.0.1.2.5.0.0-415.jar org.apache.storm.starter.ExclamationTopology mytopology
{code}
The second command failed but the exit code was 0 instead of 1. This is regression."
STORM-1860,Simple UI Announcements Notification,"As a user of a storm cluster, I would like to be informed of current or scheduled maintenance on the cluster, so that I can plan my work & debug more efficiently.


This could be a really simple UI change.  The UI could look for a special file on the local disk. If found, it would display a notification near the top of the UI with the contents of this file.

It would be useful to announce an ongoing rolling upgrade of the cluster, scheduled future downtime, current known incidents, or other, even more general communications from administrators to users."
STORM-1857,HDFS Topologies should support config parameters from a YAML file,HDFS test topologies should support specifying a yaml file for passing in config parameters (for eg. security related)
STORM-1854,Trident transactional spouts are broken in 1.0.x,"In the process of upgrading our Storm code from 0.10.0 to 1.0.0, I've run into an issue with TransactionalTridentKafkaSpout. When running one of our topologies I'm getting the following exception:

{code}
Caused by: java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.lang.Integer
	at org.apache.storm.trident.spout.PartitionedTridentSpoutExecutor$Coordinator.initializeTransaction(PartitionedTridentSpoutExecutor.java:55) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.trident.spout.PartitionedTridentSpoutExecutor$Coordinator.initializeTransaction(PartitionedTridentSpoutExecutor.java:43) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.trident.spout.TridentSpoutCoordinator.execute(TridentSpoutCoordinator.java:70) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:50) ~[storm-core-1.0.0.jar:1.0.0]
{code}

The issue appears to be caused by a change in PartitionedTridentSpoutExecutor between the two versions, specifically this method:

1.0.0 - https://github.com/apache/storm/blob/v1.0.0/storm-core/src/jvm/org/apache/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L51

{code}
public Integer initializeTransaction(long txid, Integer prevMetadata, Integer currMetadata) {
    if(currMetadata!=null) {
        return currMetadata;
    } else {
        return _coordinator.getPartitionsForBatch();            
    }
}
{code}

0.10.0 - https://github.com/apache/storm/blob/v0.10.0/storm-core/src/jvm/storm/trident/spout/PartitionedTridentSpoutExecutor.java#L51

{code}
public Object initializeTransaction(long txid, Object prevMetadata, Object currMetadata) {
    if(currMetadata!=null) {
        return currMetadata;
    } else {
        return _coordinator.getPartitionsForBatch();            
    }
}
{code}

This was introduced by: https://github.com/apache/storm/commit/9e4c3df17ffbc737210e606d3d8a9cdae8f86634

TransactionalTridentKafkaSpout uses List<GlobalPartitionInformation> for its metadata. Generally, transactional spouts should have metadata that is more complex than just an Integer. OpaquePartitionedTridentSpoutExecutor uses Object for its metadata and correctly handles the metadata used by OpaqueTridentKafkaSpout (List<GlobalPartitionInformation>).

It looks like reverting the metadata type for transactional spouts in PartitionedTridentSpoutExecutor should work, but I haven't tried this yet."
STORM-1853,Deserialization issues in Utils.javaDeserialize(),"Utils.javaDeserialize uses a custom implementation of ObjectInputStream which can be inconsistent with ObjectOutputStream class used in javaSerialize. 

One resulting issue e.g
http://mail-archives.apache.org/mod_mbox/storm-user/201605.mbox/%3CCAGOmOn0RJ33RZ0tj-%3DoKPkqNunkS4Q2Nx0ZSKHcNAMPLowuc3w%40mail.gmail.com%3E"
STORM-1852,Investigate using a ScheduledThreadPoolExecutor instead of a Timer+Executor in Disruptor Queue,"We use a DisruptorQueue with a Timer and a thread pool executor to schedule tasks periodically. 

https://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java

This could potentially be replaced with a Scheduled Thread Pool executor to perform the job in a more first class way, http://stackoverflow.com/questions/409932/java-timer-vs-executorservice.
"
STORM-1851,Nimbus impersonation authorizer in defaults.yaml causes issues in secure mode,"  ""nimbus.impersonation.authorizer"" is set to ""ImpersonationAuthorizer"" by default and this causes issues when a user tries to submit topology as a different user in secure mode since the ""nimbus.impersonation.acl"" configuration is not set by default. Users need to set nimbus.impersonation.acl first before they can submit topology as a user other than ""storm"" in secure mode.

Removing this config allows users to submit topologies as any user in secure mode by default. Users can set up impersonation by providing both authorizer and the acls in storm.yaml."
STORM-1849,HDFSFileTopology should take 3rd argument as topology name,Calling the HDFSFileTopology with the optional topology name parameter fails as the correct parameter is not passed in to the submit topology method.
STORM-1848,NotSerializableException when using storm-kafka spout with event logging,"We deployed a topology to multiple workers and set topology.eventlogger.executors to be identical to the number of workers. When the debug button in Storm UI is pressed, the spout will start throwing NotSerializableExceptions.

java.lang.RuntimeException: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.kafka.PartitionManager$KafkaMessageId at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:452) at (snip)

KafkaMessageId should be made serializable."
STORM-1846,Need a smoke test for the generated Python code,"This was inspired by STORM-1842. For that issue, simply attempting to import ttypes.py would've detected the error. I'm guessing there is no such test. It would be good to add one in order to help avoid problems like that in the future."
STORM-1845,use UTF-8 instead of default encoding,
STORM-1844,Some tests are flaky due to low timeout,"We saw a few test failures likely due to low timeouts https://github.com/apache/storm/pull/1417#issuecomment-219545865. The timeouts for these tests should be increased, and the test mentioned by [~kishorvpatil] should reflect STORM_TEST_TIMEOUT_MS."
STORM-1842,Forward references in storm.thrift cause tooling issues,"In recent versions of Storm (starting with version 1.0.0, I believe), storm.thrift uses several types before they are declared:

* HBPulse
* HBRecords
* HBNodes

These types are used in the definition of HBMessageData.

This causes issues with downstream tools. For example, generating Python wrappers for the Storm Thrift types creates a module that will not import successfully.

Would it be possible to reorder this code to define the types before using them? This appears to be a simple change."
STORM-1841,Address a few minor issues in windowing and doc,"1. Do not accept negative values for window length or sliding interval in BaseWindowedBolt
2. Added static factories for Count and Duration for ease of use.
3. Explicitly call out when the first window is evaluated for sliding windows in the windowing doc."
STORM-1837,"Running local clusters without simulating time breaks Testing.completeTopology, and may cause message loss","Since https://github.com/apache/storm/pull/810 it is no longer possible to call Testing.completeTopology when time is not simulating, because a call to advance-cluster-time is made from the function, which calls Time/advanceTime. advance-cluster-time should only be called if time is simulating.

Since https://github.com/apache/storm/pull/830 a local cluster run without time simulation may lose messages. When a worker emits messages for a worker that hasn't started yet, the message is lost. This can happen because spouts may start emitting before all workers have started, when time simulation is disabled. Local clusters usually run without message timeouts, so this will make tests relying on Testing.withLocalCluster flaky.

The problem is that there are no longer any queues to store messages for workers that haven't started yet. See https://github.com/apache/storm/pull/830/files#diff-c6ff4208ef84c7a5a1a6b8b6bd1f7d19R104. A queue should be added for messages for workers that haven't registered a receive callback yet."
STORM-1835,add lock info in thread dump,
STORM-1834,Documentation How to Generate Certificates For Local Testing SSL Setup,"This patch must be cherry picked in 0.10.x-branch, 1.x-branch, and master"
STORM-1832,Consistently slow metrics consumer triggers backpressure which will be never back to normal,"If metrics consumer is too slow to keep up processing received messages, eventually backpressure is triggered. Spout throttles, but metrics messages are not throttled so it could be chance for topology to be never back to normal.

While STORM-1698 can resolve this issue, I made this issue to clarify this behavior and mark this as 'bug'.

If you'd like to see its symptom, please refer here: https://github.com/apache/storm/pull/1324#issuecomment-218962460"
STORM-1779,Incorrect bfs-ordering in DefaultResourceAwareStrategy,"I have designed a simple diamond topology: spout, N parallel processingBolts, joinBolt.
https://github.com/smirnp/storm-benchmark/blob/1.0.0/src/storm/benchmark/DiamondTopology.java

For 6-components topopoly the breadth-first search (bfs method in DefaultResourceAwareStrategy.java:323) returns 13 components, which is obviously incorrect for further scheduling (it fails on memory limits trying to schedule already scheduled tasks in priorityToExecutorMap). 

I have reimplemented bfs with the comparison with the original-one (https://github.com/smirnp/storm-benchmark/blob/1.0.0/bfs.txt) 
Please check it and fix the bug in next releases.

P.S. storm.yaml user for scheduling is here: 
https://github.com/smirnp/storm-benchmark/blob/1.0.0/storm.yaml

Best regards,
Thanks"
STORM-1777,Backport KafkaBolt from Storm 1.0.0 to 0.10.0 to support properties configuration method,"Storm KafkaBolt requires Map object supplied under the kafka.broker.properties which makes the configuration values global i.e. there can be only one set of KafkaBolts in a given topology.

This issue has already been resolved in Storm 1.0.0 by introducing the withProducerProperties method. 

This ticket is to back port those changes to 0.10.0 so that we can use the community version rather than project specific back ports."
STORM-1776,Error when processing event java.io.FileNotFoundException,"when i am trying to start storm cluster , i got the below error.
24628 [Thread-10] INFO  o.a.s.d.supervisor - Copying resources at jar:file:/D:/.m2/repository/org/apache/storm/flux-core/1.0.0/flux-core-1.0.0.jar!/resources to C:\Users\MAGESH~1\AppData\Local\Temp\1991ad4f-acbc-44ed-b07b-24643381f7a0\supervisor\stormdist\test-1-1462884218\resources
24631 [Thread-10] ERROR o.a.s.event - Error when processing event
java.io.FileNotFoundException: Source 'file:\D:\.m2\repository\org\apache\storm\flux-core\1.0.0\flux-core-1.0.0.jar!\resources' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$fn__9351.invoke(supervisor.clj:1194) ~[storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070$fn__9088.invoke(supervisor.clj:582) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070.invoke(supervisor.clj:581) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:40) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
24638 [Thread-8] INFO  o.a.s.d.supervisor - Copying resources at jar:file:/D:/.m2/repository/org/apache/storm/flux-core/1.0.0/flux-core-1.0.0.jar!/resources to C:\Users\MAGESH~1\AppData\Local\Temp\f4009546-87fe-4885-a0b0-cb775e8b784f\supervisor\stormdist\test-1-1462884218\resources
24638 [Thread-8] ERROR o.a.s.event - Error when processing event
java.io.FileNotFoundException: Source 'file:\D:\.m2\repository\org\apache\storm\flux-core\1.0.0\flux-core-1.0.0.jar!\resources' does not exist
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.shade.org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$fn__9351.invoke(supervisor.clj:1194) ~[storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070$fn__9088.invoke(supervisor.clj:582) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9070.invoke(supervisor.clj:581) ~[storm-core-1.0.0.jar:1.0.0]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:40) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
24642 [Thread-10] ERROR o.a.s.util - Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:48) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
24643 [Thread-8] ERROR o.a.s.util - Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.event$event_manager$fn__8622.invoke(event.clj:48) [storm-core-1.0.0.jar:1.0.0]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65]
"
STORM-1775,Generate StormParserImpl before maven building instead of in packaging time,"Just like genthrift.sh genrates the generated thrift-about java source files. I think it is better generate StormParserImpl.java before maven execution.

It can reduce the complexity of storm-sql."
STORM-1774,Generate StormParserImpl before maven execution ,"Just like genthrift.sh genrates the generated thrift-about java source files. I think it is better to generate StormParserImpl.java before maven execution.

It can reduce the complexity of storm-sql."
STORM-1773,Utils.javaDeserialize() doesn't work with primitive types,"It's based on reporting from user@.
Please refer [here|http://mail-archives.apache.org/mod_mbox/storm-user/201605.mbox/%3CCAHObvqq81mVqPEi5e7C+i0r7u1hv4TqxT0Tn38dC1Exd6yUuxw@mail.gmail.com%3E] for details.

STORM-1040 (#919) replaces ObjectInputStream with ClassLoaderObjectInputStream while deserializing. But unfortunately ClassLoaderObjectInputStream has a bug which cannot handle primitive types. Please refer [IO-378|https://issues.apache.org/jira/browse/IO-378].

Fortunately IO-378 was included at latest release 2.5, so we would be OK to just upgrade the version of commons-io."
STORM-1770,Pluggable status storage in storm-Kafka,"Storm-Kafka spout store consumer offset in zookeeper . When a lot topology running in cluster , zookeeper snapshot frequently . This ticket is to make status storage pluggable , support store in zookeeper (default) and Redis ."
STORM-1768,Reduce noise in worker logs,"Much of the time when debugging a problematic topology, worker logs are filled with many statements like netty connection issues, so that more relevant information is harder to find.

Perhaps we can identify some of the noisiest logs and change their log level to DEBUG or TRACE."
STORM-1767,metrics log entries are being appended to root log,"Current setup of metrics logger ( {{storm/log4j2/worker.xml}}) uses fully qualified name of the class where the logging is happening from i.e `org.apache.storm.metric.LoggingMetricsConsumer`, which is problematic and does not achieve the original intent as stated by the METRICS appender defined in {{storm/log4j2/worker.xml}}.

Currently the metrics logger created explicitly by using the name above:
{{LoggerFactory.getLogger(""org.apache.storm.metric.LoggingMetricsConsumer"")}} or implicitly from within the {{LoggingMetricsConsumer}} by calling {{LoggerFactory.getLogger(LoggingMetricsConsumer.class)}} will be logging to **root** logger.

This happens because logger names use Java namespaces and as such create hierarchies. 

The solution is to name metrics logger outside of {{org.apache.storm.*}} namespace which is what is happening for all other non-root loggers defined within the {{storm/log4j2/worker.xml}} file. 

This will also mean a code change to {{LoggingMetricsConsumer}} class itself for it to use the logger with an explicit name matching the name defined in the {{worker.xml}} file.

The fix is easy. 
"
STORM-1762,storm pom.xml miss some dependency,"Hi, all:
    I download storm v0.9.5 from github, and want to compile and install it with maven. Unfortunately, I got the following errors when I use command ""mvn clean install -DskipTests -X"":
    ...
   [ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.18:compile (compile-clojure) on project storm-core: Clojure failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.18:compile (compile-clojure) on project storm-core: Clojure failed.
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Clojure failed.
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:453)
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:367)
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:344)
        at com.theoryinpractise.clojure.ClojureCompilerMojo.execute(ClojureCompilerMojo.java:47)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 19 more

    The trace log only shows something wrong in the process of clojure compile. To slove this problem I debug and found the deep cause: the storm-core pom.xml missing some dependency package, and I have made a path file."
STORM-1761,Storm-Solr Example Throws ArrayIndexOutOfBoundsException in Remote Cluster Mode,
STORM-1759,Viewing logs from the Storm UI doesn't work in dockerized environment,"I run the Storm using the following docker-compose.yml

{code}
version: '2'

services:
    zookeeper:
        image: jplock/zookeeper:3.4.8
        restart: always

    nimbus:
        image: 31z4/storm:1.0.0
        command: nimbus -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - zookeeper
        restart: always
        ports:
            - 6627:6627
        volumes:
            - logs:/logs

    supervisor:
        image: 31z4/storm:1.0.0
        command: supervisor -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
        restart: always
        volumes:
            - logs:/logs

    logviewer:
        image: 31z4/storm:1.0.0
        command: logviewer -c storm.log.dir=""/logs""
        restart: always
        ports:
            - 8000:8000
        volumes:
            - logs:/logs

    ui:
        image: 31z4/storm:1.0.0
        command: ui -c storm.log.dir=""/logs"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
            - logviewer
        restart: always
        ports:
            - 8080:8080
        volumes:
            - logs:/log

volumes:
    logs: {}
{code}

And opening the logs from the Storm UI doesn't work because all links are pointing to different container ids as hosts.

I guess adding an ability to explicitly specify the logviewer host in the storm.yaml would solve the issue."
STORM-1758,Distributed log search doesn't work in dockerized environment,"I run the Storm using the following docker-compose.yml

{code}
version: '2'

services:
    zookeeper:
        image: jplock/zookeeper:3.4.8
        restart: always

    nimbus:
        image: 31z4/storm:1.0.0
        command: nimbus -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - zookeeper
        restart: always
        ports:
            - 6627:6627
        volumes:
            - logs:/logs

    supervisor:
        image: 31z4/storm:1.0.0
        command: supervisor -c storm.log.dir=""/logs"" -c storm.zookeeper.servers=""[\""zookeeper\""]"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
        restart: always
        volumes:
            - logs:/logs

    logviewer:
        image: 31z4/storm:1.0.0
        command: logviewer -c storm.log.dir=""/logs""
        restart: always
        ports:
            - 8000:8000
        volumes:
            - logs:/logs

    ui:
        image: 31z4/storm:1.0.0
        command: ui -c storm.log.dir=""/logs"" -c nimbus.host=""nimbus""
        depends_on:
            - nimbus
            - logviewer
        restart: always
        ports:
            - 8080:8080
        volumes:
            - logs:/log

volumes:
    logs: {}
{code}

And distributed log search doesn't work because the Storm UI tries to access the logviewer by supervisor's container id as a host.

Here is the list of running containers
{code}
$ docker ps
7ae118eef55c        31z4/storm:1.0.0         ""bin/storm ui -c stor""   5 minutes ago       Up 5 minutes               0.0.0.0:8080->8080/tcp         stormdocker_ui_1
5a9101dc2510        31z4/storm:1.0.0         ""bin/storm supervisor""   5 minutes ago       Up 5 minutes                                              stormdocker_supervisor_1
4d954096cf18        31z4/storm:1.0.0         ""bin/storm nimbus -c ""   5 minutes ago       Up 5 minutes               0.0.0.0:6627->6627/tcp         stormdocker_nimbus_1
070080342c4f        31z4/storm:1.0.0         ""bin/storm logviewer ""   5 minutes ago       Up 5 minutes               0.0.0.0:8000->8000/tcp         stormdocker_logviewer_1
8650786a13cc        jplock/zookeeper:3.4.8   ""/opt/zookeeper/bin/z""   5 minutes ago       Up 5 minutes               2181/tcp, 2888/tcp, 3888/tcp   stormdocker_zookeeper_1
{code}

And here is what the Storm UI requests
{code}
curl 'http://5a9101dc2510:8000/search/topology-1-1462284216%2F6701%2Fworker.log?search-string=split&num-matches=1' -H 'Accept: application/json, text/javascript, */*; q=0.01' -H 'Referer: http://192.168.99.100:8080/search_result.html?search=split&id=topology-1-1462284216&count=1' -H 'Origin: http://192.168.99.100:8080' -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.86 Safari/537.36' --compressed
{code}

I guess adding an ability to explicitly specify the logviewer host in the storm.yaml would solve the issue."
STORM-1755,Revert the kafka client version upgrade in storm-kafka module,storm-kafka module does not use any feature of new API. The newer kafka client (0.9.x) is not backward compatible with 0.8.x brokers and upgrading storm-kafka version in topology could break the currently running topologies. 
STORM-1750,Report-error-and-die may not kill the worker,"The report-error-and-die function in executor.clj calls report-error, which can throw exceptions if Curator runs into any kind of trouble while registering the error. I suspect this may happen with network errors, but it can also happen if two executors for the same component throw exceptions at the same time and no errors have been registered for the component previously. This is because both calls to report-error-and-die update the lastErrorPath, and ZkStateStorage set_data doesn't catch the potential NodeExistsException that may be thrown from the create call.

If an exception is thrown from report-error, the suicide-fn is never called, and the worker keeps running sans the crashed executor."
STORM-1748,Better Trident Spout Names,"As a user, I would like to see more meaningful component names for spouts in trident topologies, so that I can make more sense of the topology functionality.

For example, automatically generated names like ""$mastercoord-bg2,3,4,5"" are not so useful for understanding."
STORM-1744,Missing javadoc in Trident code,"Some or most of the core Trident classes don't have javadoc. It makes it really difficult to use.

http://storm.apache.org/releases/2.0.0-SNAPSHOT/javadocs/index.html

Examples:
TridentTopologyBuilder
IBackingMap"
STORM-1743,consumerAutoCommitMode not work in storm-kafka-client,"poll() checks the numUncommittedOffsets, however, it's never updated under consumerAutoCommitMode."
STORM-1741,storm-env.sh unconditionally sets JAVA_HOME,"STORM-1706 introduced storm-env.sh to the binary distribution. Before 1.0.1 we weren’t including `storm-env.sh`. That file does the following:

export JAVA_HOME=${JAVA_HOME}

Which, if JAVA_HOME is not set, will set it, but leave it empty. So the clojure code in supervisor.clj `if (nil? java-home)` will evaluate to false and we’ll end up with `/bin/java` as the java command."
STORM-1735,Nimbus logs that replication was not reached when min-replication-count was reached exactly,"When waiting for replication during topology submission, Nimbus logs whether replication succeeded within the timeout or not. The check for whether to log that it timed out or succeeded is off by one."
STORM-1734,ClassNotFound error when running storm-starter topologies in local mode,"The problem is described here: http://stackoverflow.com/questions/36649346/classnotfound-error-when-running-storm-starter-topologies-in-local-mode

Please help!"
STORM-1733,Logs from bin/storm are lost because stdout and stderr are not flushed,"bin/storm.py emits the following crucial information that is lost because we don't flush the stdout before exec.

{code}
2016-04-25T08:23:43.17141 Running: java -server -Dstorm.options= -Dstorm.home= -Xmx1024m -Dlogfile.name=nimbus.log -Dlogback.configurationFile=logback/cluster.xml  backtype.storm.ui.core.nimbus
{code}

Observed Environment:
{code}
OS: CentOS release 6.5 
Kernel: 2.6.32-431.el6.x86_64
Python version: Python 2.7.2
{code}

For example, I using runit to start storm components like nimbus, ui, etc and the problem is applicable to all the components and in all the cases, I am not seeing logs that are emitted by bin/storm before {{os.execvp}} is called to actually launch the component. 

Please note that in cases where stdout and stderr is terminal, the stdout and stderr are always flushed and the bug is not applicable."
STORM-1732,Resources are deleted when worker dies,"*Lets say a worker has been started by the supervisor*

2016-04-26 16:11:48.716 [o.a.s.d.supervisor] INFO: Launching worker with assignment {:storm-id ""Lightning-1-1461683473"", :executors [[12 12] [54 54] [42 42] [24 24] [18 18] [6 6] [48 48] [30 30] [36 36]], :resources #object[org.apache.storm.generated.WorkerResources 0x10bac1e4 ""WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)""]} for this supervisor 477ae22e-1a2b-4ea3-afd5-cb969f25e732 on port 6700 with id a5d51626-6e9f-4614-9ebb-a6263c140ca2
2016-04-26 16:11:48.727 [o.a.s.d.supervisor] INFO: Launching worker with command: 'C:\LightningDeployment\Java\bin\java' '-cp' ........ 
2016-04-26 16:11:48.910 [o.a.s.config] INFO: SET worker-user a5d51626-6e9f-4614-9ebb-a6263c140ca2 LIGHTNINGVM14$


*note this bit is is new for storm 1.0.0*


2016-04-26 16:11:49.405 [o.a.s.d.supervisor] INFO: Creating symlinks for worker-id: a5d51626-6e9f-4614-9ebb-a6263c140ca2 storm-id: Lightning-1-1461683473 to its port artifacts directory
2016-04-26 16:11:50.251 [o.a.s.d.supervisor] INFO: Creating symlinks for worker-id: a5d51626-6e9f-4614-9ebb-a6263c140ca2 storm-id: Lightning-1-1461683473 for files(1): (""resources"")



*When a worker dies we correctly see some clean up and a new worker started...*



2016-04-26 16:15:35.520 [o.a.s.d.supervisor] INFO: Worker Process a5d51626-6e9f-4614-9ebb-a6263c140ca2 exited with code: 20
2016-04-26 16:15:39.674 [o.a.s.d.supervisor] INFO: Worker Process a5d51626-6e9f-4614-9ebb-a6263c140ca2 has died!
2016-04-26 16:15:39.675 [o.a.s.d.supervisor] INFO: Shutting down and clearing state for id a5d51626-6e9f-4614-9ebb-a6263c140ca2. Current supervisor time: 1461683739. State: :timed-out, Heartbeat: {:time-secs 1461683734, :storm-id ""Lightning-1-1461683473"", :executors [[12 12] [54 54] [42 42] [24 24] [18 18] [6 6] [48 48] [30 30] [-1 -1] [36 36]], :port 6700}
2016-04-26 16:15:39.676 [o.a.s.d.supervisor] INFO: Shutting down 477ae22e-1a2b-4ea3-afd5-cb969f25e732:a5d51626-6e9f-4614-9ebb-a6263c140ca2
2016-04-26 16:15:39.676 [o.a.s.config] INFO: GET worker-user a5d51626-6e9f-4614-9ebb-a6263c140ca2
2016-04-26 16:15:39.677 [o.a.s.d.supervisor] INFO: Worker Process a5d51626-6e9f-4614-9ebb-a6263c140ca2 has died!
2016-04-26 16:15:39.681 [o.a.s.d.supervisor] INFO: Worker Process a5d51626-6e9f-4614-9ebb-a6263c140ca2 has died!
2016-04-26 16:15:39.857 [o.a.s.util] INFO: Error when trying to kill 1352. Process is probably already dead.
2016-04-26 16:15:39.955 [o.a.s.util] INFO: Error when trying to kill 2372. Process is probably already dead.
2016-04-26 16:15:40.009 [o.a.s.util] INFO: Error when trying to kill 4932. Process is probably already dead.
2016-04-26 16:15:40.009 [o.a.s.d.supervisor] INFO: Sleep 10 seconds for execution of cleanup threads on worker.
2016-04-26 16:15:49.677 [o.a.s.d.supervisor] INFO: Worker Process a5d51626-6e9f-4614-9ebb-a6263c140ca2 has died!
2016-04-26 16:15:49.679 [o.a.s.d.supervisor] INFO: Worker Process a5d51626-6e9f-4614-9ebb-a6263c140ca2 has died!
2016-04-26 16:15:50.056 [o.a.s.util] INFO: Error when trying to kill 1352. Process is probably already dead.
2016-04-26 16:15:50.119 [o.a.s.util] INFO: Error when trying to kill 2372. Process is probably already dead.
2016-04-26 16:15:50.175 [o.a.s.util] INFO: Error when trying to kill 4932. Process is probably already dead.
2016-04-26 16:15:50.257 [o.a.s.config] INFO: REMOVE worker-user a5d51626-6e9f-4614-9ebb-a6263c140ca2
2016-04-26 16:15:50.257 [o.a.s.d.supervisor] INFO: Shut down 477ae22e-1a2b-4ea3-afd5-cb969f25e732:a5d51626-6e9f-4614-9ebb-a6263c140ca2
2016-04-26 16:15:50.257 [o.a.s.d.supervisor] INFO: Launching worker with assignment {:storm-id ""Lightning-1-1461683473"", :executors [[12 12] [54 54] [42 42] [24 24] [18 18] [6 6] [48 48] [30 30] [36 36]], :resources #object[org.apache.storm.generated.WorkerResources 0x20e1ad4f ""WorkerResources(mem_on_heap:0.0, mem_off_heap:0.0, cpu:0.0)""]} for this supervisor 477ae22e-1a2b-4ea3-afd5-cb969f25e732 on port 6700 with id e413447b-c9ca-417d-8e55-e10dd0edc6a4


*When the worker has been cleaned up, it seems the folders that the symlinks are pointing to are also cleaned (this maybe a windows only problem)*

*This is bad as it deletes the contents of the ""resources"" directory and hence any multilang stuff that was in those directories*

*also I think STORM-876 introduced this problem*"
STORM-1731,Avoid looking up debug / backpressure enable flags within critical path,"While profiling the result of STORM-1729, I also found that there're many places in critical path which look up the value of flags which are not updated dynamically.
(""get from map"" is on top 5 from each spout / bolt thread.)

This should be fixed."
STORM-1730,LocalCluster#shutdown() does not terminate all storm threads/thread pools.,"When using the LocalCluster in test setup.  LocalCluster#shutdown() does not shutdown all executor services it starts.  In my test case, there is a single thread pool executor service that is not shutdown and not daemon.  This keeps the jvm alive when it is expected to terminate.

Please see attached test case.  In my example, thread pool 47 is not shutdown.  Naming here is conditional on threading."
STORM-1729,Get rid of reflections while recording stats,"I don't set affects version to 2.0.0 since it only occurs on Clojure.

{code}
(set! *warn-on-reflection* true)
(load-file ""src/clj/org/apache/storm/stats.clj"")
{code}

{quote}
Reflection warning, /Users/jlim/WorkArea/JavaProjects/storm/storm-core/src/clj/org/apache/storm/stats.clj:119:3 - call to method incBy can't be resolved (target class is unknown).
Reflection warning, /Users/jlim/WorkArea/JavaProjects/storm/storm-core/src/clj/org/apache/storm/stats.clj:123:3 - call to method incBy can't be resolved (target class is unknown).
Reflection warning, /Users/jlim/WorkArea/JavaProjects/storm/storm-core/src/clj/org/apache/storm/stats.clj:149:3 - call to method incBy can't be resolved (target class is unknown).
Reflection warning, /Users/jlim/WorkArea/JavaProjects/storm/storm-core/src/clj/org/apache/storm/stats.clj:150:3 - call to method record can't be resolved (target class is unknown).
Reflection warning, /Users/jlim/WorkArea/JavaProjects/storm/storm-core/src/clj/org/apache/storm/stats.clj:154:3 - call to method incBy can't be resolved (target class is unknown).
{quote}

https://github.com/apache/storm/blob/1.x-branch/storm-core/src/clj/org/apache/storm/stats.clj#L119

We expect there's no reflection since we give a type hint while calling, but it doesn't work.
(I don't know why it doesn't work. Does generic type matter?)
Anyway, defining them into let makes them avoid reflection.

Even though we sample while recording stats, it does hurt performance with fast & high-traffic components."
STORM-1728,TransactionalTridentKafkaSpout error,"In the PartitionedTridentSpoutExecutor.java, Integer is expected to return at line 52. However at line 55, it actually returns the List Class(the function is defined at line 48 in Coordinator.java), which throws a cast exception."
STORM-1725,Kafka Spout New Consumer API - KafkaSpoutRetryExponentialBackoff method should use HashMap instead of TreeMap not to throw Exception,"The method org.apache.storm.kafka.spout.KafkaSpoutRetryExponentialBackoff#retriableTopicPartitions currently uses TreeMap but TopicPartition does not implement Comparable. Since ordering is not important, HashMap is the most appropriate data structure here, and the one I meant to use to begin with.

The fix will replace TreeMap with HashMap.

This issue was found by @jianbzhou and credit should be given to him."
STORM-1723,Introduce ClusterMetricsConsumer,"NOTE: This issue is already discussed shortly. Please refer [here|http://mail-archives.apache.org/mod_mbox/storm-dev/201604.mbox/%3CCAF5108hDCcMKxLXKUYLReOoKkNNdgW2YudweR+mKr=1hLSL2Ew@mail.gmail.com%3E] for details.

This issue focuses to introduce ClusterMetricsConsumer and provide interface to let users plugin their consumers.

ClusterMetricsConsumers will be attached to Nimbus, and leader of Nimbus will push cluster related metrics to ClusterMetricsConsumer.

Requirements of ClusterMetricsConsumer are here:

- Only leader of Nimbus should publish cluster metrics to consumer.
- Nimbus shouldn't be affected by crashing or heavy latency on consumer.
- Consumer should have resilient when crashing or Nimbus should take care of."
STORM-1720,Support GEO in storm-redis,GEO is a new feature in redis 3.2 . It's useful in Geography calculate.
STORM-1719,Introduce REST API: Topology metric stats for stream,"Apache Storm has two REST APIs for showing topology stats, /api/v1/topology/:id and showing component stats, /api/v1/topology/:id/component/:component.

Both of APIs shows metrics aggregated by topology or each component. 
It helps determining traffics between component to component, which is good for many topologies. But if users use their own topology which utilizes lots of streams, they may want to see topology stats in detail - in point of stream's view.

We already have visualization API to show the traffics between stream to stream, but it's internal API, and contains only transferred, and it shows non-aggregated result so users need to aggregate theirselves."
STORM-1717,"Support Redis INRC , INRCBY and INRCBYFLOAT",to support redis incr incrby and incrbyfloat  
STORM-1716,Add some external Jedis pool config,add some jedis pool config 
STORM-1715,Jedis Default Host,Using Jedis Protocol.DEFAULT_HOST to replace DEFAULT_HOST
STORM-1714,StatefulBolts ends up as normal bolts while using TopologyBuilder.setBolt without parallelism,StatefulBolt inherits from IRichBolt which but the TopologyBuilder.setBolt overload is chosen based on the static type of the parameter causing issues. See if StatfulBolt can be refactored to not directly inherit from IRichBolt.
STORM-1711,Kerberos principals gets mixed up while using storm-hive,Storm-hive uses UserGroupInformation.loginUserFromKeytab which updates the static variable that stores current UGI.
STORM-1710,java.lang.ClassNotFoundException: backtype.storm.generated.LSSupervisorId,"2016-04-14 09:07:22.226 o.a.s.d.supervisor [ERROR] Error on initialization of server mk-supervisor
java.lang.RuntimeException: java.lang.ClassNotFoundException: backtype.storm.generated.LSSupervisorId
	at org.apache.storm.utils.LocalState.deserialize(LocalState.java:83)
	at org.apache.storm.utils.LocalState.get(LocalState.java:130)
	at org.apache.storm.local_state$ls_supervisor_id.invoke(local_state.clj:61)
	at org.apache.storm.daemon.supervisor$standalone_supervisor$reify__9646.prepare(supervisor.clj:1228)
	at org.apache.storm.daemon.supervisor$fn__9503$exec_fn__1826__auto____9504.invoke(supervisor.clj:781)
	at clojure.lang.AFn.applyToHelper(AFn.java:160)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at clojure.core$apply.invoke(core.clj:630)
	at org.apache.storm.daemon.supervisor$fn__9503$mk_supervisor__9548.doInvoke(supervisor.clj:779)
	at clojure.lang.RestFn.invoke(RestFn.java:436)
	at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1216)
	at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1249)
	at clojure.lang.AFn.applyToHelper(AFn.java:152)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at org.apache.storm.daemon.supervisor.main(Unknown Source)
Caused by: java.lang.ClassNotFoundException: backtype.storm.generated.LSSupervisorId
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:186)
	at org.apache.storm.utils.LocalState.deserialize(LocalState.java:78)
	... 14 more
2016-04-14 09:07:22.239 o.a.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)
	at clojure.lang.RestFn.invoke(RestFn.java:423)
	at org.apache.storm.daemon.supervisor$fn__9503$mk_supervisor__9548.doInvoke(supervisor.clj:779)
	at clojure.lang.RestFn.invoke(RestFn.java:436)
	at org.apache.storm.daemon.supervisor$_launch.invoke(supervisor.clj:1216)
	at org.apache.storm.daemon.supervisor$_main.invoke(supervisor.clj:1249)
	at clojure.lang.AFn.applyToHelper(AFn.java:152)
	at clojure.lang.AFn.applyTo(AFn.java:144)
	at org.apache.storm.daemon.supervisor.main(Unknown Source)"
STORM-1708,Replay the kafka messages when no failed tuples.,"We build topology with OpaqueTridentKafkaSpout,  the configuration is :
        conf.setMaxSpoutPending(2048); 
        conf.setMessageTimeoutSecs(120);

that cause messages replayed lot of times, and there is no failed tuples in storm ui. 

After we set :
        conf.setMaxSpoutPending(10); 
        conf.setMessageTimeoutSecs(30);
everyting works fine. 

It confuse a lot , i think it's a major bug.



 "
STORM-1706,Add storm-env.ini and RELEASE to storm-dist assembly,
STORM-1704,"When logviewer_search.html opens daemon file, next search always show no result","When searching keyword in /logviewer_search.html with daemon log file, is-daemon=yes parameter is gone so search shows no result."
STORM-1703,"In local mode, process is not shutting down clearly","Process is not shutting down clearly in local mode, but ‘Ctrl + C’ can terminate the process.

Will attach log file and jstack dump file."
STORM-1702,DirectGrouping ,"http://storm.apache.org/releases/0.10.0/index.html

The link to ""Direct Groupings"" (http://storm.apache.org/releases/0.10.0/Direct-groupings.html) is broken."
STORM-1700,Introduce 'whitelist' / 'blacklist' option to MetricsConsumer,"Storm provides various metrics by default, and so on some external modules (storm-kafka).

When we register MetricsConsumer, MetricsConsumer should handle all of metrics. If MetricsConsumer cannot keep up with these metrics, only way to keep up is increasing parallelism, which seems limited. Furthermore, some users don't want to care about some metrics since unintended metrics will fill external storage.

Though MetricsConsumer itself can filter metrics by name, it would be better to support filter by Storm side. It will reduce the redundant works for Storm community.

If we provide filter options, it would be great."
STORM-1698,Asynchronous MetricsConsumerBolt,"Currently MetricsConsumerBolt is delegating MetricsConsumer to handle data points via synchronous manner.

When MetricsConsumer cannot keep up, it will trigger backpressure when (queue size + overflow buffer size) reaches high watermark, which incurs slowing down the topology in result. 

Slowing down Itself is not a problem because that’s what backpressure is for. The actual problem is that backpressure only throttles spout, not metrics. If MetricsConsumerBolt cannot keep up with incoming tuples, backpressure never ends and topology just hangs. If we turn off backpressure, we have unbounded queue and worker could throw OOME eventually.

Making MetricsConsumerBolt asynchronous can resolve this issue. One downside of making it async is that it's hard to see that MetricsConsumerBolt is keeping up now. (capacity will be always around 0)
I don't have an idea for now but I think it's still better than current.

Before making consensus about huge change of metrics, I'd love to improve current metrics without breaking backward compatible manner. It could be applied to 1.x-branch, and even 0.10.x-branch."
STORM-1697,artifacts symlink not created ,"No artifacts symlink generated under worker's current directory. Gc log, jstack and heapdump will not be working.

2016-04-07 17:43:19.909 STDERR [INFO] Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file artifacts/gc.log due to No such file or directory
2016-04-07 17:43:19.913 STDERR [INFO]"
STORM-1696,Backpressure flag not sync if zookeeper connection errors,"When there is a zk exception happens during worker-backpressure!,
there is a bad state which can block the topology from running normally any more.

The root cause: in worker/mk-backpressure-handler
if the worker-backpressure! fails once due to zk connection exception,
next time when this method gets called by WordBackpressureThread, because (when (not= prev-backpressure-flag curr-backpressure-flag) will never be true, the remote zk node can not be synced with local state.

This also explains why we will not see any problem when testing in a stable (zk never fail) environment.

Solution is quite straightforward: first change the zk status, if succeeds, change local status.

This fixes the hidden bug and removes redundant flags in executor-data and worker-data (since we can get the executor status directly from the ""_throttleOn"" boolean in the DisruptorQueue)
"
STORM-1693,Negative counts in the UI for __metrics stream,Metrics reported by UI are not correct. I am seeing negative counts for output stats in the bolt. The same application code works fine on 0.9.6 version of storm. 
STORM-1689,set request headersize for logviewer,
STORM-1688,provide ParallismKillWorkerManager to shutdown workers  in parallel,
STORM-1687,Divide by zero exception in stats,"Since uptime can be 0, this will cause ArithmeticException: Divide by zero in compute-agg-capacity.

This will happen for both stats.clj in 1.x and StatsUtil.java in master (2.0).

{noformat}
java.lang.ArithmeticException: Divide by zero
at clojure.lang.Numbers.divide(Numbers.java:156)
at clojure.core$SLASH.invoke(core.clj:986)
at clojure.lang.AFn.applyToHelper(AFn.java:156)
at clojure.lang.RestFn.applyTo(RestFn.java:132)
at clojure.core$apply.invoke(core.clj:626)
at backtype.storm.util$div.doInvoke(util.clj:355)
at clojure.lang.RestFn.invoke(RestFn.java:423)
at backtype.storm.stats$compute_agg_capacity$fn__2249.invoke(stats.clj:409)
at backtype.storm.stats$compute_agg_capacity.invoke(stats.clj:404)
at backtype.storm.stats$agg_pre_merge_topo_page_bolt.invoke(stats.clj:555)
at backtype.storm.stats$agg_topo_exec_stats_STAR_.invoke(stats.clj:724)
at backtype.storm.stats$fn__2319.invoke(stats.clj:772)
at clojure.lang.MultiFn.invoke(MultiFn.java:241)
at clojure.lang.AFn.applyToHelper(AFn.java:165)
at clojure.lang.AFn.applyTo(AFn.java:144)
at clojure.core$apply.invoke(core.clj:628)
at clojure.core$partial$fn__4230.doInvoke(core.clj:2470)
at clojure.lang.RestFn.invoke(RestFn.java:421)
at clojure.core.protocols$fn__6086.invoke(protocols.clj:143)
at clojure.core.protocols$fn_6057$G6052_6066.invoke(protocols.clj:19)
at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31)
at clojure.core.protocols$fn__6078.invoke(protocols.clj:54)
at clojure.core.protocols$fn_6031$G6026_6044.invoke(protocols.clj:13)
at clojure.core$reduce.invoke(core.clj:6289)
at backtype.storm.stats$aggregate_topo_stats.invoke(stats.clj:854)
at backtype.storm.stats$agg_topo_execs_stats.invoke(stats.clj:1008)
at backtype.storm.daemon.nimbus$fn_5838$exec_fn1478auto$reify_5862.getTopologyPageInfo(nimbus.clj:1729)
at backtype.storm.generated.Nimbus$Processor$getTopologyPageInfo.getResult(Nimbus.java:3651)
at backtype.storm.generated.Nimbus$Processor$getTopologyPageInfo.getResult(Nimbus.java:3635)
at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:39)
at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:39)
at backtype.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:143)
at org.apache.thrift7.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)


{noformat}"
STORM-1683,UI Topology Visualization should not check sys streams by default,"New system streams (beginning with two underscores {{__}}) have been added since the visualization code was added, such as  __ack_reset_timeout  and  __eventlog. The visualization code currently presents stream check boxes as unchecked if they are ack-related streams and checked otherwise.

It seems that it should present only non-system streams as checked, so that the graph shows more relevant streams by default."
STORM-1682,Kafka spout can lose partitions,"The KafkaSpout can lose partitions for a period, or hang because getBrokersInfo (https://github.com/apache/storm/blob/master/external/storm-kafka/src/jvm/org/apache/storm/kafka/DynamicBrokersReader.java#L77) may get a NoNodeException if there is no broker info in Zookeeper corresponding to the leader id in Zookeeper. When this error occurs, the spout ignores the partition until the next time getBrokersInfo is called, which isn't until the next time the spout gets an exception on fetch. If the timing is really bad, it might ignore all the partitions and never restart.

As far as I'm aware, Kafka doesn't update leader and brokerinfo atomically, so it's possible to get unlucky and hit the NoNodeException when a broker has just died.

I have a few suggestions for dealing with this. 

getBrokerInfo could simply retry the inner loop over partitions if it gets the NoNodeException (probably with a limit and a short sleep between attempts). If it fails repeatedly, the spout should be crashed.

Alternatively the DynamicBrokersReader could instead lookup all brokers in Zookeeper, create a consumer and send a TopicMetadataRequest on it. The response contains the leader for each partition and host/port for the relevant brokers.

Edit: I noticed that the spout periodically refreshes the brokers info, so the issue isn't as bad as I thought. I still think this change has value, since it avoids the spout temporarily dropping a partition."
STORM-1681,Bug in scheduling cyclic topologies when scheduling with RAS,There is a bug in the bfs algorithm in RAS that does not correctly account for components already visited during the breadth first traveral
STORM-1677,"Test resource files (.log) are excluded from source distribution, which makes logviewer-test failing","While building RC1 of Apache Storm 1.0.0 from source code distribution, I found that test for logviewer-test in storm-core is failing. It was not intermittent.

After seeking the reason, I found files on src/dev directory is different from repo, "".log"" files are all excluded.

https://github.com/apache/storm/blob/master/storm-dist/source/src/main/assembly/source.xml

We excludes .log files in all directories which normally makes sense but not for now."
STORM-1676,NullPointerException while serializing ClusterWorkerHearbeat,"`Map<ExecutorInfo,ExecutorStats> executor_stats` had null value in the key which was causing NPE during serialization. "
STORM-1674,Idle KafkaSpout consumes more bandwidth than needed,"Discovered 30 megabits of traffic flowing between a set of KafkaSpouts
and our kafka servers even though no Kafka messages were moving.
Using the wireshark kafka dissector, we were able to see that
each FetchRequest had maxWait set to 10000
and minBytes set to 0. When binBytes is set to 0 the kafka server
responds immediately when there are no messages. In turn the KafkaSpout
polls without any delay causing a constant stream of FetchRequest/
FetchResponse messages. Using a non-KafkaSpout client had a similar
traffic pattern with two key differences
1) minBytes was 1
2) maxWait was 100
With these FetchRequest parameters and no messages flowing,
the kafka server delays the FetchResponse by 100 ms. This reduces
the network traffic from megabits to the low kilobits. It also
reduced the CPU utilization of our kafka server from 140% to 2%."
STORM-1673,log4j2/worker.xml refers old package of LoggerMetricsConsumer,"We changed package path from 'backtype.storm' to 'org.apache.storm'. Source codes seem to moved properly, but missed log4j2 configuration, so metric log is logged into worker log file, not metrics file.

It should be simple patch so I'd like to include this as 1.0.0. If we don't want to include any bugfixes I'm OK to remove epic."
STORM-1672,Stats not get class cast exception,"Component page in UI
{code}
2016-03-31 14:21:44.576 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map
        at org.apache.storm.stats.StatsUtil.filterSysStreams(StatsUtil.java:1696)
        at org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt(StatsUtil.java:240)
        at org.apache.storm.stats.StatsUtil.aggCompExecStats(StatsUtil.java:1130)
        at org.apache.storm.stats.StatsUtil.aggregateCompStats(StatsUtil.java:1108)
        at org.apache.storm.stats.StatsUtil.aggCompExecsStats(StatsUtil.java:1236)
        at org.apache.storm.daemon.nimbus$fn__3490$exec_fn__789__auto__$reify__3519.getComponentPageInfo(nimbus.clj:2130)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3826)
        at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3810)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
{code}"
STORM-1671,Enable logviewer to delete directory with no yaml file ,"For those old and dead worker directories, in some weird case, there is no yaml file in it. We should enable logviewer to delete them (any dir that has/ has no yaml file in it).."
STORM-1670,LocalState#get(String) can throw FileNotFoundException which results in not removing worker heartbeats and supervisor is kind of stuck and goes down after some time.,"LocalState#get(String) can throw FileNotFoundException which may result in supervisor.clj#sync-processes which stop assigning new workers/assignments etc and supervisor goes down later.

VersionedStore#mostRecentVersionPath() can return a file only with suffix of .version but the original file for a specific version may not have been there because .version suffix was not deleted but respective data file may have been deleted in earlier cleanups. "
STORM-1669,Fix SolrUpdateBolt flush bug,"SolrUpdateBolt is setting the default tick tuple interval in the prepare() method, which is not taking effect.
This issue is the same as https://issues.apache.org/jira/browse/STORM-1219 and https://issues.apache.org/jira/browse/STORM-1654."
STORM-1668,Flux silently fails while setting a non-existent property.,"Currently, if a yaml file has a property with a name that does not exist on the java topology component object then flux silently fails by logging a message and does not throw an exception. This needs to be changed so that flux throws the exception failing topology submission so that user can take corrective action."
STORM-1667,Log the IO Exception when deleting worker pid dir,"There is a race condition since shutdown-worker is called by both sync-processes and synchronize-supervisor in supervisor. One pid directory might have been already deleted when rmr-as-user tries to delete it. 
Therefore, we should not let the FileNotFoundException get thrown and crash the supervisor."
STORM-1666,Kill from the UI fails silently.,"{code}
2016-03-30 14:02:21.613 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.NullPointerException
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:26)
        at org.apache.storm.daemon.nimbus$fn__3070$exec_fn__2175__auto__$reify__3099$iter__3210__3214$fn__3215.invoke(nimbus.clj:1888)
        at clojure.lang.LazySeq.sval(LazySeq.java:40)
        at clojure.lang.LazySeq.seq(LazySeq.java:49)
        at clojure.lang.RT.seq(RT.java:507)
        at clojure.core$seq__4128.invoke(core.clj:137)
        at clojure.core$dorun.invoke(core.clj:3009)
        at clojure.core$doall.invoke(core.clj:3025)
        at org.apache.storm.daemon.nimbus$fn__3070$exec_fn__2175__auto__$reify__3099.getTopologyInfoWithOpts(nimbus.clj:1885)
        at org.apache.storm.generated.Nimbus$Processor$getTopologyInfoWithOpts.getResult(Nimbus.java:3774)
        at org.apache.storm.generated.Nimbus$Processor$getTopologyInfoWithOpts.getResult(Nimbus.java:3758)
        at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
        at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
        at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:744)
{code}"
STORM-1663,Clicking on an active topology from storm ui home page and then refreshing the page throws exception,"The exception thrown is:

org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getTopologyPageInfo(Nimbus.java:1243)
	at org.apache.storm.generated.Nimbus$Client.getTopologyPageInfo(Nimbus.java:1228)
	at org.apache.storm.ui.core$topology_page.invoke(core.clj:638)
	at org.apache.storm.ui.core$fn__3662.invoke(core.clj:987)
	at org.apache.storm.shade.compojure.core$make_route$fn__302.invoke(core.clj:93)
	at org.apache.storm.shade.compojure.core$if_route$fn__290.invoke(core.clj:39)
	at org.apache.storm.shade.compojure.core$if_method$fn__283.invoke(core.clj:24)
	at org.apache.storm.shade.compojure.core$routing$fn__308.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__312.invoke(core.clj:111)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1204.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__724.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__3091.invoke(helpers.clj:50)
	at org.apache.storm.ui.core$catch_errors$fn__3837.invoke(core.clj:1250)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2852.invoke(keyword_params.clj:27)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2892.invoke(nested_params.clj:65)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2823.invoke(params.clj:55)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3075.invoke(flash.clj:14)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3063.invoke(session.clj:43)
	at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__2991.invoke(cookies.clj:160)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2729.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__2733.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)"
STORM-1655,Flux doesn't set return code to non-zero when there's any exception while deploying topology to remote cluster,"Flux.runCli() swallows any Exceptions when deploying topology remotely, while StormSubmitter.submitTopology() can throw exceptions.
(AlreadyAliveException, InvalidTopologyException, AuthorizationException, and so on)

It just prints warning log, and return code is 0, not non-zero.

You can easily reproduce via deploying same topology twice with Flux."
STORM-1654,HBaseBolt creates tick tuples with no interval when we don't set flushIntervalSecs  ,"As STORM-1219 addressed, we can't get value about topology's message timeout seconds at getComponentConfiguration(), so logic for applying flush interval to the half of message timeout is no effect.
Unless we set flushIntervalSeconds explicitly, tick tuple interval is set to 0 second, no interval.

Other bolts were fixed as STORM-1219, but seems missing HBaseBolt."
STORM-1647,AutoHBase doesn't send delegation token,"When submitting a topology, Nimbus appears to pick up the delegation tokens as expected (the second log line is a call I added for testing this):

{code:title=nimbus.log}
2016-03-22 12:54:55.159 o.a.s.h.s.AutoHBase [INFO] Logged into Hbase as principal = storm@MIST.COGNITIVESYSTEMS.COM
2016-03-22 12:54:55.160 o.a.s.h.s.AutoHBase [INFO] AutoHBase proxyUser: flurry@MIST.COGNITIVESYSTEMS.COM (auth:PROXY) via storm@MIST.COGNITIVESYSTEMS.COM (auth:KERBEROS)
2016-03-22 12:54:55.971 o.a.s.h.s.AutoHBase [INFO] Obtained HBase tokens, adding to user credentials.
{code}

However, when my bolt starts up, it attempts to write to HBASE as storm instead of flurry:

{code:title=Flurry-1-1458664951-worker-6700.log}
2016-03-22 16:48:10.506 b.s.d.executor [ERROR] 
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: org.apache.hadoop.hbase.security.AccessDeniedException: Insufficient permissions (user=storm@MIST.COGNITIVESYSTEMS.COM, scope=########, family=##############, params=[table=########,family=########],action=WRITE)
{code}

if I go into SecurityAuth.audit, the HBASE regionserver receives a simple token instead of a proxy token:

{code:title=SecurityAuth.audit}
2016-03-22 16:55:20,903 INFO SecurityLogger.org.apache.hadoop.hbase.Server: Connection from ##.##.##.## port: 35551 with unknown version info
2016-03-22 16:55:20,903 INFO SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager: Authorization successful for storm@MIST.COGNITIVESYSTEMS.COM (auth:TOKEN) for protocol=interface org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingInterface
{code}

For reference, core-site.xml:

{code:title=core-site.xml}
hadoop.proxyuser.storm.groups = flurry
hadoop.proxyuser.storm.hosts = *
{code}

And storm.yaml:

{code:title=storm.yaml}
hbase.kerberos.principal : 'storm@MIST.COGNITIVESYSTEMS.COM'
hbase.keytab.file : '/etc/security/keytabs/storm.headless.keytab'
nimbus.autocredential.plugins.classes : ['org.apache.storm.hbase.security.AutoHBase']
nimbus.credential.renewers.classes : ['org.apache.storm.hbase.security.AutoHBase']
nimbus.credential.renewers.freq.secs : 82800
{code}"
STORM-1645,nimbus.thrift.port command line argument leads to java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer,"When you supply the commandline parameter for a custom Nimbus thrift port, the number value is interpreted as a Long, but internally used as an Integer. This leads to a ClassCastException.

This is executed (redacted):
/opt/storm/apache-storm-0.10.0/bin/storm kill -w 10 -c nimbus.thrift.port=6627 -c nimbus.host=vm0009 #{topology}""

This is the output:
 ** [out :: vm0009] 2627 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
 ** [out :: vm0009] 2795 [main] INFO  b.s.u.Utils - Using storm.yaml from resources
 ** [out :: vm0009] 4262 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
 ** [out :: vm0009] 4287 [main] INFO  b.s.u.Utils - Using storm.yaml from resources
 ** [out :: vm0009] 4328 [main] INFO  b.s.thrift - Connecting to Nimbus at vm0009:6627 as user:
 ** [out :: vm0009] 4328 [main] INFO  b.s.u.Utils - Using defaults.yaml from resources
 ** [out :: vm0009] 4348 [main] INFO  b.s.u.Utils - Using storm.yaml from resources
 ** [out :: vm0009] Exception in thread ""main""
 ** [out :: vm0009] java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
 ** [out :: vm0009] 
 ** [out :: vm0009] at backtype.storm.thrift$nimbus_client_and_conn.invoke(thrift.clj:75)
 ** [out :: vm0009] 
 ** [out :: vm0009] at backtype.storm.thrift$nimbus_client_and_conn.invoke(thrift.clj:72)
 ** [out :: vm0009] 
 ** [out :: vm0009] 
 ** [out :: vm0009] at backtype.storm.command.kill_topology$_main.doInvoke(kill_topology.clj:26)
 ** [out :: vm0009] 
 ** [out :: vm0009] at clojure.lang.RestFn.applyTo(RestFn.java:137)
 ** [out :: vm0009] 
 ** [out :: vm0009] at backtype.storm.command.kill_topology.main(Unknown Source)
 ** [out :: vm0009]

I have seen other related issues: https://issues.apache.org/jira/browse/STORM-1578
I believe this is the same issue (internally using an Integer, but converting the input to a Long) in a different area."
STORM-1644,Shell component's executable can not be found,"Setting the working directory:
https://github.com/apache/storm/blob/a4f9f8bc5b4ca85de487a0a868e519ddcb94e852/storm-core/src/jvm/org/apache/storm/utils/ShellProcess.java#L74
 doesn't work for lookup of the executable:
http://stackoverflow.com/questions/9847242/processbuilder-cant-find-file
and results in ""CreateProcess error=2, The system cannot find the file specified"" "
STORM-1642,NullPointerException when deserialize,"Hi:
I've encountered the following NPE when storm tries to deserialize. I did not use OutputCollector concurrently in my code.  The only object we are passing between bolts are a thrift object, and we have written a serializer for it. I've attached the code of serializer and please help to check whether there are any potential bugs there.

2016-03-04 17:17:43.583 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.NullPointerException
        at com.esotericsoftware.kryo.io.Input.setBuffer(Input.java:57) ~[kryo-2.21.jar:?]
        at backtype.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:47) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:433) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132) ~[storm-core-0.10.0.jar:0.10.0]
        ... 6 more
2016-03-04 17:17:43.584 b.s.d.executor [ERROR]
java.lang.RuntimeException: java.lang.NullPointerException
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]
Caused by: java.lang.NullPointerException
        at com.esotericsoftware.kryo.io.Input.setBuffer(Input.java:57) ~[kryo-2.21.jar:?]
        at backtype.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:47) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:433) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132) ~[storm-core-0.10.0.jar:0.10.0]
        ... 6 more
2016-03-04 17:17:43.648 b.s.util [ERROR] Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:336) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.worker$fn__7188$fn__7189.invoke(worker.clj:536) [storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.daemon.executor$mk_executor_data$fn__5523$fn__5524.invoke(executor.clj:261) [storm-core-0.10.0.jar:0.10.0]
        at backtype.storm.util$async_loop$fn__545.invoke(util.clj:489) [storm-core-0.10.0.jar:0.10.0]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]"
STORM-1639,Log4j 2.1 doesn't effect when i modify the log file size or log index number in cluster.xml without restart sevice,
STORM-1638,Integration tests are failing on Windows,"Though I addressed STORM-1602, STORM-1629, STORM-1630, integration tests are still failing from Windows."
STORM-1636, Supervisor shutdown with worker id pass in being nil ,"In function kill-existing-workers-with-change-in-components in supervisor.clj:
The function tries to detect whether there is a change in assignment. The bug in this function is that the ordering of the assignment matters but it shouldn't. For example, if a worker assignment is [[1 1] [2 2]] and it changed to [[2 2] [1 1]] it will cause the supervisor to restart the worker"
STORM-1633,"Document ""storm blobstore"" to command-line-client.md","distcache-blobstore.md describes how to use Blobstore with CLI, but command-line-client.md misses `blobstore` option.

Since distcache-blobstore.md has complete guide of the usage, linking to blobstore page properly would be fine."
STORM-1630,"""create symbolic link"" needs elevation or setting privilege about creating symbolic link","I already initiated discussion thread. Please refer here.
http://mail-archives.apache.org/mod_mbox/storm-dev/201603.mbox/%3CCAF5108hVJpeZbC+JngcQmvE9HmV8HejsZ=cO1gdK3JDirumNmg@mail.gmail.com%3E

This PR is for tracking and resolving symlink issue after discussion. 

While it's just initiated, but IMO, at least ""how to resolve privilege issue manually"" should be documented since privilege issue on Windows users is introduced to Storm 1.0.0 for the first time."
STORM-1629,Files/move doesn't work properly with non-empty directory in Windows,"Distributed version of download-storm-code uses Files#move().
It runs well on *Nix (including OSX) but fails on Windows.

Javadoc describes this behavior, please refer below link.
https://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#move(java.nio.file.Path,%20java.nio.file.Path,%20java.nio.file.CopyOption...)

{quote}
When invoked to move a directory that is not empty then the directory is moved if it does not require moving the entries in the directory. For example, renaming a directory on the same FileStore will usually not require moving the entries in the directory. When moving a directory requires that its entries be moved then this method fails (by throwing an IOException). To move a file tree may involve copying rather than moving directories and this can be done using the copy method in conjunction with the Files.walkFileTree utility method.
{quote}

If directory is not empty, file system should treat ""move directory"" as ""rename"".
Unfortunately, file system on Windows 8 doesn't.

We should change the way to be compatible with both kinds of OS."
STORM-1628,Explore any optimizations to avoid storing replayed batches in trident windowing,Explore any optimizations to avoid storing replayed batches in trident windowing
STORM-1627,Handle emitting delayed triggered results in trident windowing operation.,
STORM-1626,HBaseBolt tuple counts too high in storm ui.,When a storm topology has an HbaseBolt the storm ui numbers for executed and acked tuples seem to be too high for HbaseBolt component. 
STORM-1621,[Storm][DRPC] Request failed after 60 seconds,"Hello.

We work with storm and use the DRPC to execute some algorithms on the cluster.

When the duration time on each node is less than 60 seconds, there is no trouble: client receives correct result. However when we have to solve bigger problem with the same algorithm (then duration time is more than 60 seconds) we have the following message:

Exception in thread ""main"" DRPCExecutionException(msg:Request failed) at
backtype.storm.generated.DistributedRPC$execute_result$execute_resultStandardScheme.read(DistributedRPC.java:1222) at 
backtype.storm.generated.DistributedRPC$execute_result$execute_resultStandardScheme.read(DistributedRPC.java:1200) at backtype.storm.generated...

It seems to be about the node, which sends a message to cluster with ""Request failed"" and doesn't finish algorithm. Hope somebody have a hint to solve this :(

Note that the Client-DRPC-Topology and the 10 test nodes work properly when the duration is less than the minute.

Thank you."
STORM-1613,upgrade hbase version from 0.98.4-hadoop2 to 1.1,
STORM-1608,Fix stateful topology acking behavior,"Right now the acking is automatically taken care of for the non-stateful bolts in a stateful topology. This leads to double acking if BaseRichBolts are part of the topology. For the non-stateful bolts, its better to let the bolt do the acking rather than automatically acking."
STORM-1605,storm shell script should use /bin/env python to check python version,"Currently storm script uses /usr/bin/python2.6 and /usr/bin/python to check python version, however, RHEL5 ships with python2.4 by default, so when users upgrade python to 2.7.5 without changing /usr/bin/python to 2.7.5 python binary, storm will fail to start since it thinks it's using python2.4.
It would be better to use /bin/env python to detect python version instead."
STORM-1604,Delayed transition should handle NotALeaderException,"Currently if an action(kill, rebalance) is scheduled with delay, nimbus stores the state in zookeeper and then schedules a delayed event to do final transition. If during this wait time, leader nimbus loses the leadership, when the delayed operation is executed it receives a NotALeaderException which it does not handle causing the nimbus to die. We should catch the exception and  ignore it."
STORM-1603,Storm UT fails on supervisor test in Windows (0.10.x),"supervisor UTs seem not handle file path delimeter properly so it fails on Windows consistently but no problem on other OS.

I can easily reproduce this from 0.10.x-branch, but I can't reproduce this from master or 1.x-branch since there're other tests which crash whole tests so that test phase couldn't even reach supervisor-test.

For 1.x-branch, blobstore crashes test phase on Windows. I'll link relevant issue.
For master, I'm not sure what issues lay here."
STORM-1602,Blobstore UTs are failing on Windows,"Blobstore related UTs are failed on Windows.

{code}
-------------------------------------------------------------------------------
Test set: org.apache.storm.blobstore.BlobStoreTest
-------------------------------------------------------------------------------
Tests run: 7, Failures: 0, Errors: 7, Skipped: 0, Time elapsed: 2.306 sec <<< FAILURE! - in org.apache.storm.blobstore.BlobStoreTest
testMultipleLocalFs(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 1.798 sec  <<< ERROR!
java.nio.file.AccessDeniedException: D:\storm\storm-core\target\blob-store-test-19f8e973-7c1b-4638-8679-2eb1adcac396\blobs\571\data_other\1457050287771.tmp -> D:\storm\storm-core\target\blob-store-test-19f8e973-7c1b-4638-8679-2eb1adcac396\blobs\571\data_other\data
	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
	at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
	at java.nio.file.Files.move(Files.java:1347)
	at org.apache.storm.blobstore.LocalFsBlobStoreFile.commit(LocalFsBlobStoreFile.java:127)
	at org.apache.storm.blobstore.BlobStore$BlobStoreFileOutputStream.close(BlobStore.java:324)
	at org.apache.storm.blobstore.BlobStoreTest.testMultiple(BlobStoreTest.java:397)
	at org.apache.storm.blobstore.BlobStoreTest.testMultipleLocalFs(BlobStoreTest.java:168)

testMultipleLocalFs(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 1.8 sec  <<< ERROR!
java.io.IOException: Unable to delete file: target\blob-store-test-19f8e973-7c1b-4638-8679-2eb1adcac396\blobs\571\data_other\data
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2279)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.storm.blobstore.BlobStoreTest.cleanup(BlobStoreTest.java:74)

testGetFileLength(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 0.067 sec  <<< ERROR!
java.io.IOException: Unable to delete file: target\blob-store-test-b6d39145-11ea-4aa7-ae30-28bda603fb3a\blobs\1017\data_test\data
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2279)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.storm.blobstore.BlobStoreTest.cleanup(BlobStoreTest.java:74)

testBasicLocalFs(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 0.124 sec  <<< ERROR!
java.lang.RuntimeException: java.nio.file.DirectoryNotEmptyException: D:\storm\storm-core\target\blob-store-test-b74e39dc-8ce7-4c39-ab7a-e04a8519eb61\blobs\1017\data_test
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(WindowsFileSystemProvider.java:265)
	at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)
	at java.nio.file.Files.deleteIfExists(Files.java:1118)
	at org.apache.storm.blobstore.FileBlobStoreImpl.delete(FileBlobStoreImpl.java:239)
	at org.apache.storm.blobstore.FileBlobStoreImpl.deleteKey(FileBlobStoreImpl.java:178)
	at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:226)
	at org.apache.storm.blobstore.LocalFsBlobStore$$EnhancerByMockitoWithCGLIB$$8252dffa.CGLIB$deleteBlob$5(<generated>)
	at org.apache.storm.blobstore.LocalFsBlobStore$$EnhancerByMockitoWithCGLIB$$8252dffa$$FastClassByMockitoWithCGLIB$$bca8d941.invoke(<generated>)
	at org.mockito.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:216)
	at org.mockito.internal.creation.AbstractMockitoMethodProxy.invokeSuper(AbstractMockitoMethodProxy.java:10)
	at org.mockito.internal.invocation.realmethod.CGLIBProxyRealMethod.invoke(CGLIBProxyRealMethod.java:22)
	at org.mockito.internal.invocation.realmethod.FilteredCGLIBProxyRealMethod.invoke(FilteredCGLIBProxyRealMethod.java:27)
	at org.mockito.internal.invocation.InvocationImpl.callRealMethod(InvocationImpl.java:108)
	at org.mockito.internal.stubbing.answers.CallsRealMethods.answer(CallsRealMethods.java:36)
	at org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:93)
	at org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29)
	at org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:38)
	at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:51)
	at org.apache.storm.blobstore.LocalFsBlobStore$$EnhancerByMockitoWithCGLIB$$8252dffa.deleteBlob(<generated>)
	at org.apache.storm.blobstore.BlobStoreTest.testBasic(BlobStoreTest.java:312)
	at org.apache.storm.blobstore.BlobStoreTest.testBasicLocalFs(BlobStoreTest.java:163)

testBasicLocalFs(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 0.124 sec  <<< ERROR!
java.io.IOException: Unable to delete file: target\blob-store-test-b74e39dc-8ce7-4c39-ab7a-e04a8519eb61\blobs\1017\data_test\data
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2279)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.storm.blobstore.BlobStoreTest.cleanup(BlobStoreTest.java:74)

testLocalFsWithAuth(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 0.314 sec  <<< ERROR!
java.nio.file.AccessDeniedException: D:\storm\storm-core\target\blob-store-test-77e698b7-a4bc-4719-b33e-9a8b588fc2e3\blobs\1017\data_test\1457050288321.tmp -> D:\storm\storm-core\target\blob-store-test-77e698b7-a4bc-4719-b33e-9a8b588fc2e3\blobs\1017\data_test\data
	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
	at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
	at java.nio.file.Files.move(Files.java:1347)
	at org.apache.storm.blobstore.LocalFsBlobStoreFile.commit(LocalFsBlobStoreFile.java:127)
	at org.apache.storm.blobstore.BlobStore$BlobStoreFileOutputStream.close(BlobStore.java:324)
	at org.apache.storm.blobstore.BlobStoreTest.testWithAuthentication(BlobStoreTest.java:246)
	at org.apache.storm.blobstore.BlobStoreTest.testLocalFsWithAuth(BlobStoreTest.java:158)

testLocalFsWithAuth(org.apache.storm.blobstore.BlobStoreTest)  Time elapsed: 0.314 sec  <<< ERROR!
java.io.IOException: Unable to delete file: target\blob-store-test-77e698b7-a4bc-4719-b33e-9a8b588fc2e3\blobs\1017\data_test\data
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2279)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2270)
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653)
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535)
	at org.apache.storm.blobstore.BlobStoreTest.cleanup(BlobStoreTest.java:74)
{code}

{code}
-------------------------------------------------------------------------------
Test set: org.apache.storm.localizer.LocalizerTest
-------------------------------------------------------------------------------
Tests run: 13, Failures: 3, Errors: 0, Skipped: 0, Time elapsed: 0.23 sec <<< FAILURE! - in org.apache.storm.localizer.LocalizerTest
testArchivesTar(org.apache.storm.localizer.LocalizerTest)  Time elapsed: 0.037 sec  <<< FAILURE!
java.lang.AssertionError: blob uncompressed doesn't contain symlink
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.storm.localizer.LocalizerTest.testArchives(LocalizerTest.java:295)
	at org.apache.storm.localizer.LocalizerTest.testArchivesTar(LocalizerTest.java:252)

testArchivesTgz(org.apache.storm.localizer.LocalizerTest)  Time elapsed: 0.012 sec  <<< FAILURE!
java.lang.AssertionError: blob uncompressed doesn't contain symlink
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.storm.localizer.LocalizerTest.testArchives(LocalizerTest.java:295)
	at org.apache.storm.localizer.LocalizerTest.testArchivesTgz(LocalizerTest.java:237)

testArchivesTarGz(org.apache.storm.localizer.LocalizerTest)  Time elapsed: 0.013 sec  <<< FAILURE!
java.lang.AssertionError: blob uncompressed doesn't contain symlink
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.assertTrue(Assert.java:41)
	at org.apache.storm.localizer.LocalizerTest.testArchives(LocalizerTest.java:295)
	at org.apache.storm.localizer.LocalizerTest.testArchivesTarGz(LocalizerTest.java:247)
{code}

And blobstore also makes supervisor crashed on grouping-test.

{code}
20839 [Thread-13] INFO  o.a.s.d.supervisor - Downloading code for storm id topologytest-ecc87348-ca6d-4b0a-a836-4ad3b60f7b64-1-0
20843 [Thread-13] INFO  o.a.c.f.i.CuratorFrameworkImpl - Starting
20848 [Thread-13] INFO  o.a.s.b.FileBlobStoreImpl - Creating new blob store based in D:\tmp\5e742303-a2a4-4880-a5f9-cdaf9551ad11\blobs
20933 [Thread-13] ERROR o.a.s.event - Error when processing event
java.io.IOException: Unable to delete file: D:\tmp\5d3907f5-513a-4b86-8f21-877ab3425ba1\supervisor\tmp\64ac7267-6837-4a59-95cf-afb74403d334\stormconf.ser
	at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:2279) ~[commons-io-2.4.jar:2.4]
	at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1653) ~[commons-io-2.4.jar:2.4]
	at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1535) ~[commons-io-2.4.jar:2.4]
	at org.apache.commons.io.FileUtils.moveDirectory(FileUtils.java:2756) ~[commons-io-2.4.jar:2.4]
	at org.apache.storm.daemon.supervisor$fn__9646.invoke(supervisor.clj:1160) ~[classes/:?]
	at clojure.lang.MultiFn.invoke(MultiFn.java:243) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9365$fn__9383.invoke(supervisor.clj:571) ~[classes/:?]
	at org.apache.storm.daemon.supervisor$mk_synchronize_supervisor$this__9365.invoke(supervisor.clj:570) ~[classes/:?]
	at org.apache.storm.event$event_manager$fn__8912.invoke(event.clj:40) [classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80-ea]
20957 [Thread-13] ERROR o.a.s.util - Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341) [classes/:?]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.7.0.jar:?]
	at org.apache.storm.event$event_manager$fn__8912.invoke(event.clj:48) [classes/:?]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.7.0_80-ea]
{code}

After failed UT, I can see stormconf.ser at that location, and also able to delete file via 'del' from command line."
STORM-1601,Cluster-state must check if znode exists before getting children for storm backpressure,"You see below exception in the integration tests..

{panel}
15:46:23 java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /backpressure/topologytest-22ffcfa0-8992-4258-b8b6-52346a129b58-1-0
15:46:23 	at backtype.storm.util$wrap_in_runtime.invoke(util.clj:52) ~[classes/:?]
15:46:23 	at backtype.storm.zookeeper$get_children.invoke(zookeeper.clj:168) ~[classes/:?]
15:46:23 	at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4184.get_children(zookeeper_state_factory.clj:129) ~[classes/:?]
15:46:23 	at sun.reflect.GeneratedMethodAccessor53.invoke(Unknown Source) ~[?:?]
15:46:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
15:46:23 	at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
15:46:23 	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
15:46:23 	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.cluster$mk_storm_cluster_state$reify__4091.topology_backpressure(cluster.clj:407) ~[classes/:?]
15:46:23 	at sun.reflect.GeneratedMethodAccessor210.invoke(Unknown Source) ~[?:?]
15:46:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
15:46:23 	at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
15:46:23 	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
15:46:23 	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.daemon.worker$fn__6837$exec_fn__1477__auto__$reify__6839$check_throttle_changed__6910$cb__6911.doInvoke(worker.clj:704) ~[classes/:?]
15:46:23 	at clojure.lang.RestFn.invoke(RestFn.java:408) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.cluster$issue_map_callback_BANG_.invoke(cluster.clj:183) ~[classes/:?]
15:46:23 	at backtype.storm.cluster$mk_storm_cluster_state$fn__4081.invoke(cluster.clj:239) ~[classes/:?]
15:46:23 	at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$fn__4166.invoke(zookeeper_state_factory.clj:45) ~[classes/:?]
15:46:23 	at backtype.storm.zookeeper$mk_client$reify__2993.eventReceived(zookeeper.clj:63) ~[classes/:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl$8.apply(CuratorFrameworkImpl.java:860) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl$8.apply(CuratorFrameworkImpl.java:853) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92) [curator-framework-2.5.0.jar:?]
15:46:23 	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:?]
15:46:23 	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl.processEvent(CuratorFrameworkImpl.java:850) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl.access$000(CuratorFrameworkImpl.java:57) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.CuratorFrameworkImpl$1.process(CuratorFrameworkImpl.java:138) [curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.ConnectionState.process(ConnectionState.java:152) [curator-client-2.5.0.jar:?]
15:46:23 	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522) [zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498) [zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /backpressure/topologytest-22ffcfa0-8992-4258-b8b6-52346a129b58-1-0
15:46:23 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1625) ~[zookeeper-3.4.6.jar:3.4.6-1569965]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:210) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[curator-client-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38) ~[curator-framework-2.5.0.jar:?]
15:46:23 	at sun.reflect.GeneratedMethodAccessor93.invoke(Unknown Source) ~[?:?]
15:46:23 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_60]
15:46:23 	at java.lang.reflect.Method.invoke(Method.java:497) ~[?:1.8.0_60]
15:46:23 	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
15:46:23 	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
15:46:23 	at backtype.storm.zookeeper$get_children.invoke(zookeeper.clj:166) ~[classes/:?]
15:46:23 	... 29 more
{panel}"
STORM-1599,Don't mark dependencies as provided unless they are in lib,"When we mark a dependency as provided it indicates the shade and assembly plugins to not include this particular dependency in the uber topology jar because it will be {{provided}} on the class path by the system.

We have been doing this for all of our kafka dependencies incorrectly, storm-cassandra does this for cassandra-driver-core, and storm-starter is doing it for storm-clojure as well.

This means that storm-starter does not have any version of kafka or storm-clojure packaged it the resulting jar and any example that uses kafka, TridentKafkaWordCount, will fail with missing class errors. 

storm-starter/pom.xml has should change its dependency on storm-kafka to be compile, and it should delete dependencies on kafka and kafka-clients as those should come from storm-kafka as transitive dependencies.

the main pom.xml should not have kafka-clients marked as provided in the dependency management section.

storm-kafka should remove its provided tag on kafka, and flux examples + storm-sql-kafka should remove dependencies on kafka and kafka-clients, and storm-kafka should not me marked as provided. 

the flux and sql code I am not as familiar with, but looking at them, and running `mvn dependecy:tree` and `mvn dependency:analyze` it looks like"
STORM-1596,Multiple Subject sharing Kerberos TGT - causes services to fail,"With multiple threads accessing same {{Subject}}, it can cause {{ServiceTicket}} in use be by one thread be destroyed by another thread.

Running BasicDRPCTopology with high parallelism in secure cluster would reproduce the issue.

Here is sample log from such a scenarios:
{code}
2016-01-20 15:52:26.904 o.a.t.t.TSaslTransport [ERROR] SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_40]
        at org.apache.thrift7.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:271) [storm-core-0.10.1.y.jar:0.10.1.y]
        at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:195) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:191) [storm-core-0.10.1.y.jar:0.10.1.y]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_40]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_40]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:190) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:54) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:109) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.DRPCInvocationsClient.reconnectClient(DRPCInvocationsClient.java:57) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.ReturnResults.reconnectClient(ReturnResults.java:113) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.drpc.ReturnResults.execute(ReturnResults.java:103) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$fn__6377$tuple_action_fn__6379.invoke(executor.clj:689) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__6301.invoke(executor.clj:448) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.disruptor$clojure_handler$reify__6018.onEvent(disruptor.clj:40) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:437) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:416) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.daemon.executor$fn__6377$fn__6390$fn__6441.invoke(executor.clj:801) [storm-core-0.10.1.y.jar:0.10.1.y]
        at backtype.storm.util$async_loop$fn__742.invoke(util.clj:482) [storm-core-0.10.1.y.jar:0.10.1.y]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]
Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: The ticket isn't for us (35) - BAD TGS SERVER NAME)
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:770) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more
Caused by: sun.security.krb5.KrbException: The ticket isn't for us (35) - BAD TGS SERVER NAME
        at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:73) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]
        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more
Caused by: sun.security.krb5.Asn1Exception: Identifier doesn't match expected value (906)
        at sun.security.krb5.internal.KDCRep.init(KDCRep.java:140) ~[?:1.8.0_40]
        at sun.security.krb5.internal.TGSRep.init(TGSRep.java:65) ~[?:1.8.0_40]
        at sun.security.krb5.internal.TGSRep.<init>(TGSRep.java:60) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:55) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:259) ~[?:1.8.0_40]
        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:270) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:302) ~[?:1.8.0_40]
        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:120) ~[?:1.8.0_40]
        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:458) ~[?:1.8.0_40]
        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:693) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248) ~[?:1.8.0_40]
        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179) ~[?:1.8.0_40]
        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:192) ~[?:1.8.0_40]
        ... 23 more


{code}"
STORM-1595,'Fail' messages get stuck somewhere ,"'Fail' acks seem to be getting stuck somewhere between the acker and the spout. 

After a long time - sometimes multiple minutes - the fails show up in the spout.
I tested this on master and 1.x-branch and it occurs in both places.
"
STORM-1592,clojure code calling into Utils.exitProcess throws ClassCastException,Our exception handling is not longer working on master to shut down the process when an error occurs.
STORM-1588,component page gets divide by 0 if no event loggers configured,"If you run a topology with no event loggers configured the component page gets a divide by zero trying to calculate things about the event loggers.

"
STORM-1585,Add DDL support for UDFs in Storm-sql,
STORM-1581,Repair github links in the storm documentation,Java classes have been migrated to org.apache.storm and the github links are now broken in the documentation.
STORM-1579,Got NoSuchFileException when running tests in storm-core,"Stacktrace:
125277 [Thread-1736-__eventlogger-executor[4 4]] ERROR o.a.s.m.FileBasedEventLogger - Error setting up FileBasedEventLogger.
java.nio.file.NoSuchFileException: /logs/workers-artifacts/metrics-tester-1-0/1024/events.log
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) ~[?:1.7.0_75]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) ~[?:1.7.0_75]
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) ~[?:1.7.0_75]
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) ~[?:1.7.0_75]
    at java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:430) ~[?:1.7.0_75]
    at java.nio.file.Files.newOutputStream(Files.java:172) ~[?:1.7.0_75]
    at java.nio.file.Files.newBufferedWriter(Files.java:2722) ~[?:1.7.0_75]
    at org.apache.storm.metric.FileBasedEventLogger.initLogWriter(FileBasedEventLogger.java:51) [classes/:?]
    at org.apache.storm.metric.FileBasedEventLogger.prepare(FileBasedEventLogger.java:97) [classes/:?]
    at org.apache.storm.metric.EventLoggerBolt.prepare(EventLoggerBolt.java:48) [classes/:?]
    at org.apache.storm.daemon.executor$fn__6507$bolt_transfer_fn__6522.invoke(executor.clj:792) [classes/:?]
    at clojure.lang.AFn.call(AFn.java:18) [clojure-1.7.0.jar:?]
    at org.apache.storm.utils.Utils$6.run(Utils.java:2177) [classes/:?]
    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_75]"
STORM-1578,"ClassCastException from Integer to Long for ""port"" in cluster.clj translation","(:port worker) is passed as Integer to java,
in STORM-1273, the port is defined as Long, which will cause java.lang.ClassCastException.
Funtions might be afftected:
    public void workerBackpressure(String stormId, String node, Long port, boolean on)          (confirmed)



<code>
  8953 java.lang.ClassCastException: Cannot cast java.lang.Integer to java.lang.Long
  8954     at java.lang.Class.cast(Class.java:3369) ~[?:1.8.0_60]
  8955     at clojure.lang.Reflector.boxArg(Reflector.java:427) ~[clojure-1.7.0.jar:?]
  8956     at clojure.lang.Reflector.boxArgs(Reflector.java:460) ~[clojure-1.7.0.jar:?]
  8957     at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:58) ~[clojure-1.7.0.jar:?]
  8958     at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]
  8959     at org.apache.storm.daemon.worker$mk_backpressure_handler$reify__7649.onEvent(worker.clj:160) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT
<code>"
STORM-1576,TopologyBuilder fails with ConcurrentModification in addCheckPointInputs for stateful topologies,"addCheckPointInputs adds to map while iterating over it, which needs to be fixed."
STORM-1575,TwitterSampleSpout throws NPE on close,"the global ""_twitterStream"" is not initialized in ""open"" but used in ""close"""
STORM-1574,"Better exception handling in backpressure thread, and remove backpressure dir during topology kill.","The current exception handling in WorkerBackpressureThread can cause the thread to die before we want, causing potential backpressure flag synchronizing problem. 
Also, we need to cleanup the topology backpressure directory during killing."
STORM-1571,Improvment Kafka Spout Time Metric,Use System.currentTimeMillis() to calculation time interval is better than System.nanoTime() 
STORM-1569,Allowing users to specify the nimbus thrift server queue size.,"Currently the nimbus sever in secure mode uses https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html Backed by https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/SynchronousQueue.html, Please see https://github.com/apache/thrift/blob/0.9.2/lib/java/src/org/apache/thrift/server/TThreadPoolServer.java#L132. This means that if all executor threads are busy serving a request and new requests come in we will see RejectedExecutionExceptions in logs once they have reached the retry limit. Instead we should allow the requests to be queued. This patch allows the requests to be queued by replacing SynchronousQueue with https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ArrayBlockingQueue.html with default size of 100000 requests which should be large enough for most applications. Applications can modify this default by adding the config nimbus.queue.size to their storm.yaml and bouncing nimbus.
"
STORM-1567,in defaults.yaml  'topology.disable.loadaware' should be 'topology.disable.loadaware.messaging',"{code:title=defaults.yaml|borderStyle=solid}
diff --git a/conf/defaults.yaml b/conf/defaults.yaml
index 166b249..01821e1 100644
--- a/conf/defaults.yaml
+++ b/conf/defaults.yaml
@@ -256,7 +256,7 @@ topology.bolts.outgoing.overflow.buffer.enable: false
 topology.disruptor.wait.timeout.millis: 1000
 topology.disruptor.batch.size: 100
 topology.disruptor.batch.timeout.millis: 1
-topology.disable.loadaware: false
+topology.disable.loadaware.messaging: false
 topology.state.checkpoint.interval.ms: 1000
 
 # Configs for Resource Aware Scheduler
{code}"
STORM-1566,Worker exits with error o.a.s.d.worker [ERROR] Error on initialization of server mk-worker java.lang.ClassCastException: java.lang.String cannot be cast to java.io.File,
STORM-1561,Supervisor should relaunch worker if assignments have changed,"Currently, supervisor validates new assignments against existing assignments by port. It should also check on the same port - if executors have changed."
STORM-1560,Topology stops processing after Netty catches/swallows Throwable,"In some scenarios, netty connection problems can leave a topology in an unrecoverable state. The likely culprit is the Netty {{HashedWheelTimer}} class that contains the following code:

{code}
        public void expire() {
            if(this.compareAndSetState(0, 2)) {
                try {
                    this.task.run(this);
                } catch (Throwable var2) {
                    if(HashedWheelTimer.logger.isWarnEnabled()) {
                        HashedWheelTimer.logger.warn(""An exception was thrown by "" + TimerTask.class.getSimpleName() + '.', var2);
                    }
                }
            }
        }
{code}

The exception being swallowed can be seen below:

{code}
2016-02-18 08:46:59.116 o.a.s.m.n.Client [INFO] closing Netty Client Netty-Client-/192.168.202.6:6701
2016-02-18 08:46:59.173 o.a.s.m.n.Client [INFO] waiting up to 600000 ms to send 0 pending messages to Netty-Client-/192.168.202.6:6701
2016-02-18 08:46:59.271 STDIO [ERROR] Feb 18, 2016 8:46:59 AM org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer
WARNING: An exception was thrown by TimerTask.
java.lang.RuntimeException: Giving up to scheduleConnect to Netty-Client-/192.168.202.6:6701 after 44 failed attempts. 3 messages were lost
	at org.apache.storm.messaging.netty.Client$Connect.run(Client.java:573)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$HashedWheelTimeout.expire(HashedWheelTimer.java:546)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.notifyExpiredTimeouts(HashedWheelTimer.java:446)
	at org.apache.storm.shade.org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:395)
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:745)
{code}

The netty client then never recovers, and the follows messages repeat forever:

{code}
2016-02-18 09:42:56.251 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:25.248 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:55.248 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:55.752 o.a.s.m.n.Client [ERROR] discarding 2 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:43:56.252 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
2016-02-18 09:44:25.249 o.a.s.m.n.Client [ERROR] discarding 1 messages because the Netty client to Netty-Client-/192.168.202.6:6701 is being closed
{code}
"
STORM-1558,Utils in java breaks component page due to illegal type cast,"Two methods in Utils.java:
logsFilename and eventLogsFilename, the 'port' argument was changed to String type in PR for #STORM-1538, but its caller event-log-link and worker-log-link in core.clj passes an int port, which results in illegal type cast.

Also this is possibly the cause to #STORM-1545"
STORM-1556,nimbus.clj/wait-for-desired-code-replication wrong reset for current-replication-count-jar in local mode,"https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj#L520-L521

{code}
(if (not (ConfigUtils/isLocalMode conf))
    (reset! current-replication-count-conf  (get-blob-replication-count (ConfigUtils/masterStormConfKey storm-id) nimbus)))
(reset! current-replication-count-code  (get-blob-replication-count (ConfigUtils/masterStormCodeKey storm-id) nimbus))
(reset! current-replication-count-jar  (get-blob-replication-count (ConfigUtils/masterStormJarKey storm-id) nimbus))))
{code}

We do not go to count the number of jar-replication in local mode, but will count the number of conf-replication. So is it a mistake that current-replication-count-conf and current-replication-count-jar in the wrong place?

If it is a bug, I will create a PR soon."
STORM-1555,Required field 'topology_id' is unset! seen failing integration-test in Travis CI Test,"A recent travis ci [error|https://travis-ci.org/apache/storm/jobs/109239992#L1570] was seen.  

Here is the stack trace:

{noformat}
107323 [main] ERROR i.o.a.s.t.integration-test - Error in cluster

java.lang.RuntimeException: org.apache.thrift.protocol.TProtocolException: Required field 'topology_id' is unset! Struct:LSTopoHistory(topology_id:null, time_stamp:1455491275, users:[], groups:[])

	at org.apache.storm.utils.LocalState.serialize(LocalState.java:186) ~[classes/:?]

	at org.apache.storm.utils.LocalState.put(LocalState.java:142) ~[classes/:?]

	at org.apache.storm.utils.LocalState.put(LocalState.java:136) ~[classes/:?]

	at org.apache.storm.local_state$ls_topo_hist_BANG_.invoke(local_state.clj:48) ~[classes/:?]

	at org.apache.storm.daemon.nimbus$add_topology_to_history_log.invoke(nimbus.clj:1279) ~[classes/:?]

	at org.apache.storm.daemon.nimbus$fn__4836$exec_fn__1827__auto__$reify__4865.killTopologyWithOpts(nimbus.clj:1587) ~[classes/:?]

	at sun.reflect.GeneratedMethodAccessor298.invoke(Unknown Source) ~[?:?]

	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_76]

	at java.lang.reflect.Method.invoke(Method.java:606) ~[?:1.7.0_76]

	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.7.0.jar:?]

	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.7.0.jar:?]

	at org.apache.storm.trident.testing$with_topology_STAR_.invoke(testing.clj:62) ~[classes/:?]

	at integration.org.apache.storm.trident.integration_test$fn__5251.invoke(integration_test.clj:149) [?:?]

	at clojure.test$test_var$fn__7670.invoke(test.clj:704) [clojure-1.7.0.jar:?]

	at clojure.test$test_var.invoke(test.clj:704) [clojure-1.7.0.jar:?]

	at clojure.test$test_vars$fn__7692$fn__7697.invoke(test.clj:722) [clojure-1.7.0.jar:?]

	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]

	at clojure.test$test_vars$fn__7692.invoke(test.clj:722) [clojure-1.7.0.jar:?]

	at clojure.test$default_fixture.invoke(test.clj:674) [clojure-1.7.0.jar:?]

	at clojure.test$test_vars.invoke(test.clj:718) [clojure-1.7.0.jar:?]

	at clojure.test$test_all_vars.invoke(test.clj:728) [clojure-1.7.0.jar:?]

	at clojure.test$test_ns.invoke(test.clj:747) [clojure-1.7.0.jar:?]

	at clojure.core$map$fn__4553.invoke(core.clj:2624) [clojure-1.7.0.jar:?]

	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]

	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]

	at clojure.lang.Cons.next(Cons.java:39) [clojure-1.7.0.jar:?]

	at clojure.lang.RT.boundedLength(RT.java:1735) [clojure-1.7.0.jar:?]

	at clojure.lang.RestFn.applyTo(RestFn.java:130) [clojure-1.7.0.jar:?]

	at clojure.core$apply.invoke(core.clj:632) [clojure-1.7.0.jar:?]

	at clojure.test$run_tests.doInvoke(test.clj:762) [clojure-1.7.0.jar:?]

	at clojure.lang.RestFn.invoke(RestFn.java:408) [clojure-1.7.0.jar:?]

	at org.apache.storm.testrunner$eval10993$iter__10994__10998$fn__10999$fn__11000$fn__11001.invoke(test_runner.clj:107) [?:?]

	at org.apache.storm.testrunner$eval10993$iter__10994__10998$fn__10999$fn__11000.invoke(test_runner.clj:53) [?:?]

	at org.apache.storm.testrunner$eval10993$iter__10994__10998$fn__10999.invoke(test_runner.clj:52) [?:?]

	at clojure.lang.LazySeq.sval(LazySeq.java:40) [clojure-1.7.0.jar:?]

	at clojure.lang.LazySeq.seq(LazySeq.java:49) [clojure-1.7.0.jar:?]

	at clojure.lang.RT.seq(RT.java:507) [clojure-1.7.0.jar:?]

	at clojure.core$seq__4128.invoke(core.clj:137) [clojure-1.7.0.jar:?]

	at clojure.core$dorun.invoke(core.clj:3009) [clojure-1.7.0.jar:?]

	at org.apache.storm.testrunner$eval10993.invoke(test_runner.clj:52) [?:?]

	at clojure.lang.Compiler.eval(Compiler.java:6782) [clojure-1.7.0.jar:?]

	at clojure.lang.Compiler.load(Compiler.java:7227) [clojure-1.7.0.jar:?]

	at clojure.lang.Compiler.loadFile(Compiler.java:7165) [clojure-1.7.0.jar:?]

	at clojure.main$load_script.invoke(main.clj:275) [clojure-1.7.0.jar:?]

	at clojure.main$script_opt.invoke(main.clj:337) [clojure-1.7.0.jar:?]

	at clojure.main$main.doInvoke(main.clj:421) [clojure-1.7.0.jar:?]

	at clojure.lang.RestFn.invoke(RestFn.java:421) [clojure-1.7.0.jar:?]

	at clojure.lang.Var.invoke(Var.java:383) [clojure-1.7.0.jar:?]

	at clojure.lang.AFn.applyToHelper(AFn.java:156) [clojure-1.7.0.jar:?]

	at clojure.lang.Var.applyTo(Var.java:700) [clojure-1.7.0.jar:?]

	at clojure.main.main(main.java:37) [clojure-1.7.0.jar:?]

Caused by: org.apache.thrift.protocol.TProtocolException: Required field 'topology_id' is unset! Struct:LSTopoHistory(topology_id:null, time_stamp:1455491275, users:[], groups:[])

	at org.apache.storm.generated.LSTopoHistory.validate(LSTopoHistory.java:586) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistory$LSTopoHistoryStandardScheme.write(LSTopoHistory.java:702) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistory$LSTopoHistoryStandardScheme.write(LSTopoHistory.java:628) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistory.write(LSTopoHistory.java:544) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistoryList$LSTopoHistoryListStandardScheme.write(LSTopoHistoryList.java:409) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistoryList$LSTopoHistoryListStandardScheme.write(LSTopoHistoryList.java:359) ~[classes/:?]

	at org.apache.storm.generated.LSTopoHistoryList.write(LSTopoHistoryList.java:309) ~[classes/:?]

	at org.apache.thrift.TSerializer.serialize(TSerializer.java:79) ~[libthrift-0.9.3.jar:0.9.3]

	at org.apache.storm.utils.LocalState.serialize(LocalState.java:184) ~[classes/:?]

	... 50 more
{noformat}
"
STORM-1552,Fix topology event sampling log directory ,"Run a topology and enable event inspection by clicking ""Debug"" from UI. The events are logged under ""storm-local/workers-artifacts/{storm-id}/port/events.log"". In the spout/bolt details page, the ""events"" link does not display the log file.

The events.log should be kept under logs/workers-artifacts/{storm-id}/{port}/events.log so that its viewable via logviewer."
STORM-1545,Topology Debug Event Log in Wrong Location,"Currently the {{events.log}} file is not created where the log viewer expects it so be, so the ""events"" link under debug in Storm UI returns a 404.

The file should be in:

{{$storm.log.dir/workers-artifacts/$topology_id/$port/events.log}}

but is instead in:

{{$storm.local.dir/workers-artifacts/$topology_id/$port/events.log}}"
STORM-1544,Document Debug/Sampling of Topologies,"Currently the topology/component sampling feature is undocumented, and likely confusing to users (the UI includes the ""Debug"" and ""Stop Debug"", but the functionality does not provide any indication of what it does, or how to access the sample logs).

We should document the basic functionality and configuration, as well as how to extend it."
STORM-1543,DRPCSpout should always try to reconnect disconnected DRPCInvocationsClient,"It appears, DRPCSpout skips pull request from DRPC Server if its not connected - but does not request reconnects.."
STORM-1542,Taking jstack for a worker in UI results in endless empty jstack dumps,"Resolved path for jstack command on supervisor is
/home/y/share/yjava_jdk/java/jstack which doesn't exist. command returns 127 as exit code. When a request for jstack dump is made from UI, a zookeeper node is created. Now supervisor keeps on reading this node, executes jstack command and since exit code is non-zero, doesn't delete the node afterwards. Thus supervisor keeps on executing the command forever and each invocation creates an new empty file.
{noformat}
$BINPATH/jstack $1 > ""$2/${FILENAME}""
{noformat}
"
STORM-1541,Change scope of 'hadoop-minicluster' to test,"STORM-969 added dependency 'hadoop-minicluster' but not set scope to 'test' though it's for unit test. (and normally hadoop-minicluster is)

It may come up with other unintended dependencies."
STORM-1540,Topology Debug/Sampling Breaks Trident Topologies,"Steps to reproduce:

1. Deploy a Trident topology.
2. Turn on debug/sampling.

Workers will crash with the following error:

2016-02-11 14:13:23.617 o.a.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:448) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:414) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.disruptor$consume_loop_STAR_$fn__7651.invoke(disruptor.clj:83) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.util$async_loop$fn__554.invoke(util.clj:484) [storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_72]
Caused by: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList
	at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:41) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568) ~[kryo-2.21.jar:?]
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:75) ~[kryo-2.21.jar:?]
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18) ~[kryo-2.21.jar:?]
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:486) ~[kryo-2.21.jar:?]
	at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:44) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:44) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$mk_transfer_fn$transfer_fn__8346.invoke(worker.clj:186) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__8037.invoke(executor.clj:309) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.disruptor$clojure_handler$reify__7634.onEvent(disruptor.clj:40) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:435) ~[storm-core-1.0.0-SNAPSHOT.jar:1.0.0-SNAPSHOT]
	... 6 more
"
STORM-1537,Upgrade to Kryo 3,"In storm, Kryo (2.21) is used for serialization:
https://github.com/apache/storm/blob/02a44c7fc1b7b3a1571b326fde7bcae13e1b5c8d/pom.xml#L231

The user must use the same version storm does, or there will be a java class error at runtime.

Storm depends on a quasi-abandoned library: carbonite:
https://github.com/apache/storm/blob/02a44c7fc1b7b3a1571b326fde7bcae13e1b5c8d/pom.xml#L210

which depends on Kryo 2.21 and Twitter chill 0.3.6:
https://github.com/sritchie/carbonite/blob/master/project.clj#L8

Chill, currently on 0.7.3, would like to upgrade to Kryo 3.0.3:
https://github.com/twitter/chill/pull/245

because Spark, also depending on chill, would like to upgrade for performance improvements and bugfixes.
https://issues.apache.org/jira/browse/SPARK-11416

Unfortunately, summingbird depends on storm:
https://github.com/twitter/summingbird/blob/develop/build.sbt#L34

so, if chill is upgraded, and that gets on the classpath, summingbird will break at runtime.

I propose:
1) copy the carbonite code into storm. It is likely the only consumer.
2) bump the storm kryo dependency after chill upgrades: recall that storm actually depends on chill-java. A dependency that could possibly be removed after you pull carbonite in.
3) once a new version of storm is published, summingbird (and scalding) can upgrade to the latest chill.

Also, I hope for:
4) we as a JVM community get better about classpath isolation and versioning. Diamonds like this in one big classpath make large codebases very fragile.
"
STORM-1536,Eliminate or minimize use of deprecated TimeCacheMap,
STORM-1535,Storm-HDFS - When using keytab Kerberos ticket is not renewed with multiple bolts/executors,"When logging in with a keytab, if the topology has more than one instance of an HDFS bolt then the ticket might not be automatically renewed.

Bug has been fixed for HBase bolt, code is slightly different for HDFS bolt and therefore requires further investigation"
STORM-1534,Incompatible jackson dependencies are being packed in storm package,"I see jackson-annotations-2.1.1.jar with  jackson-databind-2.6.3.jar. This is leading to exception in my application 
{noformat}
java.lang.NoClassDefFoundError: com/fasterxml/jackson/annotation/JsonProperty$Access
    at com.fasterxml.jackson.databind.introspect.POJOPropertyBuilder.findAccess(POJOPropertyBuilder.java:542) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.introspect.POJOPropertyBuilder.removeNonVisible(POJOPropertyBuilder.java:623) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector._removeUnwantedAccessor(POJOPropertiesCollector.java:697) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.collectAll(POJOPropertiesCollector.java:298) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.introspect.POJOPropertiesCollector.getJsonValueMethod(POJOPropertiesCollector.java:169) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.introspect.BasicBeanDescription.findJsonValueMethod(BasicBeanDescription.java:222) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ser.BasicSerializerFactory.findSerializerByAnnotations(BasicSerializerFactory.java:355) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ser.BasicSerializerFactory.buildMapSerializer(BasicSerializerFactory.java:771) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ser.BasicSerializerFactory.buildContainerSerializer(BasicSerializerFactory.java:580) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory._createSerializer2(BeanSerializerFactory.java:194) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ser.BeanSerializerFactory.createSerializer(BeanSerializerFactory.java:153) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.SerializerProvider._createUntypedSerializer(SerializerProvider.java:1203) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.SerializerProvider._createAndCacheUntypedSerializer(SerializerProvider.java:1157) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.SerializerProvider.findValueSerializer(SerializerProvider.java:481) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.SerializerProvider.findTypedValueSerializer(SerializerProvider.java:679) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:107) ~[jackson-databind-2.6.3.jar:2.6.3]
    at com.fasterxml.jackson.databind.ObjectMapper._convert(ObjectMapper.java:3437) ~[jackson-databind-2.6.3.jar:2.6.3]
{noformat}"
STORM-1533,topologies with metric consumers fail in config validation phase,"Storm expects the parallelism hint to be long. When the yaml is read, parallelism hint is read as an integer, and topology submission fails. 
{noformat}
,Exception in thread ""main"" java.lang.IllegalArgumentException: Field TOPOLOGY_METRICS_CONSUMER_REGISTER list entry must be of type class java.lang.Long. Object: 1 actual type: class java.lang.Integer
	at backtype.storm.validation.ConfigValidation$SimpleTypeValidator.validateField(ConfigValidation.java:90)
	at backtype.storm.validation.ConfigValidation$MetricRegistryValidator.validateField(ConfigValidation.java:476)
	at backtype.storm.validation.ConfigValidation$ListEntryCustomValidator.validateField(ConfigValidation.java:339)
	at backtype.storm.validation.ConfigValidation$ListEntryCustomValidator.validateField(ConfigValidation.java:323)
	at backtype.storm.validation.ConfigValidation.validateField(ConfigValidation.java:624)
	at backtype.storm.validation.ConfigValidation.validateFields(ConfigValidation.java:659)
	at backtype.storm.validation.ConfigValidation.validateFields(ConfigValidation.java:638)
	at backtype.storm.StormSubmitter.validateConfs(StormSubmitter.java:464)
	at backtype.storm.StormSubmitter.submit,TopologyAs(StormSubmitter.java:201)
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:288)
{noformat}"
STORM-1524,Make Storm daemon function statistics reporter pluggable,"We use codahale, metrics-clojure to gather daemon side stats, but currently the we have only three reporters available which use builder pattern. So it would be useful to have ability to plugin different reporters that can use configuration instead of builder pattern."
STORM-1521,When using Kerberos login from keytab with multiple bolts/executors ticket is not renewed,"When logging in with a keytab, if the topology has more than one instance of an HBase bolt then the ticket will not be automatically renewed.

Expected: The ticket will be automatically renewed and the bolt will be able to write to the database.
Actual: The ticket is not renewed and the bolt loses access to HBase.

Note when there is only one bolt with one executor is renews correctly.

Exception in bolt is:
2015-12-18T09:41:13.862-0500 o.a.h.s.UserGroupInformation [ERROR] PriviledgedActionException as:user@somewhere.com 
cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by 
GSSException: No valid credentials provided (Mechanism level: Failed to find any
 Kerberos tgt)]
2015-12-18T09:41:13.862-0500 o.a.h.i.RpcClient [WARN] Exception encountered 
while connecting to the server : javax.security.sasl.SaslException: GSS initiate
 failed [Caused by GSSException: No valid credentials provided (Mechanism level:
 Failed to find any Kerberos tgt)]
2015-12-18T09:41:13.863-0500 o.a.h.i.RpcClient [ERROR] SASL authentication 
failed. The most likely cause is missing or invalid credentials. Consider 
'kinit'."
STORM-1520,"Nimbus Clojure/Zookeeper issue (""stateChanged"" method not found)","Placeholder until I can gather more information for reproducing the issue.

The following appears in nimbus.log after deploying/undeploying topologies:

{code}
2016-02-02 21:34:04.308 o.a.s.s.o.a.c.f.l.ListenerContainer [ERROR] Listener (org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660@22587507) threw an exception
java.lang.IllegalArgumentException: No matching method found: stateChanged for class org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660.stateChanged(zookeeper_state_factory.clj:145)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)
	at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at org.apache.storm.shade.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:84)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)
	at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

-Basic functionality does not seem to be affected.-

Nimbus becomes unresponsive and needs to be manually restarted.
"
STORM-1516,Topology workers are not getting killed when a topology is killed.,"When topology with timebased windowing bolts are killed, respective workers are not shutdown properly and they remain running. When you want to deploy a new topology, it throws with the below Exception as the earlier worker is not shutdown. This issue is not specific with this topology though.

2016-02-02 10:07:42.845 o.a.s.d.worker [ERROR] Error on initialization of server mk-worker
org.apache.storm.shade.org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:6700
	at org.apache.storm.shade.org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.netty.Server.<init>(Server.java:101) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.messaging.netty.Context.bind(Context.java:67) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$worker_data$fn__6329.invoke(worker.clj:265) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.util$assoc_apply_self.invoke(util.clj:934) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$worker_data.invoke(worker.clj:262) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.daemon.worker$fn__6627$exec_fn__2511__auto__$reify__6629.run(worker.clj:605) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_60]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_60]
	at org.apache.storm.daemon.worker$fn__6627$exec_fn__2511__auto____6628.invoke(worker.clj:603) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]
	at clojure.core$apply.invoke(core.clj:630) ~[clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$fn__6627$mk_worker__6722.doInvoke(worker.clj:577) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker$_main.invoke(worker.clj:764) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.7.0.jar:?]
	at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.7.0.jar:?]
	at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method) ~[?:1.8.0_60]
	at sun.nio.ch.Net.bind(Net.java:433) ~[?:1.8.0_60]
	at sun.nio.ch.Net.bind(Net.java:425) ~[?:1.8.0_60]
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:1.8.0_60]
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74) ~[?:1.8.0_60]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:372) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:296) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at org.apache.storm.shade.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[?:1.8.0_60]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[?:1.8.0_60]
	at java.lang.Thread.run(Thread.java:745) ~[?:1.8.0_60]
2016-02-02 10:07:42.857 o.a.s.util [ERROR] Halting process: (""Error on initialization"")"
STORM-1515,LocalState corruption after hard reboot on Windows,"After a hard reboot on windows I'm seeing {{LocalState}} files for the supervisor that contain a few hundred NULs, resulting in a {{StreamCorruptedException}} on deserialization and the supervisor failing to start.

{noformat}
2016-01-27T17:04:10.848-0700 b.s.d.supervisor [INFO] Starting supervisor with id 45b27917-4ca0-4d96-8727-914909e3ac47 at host jtorbiak-ws.nj.invidi.com
2016-01-27T17:04:11.673-0700 b.s.event [ERROR] Error when processing event
java.lang.RuntimeException: java.io.StreamCorruptedException: invalid stream header: 00000000
        at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:56) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.utils.Utils.deserialize(Utils.java:89) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.utils.LocalState.deserializeLatestVersion(LocalState.java:65) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.utils.LocalState.snapshot(LocalState.java:47) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.utils.LocalState.get(LocalState.java:72) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:234) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:40) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.io.StreamCorruptedException: invalid stream header: 00000000
        at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:806) ~[na:1.8.0_45]
        at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299) ~[na:1.8.0_45]
        at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:51) ~[storm-core-0.9.4.jar:0.9.4]
        ... 13 common frames omitted
2016-01-27T17:04:11.674-0700 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:48) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
2016-01-27T17:04:11.695-0700 b.s.d.supervisor [INFO] Shutting down supervisor 45b27917-4ca0-4d96-8727-914909e3ac47
{noformat}

This is very similar to STORM-307, except the {{LocalState}} files contain NULs instead of being empty. I'm guessing this corruption type is specific to Windows."
STORM-1503,PacemakerClient Reconnection issue,"Worker should not restart for failure to send heartbeats to Pacemaker or worker.

Also, PacemakerClient should make reconnect efforts on failure to write on existing channel."
STORM-1501, launch worker process exception will cause supervisor process exited,"[util.clj/async-loop | https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/util.clj#L474] default kill-fn will kill current process  

when supervisor use [util.clj/launch-process | https://github.com/apache/storm/blob/master/storm-core/src/clj/org/apache/storm/util.clj#L546] to launch worker process , if exeception occurs , supervisor process will exit."
STORM-1496,Nimbus periodically throws blobstore-related exception: No matching method found: readBlob for class java.lang.String,"Blobstore periodically throws exception:

{code}
2016-01-22 16:13:21.205 o.a.s.d.nimbus [INFO] Created download session for wordcount-3-1453497194-stormjar.jar with id 0d84a3e1-e4c9-46fc-b0c7-8f02f48fb016
2016-01-22 16:13:21.499 o.a.s.d.nimbus [INFO] Created download session for wordcount-3-1453497194-stormcode.ser with id c33cbb90-92a1-4191-8ac0-24b2547503da
2016-01-22 16:13:21.503 o.a.s.d.nimbus [INFO] Created download session for wordcount-3-1453497194-stormconf.ser with id ac1ba1ce-5697-4490-84eb-2c74b6415d62
2016-01-22 16:15:19.509 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.IllegalArgumentException: No matching method found: readBlob for class java.lang.String
	at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)
	at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
	at org.apache.storm.daemon.nimbus$read_storm_topology_as_nimbus.invoke(nimbus.clj:529)
	at org.apache.storm.daemon.nimbus$try_read_storm_topology.invoke(nimbus.clj:1249)
	at org.apache.storm.daemon.nimbus$fn__7312$exec_fn__1827__auto__$reify__7341.getTopology(nimbus.clj:1776)
	at org.apache.storm.generated.Nimbus$Processor$getTopology.getResult(Nimbus.java:3878)
	at org.apache.storm.generated.Nimbus$Processor$getTopology.getResult(Nimbus.java:3862)
	at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
	at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
	at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Steps to reproduce:

1. Setup one node cluster.
2. Deploy word count topology.
3. Kill word count topology.
4. Monitor nimbus.log"
STORM-1495,Topology visualization is broken,"Clicking the show visualization button results in a stacktracke:

{code}
org.apache.storm.thrift.transport.TTransportException
	at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)
	at org.apache.storm.generated.Nimbus$Client.recv_getTopology(Nimbus.java:1333)
	at org.apache.storm.generated.Nimbus$Client.getTopology(Nimbus.java:1320)
	at org.apache.storm.ui.core$build_visualization.invoke(core.clj:491)
	at org.apache.storm.ui.core$fn__12083.invoke(core.clj:972)
	at org.apache.storm.shade.compojure.core$make_route$fn__2468.invoke(core.clj:93)
	at org.apache.storm.shade.compojure.core$if_route$fn__2456.invoke(core.clj:39)
	at org.apache.storm.shade.compojure.core$if_method$fn__2449.invoke(core.clj:24)
	at org.apache.storm.shade.compojure.core$routing$fn__2474.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2570)
	at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:632)
	at org.apache.storm.shade.compojure.core$routes$fn__2478.invoke(core.clj:111)
	at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__11535.invoke(json.clj:56)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__3544.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__4287.invoke(reload.clj:22)
	at org.apache.storm.ui.helpers$requests_middleware$fn__3771.invoke(helpers.clj:46)
	at org.apache.storm.ui.core$catch_errors$fn__12255.invoke(core.clj:1230)
	at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__3475.invoke(keyword_params.clj:27)
	at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__3515.invoke(nested_params.clj:65)
	at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__3446.invoke(params.clj:55)
	at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__3544.invoke(multipart_params.clj:103)
	at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3730.invoke(flash.clj:14)
	at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3718.invoke(session.clj:43)
	at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__3646.invoke(cookies.clj:160)
	at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__3352.invoke(servlet.clj:127)
	at org.apache.storm.shade.ring.util.servlet$servlet$fn__3356.invoke(servlet.clj:136)
	at org.apache.storm.shade.ring.util.servlet.proxy$javax.servlet.http.HttpServlet$ff19274a.service(Unknown Source)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)
	at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)
	at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
	at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
	at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
	at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
	at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
{code}"
STORM-1492,"With nimbus.seeds set to default, a nimbus for localhost may appear ""Offline""","With the default value for {{nimbus.seeds}} ({{[""localhost""]}}) Storm UI may list one ""Offline"" nimbus for localhost, and another as ""Leader"" for the resolved machine name.

Steps to reproduce (assumes ZK is running; all on local machine):

1. Clean install of 1.0.0-SNAPSHOT (do not modify {{storm.yaml}})
2. Start nimbus
3. Start supervisor
4. Start ui
5. Navigate to http://localhost:8080

A workaround is to modify {{storm.yaml}} and replace ""localhost"" with the hostname of the machine in {{nimbus.seeds}}.

While trivial to correct, this may confuse users. One approach is to simply document this behavior."
STORM-1489,Script for cleaner environment checking,"As a storm developer, I would like a common script that storm can execute to detect features of the environment (like OS), so that storm has cleaner code for enabling features at run-time.

See [original comment|https://github.com/apache/storm/pull/1012#discussion_r50280610]"
STORM-1488,UI Topology Page component last error timestamp is from 1970,"Seems to be something wrong with the parsing of the timestamp into a date string.

!screen-shot-error-time.png|thumbnail!"
STORM-1487,UI Topology Page tooltips misplaced,"!screen-shot-tooltips.png|thumbnail!

Seems the placement is off.

(Note, the error timestamp in the year 1970 will be handled in a separate issue.)"
STORM-1477,STORM HDFS bolt must acknoledge tuples after rotation,"While performing rotation we call closeOutputFile method - so data is already in HDFS and tuples must be acknoledged. Otherwise, next sync operation could fail and mark thouse tuples as failed."
STORM-1476,Filter -c options from args and add them as part of storm.options,
STORM-1475,storm-elasticsearch should support ES 2.X,
STORM-1473,Enable search on the daemon log,"Search works fine for worker log.
It would be nice that we can also enable this good feature for the daemon logs."
STORM-1472,Change long to readble date/time format for error log link,"In the error log link of UI, long of millisecond dates makes little sense to user. It would be nice to change it to be readable date. "
STORM-1470,"org.apache.hadoop:hadoop-auth not shaded, breaks spnego authentication","{noformat}
2016-01-12 20:07:45.642 o.a.s.s.o.e.j.s.ServletHandler [WARN] Error for /favicon.ico
java.lang.NoClassDefFoundError: org/apache/commons/codec/binary/Base64
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:343)
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:519)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
        at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247)
        at org.apache.storm.shade.org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)
        at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)
        at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)
        at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
        at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
        at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)
        at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

We already shade commons-codec:commons-codec, but we don't apply that shading to org.apache.hadoop:hadoop-auth.
"
STORM-1469,Unable to deploy large topologies on apache storm,"When deploying to a nimbus a topology which is larger in size >17MB, we get an exception. In storm 0.9.3 this could be mitigated by using the following config on the storm.yaml to increse the buffer size to handle the topology size. i.e. 50MB would be

nimbus.thrift.max_buffer_size: 50000000

This configuration does not resolve the issue in the master branch of storm and we cannot deploy topologies which are large in size.

Here is the log on the client side when attempting to deploy to the nimbus node:
java.lang.RuntimeException: org.apache.thrift7.transport.TTransportException
	at backtype.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:251) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:272) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopology(StormSubmitter.java:155) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at com.trustwave.siem.storm.topology.deployer.TopologyDeployer.deploy(TopologyDeployer.java:149) [siem-ng-storm-deployer-cloud.jar:]
	at com.trustwave.siem.storm.topology.deployer.TopologyDeployer.main(TopologyDeployer.java:87) [siem-ng-storm-deployer-cloud.jar:]
Caused by: org.apache.thrift7.transport.TTransportException
	at org.apache.thrift7.transport.TIOStreamTransport.read(TIOStreamTransport.java:132) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TFramedTransport.read(TFramedTransport.java:101) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:77) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.generated.Nimbus$Client.recv_submitTopology(Nimbus.java:238) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.generated.Nimbus$Client.submitTopology(Nimbus.java:222) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	at backtype.storm.StormSubmitter.submitTopologyAs(StormSubmitter.java:237) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
	... 4 more

Here is the log on the server side (nimbus.log):

2016-01-13 10:48:07.206 o.a.s.d.nimbus [INFO] Cleaning inbox ... deleted: stormjar-c8666220-fa19-426b-a7e4-c62dfb57f1f0.jar
2016-01-13 10:55:09.823 o.a.s.d.nimbus [INFO] Uploading file from client to /var/storm-data/nimbus/inbox/stormjar-80ecdf05-6a25-4281-8c78-10062ac5e396.jar
2016-01-13 10:55:11.910 o.a.s.d.nimbus [INFO] Finished uploading file from client: /var/storm-data/nimbus/inbox/stormjar-80ecdf05-6a25-4281-8c78-10062ac5e396.jar
2016-01-13 10:55:12.084 o.a.t.s.AbstractNonblockingServer$FrameBuffer [WARN] Exception while invoking!
org.apache.thrift7.transport.TTransportException: Frame size (17435758) larger than max length (16384000)!
	at org.apache.thrift7.transport.TFramedTransport.readFrame(TFramedTransport.java:137)
	at org.apache.thrift7.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift7.transport.TTransport.readAll(TTransport.java:86)
	at org.apache.thrift7.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)
	at org.apache.thrift7.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)
	at org.apache.thrift7.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)
	at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:27)
	at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
	at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
	at org.apache.thrift7.server.Invocation.run(Invocation.java:18)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


"
STORM-1465,TridentOperationContext - no way to check if metric is already registered,"There's no way to call TopologyContext.getRegisteredMetricByName, to check if metric has already been registered, when using Trident API.

The registerMetric method has been already exposed to Trident, but other methods from TopologyContext are kept hidden.

Aside from that... Why does TridentOperationContext not extend TopologyContext? (They both implement the same interface - IMetricsContext and are used in the same way)."
STORM-1455,kafka spout should not reset to the beginning of partition when offsetoutofrange exception occurs,"https://github.com/apache/storm/blob/master/external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java#L190
{noformat}
try {
            msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
        } catch (TopicOffsetOutOfRangeException e) {
            _emittedToOffset = KafkaUtils.getOffset(_consumer, _partition.topic, _partition.partition, kafka.api.OffsetRequest.EarliestTime());
            LOG.warn(""{} Using new offset: {}"", _partition.partition, _emittedToOffset);
{noformat}

If there was one old offset out of range, partition manager will re-send all the offsets from EarliestTime to _emittedOffset. "
STORM-1454,UI cannot handle topology names with spaces,"If I submit a topology with an name that contains spaces (eg, ""Linear Road Benchmark"", I cannot access the detailed topology view. If I click on the topology name in the Web UI, I get the following:

Internal Server Error
{noformat}
NotAliveException(msg:Linear+Road+Benchmark-3-1452252242)
	at backtype.storm.generated.Nimbus$getTopologyInfo_result.read(Nimbus.java:11347)
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:78)
	at backtype.storm.generated.Nimbus$Client.recv_getTopologyInfo(Nimbus.java:491)
	at backtype.storm.generated.Nimbus$Client.getTopologyInfo(Nimbus.java:478)
	at backtype.storm.ui.core$topology_page.invoke(core.clj:628)
	at backtype.storm.ui.core$fn__8020.invoke(core.clj:853)
	at compojure.core$make_route$fn__6199.invoke(core.clj:93)
	at compojure.core$if_route$fn__6187.invoke(core.clj:39)
	at compojure.core$if_method$fn__6180.invoke(core.clj:24)
	at compojure.core$routing$fn__6205.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2443)
	at compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:619)
	at compojure.core$routes$fn__6209.invoke(core.clj:111)
	at ring.middleware.reload$wrap_reload$fn__6234.invoke(reload.clj:14)
	at backtype.storm.ui.core$catch_errors$fn__8059.invoke(core.clj:909)
	at ring.middleware.keyword_params$wrap_keyword_params$fn__6876.invoke(keyword_params.clj:27)
	at ring.middleware.nested_params$wrap_nested_params$fn__6915.invoke(nested_params.clj:65)
	at ring.middleware.params$wrap_params$fn__6848.invoke(params.clj:55)
	at ring.middleware.multipart_params$wrap_multipart_params$fn__6943.invoke(multipart_params.clj:103)
	at ring.middleware.flash$wrap_flash$fn__7124.invoke(flash.clj:14)
	at ring.middleware.session$wrap_session$fn__7113.invoke(session.clj:43)
	at ring.middleware.cookies$wrap_cookies$fn__7044.invoke(cookies.clj:160)
	at ring.adapter.jetty$proxy_handler$fn__7324.invoke(jetty.clj:16)
	at ring.adapter.jetty.proxy$org.mortbay.jetty.handler.AbstractHandler$0.handle(Unknown Source)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{noformat}

The link says: http://dbis71:8080/topology.html?id=Linear+Road+Benchmark-5-1452255348
If I replace the ""+"" with spaces manually I can access the page. However, I cannot ""kill"" the topology -- a click on ""kill"" has no effect."
STORM-1453,nimbus.clj/wait-for-desired-code-replication print wrong log message,"
https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L516

"
STORM-1452,"Worker ""profiler"" actions broken by default","* The profiler script flight.bash is not packaged by default.
* The default options enable the Oracle specific flight-recorder that requires a support subscription.

The option to enable the profiler should not be enabled by default.  Other actions such as worker restart, debugging, and heap can remain enabled."
STORM-1430,UI Worker Functions: Replace pick-list with buttons,"The Component Page in the UI has a very useful set of functions that allow us to profile a worker, to restart a worker, to create a stack trace, or to save the heap.

When there are many workers, finding the correct worker host:port from the pick list is tedious.

As a user, I would like the worker functions presented as buttons directly in the Executors summary tables, so that I can easily request the functions when there are lots of workers."
STORM-1428,Nimbus fails to start after adding consumer metrics,"Setup
* Storm v0.10.0 and 0.9.2-incubating
* Setup nimbus and supervisor on the same node
* Storm configuration -- https://gist.github.com/thedebugger/a9b54de54ed38ae00cd4#file-storm-yaml

Steps to reproduce
1) Start nimbus without Logging consumer metrics
2) Start supervisor on the 
3) Submit ExclamationToplogy in the cluster mode
4) Stop nimbus
5) Add Logging consumer metric in storm.yaml
5) Start nimbus. It doesn't start. 
6) Check the error in the logs -- https://gist.github.com/thedebugger/a9b54de54ed38ae00cd4

After looking at the code, I figured out that Nimbus and metric consumer works fine when there is no storm cluster state i.e. if it is enabled in the first run.

Let me know if more information is required."
STORM-1427,add TupleUtils/listHashCode method and delete tuple.clj ,
STORM-1426,    keep backtype.storm.tuple.AddressedTuple and delete duplicated  backtype.storm.messaging.AddressedTuple ,
STORM-1424,Removed unused topology-path variable in config.clj/read-supervisor-storm-conf,
STORM-1423,storm UI in a secure env shows error even when credentials are present,storm UI in a secure env shows error even when credentials are present
STORM-1407,Incorrect authorize check for daemon log links in UI,"Incorrect authorize check for daemon log links in UI.
This causes problem when running in security mode (run as user)."
STORM-1403,Consider gc childopts in JVM calculation for resource display,Need to consider worker.gc.childopts and topology.worker.gc.childopts when calculating per-worker memory usage for UI display
STORM-1402,Test missing for topology history during merge,a test for topology history is missing in nimbus test when we merge that into community.
STORM-1401,Spurious multilang-test crashes on travis-ci,"One example here: https://travis-ci.org/apache/storm/jobs/97589141#L438

The test failure does not happen consistently; when they do, they seem to halt the test job."
STORM-1400,Netty Context removeClient() called after term() causes NullPointerException.,"Under the right conditions, we can throw a NullPointerException when shutting down.

This was seen in a travis-ci run of messaging-test [here|https://travis-ci.org/apache/storm/jobs/97494930#L1315]."
STORM-1399,Blobstore tests should write data to `target` so it gets removed when running `mvn clean`,Currently the blobstore tests create files/directories in the current working directory. It would be better to use `target` so they get deleted when running `mvn clean` and are ignored by git.
STORM-1398,TopologyDetails.getTopology was accidently removed.,"https://github.com/apache/storm/commit/121d022b9f11146f6dadc2cd402c747472cac0d1

removed getTopology, but it is a user facing API and we should put it back."
STORM-1395,Move JUnit dependency to top-level pom,"The switch to profiles for running different types of tests requires that certain submodules explicitly include the JUnit dependency. Moving it to the top level pom will allow all submodules to inherit it.

Background: the build was failing in my environment because storm-redis did not have the dependency."
STORM-1393,Sort out storm.log.dir configure and add documentation about logs,"Currently, we have reorganized logs in STORM-901 and STORM-1387, it is preferable for us to document the changes out for avoiding confusion to users. 

Also, the util/LOG-DIR and the way supervisor to get storm.log.dir in worker-launch is inaccurate since it does not take the storm-conf into account. We should fix it."
STORM-1392,Storm Cassandra Test Timeouts,"Noticed the following error in one of the travis-ci test runs.  If it makes sense, we should adjust the test timeout so that this does not fail as often.

In org.apache.storm.cassandra.DynamicStatementBuilderTest

{noformat}
java.lang.AssertionError: Cassandra daemon did not start within timeout
{noformat}


This is annoying because test for unrelated changes can fail, causing confusion."
STORM-1391,Logviewer doesn't close the file stream after reading metafile,"File handle is not released after reading metafile, even the metafile has been deleted by log-cleaner"
STORM-1389,unnecessary generatiion of projection tuples again in StateQueryProcessor#finishBatch,
STORM-1387,Make workers-artifacts directory configurable and default to storm.log.dir/workers-artifacts,"According to our discussion conclusion in STORM-901, we want to keep the log structure, and make the workers-artifacts directory location to be configurable and default to storm.log.dir/workers-artifacts"
STORM-1386,Problem using a newer version of log4j-core,"Storm 0.10.0 comes with log4j-core 2.1 and I can't find any way to override it in my app. I need 2.4.x+ in order to use the kafka appender. I have even tried to relocate the org.apache.logging.log4j in my shade configuration but even then I'm getting errors like this :

ERROR StatusLogger Log4j2 could not find a logging implementation. Please add log4j-core to the classpath. Using SimpleLogger to log to the console...

Thanks in advance."
STORM-1383,Supervisors should not crash if nimbus is unavailable,"In cases of maintenance or unexpected downtime of nimbus nodes, supervisors will crash in a loop.  This can cause a lot of confusion among users (supervisors crash repeatedly) and admins (monitoring/alerting triggered for the entire cluster).

Supervisors periodically check with nimbus to synchronize blob versions, and as part of this, a connection is made to the leader nimbus daemon.  Formerly, supervisors did not periodically contact nimbus, and so nimbus downtime did not cascade to cluster-wide supervisor failures.

It might be nice to handle the case when nimbus cannot be contacted, and continue in the normal loop.
"
STORM-1381,Client side topology submission hook.,"A client side hook is suppose to be invoked when a user submits the topology using TopologySubmitter. We already have nimbus side hook for all the topology actions however those are good if users don't want to actually inspect the topology being submitted or the classes that makes up the topology (spouts and bolts) as on nimbus side these classes are not available in class path. 

As a concrete example, in hortonworks we wanted to integrate storm with atlas to provide complete lineage of data even when it passes through a storm topology. Atlas needed to actually look inside the topology components (i.e. kafka spout to figure out what topic the data is being pulled from, or hbase bolt to figure out which cluster and what table data is being pushed into.) to give a meaningful lineage. We originally proposed that they use the server side hook but with that they had to download the user uploaded jar and add it to the class path dynamically or spin a new jvn whose output will then be read by the atlas integration hook. 

The client side hook is suppose to make it easy when the topology itself needs to be examined. We are using this in our internal repo for atlas integration."
STORM-1379,Removed Redundant Structure,Storm KafkaSpout have a static class MessageAndRealOffset which is same with Kafka API 's  kafka.message.MessageAndOffset . So I have remove MessageAndRealOffset and replace with MessageAndOffset :)
STORM-1377,nimbus_auth_test: very short timeouts causing spurious failures,"This is caused by a units mismatch.  We are waiting 30 ms for the thrift server to reply when we thought we were waiting 30s.  This means that sometimes when we expect NotAliveException, we instead get TTransportException(SocketTimeoutException), and this fails the assertions."
STORM-1376,ZK Becoming deadlocked with zookeeper_state_factory,"Since the introduction of blobstore and pacemaker we've noticed that when using nimbus with the new zookeeper_state_factory backing cluster state module, some of our ZK nodes become unresponsive and show and increasing amounts of outstanding requests (STAT 4-letter command).

Terminating storm supervisors and nimbus usually gets zookeeper to realize after a few minutes those connections are dead and to become responsive again.  In some extreme cases we have to kill that ZK nodes and bring it back up.

Our topologies ran across ~10 supervisor nodes with each having about ~400-500 executors. 

I mention the amount of executors cause I am not sure if someone made each executor by mistake start sending heartbeats instead of each worker and that might possibly be the reason for this slow down.

Final note.  If someone can jot a few ideas of why this might be happening i'd be more than happy to dig further in the storm code and submit a PR myself.  But I need some hint or direction of where to go with this..."
STORM-1375,Blobstore broke Pacemaker,"When using the new Pacemaker (which, you kind of have to, cos the alternative of not using it is abusing ZK to the point of deadlocking it...), you cannot submit topologies because _submitTopology_ is calling ClusterState's _delete_node_blobstore_ which never got implemented in _pacemaker_state_factory.clj_.

Here's a nice stack trace:
{noformat}
2015-12-05 07:07:21 b.s.d.nimbus [WARN] Topology submission exception. (topology name='cron-ba161de') #error {
 :cause org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__3956.delete_node_blobstore(Ljava/lang/String;Ljava/lang/String;)V
 :via
 [{:type java.lang.AbstractMethodError
   :message org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__3956.delete_node_blobstore(Ljava/lang/String;Ljava/lang/String;)V
   :at [sun.reflect.NativeMethodAccessorImpl invoke0 NativeMethodAccessorImpl.java -2]}]
 :trace
 [[sun.reflect.NativeMethodAccessorImpl invoke0 NativeMethodAccessorImpl.java -2]
  [sun.reflect.NativeMethodAccessorImpl invoke NativeMethodAccessorImpl.java 62]
  [sun.reflect.DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl.java 43]
  [java.lang.reflect.Method invoke Method.java 497]
  [clojure.lang.Reflector invokeMatchingMethod Reflector.java 93]
  [clojure.lang.Reflector invokeInstanceMethod Reflector.java 28]
  [backtype.storm.cluster$mk_storm_cluster_state$reify__3846 setup_blobstore_BANG_ cluster.clj 345]
  [sun.reflect.NativeMethodAccessorImpl invoke0 NativeMethodAccessorImpl.java -2]
  [sun.reflect.NativeMethodAccessorImpl invoke NativeMethodAccessorImpl.java 62]
  [sun.reflect.DelegatingMethodAccessorImpl invoke DelegatingMethodAccessorImpl.java 43]
  [java.lang.reflect.Method invoke Method.java 497]
  [clojure.lang.Reflector invokeMatchingMethod Reflector.java 93]
  [clojure.lang.Reflector invokeInstanceMethod Reflector.java 28]
  [backtype.storm.daemon.nimbus$setup_storm_code invoke nimbus.clj 467]
  [backtype.storm.daemon.nimbus$fn__7774$exec_fn__2579__auto__$reify__7803 submitTopologyWithOpts nimbus.clj 1523]
  [backtype.storm.generated.Nimbus$Processor$submitTopologyWithOpts getResult Nimbus.java 2940]
  [backtype.storm.generated.Nimbus$Processor$submitTopologyWithOpts getResult Nimbus.java 2924]
  [org.apache.thrift7.ProcessFunction process ProcessFunction.java 39]
  [org.apache.thrift7.TBaseProcessor process TBaseProcessor.java 39]
  [backtype.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor process SimpleTransportPlugin.java 158]
  [org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer invoke AbstractNonblockingServer.java 518]
  [org.apache.thrift7.server.Invocation run Invocation.java 18]
  [java.util.concurrent.ThreadPoolExecutor runWorker ThreadPoolExecutor.java 1142]
  [java.util.concurrent.ThreadPoolExecutor$Worker run ThreadPoolExecutor.java 617]
  [java.lang.Thread run Thread.java 745]]}
2015-12-05 07:07:21 o.a.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!
java.lang.AbstractMethodError: org.apache.storm.pacemaker.pacemaker_state_factory$_mkState$reify__3956.delete_node_blobstore(Ljava/lang/String;Ljava/lang/String;)V
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:497)
  at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
  at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
  at backtype.storm.cluster$mk_storm_cluster_state$reify__3846.setup_blobstore_BANG_(cluster.clj:345)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:497)
  at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)
  at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)
  at backtype.storm.daemon.nimbus$setup_storm_code.invoke(nimbus.clj:467)
  at backtype.storm.daemon.nimbus$fn__7774$exec_fn__2579__auto__$reify__7803.submitTopologyWithOpts(nimbus.clj:1523)
  at backtype.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:2940)
  at backtype.storm.generated.Nimbus$Processor$submitTopologyWithOpts.getResult(Nimbus.java:2924)
  at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:39)
  at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:39)
  at backtype.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)
  at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)
  at org.apache.thrift7.server.Invocation.run(Invocation.java:18)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
{noformat}"
STORM-1367,Issue with OpaqueTridentKafkaSpout - TridentKafkaConfig getting (java.lang.RuntimeException: kafka.common.OffsetOutOfRangeException),"I'm using trident topology with OpaqueTridentKafkaSpout.

Code snippet of TridentKafkaConfig i’m using :-

OpaqueTridentKafkaSpout kafkaSpout = null;
TridentKafkaConfig spoutConfig = new TridentKafkaConfig(new ZkHosts(""xxx.x.x.9:2181,xxx.x.x.1:2181,xxx.x.x.2:2181""), ""topic_name"");
			spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
			spoutConfig.forceFromStart = true;
                        spoutConfig.fetchSizeBytes = 147483600;
			kafkaSpout = new OpaqueTridentKafkaSpout(spoutConfig);

I get this runtime exception from one of the workers :-

java.lang.RuntimeException: storm.kafka.UpdateOffsetException at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) at clojure.lang.AFn.run(AFn.java:22) at java.lang.Thread.run(Thread.java:745) Caused by: storm.kafka.UpdateOffsetException at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:186) at storm.kafka.trident.TridentKafkaEmitter.fetchMessages(TridentKafkaEmitter.java:132) at storm.kafka.trident.TridentKafkaEmitter.doEmitNewPartitionBatch(TridentKafkaEmitter.java:113) at storm.kafka.trident.TridentKafkaEmitter.failFastEmitNewPartitionBatch(TridentKafkaEmitter.java:72) at storm.kafka.trident.TridentKafkaEmitter.emitNewPartitionBatch(TridentKafkaEmitter.java:79) at storm.kafka.trident.TridentKafkaEmitter.access$000(TridentKafkaEmitter.java:46) at storm.kafka.trident.TridentKafkaEmitter$1.emitPartitionBatch(TridentKafkaEmitter.java:204) at storm.kafka.trident.TridentKafkaEmitter$1.emitPartitionBatch(TridentKafkaEmitter.java:194) at storm.trident.spout.OpaquePartitionedTridentSpoutExecutor$Emitter.emitBatch(OpaquePartitionedTridentSpoutExecutor.java:127) at storm.trident.spout.TridentSpoutExecutor.execute(TridentSpoutExecutor.java:82) at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:370) at backtype.storm.daemon.executor$fn__5694$tuple_action_fn__5696.invoke(executor.clj:690) at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:436) at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:127) ... 6 more

But when i set spoutConfig.forceFromStart = true... It works fine for a while and then fails throwing this exception. I need a trident topology to give out accurate exactly-once processing even when the topology is restarted. 

As per some posts, I have tried setting spoutConfig :-

spoutConfig.maxOffsetBehind = Long.MAX_VALUE;

spoutConfig.startOffsetTime = kafka.api.OffsetRequest.EarliestTime();

My Kafka retention time is default - 128 Hours i.e. 7 Days and kafka producer is sending 6800 messages/second to Storm/Trident topology. I have gone through most of the posts, but none of them seem to solve this issue. 
"
STORM-1363,TridentKafkaState should handle null values from TridentTupleToKafkaMapper.getMessageFromTuple(),"If you look at the updateState API of storm.kafka.trident.TridentKafkaState. When producer is sending data its not handling if the null value is sent by mapper.getMessageFromTuple(tuple). Results into Kafka topic gets value as ""null"" string. There might be case in particular kind of exception user do not want to replay tuple and just report it and with that he needs to return null.

Also make the members as protected as I need to copy-paste the class to provide my implementation.

My updateState API looks like this

{code}
public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
      String topic = null;
		for (TridentTuple tuple : tuples) {
			if(tuple==null) {
				continue;
			}

			Object keyFromTuple = null;
			try {
				keyFromTuple = mapper.getKeyFromTuple(tuple);
				topic = topicSelector.getTopic(tuple);
				Object messageFromTuple = mapper.getMessageFromTuple(tuple);
				if (topic != null && messageFromTuple != null) {
					producer.send(new KeyedMessage(topic, keyFromTuple, messageFromTuple));
				} else {
					LOG.warn(""skipping key = "" + keyFromTuple + "", topic selector returned null."");
				}
			} catch (Exception ex) {
				String errorMsg = ""Could not send message with key = "" + keyFromTuple + "" to topic = "" + topic;
				LOG.warn(errorMsg, ex);
				throw new FailedException(errorMsg, ex);
			}
		}
	}
{code}"
STORM-1355,Storm Kafka Sport Can't Emit Message,Using Kafka-Spout reading a topic after some time spout can't read data any more 
STORM-1350,Unable to build: Too many files with unapproved licenses (RAT exception),"Issue when making build from the master branch. 

[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.11:check (default) on project storm: Too many files with unapproved license: 7 See RAT report in: /Users/myusername/Documents/apache-storm/target/rat.txt -> [Help 1]

Simple workaround is to build with maven flag to skip rat check:
mvn clean install -Drat.skip=true"
STORM-1344,"storm-jdbc build error ""object name already exists: USER_DETAILS in statement""","```
[ERROR] Failed to execute goal org.codehaus.mojo:sql-maven-plugin:1.5:execute (create-db) on project storm-jdbc: object name already exists: USER_DETAILS in statement [ /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * ""License""); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an ""AS IS"" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */
[ERROR] create table user_details (id integer, user_name varchar(100), create_date date)]
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.codehaus.mojo:sql-maven-plugin:1.5:execute (create-db) on project storm-jdbc: object name already exists: USER_DETAILS in statement [ /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * ""License""); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an ""AS IS"" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */
 create table user_details (id integer, user_name varchar(100), create_date date)]
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
	at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: object name already exists: USER_DETAILS in statement [ /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * ""License""); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an ""AS IS"" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */
 create table user_details (id integer, user_name varchar(100), create_date date)]
	at org.codehaus.mojo.sql.SqlExecMojo.execute(SqlExecMojo.java:681)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
	... 20 more
Caused by: java.sql.SQLSyntaxErrorException: object name already exists: USER_DETAILS in statement [ /** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements.  See the NOTICE file * distributed with this work for additional information * regarding copyright ownership.  The ASF licenses this file * to you under the Apache License, Version 2.0 (the * ""License""); you may not use this file except in compliance * with the License.  You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an ""AS IS"" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */
 create table user_details (id integer, user_name varchar(100), create_date date)]
	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source)
	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source)
	at org.hsqldb.jdbc.JDBCStatement.fetchResult(Unknown Source)
	at org.hsqldb.jdbc.JDBCStatement.execute(Unknown Source)
	at org.codehaus.mojo.sql.SqlExecMojo.execSQL(SqlExecMojo.java:1056)
	at org.codehaus.mojo.sql.SqlExecMojo.runStatements(SqlExecMojo.java:1018)
	at org.codehaus.mojo.sql.SqlExecMojo.access$200(SqlExecMojo.java:68)
	at org.codehaus.mojo.sql.SqlExecMojo$Transaction.runTransaction(SqlExecMojo.java:1252)
	at org.codehaus.mojo.sql.SqlExecMojo$Transaction.access$100(SqlExecMojo.java:1199)
	at org.codehaus.mojo.sql.SqlExecMojo.execute(SqlExecMojo.java:647)
	... 22 more
Caused by: org.hsqldb.HsqlException: object name already exists: USER_DETAILS
	at org.hsqldb.error.Error.error(Unknown Source)
	at org.hsqldb.error.Error.error(Unknown Source)
	at org.hsqldb.SchemaObjectSet.checkAdd(Unknown Source)
	at org.hsqldb.SchemaManager.checkSchemaObjectNotExists(Unknown Source)
	at org.hsqldb.StatementSchema.setOrCheckObjectName(Unknown Source)
	at org.hsqldb.StatementSchema.getResult(Unknown Source)
	at org.hsqldb.StatementSchema.execute(Unknown Source)
	at org.hsqldb.Session.executeCompiledStatement(Unknown Source)
	at org.hsqldb.Session.executeDirectStatement(Unknown Source)
	at org.hsqldb.Session.execute(Unknown Source)
	... 30 more
[ERROR]
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
```"
STORM-1343,Add a matrices for Trident which actually counts number of messages processed.,"If we keep trident topology running without pumping any messages in Kafka. Storm UI still show increased count. After some investigation we found one blog mentioning that its not actual processed message count  https://github.com/miguno/kafka-storm-starter/issues/5

As user, its very confusing what is this count and some time it gets misinterpreted. We have seen core storm is showing the count correctly. As trident is abstract level of core storm can't we use those matrices? 

We also found a blog where user can manually add the matrices to his code which will come up to the Storm UI. http://www.bigdata-cookbook.com/post/72320512609/storm-metrics-how-to 

Can we implement processed tuple matrices in trident directly? That will helful to end user in understanding what is topology doing & what needs to done to increase EPS of particular component. "
STORM-1300,port  backtype.storm.scheduler.resource-aware-scheduler-test to java,Test RAS
STORM-1216,button to kill all topologies in Storm UI,"In the Storm-on-Mesos project we had a [request to have an ability to ""shut down the storm cluster"" via a UI button|https://github.com/mesos/storm/issues/46].   That could be accomplished via a button in the Storm UI to kill all of the topologies.

I understand if this is viewed as an undesirable feature, but I just wanted to document the request."
STORM-1213,Remove sigar binaries from source tree,"In {{external/storm-metrics}} sigar native binaries were added to the source tree. Since Apache releases are source-only, these binaries can't be included in a release.

My initial thought was just to exclude the binaries from the source distribution, but that would mean that distributions built from a source tarball would not match the convenience binaries from a release (the sigar native binaries would not be included.

The solution I came up with was to leverage the fact that pre-built native binaries are included in the sigar maven distribution ({{sigar-x.x.x-native.jar}}) and use the maven dependency plugin to unpack them into place during the build, rather than check them into git. One benefit is that it will ensure the versions of the sigar jar and the native binaries match. Another is that mavens checksum/signature checking mechanism will also be applied.

This isn't an ideal solution since the {{sigar-x.x.x-native.jar}} only includes binaries for linux, OSX, and solaris (notably missing windows DLLs), whereas the non-maven sigar download includes support for a wider range of OSes and architectures.

I view this as an interim measure until we can find a better way to include the native binaries in the build process, rather than checking them into the source tree -- which would be a blocker for releasing."
STORM-1210,Set Output Stream id in KafkaSpout,"topicAsStreamId can only set output stream id to topic name. In some case ,we need to set output stream id to other name."
STORM-1209,Generalize StringMultiSchemeWithTopic,"STORM-817 allows a KafkaSpout to listen to multiple topics.

In this scenario, it makes sense to append the kafka topic to the tuples sent to storm.

This is partially implemented as `StringMultiSchemeWithTopic`. The idea is great, but not flexible enough, since it only works for Strings.

This should be generalized to work with any Scheme.

My proposed solution is available on https://github.com/apache/storm/pull/883"
STORM-1208,UI: NPE seen when aggregating bolt streams stats,"A stack trace is seen on the UI via its thrift connection to nimbus.

On nimbus, a stack trace similar to the following is seen:

{noformat}
2015-11-09 19:26:48.921 o.a.t.s.TThreadPoolServer [ERROR] Error occurred during processing of message.
java.lang.NullPointerException
        at backtype.storm.stats$agg_bolt_streams_lat_and_count$iter__2219__2223$fn__2224.invoke(stats.clj:346) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:?]
        at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:?]
        at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.6.0.jar:?]
        at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6078.invoke(protocols.clj:54) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6031$G__6026__6044.invoke(protocols.clj:13) ~[clojure-1.6.0.jar:?]
        at clojure.core$reduce.invoke(core.clj:6289) ~[clojure-1.6.0.jar:?]
        at clojure.core$into.invoke(core.clj:6341) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$agg_bolt_streams_lat_and_count.invoke(stats.clj:344) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.stats$agg_pre_merge_comp_page_bolt.invoke(stats.clj:439) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.stats$fn__2578.invoke(stats.clj:1093) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.MultiFn.invoke(MultiFn.java:241) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyToHelper(AFn.java:165) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]
        at clojure.core$apply.invoke(core.clj:628) ~[clojure-1.6.0.jar:?]
        at clojure.core$partial$fn__4230.doInvoke(core.clj:2470) ~[clojure-1.6.0.jar:?]
        at clojure.lang.RestFn.invoke(RestFn.java:421) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6086.invoke(protocols.clj:143) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6057$G__6052__6066.invoke(protocols.clj:19) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6078.invoke(protocols.clj:54) ~[clojure-1.6.0.jar:?]
        at clojure.core.protocols$fn__6031$G__6026__6044.invoke(protocols.clj:13) ~[clojure-1.6.0.jar:?]
        at clojure.core$reduce.invoke(core.clj:6289) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$aggregate_comp_stats_STAR_.invoke(stats.clj:1106) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.AFn.applyToHelper(AFn.java:165) ~[clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]
        at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$fn__2589.doInvoke(stats.clj:1127) ~[storm-core-0.10.1.jar:0.10.1]
        at clojure.lang.RestFn.invoke(RestFn.java:436) ~[clojure-1.6.0.jar:?]
        at clojure.lang.MultiFn.invoke(MultiFn.java:236) ~[clojure-1.6.0.jar:?]
        at backtype.storm.stats$agg_comp_execs_stats.invoke(stats.clj:1303) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.daemon.nimbus$fn__5893$exec_fn__1502__auto__$reify__5917.getComponentPageInfo(nimbus.clj:1715) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3677) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3661) ~[storm-core-0.10.1.jar:0.10.1]
        at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-0.10.1.jar:0.10.1]
        at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-0.10.1.jar:0.10.1]
        at backtype.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:143) ~[storm-core-0.10.1.jar:0.10.1]
        at org.apache.thrift7.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285) [storm-core-0.10.1.jar:0.10.1]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_40]
        at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]
{noformat}
"
STORM-1195,Doesn't start storm nimbus,"After start nimbus it crashed 

2015-11-10 11:06:03.431 o.a.s.s.o.a.c.f.s.ConnectionStateManager [INFO] State change: CONNECTED
2015-11-10 11:06:03.472 b.s.d.nimbus [ERROR] Error on initialization of server service-handler
java.lang.RuntimeException: java.lang.RuntimeException: java.util.zip.ZipException: Not in GZIP format
<------>at backtype.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:54) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.utils.Utils.deserialize(Utils.java:80) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.cluster$maybe_deserialize.invoke(cluster.clj:246) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.cluster$mk_storm_cluster_state$reify__5120.storm_base(cluster.clj:426) ~[storm-core-0.10.0.jar:0.10.0]
<------>at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.7.0_55]
<------>at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[?:1.7.0_55]
<------>at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.7.0_55]
<------>at java.lang.reflect.Method.invoke(Method.java:606) ~[?:1.7.0_55]
<------>at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:?]
<------>at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus$transition_BANG_.invoke(nimbus.clj:205) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.daemon.nimbus$transition_BANG_.invoke(nimbus.clj:200) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.daemon.nimbus$fn__6583$exec_fn__1236__auto____6584.invoke(nimbus.clj:1018) ~[storm-core-0.10.0.jar:0.10.0]
<------>at clojure.lang.AFn.applyToHelper(AFn.java:156) ~[clojure-1.6.0.jar:?]
<------>at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]
<------>at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus$fn__6583$service_handler__6703.doInvoke(nimbus.clj:1010) [storm-core-0.10.0.jar:0.10.0]
<------>at clojure.lang.RestFn.invoke(RestFn.java:421) [clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:1368) [storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.daemon.nimbus$_launch.invoke(nimbus.clj:1399) [storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.daemon.nimbus$_main.invoke(nimbus.clj:1422) [storm-core-0.10.0.jar:0.10.0]
<------>at clojure.lang.AFn.applyToHelper(AFn.java:152) [clojure-1.6.0.jar:?]
<------>at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus.main(Unknown Source) [storm-core-0.10.0.jar:0.10.0]
Caused by: java.lang.RuntimeException: java.util.zip.ZipException: Not in GZIP format
<------>at backtype.storm.utils.Utils.gunzip(Utils.java:135) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:51) ~[storm-core-0.10.0.jar:0.10.0]
<------>... 23 more
Caused by: java.util.zip.ZipException: Not in GZIP format
<------>at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:164) ~[?:1.7.0_55]
<------>at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:78) ~[?:1.7.0_55]
<------>at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:90) ~[?:1.7.0_55]
<------>at backtype.storm.utils.Utils.gunzip(Utils.java:125) ~[storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.serialization.GzipThriftSerializationDelegate.deserialize(GzipThriftSerializationDelegate.java:51) ~[storm-core-0.10.0.jar:0.10.0]
<------>... 23 more
2015-11-10 11:09:59.655 b.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
<------>at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:336) [storm-core-0.10.0.jar:0.10.0]
<------>at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus$fn__6583$service_handler__6703.doInvoke(nimbus.clj:1010) [storm-core-0.10.0.jar:0.10.0]
<------>at clojure.lang.RestFn.invoke(RestFn.java:421) [clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:1368) [storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.daemon.nimbus$_launch.invoke(nimbus.clj:1399) [storm-core-0.10.0.jar:0.10.0]
<------>at backtype.storm.daemon.nimbus$_main.invoke(nimbus.clj:1422) [storm-core-0.10.0.jar:0.10.0]
<------>at clojure.lang.AFn.applyToHelper(AFn.java:152) [clojure-1.6.0.jar:?]
<------>at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]
<------>at backtype.storm.daemon.nimbus.main(Unknown Source) [storm-core-0.10.0.jar:0.10.0]
"
STORM-1194,java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory,"can not consume topic from Kafka using storm-kafka, failed with 

java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory
	at org.apache.log4j.Logger.getLogger(Logger.java:39) ~[log4j-over-slf4j-1.6.6.jar:1.6.6]
	at kafka.utils.Logging$class.logger(Logging.scala:24) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger$lzycompute(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.utils.Logging$class.debug(Logging.scala:51) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.debug(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:64) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:44) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOrMakeConnection(SimpleConsumer.scala:142) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:69) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:124) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[kafka_2.10-0.8.1.1.jar:na]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:77) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:67) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.PartitionManager.<init>(PartitionManager.java:83) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[storm-kafka-0.9.5.jar:0.9.5]
	at backtype.storm.daemon.executor$fn__3371$fn__3386$fn__3415.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.util$async_loop$fn__460.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
67167 [Thread-20-spout] ERROR backtype.storm.daemon.executor - 
java.lang.NoClassDefFoundError: Could not initialize class org.apache.log4j.Log4jLoggerFactory
	at org.apache.log4j.Logger.getLogger(Logger.java:39) ~[log4j-over-slf4j-1.6.6.jar:1.6.6]
	at kafka.utils.Logging$class.logger(Logging.scala:24) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger$lzycompute(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.logger(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.utils.Logging$class.debug(Logging.scala:51) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.debug(BlockingChannel.scala:35) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:64) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:44) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOrMakeConnection(SimpleConsumer.scala:142) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:69) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:124) ~[kafka_2.10-0.8.1.1.jar:na]
	at kafka.javaapi.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:79) ~[kafka_2.10-0.8.1.1.jar:na]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:77) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaUtils.getOffset(KafkaUtils.java:67) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.PartitionManager.<init>(PartitionManager.java:83) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:98) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69) ~[storm-kafka-0.9.5.jar:0.9.5]
	at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135) ~[storm-kafka-0.9.5.jar:0.9.5]
	at backtype.storm.daemon.executor$fn__3371$fn__3386$fn__3415.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.util$async_loop$fn__460.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]
67235 [Thread-20-spout] ERROR backtype.storm.util - Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$fn__4694$fn__4695.invoke(worker.clj:493) [storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.daemon.executor$mk_executor_data$fn__3272$fn__3273.invoke(executor.clj:240) [storm-core-0.9.5.jar:0.9.5]
	at backtype.storm.util$async_loop$fn__460.invoke(util.clj:473) [storm-core-0.9.5.jar:0.9.5]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_80]



Here's the pom.xml

ack=true</test.extra.args>

    <!-- dependency versions -->
    <clojure.version>1.5.1</clojure.version>
    <compojure.version>1.1.3</compojure.version>
    <hiccup.version>0.3.6</hiccup.version>
    <commons-io.version>2.4</commons-io.version>
    <commons-lang.version>2.5</commons-lang.version>
    <commons-exec.version>1.1</commons-exec.version>
    <curator.version>2.5.0</curator.version>
    <json-simple.version>1.1</json-simple.version>
    <ring.version>0.3.11</ring.version>
    <clojure.tools.logging.version>0.2.3</clojure.tools.logging.version>
    <clojure.math.numeric-tower.version>0.0.1</clojure.math.numeric-tower.version>
    <carbonite.version>1.4.0</carbonite.version>
    <httpclient.version>4.3.3</httpclient.version>
    <clojure.tools.cli.version>0.2.4</clojure.tools.cli.version>
    <disruptor.version>2.10.1</disruptor.version>
    <jgrapht.version>0.9.0</jgrapht.version>
    <guava.version>16.0.1</guava.version>
    <logback-classic.version>1.0.13</logback-classic.version>
    <mockito.version>1.9.5</mockito.version>
    <storm-core.version>0.9.5</storm-core.version>
    <storm-kafka.version>0.9.5</storm-kafka.version>
    <snakeyaml.version>1.16</snakeyaml.version>
    <storm.crawler.core.version>0.7</storm.crawler.core.version>

  </properties>

  <build>
    <sourceDirectory>src/jvm</sourceDirectory>
    <testSourceDirectory>test/jvm</testSourceDirectory>
    <resources>
      <resource>
        <directory>${basedir}/multilang</directory>
      </resource>
    </resources>

    <plugins>

      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
      </plugin>

      <!--
        Bind the maven-assembly-plugin to the package phase
        this will create a jar file without the storm dependencies
        suitable for deployment to a cluster.
       -->
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
          <archive>
            <manifest>
              <mainClass />
            </manifest>
          </archive>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.3.2</version>
        <executions>
          <execution>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <killAfter>-1</killAfter>
          <mainClass>${storm.topology}</mainClass>
        </configuration>
      </plugin>
    </plugins>
  </build>

  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.testng</groupId>
      <artifactId>testng</artifactId>
      <version>6.8.5</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
      <version>${mockito.version}</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.easytesting</groupId>
      <artifactId>fest-assert-core</artifactId>
      <version>2.0M8</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.jmock</groupId>
      <artifactId>jmock</artifactId>
      <version>2.6.0</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.twitter4j</groupId>
      <artifactId>twitter4j-stream</artifactId>
      <version>3.0.3</version>
    </dependency>

    <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka_2.10</artifactId>
      <version>0.8.1.1</version>
      <exclusions>
        <exclusion>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
        </exclusion>
        <exclusion>
          <groupId>log4j</groupId>
          <artifactId>log4j</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-core</artifactId>
      <version>${storm-core.version}</version>
      <scope>provided</scope>
    </dependency>

    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-kafka</artifactId>
      <version>${storm-kafka.version}</version>
      <exclusions>
        <exclusion>
          <groupId>org.apache.zookeeper</groupId>
          <artifactId>zookeeper</artifactId>
        </exclusion>

        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-simple</artifactId>
        </exclusion>

        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-sl4j12</artifactId>
        </exclusion>

      </exclusions>
    </dependency>

    <dependency>
      <groupId>org.apache.zookeeper</groupId>
      <artifactId>zookeeper</artifactId>
      <version>3.4.6</version>
      <exclusions>
        <exclusion>
          <groupId>com.sun.jmx</groupId>
          <artifactId>jmxri</artifactId>
        </exclusion>

        <exclusion>
          <groupId>com.sun.jdmk</groupId>
          <artifactId>jmxtools</artifactId>
        </exclusion>

        <exclusion>
          <groupId>javax.jms</groupId>
          <artifactId>jms</artifactId>
        </exclusion>

      </exclusions>
    </dependency>

    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.2.1</version>
    </dependency>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>${guava.version}</version>
    </dependency>
    <dependency>
      <groupId>org.yaml</groupId>
      <artifactId>snakeyaml</artifactId>
      <version>${snakeyaml.version}</version>
    </dependency>
    <dependency>
      <groupId>com.digitalpebble</groupId>
      <artifactId>storm-crawler-core</artifactId>
      <version>${storm.crawler.core.version}</version>
    </dependency>

    <dependency>
      <groupId>com.netflix.curator</groupId>
      <artifactId>curator-test</artifactId>
      <version>1.2.5</version>

      <exclusions>
        <exclusion>
          <groupId>org.slf4j</groupId>
          <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
        <exclusion>
          <groupId>log4j</groupId>
          <artifactId>log4j</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

  </dependencies>"
STORM-1190,System load spikes in recent snapshot,"We've been running Storm's snapshots on our production cluster for a little while now (that back pressure support really helped us), and we've noticed a sudden spike in system load when going from commit@ba1250993d10ffc523c9f5464371fbeb406d216f to the current latest commit@c12e28c829fcfabc0a3a775fb9714968b7e3e349. Both versions were running the exact same topologies, and there was no significant change in workload. Not exactly sure how to even begin to debug this, so we ended up just rolling back. Thoughts?

Stats screenshots attached"
STORM-1189,client.getClusterInfo() fails with Required field 'nimbus_uptime_secs' is unset!,"Hi,

we are about to upgrade our cluster from 0.9.1-incubating to 0.10.0. I'm currently testing whether the functionality is still working with 0.10.0 in VMs.

On 0.10.0 the following throws the exception mentioned below:
...
NimbusClient nimbusClient = NimbusClient.getConfiguredClient(readConfig);
Client client = nimbusClient.getClient();
ClusterSummary clusterInfo = client.getClusterInfo();

Exception:
org.apache.thrift7.protocol.TProtocolException: Required field 'nimbus_uptime_secs' is unset! Struct:ClusterSummary(supervisors:[SupervisorSummary(host:n3.t3k.siemens.com, uptime_secs:24297, num_workers:4, num_used_workers:0, supervisor_id:4a87f03b-ea91-4b12-8cb9-9f7e26703b26, version:0.10.0.2.3.0.0-2557)], nimbus_uptime_secs:0, topologies:[])
	at backtype.storm.generated.ClusterSummary.validate(ClusterSummary.java:515)
	at backtype.storm.generated.ClusterSummary$ClusterSummaryStandardScheme.read(ClusterSummary.java:613)
	at backtype.storm.generated.ClusterSummary$ClusterSummaryStandardScheme.read(ClusterSummary.java:549)
	at backtype.storm.generated.ClusterSummary.read(ClusterSummary.java:473)
	at backtype.storm.generated.Nimbus$getClusterInfo_result$getClusterInfo_resultStandardScheme.read(Nimbus.java:16546)
	at backtype.storm.generated.Nimbus$getClusterInfo_result$getClusterInfo_resultStandardScheme.read(Nimbus.java:16531)
	at backtype.storm.generated.Nimbus$getClusterInfo_result.read(Nimbus.java:16470)
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:78)
	at backtype.storm.generated.Nimbus$Client.recv_getClusterInfo(Nimbus.java:569)
	at backtype.storm.generated.Nimbus$Client.getClusterInfo(Nimbus.java:557)

Switching back to 0.9.1-incubating showed, that the code still works fine there.

BR Michael"
STORM-1188,PartialKeyGrouping missing from storm.thrift (and can't use it via custom_object),"I'm working on a Python DSL for Storm to add to streamparse, and as part of it I realized that the new partial key grouping was never added to the Grouping struct in storm.thrift, so it's not usable outside of JVM-based topology definitions (at least not easily).  My initial thought was to just use Grouping.custom_object, but the PartialKeyGrouping constructor takes a Fields object, which isn't a type defined in storm.thrift, so I can't use it.

The fields grouping explicitly takes a list of strings in storm.thrift, so it would seem PartialKeyGrouping needs to be added in the same way."
STORM-1185,Update storm.yaml with HA configuration,replace nimbus.host with nimbus.seeds
STORM-1184,Update storm.yaml with HA configuration,replace nimbus.host with nimbus.seeds
STORM-1183,Intermittent testInOrder failures seen in Travis builds,"DisruptorQueueTests::testInOrder has some assertions that pay attention to whether or not producer instances shut down quickly.  

(https://travis-ci.org/apache/storm/jobs/89506537)

Since the test case itself focuses the ordering of processing rather than speed, I'd suggest bumping TIMEOUT up from the current 1s to 10s... maybe consistent failures with such a large timeout would be more useful as visible test failures."
STORM-1180,FLUX logo wasn't appearing quite right,Added UTF-8 to the InputStreamReader so that special chars in the splash file would be read correctly.
STORM-1179,Create Maven Profiles for Integration Tests,
STORM-1177,Build fails because DISCLAIMER file is missing,"Commit #4a57e500b3e0c3f3256e776357d1e1de1c7f5e49 removed the DISCLAIMER file, causing `mvn package` to fail."
STORM-1172,Improve clojure.test handling of stdout/stderr to match JUnit behavior,It would be nice if clojure.test would include stdout/stderr in the test reports only when there is an error or failure.
STORM-1171,UI: number of workers possibly inaccurate after rebalance,"1. Describe observed behavior.

Topology Page shows the number of workers the topology was submitted with, but if it has been rebalanced since this number can be wrong.

2. What is the expected behavior?

Topology page should show current counts, not original counts.

3. Outline the steps to reproduce the problem.

Launch a topology with 3 workers. Rebalance -n 1, load cluster summary page and topology summary page to compare.

Check that the number of executors is correct. It is likely that they are wrong.
"
STORM-1169,Validate file ownership before setting up stormdist dir,"Created for [TODO comment|https://github.com/apache/storm/blob/f3568d73c8832cdf2a6a6ec06b929c8b7bb96c10/storm-core/src/native/worker-launcher/impl/worker-launcher.c#L494] in the code.

"
STORM-1168,worker-launcher logs noisy messages when visiting files,These messages can fill up a log and make it harder to debug real issues.
STORM-1162,Add tick tuples to HDFSBolt for time-based flushing,The current HDFSBolt implementation allows a FileSizeRotationPolicy or a TimedRotationPolicy. There are use cases where the implementation requires syncing based on time with a tick tuple.
STORM-1150,The authorization  mode of Logviewer is not working well when I add other groups for log,"I add a super group for logviewer with codes :
(defn authorized-log-user? [user fname conf]
  (if (or (blank? user) (blank? fname))
    nil
    (let [groups (user-groups user)
          [user-wl group-wl] (get-log-user-group-whitelist conf fname)
          logs-users (concat (conf LOGS-USERS)
                             (conf NIMBUS-ADMINS)
                             user-wl)
          logs-groups (concat (conf LOGS-GROUPS)
                              (conf NIMBUS-ADMIN-GROUPS)   //my super group
                              group-wl)]
       (or (some #(= % user) logs-users)
           (< 0 (.size (intersection (set groups) (set group-wl))))))))

maybe the ""(set group-wl)"" should be ""(set  logs-groups)""?"
STORM-1148,Storm.py - References to storm.incubator.apache.org,The storm.py script references storm.incubator.apache.org URLs in numerous locations.  Suggest we update to storm.apache.org and verify all URLs are still correct.
STORM-1147,Storm JDBCBolt should add validation to ensure either insertQuery or table name is specified and not both.,The JDBCBolt takes either an insert query or table name but does not do any validation check to ensure only one of the two option is provided. We should add a validation check and throw an exception with proper messaging to avoid confusion.
STORM-1142,Some config validators for positive ints need to allow 0,"* topology.tasks
* topology.acker.executors
* topology.eventlogger.executors

Created on behalf of [~jerrypeng]"
STORM-1134,Windows: Fix log4j config.,
STORM-1127,Allow a Boolean value in a constructorArgs list (Flux),"Allow the constructorArgs implementation to handle receiving a boolean argument in the args list.

{noformat}
constructorArgs
- true
{noformat}"
STORM-1126,Allow a configMethod that takes no arguments (Flux),"Allow the configMethod implementation to handle calling a method that requires no arguments, by omitting the ""args"" line.

{noformat}
configMethods
- name: ""myMethod""
{noformat}"
STORM-1123,TupleImpl - Unnecessary variable initialization,"Within the backtype.storm.tuple.TupleImpl class, the following variables are set to null in the constructor or variable definition.  This is not necessary as they are null by default per the Java language specification.

_processSampleStartTime 
_executeSampleStartTime 
_meta 

Simple optimization."
STORM-1120,main-routes in backtype.storm.ui.core uses wrong keyword - schema,"We're using both 'schema' and 'scheme' keywords from main-routes but [~knusbaum] confirmed that 'scheme' is correct.

https://github.com/apache/storm/pull/717#issuecomment-146656531"
STORM-1116,"storm-hbase doesn't create a table for you, it would be great to have an option to create a table and optionally pre-split the table by passing a splitting algorithm to it.","HBase has a utility to create and pre-split a table.
hbase org.apache.hadoop.hbase.util.RegionSplitter table_name HexStringSplit -c 30 -f cf

The other algorithm choice is UniformSplit. User can also implement a custom algorithm. It would be great to be able to pass an option to create a table, perhaps in the prepare() method as well as pre-split the table based on the passed algorithm.

Something like this ""hbaseMapper().withCreateTable(numRegions, columnFamily, algorithm) where algorithm is UniformSplit, HexStringSplit and/or a custom algorithm implementation"
STORM-1115,Stale leader-lock key effectively bans all nodes from becoming leaders,"I believe this curator bug is what's in play causing the above described situation.

https://issues.apache.org/jira/browse/CURATOR-202

Whenever we were hit by this bug we'd start seeing problems in submitting topologies to nimbus, as well as having problems activating/deactivating/killing topologies.  Basically any topology that utilizes the `is-leader` macro, since no nimbus believes itself to be the leader based on LeaderLatch.hasLeadership()"
STORM-1109,Worker exists frequently due to java.net.SocketTimeoutException,"One of the supervisor is exiting frequently , following is the log

015-10-13T20:57:20.245+0530 k.c.SimpleConsumer [INFO] Reconnect due to socket error: null
2015-10-13T20:57:30.284+0530 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.net.SocketTimeoutException
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:146) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.fill(PartitionManager.java:134) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.next(PartitionManager.java:108) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:123) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__6579$fn__6594$fn__6623.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
Caused by: java.net.SocketTimeoutException: null
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:229) ~[na:1.7.0_67]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[na:1.7.0_67]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[na:1.7.0_67]
        at kafka.utils.Utils$.read(Utils.scala:395) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54) ~[stormjar.jar:na]
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) ~[stormjar.jar:na]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:71) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:108) ~[stormjar.jar:na]
        at kafka.javaapi.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:48) ~[stormjar.jar:na]
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:141) ~[stormjar.jar:na]
        ... 7 common frames omitted
2015-10-13T20:57:30.285+0530 b.s.d.executor [ERROR]
java.lang.RuntimeException: java.net.SocketTimeoutException
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:146) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.fill(PartitionManager.java:134) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.next(PartitionManager.java:108) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:123) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__6579$fn__6594$fn__6623.invoke(executor.clj:565) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
Caused by: java.net.SocketTimeoutException: null
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:229) ~[na:1.7.0_67]
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103) ~[na:1.7.0_67]
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[na:1.7.0_67]
        at kafka.utils.Utils$.read(Utils.scala:395) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54) ~[stormjar.jar:na]
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56) ~[stormjar.jar:na]
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29) ~[stormjar.jar:na]
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:71) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:110) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:109) ~[stormjar.jar:na]
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33) ~[stormjar.jar:na]
        at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:108) ~[stormjar.jar:na]
        at kafka.javaapi.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:48) ~[stormjar.jar:na]
        at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:141) ~[stormjar.jar:na]
        ... 7 common frames omitted
2015-10-13T20:57:47.906+0530 b.s.util [ERROR] Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__7028$fn__7029.invoke(worker.clj:497) [storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.executor$mk_executor_data$fn__6480$fn__6481.invoke(executor.clj:240) [storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:473) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
2015-10-13T20:57:47.907+0530 b.s.d.worker [INFO] Shutting down worker


this is usually accompanied by the following error is supervisor.

2015-10-13T20:58:21.858+0530 b.s.d.supervisor [INFO] Shutting down fa862fbe-bfd2-4b53-9abb-cd951303ecb8:42ccaf83-6a17-4dcb-92e6-47416486806d
2015-10-13T20:58:21.869+0530 b.s.event [ERROR] Error when processing event
java.io.IOException: . doesn't exist.
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:157) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147) ~[commons-exec-1.1.jar:1.1]
        at backtype.storm.util$exec_command_BANG_.invoke(util.clj:386) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$send_signal_to_process.invoke(util.clj:415) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$kill_process_with_sig_term.invoke(util.clj:426) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:197) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:267) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2625.invoke(event.clj:40) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
2015-10-13T20:58:21.877+0530 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2625.invoke(event.clj:48) [storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]


"
STORM-1106,Netty Client Connection Attempts should not be limited,The workers should not give-up making connection with other workers. This could cause the worker to be blocked forever.
STORM-1104,Nimbus HA fails to find newly downloaded code files,"Nimbus HA using Local File System code distribution is broken. We seem to be ""caching"" the return value of `(code-ids (:conf nimbus))` in the sync-code method, by overriding the `code-ids` var from a method (#854 `defn code-ids`), to a local var (#1669 `code-ids (set (code-ids (:conf nimbus)))`).

The problem is, that after downloading code for missing topologies, sync-code doesn't realize it has gotten the new missing topology files."
STORM-1103,Utils#getConfigFileInputStream can print INFO level logs each time it is called,"https://github.com/apache/storm/blob/86f2d03c2060f25ccdaf7580cfee5d9f1a9ac2e3/storm-core/src/jvm/backtype/storm/utils/Utils.java#L253

This can result in fairly noisy logs.  It would be good to move this to DEBUG level."
STORM-1096,"UI tries to impersonate wrong user when getting topology conf for authorization, impersonation is allowed by default","We have started using 0.10.0 under load and found a few issues around the UI and impersonation.

The UI when trying to connect to nimbus will impersonate other users.  Nimbus, by default allows impersonation and just outputs a warning message that it is allowed.  We really should default to not allowing impersonation.  having the authorizer configured by default does not hurt when running insecure because impersonation is not possible, but when security is enabled if someone forgets to set this config we are now insecure by default.

If you do set all of that up correctly the UI now can impersonate the wrong user when connecting to nimbus.

The UI decides which user to impersonate by pulling it from the request context.  The requestContext is populated from the HttpRequest when assert-authorized-user is called.  assert-authorized-user takes a topology-conf as a parameter.  The only way to get this topology conf is to talk to nimbus, which will get the wrong user because the request context has not been populated yet.

This just because a huge pain for users who way too often will not be able to see pages on the UI."
STORM-1094,kafka offset does not advance when deserializer yields no object ,"If a custom deserializer returns an empty list of objects as a result of deserializing messages from kafka, the partition manager does not advance the kafka offset, and the same message will continue to be deserialized and return nothing.  The code works if the deserializer returns null, but not if it returns an empty list."
STORM-1090,Nimbus HA should support `storm.local.hostname`,Nimbus HA's `NimbusInfo` class relies on each nimbus's hostname for network reachability. This is a show-stopper in situations that utilize Config.STORM_LOCAL_HOSTNAME /  `storm.local.hostname`.
STORM-1078,RateTracker.java is not thread safe,"The RateTracker class is not thread safe at all.  It may not be that big of a deal, but the rates will be off if we notify from multiple threads, like we do with disruptor.  It also has the potential to be way off if notify is being called at the same time as updateSlides.  This would result in the new bucket not being set to 0, but getting the old value that was there previously.

We want to be very careful that what we do does not impact the performance too much.  So ideally no big locks but use AtomicLongs instead."
STORM-1072,Nimbus gives incomplete cluster data to scheduler (hides dead worker slots),"1. Describe observed behavior.

Certain slots that have been assigned but have workers that have not yet sent a heartbeat are treated as ""dead"" slots, and these are not included in the cluster summary data that is passed to the scheduler.

[link|https://github.com/apache/storm/blob/8dd9e6e213210009968f39483cb69f271b2e8415/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L527] to nimbus code

For topologies whose payload is very large, this can result in scheduler results that never quite converge due to some of the slots not appearing on each call to schedule()


2. What is the expected behavior?

Nimbus may be too smart here: it seems better to give the full cluster information to the scheduler and let the scheduler make the appropriate decision about how to handle workers that are not yet up.

3. Outline the steps to reproduce the problem.

Either launch a topology with a very large jar file that takes minutes to download, or simulate by adding a sleep to the supervisor code just after the jar is downloaded.  This will cause a significant delay before the worker is up and heartbeating in.  On each scheduling run, such slots will not even be present for the scheduler logic.
"
STORM-1063,Workers cannot find log4j configuration files since default value of storm.log4j2.conf.dir is relative,"For current 0.11.0-SNAPSHOT, default value of storm.log4j2.conf.dir is ""log4j2"".

The problem is, it's never converted to absolute path.
Workers try to find cluster.xml or worker.xml from log4j2 directory with relative path.

In my experience, worker logs were printed to supervisor.log file, and it prints some error logs related to this issue.

{quote}
3568678 [Thread-25] WARN  b.s.util - Worker Process 44b1e4e1-67f4-4d44-be80-72a06c217669:990428 [Thread-2] INFO  STDERR - 2015-09-23 07:53:39,350 ERROR File not found in file system or classpath: log4j2/worker.xml
{quote}

Other daemons print logs successfully, but its format is weird.
(Maybe it picked default configuration.)

0.10.0 is not affected since it doesn't define default value of storm.log4j2.conf.dir.
storm.py concatenates STORM_DIR and log4j2 if storm.log4j2.conf.dir is not defined.

It came from STORM-976, and relevant PR is https://github.com/apache/storm/pull/684
(STORM-976 seems not backported to 0.10.x-branch.)"
STORM-1055,storm-jdbc README needs fixes and context,
STORM-1051,Netty Client.java's flushMessages produces a NullPointerException,"STORM-763 replaced `return batch != null && !batch.isEmpty();` with `if(batch.isEmpty())`... which means that if batch == null, a NullPointerException is thrown. Problem is, batch is often null, which means that 763 made Storm unusable..."
STORM-1050,Topologies with same name run on one cluster,"When a topology is submitted repeatedly very closely, or Nimbus is too busy to response the repeated submission RPC calls from the same topology, there is very high probability that Nimbus will receive the repeatedly calls from one topology, finally several topologies with same name will be running on one storm cluster."
STORM-1049,Document central classes of storm-kafka,"The `storm-kafka` project currently is very hard to use because central classes, like `KafkaSpout` and `PartitionManager` are completely documentation-free. At least these classes should be well documented, so that it's possible to guess from the code what important methods do."
STORM-1048,Add generic type to conf arugment of ISpout.open,"Changing the signature of `ISpout.open` from `void open(Map conf, TopologyContext context, SpoutOutputCollector collector)` to `void open(Map<Object,Object> conf, TopologyContext context, SpoutOutputCollector collector)` would allow, but not force callers to write warning-free code. The change should be backwards compatible because callers can ignore it like storm currently does in the `ISpout` interface.

The changes would only avoid compiler warnings about unchecked conversations and rawtypes. It would make the code longer and there'd be no functional change as `Map` defaults to `Map<Object, Object>` after type erasure.

Alternatively a generic type for the key and the value of `conf` could be introduced in `ISpout`.

I volunteer to do it in the complete source."
STORM-1047,document internals of bin/storm.py,"The `python` script `bin/storm.py` is completely undocumented regarding its internals. Function comments only include a command line interface often omitting an explanation of arguments and their default values (e.g. it should be clear why the default value of `klass` of `nimbus` is `""backtype.storm.daemon.nimbus""` because that doesn't make sense to someone unfamiliar with the storm-core implementation).

Also explanations like ""Launches the nimbus daemon. [...]"" (again `nimbus` function) is good for a command line API doc, but insufficient for a function documentation (should mention that it starts a `java` process and passes `klass` as class name to it).

How does the script use `lib/`, `extlib/` and `extlib-daemon`? It's too complex to squeeze this info out of the source code."
STORM-1043,Concurrent access to state on local FS by multiple supervisors,"Hi,

we are running storm-mesos cluster and occassionaly workers die or are ""lost"" in mesos. When this happens it often coincides with errors in logs related to supervisors local state.

By looking at the storm code it seems this might be caused by the way how multiple supervisor processes access the local state in the same directory via VersionedStore.

For example: https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/daemon/supervisor.clj#L434

Here every supervisor does this concurrently:
1. reads latest state from FS
2. possibly updates the state
3. writes the new version of the state

Some updates could be lost if there are 2+ supervisors and they execute above steps concurrently - then only the updates from last supervisor would remain on the last state version on the disk.

We observed local state changes quite often (seconds), so the likelihood of this concurrency issue occurring is high.

Some examples of exeptions:
------------------------------------------
java.lang.RuntimeException: Version already exists or data already exists
at backtype.storm.utils.VersionedStore.createVersion(VersionedStore.java:85) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.VersionedStore.createVersion(VersionedStore.java:79) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.persist(LocalState.java:101) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.put(LocalState.java:82) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.put(LocalState.java:76) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.daemon.supervisor$mk_synchronize_supervisor$this7400.invoke(supervisor.clj:382) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.event$event_manager$fn2625.invoke(event.clj:40) ~[storm-core-0.9.5.jar:0.9.5]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]

---------------------------------------
java.io.FileNotFoundException: File '/var/lib/storm/supervisor/localstate/1441034838231' does not exist
at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
at backtype.storm.utils.LocalState.deserializeLatestVersion(LocalState.java:61) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.snapshot(LocalState.java:47) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.utils.LocalState.get(LocalState.java:72) ~[storm-core-0.9.5.jar:0.9.5]
at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:234) ~[storm-core-0.9.5.jar:0.9.5]
at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
at clojure.core$partial$fn4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
at backtype.storm.event$event_manager$fn2625.invoke(event.clj:40) ~[storm-core-0.9.5.jar:0.9.5]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_60]
-----------------------------------------"
STORM-1042,clj-time.local compilation error,"In a Clojure project with a storm-core 0.10.0-beta1 dependency, attempting to compile clj-time.local throws a compiler exception, as follows:

(require '[clj-time.local :as l])
CompilerException java.lang.ClassCastException: org.apache.storm.joda.time.format.DateTimeFormatter cannot be cast to org.joda.time.format.DateTimeFormatter, compiling:(local.clj:35:3) 

Same error happens if the project is using clj-time brought in with storm-core, and with an explicit dep of clj-time."
STORM-1041,Topology with kafka spout stops processing,"Topology:
 KafkaSpout (1 task/executor) -> bolt that does grouping (1 task/executor) -> bolt that does processing (176 tasks/executors)
 8 workers
 Using Netty

Sometimes when a worker dies (we've seen it happen due to an OOM or load from a co-located worker) it will try to restart on the same node, then 20s later shutdown and start on another node.

{code}
2015-09-10 08:05:41,131 -0700 INFO        backtype.storm.daemon.supervisor:0 - Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""NoticeProcessorTopology-368-1441856754"", :executors ([9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225])} for this supervisor 8a845b9b-adaa-4943-b6a6-68fdadcc5146 on port 6701 with id 42a499b2-2c5c-43c2-be8a-a5b3f4f8a99e
2015-09-10 08:05:39,953 -0700 INFO        backtype.storm.daemon.supervisor:0 - Shutting down and clearing state for id 39c28ee2-abf9-4834-8b1f-0bd6933412e8. Current supervisor time: 1441897539. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1441897539, :storm-id ""NoticeProcessorTopology-368-1441856754"", :executors #{[9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [-1 -1] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225]}, :port 6700}
2015-09-10 08:05:22,693 -0700 INFO        backtype.storm.daemon.supervisor:0 - Launching worker with assignment #backtype.storm.daemon.supervisor.LocalAssignment{:storm-id ""NoticeProcessorTopology-368-1441856754"", :executors ([9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225])} for this supervisor f26e1fae-03bd-4fa8-9868-6a54993f3c5d on port 6700 with id 39c28ee2-abf9-4834-8b1f-0bd6933412e8
2015-09-10 08:05:21,588 -0700 INFO        backtype.storm.daemon.supervisor:0 - Shutting down and clearing state for id 4f0e4c22-6ccc-4d78-a20f-88bffb8def1d. Current supervisor time: 1441897521. State: :timed-out, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1441897490, :storm-id ""NoticeProcessorTopology-368-1441856754"", :executors #{[9 9] [41 41] [73 73] [105 105] [137 137] [169 169] [201 201] [17 17] [49 49] [81 81] [113 113] [145 145] [177 177] [209 209] [25 25] [57 57] [89 89] [121 121] [153 153] [185 185] [217 217] [-1 -1] [1 1] [33 33] [65 65] [97 97] [129 129] [161 161] [193 193] [225 225]}, :port 6700}
{code}

While the worker was dead and then killed, other workers have had netty drop messages. In theory these messages should timeout and be replayed. Our message timeout is 30s. 

{code}
2015-09-10 08:05:50,914 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone33-prod.amz.relateiq.com/10.30.101.36:6701
2015-09-10 08:05:44,904 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone33-prod.amz.relateiq.com/10.30.101.36:6701
2015-09-10 08:05:43,902 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone39-prod.amz.relateiq.com/10.30.101.5:6700
2015-09-10 08:05:27,873 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone39-prod.amz.relateiq.com/10.30.101.5:6700
2015-09-10 08:05:27,873 -0700 ERROR       b.storm.messaging.netty.Client:453 - dropping 1 message(s) destined for Netty-Client-usw2b-grunt-drone39-prod.amz.relateiq.com/10.30.101.5:6700
{code}

However these messages never timeout, and the MAX_SPOUT_PENDING has been reached, so no more tuples are emitted/processed.

"
STORM-1038,Upgrade netty transport from 3.x to 4.x,It will be nice to upgrade netty to 4.x to take advantage of its more efficient memory usage.
STORM-1034,Create Fluent API for Kafka - SpoutConfig & TridentKafkaConfig ,"Create Fluent Java API for construction of SpoutConfig & TridentSpountConfig.  At present both of these classes have public instance variables and as such instantiation and validation is poor.

The use of a Fluent Java API will provide cleaner validation and the opportunity to deprecate properties while maintaining API compatibility.

Example:

{code:title=Bar.java|borderStyle=solid}
SpoutConfig simpleConfig = new SpountConfigBuilder()
                                                        .topicName(""topicA"")
                                                        .kafkaBrokerHosts(""hostname:2181"")
                                                        .stormClientID(""storm-client-id"")
                                                         .schema(new SchemeAsMultiScheme ())
                                                         .build();
{code}
 "
STORM-1031,Cleanup for temporary directories during unit tests,"During execution of unit tests, temporary directories are created in the users TEMP/TMP directory.  The clean process does not remove these as it has no knowledge of the directory names.  Given the size of these scratch directories, it will eventually fill up the user's hard drive.

I believe this is caused by the clojure-maven-plugin but have not confirmed.

An acceptable work around would be to at least prefix the temporary directories so they could be safely deleted.
"
STORM-1027,Topology may hang because metric-tick function is a blocking call from spout,"Nathan had fixed the dining philosopher problem by putting a overflow buffer in the spout so that spout is not blocking. However, overflow buffer is not used when emitting the metric, and that could result in the deadlock. I modified the executor to use overflow buffer for emitting metrics and afterwards topology didn't hang. "
STORM-1026,Adding external classpath elements does not work,"Adding an external path and/or jar to the classpath fails to work as expected.  I would expect it to work the same as {noformat}
""${STORM_DIR}/extlib""{noformat} and {noformat}""${STORM_DIR}/extlib-daemon""{noformat}, and create a complete list of all jars in the path spec(s). 

Here is an example of the issue:

{code}
% STORM_EXT_CLASSPATH=/usr/lib/jvm/default-java/lib/jconsole.jar python
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import os
>>> import fnmatch
>>> os.getenv('STORM_EXT_CLASSPATH', None)
'/usr/lib/jvm/default-java/lib/jconsole.jar'
>>> ret = fnmatch.filter(os.listdir('/opt/HipChat/lib/'), ""*.so"")
>>> ret
['libxml2.so', 'libogg.so', 'libdbusmenu-qt5.so', 'libhipchatdbusscripting.so', 'libcanberra.so', 'libvorbis.so', 'libxcb-sync.so', 'libxslt.so', 'liblzma.so', 'libqxmpp.so', 'libvorbisfile.so', 'libssl.so', 'libpng16.so', 'libsqlite3.so', 'libz.so', 'libadl_sdk.so', 'libtdb.so', 'libuuid.so', 'libcrypto.so', 'libcanberraSoundNotification.so']
>>> ret.extend(os.getenv('STORM_EXT_CLASSPATH', None))
>>> ret
['libxml2.so', 'libogg.so', 'libdbusmenu-qt5.so', 'libhipchatdbusscripting.so', 'libcanberra.so', 'libvorbis.so', 'libxcb-sync.so', 'libxslt.so', 'liblzma.so', 'libqxmpp.so', 'libvorbisfile.so', 'libssl.so', 'libpng16.so', 'libsqlite3.so', 'libz.so', 'libadl_sdk.so', 'libtdb.so', 'libuuid.so', 'libcrypto.so', 'libcanberraSoundNotification.so', '/', 'u', 's', 'r', '/', 'l', 'i', 'b', '/', 'j', 'v', 'm', '/', 'd', 'e', 'f', 'a', 'u', 'l', 't', '-', 'j', 'a', 'v', 'a', '/', 'l', 'i', 'b', '/', 'j', 'c', 'o', 'n', 's', 'o', 'l', 'e', '.', 'j', 'a', 'r']
{code}"
STORM-1023,Nimbus server hogs 100% CPU and clients are stuck ,"Testing environment is Storm 0.9.5 / thrift java 0.7.
Test scenario: 
  Deploy storm topology in loop.
  When nimbus cleanup timeout is reached, an error is thrown by thrift server: 
  ""Exception while invoking ..."" ... TException

Test result:
  Thrift java server in nimbus goes 100% CPU in infinite loop in:

jstack:
{code}
""Thread-5"" prio=10 tid=0x00007fb134aab800 nid=0x6767 runnable [0x00007fb129c9b000]
   java.lang.Thread.State: RUNNABLE
                                      at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
                                      at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
                                      at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
                                      at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
...
at org.apache.thrift7.server.TNonblockingServer$SelectThread.select(TNonblockingServer.java:284) 
{code}

strace:
{code}
epoll_wait(70, {{EPOLLIN, {u32=866, u64=866}}, {EPOLLIN, {u32=876, u64=876}}}, 4096, 4294967295) = 2
{code}

Investigation and tests show that:
Any Exception thrown during the processor execution will bypass the call to {code} responseReady() {code} and will cause the counter {code}       readBufferBytesAllocated.addAndGet(-buffer_.array().length); {code} not to be decremented by the size of the request buffer.

After a bunch of failed requests, this counter almost reaches the max value MAX_READ_BUFFER_BYTES causing any subsequent request to be delayed forever because the following test in {code} read() {code}:
{code}           if (readBufferBytesAllocated.get() + frameSize > MAX_READ_BUFFER_BYTES)  {code} is always true.

At the end, the server thread loops in select() which immediately wakes up for read() since the content of the socket was never drained.

This loops forever between select and read() method above causing a 100% CPU on server thread.
Moreover, all client requests are stuck forever.

Example of failed request:
{code}
2015-09-01T12:19:35.954+0200 b.s.d.nimbus [WARN] Topology submission exception. (topology name='mytopology') #<IllegalArgumentException java.lang.IllegalArgumentException: /opt/SPE/share/stor
m/storm/local/nimbus/inbox/stormjar-3f8f3ba7-5420-4773-af24-bfa294cceb79.jar to copy to /opt/SPE/share/storm/storm/local/nimbus/stormdist/mytopology-87-1441102775 does not exist!>
2015-09-01T12:19:35.955+0200 o.a.t.s.TNonblockingServer [ERROR] Unexpected exception while invoking!
java.lang.IllegalArgumentException: /opt/SPE/share/storm/storm/local/nimbus/inbox/stormjar-3f8f3ba7-5420-4773-af24-bfa294cceb79.jar to copy to /opt/SPE/share/storm/storm/local/nimbus/stormdis
t/mytopology-87-1441102775 does not exist!
        at backtype.storm.daemon.nimbus$fn__3827.invoke(nimbus.clj:1173) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.MultiFn.invoke(MultiFn.java:236) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.nimbus$setup_storm_code.invoke(nimbus.clj:307) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.nimbus$fn__3724$exec_fn__1103__auto__$reify__3737.submitTopologyWithOpts(nimbus.clj:953) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.daemon.nimbus$fn__3724$exec_fn__1103__auto__$reify__3737.submitTopology(nimbus.clj:966) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.generated.Nimbus$Processor$submitTopology.getResult(Nimbus.java:1240) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.generated.Nimbus$Processor$submitTopology.getResult(Nimbus.java:1228) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.ProcessFunction.process(ProcessFunction.java:32) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.TBaseProcessor.process(TBaseProcessor.java:34) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632) ~[storm-core-0.9.5.jar:0.9.5]
        at org.apache.thrift7.server.THsHaServer$Invocation.run(THsHaServer.java:201) [storm-core-0.9.5.jar:0.9.5]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
{code} "
STORM-1021,HeartbeatExecutorService issue in Shell Spout\Bolt,"ShellSpout class (and it seems that this touches ShellBolt as well) doesn't restart when hearbeat timeout occurs. To reproduce this bug you should do the following:
1. Set supervisor.worker.timeout.secs property to e.g. 1;
2. Create a shell spout (as a standalone application, not Java class) that hangs for more than 1 second and doesn't respond on heartbeat messages, e.g. Thread.Sleep(5000);
3. After timeout Storm will try to kill the shell spout process with calling die function:
https://github.com/apache/storm/blob/v0.10.0-beta/storm-core/src/jvm/backtype/storm/spout/ShellSpout.java#L237
4. The ""die"" function will call heartBeatExecutorService.shutdownNow() function that raises InterruptedException, which is not caughted by the calling thread. In a result topology stops working properly, however you may see it in ./storm list.

I'm not Java developer and thus I'm not sure whether code below is valid, however it seems to fix the problem:

    private void die(Throwable exception) {
        heartBeatExecutorService.shutdownNow();
        try {
            heartBeatExecutorService.awaitTermination(5, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            LOG.error(""await catch "", e);
        }

        _collector.reportError(exception);
        _process.destroy();
        System.exit(11);
    }"
STORM-1019,Added missing dependency version to use of org.codehaus.mojo:make-maven-plugin ,Travis-CI reports critical warning that Maven plugin is missing version information.
STORM-1015,Store Kafka offsets with Kafka's consumer offset management api,"Current Kafka spout stores the offsets (and some other states) inside ZK with its proprietary format. This does not work well with other Kafka offset monitoring tools such as Burrow, KafkaOffsetMonitor etc. In addition, the performance does not scale well compared with offsets managed by Kafka's built-in offset management api. I have added a new option for Kafka to store the same data using Kafka's built-in offset management capability. The change is completely backward compatible with the current ZK storage option. The feature can be turned on by a single configuration option. Hope this will help people who wants to explore the option of using Kafka's built-in offset management api.

References:

https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka
https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-OffsetCommit/FetchAPI

-thanks"
STORM-1006,Storm is not garbage collecting the messages (causing memory hit),"We are reading whole file in memory around 5 MB, which is send through Kafaka to Storm. In next bolt, we performs the operation on file and sends out tuple to next bolt. After profiling we found that file (bytes of file) does not get garbage collected. So after further investigation we found that  backtype.storm.coordination.CoordinatedBolt.CoordinatedOutputCollector.emit(String, Collection<Tuple>, List<Object>) API gets the first object and use it for tracking :(. Can you confirm reason behind this? Is there any way we can send different unique id as first element in list or the unique id of tuple used as indicator.

However, for time being we have made changes in schema assigned to KafkaSpout, so that it will parse the file and send out list of values.

If you below code CoordinatedBolt, ""Object id = tuple.getValue(0);” takes the 1st element from tuple instead of taking id of tuple. This ""id"" is then saved to _tracked hashhMap(TimeCache). In our case the 0th element is files byte data. This gets stored in the _tracked map till tree of tuple doesn’t get complete. As we are processing huge data we run outofMemory issue.

Code:

public void execute(Tuple tuple) {

        *Object id = tuple.getValue(0);*

        TrackingInfo track;

        TupleType type = getTupleType(tuple);

        synchronized(_tracked) {

            track = _tracked.get(id);

            if(track==null) {

                track = new TrackingInfo();

                if(_idStreamSpec==null) track.receivedId = true;

                _tracked.put(id, track);*

            }

        }



        if(type==TupleType.ID) {

            synchronized(_tracked) {

                track.receivedId = true;

            }

            checkFinishId(tuple, type);

        } else if(type==TupleType.COORD) {

            int count = (Integer) tuple.getValue(1);

            synchronized(_tracked) {

                track.reportCount++;

                track.expectedTupleCount+=count;

            }

            checkFinishId(tuple, type);

        } else {

            synchronized(_tracked) {

                _delegate.execute(tuple);

            }

        }

    }


"
STORM-1005,Supervisor do not get running workers after restart.,"I have 8 supervisors running,2 of them shutdown by themselves.
After I restart them.One supervisor shows the right number of workers, but another shows the worker number is 0 while the workers are still running on that host."
STORM-1001,Undefined STORM_EXT_CLASSPATH adds '::' to classpath of workers,"If environment variable STORM_EXT_CLASSPATH is undefined, an empty string will used in the classpath for workers. I'm not sure how java is expected to handle this, but bad things happen, including behavior that is dependent on the working directory that the supervisor was launched in. A simple workaround is to set STORM_EXT_CLASSPATH to a path that does not exist."
STORM-996,netty-unit-tests/test-batch demonstrates out-of-order delivery,"backtype.storm.messaging.netty-unit-test/test-batch

One example of output.  Similar things happen sporadically and vary widely by number of failed assertions.

Tuples are not just skewed, but actually seem to come in out-of-order.

{quote}
actual: (not (= ""66040"" ""66041""))
at: test_runner.clj:105

expected: (= req_msg resp_msg)
actual: (not (= ""66041"" ""66042""))
at: test_runner.clj:105

expected: (= req_msg resp_msg)
actual: (not (= ""66042"" ""66040""))
at: test_runner.clj:105
{quote}
"
STORM-994,Connection leak between nimbus and supervisors,Successive deploys/undeploys of topology(ies) may result in a connection leak between nimbus and its supervisors
STORM-993,Add uptime in seconds to UI REST API,"Currently the UI returns the uptime for various components as a pretty-printed string, e.g., ""4h 2m 1s"".

It would be nice to give these uptimes in seconds as well, so that other clients can more easily deal with the data."
STORM-989,JDBC state can not support transactional,"the class JdbcState in this module storm/external/storm-jdbc does not distinguish between opaque transactional state, transactional state, and non-transactional state, it can not implement the transaction executed only once."
STORM-985, provide .editorconfig with git pre-commit hooks to check code style before commit,
STORM-977,Incorrect signal (-9) when as-user is true,https://github.com/apache/storm/blob/544e55cb8ab8878c4af500aab49bd35d4b69cd3e/storm-core/src/clj/backtype/storm/daemon/supervisor.clj#L265
STORM-976,Config storm.logback.conf.dir is specific to previous logging framework,"Storm has migrated from logback to log4j2, so we should rename this config and code that uses it.

https://github.com/apache/storm/blob/544e55cb8ab8878c4af500aab49bd35d4b69cd3e/storm-core/src/clj/backtype/storm/daemon/supervisor.clj#L664"
STORM-973,Netty-Client Connection Failed,"When Storm Topology startup in a distribution cluster I found netty connection will failed and messages will be droped by client itself. 

worker log info as following :

```
2015-08-07T11:43:18.903+0800 b.s.m.n.StormClientErrorHandler [INFO] Connection failed Netty-Client-storm-01
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_75]
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_75]
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ~[na:1.7.0_75]
	at sun.nio.ch.IOUtil.read(IOUtil.java:192) ~[na:1.7.0_75]
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379) ~[na:1.7.0_75]
	at org.apache.storm.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64) [storm-core-0.9.4.jar:0.9.4]
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) [storm-core-0.9.4.jar:0.9.4]
	at org.apache.storm.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318) [storm-core-0.9.4.jar:0.9.4]
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) [storm-core-0.9.4.jar:0.9.4]
	at org.apache.storm.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) [storm-core-0.9.4.jar:0.9.4]
	at org.apache.storm.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) [storm-core-0.9.4.jar:0.9.4]
	at org.apache.storm.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) [storm-core-0.9.4.jar:0.9.4]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_75]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_75]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
2015-08-07T11:43:19.426+0800 b.s.m.n.Client [INFO] connection attempt 1 to Netty-Client-syq-storm-01.meilishuo.com/172.16.7.25:8711 scheduled to run in 0 ms
2015-08-07T11:43:19.427+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-storm-01 is unavailable
2015-08-07T11:43:19.427+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-storm-01
2015-08-07T11:43:19.428+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-storm-01 is unavailable
2015-08-07T11:43:19.428+0800 b.s.m.n.Client [ERROR] dropping 103 message(s) destined for Netty-Client-storm-01 
2015-08-07T11:43:19.428+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-storm-01 is unavailable
2015-08-07T11:43:19.428+0800 b.s.m.n.Client [ERROR] dropping 35 message(s) destined for Netty-Client-storm-01
```"
STORM-970,UT messaging_test.clj#test-receiver-message-order build failed ,"the CI always build error recently and the failure looks really spurious
Then I build the tests locally and find : 

{code}
➜  storm git:(master) dev-tools/test-ns.py backtype.storm.messaging-test
...
[main] INFO  b.s.u.Utils - Using defaults.yaml from resources
Running backtype.storm.messaging-test
Tests run: 2, Passed: 2, Failures: 0, Errors: 1    (!!!Error occurred but travis-ci does not push this info out)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 39.593 s
[INFO] Finished at: 2015-08-05T15:20:26+08:00
[INFO] Final Memory: 27M/205M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.7.1:test (test-clojure) on project storm-core: Clojure failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.7.1:test (test-clojure) on project storm-core: Clojure failed.
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: Clojure failed.
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:464)
        at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:366)
        at com.theoryinpractise.clojure.ClojureRunTestWithJUnitMojo.execute(ClojureRunTestWithJUnitMojo.java:138)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 20 more
{code}

Seems like something went wrong of UT
{code}
 messaging_test.clj#test-receiver-message-order 
{code}
https://github.com/apache/storm/blob/master/storm-core/test/clj/backtype/storm/messaging_test.clj#L85"
STORM-963,Frozen topology (KafkaSpout + Multilang bolt),"Hi,

We've got a pretty simple topology running with Storm 0.9.5 (tried also with 0.9.4 and 0.9.6-INCUBATING) in a 3 machine cluster:

{code}kafkaSpout (3) -----> processBolt (12){code}

Some info:
- kafkaSpout reads from a topic with 3 partitions and 2 replications
- processBolt iterates throught the message and saves the results in MongoDB
- processBolt is implemented in Python and has a storm.log(""I'm doing something"") just to add a simple debug message in the logs
- The messages can be quite big (~25-40 MB) and are in JSON format
- The kafka topic has a retention of 2 hours
- We use the same ZooKeeper cluster to both Kafka and Storm

The topology gets frozen after several hours (not days) running. We don't see any message in the logs... In fact, the periodic message from s.k.KafkaUtils and s.k.ZkCoordinator disapears. As you can imagine, the message from the Bolt also dissapears. Logs are copy/pasted further on. If we redeploy the topology everything starts to work again until it becomes frozen again.

Our kafkaSpout config is:

{code}
ZkHosts zkHosts = new ZkHosts(""zkhost01:2181,zkhost02:2181,zkhost03:2181"");
SpoutConfig kafkaConfig = new SpoutConfig(zkHosts, ""topic"", ""/topic/ourclientid"", ""ourclientid"");
kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme());
kafkaConfig.fetchSizeBytes = 50*1024*1024;
kafkaConfig.bufferSizeBytes = 50*1024*1024;
{code}

We've also tried setting the following options

{code}
kafkaConfig.forceFromStart = true;
kafkaConfig.startOffsetTime = kafka.api.OffsetRequest.EarliestTime(); // Also with kafka.api.OffsetRequest.LatestTime();
kafkaConfig.useStartOffsetTimeIfOffsetOutOfRange = true;
{code}

Right now the topology is running without acking the messages since there's a bug in kafkaSpout with failed messages and deleted offsets in Kafka.

This is what can be seen in the logs in one of the workers:

{code}
2015-07-23T12:37:38.008+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:37:39.079+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:37:51.013+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:37:51.091+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:38:02.684+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:38:02.687+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:38:02.687+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2, partition=1}]
2015-07-23T12:38:02.687+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:38:02.687+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:38:02.687+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:38:09.012+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:38:41.878+0200 b.s.t.ShellBolt [INFO] ShellLog pid:28364, name:processBolt I'm doing something
2015-07-23T12:39:02.688+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:39:02.691+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:39:02.691+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:39:02.691+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:39:02.691+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:39:02.691+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:40:02.692+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:40:02.695+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:40:02.695+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:40:02.695+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:40:02.695+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:40:02.695+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:41:02.696+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:41:02.699+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:41:02.699+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:41:02.699+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:41:02.699+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:41:02.699+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
2015-07-23T12:42:02.735+0200 s.k.ZkCoordinator [INFO] Task [2/3] Refreshing partition manager connections
2015-07-23T12:42:02.737+0200 s.k.DynamicBrokersReader [INFO] Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=kafka1:9092, 1=kafka2:9092, 2=kafka3:9092}}
2015-07-23T12:42:02.737+0200 s.k.KafkaUtils [INFO] Task [2/3] assigned [Partition{host=kafka2:9092, partition=1}]
2015-07-23T12:42:02.737+0200 s.k.ZkCoordinator [INFO] Task [2/3] Deleted partition managers: []
2015-07-23T12:42:02.737+0200 s.k.ZkCoordinator [INFO] Task [2/3] New partition managers: []
2015-07-23T12:42:02.737+0200 s.k.ZkCoordinator [INFO] Task [2/3] Finished refreshing
{code}

and then it becomes frozen. Nothing is written into the nimbus log. We've checked the offsets in ZooKeeper and they're not updated:

{code}
{""topology"":{""id"":""218e58a5-6bfb-4b32-ae89-f3afa19306e1"",""name"":""our-topology""},""offset"":12047144,""partition"":1,""broker"":{""host"":""kafka2"",""port"":9092},""topic"":""topic""}
cZxid = 0x100028958
ctime = Wed Jul 01 12:22:36 CEST 2015
mZxid = 0x100518527
mtime = Thu Jul 23 12:42:41 CEST 2015
pZxid = 0x100028958
cversion = 0
dataVersion = 446913
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 183
numChildren = 0
{code}


Any ideas of what we could be missing?

PS: This was sent to the Storm user's mailing list and got 0 replies :\"
STORM-959,remove unnecessary dependency from storm-hive/pom.xml,"[org.apache.calcite:calcite-core:0.9.2-incubating|https://github.com/apache/storm/blob/master/external/storm-hive/pom.xml#L84] does not take affect at all. 
Becase of hive-exec  org.apache.calcite:calcite-core:0.9.2-incubating-SNAPSHOT will be downloaded 
{code}
Downloading: https://repository.apache.org/snapshots/org/apache/calcite/calcite-core/0.9.2-incubating-SNAPSHOT/maven-metadata.xml
Downloading: https://repository.apache.org/snapshots/org/apache/calcite/calcite-core/0.9.2-incubating-SNAPSHOT/calcite-core-0.9.2-incubating-SNAPSHOT.pom
[WARNING] The POM for org.apache.calcite:calcite-core:jar:0.9.2-incubating-SNAPSHOT is missing, no dependency information available
{code}
"
STORM-958,Add config for init params of group mapping service,"As a user, I would like a config added for initialization parameters of the group mapping service, so that I can initialize my own plug-in properly."
STORM-957,How to write Python sport/bolts to send data from Kafka via Storm to HDFS?,"We want to send some logs via Flume > Kafka > Storm > HDFS

1. Is there a sprout/bolt code available in Python to send loglines coming to Kafka queue to send/write to HDFS via Storm?

2. How much code do we have to write to integrate Kafka Storm to HDFS?"
STORM-955,replace all  backtype.storm.daemon.common.clj#StormBase with backtype.storm.generated.StormBase,
STORM-953,Acknowledging Unanchored tuple causes freeze in toplogy,"I am running a topology in the 6 node cluster on RHEL 5 and Oracle JDK 1.7. Schematic for topology as attached, there are 6 workers for spout and bolts. Topology picks up message from JMS and sends out message to bolts. Each bolt does some processing and sends out next set of tuple to downstream for further processing. All bolts are in shuffle grouping. There are two special bolts 
1. Log 
2. Ticket
Log bolt receive same tuple from each bolt for logging purpose but these tuples are unanchored. Similarly Ticket bolt receive tuple that are failed due to business logic, these are also unanchored. Bolt1 to Bolt4 and anchored tuple since we wanted message guarantee for these tuples. Each bolt receive one tuple and emit one data tuple from Bolt1 to Bolt4. 

I have observed that we were acking some unanchored tuple in Log and Ticket bolts, and it was causing topology freeze after sometime (10 mins when worker jvm heap size is set to 2 GB , 2 days when worker jvm heap was set to 16 GB). These freeze essentially slows down message read from JMS. After commenting out acking I didnt observe any slow down/freeze.
"
STORM-952,Add name for the threads in Storm process(nimbus  supervisor worker and drpc),
STORM-948,"""topology.max.spout.pending"" submitted with the component configuration is ignored","Neither Storm UI, nor the runtime component configuration received from Storm has the value that was submitted.

https://github.com/apache/storm/blob/master/storm-core/src/storm.thrift#L88

Looks like a regression in 0.9.5, 0.9.4 worked!
"
STORM-947,replace all  `backtype.storm.scheduler.ExecutorDetails`  with `backtype.storm.generated.ExecutorInfo `,"replace all  
{code}
backtype.storm.scheduler.ExecutorDetails
{code}  
with 
{code}
backtype.storm.generated.ExecutorInfo 
{code}"
STORM-946,We should remove Closed Client form cached-node+port->socket in worker,"The client may be Closed status after reconnect failed, and we will remove closed client from Context to escape memory leak.
But there is also reference for the closed Client in cached-node+port->socket in worker, for this reason we should also remove closed Client from cached-node+port->socket.  

Meanwhile there is another reason for us to do so. Think about this situation: worker A connect to worker B1 B2, but for some reason worker B1 B2 died at the same, then nimbus reschedule worker B1 B1. And new B1 B2 may partly rescheduled at the some host:port as old B1 B2, that is (old B1: host1+port1, old B2: host2+port2, new B1: host2+port2, new B2: host3+port3). Worker A realized worker B1 B2 died and start reconnect to worker B1 B2, but before new worker B1 and old B2 have the same host+port, and by the current logic, we will remove old B1 Client and and create new Client for new worker B2, and do nothing to old B2 and new B1 because they have the same host+port. This will result the topology stop processing tuples. Once we remove closed Client from cached-node+port->socket before refresh-connections, this  will not happen again."
STORM-945,"<DefaultRolloverStrategy> element is not a policy,and should not be putted in the <Policies> element.","{code}
➜  storm-current  bin/storm nimbus
Running: /usr/share/jdk-current/bin/java -server -Ddaemon.name=nimbus ... 
...
2015-07-17 10:23:36,776 ERROR Policies has no parameter that matches element DefaultRolloverStrategy
2015-07-17 10:23:36,787 ERROR Policies has no parameter that matches element DefaultRolloverStrategy
2015-07-17 10:23:36,795 ERROR Policies has no parameter that matches element DefaultRolloverStrategy
{code}"
STORM-942,Add FluxParser method parseInputStream() to eliminate disk usage.,Included reusable parseInputStream for use by parseFile and parseResource. Also allows a path for programatically creating topologies without the need to write to / read from disk.
STORM-940,Lost messages with netty,"We have a topology that mysteriously stopped several times. On the most recent occasion it had a max spout pending of 4 and there were 4 transactions in zookeeper. We are using transactional topologies and the final bolt in the topology is a global grouping with only one task. We have verified that the first transaction finished successfully right up to the last line of the finish batch, but the tx is still in zookeeper and all of the other transactions are waiting on it. I know transactional topologies have been deprecated, but regardless this shouldn't happen unless the acks are being dropped. We have had a bunch of similar issues since moving to the netty based versions and I think there is a serious reliability issue with them. Sorry I can't provide code to replicate the issue because the problem is intermittent. "
STORM-935,Update Disruptor queue version to 2.10.4,Storm now still use an old version of Disruptor queue(ver 2.10.1). This version has some potential race problems. Version 2.10.4 has fixed these bugs. https://issues.apache.org/jira/browse/STORM-503 will benifit from this update.
STORM-933,NullPointerException during KafkaSpout deactivation,"We are seeing this during a topology shutdown

2015-07-09 19:51:48-0700 b.s.util [ERROR] Async loop died!
java.lang.NullPointerException: null
        at storm.kafka.KafkaSpout.commit(KafkaSpout.java:190) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.deactivate(KafkaSpout.java:180) ~[stormjar.jar:na]
        at com.relateiq.storm.JMXActivatableSpout.deactivate(JMXActivatableSpout.java:75) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__6579$fn__6594$fn__6623.invoke(executor.clj:570) ~[storm-core-0.9.5.jar:0.9.5]
        at backtype.storm.util$async_loop$fn__459.invoke(util.clj:463) ~[storm-core-0.9.5.jar:0.9.5]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
2015-07-09 19:51:48-0700 b.s.d.executor [ERROR]

Thanks"
STORM-931,Python Scritps to Produce Formatted JIRA and GitHub Joint Reports,
STORM-930,Nimbus server shutting down,"2015-07-09 18:40:53 b.s.d.nimbus [INFO] Starting Nimbus server...
2015-07-09 18:40:54 b.s.d.nimbus [ERROR] Error when processing event
java.lang.NullPointerException: null
	at clojure.lang.Numbers.ops(Numbers.java:942) ~[clojure-1.5.1.jar:na]
	at clojure.lang.Numbers.isZero(Numbers.java:90) ~[clojure-1.5.1.jar:na]
	at backtype.storm.util$partition_fixed.invoke(util.clj:868) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.AFn.applyToHelper(AFn.java:163) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyToHelper(AFn.java:163) [clojure-1.5.1.jar:na]
	at clojure.lang.RestFn.applyTo(RestFn.java:132) ~[clojure-1.5.1.jar:na]
	at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
	at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
	at clojure.lang.RestFn.invoke(RestFn.java:408) ~[clojure-1.5.1.jar:na]
	at backtype.storm.util$map_val$iter__273__277$fn__278.invoke(util.clj:291) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.LazySeq.sval(LazySeq.java:42) ~[clojure-1.5.1.jar:na]
	at clojure.lang.LazySeq.seq(LazySeq.java:60) ~[clojure-1.5.1.jar:na]
	at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.5.1.jar:na]
	at clojure.lang.RT.next(RT.java:598) ~[clojure-1.5.1.jar:na]
	at clojure.core$next.invoke(core.clj:64) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$fn__6034.invoke(protocols.clj:146) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$fn__6005$G__6000__6014.invoke(protocols.clj:19) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$seq_reduce.invoke(protocols.clj:31) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$fn__6026.invoke(protocols.clj:54) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$fn__5979$G__5974__5992.invoke(protocols.clj:13) ~[clojure-1.5.1.jar:na]
	at clojure.core$reduce.invoke(core.clj:6177) ~[clojure-1.5.1.jar:na]
	at clojure.core$into.invoke(core.clj:6229) ~[clojure-1.5.1.jar:na]
	at backtype.storm.util$map_val.invoke(util.clj:290) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.daemon.nimbus$compute_executors.invoke(nimbus.clj:435) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.daemon.nimbus$compute_executor__GT_component.invoke(nimbus.clj:446) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:339) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.daemon.nimbus$mk_assignments$iter__7285__7289$fn__7290.invoke(nimbus.clj:665) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.LazySeq.sval(LazySeq.java:42) ~[clojure-1.5.1.jar:na]
	at clojure.lang.LazySeq.seq(LazySeq.java:60) ~[clojure-1.5.1.jar:na]
	at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.5.1.jar:na]
	at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$fn__6026.invoke(protocols.clj:54) ~[clojure-1.5.1.jar:na]
	at clojure.core.protocols$fn__5979$G__5974__5992.invoke(protocols.clj:13) ~[clojure-1.5.1.jar:na]
	at clojure.core$reduce.invoke(core.clj:6177) ~[clojure-1.5.1.jar:na]
	at clojure.core$into.invoke(core.clj:6229) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:664) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.RestFn.invoke(RestFn.java:410) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.nimbus$fn__7518$exec_fn__1491__auto____7519$fn__7524$fn__7525.invoke(nimbus.clj:994) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.daemon.nimbus$fn__7518$exec_fn__1491__auto____7519$fn__7524.invoke(nimbus.clj:993) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.timer$schedule_recurring$this__2034.invoke(timer.clj:99) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.timer$mk_timer$fn__2017$fn__2018.invoke(timer.clj:50) ~[storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.timer$mk_timer$fn__2017.invoke(timer.clj:42) [storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
2015-07-09 18:40:54 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:322) [storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.nimbus$nimbus_data$fn__6891.invoke(nimbus.clj:85) [storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.timer$mk_timer$fn__2017$fn__2018.invoke(timer.clj:68) [storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at backtype.storm.timer$mk_timer$fn__2017.invoke(timer.clj:42) [storm-core-0.9.3.2.2.6.0-2800.jar:0.9.3.2.2.6.0-2800]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
2015-07-09 18:40:54 b.s.d.nimbus [INFO] Shutting down master"
STORM-929,High CPU usage when bolt idle due to short disruptor queue wait time,"I'm running topology which has large num of executors (500) on storm 0.9.3. I find the CPU usage over 100% when topology idle. And half of the CPU usage is from kernel. I look into CPU utilization of worker process and find most of threads wait on:
     com.lmax.disruptor.BlockingWaitStrategy.waitFor(long, com.lmax.disruptor.Sequence, com.lmax.disruptor.Sequence[], com.lmax.disruptor.SequenceBarrier, long, java.util.concurrent.TimeUnit) 

I use Storm starter topology (wordcounter) to reproduce this issue. I change the sleep time of spout to 10s and executor num of  bolt to 500. So there was effectively no task to do. Again the CPU usage comes to 100% and half from  kernel. I think this may caused by frequently switching thread context due to short disruptor queue wait time."
STORM-928,Add sources->streams->fields map to Multi-Lang Handshake,"In my previous pull request for issue [STORM-789] I somehow omitted a key piece of information I wanted to add to the Multi-Lang handshake, ""sources->streams->fields"", which would allow multi-lang libraries to access Tuple values by field name instead of just by index.  The pull request I am about to submit fixes this oversight.  I hope that since this is directly related to something that was merged into the 0.10.x branch, that this fix can also make it in there."
STORM-927,Storm.cmd on Windows shouldn't exit until deployment complete,"The storm.cmd script returns before the topology has been deployed. This causes problems when attempting to automate deployments, as detecting failures is made difficult.

It looks like this is because all storm command are ultimately invoked via ""start /b"" unless STORM_DEBUG is set. Attempting to workaround by setting STORM_DEBUG doesn't work, as the fix in STORM-322 hasn't been applied to the debug command.

It looks like a relatively simple fix. Apply the STORM-322 fix to the debug route, and make the jar command use it by default."
STORM-925,Move DefaultRolloverStrategy in log4j2/worker.xml,"In ""log4j2/cluster.xml"" and ""log4j2/worker.xml"", the ""DefaultRolloverStrategy"" line (three instances) need to be move out of ""Policy"". Storm supervisor, ui, and nimbus will output errors on start when running 0.10.0 beta1

https://stackoverflow.com/questions/31046512/storm-error-on-initialization-of-server-mk-supervisor-required-field-serializ"
STORM-924,Set the file mode of the files included when packaging release packages,
STORM-922,Storm-Core Dependencies Renamed in Shade JAR,"storm-core's pom.xml file has dependencies on a variety of widely used open source libraries such as commons-io, commons-collections, guava, etc.

Any user-defined topologies that depend on the storm-core JAR then assume to have those libraries available during runtime through transitive dependencies.

However, the Shade JAR built by storm-core renames all of the package names of those libraries to move them under org.apache.storm.*.

This causes runtime NoClassDefFoundError's from for topologies that were previously succeeding (e.g., org.apache.commons.io.FileUtils).

Recommend providing those libraries since the Maven dependencies declare that they are including them.  If the goal is to provide classpath isolation for Storm vs. topologies, the original dependencies should still be provided."
STORM-918,Storm CLI could validate arguments/print usage,"It would be nice if the storm CLI printed usage information if arguments are missing.

For example, when omitting the argument to the kill sub-command, a JVM is launched and an exception complaining that a topology named 'nil' is not alive."
STORM-914,AutoHDFS should also update hdfs.config into Configuration,"When storm user a specify hdfs cluster, he will put some config such as namenode.host into hdfs.config when submit storm topology, and when HdfsBolt create HdfsConfiguration, it will use hdfs.config update HDFSConfiguration, then HDFSBolt will visit the right hdfs cluster. But in AutoHDFS's renew and getHadoopCredentials, Configuration not for the HDFS that storm user specify, we should update all the Configuration with hdfs.config."
STORM-913,Use Curator's delete().deletingChildrenIfNeeded() instead of zk/delete-recursive ,
STORM-910,Submit job failed when submitter user is also set in nimbus.supervisor.users,"When topology submitter user also set in nimbus.supervisor.users, submit topology will failed. "
STORM-908,AWS workers can't communicate due to Netty-Client hostname resolution,"Observed that there is some kind of problem blocking communication between workers.  I think manually editing the /etc/hosts file is a workaround.
ubuntu@ip-10-9-255-123
/home/ubuntu/apache-storm-0.9.4/logs/worker-6714.log
2015-06-24T07:13:07.856+0000 b.s.m.n.Client [INFO] connection attempt 24 to Netty-Client-ip-10-9-255-20.us-west-2.compute.internal/10.9.255.20:6711 scheduled to run in 387 ms
2015-06-24T07:13:08.244+0000 b.s.m.n.Client [ERROR] connection attempt 24 to Netty-Client-ip-10-9-255-20.us-west-2.compute.internal/10.9.255.20:6711 failed: java.lang.RuntimeException: Returned channel was actually not established
 "
STORM-906,"Flux ""--local --zookeeper"" dose not work","[description]
-z,--zookeeper <host:port>
When running in local mode, use the ZooKeeper at the specified <host>:<port>  instead of the in-process ZooKeeper.
But when using zookeeper string like 'host-A:2181,host-B:2181,host-C:2181', Flux dose not work well.

[command]
{code}
storm jar topo.jar org.apache.storm.flux.Flux --local --zookeeper host-A:2181,host-B:2181,host-C:2181 topo.yaml
{code}

[code]
Flux.java:
{code}
...
if(zkStr.contains("":"")){
                        String[] hostPort = zkStr.split("":"");
                        zkHost = hostPort[0];
                        zkPort = hostPort.length > 1 ? Long.parseLong(hostPort[1]) : DEFAULT_ZK_PORT;

 }
...
{code}"
STORM-903,fix class missing when ui.filter set to org.apache.hadoop.security.authentication.server.AuthenticationFilter,"When I set ui.filter=org.apache.hadoop.security.authentication.server.AuthenticationFilter, there are java.lang.NoClassDefFoundError about the ui:
```
org/apache/commons/codec/binary/Base64
java.lang.NoClassDefFoundError:
        at org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.authenticate(KerberosAuthenticationHandler.java:305) ~[hadoop-auth-2.4.0.jar:?]
        at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:357) ~[hadoop-auth-2.4.0.jar:?]
        at org.apache.storm.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:247) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:210) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.Server.handle(Server.java:369) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.http.HttpParser.parseNext(HttpParser.java:644) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.server.ssl.SslSocketConnector$SslConnectorEndPoint.run(SslSocketConnector.java:670) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at org.apache.storm.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:662) [?:1.6.0_37]
Caused by: java.lang.ClassNotFoundException: org.apache.commons.codec.binary.Base64
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202) ~[?:1.6.0_37]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.6.0_37]
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190) ~[?:1.6.0_37]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:306) ~[?:1.6.0_37]
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) ~[?:1.6.0_37]
        at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ~[?:1.6.0_37]
        ... 25 more
```

This is because missing commons-code."
STORM-875,404 Not Found responded when issuing request '/api/v1/token',"I want to upload a topology via a new added REST service '/api/v1/uploadTopology' introduced since 0.10. STORM-615 mentions a ring-session token be required before uploading the topology. Here is what I get when issuing command 'curl -i --negotiate -u : -b ~/cookiejar.txt -c ~/cookiejar.txt http://localhost:8080/api/v1/token' in a shell:

HTTP/1.1 404 Not Found
Date: Thu, 18 Jun 2015 08:52:48 GMT
Content-Type: text/html; charset=utf-8
Content-Length: 14
Server: Jetty(6.1.26)

Page not found
 
That blocks me from submitting topologies via the exposed REST interface."
STORM-874,Netty Threads do not handle Errors properly,"When low on memory, netty thread could get OOM which if not handled correctly can lead to unexpected behavior such as netty connection leaks.
{code:java}
java.lang.OutOfMemoryError: Direct buffer memory
	at java.nio.Bits.reserveMemory(Bits.java:658) ~[?:1.8.0_25]
	at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:123) ~[?:1.8.0_25]
	at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:311) ~[?:1.8.0_25]
	at org.jboss.netty.buffer.ChannelBuffers.directBuffer(ChannelBuffers.java:167) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.buffer.ChannelBuffers.directBuffer(ChannelBuffers.java:151) ~[netty-3.9.4.Final.jar:?]
	at backtype.storm.messaging.netty.MessageBatch.buffer(MessageBatch.java:101) ~[storm-core-0.9.2-incubating-security.jar:0.9.2-incubating-security]
	at backtype.storm.messaging.netty.MessageEncoder.encode(MessageEncoder.java:32) ~[storm-core-0.9.2-incubating-security.jar:0.9.2-incubating-security]
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:66) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.channel.Channels.write(Channels.java:704) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.channel.Channels.write(Channels.java:671) ~[netty-3.9.4.Final.jar:?]
	at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:248) ~[netty-3.9.4.Final.jar:?]
	at backtype.storm.messaging.netty.Client.tryDeliverMessages(Client.java:226) ~[storm-core-0.9.2-incubating-security.jar:0.9.2-incubating-security]
	at backtype.storm.messaging.netty.Client.send(Client.java:173) ~[storm-core-0.9.2-incubating-security.jar:0.9.2-incubating-security]
{code}
"
STORM-870,Tuples emitted just before spout restarts are not acked/failed,"The tuples emitted just before the spout restarts are never acked/failed after the tuple tree completes. The spout never gets notified but the worker log for the spout has the below log during its restart.

2015-06-18T14:47:01.430+0000 b.s.m.n.Client [ERROR] connection to Netty-Client-localhost/127.0.0.1:6701 is unavailable
2015-06-18T14:47:01.433+0000 b.s.m.n.Client [DEBUG] successfully connected to localhost/127.0.0.1:6701, [id: 0x9a243253, /127.0.0.1:36638 => localhost/127.0.0.1:6701] [attempt 1]
2015-06-18T14:47:01.433+0000 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-localhost/127.0.0.1:6701
"
STORM-869,kafka spout cannot fetch message if log size is above fetchSizeBytes,"let's say maxFetchSizeBytes is set to 1 megabytes, then if there exists a message that is bigger than 1 m, kafka spout just hangs and become inactive.
This is both happening in Kafka spout for bolt/spout topology and also in trident spouts."
STORM-867,fix bug with mk-ssl-connector,"There are two error about mk-ssl-connector in storm-core/src/clj/backtype/storm/ui/helpers.clj
1. set trustStore password with setTrustStoreType
2. need-client-auth and want-client-auth are boolen, but use as function"
STORM-866,Use storm.log.dir instead of storm.home in log4j2 config,"The config log4j2 use storm.home as log dir, but not storm.log.dir, we should use storm.log.dir instead."
STORM-863,UI report TTransportException,
STORM-858,nimbus start failed due to ThriftServer startup NPE,"When I run storm with master, I got NPE when nimbus start in ThriftServer
```
5213 [main] ERROR b.s.s.a.ThriftServer - ThriftServer is being stopped due to: java.lang.NullPointerException
java.lang.NullPointerException
        at backtype.storm.security.auth.AuthUtils.get(AuthUtils.java:271) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.getServerTransportFactory(KerberosSaslTransportPlugin.java:77) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.security.auth.SaslTransportPlugin.getServer(SaslTransportPlugin.java:71) ~[storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.security.auth.ThriftServer.serve(ThriftServer.java:70) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:1370) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.daemon.nimbus$_launch.invoke(nimbus.clj:1394) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at backtype.storm.daemon.nimbus$_main.invoke(nimbus.clj:1417) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
        at clojure.lang.AFn.applyToHelper(AFn.java:152) [clojure-1.6.0.jar:?]
        at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]
        at backtype.storm.daemon.nimbus.main(Unknown Source) [storm-core-0.11.0-SNAPSHOT.jar:0.11.0-SNAPSHOT]
```
After add some debug log, I found that in AuthUtils::getConfiguration, It get ""java.security.auth.login.config"" from storm_conf, but there is never someone had put ""java.security.auth.login.config"" to storm_config,(https://github.com/apache/storm/blob/master/storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java#L55), and then login_conf will always be null, so (https://github.com/apache/storm/blob/master/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java#L77) will got NPE;
"
STORM-856,"If nimbus goes down with a delayed kill topo cmd, it never comes up","

1. Describe observed behavior.
nimbus is in a crash loop

2. What is the expected behavior?
nimbus stays up

3. Outline the steps to reproduce the problem.
launch a topo
kill topo with `storm kill $topo` (no wait argument, so it defaults to 30s)
Immediately restart nimbus
"
STORM-854,[Storm-Kafka] KafkaSpout can  set the topic name as the output streamid,KafkaSpout can  set the topic name as the output streamid
STORM-852,Direct use of org.apache.log4j logger,"The following Java classes directly import and use Log4j APIs when they should be using the SLF4J Logger class.

storm-core:
  BlowfishTupleSerializer
  ShellProcess

storm-starter:
  RollingTopWords
  SkewedRollingTopWords
  IntermediateRankingsBolt
  AbstractRankerBolt
  TotalRankingsBolt
  RollingCountBolt
  RollingCountAggBolt



"
STORM-842,Drop Support for Java 1.6,
STORM-841,Thread-safeness of OutputCollector has documented contrary to two official doc.,"There're some issues with documentation.

http://storm.apache.org/documentation/Concepts.html says

{quote}
Its perfectly fine to launch new threads in bolts that do processing asynchronously. OutputCollector is thread-safe and can be called at any time.
{quote}

and http://storm.apache.org/documentation/Troubleshooting.html says

{quote}
This is caused by having multiple threads issue methods on the OutputCollector. All emits, acks, and fails must happen on the same thread. One subtle way this can happen is if you make a IBasicBolt that emits on a separate thread. IBasicBolt's automatically ack after execute is called, so this would cause multiple threads to use the OutputCollector leading to this exception. When using a basic bolt, all emits must happen in the same thread that runs execute.
{quote}

It is a contradiction, and at least for now OutputCollector is not thread-safe.
https://www.mail-archive.com/dev@storm.incubator.apache.org/msg00939.html

Since newbie of Storm users may think Concepts page as ""should read and keep it mind"", it is some kind of critical that that such important documentation page has wrong content."
STORM-840,My supervisor crashes when I kill a topology,"Hello,
I run 3 topologies inside my cluster.
Sometimes, when I kill one of them (not one specific). One supervisor goes down and restart. After few restart, it become stable.
The topology process is in ""Zombie state"" in the process list.

In version 0.9.3, all the supervisors crashed and couldn't restart. To resolve this, I had to ""rm -fr <storm-local-dir>/workers/""
So I migrate to 0.9.4 (I thought that was STORM-682).

Now it continues but no all the times, but occasionally.

I have these logs inside supervisor.log:
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Removing code for storm id nlp-11-1432906756
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Removing code for storm id nlp-11-1432906756
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down and clearing state for id 355af307-fafc-43a8-865d-0dfbf9baee33. Current supervisor time: 1432911702. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1432911702, :storm-id ""nlp-11-1432906756"", :executors #{[2 2] [3 3] [-1 -1] [1 1]}, :port 6700}
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down and clearing state for id 355af307-fafc-43a8-865d-0dfbf9baee33. Current supervisor time: 1432911702. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1432911702, :storm-id ""nlp-11-1432906756"", :executors #{[2 2] [3 3] [-1 -1] [1 1]}, :port 6700}
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down 90f0964b-c48c-4cbc-9d1c-57119c56e99c:355af307-fafc-43a8-865d-0dfbf9baee33
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down 90f0964b-c48c-4cbc-9d1c-57119c56e99c:355af307-fafc-43a8-865d-0dfbf9baee33
2015-05-29 15:01:42 b.s.event [ERROR] Error when processing event
java.io.IOException: Cannot run program ""kill"" (in directory "".""): error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) ~[na:1.8.0_45]
        at java.lang.Runtime.exec(Runtime.java:620) ~[na:1.8.0_45]
        at org.apache.commons.exec.launcher.Java13CommandLauncher.exec(Java13CommandLauncher.java:58) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.launch(DefaultExecutor.java:254) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:319) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:160) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147) ~[commons-exec-1.1.jar:1.1]
        at backtype.storm.util$exec_command_BANG_.invoke(util.clj:386) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$send_signal_to_process.invoke(util.clj:415) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$kill_process_with_sig_term.invoke(util.clj:426) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:197) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:267) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:40) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.forkAndExec(Native Method) ~[na:1.8.0_45]
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:248) ~[na:1.8.0_45]
        at java.lang.ProcessImpl.start(ProcessImpl.java:134) ~[na:1.8.0_45]
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ~[na:1.8.0_45]
        ... 19 common frames omitted
2015-05-29 15:01:42 b.s.event [ERROR] Error when processing event
java.io.IOException: Cannot run program ""kill"" (in directory "".""): error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) ~[na:1.8.0_45]
        at java.lang.Runtime.exec(Runtime.java:620) ~[na:1.8.0_45]
        at org.apache.commons.exec.launcher.Java13CommandLauncher.exec(Java13CommandLauncher.java:58) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.launch(DefaultExecutor.java:254) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.executeInternal(DefaultExecutor.java:319) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:160) ~[commons-exec-1.1.jar:1.1]
        at org.apache.commons.exec.DefaultExecutor.execute(DefaultExecutor.java:147) ~[commons-exec-1.1.jar:1.1]
        at backtype.storm.util$exec_command_BANG_.invoke(util.clj:386) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$send_signal_to_process.invoke(util.clj:415) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.util$kill_process_with_sig_term.invoke(util.clj:426) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$shutdown_worker.invoke(supervisor.clj:197) ~[storm-core-0.9.4.jar:0.9.4]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:267) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.5.1.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
        at clojure.core$apply.invoke(core.clj:619) ~[clojure-1.5.1.jar:na]
        at clojure.core$partial$fn__4190.doInvoke(core.clj:2396) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:40) ~[storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
Caused by: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.forkAndExec(Native Method) ~[na:1.8.0_45]
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:248) ~[na:1.8.0_45]
        at java.lang.ProcessImpl.start(ProcessImpl.java:134) ~[na:1.8.0_45]
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ~[na:1.8.0_45]
        ... 19 common frames omitted
2015-05-29 15:01:42 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:48) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
2015-05-29 15:01:42 b.s.util [ERROR] Halting process: (""Error when processing an event"")
java.lang.RuntimeException: (""Error when processing an event"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:325) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.event$event_manager$fn__2809.invoke(event.clj:48) [storm-core-0.9.4.jar:0.9.4]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down supervisor 90f0964b-c48c-4cbc-9d1c-57119c56e99c
2015-05-29 15:01:42 b.s.d.supervisor [INFO] Shutting down supervisor 90f0964b-c48c-4cbc-9d1c-57119c56e99c
2015-05-29 15:01:42 b.s.event [INFO] Event manager interrupted
2015-05-29 15:01:42 b.s.event [INFO] Event manager interrupted
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:host.name=storm-supervisor-01
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.version=1.8.0_45
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.vendor=Oracle Corporation
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.home=/usr/lib/jvm/jre-8-oracle-x64/jre
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.class.path=/usr/share/apache-storm-0.9.4/lib/zookeeper-3.4.6.jar:/usr/share/apache-storm-0.9.4/lib/hiccup-0.3.6.jar:/usr/share/apache-storm-0.9.4/lib/chill-java-0.3.5.jar:/usr/share/apache-storm-0.9.4/lib/commons-exec-1.1.jar:/usr/share/apache-storm-0.9.4/lib/tools.macro-0.1.0.jar:/usr/share/apache-storm-0.9.4/lib/jgrapht-core-0.9.0.jar:/usr/share/apache-storm-0.9.4/lib/ring-servlet-0.3.11.jar:/usr/share/apache-storm-0.9.4/lib/clout-1.0.1.jar:/usr/share/apache-storm-0.9.4/lib/storm-core-0.9.4.jar:/usr/share/apache-storm-0.9.4/lib/asm-4.0.jar:/usr/share/apache-storm-0.9.4/lib/tools.cli-0.2.4.jar:/usr/share/apache-storm-0.9.4/lib/disruptor-2.10.1.jar:/usr/share/apache-storm-0.9.4/lib/log4j-over-slf4j-1.6.6.jar:/usr/share/apache-storm-0.9.4/lib/clj-time-0.4.1.jar:/usr/share/apache-storm-0.9.4/lib/slf4j-api-1.7.5.jar:/usr/share/apache-storm-0.9.4/lib/clojure-1.5.1.jar:/usr/share/apache-storm-0.9.4/lib/core.incubator-0.1.0.jar:/usr/share/apache-storm-0.9.4/lib/json-simple-1.1.jar:/usr/share/apache-storm-0.9.4/lib/logback-classic-1.0.13.jar:/usr/share/apache-storm-0.9.4/lib/servlet-api-2.5.jar:/usr/share/apache-storm-0.9.4/lib/logback-core-1.0.13.jar:/usr/share/apache-storm-0.9.4/lib/jetty-6.1.26.jar:/usr/share/apache-storm-0.9.4/lib/clj-stacktrace-0.2.2.jar:/usr/share/apache-storm-0.9.4/lib/ring-devel-0.3.11.jar:/usr/share/apache-storm-0.9.4/lib/minlog-1.2.jar:/usr/share/apache-storm-0.9.4/lib/kryo-2.21.jar:/usr/share/apache-storm-0.9.4/lib/compojure-1.1.3.jar:/usr/share/apache-storm-0.9.4/lib/commons-codec-1.6.jar:/usr/share/apache-storm-0.9.4/lib/tools.logging-0.2.3.jar:/usr/share/apache-storm-0.9.4/lib/ring-jetty-adapter-0.3.11.jar:/usr/share/apache-storm-0.9.4/lib/jetty-util-6.1.26.jar:/usr/share/apache-storm-0.9.4/lib/joda-time-2.0.jar:/usr/share/apache-storm-0.9.4/lib/jline-2.11.jar:/usr/share/apache-storm-0.9.4/lib/commons-logging-1.1.3.jar:/usr/share/apache-storm-0.9.4/lib/reflectasm-1.07-shaded.jar:/usr/share/apache-storm-0.9.4/lib/carbonite-1.4.0.jar:/usr/share/apache-storm-0.9.4/lib/snakeyaml-1.11.jar:/usr/share/apache-storm-0.9.4/lib/objenesis-1.2.jar:/usr/share/apache-storm-0.9.4/lib/ring-core-1.1.5.jar:/usr/share/apache-storm-0.9.4/lib/commons-io-2.4.jar:/usr/share/apache-storm-0.9.4/lib/commons-fileupload-1.2.1.jar:/usr/share/apache-storm-0.9.4/lib/math.numeric-tower-0.0.1.jar:/usr/share/apache-storm-0.9.4/lib/commons-lang-2.5.jar:/usr/share/apache-storm-0.9.4/conf
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.library.path=/usr/local/lib:/opt/local/lib:/usr/lib
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.io.tmpdir=/tmp
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:java.compiler=<NA>
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:os.name=Linux
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:os.arch=amd64
2015-05-29 15:01:53 o.a.s.z.ZooKeeper [INFO] Client environment:os.version=3.16.0-0.bpo.4-amd64
...
"
STORM-839,Deadlock hazard in backtype.storm.messaging.netty.Client,"See the thread dump below that shows the deadlock. client-worker-1 is holding 7b5a7fa5 and waiting on 1446a1e9. Thread-10-disruptor-worker-transfer-queue is holding 1446a1e9 and is waiting on 7b5a7fa5.

(Thread dump is truncated to show only the relevant parts)

2015-05-28 15:37:15
Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.72-b04 mixed mode):


""Thread-10-disruptor-worker-transfer-queue"" - Thread t@52
   java.lang.Thread.State: BLOCKED
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.cleanUpWriteBuffer(AbstractNioWorker.java:398)
	- waiting to lock <7b5a7fa5> (a java.lang.Object) owned by ""client-worker-1"" t@25
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.writeFromUserCode(AbstractNioWorker.java:128)
	at org.apache.storm.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:84)
	at org.apache.storm.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779)
	at org.apache.storm.netty.channel.Channels.write(Channels.java:725)
	at org.apache.storm.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
	at org.apache.storm.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
	at org.apache.storm.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
	at org.apache.storm.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
	at org.apache.storm.netty.channel.Channels.write(Channels.java:704)
	at org.apache.storm.netty.channel.Channels.write(Channels.java:671)
	at org.apache.storm.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
	at backtype.storm.messaging.netty.Client.flushMessages(Client.java:480)
	- locked <1446a1e9> (a backtype.storm.messaging.netty.Client)
	at backtype.storm.messaging.netty.Client.send(Client.java:412)
	- locked <1446a1e9> (a backtype.storm.messaging.netty.Client)
	at backtype.storm.utils.TransferDrainer.send(TransferDrainer.java:54)
	at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__5014$fn__5015.invoke(worker.clj:334)
	at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__5014.invoke(worker.clj:332)
	at backtype.storm.disruptor$clojure_handler$reify__1446.onEvent(disruptor.clj:58)
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125)
	at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99)
	at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80)
	at backtype.storm.disruptor$consume_loop_STAR_$fn__1459.invoke(disruptor.clj:94)
	at backtype.storm.util$async_loop$fn__458.invoke(util.clj:463)
	at clojure.lang.AFn.run(AFn.java:24)
	at java.lang.Thread.run(Unknown Source)

   Locked ownable synchronizers:
	- None

""client-worker-1"" - Thread t@25
   java.lang.Thread.State: BLOCKED
	at backtype.storm.messaging.netty.Client.closeChannelAndReconnect(Client.java:501)
	- waiting to lock <1446a1e9> (a backtype.storm.messaging.netty.Client) owned by ""Thread-10-disruptor-worker-transfer-queue"" t@52
	at backtype.storm.messaging.netty.Client.access$1400(Client.java:78)
	at backtype.storm.messaging.netty.Client$3.operationComplete(Client.java:492)
	at org.apache.storm.netty.channel.DefaultChannelFuture.notifyListener(DefaultChannelFuture.java:427)
	at org.apache.storm.netty.channel.DefaultChannelFuture.notifyListeners(DefaultChannelFuture.java:413)
	at org.apache.storm.netty.channel.DefaultChannelFuture.setFailure(DefaultChannelFuture.java:380)
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.cleanUpWriteBuffer(AbstractNioWorker.java:437)
	- locked <7b5a7fa5> (a java.lang.Object)
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:373)
	at org.apache.storm.netty.channel.socket.nio.NioWorker.read(NioWorker.java:93)
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
	at org.apache.storm.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
	at org.apache.storm.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.apache.storm.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.apache.storm.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.apache.storm.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)

   Locked ownable synchronizers:
	- locked <75e528fd> (a java.util.concurrent.ThreadPoolExecutor$Worker)

"
STORM-836,DPRC request failed after DRPC server restarted,"When the DRPC server was killed and then restarted, the DRPC client would throw a DRPCExecutionException (msg:Request failed) for a while, off and on.

Here is my client code:
{code:java}
DRPCClient client = new DRPCClient(""hd181"", 3772);

for (int i = 0; i < 100; ++i) {
	System.out.println(client.execute(""drpcFunc"", ""aaa""));
}
{code}

and the error messages:
{code}
Exclamation-6:aaa!!!
Exception in thread ""main"" DRPCExecutionException(msg:Request failed)
	at backtype.storm.generated.DistributedRPC$execute_result.read(DistributedRPC.java:904)
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:78)
	at backtype.storm.generated.DistributedRPC$Client.recv_execute(DistributedRPC.java:92)
	at backtype.storm.generated.DistributedRPC$Client.execute(DistributedRPC.java:78)
	at backtype.storm.utils.DRPCClient.execute(DRPCClient.java:71)
	at com.enjoyor.storm.kafka.example.DRPCTest.main(DRPCTest.java:18)
{code}

As you can see, the client can get few valid results at beginning and then get a failed request.

This problem would last about 10 minutes after restarting the server, and then the server would be back to normal."
STORM-835,Netty Client hold batch object until io operation complete,"Netty Client hold batch object until IO operation complete.
Following code:
{code:title=Client.java|borderStyle=solid}
 final int numMessages = batch.size();
        pendingMessages.getAndAdd(numMessages);
        LOG.debug(""writing {} messages to channel {}"", batch.size(), channel.toString());
        ChannelFuture future = channel.write(batch);
        future.addListener(new ChannelFutureListener() {

            public void operationComplete(ChannelFuture future) throws Exception {
                pendingMessages.getAndAdd(0 - numMessages);
                if (future.isSuccess()) {
                    LOG.debug(""sent {} messages to {}"", numMessages, dstAddressPrefixedName);
                    messagesSent.getAndAdd(batch.size());
                }

{code}

batch will be a field of  anonymous inner classes.don't release after channel.write() invoked,until IO operation complete.



"
STORM-834,Class cast exception Object to Iterable,"This exception is happening after some hours of running. Unfortunately, nothing in the trace suggests the source of the error for further investigation.

2015-05-23T13:24:22.343-0400 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.daemon.executor$fn__3441$fn__3453$fn__3500.invoke(executor.clj:748) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.util$async_loop$fn__464.invoke(util.clj:463) ~[storm-core-0.9.3.jar:0.9.3]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_75]
Caused by: java.lang.ClassCastException: java.lang.Object cannot be cast to java.lang.Iterable
        at backtype.storm.util$get_iterator.invoke(util.clj:867) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.daemon.executor$mk_task_receiver$fn__3364.invoke(executor.clj:397) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.disruptor$clojure_handler$reify__1447.onEvent(disruptor.clj:58) ~[storm-core-0.9.3.jar:0.9.3]
        at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:120) ~[storm-core-0.9.3.jar:0.9.3]
        ... 6 common frames omitted
"
STORM-832,Validation for configuration settings beyond those in storm.yaml,"It would be good to provide a way to validate configuration settings that are for plug-in functionality.

One example of config for plug-in functionality is the [MultitenantScheduler user pools|https://github.com/apache/storm/blob/bafb86bb01c29bf77f312ab346dd0a9c48293c36/storm-core/src/jvm/backtype/storm/scheduler/multitenant/MultitenantScheduler.java#L52].

Validation would be nice for the stability of the cluster and so that we can provide better error details to users if something is broken due to a bad config.
"
STORM-821,storm-jdbc create a connection provider interface to decouple from hikariCP being the only connection pool implementation that can be used.,The current implementation of storm-jdbc is couple with HikariCP configuration. We propose to remove this coupling by introducing a connectionProvider interface with a default HikariCP implementation. This will allow users to do their own connection pool management or chose a different connection pooling library. 
STORM-820,"UI Topology & Component Pages have long load times with large, highly-connected Topologies","In the UI, the Topology Page and the Component Page each make a getTopologyInfoWithOpts thrift call to nimbus for executor heartbeat data. Metrics from this data are then aggregated in by the UI daemon for display.

When large topologies, with high-connectedness, are viewed in this way, the load times for each page can be minutes long.  In addition, heap usage by the nimbus JVM can grow substantially as data for each executor, component, & stream is serialized to be sent to the UI."
STORM-818,storm-eventhubs configuration improvement and refactoring,"In Storm-583 EventHubSpout and EventHubBolt implementation were added. During the review and licensing clearance process, we've improved the code base around the spout/bolt configuration. Specifically, the following changes need to be checked in:

1. EventHubBolt improvement: add event formatter to format tuples into bytes for EventHubs and support each bolt sending to a separate EH partition etc.

2. Refactor EventHubSpoutConfig, use setXXX() instead of specifying configuration in constructor for optional configurations.

3. EventHubSpoutConfig specify consumer group name

4. Workaround for Qpid bug that cannot receive messages from EventHubs in rare cases

5. Upgrade to newer Qpid version 0.32"
STORM-814,Kafka Spout performance,"I am running few test for storm topology with kafka. 

Created a topic with 16 partitions and emitted 10k messages/sec with each message 1k size.

When I set the kafka spout parallelism to 1, I am getting latency of 8.9 ms

but  when i increase the parallelism to 8, I am getting latency of 1.2 sec.

Settings : 
MAX_SPOUT SPENDING set to 1000-5000

String zkConnString = ""zookeeper1:2181"";
        String kafkaTopic = ""messages"";
        String zkRootPath = ""/kafkaStorm"";
        String zkOffSetID = ""kafka"";
"
STORM-813,Storm-starter cannot run multilang topology with exec:java with new multilang packages,"After STORM-748, storm-starter doesn't have multilang files now, and just having multilang artifact as dependency.
With supervisor it works like a charm, but now mvn exec:java with multilang topology doesn't work.

It would be better to remove broken functionality from README.markdown or make it work."
STORM-810,PartitionManager in storm-kafka should commit latest offset before close ,"When KafkaSpout refresh with kafka partitions, it will remove patitions that no longer served by this spout and close the connection. But before close the connection, PartitionManager don't commit the latest offset. As a result some other spout that serve the removed partition will replay some tuple that between last commit and close."
STORM-807,storm script does not handle whitespace.,"STORM-675 changed bin/storm from python to bash, but missed quoting arguments passed into storm.py.  This results in errors when there is white space on the command line, especially around configs."
STORM-806,use storm.zookeeper.connection.timeout in storm-kafka ZkState when newCurator,
STORM-804,HdfsBolt doesn't work after a period of time which caused by network problems,"  Two clusters i use in my  test case.
  I submit a topology which write message to hdfs in another cluster,HdfsBolt i use in this topology.It work normally in the beginning, then i intercept the message for a while(30 minites), after that, the bolt write failed all the time.
  I think  HdfsBolt.excute should reconnect and recreate filesystem after it keeping catch exception for a period of time."
STORM-796,"The ""error"" command isn't supported on ShellSpout","The `ShellBolt` can handle the ""error"" command, as shown in this file in Storm source code:

https://github.com/apache/storm/blob/2dd7a9426e5634211f14cf5c4e10e021d3420c3c/storm-core/src/jvm/backtype/storm/task/ShellBolt.java#L330

But, `ShellSpout` does not actually have a handler for ""error"".

https://github.com/apache/storm/blob/2dd7a9426e5634211f14cf5c4e10e021d3420c3c/storm-core/src/jvm/backtype/storm/spout/ShellSpout.java#L153-L175

The symptoms a multi-lang user will see here is that if their Spout throws an error and their multi-lang implementation sends an ""error"" command up to the ShellSpout, the ShellSpout will respond saying that it doesn't recognize the ""error"" command, and thus it will crash (while swallowing the exception thrown by the underlying multi-lang component).

I am about to open a PR on Github that fixes this.

Originally reported on the streamparse project in this Github issue:

https://github.com/Parsely/streamparse/issues/121"
STORM-794,Trident Topology with some situation seems not handle deactivate during graceful shutdown,"I met an issue from Trident Topology in production env.

Normally, when we kill a topology via UI, Nimbus changes Topology status to ""killed"", and when Spout determines new status, it becomes deactivated so bolts can handle remain tuples within wait-time.
AFAIK that's how Storm guarantees graceful shutdown.

But, Trident Topology seems not handle ""deactivate"" while we try shutdown topology gracefully.
MasterBatchCoordinator never stops making next transaction, so Trident Spout never stops emitting, bolts (function) always take care of tuples.

Topology setting
- 1 worker, 1 acker
- max spout pending: 1
- TOPOLOGY_TRIDENT_BATCH_EMIT_INTERVAL_MILLIS : 5
-- It may be weird but MasterBatchCoordinator's default value is 1

* Nimbus log

{code}
2015-04-20 09:59:07.954 INFO  [pool-5-thread-41][nimbus] Delaying event :remove for 120 secs for BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015
...
2015-04-20 09:59:07.955 INFO  [pool-5-thread-41][nimbus] Updated BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015 with status {:type :killed, :kill-time-secs 120}
...
2015-04-20 10:01:07.956 INFO  [timer][nimbus] Killing topology: BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015
...
2015-04-20 10:01:14.448 INFO  [timer][nimbus] Cleaning up BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015
{code}

* Supervisor log

{code}
2015-04-20 10:01:07.960 INFO  [Thread-1][supervisor] Removing code for storm id BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015
2015-04-20 10:01:07.962 INFO  [Thread-2][supervisor] Shutting down and clearing state for id 9719259e-528c-4336-abf9-592c1bb9a00b. Current supervisor time: 1429491667. State: :disallowed, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time-secs 1429491667, :storm-id ""BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015"", :executors #{[2 2] [3 3] [4 4] [5 5] [6 6] [7 7] [8 8] [9 9] [10 10] [11 11] [12 12] [13 13] [14 14] [-1 -1] [1 1]}, :port 6706}
2015-04-20 10:01:07.962 INFO  [Thread-2][supervisor] Shutting down 5bc084a2-b668-4610-86f6-9b93304d40a8:9719259e-528c-4336-abf9-592c1bb9a00b
2015-04-20 10:01:08.974 INFO  [Thread-2][supervisor] Shut down 5bc084a2-b668-4610-86f6-9b93304d40a8:9719259e-528c-4336-abf9-592c1bb9a00b
{code}

* Worker log

{code}
2015-04-20 10:01:07.985 INFO  [Thread-33][worker] Shutting down worker BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015 5bc084a2-b668-4610-86f6-9b93304d40a8 6706
2015-04-20 10:01:07.985 INFO  [Thread-33][worker] Shutting down receive thread
2015-04-20 10:01:07.988 WARN  [Thread-33][ExponentialBackoffRetry] maxRetries too large (300). Pinning to 29
2015-04-20 10:01:07.988 INFO  [Thread-33][StormBoundedExponentialBackoffRetry] The baseSleepTimeMs [100] the maxSleepTimeMs [1000] the maxRetries [300]
2015-04-20 10:01:07.988 INFO  [Thread-33][Client] New Netty Client, connect to localhost, 6706, config: , buffer_size: 5242880
2015-04-20 10:01:07.991 INFO  [client-schedule-service-1][Client] Reconnect started for Netty-Client-localhost/127.0.0.1:6706... [0]
2015-04-20 10:01:07.996 INFO  [Thread-33][loader] Shutting down receiving-thread: [BFDC-topology-DynamicCollect-68c9d7b4-72-1429491015, 6706]
...
2015-04-20 10:01:08.044 INFO  [Thread-33][Client] Closing Netty Client Netty-Client-localhost/127.0.0.1:6706
2015-04-20 10:01:08.044 INFO  [Thread-33][Client] Waiting for pending batchs to be sent with Netty-Client-localhost/127.0.0.1:6706..., timeout: 600000ms, pendings: 1
{code}

I found activating log, but cannot find deactivating log.

{code}
2015-04-20 09:50:24.556 INFO  [Thread-30-$mastercoord-bg0][executor] Activating spout $mastercoord-bg0:(1)
{code}

Please note that it doesn't work when I just push button to ""deactivate"" topology via UI.

We're changing our Topology to normal Spout-Bolt, but personally I'd like to see it resolved. "
STORM-793,Logviewer 500 response when metadata has not yet been written (with auth enabled),"When ui.filter is defined and used, and a user navigates to a logviewer link for which the logging metadata has not yet been initialized, we [throw an NPE|https://github.com/apache/storm/blob/84e8bc6d28b54056dd75375be7d316ab03125fb6/storm-core/src/clj/backtype/storm/daemon/logviewer.clj#L184] that results in a 500 response."
STORM-790,"Log ""task id is null"" instead of let worker died (NPE in consumeBatchToCursor)","In STORM-770, some users have observed that worker suddenly died with NPE in consumeBatchToCursor().

Looks like it can occur when ""task"" in ""mk-transfer-fn"" is null.

It was also an issue equal or before 0.9.2-incubating and it throws NPE, too.
Lower than 0.9.2 version, you can see NPE from KryoTupleSerializer.serialize.
And at 0.9.2 and higher version, you can see NPE from clojure.lang.RT.intCast.

Before finding root cause of this issue, it would be better to let worker not killed by this issue but just log with WARN or ERROR level.
It really makes sense cause with Guaranteeing Message Processing, after timed-out tuple will be replayed. (It isn't applied to non-ack)"
STORM-788,UI > Component Page > Input shows execute latency where it should show process latency,The execute latency value is show in place of the process latency.
STORM-786,KafkaBolt should ack tick tuples,"STORM-512 (KafkaBolt doesn't handle ticks properly) adds special-casing of tick tuples.  What is missing in the patch is that the input tuple, when it is a tick tuple, should be properly acked like normal tuples."
STORM-771,Authentication with Kerberos,"I am using Storm in a Kerberized Cluster. 
There is an user ""Robin"" in the Storm server. And I follow the steps below to generate keytab for Robin.
{noformat}
# /usr/sbin/kadmin.local
# kadmin.local: addprinc -randkey Robin@EXAMPLE.COM
# kadmin.local: xst -norandkey -k Robin.keytab Robin
# scp Robin.keytab Robin@storm_server:/home/Robin
{noformat}
After these, I login the Storm server as Robin. And authenticate Robin with his own keytab(Robin.keytab)
{noformat}
# kinit -k -t Robin.keytab Robin
{noformat}
The output of klist is
{noformat}
Ticket cache: FILE:/tmp/krb5cc_1006
Default principal: Robin@EXAMPLE.COM

Valid starting     Expires            Service principal
04/15/15 11:34:19  04/16/15 11:34:19  krbtgt/EXAMPLE.COM@EXAMPLE.COM
        renew until 04/15/15 11:34:19
{noformat}

But there was an authentication error occurred when I executed 
{noformat}
#storm list
{noformat}
The error was
{noformat}
Exception in thread ""main"" java.lang.RuntimeException: javax.security.auth.login.LoginException: No password provided
        at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:108)
        at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48)
        at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:97)
        at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:66)
        at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:47)
        at backtype.storm.thrift$nimbus_client_and_conn.invoke(thrift.clj:71)
        at backtype.storm.command.list$_main.invoke(list.clj:22)
        at clojure.lang.AFn.applyToHelper(AFn.java:159)
        at clojure.lang.AFn.applyTo(AFn.java:151)
        at backtype.storm.command.list.main(Unknown Source)
Caused by: javax.security.auth.login.LoginException: No password provided
        at com.sun.security.auth.module.Krb5LoginModule.promptForPass(Krb5LoginModule.java:878)
        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:719)
{noformat}

Here is my Kerberos settings in ""storm.yaml""
{noformat}
storm.principal.tolocal: ""backtype.storm.security.auth.KerberosPrincipalToLocal""
storm.zookeeper.superACL: ""sasl:storm""
java.security.auth.login.config: ""/etc/storm/conf/storm_jaas.conf""
nimbus.admins:
  - ""storm""
nimbus.supervisor.users:
  - ""storm""
nimbus.authorizer: ""backtype.storm.security.auth.authorizer.SimpleACLAuthorizer""
drpc.authorizer: ""backtype.storm.security.auth.authorizer.DRPCSimpleACLAuthorizer""

ui.filter: ""org.apache.hadoop.security.authentication.server.AuthenticationFilter""
ui.filter.params:
  ""type"": ""kerberos""
  ""kerberos.principal"": ""HTTP/slave""
  ""kerberos.keytab"": ""/etc/security/keytabs/spnego.service.keytab""
  ""kerberos.name.rules"": ""DEFAULT""
supervisor.enable: true
{noformat}
And ""storm_jaas.conf""
{noformat}
StormServer {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab=""/etc/security/keytabs/nimbus.service.keytab""
   storeKey=true
   useTicketCache=false
   principal=""nimbus/slave@EXAMPLE.COM"";
};
StormClient {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab=""/etc/security/keytabs/storm.service.keytab""
   storeKey=true
   useTicketCache=false
   serviceName=""nimbus""
   principal=""storm@EXAMPLE.COM"";
};
Client {
   com.sun.security.auth.module.Krb5LoginModule required
   useKeyTab=true
   keyTab=""/etc/security/keytabs/storm.service.keytab""
   storeKey=true
   useTicketCache=false
   serviceName=""zookeeper""
   principal=""storm@EXAMPLE.COM"";
};
{noformat}

By the way, the cluster is installed via Ambari 1.7.

Thanks in advanced."
STORM-770,NullPointerException in consumeBatchToCursor,"We got the following exception after our topology had been up for ~2 days, and I was wondering if it might be related. 
Looks like ""task"" in ""mk-transfer-fn"" is null, making ""(.add remote (TaskMessage. task (.serialize serializer tuple)))"" fail on NPE (worker.clj:128, storm-core-0.9.2-incubating.jar)

java.lang.RuntimeException: java.lang.NullPointerException
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$consume_loop_STAR_$fn__758.invoke(disruptor.clj:94) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.7.0_72]
Caused by: java.lang.NullPointerException: null
at clojure.lang.RT.intCast(RT.java:1087) ~[clojure-1.5.1.jar:na]
at backtype.storm.daemon.worker$mk_transfer_fn$fn__5748.invoke(worker.clj:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.daemon.executor$start_batch_transfer_GT_worker_handler_BANG$fn__5483.invoke(executor.clj:256) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$clojure_handler$reify__745.onEvent(disruptor.clj:58) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
... 6 common frames omitted,java.lang.RuntimeException: java.lang.NullPointerException

Any ideas?

P.S.
Also saw it here: 
http://mail-archives.apache.org/mod_mbox/storm-user/201501.mbox/%3CCABcMBhCusXXU=V1e66wfUATGYH1euQnd1SiOG65-Tp8xLWx0ww@mail.gmail.com%3E

https://mail-archives.apache.org/mod_mbox/storm-user/201408.mbox/%3CCAJuQM_4KXHSH2_X08ujuQR76m2C+Dswp0fCiJBmfCAeyqgsFHQ@mail.gmail.com%3E

Comment from Bobby
http://mail-archives.apache.org/mod_mbox/storm-user/201501.mbox/%3C574363643.2791948.1420470097280.JavaMail.yahoo@jws10027.mail.ne1.yahoo.com%3E

{quote}
What version of storm are you using?  Are any of the bolts shell bolts?  There is a known
issue where this can happen if two shell bolts share an executor, because they are multi-threaded. 
- Bobby
{quote}"
STORM-769,Storm can not identify JAVA_HOME if the of java home contains a space,"I installed java 1.7.0_75 in windows 7 through .exe file and setted the JAVA_HOME path which is ""D:\Program Files\Java\jdk1.7.0_75"".
Run the ""java -version"" command to make sure the jdk is ok.
After I setted up all environment variable about storm, I run the command ""storm version""
the terminate shows ""Error: JAVA_HOME is incorrectly set.""
So I delete following code in storm.cmd file
""
if not defined JAVA_HOME (
  set JAVA_HOME=c:\apps\java\openjdk7
)

if not exist %JAVA_HOME%\bin\java.exe (
  echo Error: JAVA_HOME is incorrectly set.
  goto :eof
)
""
and run ""storm version"" command again,
it shows that 'D:\Program' is not recognized as an internal or external command,operable program or batch file.
The storm.cmd script can not deal with a JAVA_HOME path which contains a space like ""D:\Program Files\Java\jdk1.7.0_75"".
"
STORM-768,Support JDK 8 compile and runtime.,"storm project support JDK8 compilation and running. 

There are some check points and tests which should be passed.

* Unittests
* Server launch
* Topology submit and running

This might not be enough. Please list up here."
STORM-767,Unresolved dependency in storm-hive,"Building storm-hive fails due to unresolved dependencies about {{pentaho-aggdesigner}}.

{code}
[ERROR] Failed to execute goal on project storm-hive: Could not resolve dependencies for project org.apache.storm:storm-hive:jar:0.11.0-SNAPSHOT: Could not find artifact org.pentaho:pentaho-aggdesigner-algorithm:jar:5.1.3-jhyde in repo.jenkins-ci.org (http://repo.jenkins-ci.org/public/) -> [Help 1]
{code}
"
STORM-766,Supervisor summary should include the version.,"With the support for rolling upgrade, different nodes in the cluster can run different versions of storm. We should include the version in SupervisorSummary just like NimbusSummary so admins can identify nodes that needs upgrading/downgrading from UI. 

As part of this change I will also add a supervisor/log link in the ui."
STORM-763,"nimbus reassigned worker A to another machine, but other worker's netty client can't connect to the new worker A ","Debian 3.16.3-2~bpo70+1 (2014-09-21) x86_64 GNU/Linux
java version ""1.7.0_03""
storm 0.9.4
cluster 50+ machines

my topology have 50+ worker, it can't emit  50000 thousand tuples in ten minutes.
sometimes one worker is reassigned to another machine by nimbus because of task heartbeat timeout:
{code}
2015-04-08T16:51:23.026+0800 b.s.d.nimbus [INFO] Executor my_topology-22-1428243953:[440 440] not alive
2015-04-08T16:51:23.026+0800 b.s.d.nimbus [INFO] Executor my_topology-22-1428243953:[90 90] not alive
2015-04-08T16:51:23.026+0800 b.s.d.nimbus [INFO] Executor my_topology-22-1428243953:[510 510] not alive
2015-04-08T16:51:23.026+0800 b.s.d.nimbus [INFO] Executor my_topology-22-1428243953:[160 160] not alive
{code}

i can see the reassigned worker is already started in storm UI,  but  other worker write error log all the time:
{code}
2015-04-08T16:56:43.091+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-host_19/192.168.163.19:5700
2015-04-08T16:56:45.660+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-host_19/192.168.163.19:5700 is unavailable
2015-04-08T16:56:45.660+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-host_19/192.168.163.19:5700
2015-04-08T16:56:45.715+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-host_19/192.168.163.19:5700 is unavailable
2015-04-08T16:56:45.716+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-host_19/192.168.163.19:5700
2015-04-08T16:56:46.277+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-host_19/192.168.163.19:5700 is unavailable
2015-04-08T16:56:46.278+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-host_19/192.168.163.19:5700
2015-04-08T16:56:46.306+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-host_19/192.168.163.19:5700 is unavailable
2015-04-08T16:56:46.306+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-host_19/192.168.163.19:5700
2015-04-08T16:56:46.586+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-host_19/192.168.163.19:5700 is unavailable
2015-04-08T16:56:46.586+0800 b.s.m.n.Client [ERROR] dropping 1 message(s) destined for Netty-Client-host_19/192.168.163.19:5700
2015-04-08T16:56:46.835+0800 b.s.m.n.Client [ERROR] connection to Netty-Client-host_19/192.168.163.19:5700 is unavailable
{code}

The worker of destined host is already started, and i can telnet 192.168.163.19 5700.
however, why the netty client can't connect to the ip:port?"
STORM-762,uptime for worker heartbeats is lost when converted to thrift,"The uptime for the worker heartbeat is lost when converted to thrift.  The calculation for the uptime when it is converted back from thrift is wrong, and is actually how old the heartbeat is, not the uptime of the worker.

I'll try to throw up a patch for this tomorrow, but if someone else wants to try before then I am fine with that."
STORM-761,RedisMapState should be able to create/update expiring Redis keys.,
STORM-758,Very busy ShellBolt subprocess with Non-ACK mode cannot respond heartbeat just in time,"As [~dashengju] stated from STORM-738, very busy ShellBolt subprocess cannot respond heartbeat just in time.
Actually it's by design constraint (more details are on STORM-513 or STORM-738), but it's better to find a way to avoid.

My approach with ACK mode is here, STORM-742
This issue points Non-ACK mode. "
STORM-756,[multilang] Introduce overflow control mechanism,"It's from STORM-738, https://issues.apache.org/jira/browse/STORM-738?focusedCommentId=14394106&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14394106

A. ShellBolt side control

We can modify ShellBolt to have sent tuple ids list, and stop sending tuples when list exceeds configured max value. In order to achieve this, subprocess should notify ""tuple id is complete"" to ShellBolt.

* It introduces new commands for multi-lang, ""proceed"" (or better name)
* ShellBolt stores in-progress-of-processing tuples list.
* Its overhead could be big, subprocess should always notify to ShellBolt when any tuples are processed.

B. subprocess side control

We can modify subprocess to check pending queue after reading tuple.
If it exceeds configured max value, subprocess can request ""delay"" to ShellBolt for slowing down.

When ShellBolt receives ""delay"", BoltWriterRunnable should stop polling pending queue and continue polling later.

How long ShellBolt wait for resending? Its unit would be ""delay time"" or ""tuple count"". I don't know which is better yet.

* It introduces new commands for multi-lang, ""delay"" (or better name)
* I don't think it would be introduced soon, but subprocess can request delay based on own statistics. (ex. pending tuple count * average tuple processed time for time unit, average pending tuple count for count unit)
** We can leave when and how much to request ""delay"" to user. User can make his/her own algorithm to control flooding.

In my opinion B seems to more natural cause current issue is by subprocess side so it would be better to let subprocess overcome it."
STORM-755,An exception occured while executing the Java class. fyp-storm-try.src.jvm.Topology ,"ubuntu@ip-10-0-0-101:~/storm/examples/fyp-storm-try$ sudo mvn exec:java -D storm.topology=fyp-storm-try.src.jvm.Topology
[INFO] Scanning for projects...
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:fyp-storm-try:jar:0.9.4
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-javadoc-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 694, column 21
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 660, column 21
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING]
[INFO]
[INFO] ------------------------------------------------------------------------
[INFO] Building fyp-storm-try 0.9.4
[INFO] ------------------------------------------------------------------------
[INFO]
[INFO] >>> exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try >>>
[INFO]
[INFO] <<< exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try <<<
[INFO]
[INFO] --- exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try ---
[WARNING]
java.lang.ClassNotFoundException: fyp-storm-try.src.jvm.Topology
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:285)
        at java.lang.Thread.run(Thread.java:745)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.838s
[INFO] Finished at: Sun Apr 05 09:09:28 UTC 2015
[INFO] Final Memory: 9M/22M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project fyp-storm-try: An exception occured while executing the Java class. fyp-storm-try.src.jvm.Topology -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException


Below are my pom.xml:
?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
  <modelVersion>4.0.0</modelVersion>
  <parent>
      <artifactId>storm</artifactId>
      <groupId>org.apache.storm</groupId>
      <version>0.9.4</version>
      <relativePath>../../pom.xml</relativePath>
  </parent>

  <groupId>org.apache.storm</groupId>
  <artifactId>fyp-storm-try</artifactId>
  <packaging>jar</packaging>

  <name>fyp-storm-try</name>


  <dependencies>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.testng</groupId>
      <artifactId>testng</artifactId>
      <version>6.8.5</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.mockito</groupId>
      <artifactId>mockito-all</artifactId>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.easytesting</groupId>
      <artifactId>fest-assert-core</artifactId>
      <version>2.0M8</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.jmock</groupId>
      <artifactId>jmock</artifactId>
      <version>2.6.0</version>
      <scope>test</scope>
    </dependency>
    <dependency>
     <groupId>org.twitter4j</groupId>
     <artifactId>twitter4j-stream</artifactId>
     <version>3.0.3</version>
    </dependency>
    <dependency>
      <groupId>org.apache.storm</groupId>
      <artifactId>storm-core</artifactId>
      <version>${project.version}</version>
      <!-- keep storm out of the jar-with-dependencies -->
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.2.1</version>
    </dependency>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
    </dependency>
  </dependencies>

  <build>
    <sourceDirectory>src/jvm</sourceDirectory>
    <testSourceDirectory>test/jvm</testSourceDirectory>
    <resources>
      <resource>
        <directory>${basedir}/multilang</directory>
      </resource>
    </resources>

    <plugins>
      <!--
        Bind the maven-assembly-plugin to the package phase
        this will create a jar file without the storm dependencies
        suitable for deployment to a cluster.
       -->
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
          <archive>
            <manifest>
              <mainClass />
            </manifest>
          </archive>
        </configuration>
        <executions>
          <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
              <goal>single</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>com.theoryinpractise</groupId>
        <artifactId>clojure-maven-plugin</artifactId>
        <extensions>true</extensions>
        <configuration>
          <sourceDirectories>
            <sourceDirectory>src/clj</sourceDirectory>
          </sourceDirectories>
        </configuration>
        <executions>
          <execution>
            <id>compile</id>
            <phase>compile</phase>
            <goals>
              <goal>compile</goal>
            </goals>
          </execution>
        </executions>
      </plugin>

      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <version>1.2.1</version>
        <executions>
          <execution>
		   <phase>test</phase>
            <goals>
              <goal>exec</goal>
            </goals>
          </execution>
        </executions>
        <configuration>
          <executable>java</executable>
          <includeProjectDependencies>true</includeProjectDependencies>
          <includePluginDependencies>false</includePluginDependencies>
          <classpathScope>compile</classpathScope>
          <mainClass>${storm.topology}</mainClass>
        </configuration>
      </plugin>
    </plugins>
  </build>
</project>
"
STORM-754,Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project fyp-storm-try: The parameters 'mainClass' for goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java are missing or invalid -> [Help 1],"ubuntu@ip-10-0-0-101:~/storm/examples/fyp-storm-try$ sudo mvn compile exec:java  
-D storm.topology=fyp.storm.try.Topology
[INFO] Scanning for projects...
[WARNING] 
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:fyp-storm-try:jar:0.9.4
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-javadoc-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 694, column 21
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.4, /root/.m2/repository/org/apache/storm/storm/0.9.4/storm-0.9.4.pom, line 660, column 21
[WARNING] 
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING] 
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING] 
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building fyp-storm-try 0.9.4
[INFO] ------------------------------------------------------------------------
[INFO] 
[INFO] --- maven-remote-resources-plugin:1.2.1:process (default) @ fyp-storm-try ---
[INFO] 
[INFO] --- maven-resources-plugin:2.5:resources (default-resources) @ fyp-storm-try ---
[debug] execute contextualize
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/ubuntu/storm/examples/fyp-storm-try/multilang
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ fyp-storm-try ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 5 source files to /home/ubuntu/storm/examples/fyp-storm-try/target/classes
[INFO] 
[INFO] --- clojure-maven-plugin:1.3.18:compile (compile) @ fyp-storm-try ---
[INFO] 
[INFO] >>> exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try >>>
[INFO] 
[INFO] <<< exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try <<<
[INFO] 
[INFO] --- exec-maven-plugin:1.2.1:java (default-cli) @ fyp-storm-try ---
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 10.364s
[INFO] Finished at: Wed Apr 01 12:43:59 UTC 2015
[INFO] Final Memory: 19M/51M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java (default-cli) on project fyp-storm-try: The parameters 'mainClass' for goal org.codehaus.mojo:exec-maven-plugin:1.2.1:java are missing or invalid -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginParameterException"
STORM-753,[storm-redis] Let Redis*StateUpdater to have additional mapper - converting Redis key/value to Storm Values,"https://issues.apache.org/jira/browse/STORM-723?focusedCommentId=14381626&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14381626

Currently Redis*StateUpdater use key prefix to let users customize key name.
Since it's less powerful, how about having 2 mappers which one is for converting tuple to key/value, and another one is for converting value from Redis to Storm Values?
You can refer https://github.com/apache/storm/blob/master/external/storm-hbase/src/main/java/org/apache/storm/hbase/trident/state/HBaseState.java.
"
STORM-752,[storm-redis] Clarify Redis*StateUpdater's expire is optional,"It would be better to clarify Redis*StateUpdater's expire is optional.

You can find relevant conversation on link.
https://issues.apache.org/jira/browse/STORM-723?focusedCommentId=14381221&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14381221"
STORM-751,Move javadoc aggregate to site phase.,"{code}

[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Storm ............................................. FAILURE [5.904s]
[INFO] maven-shade-clojure-transformer ................... SKIPPED
[INFO] storm-maven-plugins ............................... SKIPPED
[INFO] Storm Core ........................................ SKIPPED
[INFO] storm-starter ..................................... SKIPPED
[INFO] storm-kafka ....................................... SKIPPED
[INFO] storm-hdfs ........................................ SKIPPED
[INFO] storm-hbase ....................................... SKIPPED
[INFO] storm-hive ........................................ SKIPPED
[INFO] storm-jdbc ........................................ SKIPPED
[INFO] storm-redis ....................................... SKIPPED
[INFO] Storm JMS ......................................... SKIPPED
[INFO] Storm Cassandra Support ........................... SKIPPED
[INFO] Storm Binary Distribution ......................... SKIPPED
[INFO] Storm Source Distribution ......................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 6.826s
[INFO] Finished at: Fri Apr 03 14:12:15 EDT 2015
[INFO] Final Memory: 40M/2474M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project storm-kafka: Could not resolve dependencies for project org.apache.storm:storm-kafka:jar:0.10.0.2.3.0.0-1493: Could not find artifact org.apache.storm:storm-core:jar:0.10.0.2.3.0.0-1493 in public

{code}"
STORM-749,Remove CSRF check from rest API,I think we can safely get rid of the whole CSRF code. CSRF vulnerability is only exposed when websites use session based authentication. In our case we only use http authentication so we are not really vulnerable to CSRF attacks. Currently the CSRF check only hinders non browser clients.
STORM-746,Disable Spout Ack Init when there is no output task,"Suppose a user cannot easily modify the spout in the topology.
The user has temporarily disabled transferring of tuples from a spout, for debugging.

In this case, when acking is used, each time the spout emits, it sends a tuple to the acker bolt.  The bolt executes on this tuple by initializing the bit-field used for tracking when the tuple ""tree"" has completed processing (XOR-ing the new field with 0), then checking whether processing is complete (by comparing the field to 0), and finally sending an ack in reply to the spout.

Normally, this is not a problem beyond the overhead, but on at least one occasion in the course of debugging topology performance, the acker bolt's host was so overloaded that it actually could not send the reply ack back to the spout before the spout timed it out.  This resulted in a lot of Fails reported for tuples that were not supposed to go anywhere in the first place, and an unnecessary count against the max.spout.pending that evidently also makes it harder to debug.

This was very confusing to the user.


I propose that we short-cut the ack init in the case when the spout does not emit to any downstream tasks.

I do have some misgivings already about making this change, as a spout emitting nowhere could be considered outside the set of normal use cases for Storm.  That said, I will not be unhappy if someone gives a -1.
"
STORM-745,Second Commandline Parameter passed to the main class is skipped when run in windows,"Always the second parameter is getting skipped.

E:\target>storm jar StormZeroMQ.jar com.wipro.bdas.zeromq.ZMQTopology value1 value2 value3 value4 value5
Output
I=0 value=value1
I=1 value=value3
I=2 value=value4
I=3 value=value5

public class ZMQTopology {
public static void main(String[] args) throws AlreadyAliveException,
                                                InvalidTopologyException {

                            for(int i=0;i<(args.length);i++)
                            {   System.out.println(""I="" +i+ "" value=""+args[i]);
                            }

I am using the apache storm pre-built for windows.

After some amount of debugging I could find that it happens only with windows machine . I was able to reproduce the error in 2 windows machine. With both  0.9.3 and 0.9.4 .In Linux machine I could see command line parameters working perfectly.
"
STORM-744,downstream worker fail may lead to worker oom without limit pending TaskMessage  count,"storm-core/src/jvm/backtype/storm/messaging/netty/Server.java
message_queue[i] = new LinkedBlockingQueue<ArrayList<TaskMessage>>(); // here
   "
STORM-743,Invalid JSON produced when serializing TaskInfo and DataPoint types,"In experimenting with a pure Node.js topology I attempted to write a simple logging consumer Bolt by merely observing the `__metrics` streams from my other existing components.

In doing so I observed that the message sent to my tasks is not valid json. For example:
```
{
    ""comp"": ""changes"",
    ""tuple"": [backtype.storm.metric.api.IMetricsConsumer$TaskInfo@28b63f13,[[__emit-count = {}],[__process-latency = {}],[__receive = {read_pos=0, write_pos=1, capacity=1024, population=1}],[__ack-count = {}],[__transfer-count = {}],[__execute-latency = {}],[__fail-count = {}],[__sendqueue = {read_pos=-1, write_pos=-1, capacity=1024, population=0}],[__execute-count = {}]]],
    ""task"":4,
    ""stream"":""__metrics"",
    ""id"":""1702055549821416448""
}
```

This seems to be able to be tracked directly to bad JSON serialization of TaskInfo and the collection of Datapoint [1].

(Thanks!)

[1] https://github.com/nathanmarz/storm/blob/cdb116e942666973bc4eaa0df098d5bab82739e7/storm-core/src/jvm/backtype/storm/metric/api/IMetricsConsumer.java"
STORM-742,Very busy ShellBolt subprocess with ACK mode cannot respond heartbeat just in time,"As [~dashengju] stated from STORM-738, very busy ShellBolt subprocess cannot respond heartbeat just in time.

Actually it's by design constraint (more details are on STORM-513 or STORM-738), but ShellSpout avoids constraint by updating heartbeat at any type of response from subprocess.

We can apply this approach to ShellBolt and let ShellBolt avoid design constraint, too."
STORM-740,Simple Transport Client cannot configure thrift buffer size,"SimpleTransportPlugin.connect creates a new TFramedTransport, but ignores the max buffer size.  This means for really large topologies the client will blow up."
STORM-738,Multilang needs Overflow-Control mechanism and HeartBeat timeout problem,"hi, all

we have a topology, which have 3 components(spout->parser->saver) and the parser is Multilang bolt with python. We do not use ACK mechanism.

we found 2 problems with Mutilang python script.
1) the parser python scripts may hold too many tuples and consume too many memory;
2) with MultiLang heartbeat mechanism described by  https://issues.apache.org/jira/browse/STORM-513, the python script always timeout to heartbeat, even when the parser bolt is normal, cause supervisor to restart itself.

!storm_multilang.png!

ShellBolt process === Father-Process
PythonScript process === Child-Process

The reason is :
1) when topology do not use ACK mechanism, the spout do not have Overflow-control ability, if the stream have too many tuples comes,  spout will send all the tuples to parser's ShellBolt process(Father-Process);
2) parser's ShellBolt process just put the tuples to _pendingWrites queue, if the _pendingWrites queue does not have limit;
3) parser's PythonScript process(Child-Process) call readMsg() to read a tuple from STDIN, handle the tuple, and emit a new tuple to its father process through STDOUT, and then call readTaskIds() from STDIN.  Because Father-Process's queue already have too many other tuples, Child-Process will read all the tuples to pending_commands, util received TaskIds.
4) so Child-Process process's pending_commands may contains too many tuples and consume too many memory.

As to heartbeat, because there are too many pending_commands need Child-Process to handle, and Child-Process's every emit operation will need more I/O read operations from STDIN. It may need 10 seconds to handle one tuple, and this will cause the heartbeat tuple not handle quickly, and timeout will happen.

Even if Father-Process's _pendingWrites have limits, for example 1000, Child-Process may needs 1000 x 1000 read operations then it can handle the heartbeat tuple.

[~revans2] [~kabhwan] this related to Multilang and heartbeat, please help to confirm the two problems.

I think Father-Process and Child-Process need Overflow-Control Protocol to control the python script's memory usage.
And heartbeat tuple needs a separate queue(pending_heartbeats), and Child-Process handle heartbeat tuple at high priority. [~kabhwan] wish to hear your opinion.
"
STORM-737,Workers may try to send to closed connections,"There is a race condition in the worker code that can allow for a send() to be called on a closed connection.

[Discussion|https://github.com/apache/storm/pull/349#issuecomment-87778672]

The assignment mapping from task -> node+port needs to be read and used in the read lock when sending, so that an accurate mapping is used that does not include any connections that are closed."
STORM-736,Add a RESTful API to  print all of the thread's information and stack traces of Nimbus/Supervisor/Worker Process,
STORM-735,[storm-redis] Upgrade Jedis to 2.7.0,"Jedis 2.6.2 and 2.7.0 released just now.

It contains some bug fixes, applies tiny optimization, and adds some commands which are missing.

* 2.6.3: https://github.com/xetorthio/jedis/issues?q=milestone%3A2.6.3+is%3Aclosed
* 2.7.0: https://github.com/xetorthio/jedis/issues?q=milestone%3A2.7.0+is%3Aclosed
"
STORM-733,ShellBolts that don't respond to heartbeats are not being killed,"In cases where a multilang bolt is stuck (say, an infinite loop), the heartbeats are supposed to detect the issue and kill the supervisor process.

In 0.9.3 this doesn't happen due to backtype.storm.utils.ShellProcess.getErrorsString() call in ShellBolt.die()

This call, which in turn executes IOUtils.toString(processErrorStream) will block the thread until process exits. Heartbeat flow should not assume process had exited."
STORM-732,PartitionManager should remove those messages not exists in kafka from waiting queue,
STORM-730,UI: Component Page has extra } for executor ids of Bolts,"When the component is a Bolt, the Executors table Id values show an extra closing curly brace."
STORM-729,UI: Component Page Executors header is missing the selected window,It should include (window hint) as the Component Page for a Spout does.
STORM-728,"UI: Component Page ""Spout stats"" table has Emitted and Transferred in each other's column","Emitted data shows under the Transferred column, and vice versa"
STORM-725,Topology does not start,"I am running a topology with 1 Nimbus, 4 supervisor and 3 zookeeper in production environment. Most of the time when I start the storm cluster and deploy the topology, none of spouts or bolts get activated. Even though I Storm UI shows everything is running and in Active state; however, spout is not emitting any messages. I checked the logs on all nodes including nimbus, there are no errors or exceptions. In Storm UI -> Topology Summary, all the starts shows 0.

I have to restart the whole cluster few times before topology actually starts working.

I am using storm storm-0.9.0-rc3 with zookeeper zookeeper-3.4.5 on JDK 1.6.

Any Idea why is this happening? Why do I need to start the storm cluster i
"
STORM-724,[storm-redis] Document RedisStoreBolt and RedisLookupBolt which is missed ,"We had documented AbstractRedisBolt, but not RedisStoreBolt and RedisLookupBolt.
Sorry for missed thing. ;("
STORM-723,[storm-redis] RedisStateSetUpdater writes state into String but calls Set operation,"RedisStateSetUpdater writes value into String type, but it tries to retrieve Set's element count from String type which will throw JedisDataException. 

{code}
                if (this.expireIntervalSec > 0) {
                    jedis.setex(redisKey, expireIntervalSec, value);
                } else {
                    jedis.set(redisKey, value);
                }
                Long count = jedis.scard(redisKey);
{code}

Btw, Redis expire only applies to key, so above implementation seems to be invalid.
"
STORM-722,[storm-redis] Apply various Redis data types to Trident,"Since we introduced supporting various data types to storm-redis, it would be better to apply it to storm-redis Trident to enjoy same benefits.

For more details, please see STORM-691."
STORM-720,Storm.cmd should return ERRORLEVEL before exiting main block,"This JIRA is for a very small PR that I will post soon (attached patch)

Issue: The Storm.cmd does not exit with an ErrorLevel.

Impact: Any automation via windows banks on return or exit code from the program to determine success.

When can this occur: Passing wrong arguments (like wrong class name or bad topology builder code) to java will result in error and any automation program will check on the exit code.

FYI @ [~harsha_ch] [~shanyu]"
STORM-713,Storm Kafka Metrics Don't Include Topic,"Storm Kafka does a good job of tracking internal metrics, such as spout backlog (""spoutLag""), and contains both aggregate and per-partition values. Unfortunately, this data is not tied to a specific topic, and provides no way to disambiguate between multiple topics when more than one spout is present."
STORM-711,All connectors should use collector.reportError and tuple anchoring.,Currently most of our connectors log an error when a tuple fails to process during execute method. We should change the connectors so we can use collector.reportError instead. 
STORM-709,Received unexpected tuple,"Trident Topology using 2 OpaqueTridentKafkaSpouts and a TridentKafkaState.  Everything works great in production storm topology, but when running in local mode from an IDE receiving this error on startup:

{code}
java.lang.RuntimeException: java.lang.RuntimeException: Received unexpected tuple source: $mastercoord-bg1:2, stream: $commit, id: {-4957901903366351898=6364388931843393707}, [1:0]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$fn__4606$fn__4619$fn__4670.invoke(executor.clj:806) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.util$async_loop$fn__543.invoke(util.clj:475) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:na]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
Caused by: java.lang.RuntimeException: Received unexpected tuple source: $mastercoord-bg1:2, stream: $commit, id: {-4957901903366351898=6364388931843393707}, [1:0]
    at storm.trident.planner.SubtopologyBolt.execute(SubtopologyBolt.java:144) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:369) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$fn__4606$tuple_action_fn__4608.invoke(executor.clj:668) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$mk_task_receiver$fn__4529.invoke(executor.clj:424) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.disruptor$clojure_handler$reify__1229.onEvent(disruptor.clj:58) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    ... 6 common frames omitted
{code}

Also receiving this error in the same dump:
{code}
java.lang.RuntimeException: org.apache.storm.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /errors/<topology-name>/<bolt-name>-last-error
    at backtype.storm.util$wrap_in_runtime.invoke(util.clj:48) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:92) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.cluster$mk_distributed_cluster_state$reify__2234.set_data(cluster.clj:104) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.cluster$mk_storm_cluster_state$reify__2774.report_error(cluster.clj:450) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$throttled_report_error_fn$fn__4385.invoke(executor.clj:191) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.daemon.executor$mk_executor_data$fn__4439$fn__4440.invoke(executor.clj:253) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at backtype.storm.util$async_loop$fn__543.invoke(util.clj:485) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at clojure.lang.AFn.run(AFn.java:22) ~[clojure-1.6.0.jar:na]
    at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_31]
Caused by: org.apache.storm.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /errors/<topology-name>/<bolt-name>-last-error
    at org.apache.storm.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:239) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at org.apache.storm.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:193) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source) ~[na:na]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_31]
    at java.lang.reflect.Method.invoke(Method.java:483) ~[na:1.8.0_31]
    at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.6.0.jar:na]
    at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.6.0.jar:na]
    at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:91) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
    ... 7 common frames omitted
{code}"
STORM-707,Client (Netty): improve logging to help troubleshooting connection woes,The current code in Client.java has a few places where we lack proper logging to troubleshoot connection issues (like we had to do for STORM-329).
STORM-703,RedisMapState with hash key can cause network overload,"When RedisMapState is constructed to use a hash key (and store values under a hash in redis, rather than as key/value pairs in the top namespace), a multiGet actually requests ALL of the state in that hash, instead of only the keys in the batch. 

As the size of the hash grows in redis, this becomes an inordinate amount of traffic, and can cause interfaces to fall over.

* Solution
Instead of calling jedis.hgetall(hash), call jedis.hmget(hash, stringKeys[]). Also, remove the buildValuesFromMap function, as it is no longer needed.

See PR#462 on github.
https://github.com/apache/storm/pull/462"
STORM-699,storm-jdbc should support customer insert queries.,"Currently storm-jdbc insert bolt/state only supports to specify a table name and constructs a query of the form ""insert into tablename values(?,?,?)"" based on table's schema. This fails to support use cases like ""insert into as select * from"" or special cases like Phoenix that has a jdbc driver but only supports ""upsert into"". 

We should add a way so the users can specify their own custom query for the insert bolt.  This was already pointed out by [~revans2] during the PR review and we now have concrete cases that will be benefited by this feature."
STORM-698,Jackson incompatibility ring-json,"ring-json -> com.fasterxml.jackson.core:jackson-core:2.3.1 incompatibility

{code}
2015-03-04 12:34:35 b.s.d.worker [ERROR] Error on initialization of server mk-worker
java.lang.RuntimeException: java.io.InvalidClassException: com.fasterxml.jackson.core.JsonFactory; local class incompatible: stream classdesc serialVersionUID = 3306684576057132431, local class serialVersionUID = 3194418244231611666
  at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:56) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.utils.Utils.deserialize(Utils.java:95) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.utils.Utils.getSetComponentObject(Utils.java:234) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$get_task_object.invoke(task.clj:79) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$mk_task_data$fn__4304.invoke(task.clj:181) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.util$assoc_apply_self.invoke(util.clj:894) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$mk_task_data.invoke(task.clj:174) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.task$mk_task.invoke(task.clj:185) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.executor$mk_executor$fn__4499.invoke(executor.clj:337) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.core$map$fn__4245.invoke(core.clj:2557) ~[clojure-1.6.0.jar:na]
  at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:na]
  at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:na]
  at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.6.0.jar:na]
  at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.6.0.jar:na]
  at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.6.0.jar:na]
  at clojure.core.protocols$fn__6078.invoke(protocols.clj:54) ~[clojure-1.6.0.jar:na]
  at clojure.core.protocols$fn__6031$G__6026__6044.invoke(protocols.clj:13) ~[clojure-1.6.0.jar:na]
  at clojure.core$reduce.invoke(core.clj:6289) ~[clojure-1.6.0.jar:na]
  at clojure.core$into.invoke(core.clj:6341) ~[clojure-1.6.0.jar:na]
  at backtype.storm.daemon.executor$mk_executor.invoke(executor.clj:337) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at backtype.storm.daemon.worker$fn__5031$exec_fn__1694__auto__$reify__5033$iter__5038__5042$fn__5043.invoke(worker.clj:459) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.LazySeq.sval(LazySeq.java:40) ~[clojure-1.6.0.jar:na]
  at clojure.lang.LazySeq.seq(LazySeq.java:49) ~[clojure-1.6.0.jar:na]
  at clojure.lang.Cons.next(Cons.java:39) ~[clojure-1.6.0.jar:na]
  at clojure.lang.RT.next(RT.java:598) ~[clojure-1.6.0.jar:na]
  at clojure.core$next.invoke(core.clj:64) ~[clojure-1.6.0.jar:na]
  at clojure.core$dorun.invoke(core.clj:2856) ~[clojure-1.6.0.jar:na]
  at clojure.core$doall.invoke(core.clj:2871) ~[clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$fn__5031$exec_fn__1694__auto__$reify__5033.run(worker.clj:459) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at java.security.AccessController.doPrivileged(Native Method) ~[na:1.8.0_31]
  at javax.security.auth.Subject.doAs(Subject.java:422) ~[na:1.8.0_31]
  at backtype.storm.daemon.worker$fn__5031$exec_fn__1694__auto____5032.invoke(worker.clj:433) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.AFn.applyToHelper(AFn.java:178) [clojure-1.6.0.jar:na]
  at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:na]
  at clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$fn__5031$mk_worker__5108.doInvoke(worker.clj:416) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$_main.invoke(worker.clj:548) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:na]
  at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
Caused by: java.io.InvalidClassException: com.fasterxml.jackson.core.JsonFactory; local class incompatible: stream classdesc serialVersionUID = 3306684576057132431, local class serialVersionUID = 3194418244231611666
  at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:621) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1623) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351) ~[na:1.8.0_31]
  at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371) ~[na:1.8.0_31]
  at backtype.storm.serialization.DefaultSerializationDelegate.deserialize(DefaultSerializationDelegate.java:52) ~[storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  ... 40 common frames omitted
2015-03-04 12:34:35 b.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
  at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:332) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$fn__5031$mk_worker__5108.doInvoke(worker.clj:416) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker$_main.invoke(worker.clj:548) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
  at clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:na]
  at clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:na]
  at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0-SNAPSHOT.jar:0.10.0-SNAPSHOT]
2015-03-04 12:34:39 o.a.s.z.ZooKeeper [INFO] Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
{code}
"
STORM-695,"storm CLI tool reports zero exit code on error scenario, take 2","Commands such as ""storm kill non-existing-topology"" will return an exit code of zero, indicating success when in fact the command failed.

h3. How to reproduce

Here is but one example where the {{storm}} CLI tool violates shell best practices:

{code}
# Let's kill a topology that is in fact not running in the cluster.
$ storm kill i-do-not-exist-topo
<snip>
Exception in thread ""main"" NotAliveException(msg:i-do-not-exist-topo is not alive)

# Print the exit code of last command.
$ echo $?
0  # <<< but since the kill command failed this should be non-zero!
{code}

Another example is the ""storm jar"" command.  If you attempt to submit a topology that has the same name as an existing, running topology, the ""storm jar"" command will not submit the topology -- instead it will print an exception (think: ""the topology FooName is already running""), which is ok, but it will then exit with a return code of zero, which indicates success (which is wrong).

h3. Impact

This bug prevents automated deployment tools such as Ansible or Puppet as well as ad-hoc CLI scripting (think: fire-fighting Ops teams) to work properly because Storm violates shell conventions by not returning non-zero exit codes in case of failures.

h3. How to fix

From what I understand the solution is two-fold:

# The various Storm commands that are being called by the {{storm}} script must return proper exit codes.
# The {{storm}} script must store these exit codes and return itself with the respective exit code of the Storm command it actually ran.

For example, here's the current code that implements the ""storm kill"" command:

{code}
# In: bin/storm
def kill(*args):
    """"""Syntax: [storm kill topology-name [-w wait-time-secs]]

    Kills the topology with the name topology-name. Storm will 
    first deactivate the topology's spouts for the duration of 
    the topology's message timeout to allow all messages currently 
    being processed to finish processing. Storm will then shutdown 
    the workers and clean up their state. You can override the length 
    of time Storm waits between deactivation and shutdown with the -w flag.
    """"""
    exec_storm_class(
        ""backtype.storm.command.kill_topology"", 
        args=args, 
        jvmtype=""-client"", 
        extrajars=[USER_CONF_DIR, STORM_BIN_DIR])
{code}

which in turn calls the following code in {{kill_topology.clj}}:

{code}
;; In: backtype.storm.command.kill-topology
(ns backtype.storm.command.kill-topology
  (:use [clojure.tools.cli :only [cli]])
  (:use [backtype.storm thrift config log])
  (:import [backtype.storm.generated KillOptions])
  (:gen-class))

(defn -main [& args]
  (let [[{wait :wait} [name] _] (cli args [""-w"" ""--wait"" :default nil :parse-fn #(Integer/parseInt %)])
        opts (KillOptions.)]
    (if wait (.set_wait_secs opts wait))
    (with-configured-nimbus-connection nimbus
      (.killTopologyWithOpts nimbus name opts)
      (log-message ""Killed topology: "" name)
      )))
{code}

which in turn calls the following code in {{nimbus.clj}}:

{code}
;; In: backtype.storm.daemon.nimbus
      (^void killTopologyWithOpts [this ^String storm-name ^KillOptions options]
        (check-storm-active! nimbus storm-name true)
        (let [topology-conf (try-read-storm-conf-from-name conf storm-name nimbus)]
          (check-authorization! nimbus storm-name topology-conf ""killTopology""))
        (let [wait-amt (if (.is_set_wait_secs options)
                         (.get_wait_secs options)                         
                         )]
          (transition-name! nimbus storm-name [:kill wait-amt] true)
          ))
{code}

As you can see the current implementation does not pass success/failure information back to the caller."
STORM-694,Compiling storm 0.10.0 results in java.lang.ClassNotFoundException: backtype.storm.daemon.common.SupervisorInfo,"When compiling storm I get an exception in supervisor.clj
{noformat}
Exception in thread ""main"" java.lang.ClassNotFoundException: backtype.storm.daemon.common.SupervisorInfo, compiling:(supervisor.clj:16:1)
	at clojure.lang.Compiler$InvokeExpr.eval(Compiler.java:3558)
	at clojure.lang.Compiler.compile1(Compiler.java:7226)
	at clojure.lang.Compiler.compile1(Compiler.java:7216)
	at clojure.lang.Compiler.compile(Compiler.java:7292)
	at clojure.lang.RT.compile(RT.java:398)
	at clojure.lang.RT.load(RT.java:438)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5066.invoke(core.clj:5641)
	at clojure.core$load.doInvoke(core.clj:5640)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5446)
	at clojure.core$compile$fn__5071.invoke(core.clj:5652)
	at clojure.core$compile.invoke(core.clj:5651)
	at clojure.lang.Var.invoke(Var.java:379)
	at clojure.lang.Compile.main(Compile.java:81)
{noformat}

After adding an explicit {{:require}} in {{supervisor.clj}} the issue seems to be  resolved :
{noformat}
diff --git a/storm-core/src/clj/backtype/storm/daemon/supervisor.clj b/storm-core/src/clj/backtype/storm/daemon/supervisor.clj
index b9f9632..5fc5b7c 100644
--- a/storm-core/src/clj/backtype/storm/daemon/supervisor.clj
+++ b/storm-core/src/clj/backtype/storm/daemon/supervisor.clj
@@ -15,6 +15,7 @@
 ;; limitations under the License.
 (ns backtype.storm.daemon.supervisor
   (:import [java.io OutputStreamWriter BufferedWriter IOException])
+  (:require [backtype.storm.daemon.common :as common])
   (:import [backtype.storm.scheduler ISupervisor]
            [backtype.storm.utils LocalState Time Utils]
            [backtype.storm.daemon Shutdownable]
{noformat}"
STORM-693,KafkaBolt exception handling improvement,"Within the KafkaBolt execute method, an error message is logged if any sort of error occurs communicating with Kafka.  Unfortunately the input is still acknowledged.

Upon review of the HdfsBolt & HiveBolt, I believe the exception handling block should include the following two lines:

            this.collector.reportError(ex);
            this.collector.fail(input);
"
STORM-692,KakfaSpout storage of offsets in Zookeeper lost,"Constructor for SpoutConfig and inadequate documentation in storm-kafka/README.md leads to lost persistence of Kafka offset information.

The SpoutConfig class constructor takes four parameters:

hosts - Zookeeper host & root path for Kafka 
topic - Kafka topic name
zkRoot - Zookeeper path for Kafka Spout offset
id -- Identifier for KafkaSpout 

Unfortunately it also exposes two public instance variables that must also be set: zkServers & zkPort.  If these two variables are not set it uses Storm default values(localhost & 2000) which cause the offset information to be lost.

Suggest we replace the existing constructor with one that supplies all 6 required values."
STORM-691,[storm-redis] Add basic lookup / persist bolts,"Currently storm-redis provides AbstractRedisBolt for normal (not Trident) Bolt.

Jedis is easy to use so it may be enough, but we can also provide implementations of AbstractRedisBolt for simple usage.
eg. store (key, value) pair, get key's value

Since Redis has various data types and commands, we can't cover whole things, but seems like below things could be considered.

|| Type || Read || Write ||
| STRING | GET (key) | SET (key, value) |
| HASH | HGET (key, field) | HSET (key, field, value) |
| LIST | LPOP (key) | RPUSH (key, value) |
| SET | SCARD (key) | SADD (key, member) |
| SORTED SET | ZSCORE (key, member) | ZADD (key, score, member) |
| HLL (HyperLogLog) | PFCOUNT (key) | PFADD (key, element) |

Btw, since we will normally get key & value from tuple (as most external module did), HASH, SET, SORTED SET needs additional key to process."
STORM-690,[storm-redis] Broken Jedis Connection should be returned as broken ,"While using JedisPool, any broken Jedis instances should be returned as broken.
(Thanks to Apache Commons Pool 2, we can delegate verifying instances to JedisPool itself by configuration, but performance will be decreased a bit.)

We can try-with-finally with Jedis / JedisCluster instance, but Storm should be run at least JDK 6 so we can't apply it."
STORM-685,wrong output in log when committed offset is too far behind latest offset,"The log message about lag in committed offset has some incorrect arithmetic.

Created from https://github.com/apache/storm/pull/331"
STORM-681,Auto insert license header with genthrift.sh,"Current genthrift.sh does not insert license headers into generated source codes. These java codes and python codes should have license headers. 
And documentation about this command."
STORM-680,storm-kafka fails to compile with JDK 1.6,"ExponentialBackoffMsgRetryManager.java uses Long.compare(long1, long2) method which is specific to JDK 7. 
{code}
public int compare(MessageRetryRecord record1, MessageRetryRecord record2) {
  return Long.compare(record1.retryTimeUTC, record2.retryTimeUTC);
}
{code}

As per the comment in this commit https://github.com/apache/storm/commit/fd066985a74c3140f61a465cfc49c83a6ddfa713
we should enforce JDK 1.7 as the minimum requirement for storm-kafka.

Alternate fix is to change Long.compare() method to Long.compareTo() which is compatible with JDK 1.6."
STORM-679,Enforce JDK 1.7 for storm-kafka,"ExponentialBackoffMsgRetryManager.java uses Long.compare(long1, long2) method which is specific to JDK 7. 
{code}
public int compare(MessageRetryRecord record1, MessageRetryRecord record2) {
  return Long.compare(record1.retryTimeUTC, record2.retryTimeUTC);
}
{code}

As per the comment in this commit https://github.com/apache/storm/commit/fd066985a74c3140f61a465cfc49c83a6ddfa713
we should enforce JDK 1.7 as the minimum requirement for storm-kafka.

Alternate fix is to change Long.compare() method to Long.compareTo() which is compatible with JDK 1.6."
STORM-678, Storm UI Spengo filter doesn't invalidate user session immediately upon kinit as a different user,"I am using HDP 2.2 which includes fixes of  https://issues.apache.org/jira/browse/STORM-216. 

Install STORM with Nagios and Ganglia, there is no HDFS, Hadoop installed on the cluster, cluster is made of three nodes. 
Enable security as guided by Ambari, kerberize the cluster this covers everything as specified in the https://github.com/apache/storm/blob/security/SECURITY.md . 

Now submit job from 'test' user principal from the gateway node. Open Storm UI in firefox or google-chrome it shows the topology running as 'test' user. 
Now kinit with another user 'test2' refresh the UI. It still says the 'test' user . Even closing and re-opening firefox /chrome doesnt help. It lets 'test2' user kill topology of 'test' user.

This behaviour is not observed when using storm kill command in command line"
STORM-677,Maximum retries strategy may cause data loss,"h3. Background

Storm currently supports the configuration setting storm.messaging.netty.max_retries.  This setting is supposed to limit the number of reconnection attempts a Netty client will perform in case of a connection loss.

Unfortunately users have run into situations where this behavior will result in data loss:

{quote}
https://github.com/apache/storm/pull/429/files#r24681006

This could be a separate JIRA, but we ran into a situation where we hit the maximum number of reconnection attempts, and the exception was eaten because it was thrown from a background thread and it just killed the background thread. This code appears to do the same thing.
{quote}

The problem can be summarized by the following example:  Once a Netty client hits the maximum number of connection retries, it will stop trying to reconnect (as intended) but will also continue to run forever without being able to send any messages to its designated remote targets.  At this point data will be lost because any messages that the Netty client is supposed to send will be dropped (by design).  And since the Netty client is still alive and thus considered ""functional"", Storm is not able to do something about this data loss situation.

For a more detailed description please take a look at the discussion in https://github.com/apache/storm/pull/429/files#r24742354.

h3. Possible solutions

(Most of this section is copy-pasted from an [earlier discussion on this problem|https://github.com/apache/storm/pull/429/files#r24742354].)

There are at least three approaches we may consider:

# Let the Netty client die if max retries is reached, so that the Storm task has the chance to re-create a client and thus break out of the client's discard-messages-forever state.
# Let the ""parent"" Storm task die if (one of its possibly many) Netty clients dies, so that by restarting the task we'll also get a new Netty client.
# Remove the max retries semantics as well as the corresponding setting from Storm's configuration. Here, a Netty client will continue to reconnect to a remote destination forever. The possible negative impact of these reconnects (e.g. number of TCP connection attempts in a cluster) are kept in check by our exponential backoff policy for such connection retries.

My personal opinion on these three approaches:

- I do not like (1) because I feel it introduces potentially confusing semantics: We keep having a max retries setting, but it is not really a hard limit anymore. It rather becomes a ""max retries until we recreate a Netty client"", and would also reset any exponential backoff strategy of the ""previous"" Netty client instance (cf. StormBoundedExponentialBackoffRetry). If we do want such resets (but I don't think we do at this point), then a cleaner approach would be to implement such resetting inside the retry policy (again, cf. StormBoundedExponentialBackoffRetry).
- I do not like (2) because a single ""bad"" Netty client would be able to take down a Storm task, which among other things would also impact any other, working Netty clients of the Storm task.
- Option (3) seems a reasonable approach, although it breaks backwards compatibility with regard to Storm's configuration (because we'd now ignore storm.messaging.netty.max_retries).


Here's initial feedback from other developers:

{quote}
https://github.com/apache/storm/pull/429/files#r24824540

revans2: I personally prefer option 3, no maximum number of reconnection attempts. Having the client decide that it is done, before nimbus does feels like it is asking for trouble.
{quote}

{quote}
https://github.com/ptgoetz

ptgoetz: I'm in favor of option 3 as well. I'm not that concerned about storm.messaging.netty.max_retries being ignored. We could probably just log a warning that that configuration option is deprecated and will be ignored if the value is set.
{quote}

{quote}
https://github.com/apache/storm/pull/429#issuecomment-74914806

nathanmarz: Nimbus only knows a worker is having trouble when it stops sending heartbeats. If a worker gets into a bad state, the worst thing to do is have it continue trying to limp along in that bad state. It should instead suicide as quickly as possible. It seems counterintuitive, but this aggressive suiciding behavior actually makes things more robust as it prevents processes from getting into weird, potentially undefined states. This has been a crucial design principle in Storm from the beginning. One consequence of it is that any crucial system thread that receives an unrecoverable exception must suicide the process rather than die quietly.

For the connection retry problem, it's a tricky situation since it may not be able to connect because the other worker is still getting set up. So the retry policy should be somehow related to the launch timeouts for worker processes specified in the configuration. Not being able to connect after the launch timeout + a certain number of attempts + a buffer period would certainly qualify as a weird state, so the process should suicide in that case. Suiciding and restarting gets the worker back to a known state.

So in this case, I am heavily in favor of Option 2. I don't care about killing the other tasks in the worker because this is a rare situation. It is infinitely more important to get the worker back to a known, robust state than risk leaving it in a weird state permanently.
{quote}

If we decide to go with option 3, then the essence of the fix is the following modification of Client.java:

{code}
    private boolean reconnectingAllowed() {
        // BEFORE:
        // return !closing && connectionAttempts.get() <= (maxReconnectionAttempts + 1);
        return !closing;
    }
{code}"
STORM-673,Typo 'deamon' in security documentation,deamon -> daemon
STORM-672,Typo in Trident documentation example,"In the Trident API Documentation, the code reference under the filters section extends BaseFunction when it should extend BaseFilter."
STORM-670,[storm-kafka] Restore Java 1.6 compatibility,"java.lang.Long.compare(Long, Long) is only available in Java 1.7"
STORM-669,Replace links with ones to latest api document,Replace links to old api document with new ones.
STORM-668, Generate storm-ui war using maven and deployed on TomCat,"$ mvn package -Dmaven.test.skip=true -P dist -Dwar

generate storm-ui.war "
STORM-667,"Incorrect capitalization ""SHell"" in Multilang-protocol.md","-SHell bolts are asynchronous, so ShellBolt will send heartbeat tuple periodically.

should be

+Shell bolts are asynchronous, so ShellBolt will send heartbeat tuple periodically.
"
STORM-663,Create javadocs for BoltDeclarer,There's no documentation of grouping stream usage in the javadocs of BoltDeclarer<ref>http://storm.apache.org/javadoc/apidocs/backtype/storm/topology/BoltDeclarer.html</ref>. It might be sufficient to a links to sites or HTML anchors of the full text documentation.
STORM-660, IndexOutOfBoundsException with  shuffle grouping and a large number of paralleliziation hint,"Storm throws  IndexOutOfBoundsException error once in a while, when a bolt with shuffle grouping to is linked its predecessor bolt.especially with a high paralleliziation hint.
This was reported as a bug with the previous versions of storm as well.

http://stackoverflow.com/questions/19632287/indexoutofboundsexception-in-shuffle-grouping
"
STORM-659,Logviewer used with grep param returns all matches on one line,Adding the grep=searchstring parameter to the logviewer URL will search the log file for lines with the string in it.  But it then returns all matches on one line within a preformatted text block.  It should return matches each on its own line so that it is easier to read.
STORM-657,make the shutdown-worker sleep time before kill -9 configurable,"This is a continuation of STORM-183: Supervisor/worker shutdown hook should be called in distributed mode.

It would be nice to be able to configure how many seconds (or millis) to sleep for before shutting down the worker process in the shutdown-worker function in supervisor.clj"
STORM-653,missing DRPC HTTP port in SECURITY.md,"There is no description about DRPC HTTP port in SECURITY.md.
This port is used by DRPC Client as default 3774"
STORM-647,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-646,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-645,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-644,KafkaUtils repeat fetch messages which offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[FIX]
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
..."
STORM-643,KafkaUtils repeatedly fetches messages whose offset is out of range,"KafkaUtils repeat fetch messages which offset is out of range.
This happened when failed list(SortedSet<Long> failed) is not empty and some offset in it is OutOfRange.

[worker-log]
{code}
2015-02-01 10:24:27.231+0800 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [20919071816]; retrying with default start offset time from configuration. configured start offset time: [-2]
2015-02-01 10:24:27.232+0800 s.k.PartitionManager [WARN] Using new offset: 20996130717
2015-02-01 10:24:27.333+0800 s.k.KafkaUtils [WARN] Got fetch request with offset out of range: [20919071816]; retrying with default start offset time from configuration. configured start offset time: [-2]
2015-02-01 10:24:27.334+0800 s.k.PartitionManager [WARN] Using new offset: 20996130717
...
{code}

[FIX]
{code}
storm.kafka.PartitionManager.fill():
...
try {
	msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
} catch (UpdateOffsetException e) {
	 _emittedToOffset = KafkaUtils.getOffset(_consumer, _spoutConfig.topic, _partition.partition, _spoutConfig);
	LOG.warn(""Using new offset: {}"", _emittedToOffset);
	// fetch failed, so don't update the metrics

	//fix bug: remove this offset from failed list when it is OutOfRange
	if (had_failed) {
		failed.remove(offset);
	}

            return;
}
...
{code}

also: Log ""retrying with default start offset time from configuration. configured start offset time: [-2]"" is incorrect."
STORM-639,storm-maven-plugin not found,"storm-maven-plugin is required by storm-core, but it cannot be found.

```
[ERROR] Error resolving version for plugin 'org.apache.storm:storm-maven-plugins' from the repositories [local (/Users/sasakikai/.m2/repository), central (https://repo.maven.apache.org/maven2)]: Plugin not found in any plugin repository -> [Help 1]
org.apache.maven.plugin.version.PluginVersionResolutionException: Error resolving version for plugin 'org.apache.storm:storm-maven-plugins' from the repositories [local (/Users/sasakikai/.m2/repository), central (https://repo.maven.apache.org/maven2)]: Plugin not found in any plugin repository
        at org.apache.maven.plugin.version.internal.DefaultPluginVersionResolver.selectVersion(DefaultPluginVersionResolver.java:236)
        at org.apache.maven.plugin.version.internal.DefaultPluginVersionResolver.resolveFromRepository(DefaultPluginVersionResolver.java:148)
        at org.apache.maven.plugin.version.internal.DefaultPluginVersionResolver.resolve(DefaultPluginVersionResolver.java:96)
        at org.apache.maven.lifecycle.internal.LifecyclePluginResolver.resolveMissingPluginVersions(LifecyclePluginResolver.java:71)
        at org.apache.maven.lifecycle.internal.DefaultLifecycleExecutionPlanCalculator.calculateExecutionPlan(DefaultLifecycleExecutionPlanCalculator.java:116)
        at org.apache.maven.lifecycle.internal.DefaultLifecycleExecutionPlanCalculator.calculateExecutionPlan(DefaultLifecycleExecutionPlanCalculator.java:135)
        at org.apache.maven.lifecycle.internal.builder.BuilderCommon.resolveBuildPlan(BuilderCommon.java:97)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:109)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:347)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:154)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:582)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:214)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:158)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
[ERROR]
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
```

This plugin should be installed in local repository before compile storm-core."
STORM-638,UI should show up  process-id of the Worker to which an Executor is assigned,
STORM-636,UI/Monitor is slow for topologies with a large number of components,"The getTopologyInfo method in nimbus fetches from ZK all errors reported by all components.  This becomes too slow for topologies with a larger numbers of components  (bolts/spouts).

In one example, the UI consistently took over 5 minutes to load the topology page for a topology with nearly 500 components while ZK was under load.

Errors are currently stored in ZooKeeper under individual znodes per component.  This means that each call to getTopologyInfo needs to list children of each znode and then download the error znodes it finds."
STORM-633,Nimbus - HTTP Error 413 full HEAD if using kerberos authentication,"When trying to access Nimbus that is kerberized, a HTTP 413 full HEAD error is received. This seems related to the issue outlined in HADOOP-8816.

Setting the Jetty header buffer size with ring-jetty is outlined on Stackoverflow here: http://stackoverflow.com/questions/9285096/clojure-ring-using-the-ring-jetty-adapter-large-requests-give-me-a-413-full-h

The setting could be exposed like the host as done in STORM-575."
STORM-628,Storm-HBase add support to WriteBuffer/setAutoFlush,"The default value for ""autoflush"" in HTable is true. We should support our user to enable HBase writebuffer on the client side, by add a new configuration ""storm.hbase.table.autoflush"".

 "
STORM-627,Storm-hbase configuration error,"The HBaseMapState and HBaseState classes are not reading the configuration entries properly. The code reads from the ""map"" entries instead of the ""conf"" entries in the code referenced below, which is incorrect.

Details and suggested fix:

org/apache/storm/hbase/trident/state/HBaseMapState.java
line 78:
-- hbConfig.set(key, String.valueOf(map.get(key)));
++ hbConfig.set(key, String.valueOf(conf.get(key)));

org/apache/storm/hbase/trident/state/HBaseState.java
line 108:
-- hbConfig.set(key, String.valueOf(map.get(key)));
++ hbConfig.set(key, String.valueOf(conf.get(key)));

Note: it seems that a similar bug was fixed in a different file in ptgoetz/storm-hbase (https://github.com/ptgoetz/storm-hbase/commit/39797ac6914d042051d6f4504edb6e01998bccf3), but these two java files still have it."
STORM-624,Some typos in SECURITY.md,
STORM-623,Generate latest javadocs,"There is no latest javadoc on official site now.
https://storm.apache.org/documentation/Home.html
In addition to this, current javadocs are hosted outside of apache domain.

Maven build pipeline includes generation process of javadoc."
STORM-621,UI should expose more version information,"We should view the following information on UI page:
Storm Version： 0.10.0-SNAPSHOT
Git https  clone URL : https://github.com/caofangkun/apache-storm.git -r ffba148cc47a92185fa1a5db11f72982de10f106
Git last checkin id: ffba148cc47a92185fa1a5db11f72982de10f106
Branch storm-243
Compiled by somebody on Thu Jan 8 10:47:48 CST 2015
From source with checksum 97e7c942939e3e82dcb854b497991a51"
STORM-620,Duplicate maven plugin declaration,maven-javadoc-plugin is included reporting section in pom.xml doubly
STORM-619,add supervisor page to show workers running detail informations,"add supervisor page to show workers running detail information

show workers details like:
1: runging taskids 
2: uptime secs
3: topologyid
4: worker port 
5: error log link"
STORM-618, Add spoutconfig option to make kafka spout process messages at most once.,"While it's nice for kafka spout to push failed tuple back into a sorted set and try to process it again, this way of guaranteed message processing sometimes makes situation pretty bad when a failed tuple repeatedly fails in downstream bolts since PartitionManager#fill method tries to fetch from that offset repeatedly.

This is a corresponding code snippet.

    private void fill() {
...
        if (had_failed) {
            offset = failed.first();
        } else {
            offset = _emittedToOffset;
        }
...
            msgs = KafkaUtils.fetchMessages(_spoutConfig, _consumer, _partition, offset);
...

So there should be an option for a developer to decide if he wants to process failed tuple again or just skip failed tuple. One of the best thing of Storm is that spout together with trident can be implemented to guarantee at-least-once,exactly-once and at-most-once message processing."
STORM-617,In Storm secure mode re-deploying trident topology causes zookeeper ACL issue,"This issue is caused by this line https://github.com/apache/storm/blob/master/storm-core/src/jvm/backtype/storm/transactional/state/TransactionalState.java#L67

If the storm cluster nimbus is running with a kerberos principal named ""nimbus""
and supervisors are running with principal ""storm"" . Storm puts the acl on trident spout using principal ""nimbus"" and this won't be able to accessed or modified by supervisor since they are logging into zookeeper as user ""storm""."
STORM-614,storm-core mvn artifacts dependencies are not downloaded automatically,"I added 'storm-core' artefact to my gradle project. Gradle couldn't download roughly half of artefact dependencies (clj-time, ring-servlet and others), because it couldn't find them. pom.xml of 'storm-core' doesn't contain any links to any external repos.

Problem is: these dependencies are stored in 'clojars' repo, and during build of all storm, dependent projects use links from root pom.xml, and thus when 'storm-core' artefact is deployed to maven central it is broken by default. 

Suggestions to solving are: 
1) Update documentation on storm.apache.org (in Downloads section)
2) Adding 'clojars' repo url to 'storm-core' pom.xml
"
STORM-613,Fix wrong getOffset return value,"When judge getOffset return value, use 0 not NO_OFFSET as invalid offset return value."
STORM-609,add storm-redis to storm external,
STORM-608,Storm UI CSRF escape characters not work correctly,"When trying to use the REST API to active or detactive storm topology using C# Httpclient, there are random failures.
I do some more investigate about the random error 
We find some x-csrf-token getting from the first http get request contains “\/”, for example “\/dE\/k8N5H0Ora1IY9UAfx3fc7M4b0EZOMbWUXdUNn9IitAjOhmup+OiHx\/v5W+kUuWu4TkBsFsfvd7Km”
which will fail.

If I replace \/ with /  and this should be  /dE/k8N5H0Ora1IY9UAfx3fc7M4b0EZOMbWUXdUNn9IitAjOhmup+OiHx/v5W+kUuWu4TkBsFsfvd7Km,
and the second token could work successfully while the first does not.
we assume this is caused by some escape letters."
STORM-607,storm-hbase HBaseMapState should support user to customize the hbase-key & hbase-qualifier,"In HBaseMapState, user can specific hbase-qualifier by Options, and the hbase-key is composed by all the keys by multiPut's List<Object> keys.

for example, If I have stream with <deal_id, date, pv>, grouped by <deal_id, date>, then, the hbase-key is composed by<deal_id, date>, the hbase-qualifier is pv.
But when I want hbase-key is deal_id, and hbase-qualifier is pv+date, HBaseMapState can not support this."
STORM-606,Attempting to call unbound fn during bolt prepare,"
We had a bunch of topologies running very well under Storm 0.8.2 until last
week when we switched to storm 0.9.2-incubating. We use the clojure DSL,
and clojure 1.5.1 (only).

Since the change, we have a large topology (about 30 bolts, parallellism=10
or 20 per bolt, total 372 tasks on 10 workers) that fails on startup with
several bolts showing the exception :

java.lang.RuntimeException: java.lang.IllegalStateException: Attempting to
call unbound fn: #'entry-dedup.bolt/dedup__ at
backtype.storm.clojure.ClojureBolt.prepare(ClojureBolt.java:77) ...

This can occur on one or several bolts at random and is not consistent
between restarts.

The topology is indeed quite long to initialize (a dozen seconds) due to several models being loaded but this was OK in 0.8.2.

Another (shorter) topology works most of the time but shows this behaviour
on some restarts sometimes.

We found a workaround that works most of the time : start the topology in
the INACTIVE state, then wait 200 seconds, then activate it. But this
doesn't really solve our problem because sometimes Storm tries to rebalance
the topologies by itself and reassigns the topology without our little trick, effectively crashing them.

The same behavior is present with storm 0.9.3.

So maybe something changed in storm that introduces a kind of race
condition during initializaion of some bolts on larger topologies ? Maybe this is a consequence to the switch to Netty ?"
STORM-605,Attempting to call unbound fn during bolt prepare,"
We had a bunch of topologies running very well under Storm 0.8.2 until last
week when we switched to storm 0.9.2-incubating. We use the clojure DSL,
and clojure 1.5.1 (only).

Since the change, we have a large topology (about 30 bolts, parallellism=10
or 20 per bolt, total 372 tasks on 10 workers) that fails on startup with
several bolts showing the exception :

java.lang.RuntimeException: java.lang.IllegalStateException: Attempting to
call unbound fn: #'entry-dedup.bolt/dedup__ at
backtype.storm.clojure.ClojureBolt.prepare(ClojureBolt.java:77) ...

This can occur on one or several bolts at random and is not consistent
between restarts.

The topology is indeed quite long to initialize (a dozen seconds) due to several models being loaded but this was OK in 0.8.2.

Another (shorter) topology works most of the time but shows this behaviour
on some restarts sometimes.

We found a workaround that works most of the time : start the topology in
the INACTIVE state, then wait 200 seconds, then activate it. But this
doesn't really solve our problem because sometimes Storm tries to rebalance
the topologies by itself and reassigns the topology without our little trick, effectively crashing them.

The same behavior is present with storm 0.9.3.

So maybe something changed in storm that introduces a kind of race
condition during initializaion of some bolts on larger topologies ? Maybe this is a consequence to the switch to Netty ?"
STORM-602,HdfsBolt dies when the hadoop node is not available,"When the hadoop nodes are not available, HdfsBolt generates the following run time error, and dies and the topology dies with it too.

12154 [Thread-50-hdfsBolt2] ERROR backtype.storm.util - Halting process: (""Worker died"")
java.lang.RuntimeException: (""Worker died"")
        at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:319) [storm-core-0.9.3-SNAPSHOT.jar:0.9.3-SNAPSHOT]
        at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
        at backtype.storm.daemon.worker$fn__4770$fn__4771.invoke(worker.clj:452) [storm-core-0.9.3-SNAPSHOT.jar:0.9.3-SNAPSHOT]
        at backtype.storm.daemon.executor$mk_executor_data$fn__3287$fn__3288.invoke(executor.clj:239) [storm-core-0.9.3-SNAPSHOT.jar:0.9.3-SNAPSHOT]
        at backtype.storm.util$async_loop$fn__458.invoke(util.clj:467) [storm-core-0.9.3-SNAPSHOT.jar:0.9.3-SNAPSHOT]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
"
STORM-599,UI is slow due to fetching heartbeats from ZK,"The method getTopologyInfo fetches every heartbeat from ZooKeeper in order to gather statistics.  The UI calls this method via thrift to render the topology page.

When topologies are launched with thousands of executors, this fetching from ZK dramatically slows things down."
STORM-598,Newly submitted topologies do not show up on the storm ui cluser page when the Storm Cluster run out of worker slots ,"1: Set up a Storm Cluster with 1 Supervisor(4 worker slots)
2: submit topologyA and use 4 workers 
3: submit topologyB with 4 workers 

topologyB does not but should show up on the storm ui cluster page.


See Code Line 232：
https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L1232

If assignment is null, just new Assignmnet with 0 workers and 0 tasks? 
{code:title=nimbus.clj|borderStyle=solid}
Index: src/clj/backtype/storm/daemon/nimbus.clj
===================================================================
--- src/clj/backtype/storm/daemon/nimbus.clj	(revision 4324)
+++ src/clj/backtype/storm/daemon/nimbus.clj	(working copy)
@@ -1230,7 +1230,9 @@
               bases (topology-bases storm-cluster-state)
               topology-summaries (dofor [[id base] bases :when base]
 	                                  (let [assignment (.assignment-info storm-cluster-state id nil)
-                                                topo-summ (TopologySummary. id
+                                                topo-summ ( if (nil? assignment) 
+                                                           (TopologySummary. id  (:storm-name base) 0 0 0  (time-delta (:launch-time-secs base)) (extract-status-str base))
+                                                           (TopologySummary. id
                                                             (:storm-name base)
                                                             (->> (:executor->node+port assignment)
                                                                  keys
@@ -1244,7 +1246,7 @@
                                                                  set
                                                                  count)
                                                             (time-delta (:launch-time-secs base))
-                                                            (extract-status-str base))]
+                                                            (extract-status-str base)))]
                                                (when-let [owner (:owner base)] (.set_owner topo-summ owner))
                                                (when-let [sched-status (.get @(:id->sched-status nimbus) id)] (.set_sched_status topo-summ sched-status))
                                                topo-summ
{code}
"
STORM-597,spout not receve the message acker,"I have one spout and one bolt.
The program always runs greate. But sometimes the spout do not receive the ack and at debug mode the bolt have do the ack correctly.

LOG:
###1 Normally the last line says the spout has received acker.

2014-12-21 02:46:15 c.s.b.s.b.s.BaseTaskScheduleSpout [INFO] check time begin at :2014-12-21 02:46:15   end
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting: task-spout default [1419100920000, 1419100980000]
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting: task-spout __ack_init [3492883688624743717 -7308083944080730032 10]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: default, id: {3492883688624743717=-7308083944080730032}, [1419100920000, 1419100980000]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: __ack_init, id: {}, [3492883688624743717 -7308083944080730032 10]
2014-12-21 02:46:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        scan    0       cost    3 ms    time    2014-12-21 02:42:00     to      2014-12-21 02:43:00     row     null    to      null
2014-12-21 02:46:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        run final success
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting: task-bolt __ack_ack [3492883688624743717 -7308083944080730032]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: task-bolt:7, stream: __ack_ack, id: {}, [3492883688624743717 -7308083944080730032]
2014-12-21 02:46:15 b.s.d.task [INFO] Emitting direct: 10; __acker __ack_ack [3492883688624743717]
2014-12-21 02:46:15 b.s.d.executor [INFO] Processing received message source: __acker:1, stream: __ack_ack, id: {}, [3492883688624743717]
2014-12-21 02:46:15 b.s.d.executor [INFO] Acking message 1419100920000,1419100980000,60000,prmt_user_tracer,true,1419101175675,from:2014-12-21 02:42:00,to:2014-12-21 02:43:00,execute:2014-12-21 02:46:15



###2.bug case, the bolt ack is stopping at ""Emitting direct: 10; __acker __ack_ack [-5576733886177167329]"". And the spout will not receve the ack of that message.

2014-12-21 02:45:15 c.s.b.s.b.s.BaseTaskScheduleSpout [INFO] begin put task     1419100860000,1419100920000,60000,prmt_user_tracer,true,1419101115112,from:2014-12-21 02:41:00,to:2014-12-21 02:42:00,execute:2014-12-21 02:45:15
2014-12-21 02:45:15 c.s.b.s.b.s.BaseTaskScheduleSpout [INFO] check time begin at :2014-12-21 02:45:14   end
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting: task-spout default [1419100860000, 1419100920000]
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting: task-spout __ack_init [-5576733886177167329 6447998047515384780 10]
2014-12-21 02:45:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: default, id: {-5576733886177167329=6447998047515384780}, [1419100860000, 1419100920000]
2014-12-21 02:45:15 b.s.d.executor [INFO] Processing received message source: task-spout:10, stream: __ack_init, id: {}, [-5576733886177167329 6447998047515384780 10]
2014-12-21 02:45:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        scan    0       cost    4 ms    time    2014-12-21 02:41:00     to      2014-12-21 02:42:00     row     null    to      null
2014-12-21 02:45:15 c.s.b.s.b.b.HbaseTaskScheduleBolt [INFO] table      prmt_user_tracer        run final success
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting: task-bolt __ack_ack [-5576733886177167329 6447998047515384780]
2014-12-21 02:45:15 b.s.d.executor [INFO] Processing received message source: task-bolt:5, stream: __ack_ack, id: {}, [-5576733886177167329 6447998047515384780]
2014-12-21 02:45:15 b.s.d.task [INFO] Emitting direct: 10; __acker __ack_ack [-5576733886177167329]

"
STORM-596,"""topology.receiver.buffer.size""  has no effect","https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/messaging/loader.clj#L27

backtype.storm.messaging.loader#mk-receive-thread  accepts max-buffer-size as an input but the value isn't used within the function.
"
STORM-595,storm-hdfs can only work with sequence files that use Writables,"The current SequenceFormat interface requires that key() and value() return a class that implements Writable. This limitation makes it impossible to use object serialization systems like Avro or even Java serialization with the HDFS SequenceFileBolt.

Proposed solution: change SequenceFormat so that key() and value() return an Object, not a Writable. This would keep existing functionality for those implementing Writable support while allowing serialization support for those of us that need it."
STORM-593,No need of rwlock for clojure atom ,"cached-node+port->socket in worker-data is atom, there on need for rwlock endpoint-socket-lock to protect cached-node+port->socket. And after use rwlock, there will be competition between refresh-connections and message send."
STORM-592,"Update stats.clj ""rolling-window-set"" function, exchange the real argument ""num-buckets"" and ""s"" of ""rolling-window"" function","(defn rolling-window-set [updater merger extractor num-buckets & bucket-sizes]
  (RollingWindowSet. updater extractor (dofor [s bucket-sizes] (rolling-window updater merger extractor s num-buckets)) nil)
  )

(defrecord RollingWindow [updater merger extractor bucket-size-secs num-buckets buckets]) 

if not exchange the real argument ”num-buckets“ and ""s"" of “rolling-window” function, then the ""bucket-size-secs"" of RollingWindow is 30/540/4320, and the ""num-buckets"" of RollingWindow is 20

I think that the ""bucket-size-secs"" of RollingWindow is 20, and the ""num-buckets"" of RollingWindow is 30/540/4320."
STORM-591,logback.xml in store-core,"logback.xml is now being included in storm-core.jar.

This causes issues for any application that might include storm-core in it's classpath, having to override the config file location."
STORM-590,KafkaSpout should use kafka consumer api,"Following below ticket
https://github.com/apache/storm/pull/338

KafkaSpout uses kakfa internal data included zk nodes. However it should be changed to get these data from kafka consumer api provided kafka project.

"
STORM-589,Suboptimal default worker hb timeouts for nimbus & supervisor,"Both worker heartbeat timeouts for nimbus and supervisor are set to 30 seconds by default:

https://github.com/apache/storm/blob/3bbdc166bda7fb1a39b6906eda40da9bc83d5d4c/conf/defaults.yaml#L58

https://github.com/apache/storm/blob/3bbdc166bda7fb1a39b6906eda40da9bc83d5d4c/conf/defaults.yaml#L118

This means that it is when a worker dies in relation to its heartbeats that would determine whether the supervisor relaunches it or nimbus reassigns it.

If the supervisor heartbeat is found to have timed out first, it is relaunched.  If the nimbus heartbeat is found to have timed out first, it is rescheduled.

We may want the nimbus time-out to be larger than the supervisor time-out, to give the supervisor a chance to relaunch the worker before nimbus re-assigns it.

As always, users administrating clusters are encouraged to set these as needed."
STORM-588,Executor-Level Rebalance Mechanism,"I. The motivation

The current rebalance mechanism is implemented on the worker level. When rebalance operation is triggered (e.g. by adding/removing a worker), storm kills all the workers with different assignment. It means the rebalance operation has to kill certain running workers and launches them according to the new assignment. The advantage of the mechanism is the simplicity of the implementation, but possibly incurs _huge_ overhead. Actually, the restarting latency is usually more than one second, making the system almost impossible to recover under high incoming data stream rate. No system administrator dares to call rebalance, especially when the system is overloaded! To bring back the real benefits of rebalancing operation, we believe it is important to address the following problems:

*1. Resource wastage and additional initialization cost*: In most cases, the changes on worker’s assignment (if not killed) only affect a small fraction of running executors on it. Only part of them needs to be migrated or created, while the remaining can keep running on the same worker. The current implementation, however, forcefully restarts all the executors, and calls unnecessary initializations (i.e. call Bolt.prepare() and Spout.prepare()) to most of the running tasks. It not only wastes the computation resources of unaffected executors, but also amplifies the initialization costs under certain condition, e.g. index load in the bolt.

*2. Restarting workers causes avoidable in-memory data loss*: Currently, a supervisor uses “kill -9” command to kill its correspondent worker. Consequently, all the tasks on this worker have no chance to save the task data. The running states of the workers, including important information when resuming its duty, are simply lost, potentially causing unnecessary recomputation on the states.

*3. JVM restart cost, long duration and lost of HotSpot optimizations*: Restarting a JVM involves a long initialization procedure, and loses all the runtime optimizations available for the application byte-code. As far as we know, the HotSpot JVM is capable of detecting the performance-critical sections in the code and dynamically translates the Java byte codes of these hot spots into native machine code. In particular, tasks that are CPU-bound can greatly benefit from this feature. If we directly kill the worker, all the advantages of these features are lost.

II. Proposed solutions

1. At the supervisor side:
The current supervisor implementation periodically calls the “sync-processes” function to check whether a live worker should be killed: (1) the mapping relationship between the worker and the topology has changed (e.g. this worker is re-assigned to another topology or the serving topology is killed); (2) the worker’s assignment has updated (e.g. the parallelism of some bolts increases/decreases). 

In order to reuse the worker’s JVM instance as much as possible, we propose that we do not kill the workers mentioned in condition (2), but only kill those that do not belong to the topology anymore (condition (1)).

2. At the worker side: 
Because of the reuse of the JVM instance, workers needs to periodically synchronize its assigned executors. To achieve this, a new thread which is similar to the existing “refresh-connections” is launched, to kill the non-existing executors, and to start newly assigned ones. Note that, in practice, the “refresh-connections“ threads already retrieves the assignment information from the ZK, and this information can be shared with this new thread, which reduce the load of the ZK.

Due to the change of the binging from the running executors to the worker, re-routing tuple is also required. To fulfill this prepose, we need to rewrite the following two functions, “transfer-local-fn” and “transfer-fn” (note the rewrite is compulsive because these two functions are immutable in the current implementation). 

Another function needs careful modification is “WorkerTopologyContext.getThisWorkerTasks()”, because the (defn- mk-grouper … :local-or-shuffle) in “excutor.clj” depends on this function to get required context information. Therefore, in the case that an end user calls “WorkerTopologyContext.getThisWorkerTasks()” in the “prepare()”, and stores the results, if the executor has not restarted, using these results may potentially leads to inconsistency.

In summary, we propose this new executor-level rebalance mechanism, which tries to maximize the resource usage and minimize the rebalance cost. This is essential for the whole system, especially important for the the ultimate purpose on elasticity features for Storm."
STORM-586,Trident kafka spout fails instead of updating offset when kafka offset is out of range.,"Trident KafkaEmitter does not catch the newly added UpdateOffsetException which results in the spout failing repeatedly instead of automatically updating the offset to earliest time. 

PROBLEM:  Exception while using the Trident Kafka Spout.

{code}
2014-12-04 18:38:03 b.s.util ERROR Async loop died! 
java.lang.RuntimeException: storm.kafka.UpdateOffsetException 
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:107) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:78) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:77) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.daemon.executor$fn_4195$fn4207$fn_4254.invoke(executor.clj:745) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.util$async_loop$fn__442.invoke(util.clj:436) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at clojure.lang.AFn.run(AFn.java:24) clojure-1.4.0.jar:na 
at java.lang.Thread.run(Thread.java:745) na:1.7.0_71 
Caused by: storm.kafka.UpdateOffsetException: null 
at storm.kafka.KafkaUtils.fetchMessages(KafkaUtils.java:186) ~stormjar.jar:na 
at storm.kafka.trident.TridentKafkaEmitter.fetchMessages(TridentKafkaEmitter.java:132) ~stormjar.jar:na 
at storm.kafka.trident.TridentKafkaEmitter.doEmitNewPartitionBatch(TridentKafkaEmitter.java:113) ~stormjar.jar:na 
at storm.kafka.trident.TridentKafkaEmitter.failFastEmitNewPartitionBatch(TridentKafkaEmitter.java:72) ~stormjar.jar:na 
at storm.kafka.trident.TridentKafkaEmitter.access$400(TridentKafkaEmitter.java:46) ~stormjar.jar:na 
at storm.kafka.trident.TridentKafkaEmitter$2.emitPartitionBatchNew(TridentKafkaEmitter.java:233) ~stormjar.jar:na 
at storm.kafka.trident.TridentKafkaEmitter$2.emitPartitionBatchNew(TridentKafkaEmitter.java:225) ~stormjar.jar:na 
at storm.trident.spout.PartitionedTridentSpoutExecutor$Emitter$1.init(PartitionedTridentSpoutExecutor.java:125) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at storm.trident.topology.state.RotatingTransactionalState.getState(RotatingTransactionalState.java:83) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at storm.trident.topology.state.RotatingTransactionalState.getStateOrCreate(RotatingTransactionalState.java:110) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at storm.trident.spout.PartitionedTridentSpoutExecutor$Emitter.emitBatch(PartitionedTridentSpoutExecutor.java:121) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at storm.trident.spout.TridentSpoutExecutor.execute(TridentSpoutExecutor.java:82) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at storm.trident.topology.TridentBoltExecutor.execute(TridentBoltExecutor.java:369) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.daemon.executor$fn_4195$tuple_action_fn_4197.invoke(executor.clj:630) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.daemon.executor$mk_task_receiver$fn__4118.invoke(executor.clj:398) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.disruptor$clojure_handler$reify__723.onEvent(disruptor.clj:58) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:99) ~storm-core-0.9.1.2.1.7.0-784.jar:0.9.1.2.1.7.0-784 
... 6 common frames omitted 
2014-12-04 18:38:03 b.s.d.executor ERROR 
java.lang.RuntimeException: storm.kafka.UpdateOffsetException 
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:107) ~[storm-core-0.9.1.2.1.7.0
{code}"
STORM-585,Performance issue in none grouping,"In function mk-grouper, target-tasks is originally a ^List
It then becomes a clojure vector:
...
target-tasks (vec (sort target-tasks))]
...

In :none grouping case, java method '.get' is called on target-tasks object:
...
            (.get target-tasks i)
...

At run time, clojure will use introspection to find a method with a matching name and signature, which is very costly.

Using clojure built-in vector 'get' function instead of '.get' method made us gain 25% performance in our use-case. 
"
STORM-584,LoggingMetricsConsumer metrics.log file is shared by multiple topologies.,"The current cluster.xml file has special configuration for the LoggingMetricsConsumer to place it in a metrics.log file.  There are several issues with this.

The worker, where the LoggingMetricsConsumer is run, is configured using worker.xml and not cluster.xml.
The metrics.log file is shared by all workers running on the same node.  So if by chance we have LoggingMetricsConsumer instances running at the same time, the resulting log file could have a number of issues, including data corruption.

This becomes much worse under security where only the first LoggingMetricsConsumer to run on the node will own the file, and all others will fail to log metrics.

Also there is no way to access metrics.log through the ui now.

Ideally the metrics.log file will follow a pattern similar to the worker log file, and have the topology name and the worker port a part of it.  We also would want to update the logviewer daemon to allow serving up these files and deleting them after a specific period of time.  The it would be awesome if the UI could detect that a logging metrics consumer is installed, and provide links to the metrics.log file as well."
STORM-583,Add spout and bolt implementation for Azure Eventhubs,"Add spout and bolt implementations for Azure Eventhubs - a messaging service that supports AMQP protocol. Just like storm-kafka/storm-hbase, we need to add the project to the /external folder."
STORM-582,Nimbus Halt with FileNotFoundException: '../nimbus/../stormconf.ser' dose not exists," To notice, it is different from STORM-130. It is nimbus to halt. We ran into this problem several times, every time it happens is after several days of stable running. Here is the stacktrace
======================================================
2014-11-03 14:30:56 b.s.d.nimbus [INFO] Cleaning inbox ... deleted: stormjar-c1c856f0-cf8b-4299-9c20-712f169802b7.jar
2014-11-12 19:32:33 b.s.d.nimbus [ERROR] Error when processing event
java.io.FileNotFoundException: File '/tmp/storm-0.9.3/nimbus/stormdist/DNSAnalyse-7-1414992073/stormconf.ser' does not exist
        at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[commons-io-2.4.jar:2.4]
        at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[commons-io-2.4.jar:2.4]
        at backtype.storm.daemon.nimbus$read_storm_conf.invoke(nimbus.clj:89) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at backtype.storm.daemon.nimbus$read_topology_details.invoke(nimbus.clj:324) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at backtype.storm.daemon.nimbus$mk_assignments$iter__3100__3104$fn__3105.invoke(nimbus.clj:649) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at clojure.lang.LazySeq.sval(LazySeq.java:42) ~[clojure-1.5.1.jar:na]
        at clojure.lang.LazySeq.seq(LazySeq.java:60) ~[clojure-1.5.1.jar:na]
        at clojure.lang.RT.seq(RT.java:484) ~[clojure-1.5.1.jar:na]
        at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.5.1.jar:na]
        at clojure.core.protocols$seq_reduce.invoke(protocols.clj:30) ~[clojure-1.5.1.jar:na]
        at clojure.core.protocols$fn__6026.invoke(protocols.clj:54) ~[clojure-1.5.1.jar:na]
        at clojure.core.protocols$fn__5979$G__5974__5992.invoke(protocols.clj:13) ~[clojure-1.5.1.jar:na]
        at clojure.core$reduce.invoke(core.clj:6177) ~[clojure-1.5.1.jar:na]
        at clojure.core$into.invoke(core.clj:6229) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.nimbus$mk_assignments.doInvoke(nimbus.clj:648) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at clojure.lang.RestFn.invoke(RestFn.java:410) ~[clojure-1.5.1.jar:na]
        at backtype.storm.daemon.nimbus$fn__3281$exec_fn__1205__auto____3282$fn__3287$fn__3288.invoke(nimbus.clj:907) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at backtype.storm.daemon.nimbus$fn__3281$exec_fn__1205__auto____3282$fn__3287.invoke(nimbus.clj:906) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at backtype.storm.timer$schedule_recurring$this__2169.invoke(timer.clj:99) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at backtype.storm.timer$mk_timer$fn__2152$fn__2153.invoke(timer.clj:50) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT] 
        at backtype.storm.timer$mk_timer$fn__2152.invoke(timer.clj:42) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_22]
2014-11-12 19:32:33 b.s.util [INFO] Halting process: (""Error when processing an event"")
2014-11-12 19:32:33 b.s.d.nimbus [INFO] Shutting down master
=====================================================
To notice, after stable running for 9 days, without printing ""Clean up {storm-id}"" logs, nimbus halt."
STORM-579,Trident KafkaSpout throws RunTimeException when trying to re-emit a batch that is no longer in Kafka,"On version 0.9.2 - A message from the future is transmitted once, then a RuntimeException is thrown from the KafkaSpout
On version 0.9.3 it seems that UpdateOffsetException is thrown, but it seems that this will still cause the topology to be killed.

There is some faulty code that is causing a batch to be retransmitted to infinity.
Kafka Spout re-emits the batch, and as intended behaviour, has no limit on how many times it will be re-emitted (which is OK).
At some point in the future, the offset of this batch no longer exists on Kafka.
Then the real action kicks in (code snippets are taken from the v0.9.2 tag) - Kafka Spout is using KafkaUtils.fetchMessages to get the batch from Kafka.
Now let us have a look at the relevant code from fetchMessages

if (error.equals(KafkaError.OFFSET_OUT_OF_RANGE) && config.useStartOffsetTimeIfOffsetOutOfRange && errors == 0) {
  long startOffset = getOffset(consumer, topic, partitionId, config.startOffsetTime);
  LOG.warn(""Got fetch request with offset out of range: ["" + offset + ""]; "" +
  ""retrying with default start offset time from configuration. "" +
  ""configured start offset time: ["" + config.startOffsetTime + ""] offset: ["" + startOffset + ""]"");
  offset = startOffset;
}

So if the offset does not exist of Kafka anymore, we will fetch something with a different offset (not sure why this is a good idea). In practice, this will be much larger offset that originally tried to retrieve.
Now let us go back to the Kafka Spout code, now that it got some messages with a much larger offset than what it originally requested, the behaviour is really interesting.
for (MessageAndOffset msg : msgs) {
  if (offset == nextOffset) {
    break;
  }
if (offset > nextOffset) {
  throw new RuntimeException(""Error when re-emitting batch. overshot the end offset"");
}
  emit(collector, msg.message());
  offset = msg.nextOffset();
}
As you can see, at first, nothing touches the offset, so some random message from a different offset *is emitted*
Then, offset will be updated with nextOffset of current message which is of course is very large, which in the next entry to the loop will cause the ""overshot the end offset"" error."
STORM-577,long time launch worker will block supervisor heartbeat,"Supervisor heartbeat and sync-processes use the same timer, when sync-processed use a lot time that greater than nimbus.supervisor.timeout.secs, nimbus will this  supervisor was dead and move worker from this supervisor to others. At most time, block happens when worker launch consume too long time."
STORM-576,"Random ""Clojure failed"" message","Tests have been done with IBM JVM, OpenJDK, and Oracle JVM, version 7.
- 2 tests runs done with IBM JVM and 2 tests runs done with OpenJDK shown a ""Clojure failed"" error message at end of tests, with several Exceptions and traces.
- With Oracle JVM, first with jdk1.7.0_67 then with jdk1.7.0_71 has generated, first: no error, second: ""Clojure failed"" error message.

As an example, error message:
  java.lang.RuntimeException: Error when launching multilang subprocess
happens both with IBM JVM (2 times) and with Oracle JVM (only once). But not with OpenJDK.

I gonna provide logs."
STORM-574,Typo in storm-kafka README,"Fix some typos in storm-kafka README
https://github.com/apache/storm/pull/320"
STORM-569,Add Configuration to enable/disable Bolt's outgoing overflow-buffer,Add Configuration to enable/disable Bolt's outgoing overflow-buffer. This needs to be disabled by default. It would be useful only incase of topologies that have cyclic tuple flow causing livelocks. Refer to STORM-292
STORM-565,Setting topology.groups to nil could break SimpleACLAuthorizer,"https://github.com/apache/storm/blob/f8bce2225f2cfc550dcc67dff469f87062686506/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java#L114

If a user would happen to include this config, but set its value to nil, then we could see an NPE here."
STORM-564,Support worker use dynamic port,"Background: When deploy storm mixed with other services, or deploy storm by yarn and mesos or some other scheduling system, worker port conflict is really a big problem.

In order to fix this, we add worker.dynamic.port to indicate whether worker bind dynamic port or not.
When set worker.dynamic.port as true, worker will use port that specified by supervisor.slots.ports;
When set worker.dynamic.port as false, worker will bind 0, and the port that specified by supervisor.slots.ports will be nominally port in storm, which stand for worker really bind port.

And when worker launched, worker will report it's really bind port by heartbeat, and nimbus send worker's bind port by assignment."
STORM-562,"When Isolated topologies are running ""free slots"" can be wrong/misleading","The isolation scheduler will blacklist a node so that other topologies cannot run on it, but this is not reflected anywhere in the UI, and makes it difficult to know how full a cluster is.

I propose that we add in a new concept to the scheduler.  Where it can ""claim"" a slot for a topology without actually running anything on that slot.  We would then reflect the number of claimed slots in thrift interface and in the UI."
STORM-560,ZkHosts in README should use 2181 as port,"Recently I am using kafka spout to consume message from kafka server. While reading documents on https://github.com/apache/storm/tree/master/external/storm-kafka, the instructions on creating ZkHosts is really confusing. It seems like it should point to broker instead of zookeeper because the example uses port 9092. But  after couple of testing, I realized it should point to the zookeeper and use 2181. The document is not clear on that point. It would be great if we can change that. 
"
STORM-559,ZkHosts in README should use 2181 as port,"Recently I am using kafka spout to consume message from kafka server. While reading documents on https://github.com/apache/storm/tree/master/external/storm-kafka, the instructions on creating ZkHosts is really confusing. It seems like it should point to broker instead of zookeeper because the example uses port 9092. But  after couple of testing, I realized it should point to the zookeeper and use 2181. The document is not clearly on that point. It would be great if we can change that. 
"
STORM-556,netty Client  reconnect bug,"if a storm worker die then restarted and the nimbus  reassign the task on the restarted worker with same host:port， the upstream task will not reconnect the host:port and the topology will hang up。

because the connect has broken and netty Client flush timer 
if (null != channel && channel.isWritable()) {
                            flush(channel);
}
always false."
STORM-554,"the ""get-task-object"" function in task.clj,the type of first param ""topology"" should be ^StormTopology not ^TopologyContext","the ""get-task-object"" function in task.clj,the type of first param ""topology"" should be ^StormTopology not ^TopologyContext"
STORM-553,Provide standard Java/Clojure Code Style Formatter profiles,
STORM-552,add new config storm.messaging.netty.socket.backlog,"In Netty 3.7 backlog  deault value is 50 for JDK 1.6
backlog <= net.core.somaxconn"
STORM-551,"Will be better if Use ""storm.conf.dir""  instead of  ""storm.conf.file""",use --config  given the conf dir 
STORM-550,"get an error when use ""--config"" option to override config file","{code:title=Utils.java|borderStyle=solid}
Index: src/jvm/backtype/storm/utils/Utils.java
===================================================================
--- src/jvm/backtype/storm/utils/Utils.java	(revision 4021)
+++ src/jvm/backtype/storm/utils/Utils.java	(working copy)
@@ -19,7 +19,10 @@
 
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.FileNotFoundException;
 import java.io.FileOutputStream;
+import java.io.FileInputStream;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.InputStreamReader;
@@ -31,6 +34,7 @@
 import java.nio.channels.Channels;
 import java.nio.channels.WritableByteChannel;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -44,6 +48,7 @@
 
 import backtype.storm.serialization.DefaultSerializationDelegate;
 import backtype.storm.serialization.SerializationDelegate;
+
 import org.apache.curator.framework.CuratorFramework;
 import org.apache.curator.framework.CuratorFrameworkFactory;
 import org.apache.commons.lang.StringUtils;
@@ -121,6 +126,27 @@
             throw new RuntimeException(e);
         }
     }
+    
+	@SuppressWarnings(""unchecked"")
+	public static Map<String, Object> loadConfigFile(String file) {
+		Map<String, Object> result = new HashMap<String, Object>();
+		if (file == null) {
+			return result;
+		}
+		Yaml yaml = new Yaml();
+		InputStream in;
+		try {
+			in = new FileInputStream(new File(file));
+			Object obj = yaml.load(in);
+			if (!(obj instanceof Map)) {
+				return Collections.<String, Object> emptyMap();
+			}
+			result = (Map<String, Object>) obj;
+		} catch (FileNotFoundException e) {
+			throw new RuntimeException(e);
+		}
+		return result;
+	}
 
     public static Map findAndReadConfigFile(String name, boolean mustExist) {
         try {
@@ -187,7 +213,7 @@
         if (confFile==null || confFile.equals("""")) {
             storm = findAndReadConfigFile(""storm.yaml"", false);
         } else {
-            storm = findAndReadConfigFile(confFile, true);
+            storm = loadConfigFile(confFile);
         }
         ret.putAll(storm);
         ret.putAll(readCommandLineOpts());
{code}


bin/storm --config ~/deploy/storm-conf/storm.yaml  supervisor 

{code:title=Could not find config file on classpath|borderStyle=solid}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:171)
	at backtype.storm.config$loading__4910__auto__.invoke(config.clj:17)
	at backtype.storm.config__init.load(Unknown Source)
	at backtype.storm.config__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at clojure.lang.RT.loadClassForName(RT.java:2098)
	at clojure.lang.RT.load(RT.java:430)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5417)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:621)
	at clojure.core$use.doInvoke(core.clj:5507)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at backtype.storm.daemon.common$loading__4910__auto__.invoke(common.clj:16)
	at backtype.storm.daemon.common__init.load(Unknown Source)
	at backtype.storm.daemon.common__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at clojure.lang.RT.loadClassForName(RT.java:2098)
	at clojure.lang.RT.load(RT.java:430)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5417)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:621)
	at clojure.core$use.doInvoke(core.clj:5507)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at backtype.storm.daemon.supervisor$loading__4910__auto__.invoke(supervisor.clj:16)
	at backtype.storm.daemon.supervisor__init.load(Unknown Source)
	at backtype.storm.daemon.supervisor__init.<clinit>(Unknown Source)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:249)
	at clojure.lang.RT.loadClassForName(RT.java:2098)
	at clojure.lang.RT.load(RT.java:430)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.lang.Var.invoke(Var.java:415)
	at backtype.storm.daemon.supervisor.<clinit>(Unknown Source)
Caused by: java.lang.RuntimeException: Could not find config file on classpath /home/caokun/deploy/storm-conf/storm.yaml
	at backtype.storm.utils.Utils.findAndReadConfigFile(Utils.java:129)
	at backtype.storm.utils.Utils.readStormConfig(Utils.java:190)
	at backtype.storm.utils.Utils.<clinit>(Utils.java:71)
	... 57 more
Could not find the main class: backtype.storm.daemon.supervisor.  Program will exit.
{code}"
STORM-549,"""topology.enable.message.timeouts"" does nothing","The configuration option, added on 0.8.0 was never implemented:

➜  apache-storm-0.9.2-incubating ag ""topology.enable.message.timeouts""
conf/defaults.yaml
122:topology.enable.message.timeouts: true

storm-core/src/jvm/backtype/storm/Config.java
512:    public static final String TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS = ""topology.enable.message.timeouts"";

➜  apache-storm-0.9.2-incubating ag TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS
storm-core/src/jvm/backtype/storm/Config.java
512:    public static final String TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS = ""topology.enable.message.timeouts"";
513:    public static final Object TOPOLOGY_ENABLE_MESSAGE_TIMEOUTS_SCHEMA = Boolean.class;"
STORM-548,"Receive Thread Shutdown hook should connect to local hostname but not ""localhost"" ","backtype.storm.messaging.loader#launch-receive-thread!
kill-socket should connect to local hostname but not ""localhost""

See Code Line 72:
https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/messaging/loader.clj#L72


{code:title=loader.clj|borderStyle=solid}
Index: src/clj/backtype/storm/messaging/loader.clj
===================================================================
--- src/clj/backtype/storm/messaging/loader.clj	(revision 4017)
+++ src/clj/backtype/storm/messaging/loader.clj	(working copy)
@@ -65,11 +65,12 @@
    :kill-fn (fn [t] (System/exit 1))
    :priority Thread/NORM_PRIORITY]
   (let [max-buffer-size (int max-buffer-size)
+        local-hostname (memoized-local-hostname)
         socket (.bind ^IContext context storm-id port)
         thread-count (if receiver-thread-count receiver-thread-count 1)
         vthreads (mk-receive-threads context storm-id port transfer-local-fn daemon kill-fn priority socket max-buffer-size thread-count)]
     (fn []
-      (let [kill-socket (.connect ^IContext context storm-id ""localhost"" port)]
+      (let [kill-socket (.connect ^IContext context storm-id local-hostname port)]
         (log-message ""Shutting down receiving-thread: ["" storm-id "", "" port ""]"")
         (.send ^IConnection kill-socket
                   -1 (byte-array []))
 {code}  "
STORM-547,Build Problem(s),"David-Laxers-MacBook-Pro:leiningen davidlaxer$ which lein
/Users/davidlaxer/bin/lein
David-Laxers-MacBook-Pro:leiningen davidlaxer$ lein
Could not find artifact leiningen-core:leiningen-core:jar:2.5.1-SNAPSHOT in clojars (https://clojars.org/repo/)
This could be due to a typo in :dependencies or network issues.
If you are behind a proxy, try setting the 'http_proxy' environment variable.
David-Laxers-MacBook-Pro:leiningen davidlaxer$ cd
David-Laxers-MacBook-Pro:~ davidlaxer$ which lein
/Users/davidlaxer/bin/lein
David-Laxers-MacBook-Pro:~ davidlaxer$ lein
Leiningen is a tool for working with Clojure projects.

Several tasks are available:
bluuugh             Dummy task for tests.
change              Rewrite project.clj by applying a function.
check               Check syntax and warn on reflection.
classpath           Print the classpath of the current project.
clean               Remove all files from project's target-path.
compile             Compile Clojure source into .class files.
deploy              Build and deploy jar to remote repository.
deps                Download all dependencies.
do                  Higher-order task to perform other tasks in succession.
downloads           Calculate download statistics from logs.
echo                Task: 'echo' not found
help                Display a list of tasks or help for a given task.
install             Install the current project to the local repository.
jar                 Package up all the project's files into a jar file.
javac               Compile Java source files.
new                 Generate project scaffolding based on a template.
one-or-two          Dummy task for tests
plugin              DEPRECATED. Please use the :user profile instead.
pom                 Write a pom.xml file to disk for Maven interoperability.
pprint              Task: 'pprint' not found
leiningen.project  Problem loading: java.lang.RuntimeException: Unable to resolve symbol: defproject in this context, compiling:(leiningen/project.clj:4:1)
release             Perform :release-tasks.
repl                Start a repl session either with the current project or standalone.
retest              Run only the test namespaces which failed last time around.
run                 Run a -main function with optional command-line arguments.
search              Search remote maven repositories for matching jars.
show-profiles       List all available profiles or display one if given an argument.
sirius              Task: 'sirius' not found
test                Run the project's tests.
trampoline          Run a task without nesting the project's JVM inside Leiningen's.
uberjar             Package up the project files and dependencies into a jar file.
update-in           Perform arbitrary transformations on your project map.
upgrade             Upgrade Leiningen to specified version or latest stable.
var-args            Dummy task for tests.
vcs                 Interact with the version control system.
version             Print version for Leiningen and the current JVM.
with-profile        Apply the given task with the profile(s) specified.
zero                Dummy task for tests.

Run `lein help $TASK` for details.

Global Options:
  -o             Run a task offline.
  -U             Run a task after forcing update of snapshots.
  -h, --help     Print this help or help for a specific task.
  -v, --version  Print Leiningen's version.

See also: readme, faq, tutorial, news, sample, profiles, deploying, gpg,
mixed-source, templates, and copying.
David-Laxers-MacBook-Pro:~ davidlaxer$ 
"
STORM-546,Local hostname configuration ignored by executor,"The executor reports hostname using `util/memoized-local-hostname`, but doesn't check the configuration to see whether `STORM_LOCAL_HOSTNAME` has been set. Under this change the executor checks the configuration before falling back to guessing the local hostname."
STORM-545,DRPC does not return a cause of failure,"Throwing the {{backtype.storm.topology.FailedException}} is a way  to report an error. Unfortunately the clue of the error is not transmitted to the {{backtype.storm.utils.DRPCClient}}. {{DRPCClient}} catches {{backtype.storm.generated.DRPCExecutionException}} that does not carry any cause. It carries a static message ??""Request failed""??.
The optimal solution would be carrying {{FailedException}} or its subclass, or at least its message.
"
STORM-543,Storm failing all input messages (although successfully processed) post restarting worker on the same supervisor slot,"Storm failing all input messages (although successfully processed) post restarting worker on the same supervisor slot.

Steps to simulate the behaviour,
1. Run topology(spout as single instance and multiple instances of bolts)
on multiple workers.
2. Identify the slot on which the single spout instance is running (from STORM UI) and kill it (using kill -9)
3. See if the supervisor started the worker on the same supervisor port. If not then repeat step 2 untill you get supervisor on the same slot as previous one.
4. Pump in a message into the topology.
5. You will see message being processed successfully but acker failing the tuple after the message times out (defaults to 30 secs).
"
STORM-542,Topology summary executors/workers count labels are mixed up,"The ""Num executors"" and ""Num workers"" labels under the ""Topology summary"" section in the main storm ui page are mixed up, their order should be the other way around."
STORM-541,Build produces maven warnings,"The maven compilation task produces the following warnings because version numbers are missing from some plugins in the pom.xml file. I have created a patch file.

[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:maven-shade-clojure-transformer:jar:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.3-rc2-SNAPSHOT, /home/jez/storm/pom.xml, line 661, column 21
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:storm-core:jar:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.3-rc2-SNAPSHOT, /home/jez/storm/pom.xml, line 661, column 21
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:storm-starter:jar:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.3-rc2-SNAPSHOT, /home/jez/storm/pom.xml, line 661, column 21
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:storm-kafka:jar:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.3-rc2-SNAPSHOT, /home/jez/storm/pom.xml, line 661, column 21
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:storm-hdfs:jar:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.3-rc2-SNAPSHOT, /home/jez/storm/pom.xml, line 661, column 21
[WARNING]
[WARNING] Some problems were encountered while building the effective model for com.github.ptgoetz:storm-hbase:jar:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ org.apache.storm:storm:0.9.3-rc2-SNAPSHOT, /home/jez/storm/pom.xml, line 661, column 21
[WARNING]
[WARNING] Some problems were encountered while building the effective model for org.apache.storm:storm:pom:0.9.3-rc2-SNAPSHOT
[WARNING] 'reporting.plugins.plugin.version' for org.apache.maven.plugins:maven-surefire-report-plugin is missing. @ line 661, column 21
[WARNING]
[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.
[WARNING]
[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.
[WARNING]



PATCH:

diff --git a/pom.xml b/pom.xml
index b63d332..37a5acd 100644
--- a/pom.xml
+++ b/pom.xml
@@ -656,10 +656,12 @@
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-javadoc-plugin</artifactId>
+                <version>2.9</version>
             </plugin>
             <plugin>
                 <groupId>org.apache.maven.plugins</groupId>
                 <artifactId>maven-surefire-report-plugin</artifactId>
+                 <version>2.16</version>
                 <configuration>
                     <reportsDirectories>
                         <file>${project.build.directory}/test-reports</file>
@@ -694,6 +696,7 @@
             <plugin>
             <groupId>org.apache.maven.plugins</groupId>
             <artifactId>maven-javadoc-plugin</artifactId>
+            <version>2.9</version>
             </plugin>
             <plugin>
                 <groupId>org.apache.rat</groupId>
"
STORM-538,Guava com.google.thirdparty.publicsuffix is not shaded,"Hi!

I've just tried last 0.9.3-rc1 release which is available on http://storm.apache.org and I got an issue with seems to be linked to STORM-447.

Here is the following stacktrace (stripping our package from trace) :
{noformat}
java.lang.NoSuchFieldError: EXACT
at com.google.common.net.InternetDomainName.findPublicSuffix(InternetDomainName.java:173)
at com.google.common.net.InternetDomainName.<init>(InternetDomainName.java:158)
at com.google.common.net.InternetDomainName.from(InternetDomainName.java:213) 
[...]
{noformat}

??EXACT field is expected to be present in com.google.thirdparty.publicsuffix.PublicSuffixPatterns??

Having a quick look at storm-core-0.9.3-rc1.jar contents :
* com.google.common is relocated to org.apache.storm.guava
* com.google.thirdparty.publicsuffix is *not* relocated so it might conflict with user code (that's my case)

Regards,"
STORM-537,A worker reconnects infinitely to another dead worker,"We're using 0.9.3-rc1. Most probably this wrong behavior was introduced as a side efffect for STORM-409. When I kill a worker, another worker starts to print messages like:
{noformat}
2014-10-20 11:45:03 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-<HOST>:4706... [0]
2014-10-20 11:45:03 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-<HOST>:4706... [1]
2014-10-20 11:45:03 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-<HOST>:4706... [2]
..... so on
{noformat}
Then it reaches default 300 max_retries and starts the cycle again:
{noformat}
2014-10-20 11:54:38 b.s.m.n.Client [INFO] connection established to a remote host Netty-Client-<HOST>:4706, [id: 
0xec088412, /<HOST>:39795 :> <HOST>:4706]
2014-10-20 11:54:38 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-<HOST>:4706... [0]
2014-10-20 11:54:38 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-<HOST>:4706... [1]
2014-10-20 11:54:38 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-<HOST>:4706... [2]
{noformat}
And so on infinitely... 

An issue most probably is in backtype.storm.messaging.netty.Client#connect method in following place which determines that we give up on reconnection:
{code}
if (null != channel) {
    LOG.info(""connection established to a remote host "" + name() + "", "" + channel.toString());
    channelRef.set(channel);
} else {
    close();
    throw new RuntimeException(""Remote address is not reachable. We will close this client "" + name());
}
{code}
I guess (not tried yet), that _channel_ object is not _null_ if this is a real reconnection. So the method return a _channel_ object and then reconnection starts again and again.

This might be fixed by adding explicity *current = null;* into following code block of the same method:
{code}
if (!future.isSuccess()) {
    if (null != current) {
        current.close();
    }
}
{code}

"
STORM-536,java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/TBinaryProtocol on windows,"When I run a program which works with Storm and cassandra in windows7 I got the following error:

java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/TBinaryProtocol
	at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl.createConnection(ThriftSyncConnectionFactoryImpl.java:89) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.SimpleHostConnectionPool.tryOpenAsync(SimpleHostConnectionPool.java:416) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.SimpleHostConnectionPool.borrowConnection(SimpleHostConnectionPool.java:181) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.RoundRobinExecuteWithFailover.borrowConnection(RoundRobinExecuteWithFailover.java:66) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.AbstractExecuteWithFailoverImpl.tryOperation(AbstractExecuteWithFailoverImpl.java:67) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:253) ~[Storm-starter.jar:na]
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl.executeOperation(ThriftKeyspaceImpl.java:465) ~[Storm-starter.jar:na]
"
STORM-534,Store Nimbus Server Information in zookeeper path {storm.zookeeper.root}/nimbus,"1) {nimbus.host} {nimbus.thrift.port} {storm.version} will be stored in  {storm.zookeeper.root}/nimbus like  ""localhost:8826:0.9.3-r1234""

2) Storm Clients only need to configure {storm.zookeeper.root} to get Nimbus Server Information, Configuration like {nimbus.host} {nimbus.thrift.port} {storm.version} will be ignored"
STORM-532,"Supervisor should restart worker immediately, if the worker process does not exist any more ","For now 
if the worker process does not exist any more 
Supervisor will have to wait a few seconds for worker heartbeart timeout and restart worker .

If supervisor knows the worker processid  and check if the process exists in the sync-processes thread ,may need less time to restart worker.

1: record worker process id in the worker local heartbeart 
2: in supervisor  sync-processes ,get process id from worker local heartbeat 
and check if the process exits 
3: if not restart it immediately"
STORM-531,"NoOutputException error when a topology is submitted: ""java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read. Serializer Exception:""","When I want to submit the WordCountTopology, after running the storm jar ...., the following error comes and it does not the topology in the ui!

23543 [Thread-31] ERROR backtype.storm.task.ShellBolt - Halting process: ShellBolt died.
java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:
(Unable to capture error stream)

        at backtype.storm.utils.ShellProcess.readShellMsg(ShellProcess.java:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:116) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_24]
23552 [Thread-31] ERROR backtype.storm.daemon.executor - 
java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:
(Unable to capture error stream)

        at backtype.storm.utils.ShellProcess.readShellMsg(ShellProcess.java:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:116) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_24]
23554 [main] INFO  backtype.storm.daemon.executor - Shut down executor split:[6 6]
23554 [main] INFO  backtype.storm.daemon.executor - Shutting down executor split:[7 7]
23555 [Thread-18-split] INFO  backtype.storm.util - Async loop interrupted!
23555 [Thread-22-spout] INFO  backtype.storm.daemon.task - Emitting: spout default [an apple a day keeps the doctor away]
23557 [Thread-17-disruptor-executor[7 7]-send-queue] INFO  backtype.storm.util - Async loop interrupted!
23559 [main] INFO  backtype.storm.daemon.executor - Shut down executor split:[7 7]
23559 [main] INFO  backtype.storm.daemon.executor - Shutting down executor spout:[8 8]
23560 [Thread-19-disruptor-executor[8 8]-send-queue] INFO  backtype.storm.util - Async loop interrupted!
23560 [Thread-20-spout] INFO  backtype.storm.util - Async loop interrupted!
23560 [main] INFO  backtype.storm.daemon.executor - Shut down executor spout:[8 8]
23560 [main] INFO  backtype.storm.daemon.executor - Shutting down executor spout:[9 9]
23561 [Thread-33] ERROR backtype.storm.task.ShellBolt - Halting process: ShellBolt died.
java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:


        at backtype.storm.utils.ShellProcess.readShellMsg(ShellProcess.java:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:116) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_24]
23561 [Thread-33] ERROR backtype.storm.daemon.executor - 
java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:


        at backtype.storm.utils.ShellProcess.readShellMsg(ShellProcess.java:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:116) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_24]
23562 [Thread-32] ERROR backtype.storm.task.ShellBolt - Halting process: ShellBolt died.
java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:


        at backtype.storm.utils.ShellProcess.readShellMsg(ShellProcess.java:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:116) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_24]
23562 [Thread-32] ERROR backtype.storm.daemon.executor - 
java.lang.RuntimeException: backtype.storm.multilang.NoOutputException: Pipe to subprocess seems to be broken! No output read.
Serializer Exception:


        at backtype.storm.utils.ShellProcess.readShellMsg(ShellProcess.java:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:116) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) [na:1.6.0_24]
23562 [Thread-21-disruptor-executor[9 9]-send-queue] INFO  backtype.storm.util - Async loop interrupted!
23564 [Thread-32] ERROR org.apache.storm.zookeeper.server.NIOServerCnxnFactory - Thread Thread[Thread-32,5,main] died
java.lang.RuntimeException: java.lang.InterruptedException
        at backtype.storm.util$wrap_in_runtime.invoke(util.clj:44) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$exists_node_QMARK_$fn__1889.invoke(zookeeper.clj:102) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$exists_node_QMARK_.invoke(zookeeper.clj:98) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$mkdirs.invoke(zookeeper.clj:114) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$mkdirs.invoke(zookeeper.clj:115) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.cluster$mk_distributed_cluster_state$reify__2136.mkdirs(cluster.clj:119) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.cluster$mk_storm_cluster_state$reify__2593.report_error(cluster.clj:397) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.daemon.executor$throttled_report_error_fn$fn__4101.invoke(executor.clj:179) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.daemon.executor$fn__4321$fn$reify__4366.reportError(executor.clj:737) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.OutputCollector.reportError(OutputCollector.java:223) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt.die(ShellBolt.java:303) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt.access$800(ShellBolt.java:68) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:137) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) ~[na:1.6.0_24]
Caused by: java.lang.InterruptedException: null
        at java.lang.Object.wait(Native Method) ~[na:1.6.0_24]
        at java.lang.Object.wait(Object.java:502) ~[na:1.6.0_24]
        at org.apache.storm.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.zookeeper.ZooKeeper.exists(ZooKeeper.java:1040) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:172) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:161) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:157) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:148) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:36) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$exists_node_QMARK_$fn__1889.invoke(zookeeper.clj:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        ... 12 common frames omitted
23565 [Thread-33] ERROR org.apache.storm.zookeeper.server.NIOServerCnxnFactory - Thread Thread[Thread-33,5,main] died
java.lang.RuntimeException: java.lang.InterruptedException
        at backtype.storm.util$wrap_in_runtime.invoke(util.clj:44) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$exists_node_QMARK_$fn__1889.invoke(zookeeper.clj:102) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$exists_node_QMARK_.invoke(zookeeper.clj:98) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$mkdirs.invoke(zookeeper.clj:114) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$mkdirs.invoke(zookeeper.clj:115) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.cluster$mk_distributed_cluster_state$reify__2136.mkdirs(cluster.clj:119) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.cluster$mk_storm_cluster_state$reify__2593.report_error(cluster.clj:397) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.daemon.executor$throttled_report_error_fn$fn__4101.invoke(executor.clj:179) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.daemon.executor$fn__4321$fn$reify__4366.reportError(executor.clj:737) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.OutputCollector.reportError(OutputCollector.java:223) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt.die(ShellBolt.java:303) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt.access$800(ShellBolt.java:68) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:137) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) ~[na:1.6.0_24]
Caused by: java.lang.InterruptedException: null
        at java.lang.Object.wait(Native Method) ~[na:1.6.0_24]
        at java.lang.Object.wait(Object.java:502) ~[na:1.6.0_24]
        at org.apache.storm.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.zookeeper.ZooKeeper.exists(ZooKeeper.java:1040) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:172) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:161) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:157) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:148) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:36) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$exists_node_QMARK_$fn__1889.invoke(zookeeper.clj:101) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        ... 12 common frames omitted
23568 [Thread-22-spout] INFO  backtype.storm.util - Async loop interrupted!
23570 [main] INFO  backtype.storm.daemon.executor - Shut down executor spout:[9 9]
23570 [main] INFO  backtype.storm.daemon.executor - Shutting down executor spout:[10 10]
23571 [Thread-23-disruptor-executor[10 10]-send-queue] INFO  backtype.storm.util - Async loop interrupted!
23573 [Thread-31] ERROR org.apache.storm.zookeeper.server.NIOServerCnxnFactory - Thread Thread[Thread-31,5,main] died
java.lang.RuntimeException: java.lang.InterruptedException
        at backtype.storm.util$wrap_in_runtime.invoke(util.clj:44) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:91) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$mkdirs.invoke(zookeeper.clj:117) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.zookeeper$mkdirs.invoke(zookeeper.clj:115) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.cluster$mk_distributed_cluster_state$reify__2136.mkdirs(cluster.clj:119) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.cluster$mk_storm_cluster_state$reify__2593.report_error(cluster.clj:397) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.daemon.executor$throttled_report_error_fn$fn__4101.invoke(executor.clj:179) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.daemon.executor$fn__4321$fn$reify__4366.reportError(executor.clj:737) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.OutputCollector.reportError(OutputCollector.java:223) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt.die(ShellBolt.java:303) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt.access$800(ShellBolt.java:68) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at backtype.storm.task.ShellBolt$1.run(ShellBolt.java:137) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at java.lang.Thread.run(Thread.java:679) ~[na:1.6.0_24]
Caused by: java.lang.InterruptedException: null
        at java.lang.Object.wait(Native Method) ~[na:1.6.0_24]
        at java.lang.Object.wait(Object.java:502) ~[na:1.6.0_24]
        at org.apache.storm.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.zookeeper.ZooKeeper.create(ZooKeeper.java:781) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:239) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at org.apache.storm.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:193) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_24]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.6.0_24]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.6.0_24]
        at java.lang.reflect.Method.invoke(Method.java:616) ~[na:1.6.0_24]
        at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93) ~[clojure-1.5.1.jar:na]
        at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28) ~[clojure-1.5.1.jar:na]
        at backtype.storm.zookeeper$create_node.invoke(zookeeper.clj:90) ~[storm-core-0.9.3-rc1.jar:0.9.3-rc1]
        ... 11 common frames omitted
"
STORM-530,Upgrade version of HttpClient,"Storm 0.9.2 seems to ship with version 4.1.1 of HttpClient in its lib directory.  This version is very out dated (GA in May of 2011) and becomes problematic when integrating tools like Solr with Storm.   Solr 4.7.2 for example depends on a much newer version 4.3.1 (GA in October 2013).  This may be resolved by STORM-447, but wanted to make sure this was part of that fix.

The workaround seems to be replacing the older versions in the lib directory with newer ones, but that shouldn't be required for such a simple integration."
STORM-529,Python BasicBolt does not behave as the Java BasicBolt,We expect the basicbolt on exception to report an error and fail the tuple. Instead it just reports an error and stops processing any more tuples.
STORM-528,examples/storm-starter/multilang/resources/storm.py diverged,"As we know storm.py is committed three times. One of these files (the one in examples) have diverged from the other two. It also introduces some strange behavior as it calls ""log"" instead of ""error"" when a bolt fails.
I would recommend to override the storm/examples version with the one in storm-core and then have the diff as a new pull request"
STORM-527,"update worker.clj -- delete ""missing-tasks"" checking","missing-tasks set is created by two times filter my-assignment map, so i think keys(my-assignment) contains missing-tasks set.
missing-tasks is always empty. (empty? missing-tasks) always return true. 

(let [missing-tasks (->> needed-tasks
                                       (filter (complement my-assignment)))]
                (when-not (empty? missing-tasks)
                  (log-warn ""Missing assignment for following tasks: "" (pr-str missing-tasks))
                  ))

https://github.com/apache/storm/commit/4b65d8152527f6770db6d103a7e2950f1a1f5f91"
STORM-526,Nimbus triggered complete removal of all topologies due to maintenance in 2 out of 3 zookeeper servers,"We use a cluster of 3 zookeepers, all 3 ip addresses are in the storm.yml file. We were restarting one zookeeper, and once it was ready, we restarted the second zookeeper. All this time the third zookeeper was ""green"" (as monitored by Netfix Exhibitor).

At this same time nimbus has ""decided"" to remove all topologies (log entry is ""Corrupt topology my-topology-xxx has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up..."").

I looked at the relevant code and I am not entirely sure the log message describes correctly the code.

Could anyone please read the nimbus.clj#cleanup-corrupt-topologies and explain under what conditions does nimbus act in that way ?
https://github.com/apache/storm/blob/v0.9.2-incubating/storm-core/src/clj/backtype/storm/daemon/nimbus.clj#L854


Log file:
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-1-2-1412151059 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-0-1-1412151059 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-3-4-1412151062 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Corrupt topology my-topology-2-3-1412151060 has state on zookeeper but doesn't have a local dir on Nimbus. Cleaning up...
2014-10-01 10:47:19 b.s.d.nimbus [INFO] Starting Nimbus server...
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-1-2-1412151059
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-0-1-1412151059
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-3-4-1412151062
2014-10-01 10:47:20 b.s.d.nimbus [INFO] Cleaning up my-topology-2-3-1412151060
2014-10-01 10:52:16 b.s.d.nimbus [INFO] Shutting down master


"
STORM-525,UI Component Page Executor Uptimes are not sorted correctly,"When manually sorting the Uptime column on a bolt component page, the times are incorrectly sorted (by ASCII)."
STORM-522,NullPointerException in storm.kafka.ZkCoordinator,"We've been seeing these about once a week. The nimbus will kill and reassign the topology (on the same hardware) get caught up to ""now"" in kafka and happily continue running until the next time it encounters an event like this.

{code}
2014-09-30 13:59:40 s.k.ZkCoordinator [INFO] Deleted partition managers: [10.1.2.1:9092:0, 10.1.2.2:9092:1, 10.1.2.3:9092:3]
2014-09-30 13:59:40 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.NullPointerException
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:148) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:77) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:94) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__5573$fn__5588$fn__5617.invoke(executor.clj:563) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]
Caused by: java.lang.NullPointerException: null
        at storm.kafka.DynamicPartitionConnections.unregister(DynamicPartitionConnections.java:39) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.close(PartitionManager.java:205) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:135) ~[stormjar.jar:na]
        ... 6 common frames omitted
2014-09-30 13:59:40 b.s.d.executor [ERROR]
java.lang.RuntimeException: java.lang.NullPointerException
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:148) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:77) ~[stormjar.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:94) ~[stormjar.jar:na]
        at backtype.storm.daemon.executor$fn__5573$fn__5588$fn__5617.invoke(executor.clj:563) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
        at java.lang.Thread.run(Thread.java:745) [na:1.7.0_55]
Caused by: java.lang.NullPointerException: null
        at storm.kafka.DynamicPartitionConnections.unregister(DynamicPartitionConnections.java:39) ~[stormjar.jar:na]
        at storm.kafka.PartitionManager.close(PartitionManager.java:205) ~[stormjar.jar:na]
        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:135) ~[stormjar.jar:na]
        ... 6 common frames omitted
2014-09-30 13:59:40 b.s.util [INFO] Halting process: (""Worker died"")
{code}"
STORM-521,UI: No way of telling easily that tuples failed because of a TOPOLOGY_MESSAGE_TIMEOUT_SECS,"In the UI and in general there's no easy way to figure out that tuples have failed because of a timeout becoming fully processed.  

It would really great if one could tell what was the last bolt a tuple in the tuple DAG got processed in before the whole tree failed because of a timeout."
STORM-520,Using storm.local.hostname causes bolts to not communicate with each other,"In our setup we run different supervisors as docker container on top of CoreOS.

If I use docker's -h (host) and give the container the same hostname as the host machine, the bolts are able to register with nimbus correctly, talk to each other as well as transfer data inside the same container between different workers located on the same machine.

Utilizing 'storm.local.hostname' instead of using docker's -h option results in the bolts having a hard time (read: unable to) communicating between workers on the same machine/container (we only run one container per physical machine which contains 4 workers + supervisor) and having sporadic success (50/50) communicating with bolts in different machines."
STORM-519,HBaseLookupBolt should pass the input tuple to ValueMapper to allow users to enrich input tuple.,"Currently the hbase lookup bolt only passes the result returned after the lookup is performed to HBaseValueMapper interface. This restricts the user to only return result instance's variable as values that can be emitted. 

The general use case is , user receives a tuple , it looks up some value from hbase for this tuple, the lookup bolt returns some combination of original tuple and new lookup value. With the current interface the user can only return/emit values from the lookup and not from original tuple. I propose to pass the original input tuple along with the result to interface."
STORM-518,"subprocess.py not found while executing ""bin/storm nimbus"" python startup script error ","While starting storm via script ""bin/storm nimbus"" in 0.9.2
It kept giving the following python error under Ubuntu 12.

------------------------ERROR---------------------------------------------
 p = sub.Popen(command, stdout=sub.PIPE)

  File ""/usr/lib64/python2.6/subprocess.py"", line 639, in __init__
    errread, errwrite)

  File ""/usr/lib64/python2.6/subprocess.py"", line 1228, in _execute_child
    raise child_exception

OSError: [Errno 2] No such file or directory
-------------------------------------------------------------------------------
by putting additional
""shell=True"" param in each call of  p = sub.Popen(command, stdout=sub.PIPE) solved the issue.

"
STORM-517,"Support for ""-Dservice="" in bin/storm, via JAVA_SERVICE_NAME environment variable","*Reasoning:*

Currently the way that _bin/storm_ starts the various storm processes (nimbus, ui, supervisor) results in a process list entry like the following, for each process:

{code}
ubuntu   21940  0.4  3.9 2294904 151384 ?      Ssl  Oct03   2:47 java -server -Dstorm.options= -Dstorm.home=/home/ubuntu/apache-storm-0.9.2-incubating -Djava.library.path=/usr/local/lib -Dstorm.conf.file= -cp /home/ubuntu/apache-storm-0.9.2-incubating/lib/asm-4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-logging-1.1.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-devel-0.3.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/storm-core-0.9.2-incubating.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/log4j-over-slf4j-1.6.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/tools.cli-0.2.4.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-servlet-0.3.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-codec-1.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/logback-classic-1.0.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clj-time-0.4.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/httpclient-4.3.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/slf4j-api-1.6.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/httpcore-4.3.2.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/carbonite-1.4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clout-1.0.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jetty-6.1.26.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clojure-1.5.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-io-2.4.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-exec-1.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jgrapht-core-0.9.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/curator-client-2.4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/tools.macro-0.1.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/zookeeper-3.4.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jline-2.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/minlog-1.2.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/tools.logging-0.2.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/reflectasm-1.07-shaded.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/guava-13.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-core-1.1.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/logback-core-1.0.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/netty-3.6.3.Final.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/ring-jetty-adapter-0.3.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/core.incubator-0.1.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-lang-2.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/netty-3.2.2.Final.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/curator-framework-2.4.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/disruptor-2.10.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/servlet-api-2.5-20081211.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/clj-stacktrace-0.2.4.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/chill-java-0.3.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/compojure-1.1.3.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/kryo-2.21.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/jetty-util-6.1.26.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/servlet-api-2.5.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/json-simple-1.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/commons-fileupload-1.2.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/snakeyaml-1.11.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/math.numeric-tower-0.0.1.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/joda-time-2.0.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/hiccup-0.3.6.jar:/home/ubuntu/apache-storm-0.9.2-incubating/lib/objenesis-1.2.jar:/home/ubuntu/apache-storm-0.9.2-incubating/conf -Xmx1024m -Dlogfile.name=nimbus.log -Dlogback.configurationFile=/home/ubuntu/apache-storm-0.9.2-incubating/logback/cluster.xml backtype.storm.daemon.nimbus
{code}

This means that it can be difficult to differentiate between the processes in the process list and monitoring tools that use SNMP to get the process list (such as Zenoss) can't differentiate because the process list arguments gets truncated way before the list of .jars in the text stops being the same.

Ideally we'd be able to set *-Dservice=foo* in _bin/storm_ via an environment variable, so it's easy to set with things like upstart, runit, etc

*Fix:*

I've added support for *-Dservice=* in bin/storm, via a JAVA_SERVICE_NAME environment variable, in _bin/storm_

You can see the changes I made against _bin/storm_ from 0.9.2-incubating in https://github.com/solarce/storm-cookbook/blob/master/templates/default/bin_storm.py.erb and how I use it in an upstart template, https://github.com/solarce/storm-cookbook/blob/master/templates/default/storm-upstart-conf.erb#L19

I'm going to submit the same changes against master on github"
STORM-516,Thrift source files in storm-core.jar remain in package org.apache.thrift instead of thrift7,"All thrift *.java source files in the storm-core package remain in the package org.apache.thirft instead of org.apache.thrift7

The compiled *.class files however get relocated by the shade plugin

steps to reproduce:
1) build storm using mvn package -DskipTests
2) unzip -p storm-core/target/storm-core-0.9.2-incubating.jar org/apache/thrift7/TException.java

results in:
snip
package org.apache.thrift;
snip

The problem seems to have been introduced with this commit: https://github.com/apache/storm/commit/22ddd6e6d5c78e36610366e71ea879b283575e01"
STORM-515,Clojure documentation and examples,"Clojure storm-starter example is extremely basic (https://github.com/apache/storm/blob/master/examples/storm-starter/src/clj/storm/starter/clj/word_count.clj) and doesn't demonstrate many use cases.

Clojure documentation page (http://storm.incubator.apache.org/documentation/Clojure-DSL.html) has unreadable code samples, and glosses over many details in how bolts are setup, how to initialize parametarized bolts, where the requires such as execute come from, and so on.

It would be nice to see clear documentation with properly documented examples for each of the use cases."
STORM-514,Update storm-starter README now that Storm has graduated from Incubator,The usage instructions in the storm-starter README still refer to the pre-graduation code setup (e.g. git repo urls).
STORM-513,ShellBolt keeps sending heartbeats even when child process is hung,"If I'm understanding everything correctly with how ShellBolts work, the Java ShellBolt executor is the part of the topology that sends heartbeats back to Nimbus to let it know that a particular multilang bolt is still alive.  The problem with this is that if the multilang subprocess/bolt severely hangs (i.e., it will not even respond to {{SIGALRM}} and the like), the Java ShellBolt does not seem to notice or care. Simply having the tuple get replayed when it times out will not suffice either, because the subprocess will still be stuck.

The most obvious way to handle this seem to be to add heartbeating to the multilang protocol itself, so that the ShellBolt expects a message of some kind every {{timeout}} seconds."
STORM-511,Storm-Kafka spout keeps sending fetch requests with invalid offset,"With default behaviour (KafkaConfig.useStartOffsetTimeIfOffsetOutOfRange == true) when Kafka returns the error about offset being out of range, storm.kafka.KafkaUtils.fetchMessages tries to fix offset in local scope and retry fetch request. But if there are no more messages appeared under that specified partition it will never update the PartitionManager, but keep sending tons of requests with invalid offset to Kafka broker. On both sides Storm and Kafka logs grow extremely quick during that time."
STORM-510,Netty messaging client blocks transfer thread on reconnect,"The latest netty client code will attempt to reestablish the connection on failure as part of the send method call.  It will block until the connection is established or a timeout happens, by default this is about 30 seconds, which is also the default tuple timeout.  

This is exacerbated by the read lock that is held during the send, that prevents the node->socket mapping from changing while we are sending.  This is mostly so that we don't close connections while we are trying to write to them, which would cause an exception.  But this makes it so if there are multiple workers on a node that all get rescheduled we will wait the full 30 seconds to timeout for each worker.

send must be non-blocking in the current design of the worker, or it will prevent other messages from being delivered, and is likely to cause many many messages to timeout on a reschedule."
STORM-509,(Security) Make groups checking specific for SimpleACLAuthorizer.,"SimpleACLAuthorizer has groups support right now, but it only validates that the user performing an action and the user running the topology have at least one group in common. This is far from ideal, because unix groups are often used to denote OS System permissions and there is typically a users group that everyone belongs to.  We really should have a separate set of configs for the explicit groups that we want to grant permissions to, instead of the groups the user is a part of."
STORM-508,Update DEVELOPER.md now that Storm has graduated from Incubator,The current DEVELOPER.md still contains references to outdated git repositories (GitHub and ASF).
STORM-507,After STORM-500 visualization update causes spinner to appear,When the visualization of the topology is active each time it polls to get updated stats the spinner appears.  This is not ideal.
STORM-506,"UI Counts acks & fails from bolts in ""Topology Stats""","The aggregation for the total topology statistics should count acks and fails for tuple ""trees"" only, but it includes acks and fails of bolts as well.

[code|https://github.com/apache/storm/blob/cfcedcf4ad5e5c96fe3f30a0b710cfc604063231/storm-core/src/clj/backtype/storm/ui/core.clj#L344]
"
STORM-505,Debug Log Message in logviewer causes exception.,"An incorrectly-constructed list of file names passed to a log-debug is causing a run-time exception when DEBUG log level is used.

This prevents log clean-up from happening correctly."
STORM-503,Short disruptor queue wait time leads to high CPU usage when idle,"I am fairly new to storm, but I observed some behavior which I believe may be unintended and wanted to report it...

I was experimenting with using storm on a topology which had large numbers of threads (30) and was running on a single node for test purposes and noticed that even when no tuples were being processed, there was over 100% CPU utilization.

I became concerned and investigated by attempting to reproduce with a very simple topology. I took the WordCountTopology from storm-starter and ran it in  an Ubuntu VM. I increased the sleep time in the RandomSentenceSpout that feeds the topology to 10 seconds so that there was effectively no work to do. I then modified the topology so that there were 30 threads for each bolt and only one instance of the spout. When I ran the topology I noticed that there was again 100% CPU usage when idle even on this very simple topology. After extensive experimentation (netty vs. zeromq, 0.9.3, 0.9.2, 0.9.1, multiple JVM versions) I used yourkit and found that the high utilization was coming from DisruptorQueue.consumeBatchWhenAvailable where there is this code:

final long availableSequence = _barrier.waitFor(nextSequence, 10, TimeUnit.MILLISECONDS);

I increased to 100 ms and was able to reduce the CPU utilization when idle. I am new to storm, so I am not sure what affect modifying this number has. I Is this expected behavior from storm? I would like to propose modifying the code so that this wait is configurable if possible...

Thank You"
STORM-501,Missing StormSubmitter API,"Recently progress support was added to storm submitter, but in my review I missed that we had dropped support for the API

StormSubmitter.submitTopology(String,Map,StormTopology,SubmitOptions)

This is not a very commonly used API, but I do have some customers where were relying on it, and when it was dropped they broke."
STORM-500,Add Spinner when UI is loading stats from nimbus,"As UI makes ajax calls to download stats from nimbus, it would be nice to have spinner suggesting user about page loading is in progress.  "
STORM-499,Document and clean up shaded dependncy resolution with maven,"After STORM-447 went in several dependencies have been shaded.  They no longer exist in their normal form on the storm classpath, but the pom.xml that is installed with maven lists them as provided.

This can become an issue if their topology does not call out dependencies properly and they depend on something like guava, assuming that it will be available from storm.

from https://github.com/apache/incubator-storm/pull/219  it looks like we should add

<keepDependenciesWithProvidedScope>false</keepDependenciesWithProvidedScope>

to help fix this, and add some documentation about how the maven assembly plug-in decides what to pull in, to give users instructions on how to depend on the shaded version of a package if they want to, or to be sure that they are including all of their real dependencies in their pom so in the future when we shade other things they are not caught off guard."
STORM-498,storm-kafka: make ZK connection timeout configurable in Kafka spout,"Currently the Kafka spout uses a hardcoded ZK connection timeout of 15 seconds in {{external/storm-kafka/src/jvm/storm/kafka/DynamicBrokersReader.java}}:

{code}
_curator = CuratorFrameworkFactory.newClient(
    zkStr,
    Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT)),
    15000,
    new RetryNTimes(Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES)),
        Utils.getInt(conf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));
{code}

We should make this setting configurable, similar to the ZK session timeout setting."
STORM-497,"b.s.m.netty.Server.getMessageQueueId is not thread safe, and can return null","b.s.m.netty.Server.getMessageQueueId

https://github.com/apache/incubator-storm/blob/c5c3571ca15ee2dd675fb3cac44bd0f926ccfc67/storm-core/src/jvm/backtype/storm/messaging/netty/Server.java#L132-150

If there is contention when updating taskToQueueId subsequent threads through the synchronized block do not set queueId and the function returns null.

Also reading from a HashMap while someone else is updating it, even with a lock held, can result in a deadlock.  I have seen this happen before on Hadoop.  It is likely to be very rare, but I would rather use transactional memory, aka an AtomicReference, to avoid this entirely.  "
STORM-495,Add delayed retries to KafkaSpout,"If a tuple in the topology originates from the KafkaSpout from the external/storm-kafka sources, and if a bolt in the topology indicates a failure by calling fail() on its OutputCollector, the KafkaSpout will immediately retry the message.

We wish to use this failure and retry behavior in our ingestion system whenever we experience a recoverable error from a downstream system, such as a 500 or 503 error from a service we depend on.  But with the current KafkaSpout behavior, doing so results in a tight loop where we retry several times over a few seconds and then give up.  I want to be able to delay retry to give the downstream service some time to recover.  Ideally, I would like to have configurable, exponential backoff retry.
"
STORM-493,Workers don't inherit storm.conf.file/storm.options properties of the supervisor,"If we override some configuration parameters on the command line (using storm -c ""param=value"") when we launch the supervisor, workers don't inherit them.

{noformat}
> cat conf/storm.yaml
storm.zookeeper.servers:
     - ""127.0.0.1""
nimbus.host: ""127.0.0.1""
storm.zookeeper.root: ""/stormtest""
storm.local.dir: ""storm-local-main""

> python bin/storm -c ""storm.local.dir=\""storm-local-custom\"""" supervisor

> less logs/worker-6701.log
[...]
2014-09-10 09:35:00 o.a.s.z.s.ZooKeeperServer [INFO] Server environment:user.dir=/optc/2014-09-05-5aae7686
2014-09-10 09:35:01 b.s.d.worker [INFO] Launching worker for mytopo-1-1410334488 on 96f32da2-2043-4371-988b-ec9ca107ce69:6701 with id b9178c80-922b-4b8d-9984-7cfaf06f3c86 and conf {""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.tick.tuple.freq.secs"" nil, ""topology.builtin.metrics.bucket.size.secs"" 60, ""topology.fall.back.on.java.serialization"" true, ""topology.max.error.report.per.interval"" 5, ""zmq.linger.millis"" 5000, ""topology.skip.missing.kryo.registrations"" false, ""storm.messaging.netty.client_worker_threads"" 1, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""nimbus.reassign"" true, ""topology.trident.batch.emit.interval.millis"" 500, ""storm.messaging.netty.flush.check.interval.ms"" 10, ""nimbus.monitor.freq.secs"" 10, ""logviewer.childopts"" ""-Xmx128m"", ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""topology.executor.send.buffer.size"" 1024, ""storm.local.dir"" ""storm-local-main"", ""storm.messaging.netty.buffer_size"" 5242880, ""supervisor.worker.start.timeout.secs"" 120, ""topology.enable.message.timeouts"" true, ""nimbus.cleanup.inbox.freq.secs"" 600, ""nimbus.inbox.jar.expiration.secs"" 3600, ""drpc.worker.threads"" 64, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.DefaultSerializationDelegate"", ""topology.worker.shared.thread.pool.size"" 4, ""nimbus.host"" ""127.0.0.1"", ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2181, ""transactional.zookeeper.port"" nil, ""topology.executor.receive.buffer.size"" 1024, ""transactional.zookeeper.servers"" nil, ""storm.zookeeper.root"" ""/stormtest"", ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""supervisor.enable"" true, ""storm.messaging.netty.server_worker_threads"" 1, ""storm.zookeeper.servers"" [""127.0.0.1""], ""transactional.zookeeper.root"" ""/transactional"", ""topology.acker.executors"" nil, ""topology.transfer.buffer.size"" 1024, ""topology.worker.childopts"" nil, ""drpc.queue.size"" 128, ""worker.childopts"" ""-Xmx768m"", ""supervisor.heartbeat.frequency.secs"" 5, ""topology.error.throttle.interval.secs"" 10, ""zmq.hwm"" 0, ""drpc.port"" 3772, ""supervisor.monitor.frequency.secs"" 3, ""drpc.childopts"" ""-Xmx768m"", ""topology.receiver.buffer.size"" 8, ""task.heartbeat.frequency.secs"" 3, ""topology.tasks"" nil, ""storm.messaging.netty.max_retries"" 300, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""nimbus.thrift.max_buffer_size"" 1048576, ""topology.max.spout.pending"" nil, ""storm.zookeeper.retry.interval"" 1000, ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""supervisor.slots.ports"" [6700 6701 6702 6703], ""topology.environment"" nil, ""topology.debug"" false, ""nimbus.task.launch.secs"" 120, ""nimbus.supervisor.timeout.secs"" 60, ""topology.message.timeout.secs"" 30, ""task.refresh.poll.secs"" 10, ""topology.workers"" 1, ""supervisor.childopts"" ""-Xmx256m"", ""nimbus.thrift.port"" 6627, ""topology.stats.sample.rate"" 0.05, ""worker.heartbeat.frequency.secs"" 1, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""nimbus.task.timeout.secs"" 30, ""storm.zookeeper.connection.timeout"" 15000, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""drpc.invocations.port"" 3773, ""logviewer.port"" 8000, ""zmq.threads"" 1, ""storm.zookeeper.retry.times"" 5, ""topology.worker.receiver.thread.count"" 1, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""topology.state.synchronization.timeout.secs"" 60, ""supervisor.worker.timeout.secs"" 30, ""nimbus.file.copy.expiration.secs"" 600, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""logviewer.appender.name"" ""A1"", ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.request.timeout.secs"" 600, ""storm.local.mode.zmq"" false, ""ui.port"" 8080, ""nimbus.childopts"" ""-Xmx1024m"", ""storm.cluster.mode"" ""distributed"", ""topology.max.task.parallelism"" nil, ""storm.messaging.netty.transfer.batch.size"" 262144, ""topology.classpath"" nil}
2014-09-10 09:35:01 b.s.util [DEBUG] Touching file at storm-local-main/workers/b9178c80-922b-4b8d-9984-7cfaf06f3c86/pids/30958
2014-09-10 09:35:01 b.s.d.worker [ERROR] Error on initialization of server mk-worker
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method) ~[na:1.7.0_60]
	at java.io.File.createNewFile(File.java:1006) ~[na:1.7.0_60]
	at backtype.storm.util$touch.invoke(util.clj:519) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at backtype.storm.daemon.worker$fn__6535$exec_fn__1474__auto____6536.invoke(worker.clj:362) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$fn__6535$mk_worker__6591.doInvoke(worker.clj:354) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$_main.invoke(worker.clj:461) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
2014-09-10 09:35:01 b.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:319) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$fn__6535$mk_worker__6591.doInvoke(worker.clj:354) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$_main.invoke(worker.clj:461) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
[...]
{noformat}

The worker tries to use the local directory defined in the configuration file (storm-local-main) instead of the local directory defined on the command line (storm-local-custom).

----

There is a similar problem with configuration file name. If we specify it on the command line, workers don't inherit it and search the default ""storm.yaml"" file in the classpath.

{noformat}
> cat conf/storm.yaml
storm.zookeeper.servers:
     - ""127.0.0.1""
nimbus.host: ""127.0.0.1""
storm.zookeeper.root: ""/stormtest""
storm.local.dir: ""storm-local-default""

> cat conf/storm-custom.yaml
storm.zookeeper.servers:
     - ""127.0.0.1""
nimbus.host: ""127.0.0.1""
storm.zookeeper.root: ""/stormtest""
storm.local.dir: ""storm-local-custom""

> python bin/storm --config storm-custom.yaml supervisor

> less logs/worker-6700.log
[...]
2014-09-10 10:16:01 o.a.s.z.s.ZooKeeperServer [INFO] Server environment:user.dir=/optc/2014-09-05-5aae7686
2014-09-10 10:16:01 b.s.d.worker [INFO] Launching worker for mytopo-1-1410336834 on 38c99fbd-e5e5-4c53-b704-7ec460f5e227:6700 with id 2133bad6-432d-4c63-a156-1184064d296b and conf {""dev.zookeeper.path"" ""/tmp/dev-storm-zookeeper"", ""topology.tick.tuple.freq.secs"" nil, ""topology.builtin.metrics.bucket.size.secs"" 60, ""topology.fall.back.on.java.serialization"" true, ""topology.max.error.report.per.interval"" 5, ""zmq.linger.millis"" 5000, ""topology.skip.missing.kryo.registrations"" false, ""storm.messaging.netty.client_worker_threads"" 1, ""ui.childopts"" ""-Xmx768m"", ""storm.zookeeper.session.timeout"" 20000, ""nimbus.reassign"" true, ""topology.trident.batch.emit.interval.millis"" 500, ""storm.messaging.netty.flush.check.interval.ms"" 10, ""nimbus.monitor.freq.secs"" 10, ""logviewer.childopts"" ""-Xmx128m"", ""java.library.path"" ""/usr/local/lib:/opt/local/lib:/usr/lib"", ""topology.executor.send.buffer.size"" 1024, ""storm.local.dir"" ""storm-local-default"", ""storm.messaging.netty.buffer_size"" 5242880, ""supervisor.worker.start.timeout.secs"" 120, ""topology.enable.message.timeouts"" true, ""nimbus.cleanup.inbox.freq.secs"" 600, ""nimbus.inbox.jar.expiration.secs"" 3600, ""drpc.worker.threads"" 64, ""storm.meta.serialization.delegate"" ""backtype.storm.serialization.DefaultSerializationDelegate"", ""topology.worker.shared.thread.pool.size"" 4, ""nimbus.host"" ""127.0.0.1"", ""storm.messaging.netty.min_wait_ms"" 100, ""storm.zookeeper.port"" 2181, ""transactional.zookeeper.port"" nil, ""topology.executor.receive.buffer.size"" 1024, ""transactional.zookeeper.servers"" nil, ""storm.zookeeper.root"" ""/stormtest"", ""storm.zookeeper.retry.intervalceiling.millis"" 30000, ""supervisor.enable"" true, ""storm.messaging.netty.server_worker_threads"" 1, ""storm.zookeeper.servers"" [""127.0.0.1""], ""transactional.zookeeper.root"" ""/transactional"", ""topology.acker.executors"" nil, ""topology.transfer.buffer.size"" 1024, ""topology.worker.childopts"" nil, ""drpc.queue.size"" 128, ""worker.childopts"" ""-Xmx768m"", ""supervisor.heartbeat.frequency.secs"" 5, ""topology.error.throttle.interval.secs"" 10, ""zmq.hwm"" 0, ""drpc.port"" 3772, ""supervisor.monitor.frequency.secs"" 3, ""drpc.childopts"" ""-Xmx768m"", ""topology.receiver.buffer.size"" 8, ""task.heartbeat.frequency.secs"" 3, ""topology.tasks"" nil, ""storm.messaging.netty.max_retries"" 300, ""topology.spout.wait.strategy"" ""backtype.storm.spout.SleepSpoutWaitStrategy"", ""nimbus.thrift.max_buffer_size"" 1048576, ""topology.max.spout.pending"" nil, ""storm.zookeeper.retry.interval"" 1000, ""topology.sleep.spout.wait.strategy.time.ms"" 1, ""nimbus.topology.validator"" ""backtype.storm.nimbus.DefaultTopologyValidator"", ""supervisor.slots.ports"" [6700 6701 6702 6703], ""topology.environment"" nil, ""topology.debug"" false, ""nimbus.task.launch.secs"" 120, ""nimbus.supervisor.timeout.secs"" 60, ""topology.message.timeout.secs"" 30, ""task.refresh.poll.secs"" 10, ""topology.workers"" 1, ""supervisor.childopts"" ""-Xmx256m"", ""nimbus.thrift.port"" 6627, ""topology.stats.sample.rate"" 0.05, ""worker.heartbeat.frequency.secs"" 1, ""topology.tuple.serializer"" ""backtype.storm.serialization.types.ListDelegateSerializer"", ""topology.disruptor.wait.strategy"" ""com.lmax.disruptor.BlockingWaitStrategy"", ""topology.multilang.serializer"" ""backtype.storm.multilang.JsonSerializer"", ""nimbus.task.timeout.secs"" 30, ""storm.zookeeper.connection.timeout"" 15000, ""topology.kryo.factory"" ""backtype.storm.serialization.DefaultKryoFactory"", ""drpc.invocations.port"" 3773, ""logviewer.port"" 8000, ""zmq.threads"" 1, ""storm.zookeeper.retry.times"" 5, ""topology.worker.receiver.thread.count"" 1, ""storm.thrift.transport"" ""backtype.storm.security.auth.SimpleTransportPlugin"", ""topology.state.synchronization.timeout.secs"" 60, ""supervisor.worker.timeout.secs"" 30, ""nimbus.file.copy.expiration.secs"" 600, ""storm.messaging.transport"" ""backtype.storm.messaging.netty.Context"", ""logviewer.appender.name"" ""A1"", ""storm.messaging.netty.max_wait_ms"" 1000, ""drpc.request.timeout.secs"" 600, ""storm.local.mode.zmq"" false, ""ui.port"" 8080, ""nimbus.childopts"" ""-Xmx1024m"", ""storm.cluster.mode"" ""distributed"", ""topology.max.task.parallelism"" nil, ""storm.messaging.netty.transfer.batch.size"" 262144, ""topology.classpath"" nil}
2014-09-10 10:16:01 b.s.util [DEBUG] Touching file at storm-local-default/workers/2133bad6-432d-4c63-a156-1184064d296b/pids/31689
2014-09-10 10:16:01 b.s.d.worker [ERROR] Error on initialization of server mk-worker
java.io.IOException: No such file or directory
	at java.io.UnixFileSystem.createFileExclusively(Native Method) ~[na:1.7.0_60]
	at java.io.File.createNewFile(File.java:1006) ~[na:1.7.0_60]
	at backtype.storm.util$touch.invoke(util.clj:519) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at backtype.storm.daemon.worker$fn__6535$exec_fn__1474__auto____6536.invoke(worker.clj:362) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:185) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at clojure.core$apply.invoke(core.clj:617) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$fn__6535$mk_worker__6591.doInvoke(worker.clj:354) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$_main.invoke(worker.clj:461) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
2014-09-10 10:16:01 b.s.util [ERROR] Halting process: (""Error on initialization"")
java.lang.RuntimeException: (""Error on initialization"")
	at backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:319) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$fn__6535$mk_worker__6591.doInvoke(worker.clj:354) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker$_main.invoke(worker.clj:461) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.applyToHelper(AFn.java:172) [clojure-1.5.1.jar:na]
	at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.5.1.jar:na]
	at backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
{noformat}

The worker tries to use the local directory ""storm-local-default"" defined in the configuration file ""storm.yaml"" instead of using the local directory ""storm-local-custom"" defined in the file ""storm-custom.yaml"" specified on the command line."
STORM-491,(Security) Security Enhancements Break Storm on Windows,"Some of the enhancements made under the security branch will break functionality of Storm in windows environments.

I'm fine with making support for security on Windows a separate effort/jira, but basic Storm functionality (i.e. with security turned off) should work under windows before we consider merging the security branch to master.

"
STORM-490,Build Broken Under Windows,Currently unit tests fail due to the addition of platform-dependent code (path related).
STORM-488,storm CLI tool reports zero exit code on error scenario,"The {{storm}} CLI tool incorrectly reports a zero exit code (""success"") when it should report a non-zero exit code (""error"") under the following condition.

h3. How to reproduce

{code}
$STORM_HOME/bin/storm unsupportedCommand
{code}

h3. Actual result (incorrect)

The command above prints the usage help of the {{storm}} CLI tool, which is ok.  However, the exit code is 0, indicating success.

{code}
$ /opt/storm/bin/storm unsupportedCommand
Unknown command: [storm unsupportedCommand]
Commands:
	activate
	classpath
	deactivate
	dev-zookeeper
	drpc
	help
	jar
	kill
	list
	localconfvalue
	logviewer
	nimbus
	rebalance
	remoteconfvalue
	repl
	shell
	supervisor
	ui
	version

Help:
	help
	help <command>

Documentation for the storm client can be found at http://storm.incubator.apache.org/documentation/Command-line-client.html

Configs can be overridden using one or more -c flags, e.g. ""storm list -c nimbus.host=nimbus.mycompany.com""

$ echo $?
0    << zero exit code, should be non-zero
{code}

The problem of the current behavior is that automated deployment tools such as Puppet or Ansible fail to work correctly because they assume a zero exit code indicates success.

As a concrete example, imagine you mistype the command to submit a topology jar file to a Storm cluster:

{code}
# Doh, forgot to put the ""jar"" command between `storm` and `mytopology.jar`!
$ /opt/storm/bin/storm mytopology.jar my.Class

# Correct would be:
#    $ /opt/storm/bin/storm jar mytopology.jar my.Class
{code}

In this example, even though the user mistyped the submit command in e.g. an Ansible script, the script would not be able to tell -- the storm CLI tool incorrectly reports that the (wrong) command completed successfully.

h3. Expected result (correct)

Running the storm CLI tool with an unsupported command should return a non-zero exit code to clearly indicate an error condition.

*The only remaining question would be:  What would be the actual, non-zero exit code?*

For example, running the storm CLI tool without any argument returns an exit code of 255:

{code}
$ /opt/storm/bin/storm
...snip...
$ echo $?
255
{code}

Unfortunately, my understanding is that we haven't defined the semantics of non-zero exit codes.  Their values are IMHO not deliberately chosen.

For instance, if you run the ""storm jar"" command these are (some of) its possible exit codes:

{code}
0: success
1:
  - topology of same name already exists
    `Exception in thread ""main"" java.lang.RuntimeException: Topology with name `mytopology` already exists on cluster`
  - class cannot be found (`storm jar storm-starter.jar storm.starter.IDoNotExistAtAll`)
    `Exception in thread ""main"" java.lang.NoClassDefFoundError: storm/starter/IDoNotExistAtAll`  (<<< logged to STDERR)
  - incorrect use of command line, e.g. wrong number of command line arguments
{code}

As you can see, we return an exit code of 1 for a variety of error conditions.  On the positive side, at least it's clear that there was an error.  On the negative side, we cannot tell different error conditions apart because the same exit code is used."
STORM-481,A specific trident topology pattern causes batches to never complete,"We've discovered that a very specific pattern of partitioning and state persists (that we happen to use) causes a trident topology to never commit batches.

We've boiled the pattern we use down to this:

{code}
        /->Op->\     /->Op->\
Spout ->-------->->->-------->-x-> Persist
                   \-> Persist
{code}

If there is a {{partitionBy}} at x, then the batches never complete. If the partitioning is not there, then the topology works fine.

This is reproduced by the attached java testcase. The test will hang (FeederBatchSpout hangs on acquiring a semaphore) if {{failTest}} is true"
STORM-479,(Security) Multi-tenant scheduler needs extra check for used slots,"When there is mismatch between supervisor heartbeats reported number for slots and actual slots used by the worker on the node, scheduler fails with IllegalArgumentException - causing failure to schedule. "
STORM-477,Incorrectly set JAVA_HOME is not detected,"If JAVA_HOME is incorrectly set in a user's environment when launching storm,   

it fails with an error message that is confusing to end users.

Traceback (most recent call last):
  File ""/home/y/bin/storm"", line 485, in <module>
    main()
  File ""/home/y/bin/storm"", line 482, in main
    (COMMANDS.get(COMMAND, unknown_command))(*ARGS)
  File ""/home/y/bin/storm"", line 225, in listtopos
    extrajars=[USER_CONF_DIR, STORM_DIR + ""/bin""])
  File ""/home/y/bin/storm"", line 153, in exec_storm_class
    ] + jvmopts + [klass] + list(args)
  File ""/home/y/bin/storm"", line 97, in confvalue
    p = sub.Popen(command, stdout=sub.PIPE)
  File ""/usr/lib64/python2.6/subprocess.py"", line 642, in __init__
    errread, errwrite)
  File ""/usr/lib64/python2.6/subprocess.py"", line 1234, in _execute_child
    raise child_exception

It would be nice if this were either detected and a proper error message printed, or if it warned and fell back to the java found in PATH."
STORM-476,Null message payloads in kafka will result in an NPE and a failing spout,Null message payloads in kafka will result in an NPE and a failing spout.
STORM-475,Storm UI pages do not use UTF-8,
STORM-473,Provide BASH script (storm.sh),To provide BASH script (storm.sh) as an alternative to existing Python script (storm).
STORM-472,Non-completeable tests should receive an error message showing which spout is incomplete,"When testing with a spout that is not-complete, an exception is raised saying ""Cannot complete topology unless every spout is a CompletableSpout (or mocked to be)"", which is descriptive, but it leaves it unclear which spout is at fault. 

I submitted a fix at https://github.com/apache/incubator-storm/pull/239"
STORM-471,When the worker process shoots itself call Runtime.halt,As per the discussion in STORM-461 we don't want the worker to send a signal to itself through kill.  Instead we should call Runtime.halt. 
STORM-470,DisruptorQueue catch blocks do not capture stack trace,"The catch blocks for many of the Exceptions in the DisruptorQueue.java file do not extract the stack trace for debugging with the result being that errors cannot readily be diagnosed. The stack trace output should become part of the subsequent Runtime exception text such that it can be used in the diagnosis of problems. As it is now, all that a person gets is the error message which depends highly on the quality of the error text that the code author wrote for the class that raised the error. In many cases, this can be poor."
STORM-469,Storm UI Last Error Detail Insufficient for debugging,The error text that is captured in the Storm UI is insufficient to debug the issue as it gets cut off and does not link to the actual error to allow a person to get more detail. This means that the debugging is unnecessarily blind. This prevents diagnosis of the error.
STORM-468,java.io.NotSerializableException should be explained,"The occurrence of the NotSerializableException and how to avoid it should be better documented and the error from the code should be expanded to include some human readable message because it took a lot of searching to find out that the spouts and bolts need to create their variables in the prepare method in order to avoid this problem.

The error text output could state that this is how to solve the problem."
STORM-467,storm jar executes non-existant or old jar files and classes,"When issuing the storm jar command, the command will launch with some cached version of the jar contents even if no jar file is now present at the location specified on the command line. This should instead cause an error so that a user is actually running what they think they are.

The second part of this is that some part of storm is caching topology classes so that when debugging errors, old code is executed instead of the new version of a class. I would argue that storm should attempt to destroy cached topology classes if presented with a new version or when an active topology is terminated. Again this is to avoid running versions of code that are not those which have been specified."
STORM-464,Simulated time advanced after test cluster exits causes intermitent test failures,"As part of STORM-200 a simulated time cluster was hanging on shutdown because time was not advancing while the cluster was shutting down.  A fix was put in for this to simulate time advancement in the background while the cluster was shut down, but the code did not wait for background process to finish.  This resulted in simulated time being advanced in the background for other tests.  It is a simple fix, just wait for the background process to stop before returning.

I'll put up a pull request shortly

```
diff --git a/storm-core/src/clj/backtype/storm/testing.clj b/storm-core/src/clj/backtype/storm/testing.clj
index 54f40e0..0e4b23c 100644
--- a/storm-core/src/clj/backtype/storm/testing.clj
+++ b/storm-core/src/clj/backtype/storm/testing.clj
@@ -235,10 +235,11 @@
          (log-error t# ""Error in cluster"")
          (throw t#))
        (finally
-         (let [keep-waiting?# (atom true)]
-           (future (while @keep-waiting?# (simulate-wait ~cluster-sym)))
+         (let [keep-waiting?# (atom true)
+               f# (future (while @keep-waiting?# (simulate-wait ~cluster-sym)))]
            (kill-local-storm-cluster ~cluster-sym)
-           (reset! keep-waiting?# false))))))
+           (reset! keep-waiting?# false)
+            @f#)))))
 
 (defmacro with-simulated-time-local-cluster
   [& args]
```"
STORM-463,lack of static helper registerMetricsConsumer for backtype.storm.Config,"In backtype.storm.Config there is the concept in design to use regular maps, while providing only helpers to operate on that map.
Most of the methods are using this concept by providing static versions.

For example, {{registerSerialization}} method.

{code}
public static void registerSerialization(Map conf, Class klass) {
    getRegisteredSerializations(conf).add(klass.getName());
}

public void registerSerialization(Class klass) {
    registerSerialization(this, klass);
}
{code}

However recently added metrics interface doesn't follow this approach.

Unfortunately, this breaks the concept of using HashMap as configuration container by user. And it is still only HashMap inside storm.

Also, not having static version of {{registerMetricsConsumer}}, while introducing metrics feature breaks the configuration code in clojure that previosly was just a map.
Without static helper this code should be instance of {{backtype.storm.Config}} class, and it is not possible to keep configuration as simple as in pseudocode example below in submitter call.

{code}
(let [topology-config (doto { TOPOLOGY-DEBUG false
                              TOPOLOGY-STATS-SAMPLE-RATE 0.01
                              TOPOLOGY-RECEIVER-BUFFER-SIZE 32
                              TOPOLOGY-TRANSFER-BUFFER-SIZE 4096
                              TOPOLOGY-EXECUTOR-RECEIVE-BUFFER-SIZE 2048 }
                        ;; using static helper to add serialization class to config.
                        storm.backtype.Config/registerSerialization EXMPLSerializer 
                                                                    serializer)]

  (StormSubmitter/submitTopology
    topology-name
    topology-config
    (mk-topology)))
{code}

Proposed solution to create a static version of `registerMetricsConsumer` they way it is done by all other {{backtype.storm.Config}} helpers.
"
STORM-460,CSRF vulnerability in storm UI," If you are already kerberized and visit a webpage that malliciosly posts this request
http://storm-ui-root/api/v1/topology/storm-6-1406915499/kill/30
it will end up killing a topology with id storm-6-1406915499. 

To reproduce the issue, open storm ui in secure or non secure mode once using any browser. You can then close the storm ui tab. Usig chrome https://chrome.google.com/webstore/detail/postman-rest-client/fdmmgilgnpjigdojojpjoooidkmcomcm or curl post this request 

http://<storm-ui-root>/api/v1/topology/<your topology id>/kill/30

It should end up killing the topology.

See http://en.wikipedia.org/wiki/Cross-site_request_forgery to understand CSRF vulenrability."
STORM-458,sample spout uses incorrect name when connecting bolt,
STORM-456,Storm UI: cannot navigate to topology page when name contains spaces,"1. Create a Java class that makes your topology
2. Submit the topology with a name that contains spaces: StormSubmitter.submitTopology(""I Dont Want to Use Underscores"", conf, builder.createTopology());
3. Submit the jar to storm: storm jar yourUberJar your.topology.class.name
4. Open Storm UI in your browser
5. Click the link for the submitted topology under ""Topology Summary""

Result: Page refreshes but does not show the topology page

Expected Result: clicking the link should take you to the topology screen like it does for topologies whose names do not contain spaces OR an error should be returned if a user submits a topology whose names contains spaces and this is not supported."
STORM-455,Error-level messages from ShellBolts should turn into ReportedFailedExceptions,"Now that support was added for logging levels in the multilang protocol, it would be really nice if those of us writing Bolts in other languages had the ability to raise {{ReportedFailedException}} so that errors show up in the Storm UI. 

I propose that any time an {{error}} level logging message is sent to a ShellBolt, it raises a {{ReportedFailedException}}."
STORM-454,Storm Rest UI shows window in milliseconds but should be seconds,"In the STORM-UI-REST-API.md doc file, the parameter for ""window"" appears to be in seconds, but the documentation says milliseconds."
STORM-453,Upgrade to curator 2.5.0,Upgrade to curator 2.5.0 (implicit upgrade zookeeper and netty) change will allow Cassandra driver integration in bolt/spout without dependencies shadowing or other tricks...
STORM-451,Latest storm does not build due to a pom.xml error in storm-hdfs pom.xml,"Do a Maven clean install to see the error

In order to reproduce run mvn clean install from root"
STORM-450,Netty can cause error on clean shutdown of worker,"We recently had an issue where a worker process was shutdown cleaning on 0.9.0.  The reason the worker shutdown cleanly is not the issue here, but it caused a cascading failure that made a connected worker shutdown too.  This is going to be even more problematic in newer versions of storm when we give the worker time to shutdown cleanly instead of just shooting it with a kill -9

Ideally the client should continue to try and reconnect, because the worker may have exited on its own and will be re-spawned shortly.  If it is rescheduled elsewhere the worker will eventually detect it and reroute things accordingly.  This is what happens already when the connection is just closed.  There really is no reason to have one side know when the other side is shutting down.  

{code}
2014-08-11 19:00:17 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: Client is being closed, and does not take requests any more
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:130) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:101) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.disruptor$consume_loop_STAR_$fn__1999.invoke(disruptor.clj:74) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.util$async_loop$fn__421.invoke(util.clj:400) ~[storm-core-0.9.0-wip21.jar:na]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
	at java.lang.Thread.run(Thread.java:722) [na:1.7.0_17]
Caused by: java.lang.RuntimeException: Client is being closed, and does not take requests any more
	at backtype.storm.messaging.netty.Client.send(Client.java:118) ~[storm-netty-0.9.0-wip21.jar:na]
	at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__4922$fn__4923.invoke(worker.clj:342) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__4922.invoke(worker.clj:331) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.disruptor$clojure_handler$reify__1986.onEvent(disruptor.clj:43) ~[storm-core-0.9.0-wip21.jar:na]
	at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:127) ~[storm-core-0.9.0-wip21.jar:na]
	... 6 common frames omitted
2014-08-11 19:00:17 b.s.util [INFO] Halting process: (""Async loop died!"")
{code}"
STORM-448,Apache Storm Nimbus getting dead,"Apache storm nimbus getting dead due to serialVersionUID mismatch issue.


Caused by: java.io.InvalidClassException: backtype.storm.daemon.common.SupervisorInfo; local class incompatible: stream classdesc serialVersionUID = -292327522856879
2176, local class serialVersionUID = -6599159272110777305
        at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at backtype.storm.utils.Utils.deserialize(Utils.java:55)
        ... 29 more
"
STORM-443,multilang log's loglevel protocol  can cause hang,"Storm now support logging level to multilang protocol spout and bolt. But the implementation is not compatible with old log protocol with no logging level.

With old topology who use old protocol, when they send log with no loglevel,  JsonSerializer's readShellMsg function will throw NPE at:
        if (command.equals(""log"")) {
            long logLevel = (Long)msg.get(""level"");  //throw NPE at here
            shellMsg.setLogLevel((int)logLevel);
        }
ShellBolt will catch the NPE, and call die(), and die() will get error info from sub process's error stream in _process.getProcessTerminationInfoString(), but the error stream have no data come,  it will hangs.
    private void die(Throwable exception) {
        String processInfo = _process.getProcessInfoString() + _process.getProcessTerminationInfoString();
        _exception = new RuntimeException(processInfo, exception);
    }

This PR will fix the problem of log level implementation to compatible with old log protocol. And the die problem should be solved by [~xiaokang] 's PR https://github.com/apache/incubator-storm/pull/46"
STORM-442,multilang ShellBolt/ShellSpout die() can be hang when Exception happened,"In ShellBolt,  the _readerThread read command from python/shell process, and handle like this:

 try {
        ShellMsg shellMsg = _process.readShellMsg();
        ...                
 } catch (InterruptedException e) {
 } catch (Throwable t) {
        die(t);
 }

And in the die function, getProcessTerminationInfoString will read getErrorsString() from processErrorStream.

 private void die(Throwable exception) {
 
         String processInfo = _process.getProcessInfoString() + _process.getProcessTerminationInfoString();
 
         _exception = new RuntimeException(processInfo, exception);
 
 }

so when ShellBolt got exception(for example, readShellMsg() throw NPE ) ,  but it is not an error from sub process,  then getProcessTerminationInfoString will be hang because processErrorStream have no data to read.

On the other hand, as [~xiaokang] says ShellBolt should fail fast on exception ( https://github.com/apache/incubator-storm/pull/46 ) , I think it is not a good idea to read error info from stream.
Because [~xiaokang] 's PR is based old version, so I will move his code to this PR, and modify some other place in ShellSpout."
STORM-440,NimbusClient throws NPE if Config.STORM_THRIFT_TRANSPORT_PLUGIN is not set,"We just upgraded from 0.8.2 to 0.9.2 and noticed that when constructing a NimbusClient if Config.STORM_THRIFT_TRANSPORT_PLUGIN is not specified then AuthUtils[1] throws a NPE.

[1] - https://github.com/bbaugher/incubator-storm/blob/master/storm-core/src/jvm/backtype/storm/security/auth/AuthUtils.java#L73-L74"
STORM-439,UI unable to view topologies with percent encoded values in id,"We recently upgraded to 0.9.2 from 0.8.2 or so. One of our topologies had a percent encoded value in the id 'destination%F5backfill' (or interpreted 'destination_backfill'). The UI is unable to view this topology giving us the error,

{code}
NotAliveException(msg:destination_backfill-2-1407261676)
at backtype.storm.generated.Nimbus$getTopologyInfo_result.read(Nimbus.java:11347)
	at org.apache.thrift7.TServiceClient.receiveBase(TServiceClient.java:78)
	at backtype.storm.generated.Nimbus$Client.recv_getTopologyInfo(Nimbus.java:491)
	at backtype.storm.generated.Nimbus$Client.getTopologyInfo(Nimbus.java:478)
	at backtype.storm.ui.core$topology_page.invoke(core.clj:587)
	at backtype.storm.ui.core$fn__8229.invoke(core.clj:796)
	at compojure.core$make_route$fn__3365.invoke(core.clj:93)
	at compojure.core$if_route$fn__3353.invoke(core.clj:39)
	at compojure.core$if_method$fn__3346.invoke(core.clj:24)
	at compojure.core$routing$fn__3371.invoke(core.clj:106)
	at clojure.core$some.invoke(core.clj:2443)
	at compojure.core$routing.doInvoke(core.clj:106)
	at clojure.lang.RestFn.applyTo(RestFn.java:139)
	at clojure.core$apply.invoke(core.clj:619)
	at compojure.core$routes$fn__3375.invoke(core.clj:111)
	at ring.middleware.reload$wrap_reload$fn__7540.invoke(reload.clj:14)
	at backtype.storm.ui.core$catch_errors$fn__8268.invoke(core.clj:858)
	at ring.middleware.keyword_params$wrap_keyword_params$fn__4029.invoke(keyword_params.clj:27)
	at ring.middleware.nested_params$wrap_nested_params$fn__4068.invoke(nested_params.clj:65)
	at ring.middleware.params$wrap_params$fn__4001.invoke(params.clj:55)
	at ring.middleware.multipart_params$wrap_multipart_params$fn__4096.invoke(multipart_params.clj:103)
	at ring.middleware.flash$wrap_flash$fn__4277.invoke(flash.clj:14)
	at ring.middleware.session$wrap_session$fn__4266.invoke(session.clj:43)
	at ring.middleware.cookies$wrap_cookies$fn__4197.invoke(cookies.clj:160)
	at ring.adapter.jetty$proxy_handler$fn__7179.invoke(jetty.clj:16)
	at ring.adapter.jetty.proxy$org.mortbay.jetty.handler.AbstractHandler$0.handle(Unknown Source)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}

So it seems the UI is translating the percent encoded value which causes this issue. We shouldn't be putting these values in our topologies id in the first place but thought I would log this to see if you wanted to try to limit what characters are allowed in a topology id."
STORM-437,multilang JsonSerializer does not enforce inputstream UTF-8 encoding,"On some machines UTF-8 gets corrupted over the multilang protocol. Analysis of the problem leads to JsonSerializer usage of InputStreamReader when reading from stdin.
InputStreamReader uses the JVM defaults, which is usually UTF-8 but not always. 

Temporary Workaround:
Edit storm/conf/storm.yaml and enforce the default JVM charset as follows:
worker.childopts: ""-Xmx768m -Dfile.encoding=UTF-8""

Required Fix in JsonSerializer:
Pass the string ""UTF-8"" to the InputStreamReader constructor as second argument.

Notes: 
The implementation already enforces UTF-8 when writing to stdout, so there is no other fix needed there.
python simplejson and ruby json gem use  UTF-8 as the default.
"
STORM-436,Clean up Clojure namespace declarations,"Some of the Clojure namespace declarations in the storm project are messy and non-idiomatic. https://github.com/apache/incubator-storm/blob/master/storm-core/src/clj/backtype/storm/ui/core.clj#L18-L38 is a good example of this.

There are a few things I'd like to improve:
1. Coalesce multiple use/require/import's into a single use/require/import.
2. Order imports use, require, import.
3. Optionally, replacing use with some mix of
    * [... :refer :all]
    * Referring just the vars that are used
    * Qualifying the namespace imports

Is there a reason why the namespaces were done in the way they have been? What would be the preferred way to do 3?"
STORM-435,Storm kafka needs better documentation,Storm ships with kafka spout implementation. However there is very little documentation around how to use these spouts and what is role of different components. We have noticed a number of users asking similar questions in storm-user forum. We need to update the README file so users can easily understand how to use these spouts and what are the different configuration options for them.
STORM-434,"Supervisor dies with ""[ERROR] Error when processing even""","I have a one topology running on my production cluster. This topology has run for some weeks without fails, but few days ago my supervisor died with this error:

tail /var/log/storm/supervisor.log
2014-07-27 23:15:26 b.s.event [ERROR] Error when processing event
java.lang.RuntimeException: java.io.EOFException
    at backtype.storm.utils.Utils.deserialize(Utils.java:86) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at backtype.storm.utils.LocalState.snapshot(LocalState.java:45) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at backtype.storm.utils.LocalState.get(LocalState.java:56) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at backtype.storm.daemon.supervisor$read_worker_heartbeat.invoke(supervisor.clj:77) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at backtype.storm.daemon.supervisor$read_worker_heartbeats$iter__4842__4846$fn__4847.invoke(supervisor.clj:90) ~[na:na]
    at clojure.lang.LazySeq.sval(LazySeq.java:42) ~[clojure-1.4.0.jar:na]
    at clojure.lang.LazySeq.seq(LazySeq.java:60) ~[clojure-1.4.0.jar:na]
    at clojure.lang.RT.seq(RT.java:473) ~[clojure-1.4.0.jar:na]
    at clojure.core$seq.invoke(core.clj:133) ~[clojure-1.4.0.jar:na]
    at clojure.core$dorun.invoke(core.clj:2725) ~[clojure-1.4.0.jar:na]
    at clojure.core$doall.invoke(core.clj:2741) ~[clojure-1.4.0.jar:na]
    at backtype.storm.daemon.supervisor$read_worker_heartbeats.invoke(supervisor.clj:89) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at backtype.storm.daemon.supervisor$read_allocated_workers.invoke(supervisor.clj:106) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:209) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    at clojure.lang.AFn.applyToHelper(AFn.java:161) [clojure-1.4.0.jar:na]
    at clojure.lang.AFn.applyTo(AFn.java:151) [clojure-1.4.0.jar:na]
    at clojure.core$apply.invoke(core.clj:603) ~[clojure-1.4.0.jar:na]
    at clojure.core$partial$fn__4070.doInvoke(core.clj:2343) ~[clojure-1.4.0.jar:na]
    at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.4.0.jar:na]
    at backtype.storm.event$event_manager$fn__2593.invoke(event.clj:39) ~[na:na]
    at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
    at java.lang.Thread.run(Unknown Source) [na:1.7.0_03]
Caused by: java.io.EOFException: null
    at java.io.ObjectInputStream$PeekInputStream.readFully(Unknown Source) ~[na:1.7.0_03]
    at java.io.ObjectInputStream$BlockDataInputStream.readShort(Unknown Source) ~[na:1.7.0_03]
    at java.io.ObjectInputStream.readStreamHeader(Unknown Source) ~[na:1.7.0_03]
    at java.io.ObjectInputStream.<init>(Unknown Source) ~[na:1.7.0_03]
    at backtype.storm.utils.Utils.deserialize(Utils.java:81) ~[storm-core-0.9.1-incubating-mmx2.jar:0.9.1-incubating-mmx2]
    ... 21 common frames omitted
2014-07-27 23:15:26 b.s.util [INFO] Halting process: (""Error when processing an event"")

He tried wake up but I died with the same error all time. 

I have fixed the problem when I delete my temporally storm directory ""/tmp/storm"" But the next day, I found the same problem again. I deleted the directory again and now the topology runs fine but I think the error ""[ERROR] Error when processing even"" isn't normal and I have decided report it."
STORM-432,Enable version-agnostic serialization of storm's metadata,"For STORM-376, we've enabled compression of storm's metadata it serializes with Utils#serialize and Utils#deserialize. To facilitate simpler upgrades, we should add functionality to handle deserializing data written by a different version than that running."
STORM-427,(Security) AutoTGT with HBase can expose JVM kerberos issue,"The oracle JVM with in all versions I have looked at has a bug where it is possible for the JVM to use a service ticket instead of a TGT when requesting  a service ticket from the KDC.

The way the JVM code works right now is that when it looks for the TGT to use to connect to the KDC it will iterate over the all of the KerberosTickets in the private credentials, but it will pull out and use the first ticket that is for the current client.  The private credentials set is actually backed by a linked list, so the order they are scanned is insertion order.  Because a TGT is going to be inserted before any service tickets in the common case all is fine, the issue only shows up when we insert in a new TGT after other still valid service tickets.

This also only shows up when you are talking to more then one service, like we do with hbase.  If it were talking to just one service then the java code would reuse the valid service ticket instead of trying to get a new service ticket.  I'll put up a pull request shortly."
STORM-426,Upgrade to Clojure 1.6,"It would be very nice if it was possible to use Clojure 1.6 with Storm.

I'm currently testing it at the moment, and the storm-core tests fail, but I'm struggling to narrow down exactly what the issue is.

With three runs of {{mvn install}} on storm-core, I get 3 different issues/exceptions.

* A FileNotFoundException (test1.log)
* The tests hang and never complete (test2.log)
* A report that 'There were some failures', but I struggle to find where exactly the failure is amongst all the log files. (test3.log)

I've attached the test logs in question.

For reference I also included master.log to show my results when running tests against master.

To determine the issues, I have run each of the tests individually from the REPL."
STORM-424,Issue on KafkaOffsetMetric(external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java)," if (latestTimeOffset == 0 || earliestTimeOffset == 0) {
     LOG.warn(""No data found in Kafka Partition "" + partition.getId());
     return null;
}
The earliestTimeOffset is 0 by default for a partition, and only after log.retention(segment be deleted), earliestTimeOffset will larger than 0.
So you will always see the warning, if you run storm-kafka on a new patition, even if there are much data in your partition"
STORM-423,Allow a SpoutConfig to specify to begin at the end/newest entry on the topic,"Based on this exchange 
http://mail-archives.apache.org/mod_mbox/incubator-storm-user/201407.mbox/%3C1406236756.15830.145373285.397958AA%40webmail.messagingengine.com%3E

It would be nice to be able to set some parameter in the SpoutConfig that would allow a user to specify that the spout should begin with the end/newest message on the topic.  In case there was historical information on a topic that we don't care about."
STORM-422,storm.cmd limits the number of arguments that can be passed to 'storm jar' on Windows,"storm.cmd only allows a fixed number of arguments to be passed to 'storm jar' on Windows.

This is resolved by pull request 201 (https://github.com/apache/incubator-storm/pull/201)"
STORM-421,Memoize local hostname lookup in executor,
STORM-420,Missing quotes in storm-starter python code,"https://github.com/apache/incubator-storm/blob/07a561aeec294bd92e15d155c5e1249c1bb0d5d5/examples/storm-starter/multilang/resources/storm.py#L90

Missing quotes cause a python run-time error."
STORM-418,using storm kafka spout doesn't scale when increasing topic partition,"We are running a topology of kafka spout + single bolt on a single supervisor machine.

Bolt functionality is limited to log parsed data from spout only.

When Trying to consume same type of data by different topics using varying topic partitions topology throughput remain same in order of 70000 msg/sec.

However, when multiple spout instances are used in same topology , we were able to increase topology throughput to more than 100,000 msg/sec.

 We have tried to increase kafka topic partition from 8 to 200, however topology throughput remains same for sinlge spout instance."
STORM-414,support logging level to multilang protocol spout and bolt,"@mahall create a PR: https://github.com/apache/incubator-storm/pull/24 , Added logging level to multilang protocol spout and bolt.  But he closed it with no reason.

@msukmanowsky create a PR before to apache: https://github.com/nathanmarz/storm/pull/626 , Allow ShellBolts to optionally specify the logging level. But he did not modify ShellSpout and storm.py.

This improvement add optional logging level to Multilang Protocol's log method. And add implementation  in python and ruby.
"
STORM-413,Investigate Tests using Simulated Time and Mocking,"Several issues with running storm's tests have arisen that are related to timing and race conditions.

Hang: STORM-200 (https://github.com/apache/incubator-storm/commit/573c42a64885dac9a6a0d4c69a754500b607a8f1)

Unexpected Exception: STORM-403

We should investigate tests that use simulated time while mocking out functions to ensure that bindings are properly set where they need to be."
STORM-410,(Security) add groups support to logviewer,"Once STORM-347 goes in we should look at allowing topologies to grant access to the UI and logs by group, not just by user.  This should probably involve adding in new configs for ui and logs groups.  Updating the code that does checks to also check if the user is a part of these groups, and updating the logviewer metadata file to include the groups allowed."
STORM-409,(Security) add groups authorization support to DRPC,"Once STORM-407 goes in DRPC should hopefully not be slowed down by checking for groups.  We should add in support for groups in the ACLs, mostly for those submitting requests, but support on the topology side would be good too."
STORM-408,Cross-Site Scripting security vulnerability,"There are Cross-Site Scripting security vulnerabilities in Apache Storm.

The risk is that it is possible to steal or manipulate customer session and cookies, which might be used to impersonate a legitimate user, allowing the hacker to view or alter user records, and to perform transactions as that user.

The reason is that sanitation of hazardous characters was not performed correctly on user input.
"
STORM-407,(Security) add caching to groups support,"Once STORM-347 goes in there are a number of other places that would be good to allow for groups support including the ui, logviewer, and specifically DRPC.  DRPC is called frequently enough that  we should have some simple caching to avoid slowing down the DRPC server.

We probably want something that does both positive and negative caching but with a reasonable timeout of about a min. "
STORM-406,Trident topologies getting stuck when using Netty transport (reproducible),"When using the new, default Netty transport, Trident topologies sometimes get stuck, while under ZeroMQ everything is working fine.

I can reliably reproduce this issue by killing a Storm worker on a running Trident topology. If the worker gets re-spawned on the same slot (port), the topology stops processing. But if the worker re-spawns on a different port, topology processing continues normally.

The Storm cluster configuration is pretty standard, there are two Supervisor nodes, one node has also Nimbus, UI and DRPC running on it. I have four slots per Supervisor, and run my test topology with setNumWorkers set to 8 so that it occupies all eight slots across the cluster. Killing a worker in this configuration will always re-spawn the worker on the same node and slot (port), thus causing the topology to stop processing. This is 100% reproducible on a few Storm clusters of mine, across multiple Storm versions (0.9.0.1, 0.9.1, 0.9.2).

I have reproduced this with multiple Trident topologies, the simplest of which is the TridentWordCount topology from storm-starter. I've just modified it a little to add an additional Trident filter to log the tuple throughput: https://github.com/dschiavu/storm-trident-stuck-topology

Non-transactional Trident topologies just silently stop processing, while transactional topologies continuously retry the batches and are re-emitted by the spout, however they never get processed by the next bolts in the chain so they time out."
STORM-405,Add kafka trident state so messages can be sent to kafka topics,Currently storm has a bolt for writing to kafka but we have no implementation of trident state. We need a trident state implementation that allows wrting tuples directly to kafka topics as part of trident topology. 
STORM-404,Worker on one machine crashes due to a failure of another worker on another machine,"I have two workers (one on each machine). The first worker(10.30.206.125) had a problem starting (could not find Nimbus host), however the second worker crashed too since it could not connect to the first worker.

This looks like a cascading failure, which seems like a bug.

{code}
2014-07-15 17:43:32 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [17]
2014-07-15 17:43:33 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [18]
2014-07-15 17:43:34 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [19]
2014-07-15 17:43:35 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [20]
2014-07-15 17:43:36 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [21]
2014-07-15 17:43:37 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [22]
2014-07-15 17:43:38 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [23]
2014-07-15 17:43:39 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [24]
2014-07-15 17:43:40 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [25]
2014-07-15 17:43:41 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [26]
2014-07-15 17:43:42 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [27]
2014-07-15 17:43:43 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [28]
2014-07-15 17:43:44 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [29]
2014-07-15 17:43:45 b.s.m.n.Client [INFO] Reconnect started for Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700... [30]
2014-07-15 17:43:46 b.s.m.n.Client [INFO] Closing Netty Client Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700
2014-07-15 17:43:46 b.s.m.n.Client [INFO] Waiting for pending batchs to be sent with Netty-Client-ip-10-30-206-125.ec2.internal/10.30.206.125:6700..., timeout: 600000ms, pendings: 0
2014-07-15 17:43:46 b.s.util [ERROR] Async loop died!
java.lang.RuntimeException: java.lang.RuntimeException: Client is being closed, and does not take requests any more
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:128) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:99) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$consume_loop_STAR_$fn__758.invoke(disruptor.clj:94) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.util$async_loop$fn__457.invoke(util.clj:431) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
at java.lang.Thread.run(Thread.java:745) [na:1.7.0_60]
Caused by: java.lang.RuntimeException: Client is being closed, and does not take requests any more
at backtype.storm.messaging.netty.Client.send(Client.java:194) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.TransferDrainer.send(TransferDrainer.java:54) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__5927$fn__5928.invoke(worker.clj:322) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.daemon.worker$mk_transfer_tuples_handler$fn__5927.invoke(worker.clj:320) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.disruptor$clojure_handler$reify__745.onEvent(disruptor.clj:58) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:125) ~[storm-core-0.9.2-incubating.jar:0.9.2-incubating]
... 6 common frames omitted
2014-07-15 17:43:46 b.s.util [INFO] Halting process: (""Async loop died!"")
{code}"
STORM-402,FileNotFoundException when using storm with apache tika,"I have started using storm with apache tika for text extraction and I encountered a FileNotFoundException emanating from the download-storm-code method in supervisor.clj when running in local mode (I am able to submit to a remote cluster that I created):

7887 [Thread-5] INFO  backtype.storm.daemon.supervisor - Downloading code for storm id LocalTopology-1-1405005513 from /tmp/0de4961a-8694-4646-bb3c-b8e2d49da288/nimbus/stormdist/LocalTopology-1-1405005513
7906 [Thread-5] INFO  backtype.storm.daemon.supervisor - Copying resources at jar:file:/home/milad/.m2/repository/edu/ucar/netcdf/4.2-min/netcdf-4.2-min.jar!/resources to /tmp/61a2365c-f99f-49d6-9bb3-93f84e237fd2/supervisor/stormdist/LocalTopology-1-1405005513/resources
7907 [Thread-5] ERROR backtype.storm.event - Error when processing event
java.io.FileNotFoundException: Source 'file:/home/milad/.m2/repository/edu/ucar/netcdf/4.2-min/netcdf-4.2-min.jar!/resources' does not exist
	at org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1368) ~[commons-io-2.4.jar:2.4]
	at org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1261) ~[commons-io-2.4.jar:2.4]
	at org.apache.commons.io.FileUtils.copyDirectory(FileUtils.java:1230) ~[commons-io-2.4.jar:2.4]
	at backtype.storm.daemon.supervisor$fn__6392.invoke(supervisor.clj:535) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.MultiFn.invoke(MultiFn.java:236) ~[clojure-1.5.1.jar:na]
	at backtype.storm.daemon.supervisor$mk_synchronize_supervisor$this__6286.invoke(supervisor.clj:327) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at backtype.storm.event$event_manager$fn__2391.invoke(event.clj:39) ~[storm-core-0.9.3-incubating-SNAPSHOT.jar:0.9.3-incubating-SNAPSHOT]
	at clojure.lang.AFn.run(AFn.java:24) [clojure-1.5.1.jar:na]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_55]
7920 [Thread-5] INFO  backtype.storm.util - Halting process: (""Error when processing an event"")

In this case, the download-storm-code method in supervisor.clj is trying to load resources from the URL:

jar:file:/home/milad/.m2/repository/edu/ucar/netcdf/4.2-min/netcdf-4.2-min.jar!/resources

The relevant code in download-storm-code seems to be:

  (log-message ""Copying resources at "" (str url) "" to "" target-dir)
  (FileUtils/copyDirectory (File. (.getFile url)) (File. target-dir))

Since the url is of the form: 
jar:file:/home/milad/.m2/repository/edu/ucar/netcdf/4.2-min/netcdf-4.2-min.jar!/resources

the getFile method simply returns:
file:/home/milad/.m2/repository/edu/ucar/netcdf/4.2-min/netcdf-4.2-min.jar!/resources

which is incorrect and leads to the exception."
STORM-399,Kafka Spout defaulting to latest offset when current offset is older then 100k,"Using storm and storm-kafka 0.9.2-incubating

In the storm kafka spout the default for maxOffsetBehind is 100000
see https://github.com/apache/incubator-storm/blob/v0.9.2-incubating/external/storm-kafka/src/jvm/storm/kafka/KafkaConfig.java#L38

This default is too low and causes the kafka spout to start from the latest offset instead of the last committed offset without warning.
see https://github.com/apache/incubator-storm/blob/v0.9.2-incubating/external/storm-kafka/src/jvm/storm/kafka/PartitionManager.java#L95

Producing the following log output from the storm worker processes

{code}
2014-07-09 18:02:15 s.k.PartitionManager [INFO] Read last commit
offset from zookeeper: 15266940; old topology_id:
ef3f1f89-f64c-4947-b6eb-0c7fb9adb9ea - new topology_id:
5747dba6-c947-4c4f-af4a-4f50a84817bf
2014-07-09 18:02:15 s.k.PartitionManager [INFO] Last commit offset
from zookeeper: 15266940
2014-07-09 18:02:15 s.k.PartitionManager [INFO] Commit offset 22092614
is more than 100000 behind, resetting to startOffsetTime=-2
2014-07-09 18:02:15 s.k.PartitionManager [INFO] Starting Kafka
prd-use1c-pr-08-kafka-kamq-0004:4 from offset 22092614
{code}

To fix this problem I ended up setting spout config in my topology like so

{code}
spoutConf.maxOffsetBehind = Long.MAX_VALUE;
{code}

Why would the kafka spout skip to the latest offset if the current offset
is more then 100000 behind by default?

This seems like a bad default value, the spout literally skipped over
months of data without any warning.

Are the core contributors open to accepting a pull request that would set
the default to Long.MAX_VALUE?
"
STORM-397,Nimbus does not reassign a topology when the supervisor dies,"We're running two topologies on a cluster with 3 supervisors. By default, both topologies are assigned onto the same supervisor. If that supervisor dies, storm reassigns one topology to another supervisor but not the other, leaving the second topology inactive.

There are various symptoms/possible causes of this problem. In the nimbus logs, from when the topologies are initially submitted, nimbus is continually trying to reassign the second topology to the same supervisor every 10 seconds:

{noformat}
2014-07-09 14:17:11 -: b.s.d.nimbus [INFO] Setting new assignment for topology id Sync-1-1404911509: #backtype.storm.daemon.common.Assignment{:master-code-dir ""/storm/nimbus/stormdist/Sync-1-1404911509"", :node->host {""9f5f2ddd-40ee-4ac1-b705-2957089af330"" ""sc-beta-r""}, :executor->node+port {[6 6] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [11 11] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [5 5] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [9 10] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [12 12] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [3 4] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [7 8] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [1 2] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703]}, :executor->start-time-secs {[1 2] 1404911831, [7 8] 1404911831, [3 4] 1404911831, [12 12] 1404911831, [9 10] 1404911831, [5 5] 1404911831, [11 11] 1404911831, [6 6] 1404911831}}
2014-07-09 14:17:21 -: b.s.d.nimbus [INFO] Setting new assignment for topology id Sync-1-1404911509: #backtype.storm.daemon.common.Assignment{:master-code-dir ""/storm/nimbus/stormdist/Sync-1-1404911509"", :node->host {""9f5f2ddd-40ee-4ac1-b705-2957089af330"" ""sc-beta-r""}, :executor->node+port {[6 6] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [9 10] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [5 5] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [11 11] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [12 12] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [3 4] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [7 8] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [1 2] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703]}, :executor->start-time-secs {[1 2] 1404911841, [7 8] 1404911841, [3 4] 1404911841, [12 12] 1404911841, [9 10] 1404911841, [5 5] 1404911841, [11 11] 1404911841, [6 6] 1404911841}}
2014-07-09 14:17:32 -: b.s.d.nimbus [INFO] Setting new assignment for topology id Sync-1-1404911509: #backtype.storm.daemon.common.Assignment{:master-code-dir ""/storm/nimbus/stormdist/Sync-1-1404911509"", :node->host {""9f5f2ddd-40ee-4ac1-b705-2957089af330"" ""sc-beta-r""}, :executor->node+port {[6 6] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [11 11] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [5 5] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [9 10] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [12 12] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [3 4] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [7 8] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703], [1 2] [""9f5f2ddd-40ee-4ac1-b705-2957089af330"" 6703]}, :executor->start-time-secs {[1 2] 1404911852, [7 8] 1404911852, [3 4] 1404911852, [12 12] 1404911852, [9 10] 1404911852, [5 5] 1404911852, [11 11] 1404911852, [6 6] 1404911852}}
{noformat}

These log messages continue after the supervisor it's running on dies - nimbus continually tries to reassign to a dead supervisor. Note that the other topology is reassigned elsewhere without problems.

If the broken topology is rebalanced, only then does nimbus assign the topology to a working supervisor.

Another symptom of this is that, when the machines running storm are started, only one topology is running on startup. The second topology is not assigned to a supervisor. Again, it takes a rebalance for nimbus to actually assign the topology somewhere.

A couple of possibly related bugs are STORM-256 and STORM-341, but I don't really understand those bugs enough to be able to link it to these problems.

This is a major issue for us. One of the reasons for using storm is that if a supervisor were to die, storm would automatically fail over to another supervisor. This does not happen, leaving our cluster with a SPOF."
STORM-396,NullPointerException in ShellSpout when multilang message is illegal,"In one occurrence our new nodejs multilang integration sent a well formed json object without the ""command"" property. The result was a NPE that is fixed in the accompanied pull request"
STORM-394,"Messages has expired, OFFSET_OUT_OF_RANGE, new offset startOffsetTime, no new messages, again and again","Issue created here (https://github.com/wurstmeister/storm-kafka-0.8-plus/issues/55) but closed since the module is maintened under the Storm umbrella now.

I think there might be a case that is not covered :
0) messages in Kafka has expired
1) so offset stored in Zookeeper are no longer valid
2) error OFFSET_OUT_OF_RANGE is thrown
3) getOffset with startOffsetTime
4) retry the fetch with the returned startOffset
5) get an ByteBufferMessageSet but empty

KafkaUtils.fetchMessages seeems to be called again and again with the old offset and we get to step 2 again.
I guess the new startOffset is not commited to Zookeeper since we do not have new messages.

This can happen in the case of a topology restart, so it goes through the TridentKafkaEmitter.reEmitPartitionBatch"
STORM-393,Add topic to KafkaSpout output,"It would be beneficial to have topic as a tuple value emitted from KafkaSpout.
Not only it is useful if STORM-392 is implemented, but also in case when we have more than one KafkaSpout in a system"
STORM-392,Implement WhiteList topics for KafkaSpout,"A ""default"" high-level kafka consumer has the ability to read from multiple topics using a WhiteList notation (which is basically just a regex).

It would be great to have that ability in KafkaSpout too."
STORM-391,KafkaSpout to await for the topic,"When topic does not yet exist and the consumer is asked to consume from it, the default behaviour for Kafka heigh-level consumer is to ""await"" for the topic without a failure.

KafkaSpout currently fails trying to get the partition information about the topic that does not exist.

It may be a good idea to have the same common behaviour in KafkaSpout and it can probably be implemented through the zookeeper watchers: if topic does not exist, then set up a watcher and don't do anything until it yields."
STORM-390,/lib contains two conflicting versions of netty,"From: https://storm.incubator.apache.org/downloads.html

0.9.2-incubating versions include within the /lib directory:

netty-3.2.2.Final.jar
netty-3.6.3.Final.jar

This caused me issues when using w/ Datastax Java Driver for Cassandra, the 3.2.2 version was being used by Storm and conflicting with the version of netty used by the driver.

Deleting 3.2.2 from the lib directory solved my problem."
STORM-389,Wrong default wait time when killing a topology from the UI,"When a topology is killed from the Storm UI, the wait time displayed in the confirm box is always the default value, 30 secs. The ""topology.message.timeout.secs"" setting is not used."
STORM-388,make supervisor more resilient to missing .ser files,"Currently supervisor process can not run without some kind of supervisor software like systemd. It exits too often on missing .ser file error with [INFO] Halting process

examples:

a)

2014-07-03 20:32:53 b.s.d.supervisor [INFO] Shutting down and clearing state for
 id efd37b78-eb69-46a1-b317-9b5b4ba00584. Current supervisor time: 1404412373. S
tate: :timed-out, Heartbeat: #backtype.storm.daemon.common.WorkerHeartbeat{:time
-secs 1404412311, :storm-id ""Storm-throughput-test-7-1404411531"", :executors #{[
2 2] [4 4] [6 6] [-1 -1]}, :port 6702}
2014-07-03 20:32:53 b.s.d.supervisor [INFO] Shutting down 55f2b426-c170-4e48-a76
8-2a82c0f383ce:efd37b78-eb69-46a1-b317-9b5b4ba00584
2014-07-03 20:32:54 b.s.d.supervisor [INFO] Removing code for storm id Storm-thr
oughput-test-7-1404411531
2014-07-03 20:32:55 b.s.d.supervisor [INFO] Shut down 55f2b426-c170-4e48-a768-2a
82c0f383ce:efd37b78-eb69-46a1-b317-9b5b4ba00584
2014-07-03 20:32:55 b.s.d.supervisor [INFO] Launching worker with assignment #ba
cktype.storm.daemon.supervisor.LocalAssignment{:storm-id ""Storm-throughput-test-
7-1404411531"", :executors ([6 6] [4 4] [2 2])} for this supervisor 55f2b426-c170
-4e48-a768-2a82c0f383ce on port 6702 with id 6518a348-1fea-4401-8b7b-365b4ac3627
9
2014-07-03 20:32:55 b.s.event [ERROR] Error when processing event
java.io.FileNotFoundException: File 'storm-local/supervisor/stormdist/Storm-thro
ughput-test-7-1404411531/stormconf.ser' does not exist

b)

2014-07-03 20:32:43 o.a.z.ClientCnxn [INFO] Socket connection established to localhost/127.0.0.1:2181, initiating session
2014-07-03 20:32:51 o.a.z.ClientCnxn [INFO] Unable to reconnect to ZooKeeper service, session 0x146fb27b8400027 has expired, closing socket connection
2014-07-03 20:32:51 o.a.c.f.s.ConnectionStateManager [INFO] State change: LOST
8d-1069-44e3-b3ca-c25390cbf719
2014-07-03 10:29:22 b.s.d.supervisor [INFO] Removing code for storm id Storm-throughput-test-1-140433
5149
2014-07-03 10:29:22 b.s.d.supervisor [INFO] Shut down 167cf900-2ec6-499b-9c09-12c1e48dbc08:f776588d-1
069-44e3-b3ca-c25390cbf719
2014-07-03 10:29:22 b.s.d.supervisor [INFO] Launching worker with assignment #backtype.storm.daemon.s
upervisor.LocalAssignment{:storm-id ""Storm-throughput-test-1-1404335149"", :executors ([3 3] [5 5] [4 
4] [2 2] [1 1])} for this supervisor 167cf900-2ec6-499b-9c09-12c1e48dbc08 on port 6702 with id 1dd28a
8e-53cd-4af3-a4ae-7ebae0b9427f
2014-07-03 10:29:22 b.s.event [ERROR] Error when processing event
java.io.FileNotFoundException: File 'storm-local/supervisor/stormdist/Storm-throughput-test-1-1404335
149/stormconf.ser' does not exist

in both cases there were problems with zookeeper connection event failure before missing .ser file error."
STORM-387,Memory leak in worker,"There is memory leak in worker. I can reproduce it every time with following code: 

https://github.com/hsn10/stormtest

worker running bolt 'rtt' only leaks memory. Deploy topology and leave it about 15 minutes running until worker is killed by supervisor due to heartbeat timeout. It timeouts because its busy running gc all the time as you can see in jconsole.

I was able to do memory dump, but due to its size jhat tool was not able to load it in reasonable time (i killed it after 30 minutes)"
STORM-384,Storm UI showing incorrect number of workers and executors,"Hi,

We are using Storm 0.9.2 and we have observed that Storm UI is showing incorrect number of workers and executors. Number of workers and number of executors values are swapped . 

We have set number of workers to 20 and number of executors is 42. However, Storm UI is displaying number of workers as 42 and number of executors as 20."
STORM-383,Test time-outs when running metrics_test/test-builtin-metrics-2,"https://github.com/apache/incubator-storm/pull/38#issuecomment-47627909

In the course of testing a metrics patch, we found that metrics_test/test-builtin-metrics-2 caused test time-outs."
STORM-382,"UI topology summary page display wrong order of tasksTotal, workersTotal and executorsTotal","Storm UI index page, topology summary section display wrong order of tasksTotal, workersTotal and executorsTotal.
The problem is also shown on topology page, topology summary section.

In ui topology summary page, it displays Num Workers, Num executors, Num tasks while the corresponding date it renders are tasksTotal, workersTotal, executorsTotal. The order is wrong."
STORM-381,Replace broken jquery.tablesorter.min.js to latest (2.17.3),"We're having issue on Storm 0.9.2-incubating, javascript error from Storm UI.
(It's already issued : STORM-379)

It's syntax error from jquery.tablesorter.min.js,
I'm not familiar with javascript, so I've just replaced it to latest version (2.17.3), and it works!

I don't have strong reason to use latest version, just resolve problem.
So reviewers have a reason or issue to not use latest version, surely it can be changed.

I'll make pull request to continue working on it."
STORM-379,Storm UI javascript error,"Storm UI javascript error in Topology Summary page

Uncaught SyntaxError: Unexpected token ; jquery.tablesorter.min.js:4
Uncaught TypeError: Cannot read property 'addParser' of undefined script.js:18
Uncaught TypeError: undefined is not a function topology.html?id=notifier-storm-2-1404199903:88"
STORM-378,"SleepSpoutWaitStrategy.emptyEmit should use  the variable ""streak""","{code:java}
Index: src/jvm/backtype/storm/spout/SleepSpoutWaitStrategy.java
===================================================================
--- src/jvm/backtype/storm/spout/SleepSpoutWaitStrategy.java	(revision 2868)
+++ src/jvm/backtype/storm/spout/SleepSpoutWaitStrategy.java	(working copy)
@@ -18,6 +18,8 @@
 package backtype.storm.spout;
 
 import backtype.storm.Config;
+import backtype.storm.utils.Utils;
+
 import java.util.Map;
 
 
@@ -27,13 +29,14 @@
     
     @Override
     public void prepare(Map conf) {
-        sleepMillis = ((Number) conf.get(Config.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS)).longValue();
+        sleepMillis = Utils.getLong(
+            conf.get(Config.TOPOLOGY_SLEEP_SPOUT_WAIT_STRATEGY_TIME_MS), 500);
     }
 
     @Override
     public void emptyEmit(long streak) {
         try {
-            Thread.sleep(sleepMillis);
+            Thread.sleep(Math.abs(sleepMillis + streak));
         } catch (InterruptedException e) {
             throw new RuntimeException(e);
         }
Index: src/jvm/backtype/storm/utils/Utils.java
===================================================================
--- src/jvm/backtype/storm/utils/Utils.java	(revision 2888)
+++ src/jvm/backtype/storm/utils/Utils.java	(working copy)
@@ -325,6 +325,24 @@
           throw new IllegalArgumentException(""Don't know how to convert "" + o + "" + to int"");
       }
     }
+    
+    public static Long getLong(Object o, long defaultValue) {
+
+      if (o == null) {
+        return defaultValue;
+      }
+
+      if (o instanceof String) {
+        return Long.valueOf(String.valueOf(o));
+      } else if (o instanceof Integer) {
+        Integer value = (Integer) o;
+        return Long.valueOf((Integer) value);
+      } else if (o instanceof Long) {
+        return (Long) o;
+      } else {
+        return defaultValue;
+      }
+    }
 
     public static boolean getBoolean(Object o, boolean defaultValue) {
       if (null == o) {
{code}
"
STORM-377,hs_err_pid.log,"worker died and i found some hs_err_pid.log per minute.
it's fine after i reinstall the zmq
thank you


#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f161193d1c0, pid=11021, tid=139733713794816
#
# JRE version: 6.0_45-b06
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.45-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libzmq.so.3+0x261c0]  zmq::signaler_t::signaler_t()+0x30
#
# If you would like to submit a bug report, please visit:
#   http://java.sun.com/webapps/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x00007f164400a800):  JavaThread ""main"" [_thread_in_native, id=11022, stack(0x00007f164a4fb000,0x00007f164a5fc000)]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x00000000000002f9

Registers:
RAX=0x0000000000000001, RBX=0x0000000000000001, RCX=0x0000000000000004, RDX=0x00007f164a5f9f7c
RSP=0x00007f164a5f9f38, RBP=0x0000000000000011, RSI=0x0000000000000011, RDI=0x0000000000000001
R8 =0x00007f164400b0f0, R9 =0x00000000a328f240, R10=0x00007f164a5f9cf0, R11=0x00007f16119529c0
R12=0x00007f164a5f9f7c, R13=0x0000000000000004, R14=0x00007f164a5fa040, R15=0x00007f164400a800
RIP=0x00007f161193d1c0, EFLAGS=0x0000000000010202, CSGSFS=0xffff000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f164a5f9f38)
0x00007f164a5f9f38:   00007f16119529ed 0000000000000001
0x00007f164a5f9f48:   0000000000000011 00007f164400a9d0
0x00007f164a5f9f58:   0000000000001388 00000000fc7e6a50
0x00007f164a5f9f68:   00007f1611b688f9 0000000000000000
0x00007f164a5f9f78:   00001388fc7e6a50 00000000fc7e6a50
0x00007f164a5f9f88:   00007f164a5fa008 0000000000000000
0x00007f164a5f9f98:   00007f1641010eee 00007f164a5fa088
0x00007f164a5f9fa8:   00007f164a5fa028 00007f16440092b0
0x00007f164a5f9fb8:   00007f1644009688 00007f164400a800
0x00007f164a5f9fc8:   00007f164a5f9fc8 00000000fc7e6a50
0x00007f164a5f9fd8:   00007f164a5fa040 00000000fc7e7170
0x00007f164a5f9fe8:   0000000000000000 00000000fc7e6a50
0x00007f164a5f9ff8:   0000000000000000 00007f164a5fa028
0x00007f164a5fa008:   00007f164a5fa088 00007f1641005a82
0x00007f164a5fa018:   0000000000000000 00007f164100df58
0x00007f164a5fa028:   0000000000001388 0000000000000001
0x00007f164a5fa038:   0000000000000011 00000000a328f240
0x00007f164a5fa048:   00007f164a5fa048 00000000fc7e5109
0x00007f164a5fa058:   00007f164a5fa0a8 00000000fc7e7170
0x00007f164a5fa068:   0000000000000000 00000000fc7e5118
0x00007f164a5fa078:   00007f164a5fa028 00007f164a5fa098
0x00007f164a5fa088:   00007f164a5fa0f0 00007f1641005a82
0x00007f164a5fa098:   0000000000001388 00000000a0e2bf40
0x00007f164a5fa0a8:   00000000a328f240 00007f164a5fa0b0
0x00007f164a5fa0b8:   00000000fc7eccfe 00007f164a5fa118
0x00007f164a5fa0c8:   00000000fc7ed2c8 0000000000000000
0x00007f164a5fa0d8:   00000000fc7ecd40 00007f164a5fa098
0x00007f164a5fa0e8:   00007f164a5fa108 00007f164a5fa170
0x00007f164a5fa0f8:   00007f1641005e03 00000000a328f240
0x00007f164a5fa108:   0000000000000000 0000000000000000
0x00007f164a5fa118:   00000000a2dce6e8 00000000a2dd3548
0x00007f164a5fa128:   00000000a2dfbcd0 00007f164a5fa130 

Instructions: (pc=0x00007f161193d1c0)
0x00007f161193d1a0:   85 c0 75 0f 8b 3b e8 c5 e8 fe ff 8b 7b 04 e8 bd
0x00007f161193d1b0:   e8 fe ff e8 68 54 fe ff 89 43 08 5b c3 90 90 90
0x00007f161193d1c0:   81 bf f8 02 00 00 af ec dd ba 0f 94 c0 c3 66 90
0x00007f161193d1d0:   48 8d 87 00 03 00 00 c3 0f 1f 84 00 00 00 00 00 

Register to memory mapping:

RAX=0x0000000000000001 is an unknown value
RBX=0x0000000000000001 is an unknown value
RCX=0x0000000000000004 is an unknown value
RDX=0x00007f164a5f9f7c is pointing into the stack for thread: 0x00007f164400a800
RSP=0x00007f164a5f9f38 is pointing into the stack for thread: 0x00007f164400a800
RBP=0x0000000000000011 is an unknown value
RSI=0x0000000000000011 is an unknown value
RDI=0x0000000000000001 is an unknown value
R8 =0x00007f164400b0f0 is an unknown value
R9 =
[error occurred during error reporting (printing register info), id 0xb]

Stack: [0x00007f164a4fb000,0x00007f164a5fc000],  sp=0x00007f164a5f9f38,  free space=1019k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  [libzmq.so.3+0x261c0]  zmq::signaler_t::signaler_t()+0x30

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  org.zeromq.ZMQ$Socket.setLongSockopt(IJ)V+0
j  org.zeromq.ZMQ$Socket.setLinger(J)V+17
j  zilch.mq$set_linger.invoke(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+14
j  backtype.storm.messaging.zmq.ZMQContext.connect(Ljava/lang/String;Ljava/lang/String;I)Lbacktype/storm/messaging/IConnection;+91
j  backtype.storm.daemon.worker$mk_refresh_connections$this__5827$iter__5834__5838$fn__5839.invoke()Ljava/lang/Object;+474
J  clojure.lang.RT.seq(Ljava/lang/Object;)Lclojure/lang/ISeq;
J  clojure.core$seq.invoke(Ljava/lang/Object;)Ljava/lang/Object;
j  clojure.core$dorun.invoke(Ljava/lang/Object;)Ljava/lang/Object;+10
j  clojure.core$doall.invoke(Ljava/lang/Object;)Ljava/lang/Object;+10
j  backtype.storm.daemon.worker$mk_refresh_connections$this__5827.invoke(Ljava/lang/Object;)Ljava/lang/Object;+512
j  backtype.storm.daemon.worker$fn__5882$exec_fn__1229__auto____5883.invoke(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+620
j  clojure.lang.AFn.applyToHelper(Lclojure/lang/IFn;Lclojure/lang/ISeq;)Ljava/lang/Object;+416
j  clojure.lang.AFn.applyTo(Lclojure/lang/ISeq;)Ljava/lang/Object;+8
j  clojure.core$apply.invoke(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+26
j  backtype.storm.daemon.worker$fn__5882$mk_worker__5938.doInvoke(Ljava/lang/Object;)Ljava/lang/Object;+16
j  clojure.lang.RestFn.invoke(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+123
j  backtype.storm.daemon.worker$_main.invoke(Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;+77
j  clojure.lang.AFn.applyToHelper(Lclojure/lang/IFn;Lclojure/lang/ISeq;)Ljava/lang/Object;+261
j  clojure.lang.AFn.applyTo(Lclojure/lang/ISeq;)Ljava/lang/Object;+8
j  backtype.storm.daemon.worker.main([Ljava/lang/String;)V+29
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( => current thread )
  0x00007f1644bf6000 JavaThread ""Thread-4"" daemon [_thread_blocked, id=11049, stack(0x00007f1611513000,0x00007f1611614000)]
  0x00007f1644ddd000 JavaThread ""Thread-3"" daemon [_thread_blocked, id=11048, stack(0x00007f1611614000,0x00007f1611715000)]
  0x00007f1644ddc000 JavaThread ""Thread-2"" daemon [_thread_blocked, id=11047, stack(0x00007f1611715000,0x00007f1611816000)]
  0x00007f164509b800 JavaThread ""Thread-1"" daemon [_thread_blocked, id=11046, stack(0x00007f1611816000,0x00007f1611917000)]
  0x00007f1644cc4800 JavaThread ""Thread-0"" daemon [_thread_blocked, id=11045, stack(0x00007f1630055000,0x00007f1630156000)]
  0x00007f1644aca800 JavaThread ""CuratorFramework-1"" [_thread_blocked, id=11044, stack(0x00007f1611d6b000,0x00007f1611e6c000)]
  0x00007f16450b5800 JavaThread ""ConnectionStateManager-0"" [_thread_blocked, id=11043, stack(0x00007f1611f6d000,0x00007f161206e000)]
  0x00007f1644f71000 JavaThread ""main-EventThread"" daemon [_thread_blocked, id=11042, stack(0x00007f161206e000,0x00007f161216f000)]
  0x00007f164509f800 JavaThread ""main-SendThread(s254:2181)"" daemon [_thread_in_native, id=11041, stack(0x00007f1611e6c000,0x00007f1611f6d000)]
  0x00007f164412d800 JavaThread ""Low Memory Detector"" daemon [_thread_blocked, id=11035, stack(0x00007f164072c000,0x00007f164082d000)]
  0x00007f164412b000 JavaThread ""C2 CompilerThread1"" daemon [_thread_blocked, id=11034, stack(0x00007f164082d000,0x00007f164092e000)]
  0x00007f1644128800 JavaThread ""C2 CompilerThread0"" daemon [_thread_blocked, id=11033, stack(0x00007f164092e000,0x00007f1640a2f000)]
  0x00007f1644126800 JavaThread ""Signal Dispatcher"" daemon [_thread_blocked, id=11032, stack(0x00007f1640a2f000,0x00007f1640b30000)]
  0x00007f1644124800 JavaThread ""Surrogate Locker Thread (Concurrent GC)"" daemon [_thread_blocked, id=11031, stack(0x00007f1640b30000,0x00007f1640c31000)]
  0x00007f16440fc000 JavaThread ""Finalizer"" daemon [_thread_blocked, id=11030, stack(0x00007f1640c31000,0x00007f1640d32000)]
  0x00007f16440fa000 JavaThread ""Reference Handler"" daemon [_thread_blocked, id=11029, stack(0x00007f1648052000,0x00007f1648153000)]
=>0x00007f164400a800 JavaThread ""main"" [_thread_in_native, id=11022, stack(0x00007f164a4fb000,0x00007f164a5fc000)]

Other Threads:
  0x00007f16440f3000 VMThread [stack: 0x00007f1640d32000,0x00007f1640e33000] [id=11028]
  0x00007f1644138800 WatcherThread [stack: 0x00007f164062b000,0x00007f164072c000] [id=11036]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap
 par new generation   total 290304K, used 143655K [0x000000009ae00000, 0x00000000b2800000, 0x00000000b2800000)
  eden space 193536K,  74% used [0x000000009ae00000, 0x00000000a3a49ca0, 0x00000000a6b00000)
  from space 96768K,   0% used [0x00000000a6b00000, 0x00000000a6b00000, 0x00000000ac980000)
  to   space 96768K,   0% used [0x00000000ac980000, 0x00000000ac980000, 0x00000000b2800000)
 concurrent mark-sweep generation total 1185792K, used 0K [0x00000000b2800000, 0x00000000fae00000, 0x00000000fae00000)
 concurrent-mark-sweep perm gen total 27392K, used 27246K [0x00000000fae00000, 0x00000000fc8c0000, 0x0000000100000000)

Code Cache  [0x00007f1641000000, 0x00007f1641270000, 0x00007f1644000000)
 total_blobs=586 nmethods=291 adapters=249 free_code_cache=48638080 largest_free_block=12800

Dynamic libraries:
40000000-40009000 r-xp 00000000 08:03 15860968                           /usr/java/jdk1.6.0_45/bin/java
40108000-4010a000 rwxp 00008000 08:03 15860968                           /usr/java/jdk1.6.0_45/bin/java
40bf1000-40c12000 rwxp 00000000 00:00 0                                  [heap]
9ae00000-fc8c0000 rwxp 00000000 00:00 0 
fc8c0000-100000000 rwxp 00000000 00:00 0 
31f4000000-31f4020000 r-xp 00000000 08:03 13631490                       /lib64/ld-2.12.so
31f421f000-31f4220000 r-xp 0001f000 08:03 13631490                       /lib64/ld-2.12.so
31f4220000-31f4221000 rwxp 00020000 08:03 13631490                       /lib64/ld-2.12.so
31f4221000-31f4222000 rwxp 00000000 00:00 0 
31f4400000-31f4402000 r-xp 00000000 08:03 13631510                       /lib64/libdl-2.12.so
31f4402000-31f4602000 ---p 00002000 08:03 13631510                       /lib64/libdl-2.12.so
31f4602000-31f4603000 r-xp 00002000 08:03 13631510                       /lib64/libdl-2.12.so
31f4603000-31f4604000 rwxp 00003000 08:03 13631510                       /lib64/libdl-2.12.so
31f4800000-31f498b000 r-xp 00000000 08:03 13631495                       /lib64/libc-2.12.so
31f498b000-31f4b8a000 ---p 0018b000 08:03 13631495                       /lib64/libc-2.12.so
31f4b8a000-31f4b8e000 r-xp 0018a000 08:03 13631495                       /lib64/libc-2.12.so
31f4b8e000-31f4b8f000 rwxp 0018e000 08:03 13631495                       /lib64/libc-2.12.so
31f4b8f000-31f4b94000 rwxp 00000000 00:00 0 
31f4c00000-31f4c17000 r-xp 00000000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4c17000-31f4e17000 ---p 00017000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4e17000-31f4e18000 r-xp 00017000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4e18000-31f4e19000 rwxp 00018000 08:03 13631534                       /lib64/libpthread-2.12.so
31f4e19000-31f4e1d000 rwxp 00000000 00:00 0 
31f5000000-31f5083000 r-xp 00000000 08:03 13631502                       /lib64/libm-2.12.so
31f5083000-31f5282000 ---p 00083000 08:03 13631502                       /lib64/libm-2.12.so
31f5282000-31f5283000 r-xp 00082000 08:03 13631502                       /lib64/libm-2.12.so
31f5283000-31f5284000 rwxp 00083000 08:03 13631502                       /lib64/libm-2.12.so
31f5400000-31f5407000 r-xp 00000000 08:03 13631919                       /lib64/librt-2.12.so
31f5407000-31f5606000 ---p 00007000 08:03 13631919                       /lib64/librt-2.12.so
31f5606000-31f5607000 r-xp 00006000 08:03 13631919                       /lib64/librt-2.12.so
31f5607000-31f5608000 rwxp 00007000 08:03 13631919                       /lib64/librt-2.12.so
3201800000-3201816000 r-xp 00000000 08:03 13631922                       /lib64/libgcc_s-4.4.7-20120601.so.1
3201816000-3201a15000 ---p 00016000 08:03 13631922                       /lib64/libgcc_s-4.4.7-20120601.so.1
3201a15000-3201a16000 rwxp 00015000 08:03 13631922                       /lib64/libgcc_s-4.4.7-20120601.so.1
3202400000-32024e8000 r-xp 00000000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32024e8000-32026e8000 ---p 000e8000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32026e8000-32026ef000 r-xp 000e8000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32026ef000-32026f1000 rwxp 000ef000 08:03 15731598                       /usr/lib64/libstdc++.so.6.0.13
32026f1000-3202706000 rwxp 00000000 00:00 0 
3204c00000-3204c16000 r-xp 00000000 08:03 13631635                       /lib64/libnsl-2.12.so
3204c16000-3204e15000 ---p 00016000 08:03 13631635                       /lib64/libnsl-2.12.so
3204e15000-3204e16000 r-xp 00015000 08:03 13631635                       /lib64/libnsl-2.12.so
3204e16000-3204e17000 rwxp 00016000 08:03 13631635                       /lib64/libnsl-2.12.so
3204e17000-3204e19000 rwxp 00000000 00:00 0 
7f15d4000000-7f15d4021000 rwxp 00000000 00:00 0 
7f15d4021000-7f15d8000000 ---p 00000000 00:00 0 
7f15dc000000-7f15dc021000 rwxp 00000000 00:00 0 
7f15dc021000-7f15e0000000 ---p 00000000 00:00 0 
7f15e0000000-7f15e0021000 rwxp 00000000 00:00 0 
7f15e0021000-7f15e4000000 ---p 00000000 00:00 0 
7f15e4000000-7f15e4021000 rwxp 00000000 00:00 0 
7f15e4021000-7f15e8000000 ---p 00000000 00:00 0 
7f15e8000000-7f15e8021000 rwxp 00000000 00:00 0 
7f15e8021000-7f15ec000000 ---p 00000000 00:00 0 
7f15ec000000-7f15ec021000 rwxp 00000000 00:00 0 
7f15ec021000-7f15f0000000 ---p 00000000 00:00 0 
7f15f0000000-7f15f0021000 rwxp 00000000 00:00 0 
7f15f0021000-7f15f4000000 ---p 00000000 00:00 0 
7f15f4000000-7f15f4021000 rwxp 00000000 00:00 0 
7f15f4021000-7f15f8000000 ---p 00000000 00:00 0 
7f15f8000000-7f15f8021000 rwxp 00000000 00:00 0 
7f15f8021000-7f15fc000000 ---p 00000000 00:00 0 
7f15fc000000-7f15fc021000 rwxp 00000000 00:00 0 
7f15fc021000-7f1600000000 ---p 00000000 00:00 0 
7f1600000000-7f1600021000 rwxp 00000000 00:00 0 
7f1600021000-7f1604000000 ---p 00000000 00:00 0 
7f1604000000-7f1605508000 rwxp 00000000 00:00 0 
7f1605508000-7f1608000000 ---p 00000000 00:00 0 
7f1608000000-7f16094a0000 rwxp 00000000 00:00 0 
7f16094a0000-7f160c000000 ---p 00000000 00:00 0 
7f160c000000-7f160c021000 rwxp 00000000 00:00 0 
7f160c021000-7f1610000000 ---p 00000000 00:00 0 
7f1611513000-7f1611516000 ---p 00000000 00:00 0 
7f1611516000-7f1611614000 rwxp 00000000 00:00 0 
7f1611614000-7f1611617000 ---p 00000000 00:00 0 
7f1611617000-7f1611715000 rwxp 00000000 00:00 0 
7f1611715000-7f1611718000 ---p 00000000 00:00 0 
7f1611718000-7f1611816000 rwxp 00000000 00:00 0 
7f1611816000-7f1611819000 ---p 00000000 00:00 0 
7f1611819000-7f1611917000 rwxp 00000000 00:00 0 
7f1611917000-7f1611962000 r-xp 00000000 08:03 15744848                   /usr/local/lib/libzmq.so.3.1.0
7f1611962000-7f1611b61000 ---p 0004b000 08:03 15744848                   /usr/local/lib/libzmq.so.3.1.0
7f1611b61000-7f1611b65000 rwxp 0004a000 08:03 15744848                   /usr/local/lib/libzmq.so.3.1.0
7f1611b65000-7f1611b6b000 r-xp 00000000 08:03 15744808                   /usr/local/lib/libjzmq.so.0.0.0
7f1611b6b000-7f1611d6a000 ---p 00006000 08:03 15744808                   /usr/local/lib/libjzmq.so.0.0.0
7f1611d6a000-7f1611d6b000 rwxp 00005000 08:03 15744808                   /usr/local/lib/libjzmq.so.0.0.0
7f1611d6b000-7f1611d6e000 ---p 00000000 00:00 0 
7f1611d6e000-7f1611e6c000 rwxp 00000000 00:00 0 
7f1611e6c000-7f1611e6f000 ---p 00000000 00:00 0 
7f1611e6f000-7f1611f6d000 rwxp 00000000 00:00 0 
7f1611f6d000-7f1611f70000 ---p 00000000 00:00 0 
7f1611f70000-7f161206e000 rwxp 00000000 00:00 0 
7f161206e000-7f1612071000 ---p 00000000 00:00 0 
7f1612071000-7f161216f000 rwxp 00000000 00:00 0 
7f161216f000-7f1618000000 r-xp 00000000 08:03 15738011                   /usr/lib/locale/locale-archive
7f1618000000-7f1618021000 rwxp 00000000 00:00 0 
7f1618021000-7f161c000000 ---p 00000000 00:00 0 
7f161c000000-7f161c021000 rwxp 00000000 00:00 0 
7f161c021000-7f1620000000 ---p 00000000 00:00 0 
7f1620000000-7f1620021000 rwxp 00000000 00:00 0 
7f1620021000-7f1624000000 ---p 00000000 00:00 0 
7f1624000000-7f1624021000 rwxp 00000000 00:00 0 
7f1624021000-7f1628000000 ---p 00000000 00:00 0 
7f1628000000-7f1628021000 rwxp 00000000 00:00 0 
7f1628021000-7f162c000000 ---p 00000000 00:00 0 
7f162c000000-7f162c021000 rwxp 00000000 00:00 0 
7f162c021000-7f1630000000 ---p 00000000 00:00 0 
7f1630055000-7f1630058000 ---p 00000000 00:00 0 
7f1630058000-7f1630156000 rwxp 00000000 00:00 0 
7f1630156000-7f163015c000 r-xp 00000000 08:03 16256555                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libmanagement.so
7f163015c000-7f163025b000 ---p 00006000 08:03 16256555                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libmanagement.so
7f163025b000-7f163025d000 rwxp 00005000 08:03 16256555                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libmanagement.so
7f163025d000-7f1630b85000 rwxp 00000000 00:00 0 
7f1630b85000-7f1630b98000 r-xp 00000000 08:03 16256559                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnet.so
7f1630b98000-7f1630c99000 ---p 00013000 08:03 16256559                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnet.so
7f1630c99000-7f1630c9c000 rwxp 00014000 08:03 16256559                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnet.so
7f1630c9c000-7f1630c9f000 r-xs 000cc000 08:03 16525449                   /usr/java/jdk1.6.0_45/jre/lib/ext/localedata.jar
7f1630c9f000-7f1634000000 rwxp 00000000 00:00 0 
7f1634000000-7f1634021000 rwxp 00000000 00:00 0 
7f1634021000-7f1638000000 ---p 00000000 00:00 0 
7f1638000000-7f1638021000 rwxp 00000000 00:00 0 
7f1638021000-7f163c000000 ---p 00000000 00:00 0 
7f163c000000-7f163c021000 rwxp 00000000 00:00 0 
7f163c021000-7f1640000000 ---p 00000000 00:00 0 
7f164003e000-7f1640045000 r-xp 00000000 08:03 16256560                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnio.so
7f1640045000-7f1640144000 ---p 00007000 08:03 16256560                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnio.so
7f1640144000-7f1640146000 rwxp 00006000 08:03 16256560                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libnio.so
7f1640146000-7f164014b000 r-xs 00029000 08:03 14032062                   /app/storm-supervisor/data/supervisor/stormdist/oracle-13-1403837712/stormjar.jar
7f164014b000-7f1640152000 r-xs 0004e000 08:03 15204423                   /app/storm-supervisor/lib-ext/mongo-java-driver-2.8.0.jar
7f1640152000-7f1640158000 r-xs 00031000 08:03 15204404                   /app/storm-supervisor/lib-ext/jaxen-1.1.3.jar
7f1640158000-7f164015c000 r-xs 00021000 08:03 15204408                   /app/storm-supervisor/lib-ext/jersey-json-1.9.jar
7f164015c000-7f164015e000 r-xs 00006000 08:03 15204393                   /app/storm-supervisor/lib-ext/htrace-core-2.01.jar
7f164015e000-7f1640162000 r-xs 00014000 08:03 15204430                   /app/storm-supervisor/lib-ext/xz-1.0.jar
7f1640162000-7f1640164000 r-xs 00000000 08:03 15204355                   /app/storm-supervisor/lib-ext/aopalliance-1.0.jar
7f1640164000-7f1640165000 r-xs 00007000 08:03 15204397                   /app/storm-supervisor/lib-ext/jackson-xc-1.8.3.jar
7f1640165000-7f1640177000 r-xs 0012c000 08:03 15204389                   /app/storm-supervisor/lib-ext/hadoop-yarn-common-2.2.0.jar
7f1640177000-7f16401f5000 r-xs 007f5000 08:03 15204426                   /app/storm-supervisor/lib-ext/scala-library-2.9.2.jar
7f16401f5000-7f1640228000 r-xs 0024a000 08:03 15204417                   /app/storm-supervisor/lib-ext/kafka_2.9.2-0.8.0.jar
7f1640228000-7f164022d000 r-xs 0002a000 08:03 15204361                   /app/storm-supervisor/lib-ext/commons-beanutils-1.7.0.jar
7f164022d000-7f164022e000 r-xs 00003000 08:03 15204431                   /app/storm-supervisor/lib-ext/xmlenc-0.52.jar
7f164022e000-7f1640231000 r-xs 00018000 08:03 15204357                   /app/storm-supervisor/lib-ext/bson-2.9.3.jar
7f1640231000-7f1640243000 r-xs 0009d000 08:03 15204409                   /app/storm-supervisor/lib-ext/jersey-server-1.9.jar
7f1640243000-7f164024f000 r-xs 000be000 08:03 15204390                   /app/storm-supervisor/lib-ext/hbase-client-0.96.1-hadoop2.jar
7f164024f000-7f1640253000 r-xs 00020000 08:03 15204368                   /app/storm-supervisor/lib-ext/commons-digester-1.8.jar
7f1640253000-7f164025a000 r-xs 00039000 08:03 15204364                   /app/storm-supervisor/lib-ext/commons-codec-1.7.jar
7f164025a000-7f164025f000 r-xs 0004a000 08:03 15204410                   /app/storm-supervisor/lib-ext/jets3t-0.6.1.jar
7f164025f000-7f164026f000 r-xs 00081000 08:03 15204419                   /app/storm-supervisor/lib-ext/mchange-commons-java-0.2.5.jar
7f164026f000-7f1640276000 r-xs 00068000 08:03 15204358                   /app/storm-supervisor/lib-ext/c3p0-0.9.5-pre2.jar
7f1640276000-7f164027a000 r-xs 0002f000 08:03 15204362                   /app/storm-supervisor/lib-ext/commons-beanutils-core-1.8.0.jar
7f164027a000-7f1640280000 r-xs 00044000 08:03 15204405                   /app/storm-supervisor/lib-ext/jedis-2.4.2.jar
7f1640280000-7f1640281000 r-xs 00004000 08:03 15204395                   /app/storm-supervisor/lib-ext/jackson-jaxrs-1.8.3.jar
7f1640281000-7f1640286000 r-xs 0005f000 08:03 15204398                   /app/storm-supervisor/lib-ext/jasper-compiler-5.5.23.jar
7f1640286000-7f1640298000 r-xs 001d6000 08:03 15204403                   /app/storm-supervisor/lib-ext/jdbc-11.2.0.1.0.jar
7f1640298000-7f164029a000 r-xs 00006000 08:03 15204422                   /app/storm-supervisor/lib-ext/paranamer-2.3.jar
7f164029a000-7f164029c000 r-xs 0000f000 08:03 15204411                   /app/storm-supervisor/lib-ext/jettison-1.1.jar
7f164029c000-7f16402a3000 r-xs 00044000 08:03 15204356                   /app/storm-supervisor/lib-ext/avro-1.7.4.jar
7f16402a3000-7f16402a8000 r-xs 00036000 08:03 15204366                   /app/storm-supervisor/lib-ext/commons-compress-1.4.1.jar
7f16402a8000-7f16402ab000 r-xs 0002b000 08:03 15204413                   /app/storm-supervisor/lib-ext/jsch-0.1.42.jar
7f16402ab000-7f1640343000 r-xs 007ae000 08:03 15204425                   /app/storm-supervisor/lib-ext/scala-compiler-2.8.0.jar
7f1640343000-7f1640346000 r-xs 00016000 08:03 15204414                   /app/storm-supervisor/lib-ext/jsp-api-2.1.jar
7f1640346000-7f1640348000 r-xs 000f2000 08:03 15204427                   /app/storm-supervisor/lib-ext/snappy-java-1.0.4.1.jar
7f1640348000-7f164034e000 r-xs 00054000 08:03 15204391                   /app/storm-supervisor/lib-ext/hbase-common-0.96.1-hadoop2.jar
7f164034e000-7f1640351000 r-xs 00019000 08:03 15204369                   /app/storm-supervisor/lib-ext/commons-el-1.0.jar
7f1640351000-7f1640353000 r-xs 00009000 08:03 15204363                   /app/storm-supervisor/lib-ext/commons-cli-1.2.jar
7f1640353000-7f1640356000 r-xs 00012000 08:03 15204421                   /app/storm-supervisor/lib-ext/metrics-core-2.2.0.jar
7f1640356000-7f1640358000 r-xs 0000e000 08:03 15204432                   /app/storm-supervisor/lib-ext/zkclient-0.3.jar
7f1640358000-7f164035e000 r-xs 00045000 08:03 15204370                   /app/storm-supervisor/lib-ext/commons-httpclient-3.1.jar
7f164035e000-7f1640360000 r-xs 0000b000 08:03 15204385                   /app/storm-supervisor/lib-ext/hadoop-auth-2.2.0.jar
7f1640360000-7f164036a000 r-xs 0008a000 08:03 15204433                   /app/storm-supervisor/lib-ext/zookeeper-3.3.4.jar
7f164036a000-7f1640382000 r-xs 0014c000 08:03 15204387                   /app/storm-supervisor/lib-ext/hadoop-mapreduce-client-core-2.2.0.jar
7f1640382000-7f1640391000 r-xs 0009f000 08:03 15204382                   /app/storm-supervisor/lib-ext/guice-3.0.jar
7f1640391000-7f1640393000 r-xs 00005000 08:03 15204428                   /app/storm-supervisor/lib-ext/stax-api-1.0.1.jar
7f1640393000-7f1640395000 r-xs 00002000 08:03 15204379                   /app/storm-supervisor/lib-ext/findbugs-annotations-1.3.9-1.jar
7f1640395000-7f164039e000 r-xs 0006f000 08:03 15204418                   /app/storm-supervisor/lib-ext/log4j-1.2.17.jar
7f164039e000-7f16403a1000 r-xs 00018000 08:03 15204371                   /app/storm-supervisor/lib-ext/commons-io-1.4.jar
7f16403a1000-7f16403a9000 r-xs 0007b000 08:03 15204424                   /app/storm-supervisor/lib-ext/protobuf-java-2.5.0.jar
7f16403a9000-7f16403ab000 r-xs 00000000 08:03 15204420                   /app/storm-supervisor/lib-ext/metrics-annotation-2.2.0.jar
7f16403ab000-7f16403ad000 r-xs 00009000 08:03 15204415                   /app/storm-supervisor/lib-ext/energy-commons-mini-1.0.16.jar
7f16403ad000-7f16403bc000 r-xs 000b0000 08:03 15204396                   /app/storm-supervisor/lib-ext/jackson-mapper-asl-1.9.9.jar
7f16403bc000-7f16403bf000 r-xs 00017000 08:03 15204401                   /app/storm-supervisor/lib-ext/jaxb-api-2.2.2.jar
7f16403bf000-7f16403d2000 r-xs 000c7000 08:03 15204402                   /app/storm-supervisor/lib-ext/jaxb-impl-2.2.3-1.jar
7f16403d2000-7f16403dc000 r-xs 00066000 08:03 15204406                   /app/storm-supervisor/lib-ext/jersey-core-1.9.jar
7f16403dc000-7f16403dd000 r-xs 00003000 08:03 15204407                   /app/storm-supervisor/lib-ext/jersey-guice-1.9.jar
7f16403dd000-7f16403e1000 r-xs 00035000 08:03 15204394                   /app/storm-supervisor/lib-ext/jackson-core-asl-1.9.9.jar
7f16403e1000-7f16403ef000 r-xs 0007f000 08:03 15204365                   /app/storm-supervisor/lib-ext/commons-collections-3.2.1.jar
7f16403ef000-7f16403f0000 r-xs 00000000 08:03 15204400                   /app/storm-supervisor/lib-ext/javax.inject-1.jar
7f16403f0000-7f16403f3000 r-xs 0000e000 08:03 15204429                   /app/storm-supervisor/lib-ext/storm-kafka-plus-1.0.0-SNAPSHOT.jar
7f16403f3000-7f1640404000 r-xs 0010a000 08:03 15204388                   /app/storm-supervisor/lib-ext/hadoop-yarn-api-2.2.0.jar
7f1640404000-7f1640406000 r-xs 00011000 08:03 15204399                   /app/storm-supervisor/lib-ext/jasper-runtime-5.5.23.jar
7f1640406000-7f1640430000 r-xs 002d4000 08:03 15204392                   /app/storm-supervisor/lib-ext/hbase-protocol-0.96.1-hadoop2.jar
7f1640430000-7f1640436000 r-xs 00043000 08:03 15204367                   /app/storm-supervisor/lib-ext/commons-configuration-1.6.jar
7f1640436000-7f164043b000 r-xs 00048000 08:03 15204377                   /app/storm-supervisor/lib-ext/dom4j-1.6.1.jar
7f164043b000-7f164043d000 r-xs 00004000 08:03 15204378                   /app/storm-supervisor/lib-ext/kafka-client-1.0.0-SNAPSHOT.jar
7f164043d000-7f164044e000 r-xs 000bb000 08:03 15204374                   /app/storm-supervisor/lib-ext/commons-math-2.1.jar
7f164044e000-7f1640453000 r-xs 0003e000 08:03 15204375                   /app/storm-supervisor/lib-ext/commons-net-3.1.jar
7f1640453000-7f164047e000 r-xs 00271000 08:03 15204386                   /app/storm-supervisor/lib-ext/hadoop-common-2.2.0.jar
7f164047e000-7f1640480000 r-xs 0000e000 08:03 15204354                   /app/storm-supervisor/lib-ext/activation-1.1.jar
7f1640480000-7f1640485000 r-xs 00041000 08:03 15204372                   /app/storm-supervisor/lib-ext/commons-lang-2.6.jar
7f1640485000-7f1640487000 r-xs 0000b000 08:03 15204412                   /app/storm-supervisor/lib-ext/jopt-simple-3.2.jar
7f1640487000-7f1640489000 r-xs 0000e000 08:03 15204383                   /app/storm-supervisor/lib-ext/guice-servlet-3.0.jar
7f1640489000-7f164048b000 r-xs 00003000 08:03 15204384                   /app/storm-supervisor/lib-ext/hadoop-annotations-2.2.0.jar
7f164048b000-7f164048e000 r-xs 00018000 08:03 15204376                   /app/storm-supervisor/lib-ext/commons-pool2-2.0.jar
7f164048e000-7f1640496000 r-xs 0004e000 08:03 14024880                   /app/storm-supervisor/lib/httpclient-4.1.1.jar
7f1640496000-7f1640498000 r-xs 00014000 08:03 14024885                   /app/storm-supervisor/lib/jline-0.9.94.jar
7f1640498000-7f1640499000 r-xs 00003000 08:03 14024887                   /app/storm-supervisor/lib/json-simple-1.1.jar
7f1640499000-7f164049b000 r-xs 00018000 08:03 14024906                   /app/storm-supervisor/lib/servlet-api-2.5.jar
7f164049b000-7f164049c000 r-xs 00002000 08:03 14024863                   /app/storm-supervisor/lib/clj-time-0.4.1.jar
7f164049c000-7f164049e000 r-xs 0000b000 08:03 14024877                   /app/storm-supervisor/lib/disruptor-2.10.1.jar
7f164049e000-7f16404a0000 r-xs 0000d000 08:03 14024867                   /app/storm-supervisor/lib/commons-codec-1.4.jar
7f16404a0000-7f16404a7000 r-xs 00035000 08:03 14024884                   /app/storm-supervisor/lib/jgrapht-0.8.3.jar
7f16404a7000-7f16404b0000 r-xs 0004d000 08:03 14024894                   /app/storm-supervisor/lib/logback-core-1.0.6.jar
7f16404b0000-7f16404b4000 r-xs 00024000 08:03 14024890                   /app/storm-supervisor/lib/kryo-2.17.jar
7f16404b4000-7f16404b7000 r-xs 0001b000 08:03 14024888                   /app/storm-supervisor/lib/junit-3.8.1.jar
7f16404b7000-7f16404bd000 r-xs 00038000 08:03 14024893                   /app/storm-supervisor/lib/logback-classic-1.0.6.jar
7f16404bd000-7f16404e1000 r-xs 001aa000 08:03 14024878                   /app/storm-supervisor/lib/guava-13.0.jar
7f16404e1000-7f16404e3000 r-xs 00004000 08:03 14024901                   /app/storm-supervisor/lib/ring-core-1.1.5.jar
7f16404e3000-7f16404e7000 r-xs 00028000 08:03 14024883                   /app/storm-supervisor/lib/jetty-util-6.1.26.jar
7f16404e7000-7f16404e8000 r-xs 00000000 08:03 14024865                   /app/storm-supervisor/lib/clojure-complete-0.2.3.jar
7f16404e8000-7f16404ef000 r-xs 0003c000 08:03 14024908                   /app/storm-supervisor/lib/snakeyaml-1.11.jar
7f16404ef000-7f16404f0000 r-xs 00001000 08:03 14024902                   /app/storm-supervisor/lib/ring-devel-0.3.11.jar
7f16404f0000-7f16404f2000 r-xs 0000a000 08:03 14024847                   /app/storm-supervisor/lib/asm-4.0.jar
7f16404f2000-7f16404f7000 r-xs 00040000 08:03 14024871                   /app/storm-supervisor/lib/commons-lang-2.5.jar
7f16404f7000-7f16404f8000 r-xs 00001000 08:03 14024910                   /app/storm-supervisor/lib/tools.logging-0.2.3.jar
7f16404f8000-7f16404f9000 r-xs 00000000 08:03 14024866                   /app/storm-supervisor/lib/clout-1.0.1.jar
7f16404f9000-7f1640511000 r-xs 0010e000 08:03 14024898                   /app/storm-supervisor/lib/netty-3.6.3.Final.jar
7f1640511000-7f1640512000 r-xs 00000000 08:03 14024874                   /app/storm-supervisor/lib/core.incubator-0.1.0.jar
7f1640512000-7f1640513000 r-xs 00001000 08:03 14024911                   /app/storm-supervisor/lib/tools.macro-0.1.0.jar
7f1640513000-7f1640514000 r-xs 00000000 08:03 14024904                   /app/storm-supervisor/lib/ring-servlet-0.3.11.jar
7f1640514000-7f1640516000 r-xs 0000d000 08:03 14024848                   /app/storm-supervisor/lib/carbonite-1.5.0.jar
7f1640516000-7f1640518000 r-xs 00005000 08:03 14024907                   /app/storm-supervisor/lib/slf4j-api-1.6.5.jar
7f1640518000-7f1640527000 r-xs 0007c000 08:03 14024886                   /app/storm-supervisor/lib/joda-time-2.0.jar
7f1640527000-7f1640529000 r-xs 0000d000 08:03 14024872                   /app/storm-supervisor/lib/commons-logging-1.1.1.jar
7f1640529000-7f164052b000 r-xs 00004000 08:03 14024892                   /app/storm-supervisor/lib/log4j-over-slf4j-1.6.6.jar
7f164052b000-7f164052c000 r-xs 00001000 08:03 14024862                   /app/storm-supervisor/lib/clj-stacktrace-0.2.2.jar
7f164052c000-7f164052d000 r-xs 00000000 08:03 14024903                   /app/storm-supervisor/lib/ring-jetty-adapter-0.3.11.jar
7f164052d000-7f164052f000 r-xs 00005000 08:03 14024875                   /app/storm-supervisor/lib/curator-client-1.0.1.jar
7f164052f000-7f1640532000 r-xs 00018000 08:03 14024870                   /app/storm-supervisor/lib/commons-io-1.4.jar
7f1640532000-7f1640536000 r-xs 00015000 08:03 14024876                   /app/storm-supervisor/lib/curator-framework-1.0.1.jar
7f1640536000-7f164053b000 r-xs 00028000 08:03 14024881                   /app/storm-supervisor/lib/httpcore-4.1.jar
7f164053b000-7f164055c000 r-xs 00162000 08:03 14024897                   /app/storm-supervisor/lib/mockito-all-1.9.5.jar
7f164055c000-7f164055e000 r-xs 0000f000 08:03 14024900                   /app/storm-supervisor/lib/reflectasm-1.07-shaded.jar
7f164055e000-7f1640560000 r-xs 00008000 08:03 14024912                   /app/storm-supervisor/lib/tools.nrepl-0.2.3.jar
7f1640560000-7f1640561000 r-xs 00001000 08:03 14024873                   /app/storm-supervisor/lib/compojure-1.1.3.jar
7f1640561000-7f164059d000 r-xs 00308000 08:03 14024864                   /app/storm-supervisor/lib/clojure-1.4.0.jar
7f164059d000-7f16405a0000 r-xs 0000c000 08:03 14024869                   /app/storm-supervisor/lib/commons-fileupload-1.2.1.jar
7f16405a0000-7f16405a7000 r-xs 0007d000 08:03 14024882                   /app/storm-supervisor/lib/jetty-6.1.26.jar
7f16405a7000-7f16405a9000 r-xs 0001f000 08:03 14024905                   /app/storm-supervisor/lib/servlet-api-2.5-20081211.jar
7f16405a9000-7f16405ab000 r-xs 00000000 08:03 14024895                   /app/storm-supervisor/lib/math.numeric-tower-0.0.1.jar
7f16405ab000-7f16405b1000 r-xs 00044000 08:03 14024891                   /app/storm-supervisor/lib/libthrift7-0.7.0-2.jar
7f16405b1000-7f16405b3000 r-xs 0000b000 08:03 14024868                   /app/storm-supervisor/lib/commons-exec-1.1.jar
7f16405b3000-7f16405b4000 r-xs 00000000 08:03 14024909                   /app/storm-supervisor/lib/tools.cli-0.2.2.jar
7f16405b4000-7f16405b5000 r-xs 00001000 08:03 14024879                   /app/storm-supervisor/lib/hiccup-0.3.6.jar
7f16405b5000-7f16405b7000 r-xs 00000000 08:03 14024896                   /app/storm-supervisor/lib/minlog-1.2.jar
7f16405b7000-7f16405c0000 r-xs 0008a000 08:03 14024913                   /app/storm-supervisor/lib/zookeeper-3.3.3.jar
7f16405c0000-7f16405c2000 r-xs 00007000 08:03 14024899                   /app/storm-supervisor/lib/objenesis-1.2.jar
7f16405c2000-7f16405c3000 r-xs 00003000 08:03 14024889                   /app/storm-supervisor/lib/jzmq-2.1.0.jar
7f16405c3000-7f16405c5000 r-xs 00009000 08:03 14024936                   /app/storm-supervisor/storm-netty-0.9.0.1.jar
7f16405c5000-7f16405c6000 r-xs 00000000 08:03 14024934                   /app/storm-supervisor/storm-console-logging-0.9.0.1.jar
7f16405c6000-7f164062b000 r-xs 004b8000 08:03 14024935                   /app/storm-supervisor/storm-core-0.9.0.1.jar
7f164062b000-7f164062c000 ---p 00000000 00:00 0 
7f164062c000-7f164072c000 rwxp 00000000 00:00 0 
7f164072c000-7f164072f000 ---p 00000000 00:00 0 
7f164072f000-7f164082d000 rwxp 00000000 00:00 0 
7f164082d000-7f1640830000 ---p 00000000 00:00 0 
7f1640830000-7f164092e000 rwxp 00000000 00:00 0 
7f164092e000-7f1640931000 ---p 00000000 00:00 0 
7f1640931000-7f1640a2f000 rwxp 00000000 00:00 0 
7f1640a2f000-7f1640a32000 ---p 00000000 00:00 0 
7f1640a32000-7f1640b30000 rwxp 00000000 00:00 0 
7f1640b30000-7f1640b33000 ---p 00000000 00:00 0 
7f1640b33000-7f1640c31000 rwxp 00000000 00:00 0 
7f1640c31000-7f1640c34000 ---p 00000000 00:00 0 
7f1640c34000-7f1640d32000 rwxp 00000000 00:00 0 
7f1640d32000-7f1640d33000 ---p 00000000 00:00 0 
7f1640d33000-7f1640e67000 rwxp 00000000 00:00 0 
7f1640e67000-7f1641000000 r-xs 03087000 08:03 16126619                   /usr/java/jdk1.6.0_45/jre/lib/rt.jar
7f1641000000-7f1641270000 rwxp 00000000 00:00 0 
7f1641270000-7f164520e000 rwxp 00000000 00:00 0 
7f164520e000-7f1648000000 ---p 00000000 00:00 0 
7f1648052000-7f1648055000 ---p 00000000 00:00 0 
7f1648055000-7f16483d9000 rwxp 00000000 00:00 0 
7f16483d9000-7f16483da000 ---p 00000000 00:00 0 
7f16483da000-7f164913a000 rwxp 00000000 00:00 0 
7f164913a000-7f1649156000 rwxp 00000000 00:00 0 
7f1649156000-7f1649aac000 rwxp 00000000 00:00 0 
7f1649aac000-7f1649ac7000 rwxp 00000000 00:00 0 
7f1649ac7000-7f1649ac8000 rwxp 00000000 00:00 0 
7f1649ac8000-7f1649ac9000 ---p 00000000 00:00 0 
7f1649ac9000-7f1649bc9000 rwxp 00000000 00:00 0 
7f1649bc9000-7f1649bca000 ---p 00000000 00:00 0 
7f1649bca000-7f1649cca000 rwxp 00000000 00:00 0 
7f1649cca000-7f1649ccb000 ---p 00000000 00:00 0 
7f1649ccb000-7f1649dcb000 rwxp 00000000 00:00 0 
7f1649dcb000-7f1649dcc000 ---p 00000000 00:00 0 
7f1649dcc000-7f1649ed6000 rwxp 00000000 00:00 0 
7f1649ed6000-7f1649f8c000 rwxp 00000000 00:00 0 
7f1649f8c000-7f1649f9a000 r-xp 00000000 08:03 16256568                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libzip.so
7f1649f9a000-7f164a09c000 ---p 0000e000 08:03 16256568                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libzip.so
7f164a09c000-7f164a09f000 rwxp 00010000 08:03 16256568                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libzip.so
7f164a09f000-7f164a0a0000 rwxp 00000000 00:00 0 
7f164a0a0000-7f164a0ac000 r-xp 00000000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a0ac000-7f164a2ac000 ---p 0000c000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a2ac000-7f164a2ad000 r-xp 0000c000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a2ad000-7f164a2ae000 rwxp 0000d000 08:03 13631517                   /lib64/libnss_files-2.12.so
7f164a2ae000-7f164a2bd000 r-xs 00667000 08:03 16126602                   /usr/java/jdk1.6.0_45/jre/lib/charsets.jar
7f164a2bd000-7f164a2e6000 r-xp 00000000 08:03 16256546                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libjava.so
7f164a2e6000-7f164a3e5000 ---p 00029000 08:03 16256546                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libjava.so
7f164a3e5000-7f164a3ec000 rwxp 00028000 08:03 16256546                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libjava.so
7f164a3ec000-7f164a3f9000 r-xp 00000000 08:03 16256567                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libverify.so
7f164a3f9000-7f164a4f8000 ---p 0000d000 08:03 16256567                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libverify.so
7f164a4f8000-7f164a4fb000 rwxp 0000c000 08:03 16256567                   /usr/java/jdk1.6.0_45/jre/lib/amd64/libverify.so
7f164a4fb000-7f164a4fe000 ---p 00000000 00:00 0 
7f164a4fe000-7f164a5fc000 rwxp 00000000 00:00 0 
7f164a5fc000-7f164af1a000 r-xp 00000000 08:03 16388640                   /usr/java/jdk1.6.0_45/jre/lib/amd64/server/libjvm.so
7f164af1a000-7f164b01c000 ---p 0091e000 08:03 16388640                   /usr/java/jdk1.6.0_45/jre/lib/amd64/server/libjvm.so
7f164b01c000-7f164b1d2000 rwxp 00920000 08:03 16388640                   /usr/java/jdk1.6.0_45/jre/lib/amd64/server/libjvm.so
7f164b1d2000-7f164b20f000 rwxp 00000000 00:00 0 
7f164b20f000-7f164b216000 r-xp 00000000 08:03 16256297                   /usr/java/jdk1.6.0_45/jre/lib/amd64/jli/libjli.so
7f164b216000-7f164b317000 ---p 00007000 08:03 16256297                   /usr/java/jdk1.6.0_45/jre/lib/amd64/jli/libjli.so
7f164b317000-7f164b319000 rwxp 00008000 08:03 16256297                   /usr/java/jdk1.6.0_45/jre/lib/amd64/jli/libjli.so
7f164b319000-7f164b31a000 rwxp 00000000 00:00 0 
7f164b31f000-7f164b327000 rwxs 00000000 08:03 2883591                    /tmp/hsperfdata_root/11021
7f164b327000-7f164b328000 rwxp 00000000 00:00 0 
7f164b328000-7f164b329000 r-xp 00000000 00:00 0 
7f164b329000-7f164b32a000 rwxp 00000000 00:00 0 
7fff411d7000-7fff411ee000 rwxp 00000000 00:00 0                          [stack]
7fff411ff000-7fff41200000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Xms1536m -Xmx1536m -Xmn378m -XX:SurvivorRatio=2 -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=65 -Xloggc:/app/storm-supervisor/logs/worker-6701-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+HeapDumpOnOutOfMemoryError -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dlogfile.name=worker-6701.log -Dstorm.home=/app/storm-supervisor -Dlogback.configurationFile=/app/storm-supervisor/logback/cluster.xml -Dstorm.id=oracle-13-1403837712 -Dworker.id=ae1a2f48-91eb-4b8c-96ac-f0340a78d441 -Dworker.port=6701 
java_command: backtype.storm.daemon.worker oracle-13-1403837712 57bf1040-b334-4bb0-a21c-90d8176c98c7 6701 ae1a2f48-91eb-4b8c-96ac-f0340a78d441
Launcher Type: SUN_STANDARD

Environment Variables:
JAVA_HOME=/usr/java/jdk1.6.0_45
CLASSPATH=/usr/java/jdk1.6.0_45/lib/dt.jar:/usr/java/jdk1.6.0_45/lib/tools.jar:/app/zookeeper/lib
PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin:/usr/java/jdk1.6.0_45/bin:/app/zookeeper/bin:/root/bin
LD_LIBRARY_PATH=/usr/java/jdk1.6.0_45/jre/lib/amd64/server:/usr/java/jdk1.6.0_45/jre/lib/amd64:/usr/java/jdk1.6.0_45/jre/../lib/amd64:/usr/local/lib:/opt/local/lib:/usr/lib
SHELL=/bin/bash

Signal Handlers:
SIGSEGV: [libjvm.so+0x862a30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGBUS: [libjvm.so+0x862a30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGFPE: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGPIPE: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGXFSZ: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGILL: [libjvm.so+0x7106f0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGUSR1: SIG_DFL, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGUSR2: [libjvm.so+0x713520], sa_mask[0]=0x00000004, sa_flags=0x10000004
SIGHUP: SIG_IGN, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGINT: [libjvm.so+0x713120], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGTERM: [libjvm.so+0x713120], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGQUIT: [libjvm.so+0x713120], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004


---------------  S Y S T E M  ---------------

OS:Red Hat Enterprise Linux Server release 6.4 (Santiago)

uname:Linux 2.6.32-358.el6.x86_64 #1 SMP Tue Jan 29 11:47:41 EST 2013 x86_64
libc:glibc 2.12 NPTL 2.12 
rlimit: STACK 10240k, CORE 0k, NPROC 61813, NOFILE 200000, AS infinity
load average:20.05 19.75 21.56

/proc/meminfo:
MemTotal:        7932088 kB
MemFree:         2699168 kB
Buffers:          242928 kB
Cached:          1789068 kB
SwapCached:         4288 kB
Active:          2858560 kB
Inactive:        2014480 kB
Active(anon):    2076152 kB
Inactive(anon):   768512 kB
Active(file):     782408 kB
Inactive(file):  1245968 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:       8388600 kB
SwapFree:        8369768 kB
Dirty:               784 kB
Writeback:             0 kB
AnonPages:       2778976 kB
Mapped:            25216 kB
Shmem:              3608 kB
Slab:             253560 kB
SReclaimable:     221768 kB
SUnreclaim:        31792 kB
KernelStack:        3040 kB
PageTables:        18696 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    12354644 kB
Committed_AS:    5425796 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      368384 kB
VmallocChunk:   34359365100 kB
HardwareCorrupted:     0 kB
AnonHugePages:   2635776 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:        8192 kB
DirectMap2M:     8263680 kB


CPU:total 4 (2 cores per cpu, 2 threads per core) family 6 model 58 stepping 9, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, ht

/proc/cpuinfo:
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 1600.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 0
cpu cores	: 2
apicid		: 0
initial apicid	: 0
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 1
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 3300.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 1
cpu cores	: 2
apicid		: 2
initial apicid	: 2
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 2
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 1600.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 0
cpu cores	: 2
apicid		: 1
initial apicid	: 1
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:

processor	: 3
vendor_id	: GenuineIntel
cpu family	: 6
model		: 58
model name	: Intel(R) Core(TM) i3-3220 CPU @ 3.30GHz
stepping	: 9
cpu MHz		: 1600.000
cache size	: 3072 KB
physical id	: 0
siblings	: 4
core id		: 1
cpu cores	: 2
apicid		: 3
initial apicid	: 3
fpu		: yes
fpu_exception	: yes
cpuid level	: 13
wp		: yes
flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt tsc_deadline_timer xsave avx f16c lahf_lm arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms
bogomips	: 6585.33
clflush size	: 64
cache_alignment	: 64
address sizes	: 36 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 7932088k(2699168k free), swap 8388600k(8369768k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (20.45-b01) for linux-amd64 JRE (1.6.0_45-b06), built on Mar 26 2013 14:07:02 by ""java_re"" with gcc 3.2.2 (SuSE Linux)

time: Fri Jun 27 11:10:30 2014
elapsed time: 2 seconds

"
STORM-374,storm0.9.1&kafka0.8.1 with storm-kafka-0.8-plus-0.5.0 can not run,"ERROR backtype.storm.util - Async loop died!
java.lang.NoSuchMethodError: com.netflix.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lcom/netflix/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at storm.kafka.ZkState.writeBytes(ZkState.java:59) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.ZkState.writeJSON(ZkState.java:53) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.PartitionManager.commit(PartitionManager.java:188) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.commit(KafkaSpout.java:169) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:134) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at backtype.storm.daemon.executor$eval5100$fn__5101$fn__5116$fn__5145.invoke(executor.clj:562) ~[na:na]
        at backtype.storm.util$async_loop$fn__390.invoke(util.clj:433) ~[na:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_45]
12949 [Thread-27-words] ERROR backtype.storm.daemon.executor - 
java.lang.NoSuchMethodError: com.netflix.curator.framework.api.CreateBuilder.creatingParentsIfNeeded()Lcom/netflix/curator/framework/api/ProtectACLCreateModePathAndBytesable;
        at storm.kafka.ZkState.writeBytes(ZkState.java:59) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.ZkState.writeJSON(ZkState.java:53) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.PartitionManager.commit(PartitionManager.java:188) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.commit(KafkaSpout.java:169) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:134) ~[storm-kafka-0.8-plus-0.5.0-SNAPSHOT.jar:na]
        at backtype.storm.daemon.executor$eval5100$fn__5101$fn__5116$fn__5145.invoke(executor.clj:562) ~[na:na]
        at backtype.storm.util$async_loop$fn__390.invoke(util.clj:433) ~[na:na]
        at clojure.lang.AFn.run(AFn.java:24) [clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_45]
12971 [Thread-27-words] INFO  backtype.storm.util - Halting process: (""Worker died"")"
STORM-373,Provide Additional String substitutions for *.worker.childopts ,"As a storm application developer, I would like to name my GC log uniquely, so
that the log is not blown away each time the worker restarts.

Add %WORKER-ID% (worker-id) %STORM-ID% (storm-id) to the substitution whenlaunching the worker JVM."
STORM-372,Typo in storm_env.ini,The `conf/storm_env.ini` file has a couple of typos in the comments. Pull request to fix them is at: https://github.com/apache/incubator-storm/pull/156
STORM-370,Component page UI details are blank,"Symptom: On component page - most of the details are blank. If the component
json response has some of list elements empty ( e.g. boltStats, executors
or spoutStats, outputStats inputStats).

In such situation, I noticed earlier the component page call to js
tablesorter  throws exception ( - wherever table has 0 rows). This
exception stops further rendering of elements.  "
STORM-369,Storm ui topology summary the page displays an worng,Move to the end of taskstotal
STORM-368,Trident groupBy().aggregate() produces no results if Spout.nextTuple always emits,"I was debugging an issue with Pig-Squeal and noticed that my Combine Aggregators weren't working with the RandomSentenceSpout.  After some investigation, I discovered that no values are emitted from the aggregate if the underlying Spout always emits tuples.

I will attach a test case to demonstrate shortly."
STORM-361,Add JSON-P support to Storm UI API,"The recent API that is being released in Storm UI with 0.9.2 is great, but it'd be useful if the API supported an optional {{?callback}} parameter that would provide a wrapped [JSON-P response|http://json-p.org/] for all endpoints.

*Example:*
{code}
GET https://my-storm-head.cogtree.com/api/v1/cluster/summary?callback=myFunction

myFunction({""stormVersion"":""0.9.2-incubating"",""nimbusUptime"":""4m 55s"",""supervisors"":2,""slotsTotal"":32,""slotsUsed"":0,""slotsFree"":32,""executorsTotal"":0,""tasksTotal"":0});
{code}

This would allow users to write clients similar to [elasticsearch-head|https://github.com/mobz/elasticsearch-head] (which I was hoping to build) where they could use a single UI to manage multiple Storm environments.

Having a JSON-P {{?callback}} param would allow clients to get around browsers enforcing same-origin policies.

The JSON-P site has examples of valid callback parameters:

{code}
functionName({JSON});

obj.functionName({JSON});

obj[""function-name""]({JSON});
{code}"
STORM-359,Logviewer paging and download,"Sometimes it is easier to access logs via the UI for quick browsing, without needing to access a separate logging facility or shell access to a particular machine.

Add paging and the ability to download a worker's log file to the logviewer daemon."
STORM-356,Duplicated servlet-api dependency in lib classpath,"Two different versions of _servlet-api_ dependencies are found in _lib_ folder:

{noformat}
knowak@knowak-dell:~/incubator-storm-0.9.2-incubating/storm-dist/binary/target/apache-storm-0.9.2-incubating/lib$ ls -al | grep servlet-api
-rw-rw-r-- 1 knowak knowak  134133 Jan 13 11:44 servlet-api-2.5-20081211.jar
-rw-rw-r-- 1 knowak knowak  105112 Jan 13 11:44 servlet-api-2.5.jar
{noformat}

It seems to be a similar issue to STORM-355 and listing dependencies from _pom.xml_ confirms that:

{noformat}
knowak@knowak-dell:~/incubator-storm-0.9.2-incubating/storm-core$ mvn dependency:list | grep servlet-api
[INFO]    javax.servlet:servlet-api:jar:2.5:compile
[INFO]    org.mortbay.jetty:servlet-api:jar:2.5-20081211:compile
{noformat}

The older ones comes transitively from _commons-fileupload_ dependency (I guess the preferred candidate to be excluded) and the newer one from _jetty_ dependency."
STORM-355,Duplicated netty dependency in lib classpath,"Both _netty_ dependencies - [_3.2.2.Final_ (from _org.jboss.netty_)|http://search.maven.org/#artifactdetails|org.jboss.netty|netty|3.2.2.Final|bundle] and [_3.6.3.Final_ (from _io.netty_)|http://search.maven.org/#artifactdetails|io.netty|netty|3.6.3.Final|bundle] - are found in _lib_ folder:

{noformat}
knowak@knowak-dell:~/incubator-storm-0.9.2-incubating/storm-dist/binary/target/apache-storm-0.9.2-incubating/lib$ ls -al | grep netty
-rw-rw-r-- 1 knowak knowak  785556 Jun 17 12:38 netty-3.2.2.Final.jar
-rw-rw-r-- 1 knowak knowak 1202373 Jan 27 11:14 netty-3.6.3.Final.jar
{noformat}

as both of them are configured in _pom.xml_:

{noformat}
knowak@knowak-dell:~/incubator-storm-0.9.2-incubating/storm-core$ mvn dependency:list | grep netty
[INFO]    io.netty:netty:jar:3.6.3.Final:compile
[INFO]    org.jboss.netty:netty:jar:3.2.2.Final:compile
{noformat}

and it seems that the older one is transitively brought by [_zookeeper_ dependency|http://search.maven.org/remotecontent?filepath=org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.pom]:

{noformat}
knowak@knowak-dell:~/incubator-storm-0.9.2-incubating/storm-core$ mvn dependency:tree
...
[INFO] +- org.apache.curator:curator-framework:jar:2.4.0:compile
[INFO] |  +- org.apache.curator:curator-client:jar:2.4.0:compile
[INFO] |  \- org.apache.zookeeper:zookeeper:jar:3.4.5:compile
[INFO] |     +- jline:jline:jar:2.11:compile
[INFO] |     \- org.jboss.netty:netty:jar:3.2.2.Final:compile
...
[INFO] +- io.netty:netty:jar:3.6.3.Final:compile
...
{noformat}

so possibly [_curator-framework_ dependency section|https://github.com/apache/incubator-storm/blob/master/pom.xml#L349] needs adding an exclusion of _org.jboss.netty_:_netty_"
STORM-354,Allow users to pass TEST-TIMEOUT-MS as param for complete-topology,"It would be nice if complete-topology allowed a user to pass in the default timeout as a parameter (rather than just setting it as 5000ms). I had a test that kept failing by taking too long.

This PR adds functionality without breaking any existing code. Tests pass.

(PS - not sure if this is the proper way to submit a PR for an apache project. I'm open to feedback. Thanks)"
STORM-352,[storm-kafka] PartitionManager does not save offsets to ZooKeeper,"The logic to determine whether offsets should be written to ZK is wrong. As a result, offsets are never saved."
STORM-351,multilang python process fall into endless loop,"1. steps to reproduce
    1)  write a topology with a python bolt, run the topology on storm; then there will be two process for the bolt: the worker(java process for ShellBolt), python process.
    2）kill -9  the worker(java process for ShellBolt);

2. expected behavior
    the worker exit and the python process exist

3. actual, incorrect behavior
    the worker exit, but the python process never exist and fall into endless loop

4. analyse
    in storm.py，read tuple from stdin with follow function:

def readMsg():
    msg = """"
    while True:
        line = sys.stdin.readline()[0:-1]
        if line == ""end"":
            break
        msg = msg + line + ""\n""
    return json_decode(msg[0:-1])

    when sys.stdin is closed, EOF is encountered, readline() return None, so readMsg fall into endless loop."
STORM-341,assignment reassignment,"I met a problem: in nimubs code， compute-topology->scheduler-assignment function,Will convert clojure's map to a Java map,but this will result in order change, leading to the assignment reassignment
I add log to this
--------------------------------------code1
 _ (log-message ""to java objec"" executor->slot)
_ (log-message ""-------------------------"")]]
{tid (SchedulerAssignmentImpl. tid executor->slot)})))

--------------------------------------code2
 topology->scheduler-assignment (compute-topology->scheduler-assignment nimbus
                                                                               existing-assignments
                                                                               topology->alive-executors)
 _ (log-message ""end of compute "" topology->scheduler-assignment)
--------------------------------------code3 , I implement SchedulerAssignmentImpl toString code
  @Override
    public String toString() {
    	String info = """";
    	if(this.getExecutorToSlot() != null) {
    		for(Entry<ExecutorDetails, WorkerSlot> entry : this.getExecutorToSlot().entrySet()) {
    			info += ""["" + entry.getKey().getStartTask() + "","" + entry.getKey().getEndTask() + ""]"";
    		}
    	}
    	return info;
    }

--------------------------------------------> print log

2014-06-05 16:07:26 nimbus [INFO] to java objec{#<ExecutorDetails [3, 3]> #<WorkerSlot d631169e-8bc1-4768-98b0-6c1a0b55c355:6710:1>, #<ExecutorDetails [6, 6]> #<WorkerSlot d631169e-8bc1-4768-98b0-6c1a0b55c355:6710:1>, #<ExecutorDetails [5, 5]> #<WorkerSlot d631169e-8bc1-4768-98b0-6c1a0b55c355:6710:1>, #<ExecutorDetails [4, 4]> #<WorkerSlot d631169e-8bc1-4768-98b0-6c1a0b55c355:6710:1>, #<ExecutorDetails [2, 2]> #<WorkerSlot d631169e-8bc1-4768-98b0-6c1a0b55c355:6710:1>, #<ExecutorDetails [1, 1]> #<WorkerSlot d631169e-8bc1-4768-98b0-6c1a0b55c355:6710:1>}
2014-06-05 16:07:26 nimbus [INFO] -------------------------
2014-06-05 16:07:26 nimbus [INFO] end of compute {""case_1#1-1-1401955622"" #<SchedulerAssignmentImpl [6,6][5,5][3,3][4,4][2,2][1,1]>}

-------------------------------------------------------------------------------
i think we could fix this question add this code to function changed-executors
;        executor->node+port (if executor->node+port (sort executor->node+port) nil)
;        new-executor->node+port (if new-executor->node+port (sort new-executor->node+port) nil)"
STORM-340,Add topology submission REST API and UI button,"STORM-205 added REST API to Storm, but these API are only used to monitor/manage a topology. Users still have to submit a new topology through command line. We can design a new REST API to achieve this so that a client outside of a Storm cluster can submit and manage topologies remotely.

This is the API I have in mind:
/api/topology/:id (PUT)
In the body the user can specify the jar location and class name. 

However, user still need to upload the jar to nimbus, for which we have two choices:
1) design an API to upload the jar (e.g. in base64) then reference the upload location in topology submission API:
/api/jarupload/:loc (POST)
2) make storm support hadoop file system so that we can upload the jar to HDFS then call the submission API.

Further we can add a button to storm UI, using which one can submit a new topology."
STORM-339,Severe memory leak to OOM when ackers disabled,"Without any ackers enabled, fast component  will continuously leak memory and causing OOM problems when target component is slow. The OOM problem can be reproduced by running this fast-slow-topology:

https://github.com/Gvain/storm-perf-test/tree/fast-slow-topology

with command:

{code}
$ storm jar storm_perf_test-1.0.0-SNAPSHOT-jar-with-dependencies.jar com.yahoo.storm.perftest.Main --spout 1 --bolt 1 --workers 2 --testTime 600 --messageSize 6400
{code}

And the worker childopts with {{-Xms2g -Xmx2g -Xmn512m ...}}.

At the same time, the executed count of target component is far behind from the emitted count of source component.  I guess it could be that netty client is buffering too much messages in its message_queue as target component sends back OK/Failure Response too slowly. "
STORM-338,Move towards idiomatic Clojure style,"See discussion at https://github.com/apache/incubator-storm/pull/130

To summarize my suggestion, can we go with these style and coding recommendations?
* http://dev.clojure.org/display/community/Library+Coding+Standards
* https://github.com/bbatsov/clojure-style-guide"
STORM-336,Logback version should be upgraded to 1.0.12 from 1.0.6,we can incorporate a fix for this issue http://jira.qos.ch/browse/LOGBACK-749 
STORM-334,Unable to compile storm code,"I have trying to compile the storm code using mvn clean install -X but it keeps failing with the following error,
{code:borderStyle=solid}
[DEBUG] Command line: [java, -Dclojure.compile.path=/Users/gkhare/git/incubator-storm/storm-core/target/classes, -jar, /var/folders/x9/gsh9kn3x5wjch7jfnwh_mf4dn5z616/T/clojuremavenplugin9203516485011075775jar, backtype.storm.command.deactivate, backtype.storm.tuple, backtype.storm.command.shell-submission, backtype.storm.bootstrap, backtype.storm.daemon.acker, backtype.storm.log, backtype.storm.ui.helpers, backtype.storm.testing4j, backtype.storm.config, backtype.storm.zookeeper, backtype.storm.daemon.nimbus, backtype.storm.daemon.builtin-metrics, backtype.storm.ui.core, backtype.storm.event, backtype.storm.command.rebalance, backtype.storm.metric.testing, backtype.storm.command.dev-zookeeper, backtype.storm.daemon.common, backtype.storm.daemon.executor, backtype.storm.daemon.worker, backtype.storm.command.config-value, backtype.storm.process-simulator, backtype.storm.testing, backtype.storm.daemon.supervisor, backtype.storm.disruptor, backtype.storm.thrift, backtype.storm.util, backtype.storm.daemon.logviewer, backtype.storm.messaging.loader, backtype.storm.scheduler.IsolationScheduler, backtype.storm.clojure, backtype.storm.stats, backtype.storm.messaging.local, backtype.storm.timer, backtype.storm.LocalDRPC, backtype.storm.scheduler.DefaultScheduler, backtype.storm.command.list, backtype.storm.cluster, backtype.storm.scheduler.EvenScheduler, backtype.storm.daemon.task, backtype.storm.daemon.drpc, storm.trident.testing, backtype.storm.command.activate, backtype.storm.command.kill-topology, backtype.storm.LocalCluster]
Compiling backtype.storm.command.deactivate to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.tuple to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.command.shell-submission to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.bootstrap to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.daemon.acker to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.log to /Users/gkhare/git/incubator-storm/storm-core/target/classes
Compiling backtype.storm.ui.helpers to /Users/gkhare/git/incubator-storm/storm-core/target/classes
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 18.097s
[INFO] Finished at: Thu May 29 16:00:32 IST 2014
[INFO] Final Memory: 25M/278M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.20:compile (compile-clojure) on project storm-core: Clojure failed. -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.theoryinpractise:clojure-maven-plugin:1.3.20:compile (compile-clojure) on project storm-core: Clojure failed.
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:217)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
	at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
	at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
	at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
	at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)
	at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)
	at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)
	at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)
Caused by: org.apache.maven.plugin.MojoExecutionException: Clojure failed.
	at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:463)
	at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:379)
	at com.theoryinpractise.clojure.AbstractClojureCompilerMojo.callClojureWith(AbstractClojureCompilerMojo.java:356)
	at com.theoryinpractise.clojure.ClojureCompilerMojo.execute(ClojureCompilerMojo.java:40)
	at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
	at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
	... 19 more
{code}

This is possibly failing for following namespace
_backtype.storm.ui.helpers_
"
STORM-331,storm-kafka: bump Kafka version dependency to 0.8.1.1,"The Kafka project has recently published the hot fix release Kafka 0.8.1.1, which addresses a number of bugs that, essentially, forced a lot of users to revert from 0.8.1 to 0.8.0.

storm-kafka still depends on the buggy 0.8.1 Kafka release, and thus should be updated to use 0.8.1.1 instead.

I performed basic functional testing of Kafka<->Storm interoperability using Storm 0.9.1 and Kafka 0.8.1.1, and the integration looks ok (I would have been surprised if it didn't).  Hence I'd say it's safe to bump the Kafka version of 0.8.1.1."
STORM-330,storm.messaging.netty.max_retries option in config file not being used if > 30,"I have been trying to set the storm.messaging.netty.max_retries to 240 because of connection issues when one worker takes a longer time to start its processes. But due to this line https://github.com/apache/incubator-storm/blob/1a0b46e95ab4ac467525314a75819a75dec92c40/storm-core/src/jvm/backtype/storm/messaging/netty/Client.java#L73 the max_retries is capped at 30. I am guessing this is a bug or at least should be noted somewhere in the documentation?

"
STORM-329,Fix cascading Storm failure by improving reconnection strategy and buffering messages,"_Note: The original title of this ticket was: ""Add Option to Config Message handling strategy when connection timeout""._


This is to address a [concern brought up|https://github.com/apache/incubator-storm/pull/103#issuecomment-43632986] during the work at STORM-297:

{quote}
[~revans2] wrote: Your logic makes since to me on why these calls are blocking. My biggest concern around the blocking is in the case of a worker crashing. If a single worker crashes this can block the entire topology from executing until that worker comes back up. In some cases I can see that being something that you would want. In other cases I can see speed being the primary concern and some users would like to get partial data fast, rather then accurate data later.

Could we make it configurable on a follow up JIRA where we can have a max limit to the buffering that is allowed, before we block, or throw data away (which is what zeromq does)?
{quote}

If some worker crash suddenly, how to handle the message which was supposed to be delivered to the worker?

1. Should we buffer all message infinitely?
2. Should we block the message sending until the connection is resumed?
3. Should we config a buffer limit, try to buffer the message first, if the limit is met, then block?
4. Should we neither block, nor buffer too much, but choose to drop the messages, and use the built-in storm failover mechanism? 

"
STORM-328,"Config.java and Utils.get{Int,Long} are not in sync for floating point","For most numeric configuration values Config.java has a type of Number listed, but the values are parsed using Utils.getInt or Utils.getLong, which means if someone gave a floating point number it would pass the ConfigValidation, but would blow up when it is used.  This is most critical for values that Nimbus reads, but would be good to have them consistent everywhere.

We should also check that getInt works properly for numbers that are larger then would fit in an integer.

Either we need to update Utils to be more lenient when looking at doubles, or we need to make Config more strict, which may be difficult with the YAML and JSON parsing that happens, where ints are often converted into longs."
STORM-326,task metric send repeatedly,"Tasks send unnecessary Metrics and the repeated time is equal to the number of tasks in the executor, which runs them.

Scenario: each time, a task will send repeated metric-tuples to the metricConsumer bolt (the 1st metric-tuple is normal, but the rest contain mostly zero-valued results). 

Causality: when a task receives a metric-tick-tuple, it ""mistakenly"" triggers all the tasks (belonging to the same executor) sending out metrics.
  "
STORM-325,storm-kafka: dependencies scope to provided,"Please change the scala and kafka dependencies scope to provided, in this case the user can decide which scala and kafka version will be used with storm-kafka..."
STORM-324,Require notes on internal of storm,"Require information about the internals of __tick and __metrics_tick.
The only information about __tick tuples was from 0.8 release notes.

This is more from a troubleshooting perspective since these tuples are generated by the system so frequently (can be seen when topology.debug is true)

The kind of information that would help:
1. The mechanism of collecting metrics from each executor, spout or bolt
2. Looking at the tuple from debug logs, how to figure out which bolt these are intended to?
2. How does a bolt/spout/executor respond to it?
etc.
"
STORM-323,Unacknowledged __tick and __metrics_tick tuples hangs worker processes,"Symptoms observed:
1. One of the bolts not getting executed after about 5 days of run
2. Spout gradually slows down and finally stops calling nextTuple()
3. Topology is non-functional since there is no exchange of tuples across worker processes

Notes from troubleshooting:
1. There is a transfer of data across worker processes but the bolt is not receiving the tuples
2. backtype.storm.messaging.netty.Server#message_queue is not getting consumed.
3. Later on found that there are several __tick and __metrics_tick tuples piling up in memory over a period of time. This piling up is gradual and probably the reason why it takes so long for it to cause any visible problems.

I have shared access to thread dumps and topology layout at https://drive.google.com/folderview?id=0B2F_3UACQZNESXpwZlA4MFlqSVU&usp=drive_web
"
STORM-322,Windows Scripts do not handle spaces in JAVA_HOME path,"If Java is installed to a path that has spaces, the windows scripts will error."
STORM-320,Support STORM_CONF_DIR environment variable for alternate conf dir,"In many apache big data projects (Hadoop, Hbase, to name a few), the user can specify an alternate config file directory with PROJECT_CONF_DIR, such as HADOOP_CONF_DIR or HBASE_CONF_DIR.

It'd be useful to have this feature in Storm instead of hard coding a different location into bin/storm.

I've written a simple patch, which I will attach and also send a pull request on github."
STORM-317,Add SECURITY.md to release binaries,We have recently added security documentation for Storm (STORM-306).  We should also include this document into release binaries.
STORM-314,Storm breaks tools.cli upgrades on Clojure projects that depend on Storm,"We're working on new a Python + Storm interop library called streamparse (https://github.com/Parsely/streamparse/). To submit topologies to Storm and run local clusters, it leverages lein and the Clojure DSL. In the project we create for Storm, our lein project.clj configuration includes these dependencies:

  :dependencies [
                 [storm ""0.9.0.1""]
                 [org.clojure/clojure ""1.5.1""]
                 [org.clojure/data.json ""0.2.4""]
                 [org.clojure/tools.cli ""0.3.1""]
                 ]

The last dependency, org.clojure/tools.cli, is problematic. This is because Storm apparently bundles org.clojure/tools.cli 0.2.x, and due to the way Storm is compiled, it masks over the 0.3.1 dependency, which changes the API dramatically. I discussed this with technomancy (lein's creator) on IRC, and he said this was probably due to ""AOT"" -- ahead-of-time compilation -- causing incorrect classpath resolution to Storm's bundled version

To work around the issue right now, I need to add 

    :exclusions [org.clojure/tools.cli]

to my project.clj. However, I am filing this bug because as the lein author says in the project's FAQ:

""You may also want to report a bug with the dependency that uses hard version ranges as they cause all kinds of problems and exhibit unintuitive behaviour."""
STORM-309,Main class param should be quoted,"In windows, maven throws ""Unknown lifecycle phase"" if mainclass parameter is not quoted"
STORM-307,"After host crash, supervisor is unable to restart itself","Hi,

I've observed [multiple times|#links] that supervisor state de-serialisation after host crash or reboot can fail. Supervisor is then unable to come up without manual intervention. AFAICT, it seems that serialized supervisor state if invalid and coun't be read at next start.

Observed error in supervisor log :
{noformat}
2014-04-29 19:38:35 c.n.c.f.i.CuratorFrameworkImpl [INFO] Starting
2014-04-29 19:38:35 o.a.z.ZooKeeper [INFO] Initiating client connection, connectString=127.0.0.1:2181/storm sessionTimeout=20000 watcher=com.netflix.curator.ConnectionState@18d055e0
2014-04-29 19:38:35 o.a.z.ClientCnxn [INFO] Opening socket connection to server /127.0.0.1:2181
2014-04-29 19:38:35 o.a.z.ClientCnxn [INFO] Socket connection established to localhost/127.0.0.1:2181, initiating session
2014-04-29 19:38:35 o.a.z.ClientCnxn [INFO] Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x145a7cc1c7e48b1, negotiated timeout = 20000
2014-04-29 19:38:35 b.s.d.supervisor [INFO] Starting supervisor with id 71b01216-9d00-4fb6-8538-6673058ab5ef at host storm
2014-04-29 19:38:36 b.s.event [ERROR] Error when processing event
java.lang.RuntimeException: java.io.EOFException
        at backtype.storm.utils.Utils.deserialize(Utils.java:86) ~[storm-core-0.9.1-incubating.jar:0.9.1-incubating]
        at backtype.storm.utils.LocalState.snapshot(LocalState.java:45) ~[storm-core-0.9.1-incubating.jar:0.9.1-incubating]
        at backtype.storm.utils.LocalState.get(LocalState.java:56) ~[storm-core-0.9.1-incubating.jar:0.9.1-incubating]
        at backtype.storm.daemon.supervisor$sync_processes.invoke(supervisor.clj:207) ~[storm-core-0.9.1-incubating.jar:0.9.1-incubating]
        at clojure.lang.AFn.applyToHelper(AFn.java:161) ~[clojure-1.4.0.jar:na]
        at clojure.lang.AFn.applyTo(AFn.java:151) ~[clojure-1.4.0.jar:na]
        at clojure.core$apply.invoke(core.clj:603) ~[clojure-1.4.0.jar:na]
        at clojure.core$partial$fn__4070.doInvoke(core.clj:2343) ~[clojure-1.4.0.jar:na]
        at clojure.lang.RestFn.invoke(RestFn.java:397) ~[clojure-1.4.0.jar:na]
        at backtype.storm.event$event_manager$fn__2593.invoke(event.clj:39) ~[na:na]
        at clojure.lang.AFn.run(AFn.java:24) ~[clojure-1.4.0.jar:na]
        at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]
Caused by: java.io.EOFException: null
        at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2323) ~[na:1.7.0_25]
        at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2792) ~[na:1.7.0_25]
        at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:799) ~[na:1.7.0_25]
        at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299) ~[na:1.7.0_25]
        at backtype.storm.utils.Utils.deserialize(Utils.java:81) ~[storm-core-0.9.1-incubating.jar:0.9.1-incubating]
        ... 11 common frames omitted
2014-04-29 19:38:36 b.s.util [INFO] Halting process: (""Error when processing an event"")
{noformat}

Current workaround : full stop supervisor daemon and delete all Storm's data/supervisor directory helped, and after restarting Supervisor is now running smoothly. 

{anchor:links} Here is some references of very similar issues :
* http://mail-archives.apache.org/mod_mbox/storm-user/201402.mbox/%3C23100d14e7ac4cef947f7236ef8963e1@BY2PR08MB144.namprd08.prod.outlook.com%3E
* https://groups.google.com/forum/#!topic/storm-user/SL9FK9XeoI8
* https://groups.google.com/forum/#!topic/storm-user/2gapTYTRrX8

Regards,
"
STORM-303,Forward port of storm-kafka work,This is a placeholder issue for the patch at https://github.com/apache/incubator-storm/pull/94.
STORM-302,Fix Indentation for pom.xml in storm-dist,The build->plugins in storm-dist/binary/pom.xml and storm-dist/source/pom.xml have one of the plugin miss the indentation. 
STORM-301,Fix Indentation for pom.xml in storm-dist,The build->plugins in storm-dist/binary/pom.xml and storm-dist/source/pom.xml have one of the plugin miss the indentation. 
STORM-300,Add need_task_ids field to the multilang protocol description,"The Storm ShellBolt is aware of a need_task_ids field in messages received from non-JVM bolts (https://github.com/apache/incubator-storm/blob/master/storm-core/src/jvm/backtype/storm/task/ShellBolt.java :line 233).

This flag determines whether a ShellBolt will in fact respond with task IDs to a  request from a non-JVM bolt. No mention of this flag is made in the Storm multilang protocol.

I believe this flag should be added the the multilang protocol description or removed from its implementation.n Personally, I'm not a great fan of how this functionality has been implemented (on a per message basis)."
STORM-299,tick tuples stop in a long running topology,"We are running a topology that processes about 10K tuples a second on a single worker. There are about 80 executor threads across 3 different Bolt types, each receiving a tick tuple at a frequency of 1 second. We observe that the tick tuples stop coming in after few hours or sometime even after a day. The nimbus UI also shows that the tick tuples count has frozen whereas the same set of bolts continue to process tuples from other spouts. I have taken a stack trace and it shows following:

""Thread-8"" daemon prio=10 tid=0x00007f1a0fed2000 nid=0x4438 runnable [0x00007f19a5e01000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:349)
        at com.lmax.disruptor.AbstractMultithreadedClaimStrategy.waitForFreeSlotAt(AbstractMultithreadedClaimStrategy.java:99)
        at com.lmax.disruptor.AbstractMultithreadedClaimStrategy.incrementAndGet(AbstractMultithreadedClaimStrategy.java:49)
        at com.lmax.disruptor.Sequencer.next(Sequencer.java:127)
        at backtype.storm.utils.DisruptorQueue.publish(DisruptorQueue.java:113)
        at backtype.storm.disruptor$publish.invoke(disruptor.clj:51)
        at backtype.storm.disruptor$publish.invoke(disruptor.clj:53)
        at backtype.storm.daemon.executor$setup_ticks_BANG_$fn__3885.invoke(executor.clj:299)
        at backtype.storm.timer$schedule_recurring$this__1839.invoke(timer.clj:79)
        at backtype.storm.timer$mk_timer$thread_fn__1822$fn__1823.invoke(timer.clj:32)
        at backtype.storm.timer$mk_timer$thread_fn__1822.invoke(timer.clj:25)
        at clojure.lang.AFn.run(AFn.java:24)
        at java.lang.Thread.run(Thread.java:744)
 
   Locked ownable synchronizers:
        - None
 
""Thread-268-__system"" prio=10 tid=0x00007f1a0ef45000 nid=0x46b5 runnable [0x00007f17ef574000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x000000073d9a2060> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2176)
        at com.lmax.disruptor.BlockingWaitStrategy.waitFor(BlockingWaitStrategy.java:87)
        at com.lmax.disruptor.ProcessingSequenceBarrier.waitFor(ProcessingSequenceBarrier.java:54)
        at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:56)
        at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:62)
        at backtype.storm.daemon.executor$eval4023$fn__4024$fn__4038$fn__4089.invoke(executor.clj:749)
        at backtype.storm.util$async_loop$fn__364.invoke(util.clj:400)
        at clojure.lang.AFn.run(AFn.java:24)
        at java.lang.Thread.run(Thread.java:744)
 
   Locked ownable synchronizers:
        - None

We are running storm-0.9.0_wip21 and I would be glad to provide any other information.

Also see https://issues.apache.org/jira/browse/STORM-106.

Thanks,
Sushant"
STORM-296,Storm kafka unit tests are failing on windows,storm kafka unit tests fails on windows due to the default kafka tmp dir path.
STORM-294,Commas not escaped in command line,Now that parsing of the command line is done with JSON commas should be escaped so that they can be passed into the client.
STORM-292,emit blocks the publishing bolt if disrupter queue is full   ,"During testing, i notice once the disruptor queue is full, it blocks (timed_wait ) the publishing bolt essentially creating slowdown live-lock. Should the outputCollector.emit be a non-blocking call?

Also, configs for better control on the disruptor queue/buffer size seem to not have any impact.
 
""topology.executor.receive.buffer.size""
""topology.receiver.buffer.size""
""topology.executor.send.buffer.size""
""topology.transfer.buffer.size""

Below is example of topology that re-creates live lock scenario	 with disruptor queue.

https://github.com/kishorvpatil/incubator-storm/blob/dqueue-full/examples/storm-starter/src/jvm/storm/starter/SplitJoinTopology.java

""Thread-9-splitjoinbolt"" prio=10 tid=0x00007fe518a9f000 nid=0x64b5 waiting on condition [0x00007fe5144f2000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:349)
        at com.lmax.disruptor.SingleThreadedClaimStrategy.waitForFreeSlotAt(SingleThreadedClaimStrategy.java:129)
        at com.lmax.disruptor.SingleThreadedClaimStrategy.incrementAndGet(SingleThreadedClaimStrategy.java:81)
        at com.lmax.disruptor.Sequencer.next(Sequencer.java:127)
        at backtype.storm.utils.DisruptorQueue.publish(DisruptorQueue.java:113)
        at backtype.storm.disruptor$publish.invoke(disruptor.clj:51)
        at backtype.storm.daemon.executor$mk_executor_transfer_fn$this__4913.invoke(executor.clj:176)
        at backtype.storm.daemon.executor$mk_executor_transfer_fn$this__4913.invoke(executor.clj:183)
        at backtype.storm.daemon.executor$mk_executor_transfer_fn$this__4913.invoke(executor.clj:185)
        at backtype.storm.daemon.executor$fn__5141$fn__5155$bolt_emit__5184.invoke(executor.clj:683)
        at backtype.storm.daemon.executor$fn__5141$fn$reify__5190.emit(executor.clj:693)
        at backtype.storm.task.OutputCollector.emit(OutputCollector.java:186)
        at backtype.storm.task.OutputCollector.emit(OutputCollector.java:32)
        at backtype.storm.topology.BasicOutputCollector.emit(BasicOutputCollector.java:19)
        at storm.starter.bolt.SplitAndCountBolt.execute(SplitAndCountBolt.java:24)
        at backtype.storm.topology.BasicBoltExecutor.execute(BasicBoltExecutor.java:33)
        at backtype.storm.daemon.executor$fn__5141$tuple_action_fn__5143.invoke(executor.clj:634)
"
STORM-290,Exclude log4j and slf4j-log4j from curator dependency.,"The log4j and slf4j dependencies from curator/zookeeper sneak onto the class path as transient dependencies when storm-core is included as a project dependency. This causes an illegal argument exception due to multiple log bindings on the class path.

The solution is to exclude the log4j and slf4j dependencies in the storm-core pom rather than the parent pom.



"
STORM-289,Trident DRPC memory leak,"storm.trident.spout.RichSpoutBatchTriggerer.java

Missing code ""_msgIdToBatchId.put(r, batchIdVal);"" in emit method.

As a consequence, ack/fail will get null batchId for each DRPC invoking, which makes _finishConditions map larger and larger until out of memory."
STORM-275,"Topologies, Spouts and Bolts with special characters in name cannot be viewed in ui","If you run a topology with a spout or bolt that contains a whitespace in the name (i.e. ""Do some thing"") then the URL towards the component page will look something like this:

/topology/TOPONAME-12-12345678/component/Do+some+thing

The page that is opened on that link will try to show a component called

Do+some+thing

which it doesn't have."
STORM-274,Add support for command remoteconfvalue in storm.cmd,"

Currently storm.cmd does not support remoteconfvalue as supported by its linux counterpart storm.py.
"
STORM-272,Make worker receiver thread number configurable,"In my profiling, I found the receiver thread of worker can be a performance bottle-neck. 

Now each worker has single receiver thread, and it is responsbile to transfer information generated from multiple netty client to tens of executor disruptor queue. It is too much for busy topology.

Without this fix, we have to increase the number of workers, which will create more intra-worker traffic that we don't want. 
 
I suggest that we can add a config called ""worker.receiver.thread.count"" to control the parallism of the receiver thread, and make it default to 1."
STORM-271,Naming storm threads more meaningful names.,"In my test topology, each worker has 108 threads. Some  threads are generated from a thread pool with a vague name.

For example, the netty threads, the worker receiver thread, the worker transfer threads, the disruptor thread are not good named.

May be we should name them with a more meaningful name so that it is more easy for us to track the performance issues.

Here is the naming I used:

name            description
-------------------------------------
worker-transfer-thread: the single transfer thread for each worker
worker-receiver-thread: the receiver thread for each worker
netty-server-boss-host-port-seqenceId: the netty server boss thread.
netty-server-worker-host-port-sequenceId:the netty server worker thread
executor-disruptor-transfer-thread-[executorId]: executor transfer thread for disrupotor queue.
netty-client-boss-targetHost-port-sequenceId: the netty client bos thread
netty-client-worker-targetHost-port-sequenceId: the netty client worker thread."
STORM-269,Any readable file exposed via UI log viewer,"Note: This is actually version 0.9.0.1 but I couldn't choose that in the dropdown. I suspect that the problem still exists.

I found that it's possible to access any readable file on the system via the UI worker log viewer. To reproduce, navigate to:

http://<host:port>/log?file=../../../../../../../../etc/passwd"
STORM-268,Update guava to v16,"Storm currently uses guava 13, released August 2012. There are several things I would like to use in the latest release (v16) in my topology code, as well as various performance improvements and bug fixes. Could the storm dependency on guava be upgraded to v16.0.1?"
STORM-265,Upgrade Clojure to 1.5 (or 1.6),"Storm's version of Clojure is still pegged to the soon to be two versions old 1.4.

1.6 (currently in RC3) bring some version nice improvement and speedups in regard to hashcode and other things. Additionally because Storm's libs and user libs collide, any clojure libraries used with storm must be compatible back to 1.4, which can be quite a bother. "
STORM-263,Update Kryo version to 2.21+,"In a nutshell: As reported by Twitter (see below) there are apparently issues with Kryo versions prior to 2.21 which are causing data corruption [1].  Also, albeit of lesser critical importance, is that Storm's current insistence on Kryo 2.17 prevents (or at least unnecessarily complicates) the use of Twitter Chill/Bijection, which are helpful utility libraries to simplify data serialization in Storm topologies (e.g. when using Avro).

For this reason we may consider upgrading Storm's version of carbonite -- a Clojure library for Kryo -- which through a transitive dependency determines the actual Kryo version that Storm uses.

## Background

I originally discovered this when I ran into a version conflict between the Kryo versions used by Storm and by Twitter Chill.  Storm 0.9.1-incubating (latest version) uses Kryo 2.17 whereas Chill (latest version) uses Kryo 2.21.  Without resorting to `exclude` tricks in my build file I couldn't integrate Chill with Storm, and I wanted to use Chill/Bijection to simplify Avro encoding/decoding in my Storm topologies.

I filed an issue at the Chill project:

* CHILL-173: Kryo version conflict between Chill and Storm 0.9.1-incubating causes Avro serialization to fail [2]

Ian O'Connell (@ianoc) replied and pointed out that due to data corruption issues seen in production at Twitter when using Kryo < 2.21 the Chill project cannot downgrade from Kryo 2.21 to Storm's 2.17 version (Scalding, Spark, and Summingbird all use chill with Kryo at 2.21).

Storm as of 0.9.2 (trunk/master) is currently configured to use carbonite 1.3.2.  See the top-level pom.xml.

    <carbonite.version>1.3.2</carbonite.version>

Carbonite 1.3.2 depends on Kryo 2.17, where as the recently 1.3.3 depends on Kryo 2.21:
https://github.com/sritchie/carbonite/blob/master/project.clj

## Carbonite*

Storm uses {{com.twitter:carbonite}}, which is maintained by Sam Ritchie (@sritchie) at https://github.com/sritchie/carbonite.  Sam would be ok with a patch for carbonite to address this Kryo versioning issue, if needed.

[1] https://github.com/twitter/chill/issues/173#issuecomment-36534229
[2] https://github.com/twitter/chill/issues/173"
STORM-261,Workers should commit suicide if not scheduled any more.,"I know this is a bit far fetched.

If for some reason a supervisor dies and does not come back up again, dead HDD for example, but the workers remain up, and the scheduler decides to move the worker to a new host, a rebalance for instance, the old workers will never go away.  Ideally the worker should know that it is not running in the correct place any more and die instead of waiting for the supervisor to kill it."
STORM-260,Race condition in backtype.storm.utils.Time,"Some of my test runs were occasionally failing with a NullPointerException on [backtype.storm.utils.Time.java:64|https://github.com/apache/incubator-storm/blob/master/storm-core/src/jvm/backtype/storm/utils/Time.java#L64].

After a bit of investigation, it seems there's a race condition here; if we disable simulating mode while a thread is currently sleeping, then when it wakes up it won't re-check if it's still in ""simulating"" mode, it'll try to remove the sleep time, and get the NPE.

The attached patch is a fairly straightforward fix."
STORM-254,one Spout/Bolt can register metric twice with same name,"In a Bolt's prepare method, we can register metrics twice with the same name, using different timeBucketSizeInSecs parameter, like this:

    public void prepare(Map stormConf, TopologyContext context) {
    	mapper = new ObjectMapper();
    	
    	cMetric = new MTCountMetric();
    	context.registerMetric(""JavaBoltCount"", cMetric, 120);
    	ccMetric = new MTCountMetric();
    	context.registerMetric(""JavaBoltCount"", ccMetric, 60);
    }

----------------------------------------------------------------------------------------
This is caused by TopologyContext's registerMetric. In TopologyContext, all registered metrics holds in a map defined below:

private Map<Integer,Map<Integer, Map<String, IMetric>>> _registeredMetrics;

timeBucketSizeInSecs ----> __taskId ----> metricName ----> metirc
"
STORM-130,[Storm 0.8.2]: java.io.FileNotFoundException: File '../stormconf.ser' does not exist,"https://github.com/nathanmarz/storm/issues/438

Hi developers,

We met critical issue with deploying storm topology to our prod cluster.

After deploying topology we got trace on workers (Storm 0.8.2/zookeeper-3.3.6) :

2013-01-14 10:57:39 ZooKeeper [INFO] Initiating client connection, connectString=zookeeper1.company.com:2181,zookeeper2.company.com:2181,zookeeper3.company.com:2181 sessionTimeout=20000 watcher=com.netflix.curator.ConnectionState@254ba9a2
2013-01-14 10:57:39 ClientCnxn [INFO] Opening socket connection to server zookeeper1.company.com/10.72.209.112:2181
2013-01-14 10:57:39 ClientCnxn [INFO] Socket connection established to zookeeper1.company.com/10.72.209.112:2181, initiating session
2013-01-14 10:57:39 ClientCnxn [INFO] Session establishment complete on server zookeeper1.company.com/10.72.209.112:2181, sessionid = 0x13b3e4b5c780239, negotiated timeout = 20000
2013-01-14 10:57:39 zookeeper [INFO] Zookeeper state update: :connected:none
2013-01-14 10:57:39 ZooKeeper [INFO] Session: 0x13b3e4b5c780239 closed
2013-01-14 10:57:39 ClientCnxn [INFO] EventThread shut down
2013-01-14 10:57:39 CuratorFrameworkImpl [INFO] Starting
2013-01-14 10:57:39 ZooKeeper [INFO] Initiating client connection, connectString=zookeeper1.company.com:2181,zookeeper2.company.com:2181,zookeeper3.company.com:2181/storm sessionTimeout=20000 watcher=com.netflix.curator.ConnectionState@33a998c7
2013-01-14 10:57:39 ClientCnxn [INFO] Opening socket connection to server zookeeper1.company.com/10.72.209.112:2181
2013-01-14 10:57:39 ClientCnxn [INFO] Socket connection established to zookeeper1.company.com/10.72.209.112:2181, initiating session
2013-01-14 10:57:39 ClientCnxn [INFO] Session establishment complete on server zookeeper1.company.com/10.72.209.112:2181, sessionid = 0x13b3e4b5c78023a, negotiated timeout = 20000
2013-01-14 10:57:39 worker [ERROR] Error on initialization of server mk-worker
java.io.FileNotFoundException: File '/tmp/storm/supervisor/stormdist/normalization-prod-1-1358161053/stormconf.ser' does not exist
at org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:137)
at org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1135)
at backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:138)
at backtype.storm.daemon.worker$worker_data.invoke(worker.clj:146)
at backtype.storm.daemon.worker$fn__4348$exec_fn__1228__auto____4349.invoke(worker.clj:332)
at clojure.lang.AFn.applyToHelper(AFn.java:185)
at clojure.lang.AFn.applyTo(AFn.java:151)
at clojure.core$apply.invoke(core.clj:601)
at backtype.storm.daemon.worker$fn__4348$mk_worker__4404.doInvoke(worker.clj:323)
at clojure.lang.RestFn.invoke(RestFn.java:512)
at backtype.storm.daemon.worker$_main.invoke(worker.clj:433)
at clojure.lang.AFn.applyToHelper(AFn.java:172)
at clojure.lang.AFn.applyTo(AFn.java:151)
at backtype.storm.daemon.worker.main(Unknown Source)
2013-01-14 10:57:39 util [INFO] Halting process: (""Error on initialization"")

Supervisor trace:

2013-01-14 10:59:01 supervisor [INFO] d6735377-f0d6-4247-9f35-c8620e2b0e26 still hasn't started
2013-01-14 10:59:02 supervisor [INFO] d6735377-f0d6-4247-9f35-c8620e2b0e26 still hasn't starte
.......
2013-01-14 10:59:34 supervisor [INFO] d6735377-f0d6-4247-9f35-c8620e2b0e26 still hasn't started
2013-01-14 10:59:35 supervisor [INFO] Worker d6735377-f0d6-4247-9f35-c8620e2b0e26 failed to start
2013-01-14 10:59:35 supervisor [INFO] Worker 234264c6-d9d6-4e8a-ab0a-8926bdd6b536 failed to start
2013-01-14 10:59:35 supervisor [INFO] Shutting down and clearing state for id 234264c6-d9d6-4e8a-ab0a-8926bdd6b536. Current supervisor time: 1358161175. State: :disallowed, Heartbeat: nil
2013-01-14 10:59:35 supervisor [INFO] Shutting down d5c3235f-5880-4be8-a759-5654b3df6a27:234264c6-d9d6-4e8a-ab0a-8926bdd6b536
2013-01-14 10:59:35 util [INFO] Error when trying to kill 4819. Process is probably already dead.
2013-01-14 10:59:35 supervisor [INFO] Shut down d5c3235f-5880-4be8-a759-5654b3df6a27:234264c6-d9d6-4e8a-ab0a-8926bdd6b536
2013-01-14 10:59:35 supervisor [INFO] Shutting down and clearing state for id d6735377-f0d6-4247-9f35-c8620e2b0e26. Current supervisor time: 1358161175. State: :disallowed, Heartbeat: nil
2013-01-14 10:59:35 supervisor [INFO] Shutting down d5c3235f-5880-4be8-a759-5654b3df6a27:d6735377-f0d6-4247-9f35-c8620e2b0e26
2013-01-14 10:59:35 util [INFO] Error when trying to kill 4809. Process is probably already dead.
2013-01-14 10:59:35 supervisor [INFO] Shut down d5c3235f-5880-4be8-a759-5654b3df6a27:d6735377-f0d6-4247-9f35-c8620e2b0e26

Thanks!
Oleg M.

----------
xiaokang: We also encountered this problem and the DEBUG log showed that storm-code-map is null and new-assignment is not null. I may be that new-assignment is got from zk after storm-code-map. So we changed the storm-code-map after all-assignment and the problem disapeared.

(defn mk-synchronize-supervisor [supervisor sync-processes event-manager processes-event-manager]
  (fn this []
(let [conf (:conf supervisor)
      storm-cluster-state (:storm-cluster-state supervisor)
      ^ISupervisor isupervisor (:isupervisor supervisor)
      ^LocalState local-state (:local-state supervisor)
      sync-callback (fn [& ignored] (.add event-manager this))
      storm-code-map (read-storm-code-locations storm-cluster-state sync-callback)
      downloaded-storm-ids (set (read-downloaded-storm-ids conf))
      all-assignment (read-assignments
                       storm-cluster-state
                       (:supervisor-id supervisor)
                       sync-callback)
      new-assignment (->> all-assignment
                          (filter-key #(.confirmAssigned isupervisor %)))
      assigned-storm-ids (assigned-storm-ids-from-port-assignments new-assignment)
      existing-assignment (.get local-state LS-LOCAL-ASSIGNMENTS)]
  (log-debug ""Synchronizing supervisor"")
  (log-debug ""Storm code map: "" storm-code-map)
  (log-debug ""Downloaded storm ids: "" downloaded-storm-ids)
  (log-debug ""All assignment: "" all-assignment)
  (log-debug ""New assignment: "" new-assignment)

----------
NJtwentyone: I ran into having a similar problem once. Haven't had it lately. I was going to investigate more and before I created a posted but... There are two schedule-recurring functions of intertest in the function (mk-supervisor).

(schedule-recurring (:timer supervisor) 0 10 (fn ))
(schedule-recurring (:timer supervisor)
0
(conf SUPERVISOR-MONITOR-FREQUENCY-SECS)
(fn )))

So function-1 (synchronize-supervisor) will eventually remove that dir [../stormconf.ser]
And function-2 (sync-processes) will eventually hang wanting to create workers saying repeatedly
2013-01-13 22:13:13 b.s.d.supervisor [INFO] 6f73facd-8722-4b83-959c-a7b396c61224 still hasn't started

I'm using the distributed setup of (Storm 0.8.2/zookeeper-3.4.5) on my mac and a hp using storm.starter.WordCountTopology

----------
miggi: Well this problem was resolved only after reinstalling new instance of ZK (3.3.6), we had other issues with deploy topology on 0.8.1 version, so it was decided use new instance ZK. 
Btw: removing storm data folders didn't help

----------
nathanmarz: For those of you hitting the problem, it would be really useful if you could turn on DEBUG logging on the supervisor and show those logs when the problem is hit.

----------
miggi: After few months stable work - we ran into the same issue again.

Logs (""topology.debug: true"") 
Supervisor logs: http://cl.ly/0p2W3N0A2V3S
Worker logs: http://cl.ly/2w1j1r0a3X46

Cleaning/restarting ZK/Nimbus/Worker components didn't help. 
Now we down: haven't tried solution proposed by xiaokang but probably it would be our next steps.

Any fresh ideas ?

----------
devoncrouse: +1 - ran into this twice now. We've ""reset"" things by clearing out Storm stuff in Zk, wiping out Storm data directory, and restarting the topology, but it happens again in time.

Storm 0.9.0-wip16
Zk 3.3.6

----------
d2r: +1 seen this happen many times. same work-around
Most recently Storm 0.8.2-wip22

I'll try to collect a look and debug logs from the supervisor and have a look.

----------
miggi: Hey guys, 
try restart your cluster and ensure that all previous workers (java processes) was killed after this operation. 
Also we've updated cluster to storm 0.8.3-wip1 with possible fix.

Hope it helps 
----––––
Oleg M.

----------
devoncrouse: Simple restart doesn't cut it; have to totally clear out all Storm state in Zk and data directory (cluster otherwise quickly becomes unbalanced with same errors in the logs) - and again, we're on 0.9.0-wip16.

----------
xiaokang: We encountered the same problem in production cluster. The log showed that a worker died and supervisor and nimbus found that at the same time. Than nimbus assign the worker to other host and the supervisor's sync-supervisor delete topo's code while sync-process try to restart the died timeouted worker.

Our solution is change supervisor's woker timeout from 30 to 5 seconds and nimbus remain 30. It works.

----------
d2r: We have tried the same work-around from @xiaokang, but we still see the issue.

----------
nathanmarz: @d2r This is fixed in the 0.8.3 wip, please try that out and let me know if it fixes the issue for you.

----------
d2r: @nathanmarz Yes, pulling in the fix to supervisor.clj from 0.8.3 did resolve the issue for us. Thanks!

----------
devoncrouse: Is this fix going to be applied to the newer builds (e.g. 0.9.0) as well?

d5207b5

---------
viceyang: We encountered the same problem, our version is 0.8.3, i read supervisor code, it seems not a bug, the key reason is ""synchronize-supervisor"" which responsible for download file and remove file thread and ""sync-processes"" which responsible for start worker process thread is Asynchronous. 
see this case: in synchronize-supervisor read assigment information from zk, supervisor download necessary file from nimbus and write local state . in aother thread sync-processes funciton read local state to launch workor process, when the worker process has not start ,synchronize-supervisor function is called again topology's assignment information has changed (cased by rebalance,or worker time out etc.) worker assignment to this supervisor has move to another supervisor, synchronize-supervisor remove the unnecessary file (jar file and ser file etc.) , after this, worker launched by "" sync-processes"" ,ser file was not exsit , this issue occur. 
Though in this issue exception occured but logic sees right. this excetion can't compelety Eliminate unless change thread Asynchronous feather. 
there is some way reduction the exception occur, @xiaokang metioned ""change supervisor's woker timeout from 30 to 5 seconds and nimbus remain 30"" this method redeuce nimbus reassign topology. another way change ""synchronize-supervisor"" thread loop time to a longger than 10(default time) sec, such as 30 sec。"
